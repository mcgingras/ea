[{"_id": "ER4gAtS5LAx2T3Y98", "title": "A framework for comparing global problems in terms of expected impact", "postedAt": "2016-04-26T18:43:00.000Z", "htmlBody": "<p>Suppose you\u2019re trying to figure out whether to learn about health in developing countries; or whether to become a researcher in solar energy; or whether to campaign for criminal justice reform in the USA. Which of these areas is most effective to focus on?</p><p>A year\u2019s work on some problems can help a lot more people than a year\u2019s worth of work on other problems. In fact, our analysis suggests that which problem areas you choose to work on may be the single biggest determinant of the <a href=\"https://80000hours.org/articles/what-is-social-impact-definition/\">social impact</a> you have with your career.</p><p>We often make use of an informal framework for comparing problems in terms of their potential for an additional person to have a positive impact: <strong>scale</strong>, <strong>neglectedness</strong>, <strong>solvability</strong> and <strong>personal fit</strong>. Here\u2019s a <a href=\"https://80000hours.org/career-guide/most-pressing-problems/\">popular introduction to the framework</a>.</p><p>Applying the informal version of the framework is useful, and good enough for many situations, but it can lead to some issues like double counting. In this article, we outline a more precise, quantitative version of the framework, and give more details on how to apply it to make your own comparisons of areas.</p><p>The framework was first created by <a href=\"http://blog.givewell.org/2014/05/22/narrowing-down-u-s-policy-areas/\">Open Philanthropy</a>. We further developed this process in collaboration with staff at the <a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>, a research group at the University of Oxford that advises policy-makers and major decision-makers on how to prioritise global problems (learn more about <a href=\"https://80000hours.org/problem-profiles/global-priorities-research/\">global priorities research</a>).</p><p>This framework is just one tool we use to compare different global problems, and has a number of weaknesses. We discuss some of the pros and cons of this approach compared to qualitative approaches and cost-effectiveness analysis at the end of the article. Here\u2019s a more comprehensive <a href=\"https://80000hours.org/articles/comparing-problems/\">process you can use to compare global issues</a>. If you\u2019re coordinating with a community there are also some <a href=\"https://80000hours.org/articles/coordination/\">further factors to consider</a>.</p><p>If you just want to see the framework in action, here\u2019s a <a href=\"https://80000hours.org/articles/cause-selection/\">list of scores from 2017</a>.</p><p><i>This article was mostly written in 2017. We added some quick updates in October 2019 to further flesh out and update some of the main points, though it doesn\u2019t cover all our thinking on this issue.</i></p><h2><strong>Introducing how we define the factors</strong></h2><p>Ultimately what we want to know is the expected \u2018good done\u2019 per unit of resources invested in the problem. A unit of resources could be a year of labour or a dollar of donations, or some other measure.</p><p>This is hard to estimate by itself, so we need to break it down into components that we can estimate individually.</p><p>In our <a href=\"https://80000hours.org/career-guide/most-pressing-problems/\">introductory article</a>, we gave a rough breakdown into some qualitative factors. Here is more precise, quantified version of the same break down:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668002122/mirroredImages/ER4gAtS5LAx2T3Y98/da5tptuympyywhsqhwgr.png\" srcset=\"https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-03-at-9.57.11-pm-1024x420.png 1024w, https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-03-at-9.57.11-pm-300x123.png 300w, https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-03-at-9.57.11-pm-768x315.png 768w, https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-03-at-9.57.11-pm.png 1049w\"></figure><p>A good reason to do it this way, is that if we multiply these three terms together, we will get back to \u2018good done\u2019 / \u2018extra person or $\u2019 allocated to the problem:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668002122/mirroredImages/ER4gAtS5LAx2T3Y98/i69l6exmosf5fwiwsn5x.png\" srcset=\"https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-07-at-7.58.39-pm.png 1171w, https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-07-at-7.58.39-pm-300x128.png 300w, https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-07-at-7.58.39-pm-768x329.png 768w, https://80000hours.org/wp-content/uploads/2016/04/Screen-Shot-2017-02-07-at-7.58.39-pm-1024x438.png 1024w\"></figure><p>So, now we\u2019ve broken \u201cgood done per $\u201d into three components that each have a quantitative definition. What are these components, in plain English?</p><ul><li><strong>Scale</strong> \u2013 if we solved the problem, how good would it be?</li><li><strong>Solvability</strong> \u2013 if we doubled the resources dedicated to solving this problem, what fraction of the problem would we expect to solve?</li><li><strong>Neglectedness</strong> \u2013 how many resources are already going towards solving this problem?</li></ul><p>Finally, if you\u2019re trying to figure out what problem <i>you</i> should work on, you can add bonus points for problems that you are better suited to working on, which we explain in more depth later.</p><p>Below we\u2019ll discuss how to assess each in turn, but before that we have a few more remarks on how to set up the analysis.</p><h2><strong>Defining a problem carefully</strong></h2><p>Before you go on to make your assessment, make sure you have a clear description of the scope of the problems you want to compare. This will help you to be consistent when scoring each factor. For example, if we were evaluating \u2018global health\u2019 we would need to be clear about:</p><ul><li>Which diseases are included (e.g. TB, HIV, Malaria, etc)</li><li>Which countries we are considering (i.e. only the poorest countries, or also middle income countries?)</li></ul><p>A challenge of any framework of this kind will be that carefully chosen \u2018narrow\u2019 problems tend to do better than broadly defined ones. For example, \u2018combating malaria\u2019 will look more pressing than \u2018global health\u2019 because malaria is a particularly promising health problem to work on. Similarly, improving health in Kenya is going to look more impressive than improving health in Costa Rica. There\u2019s nothing wrong with these findings \u2013 but they could create a misleading impression if a broadly defined problem is compared with a narrowly defined one. If someone were motivated they could make a problem look more or less pressing by defining it differently \u2013 and this is something to be aware of in interpreting these scores.</p><h2><strong>Creating a (logarithmic) scale</strong></h2><p>If you try to compare different areas using these scores you\u2019ll find they vary hugely. For instance, while about <a href=\"https://80000hours.org/problem-profiles/health-in-poor-countries/\">$300bn is spent on global health annually</a>, under <a href=\"https://80000hours.org/problem-profiles/factory-farming/\">$100m is spent trying to tackle factory farming</a>. So, factory farming is over 1,000 times more neglected than global health.</p><p>This means it\u2019s more convenient to use a \u201clogarithmic scale\u201d to rate each component. The way we do this is every two points we add to a problem means that it\u2019s 10x more effective. For instance, if we give one problem a neglectedness score of 4 and another of 6, then we mean the second one is 10 times more neglected.</p><p>This is like the <a href=\"https://en.wikipedia.org/wiki/Richter_magnitude_scale\">Richter scale</a> used to measure earthquakes. An earthquake that that\u2019s Richter 8 is actually 10-times more powerful than one that\u2019s Richter 7.</p><p>Using a logarithmic scale for each part also means that rather than having to multiply <i>Scale</i>, <i>Solvability</i> and <i>Neglectedness</i> in order to get our overall cost-effectiveness estimate, we can simply add them together. (As the quantitatively inclined among you might recall from high school, this is because log(AB) = log(A) + log(B).)</p><p>To make the scores easy to read we put them all on a scale between 0 and 16. For comparisons of cost-effectiveness between different problems, we will just be looking at the <i>difference</i> in score between them.</p><h2><strong>How to assess scale</strong></h2><h3><strong>Definition</strong></h3><p>If we solved this problem, by how much would the world become a better place?</p><p>For example, cancer is a bigger problem than malaria because it is responsible for 8% of all ill-health worldwide (measured in <a href=\"https://en.wikipedia.org/wiki/Quality-adjusted_life_year\">QALYs</a> lost), whereas malaria is responsible for 2.7% of ill-health worldwide.<a href=\"https://80000hours.org/articles/problem-framework/#fn-1\"><sup>1</sup></a> If we got rid of all cancer it would reduce ill-health significantly more than if we got rid of all malaria.</p><p>One way to measure the scale of a problem in terms of its effect on wellbeing, because this is something most people care about, and we have tools to allow comparison between different kinds of benefits. (Though we don\u2019t claim wellbeing is all that matters \u2013 read more about <a href=\"https://80000hours.org/articles/the-meaning-of-making-a-difference/\">the definition</a>.)</p><p>This means scale can be increased by either (i) affecting a larger number of people (ii) affecting the same number of people in a bigger way, including both short-term and long-term effects. We use a broad notion of wellbeing, so the effect could improve many aspects of someone\u2019s life, including: happiness, health, a sense of meaning, positive relationships, and so on.</p><p>In practice, we take a <a href=\"https://80000hours.org/articles/future-generations\">longtermist perspective</a>, so for us, assessing scale comes down to working out which issues have the greatest significance for future generations.</p><p>If you have different values from us, you can use the framework with a different definition of scale.</p><p>It can also be useful to group instrumental sources of value within scale, such as gaining information about which issues are most important, or building a movement around a set of issues. Ideally, one would also capture the spillover benefits of progress on this problem on other problems. Coordination considerations, as briefly covered later, can also change how to assess scale.</p><p>Note that we also defined scale as the good done by solving the whole problem; however, you could also do the analysis with solving 10% of the problem, so long as you do that consistently with the other factors as well.</p><h3><strong>How to assess it</strong></h3><p>Sometimes it\u2019s possible to make fairly precisely quantified comparisons of scale, like in the case of cancer vs. malaria above.</p><p>However, often this is not the case. This is particularly true when you try to factor in the long-run and indirect effects of solving a problem. Suppose you make a breakthrough in physics \u2013 how many people are ultimately going to be affected? It\u2019s hard to say, but we shouldn\u2019t conclude from this that physics breakthroughs don\u2019t matter.</p><p>To make more wide ranging comparisons between problems, you need to turn to \u201cyardsticks\u201d for scale. These are more measurable ways of comparing scale that we hope will correlate with long-run social impact.</p><p>For instance, economists often use GDP growth as a convenient yardstick for economic progress (although it has many weaknesses). Nick Bostrom has argued that the key yardstick for long run welfare should be whether an action increases or decreases the risk of the end of civilization \u2013 what he called <a href=\"http://www.nickbostrom.com/existential/risks.html\">existential risk</a>.</p><p>Here is one set of yardsticks (on the top row), which we assess with the following rubric:</p><figure class=\"table\"><table style=\"background-color:transparent;border-left:2px solid rgb(221, 221, 221);border-right:2px solid rgb(221, 221, 221);border-top:2px solid rgb(221, 221, 221)\"><thead><tr><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;height:50px;width:6000px\" colspan=\"1\" rowspan=\"1\"><strong>If we solved this problem it would be equivalent to:</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;height:50px;width:6000px\" colspan=\"1\" rowspan=\"1\"><strong>A reduction in the risk of extinction (or increase in the expected value of the future) of:</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;height:50px;width:6000px\" colspan=\"1\" rowspan=\"1\"><strong>Raising global economic output proportionally by this amount per year:</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;height:50px;width:7000px\" colspan=\"1\" rowspan=\"1\"><strong>Increase in income among the world\u2019s poorest 2 billion people:</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;height:50px;width:7000px\" colspan=\"1\" rowspan=\"1\"><strong>Saving this many years of healthy life each year:</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;height:50px;width:7000px\" colspan=\"1\" rowspan=\"1\"><strong>Illustrative Example</strong></th></tr></thead><tbody><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">16</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10%</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">&nbsp;</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">&nbsp;</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">&nbsp;</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Eliminate the risk of both nuclear war and pandemics</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">14</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1%</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$100 trillion</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$3 trillion</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1 billion QALYs</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Eliminate extreme poverty</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">12</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.1%</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$10 trillion</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$300 billion</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100 million QALYs</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Cure cancer</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.01%</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$1 trillion</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$30 billion</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10 million QALYs</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Increase aid by a third and spent it on cash transfers</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">8</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.001%</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$100 billion</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$3 billion</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1 million QALYs</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Eliminate land use restrictions in major US cities</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">6</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.0001%</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$10 billion</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$300 million</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100,000 QALYs</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Remove 5 min/day needless red tape for US teachers</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">4</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.00001%</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$1 billion</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$30 million</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10,000 QALYs</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Identify all risky asteroids</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">2</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.000001%</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$100 million</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$3 million</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1,000 QALYs</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Turn 10,000 people vegan</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.0000001%</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$10 million</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">300000</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100 QALYs</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Save 3 lives</td></tr></tbody></table></figure><p>This process is most robust when you\u2019re comparing problems that use the same yardstick e.g. comparing several problems in terms of how much they improve health. <strong>The tradeoffs </strong><i><strong>across</strong></i><strong> the columns are extremely uncertain</strong>, and an active topic of research by groups like the Future of Humanity Institute.</p><p><strong>The trade offs across columns are also very sensitive to big worldview and value judgements.</strong> People disagree over what most matters to wellbeing, how to value people in the future, and how to value non-humans. For example, some people believe that general economic growth is not good at all, because of unintended side effects (e.g. climate change, or the more rapid invention of dangerous new technologies). (<a href=\"https://80000hours.org/problem-quiz/\">This tool</a> leads you through some of the most important judgement calls.)</p><p>The rubric above reflects our own considered judgement calls at how to trade different yardsticks against each other. We don\u2019t fully explain our reasoning, which is hard to make explicit. Read more about <a href=\"https://80000hours.org/articles/yardsticks/\">how to choose a yardstick</a> and also see <a href=\"https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom/\">Crucial Considerations and Wise Philanthropy</a> by Nick Bostrom. (Unfortunately, this article and our choice of yardsticks are not fully up-to-date with our latest thinking, though the broad ideas and direction are still correct.)</p><p>If a problem helps with several columns, then focus on the column where the problem has the largest effect. Since each row corresponds to a factor of ten, the top rows will dominate the overall assessment of scale.</p><h2><strong>How to assess how neglected a problem is</strong></h2><h3><strong>Definition</strong></h3><p>How many people, or dollars, are currently being dedicated to solving the problem?</p><h3><strong>Why is it important?</strong></h3><p>After a large amount of resources have been dedicated to a problem, you\u2019ll hit <i>diminishing returns</i>. This is because people take the best opportunities for impact first, so as more and more resources get invested, it becomes harder and harder to make a difference. It\u2019s therefore often better to focus on problems that have been neglected by others.</p><p>For instance, mass immunisation of children is an extremely effective intervention to improve global health, but it is already being vigorously pursued by governments and several major foundations, including the Gates Foundation. This makes it less likely to be a top opportunity for future donors.</p><p>It\u2019s also valuable to explore new problems because this can help us figure out which problems are in fact most pressing. That is to say, there is an additional \u2018value of information\u2019 from trying new things. If no-one has worked on a problem before, then it could easily turn out to be more solvable than is currently thought.</p><p>There are <a href=\"https://80000hours.org/2015/11/stop-talking-about-declining-returns-in-small-organisations/\">some mechanisms by which problem areas can see increasing returns</a> rather than diminishing returns. However, we think there <a href=\"http://www.fhi.ox.ac.uk/cost-effectiveness-of-research-overview/\">are good theoretical and empirical arguments</a> that diminishing returns are the norm, and that returns most likely diminish logarithmically. Increasing returns might hold at very small scales within problem areas, though we\u2019re not even sure about that due to the value of information benefits mentioned above. (Increasing returns seem more likely to be common within organisations rather than problem areas.)</p><p>Also note that neglectedness is only a good proxy if the area is being neglected <i>for bad reasons</i> by other actors. However, we think that society\u2019s mechanisms for doing good are far from efficient, so all else equal, neglectedness is a good sign.</p><p>One particularly important way that a problem can end up neglected for bad reasons is if other people simply don\u2019t value it. <a href=\"https://80000hours.org/2014/01/neglectedness-and-impact/\">This article</a> argues that if you care about something X times more than the average person, you should expect to be able to have X times as much impact by working on that area (by your lights). For instance, we think that the interests of future generations are dramatically undervalued by society, so by working on issues that aid future generations, we can have far more impact.</p><h3><strong>How to assess it</strong></h3><figure class=\"table\"><table style=\"background-color:transparent;border-left:2px solid rgb(221, 221, 221);border-right:2px solid rgb(221, 221, 221);border-top:2px solid rgb(221, 221, 221)\"><thead><tr><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:106px\" colspan=\"1\" rowspan=\"1\"><strong>Crowdedness Score</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:105px\" colspan=\"1\" rowspan=\"1\"><strong>What is the direct annual spending on the problem?</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:114px\" colspan=\"1\" rowspan=\"1\"><strong>What is the number of full time staff working on the problem?</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:128px\" colspan=\"1\" rowspan=\"1\"><strong>What is the number of active supporters of work on the problem?</strong></th></tr></thead><tbody><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">12</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$100,000 or less</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1 or less</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1,000 or less</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$1 million</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10000</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">8</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$10 million</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100000</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">6</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$100 million</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1000</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1 million</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">4</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$1 billion</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10000</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10 million</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">2</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$10 billion</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100000</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100 million</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">$100 billion</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1 million</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1 billion (i.e. everyone)</td></tr></tbody></table></figure><h3><strong>A challenge \u2013 direct vs. indirect &amp; future effort</strong></h3><p>Often resources are unintentionally dedicated to solving a problem by groups that may be self-interested, or working on an adjacent problem. We refer to this as \u2018indirect effort\u2019, in contrast with the \u2018direct effort\u2019 of groups consciously focused on the problem. These indirect efforts can be substantial. For example, not much money is spent on research to prevent the causes of ageing directly, but many parts of biomedical research are contributing by answering related questions or developing better methods. While this work may not be well targeted on reducing ageing specifically, much more is spent on biomedical research in general than anti-ageing research specifically. Most of the progress on preventing ageing is probably due to these indirect efforts.</p><p>Indirect efforts are hard to measure, and even harder to adjust for how useful they are for solving the problem at hand.</p><p>For this reason we usually score only \u2018direct effort\u2019 on a problem. Won\u2019t this be a problem, because we will be undercounting the total effort? No, because we will adjust for this in the next factor: <i>Solvability</i>. Problems where most of the effective effort is occurring indirectly will not be solved as quickly by a large increase in \u2018direct effort\u2019.</p><p>One could also use a directed-weighted measure of effort. So long as it was applied consistently in evaluating both <i>Neglectedness</i> and <i>Solvability</i>, it should lead to roughly the same answer.</p><p>Another challenge is how to take account of the fact that some problems might receive much more <i>future</i> effort than others. We don\u2019t have a general way to solve this, except (i) it\u2019s reason not to give extremely low neglectedness scores to any area (ii) one can try to consider the future direction of resources rather than only resources today.</p><h3><strong>More tips on how to assess</strong></h3><p>Rather than trying to assess neglectedness directly, you can also consider rules of thumb like the following. These help you to work out how neglected it is and whether it\u2019s being neglected for bad reasons.</p><ul><li>Is there any reason to expect this problem not to be solved by: (i) markets (ii) government (iii) other individuals looking to have a social impact?</li><li>Within research, is this a new field, or at the intersection of two disciplines? These areas are most likely to get neglected by academia. (<a href=\"https://80000hours.org/articles/research-2/#what-are-the-highest-impact-research-topics\">Read more about choosing a research topic</a>.)</li><li>If you don\u2019t work on the problem, how likely is it someone else will step in instead?</li><li>If you work on this problem, will you learn more about how pressing it is compared to other problems?</li></ul><p>Thinking through these questions can increase your confidence you haven\u2019t missed anything in your estimate.</p><p>Note that it\u2019s important to assess scale and neglectedness as a pair. Ultimately we care about the ratio of the two, so you need to make sure you\u2019re assessing the <i>same problem in both cases</i>. If you use a different definition of the problem in each case, it will throw off your results.</p><p>If several different kinds of input are being dedicated to a problem, use the column with the lowest score. That will be where most of the total resources are: e.g. if $10bn per year is invested in a problem and 1,000 full-time people work on it, then the money dominates, so the score is 4, not 8.</p><p>Finally, we are reluctant to give very high scores for neglectedness. Even obscure problems usually attract the attention of some group in the world, and we may simply not know about them. So unless we\u2019ve done a comprehensive search to show otherwise, we will assume that at least $1 million is being directed towards a problem.</p><h2><strong>How to assess how solvable a problem is</strong></h2><h3><strong>Definition</strong></h3><p>If we doubled direct effort on this problem, what fraction of the remaining problem would we expect to solve?</p><h3><strong>Why is it important?</strong></h3><p>Even if a problem is hugely important and highly neglected, that doesn\u2019t mean it\u2019s an important cause to focus on. There might simply be very little we can do about it.</p><p>For example, ageing is a problem that is huge in scale: almost two thirds of global ill health is a result of ageing in some way. It\u2019s also highly neglected: there are only a tiny number of research institutes focused on trying to prevent the causes of physical ageing (rather than to treat its symptoms, like cancer, stroke, Alzheimer\u2019s, and so on). However, one reason it\u2019s neglected is because many scientists believe it to be very hard to solve, which is a major reason against working on the problem right now (though its other advantages could be enough to offset this downside).</p><h3><strong>How to assess it</strong></h3><p>We use this rubric:</p><figure class=\"table\"><table style=\"background-color:transparent;border-left:2px solid rgb(221, 221, 221);border-right:2px solid rgb(221, 221, 221);border-top:2px solid rgb(221, 221, 221)\"><thead><tr><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:92px\" colspan=\"1\" rowspan=\"1\"><strong>Solvability score</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:431px\" colspan=\"1\" rowspan=\"1\"><strong>The doubling of the direct effort described in \u2018Neglectedness\u2019 would be expected to solve this much of the problem (defined in Scale):</strong></th></tr></thead><tbody><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">8</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">100%</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">6</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">10%</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">4</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">1%</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">2</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.1%</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0.01%</td></tr></tbody></table></figure><p>Some rules of thumb we consider include:</p><ul><li>Are there cost-effective interventions for making progress on this problem with rigorous evidence behind them? (ideally high up the <a href=\"https://en.wikipedia.org/wiki/Hierarchy_of_evidence\">hierarchy of evidence</a>).</li><li>Are there promising but unproven interventions that can be cheaply tested?</li><li>Are there theoretical arguments that progress should be possible, such as a good track record in a related area? (e.g. we can\u2019t prove that medical research will be effective ahead of time, but the area has a strong track record, and rough estimates suggest it\u2019s very effective).</li><li>Are there interventions that could make a huge contribution to solving the problem, even if unlikely to work?</li></ul><p>In general, we\u2019re looking to find the best interventions to make progress on the problem, then evaluate them based on (i) potential upside (ii) likelihood of upside. We consider all forms of evidence, from rigorous trial data and speculative arguments. We take a <a href=\"https://en.wikipedia.org/wiki/Bayesian_inference\">Bayesian</a> approach to evaluating both factors \u2013 our prior is that the intervention isn\u2019t very effective, then we update away from that depending on the strength of the evidence (see an <a href=\"https://80000hours.org/2012/12/how-to-judge-your-chances-of-success/\">example</a>). <a href=\"http://lesswrong.com/lw/hzu/model_combination_and_adjustment/\">Read more about making these kinds of estimates</a>.</p><h3><strong>Challenges in assessment</strong></h3><p>This is typically the hardest of the three factors to score because it requires anticipating the future, rather than simply measuring things that currently exist.</p><p>In some cases, you can estimate solvability based on the cost-effectiveness of existing techniques in a field. For example, we have a sense of how many lives would be saved by increasing spending on global health interventions based on past experience tackling HIV, malaria, tuberculosis and so on.</p><p>In other cases \u2013 where solving a problem requires innovative techniques \u2013 the scores are usually assigned based on judgement calls, ideally based on a survey of expert opinion.</p><p>Some approaches to solving problems are incremental (e.g. distributing bednets to reduce exposure to malaria-carrying mosquitoes); others offer some chance of solving a lot of the problem all at once (e.g. inventing a new malaria vaccine). For scoring we use the \u2018expected value\u2019 approach. That is, a 10% chance of solving all of a problem is scored the same as a project that would definitely reduce it by 10%. (While <a href=\"https://concepts.effectivealtruism.org/concepts/risk-aversion/\">\u2018risk aversion\u2019</a> about different outcomes means that these aren\u2019t necessarily equally valuable, it\u2019s a good approximation.)</p><p>As discussed above in <i>Neglectedness</i>, problems for which most of the work is being performed indirectly (e.g. by for-profits doing related things) will likely be solved more slowly through an increase in \u2018direct\u2019 work. This is because many promising approaches will already have been attempted by other groups.</p><h2><strong>What do the summed scores mean?</strong></h2><p>To do a sanity-check we can add these scores and convert them back into a measure of actual impact from one additional person working on a problem:</p><figure class=\"table\"><table style=\"background-color:transparent;border-left:2px solid rgb(221, 221, 221);border-right:2px solid rgb(221, 221, 221);border-top:2px solid rgb(221, 221, 221)\"><thead><tr><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:129px\" colspan=\"1\" rowspan=\"1\"><strong>If the problem has this score:</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:176px\" colspan=\"1\" rowspan=\"1\"><strong>One extra person working on a problem...</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:183px\" colspan=\"1\" rowspan=\"1\"><strong>One extra person working on a problem...</strong></th></tr></thead><tbody><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">28</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Saves 1 million QALYs per year</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Reduces existential risk by 0.001%</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">24</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Saves 10,000 QALYs per year</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Reduces existential risk by 0.00001%</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">20</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Saves 100 QALYs per year (2 lives)</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">Reduces existential risk by 0.0000001%</td></tr></tbody></table></figure><p>However, these figures are extremely approximate, so we don\u2019t recommend putting weight on them specifically. Rather, we prefer to use the scores to make <i>relative comparisons</i> rather than absolute estimates.</p><h2><strong>How to assess personal fit</strong></h2><p>While personal fit is not assessed in our problem profiles, it is relevant to your personal decisions. If you enter an area that you find totally demotivating, then you\u2019ll have almost no impact. Within a field, the top performers often have <a href=\"https://80000hours.org/career-guide/personal-fit/\">10 to 100 times</a> as much impact as the median.</p><p>If you are comparing different problems you can use these extra scores to give a bonus to problems you are well suited to tackling.</p><h3><strong>Definition</strong></h3><p>Given your skills, resources, knowledge, connections and passions, how likely are you to excel in this area?</p><h3><strong>How can it be assessed?</strong></h3><ul><li>What\u2019s your most valuable career capital? Is it especially relevant to one problem and not the others?</li><li>How motivated do you expect to be if you worked on this problem?</li><li>What specific roles could you take in this problem, and do you expect you\u2019d excel at them?</li></ul><p>Here\u2019s our introductory advice on <a href=\"https://80000hours.org/articles/personal-fit/\">how to assess personal fit</a>, and some <a href=\"https://80000hours.org/articles/framework/#personal-fit\">further questions for making predictions</a>.</p><p>Here\u2019s a rubric you could use:</p><figure class=\"table\"><table style=\"background-color:transparent;border-left:2px solid rgb(221, 221, 221);border-right:2px solid rgb(221, 221, 221);border-top:2px solid rgb(221, 221, 221)\"><thead><tr><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:200px\" colspan=\"1\" rowspan=\"1\"><strong>Personal fit score</strong></th><th style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid;padding:10px 20px 10px 15px;width:428px\" colspan=\"1\" rowspan=\"1\"><strong>How well matched are your skills for this area?</strong></th></tr></thead><tbody><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">4</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">You are exceptionally well-suited to the area. You are well-motivated and could be a world-leader in the field.</td></tr><tr><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">2</td><td style=\"background-color:rgb(245, 245, 245);border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">You are a reasonable fit for the field. Quite motivated and some relevant skills.</td></tr><tr><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">0</td><td style=\"background-color:transparent;border-bottom:2px solid rgb(221, 221, 221);padding:10px 15px\">You are an actively bad fit for this field because you couldn\u2019t be motivated to work on it or have no relevant skills.</td></tr></tbody></table></figure><p>Note that the importance of personal fit depends on how you\u2019re planning to contribute. A great entrepreneur or researcher has <i>far</i> more impact than an average one, so if you\u2019re planning to contribute in either of those ways, personal fit matters a lot. However, if you\u2019re earning to give, personal fit is less relevant because you\u2019re sending money rather than your unique skills. So to assess personal fit in more depth, you could estimate your percentile in the field, then multiply by a factor that depends on the variation of performance in the field.</p><p>Bear in mind that it\u2019s easy to underestimate the extent to which you can become knowledgeable and passionate about a new problem. We\u2019re biased towards continuing with what we\u2019ve done before\u2014<a href=\"https://80000hours.org/2012/10/sunk-costs-in-careers/\">the \u2018sunk cost fallacy\u2019</a>\u2014and we <a href=\"https://80000hours.org/2015/02/we-change-more-than-we-expect-so-keep-your-options-open/\">underestimate how much our preferences and passions will change</a>.</p><p>Finally, remember that a single problem can often be tackled in multiple ways. If you want to work on global health, you could work on the ground in the developing world, conduct biomedical research, go into politics and many other options besides. If one isn\u2019t a good fit for you, another might be.</p><h2><strong>Other factors for comparing career opportunities</strong></h2><p>To come to an all considered view on whether to take a job, you also need to consider the other factors in our <a href=\"https://80000hours.org/articles/framework/\">career framework</a>, such as:</p><ol><li>How influential a role you can get.</li><li>How much career capital you can get.</li><li>The value of information of working on this option.</li></ol><p>In this article, we only cover comparisons of problem areas, but that\u2019s not all that\u2019s relevant.</p><h2><strong>How should we interpret the results?</strong></h2><p>If you\u2019ve used our rubric above, you can add the scores together to get a rough answer of which problem will be more effective to work on.<a href=\"https://80000hours.org/articles/problem-framework/#fn-2\"><sup>2</sup></a> Bear in mind that these scores are imprecise, and adding them increases the uncertainty even further, because we only measure each one imprecisely. This means you need to take your final summed score with a grain of salt \u2013 or rather a lot of salt.</p><p>Within 80,000 Hours, if the difference in score between two problems is 4 or larger, we have a reasonable level of confidence that it\u2019s a more effective problem to work on. If the difference is 3 or smaller it looks more like a close call.</p><p>The scores we get when using this framework suggest that some problems are 10,000x more effective to work on than others. However, we don\u2019t believe that the differences really are that large. For one, our scores have to be tempered by common sense judgements about the world. If the score for one problem seems very high, then it\u2019s possible we\u2019ve simply made a mistake and don\u2019t realise it. For two, because the future is so unpredictable, work on problems that don\u2019t seem pressing could turn out to be very useful in unexpected ways. That puts a limit on how much more pressing one problem can be than another.</p><p>Some other reasons for being modest about what such prioritisation research can show us are <a href=\"http://reducing-suffering.org/why-charities-dont-differ-astronomically-in-cost-effectiveness/\">discussed here</a>.</p><p>For more tips on making difficult judgement calls, take a look at <a href=\"https://80000hours.org/articles/making-an-assessment/\">our checklist</a>.</p><h2><strong>How does this approach compare with ordinary cost-effectiveness analysis?</strong></h2><p>What we want to know is this: \u2018if I add an additional unit of resources to solving this problem, how much good will be accomplished\u2019? The approach above looks at a problem from a bird\u2019s eye view, and tries to assess how important it is to allocate more resources to solving it.</p><p>An alternative approach would be to look at the <a href=\"https://en.wikipedia.org/wiki/Cost-effectiveness_analysis\">cost-effectiveness</a> of past interventions to tackle different problems, and compare them directly against one another. For example, you could look at studies of different approaches we know about to improve education or health, and then calculate which one would help people more with an additional $1 million in funding. If this kind of cost-effectiveness data is available, and you already have a common outcome <a href=\"https://80000hours.org/articles/cause-selection/yardsticks/\">yardstick</a>, this is a sensible approach. For instance, in health economics, people often calculate \u201cQALYs per dollar\u201d for different interventions.</p><p>If you\u2019re comparing two problems that use different yardsticks, you can still compare them so long as you have a conversion factor, though the comparisons become much more uncertain. For instance, you could compare health interventions to climate change interventions by defining the rate at which you\u2019d trade 1 QALY for 1 tonne of carbon dioxide averted. Our rubric in the scale section above shows roughly how we\u2019d trade some yardsticks against others.</p><p>Alternatively you can try to convert all the benefits into dollar terms, and perform a \u2018<a href=\"https://en.wikipedia.org/wiki/Cost-benefit_analysis\">cost-benefit analysis</a>\u2019. This is expressed as a ratio of costs to benefits, both in dollars.</p><p>The main reason not to take this approach is that it\u2019s extremely hard in many cases:</p><ol><li>Political advocacy, in which the circumstances you are working with are constantly shifting.</li><li>Original research, where no-one knows how long it will take to make a new discovery.</li><li>Any field in which no interventions are known, or the ones that we know about are not well studied.</li></ol><p>It\u2019s for this reason we have created the alternative framework above that can be applied to almost any problem.</p><h2><strong>Advantages and disadvantages of quantitative problem prioritisation</strong></h2><p>There are several benefits to going through the process above:</p><ul><li>Explicitly quantifying outcomes can enable you to notice large, robust differences in effectiveness that might be difficult to notice qualitatively, and help you to avoid <a href=\"https://en.wikipedia.org/wiki/Scope_neglect\">scope neglect</a>.</li><li>Going through the process of making these estimates is a great way to test your understanding of a problem, since it forces you to be explicit about your assumptions and how they fit together.</li><li>A clearly laid out analysis can help others to understand and critique your reasoning, further helping you to understand the problem and how pressing it is.</li></ul><p>But there is a major downside we need to keep in mind:</p><ul><li>In practice, these types of estimates usually involve very high levels of uncertainty. This means their results are not robust: different assumptions can greatly alter the conclusion of the analysis. As a result, there is a danger of being misled by an incomplete model, when it would have been better to go with a broader qualitative analysis, or simple common sense.</li></ul><p>This is why we don\u2019t simply go with the results of our scores. Rather, we consider other forms of evidence in our <a href=\"https://80000hours.org/problem-profiles/\">problem profiles</a> to make an overall assessment.</p><p>For more on this topic, see <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">GiveWell\u2019s discussion</a> of the weaknesses of \u2018sequence thinking\u2019 (which corresponds to an approach that\u2019s heavily reliant on cost-effectiveness analysis) compared to \u2018cluster thinking\u2019. You can also see a <a href=\"http://reducing-suffering.org/quantify-with-care/\">discussion of the pros and cons of quantification</a>.</p><h2><strong>How to factor in coordination?</strong></h2><p>An individual can only focus on one or two areas at a time, but a large group of people working together should most likely spread out over several.</p><p>When this happens, there are additional factors to consider when choosing a problem area. Instead of aiming to identify the single most pressing issue at the margin, the aim is to work out:</p><ol><li>The ideal <i>allocation</i> of people over issues, and which direction that allocation should move in.</li><li>Where your comparative advantage lies compared to others in the group.</li></ol><p>We call this the \u2018portfolio approach\u2019. <a href=\"https://80000hours.org/articles/coordination/#3-take-the-portfolio-approach\">Read more</a>.</p><p>Factoring in coordination can also have other effects on which problems to prioritise. For instance, it can be worth doing more work on an area than it first seems in order to compromise with or do moral trade with the other people you\u2019re coordinating with. <a href=\"https://80000hours.org/articles/coordination/#be-more-willing-to-compromise\">Read more</a>.</p><h2><strong>Conclusion</strong></h2><p>We\u2019ve shown how to compare different problems on each of our framework factors \u2013 scale, neglectedness, solvability and personal fit.</p><p>While it is hard to measure effectiveness precisely, the differences identified between problems are often very large. This suggests that even inaccurate measurements could be a useful guide, compared to relying on intuition alone.</p><h2><strong>Further reading</strong></h2><ul><li><a href=\"https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom/\">Crucial considerations and wise philanthropy</a></li><li><a href=\"https://80000hours.org/articles/coordination/#3-take-the-portfolio-approach\">Take the portfolio approach to maximizing impact as a community.</a></li></ul><h2><strong>Read next</strong></h2><p>This is a supporting article in our <a href=\"https://80000hours.org/key-ideas\">key ideas series</a>. Read the <a href=\"https://80000hours.org/articles/solutions/\">next article</a> in the series.</p><p>&nbsp;</p><p>&nbsp;</p><p><i>This work is licensed under a&nbsp;</i><a href=\"https://creativecommons.org/licenses/by/4.0/\"><i>Creative Commons Attribution 4.0 International License</i></a><i>.</i></p><p><br>&nbsp;</p>", "user": {"username": "Robert_Wiblin"}}, {"_id": "S9XaptoyfzpjxCeJZ", "title": "Health in poor countries problem profile", "postedAt": "2016-04-01T17:59:00.000Z", "htmlBody": "<h2><strong>Summary</strong></h2><p>Every year around ten million people in poorer countries die of illnesses that can be very cheaply prevented or managed, including malaria, HIV, tuberculosis and diarrhoea.</p><p>Around $100 is spent on the healthcare of the poorest 2 billion people per capita each year (adjusted for purchasing power). As a result there remain many opportunities to scale up treatments that are known to prevent or cure these conditions.</p><p>Options for working on the problem include serving as a donor to effective projects, working as an economist in intergovernmental organizations such as the World Bank or World Health Organization, or starting or working in a nonprofit that scales up proven treatments.</p><h3><strong>Our overall view</strong></h3><p><strong>Sometimes recommended</strong></p><p>This is a pressing problem to work on, but you may be able to have an even bigger impact by working on something else.</p><p><strong>Scale</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr4e6pw9qud9\"><sup><a href=\"#fnr4e6pw9qud9\">[1]</a></sup></span>&nbsp;&nbsp;</p><p>We think work to alleviate global health problems has the potential for a large positive impact. The damage done by easily preventable diseases in the least developed countries plus India amounts to between 200 million DALYs and 500 million DALYs per year.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv82i2dn5ar\"><sup><a href=\"#fnv82i2dn5ar\">[2]</a></sup></span></p><p><strong>Neglectedness</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv2txz3q783m\"><sup><a href=\"#fnv2txz3q783m\">[3]</a></sup></span><strong>&nbsp;&nbsp;</strong></p><p>This issue is much less neglected than most others we prioritise. Current spending in the least developed countries plus India is about $300 billion per year.</p><p><strong>Solvability</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyrxr56czu7\"><sup><a href=\"#fnyrxr56czu7\">[4]</a></sup></span>&nbsp;&nbsp;</p><p>Making progress on alleviating global health problems seems highly tractable. It is mostly a matter of scaling up approaches that are known with near certainty to work if done correctly.</p><p><strong>Profile depth</strong></p><p>Exploratory&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm2gd4m4lke\"><sup><a href=\"#fnm2gd4m4lke\">[5]</a></sup></span></p><p>This is one of many profiles we've written to help people find the most pressing problems they can solve with their careers. <a href=\"https://80000hours.org/career-guide/most-pressing-problems/\">Learn more</a> about how we compare different problems, see how we try to <a href=\"https://80000hours.org/articles/problem-framework/\">score them numerically</a>, and see <a href=\"https://80000hours.org/problem-profiles/\">how this problem compares to the others</a> we've considered so far.</p><h2><strong>What is the problem?</strong></h2><p>Every year around ten million people in poorer countries die of illnesses that can be very cheaply prevented or managed, including malaria, HIV, tuberculosis and diarrhoea. Tens of millions more suffer from persistent undernutrition or parasitic diseases that cause them to be less mentally and physically capable than they otherwise would be.</p><h2><strong>Why is this problem pressing?</strong></h2><h3><strong>What is our recommendation based on?</strong></h3><p>Focussing on basic health treatments in the developing world is supported by <a href=\"http://www.givewell.org/international\">GiveWell</a>, the <a href=\"http://globalprioritiesproject.org/2015/09/flowhart/\">Global Priorities Project</a>, the <a href=\"http://www.copenhagenconsensus.com/copenhagen-consensus-iii/outcome\">Copenhagen Consensus</a>. In addition to that the <a href=\"http://www.gatesfoundation.org/What-We-Do\">Bill and Melinda Gates Foundation</a> with which we share many values, spends most of its money on this cause. Our recommendation is a compilation of their findings, as well as basic data from the <a href=\"http://www.who.int/healthinfo/global_burden_disease/en/\">Global Burden of Disease</a> and <a href=\"http://data.worldbank.org/\">World Bank</a>, among others.</p><h3><strong>Why is it pressing?</strong></h3><p>These diseases cause unnecessary suffering and death both to victims and their families. They also lead to a range of other negative effects:</p><ul><li>Lower educational attainment.</li><li>Lethargy and reduced ability to think and work.</li><li>Worse health later in life.</li><li>Higher birth rates to compensate for infant mortality.</li></ul><p>In many cases these diseases or their impacts can be largely eliminated with cheap technologies that are known to work and have existed for decades. For example:</p><ul><li>Malaria is prevented by insecticide-treated bednets.</li><li>TB is almost always cured by sustained treatment with antibiotics (so called DOTS).</li><li>People with HIV live nearly normal lifespans, and rarely pass on the virus to others, if promptly and consistently treated with anti-retroviral drugs.</li><li>Diarrhoea can be prevented through better sanitation, and death prevented by oral rehydration therapy.</li><li>Parasitic diseases can be cured with a pill that costs under $1 a year.</li><li>A range of other diseases can be prevented through the the basic vaccination program (e.g. diphtheria, whooping cough, etc).</li></ul><p>While the cost-effectiveness of the above approaches ranges quite widely, they can in most cases generate an extra year of healthy life for under $1,000, in a few cases for less than $100.</p><p>Over the last 60 years, death rates from several of these diseases have been more than halved using these techniques, suggesting a very clear way to make progress.</p><h3><strong>What are the major arguments against it being pressing?</strong></h3><ul><li>You might think that it is not a particularly neglected problem, given it is very widely recognised and is funded by organizations, including aid agencies, with billions of dollars to spend each year. Governments in developing countries are also making significant progress in improving health, although some gaps certainly remain in practice. In this view it will be hard to find exceptional opportunities because there are so many other people trying to do so.</li><li>You might worry that reducing poverty and improving health in poor countries will not have major long-run effects on the future, which will instead be determined in other ways, for example through war or the invention of new technologies.</li><li>You might think that other means for reducing poverty will be more effective, such as reforming government and legal institutions in developing countries.</li></ul><h3><strong>Key judgement calls made to prioritise this problem</strong></h3><ul><li>That the lives of people in other countries are not much less important than the lives of people in the country you (probably) live in.</li><li>That improving health will cause developing countries to become sustainably richer and nicer to live in, for example by reducing fertility or improving education and governance.</li></ul><h2><strong>What can you do about this problem?</strong></h2><h3><strong>What\u2019s most needed to contribute to this problem?</strong></h3><p>Deliver basic health services to all people who have or are at risk of contracting easily prevented contagious diseases. For example:</p><ul><li>Get all children to receive the basic schedule of vaccinations (currently around 85% do). <a href=\"https://80000hours.org/problem-profiles/health-in-poor-countries/#fn-2\"><sup>2</sup></a> (Listen to our <a href=\"https://80000hours.org/podcast/episodes/varsha-venugopal-vaccinations-children-india/\">2021 podcast with Varsha Venugopal about recent work in this space</a>.)</li><li>Get everyone exposed to malaria sleeping under bednets. Currently a bit over half of people in the relevant parts of Africa have access to bednets. <a href=\"https://80000hours.org/problem-profiles/health-in-poor-countries/#fn-3\"><sup>3</sup></a></li><li>Get all TB cases treated \u2013 currently at least a third are not diagnosed. <a href=\"https://80000hours.org/problem-profiles/health-in-poor-countries/#fn-4\"><sup>4</sup></a></li><li>Ensure everyone has access to clean drinking water \u2013 currently at least a billion people do not.<a href=\"https://80000hours.org/problem-profiles/health-in-poor-countries/#fn-5\"><sup>5</sup></a></li></ul><p>This is primarily a funding and logistical issue. The treatments are usually simple and do not require advanced medical training to deliver (though treatment of TB and HIV requires medical oversight).</p><h3><strong>What skill sets and resources are most needed?</strong></h3><ul><li>The ability to fundraise large sums, or move money within bureaucracies to better projects.</li><li>People with <a href=\"http://blog.givewell.org/2013/03/21/trying-and-failing-to-find-more-funding-gaps-for-delivering-proven-cost-effective-interventions/\">on-the-ground logistical skills in international development</a> (i.e. the kind of person who could get 100,000 malaria nets distributed in Africa).</li><li>Entrepreneurs (mostly in nonprofits but also sometimes for-profits) who could found one of <a href=\"https://80000hours.org/2015/11/why-and-how-to-found-a-givewell-non-profit/\">these charities</a>.</li><li>Development economists and cost-effectiveness researchers, including economists, statisticians and disease control experts.</li><li>Money to fund GiveWell recommended charities.</li></ul><p>We think that people capable of starting outstanding projects in this area are likely to be able to attract the necessary funding, making the area mostly <a href=\"https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/\">talent constrained</a>.</p><h3><strong>Who is working on this problem?</strong></h3><ul><li>See our <a href=\"http://wiki.80000hours.org/index.php/Places_we_sometimes_recommend_people_apply_to_work#Development_and_global_health\">extended list of places to potentially apply to work at</a> within this cause.</li><li>Domestic health departments in developing countries.</li><li>A range of foundations such as the <a href=\"http://www.gatesfoundation.org/\">Bill and Melinda Gates Foundation</a> and <a href=\"https://ciff.org/\">Children\u2019s Investment Fund Foundation</a>.</li><li>Also intergovernmental organizations such as the <a href=\"http://who.int/\">World Health Organization</a>, <a href=\"http://www.gainhealth.org/\">Global Alliance for Improved Nutrition</a>, <a href=\"http://www.gavi.org/\">GAVI</a>, <a href=\"http://www.worldbank.org/\">World Bank</a>, and the <a href=\"http://www.theglobalfund.org/en/\">Global Fund</a>.</li><li>A range of NGOs such as <a href=\"http://www.doctorswithoutborders.org/our-work\">Doctors Without Borders</a>, <a href=\"http://www.care-international.org/\">CARE</a>, <a href=\"http://www.unicef.org/\">UNICEF</a> and <a href=\"http://www.evidenceaction.org/\">Evidence Action</a>.</li></ul><h3><strong>What can you concretely do to help?</strong></h3><ul><li>Donate to a <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell recommended charity</a> today.</li><li>Take the <a href=\"http://givingwhatwecan.org/\">Giving What We Can pledge</a> to donate at least 10% of your income to people in extreme poverty.</li><li>Plan a career around founding a new global health nonprofit \u2013 <a href=\"https://80000hours.org/2015/11/why-and-how-to-found-a-givewell-non-profit/\">see our profile for next steps</a>.</li><li>Apply to <a href=\"http://www.givewell.org/about/jobs\">work at GiveWell</a> (<a href=\"http://www.givewell.org/about/jobs/senior-research-analyst\">example research role</a>) or at one of these <a href=\"http://wiki.80000hours.org/index.php/Places_we_sometimes_recommend_people_apply_to_work#Development_and_global_health\">dozens of organisations</a> in the field.</li><li>Study an <a href=\"https://80000hours.org/career-reviews/economics-phd/\">economics PhD</a>, then apply your skills to development, for example by working in an intergovernmental organization such as the World Bank or World Health Organization.</li><li>Become a <a href=\"https://80000hours.org/career-reviews/foundation-program-manager/\">grantmaker in a foundation that funds (or could fund) global health projects</a>, for example the Gates Foundation.</li><li>Get a job in any organization scaling up proven health treatments, for example <a href=\"https://www.againstmalaria.com/\">Against Malaria Foundation</a>, <a href=\"http://www3.imperial.ac.uk/schisto\">SCI</a>, <a href=\"http://www.stoptb.org/\">Stop TB</a>, <a href=\"https://evidenceaction.bamboohr.com/jobs/\">Evidence Action</a> and <a href=\"http://projecthealthychildren.org/\">Project Healthy Children</a>.</li><li>Become a <a href=\"https://80000hours.org/career-reviews/biomedical-research/\">biomedical researcher</a> and work on better ways to prevent neglected diseases of the poor.</li></ul><h2><strong>Some especially recommended organisations</strong></h2><ul><li><a href=\"http://www.givewell.org/\"><strong>GiveWell</strong></a> conducts thorough research to find the best charities available to help people in the developing world. <a href=\"http://www.givewell.org/about/jobs\">See current vacancies</a>.</li><li><a href=\"https://www.cgdev.org/\"><strong>The Center for Global Development</strong></a> is a U.S. nonprofit think tank that focuses on international development. <a href=\"https://www.cgdev.org/page/job-opportunities-0\">See current vacancies</a>.</li><li><a href=\"https://www.evidenceaction.org/\"><strong>Evidence Action</strong></a> scales proven interventions to improve life for the global poor. Their <a href=\"https://www.evidenceaction.org/dewormtheworld/\">Deworm the World Initiative</a> is one of GiveWell\u2019s top-rated charities. <a href=\"https://evidenceaction.bamboohr.com/jobs/\">See current vacancies</a>.</li><li><a href=\"http://www.charityentrepreneurship.com/\"><strong>Charity Entrepreneurship</strong></a> helps people start new charities that have the potential to become recommended by GiveWell.</li><li><a href=\"https://www.againstmalaria.com/\"><strong>Against Malaria Foundation</strong></a> is one of charity evaluator GiveWell\u2019s top charities and provides funding for antimalarial bed net distributions.</li><li><a href=\"http://www.imperial.ac.uk/schistosomiasis-control-initiative\"><strong>Schistosomiasis Control Initiative</strong></a> is one of charity evaluator GiveWell\u2019s top charities and works with governments across Sub-Saharan Africa and Yemen to develop national schistosomiasis control programmes.</li><li><a href=\"http://www.poverty-action.org/\"><strong>Innovations for Poverty Action</strong></a> is a nonprofit research and policy organisation which, since its inception in 2002, has conducted over 600 randomised controlled trials and other evaluations. <a href=\"http://www.poverty-action.org/work-with-ipa/current-opportunities/\">See current vacancies</a>.</li><li><a href=\"https://www.givedirectly.org/\"><strong>GiveDirectly</strong></a>, one of GiveWell\u2019s top-rated charities, distributes unconditional cash transfers to people living in East Africa. <a href=\"https://givedirectly.recruiterbox.com/\">See current vacancies</a>.</li></ul><h2><strong>Related issues</strong></h2><p>Our impression is that treating and preventing infectious diseases, especially malaria, is the most cost-effective health intervention right now.</p><p>There are two other sub-issues within global health, however, that seem worth highlighting:</p><ul><li><strong>Smoking in developing countries</strong> \u2014 while smoking rates in the US and UK have been falling, smoking in China and the developing world is on the rise, bringing a large health toll. <a href=\"https://80000hours.org/problem-profiles/tobacco/\">Read more</a>.</li><li><strong>Pain relief</strong> \u2014 most people around the world lack access to adequate pain relief, which leads to widespread suffering due to injuries, chronic health conditions, and disease. One natural approach is increasing access to cheap pain relief medications that are common in developed countries. One group working in this area is the <a href=\"https://www.preventsuffering.org/\">Organisation for the Prevention of Intense Suffering</a>. <a href=\"http://effective-altruism.com/ea/16r/increasing_access_to_pain_relief_in_developing/\">Read more</a>.</li></ul><h2><strong>Further reading</strong></h2><ul><li>Podcast: <a href=\"https://80000hours.org/podcast/episodes/karen-levy-misaligned-incentives-in-global-development/\">Karen Levy on fads and misaligned incentives in global development, and scaling deworming to reach hundreds of millions.</a></li><li>Podcast: <a href=\"https://80000hours.org/podcast/episodes/james-snowden-givewell-research/\">Finding the best charity requires estimating the unknowable. Here\u2019s how GiveWell tries to do that, according to researcher James Snowden.</a></li><li>Podcast: <a href=\"https://80000hours.org/2017/10/claire-walsh-evidence-in-development/\">We can use science to end poverty faster. But how much do governments listen to it anyway?</a></li><li>Podcast: <a href=\"https://80000hours.org/2018/01/ofir-reich-data-science/\">Ofir Reich on using data science to end poverty and the spurious action/inaction distinction</a></li><li>Podcast: <a href=\"https://80000hours.org/2018/03/leah-utyasheva-pesticide-suicide-prevention/\">The nonprofit that figured out how to massively cut suicide rates in Sri Lanka, and their plan to do the same around the world</a></li><li>Podcast: <a href=\"https://80000hours.org/podcast/episodes/eva-vivalt-social-science-generalizability/\">Dr Eva Vivalt\u2019s research suggests social science findings don\u2019t generalize. So evidence-based development \u2013 what is it good for?</a></li><li>Podcast: <a href=\"https://80000hours.org/podcast/episodes/james-tibenderana-malaria-control-and-elimination/\">James Tibenderana on the state of the art in malaria control and elimination</a></li><li><a href=\"https://www.rev.com/transcript-editor/shared/gijVt4fZ1bjUNXkcOewIjwyK67-T5Q8pq8Q2UVBIMhlzbdgpkoAkbuSiwYuQsxjJewuTIU9SE5GtTR76bdo82ixiq0Q?loadFrom=SharedLink\">Career profile interview with Catherine Hollander and Olivia Larsen</a>, research analysts (outreach focus) at GiveWell</li><li>See our related profile on <a href=\"https://80000hours.org/problem-profiles/tobacco/\">smoking in the developing world</a>.</li><li><a href=\"https://80000hours.org/articles/effective-social-program/\">Is it fair to say that most social programmes don\u2019t work?</a></li><li>One of the most famous essays on this topic is <a href=\"http://www.utilitarian.net/singer/by/1972----.htm\">Famine, Affluence and Morality</a> by Peter Singer.</li><li>Toby Ord wrote about why it\u2019s particularly important to make sure health resources are spent in a cost-effective manner: <a href=\"https://www.givingwhatwecan.org/sites/givingwhatwecan.org/files/attachments/moral_imperative.pdf\">The Moral Imperative Towards Cost Effectiveness</a>.</li><li><a href=\"http://www.cgdev.org/initiative/millions-saved\">Millions Saved</a>, a book about global health successes.</li><li><a href=\"http://www.givewell.org/international/technical/programs\">GiveWell reports on different health interventions</a>.</li><li><a href=\"http://ranksmartsolutions.com/\">Copenhagen Consensus Centre research</a>.</li><li><a href=\"https://en.wikipedia.org/wiki/Disease_Control_Priorities_Project\">Disease Control Priorities Project</a>.</li><li><a href=\"https://en.wikipedia.org/wiki/Global_health#Health_conditions\">Wikipedia entry on global health</a>.</li><li><a href=\"https://www.givingwhatwecan.org/charities/\">Giving What We Can\u2019s research</a></li><li>The <a href=\"https://www.healthdata.org/gbd\">Global Burden of Disease</a> study.</li></ul><p>&nbsp;</p><p>&nbsp;</p><p><i>This work is licensed under a&nbsp;</i><a href=\"https://creativecommons.org/licenses/by/4.0/\"><i>Creative Commons Attribution 4.0 International License</i></a><i>.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr4e6pw9qud9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr4e6pw9qud9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If we solved this problem, by how much would the world become a better place? <a href=\"https://80000hours.org/articles/problem-framework/#how-to-assess-scale\">Read more</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv82i2dn5ar\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv82i2dn5ar\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The population of these countries is around 2 billion. To prevent 100 million DALYs each year each person in these countries would have to be given an average of 1/20th of a DALY each year. Given an existing life expectancy of around 65, this would require extending life expectancy by 3.25 years, or the equivalent in improved quality of health. This seems possible and if anything small relative to health gains achieved by other countries that have eliminated easily prevented diseases in the past.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv2txz3q783m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv2txz3q783m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>How many resources are already being dedicated to tackling this problem? <a href=\"https://80000hours.org/articles/problem-framework/#how-to-assess-how-neglected-a-problem-is\">Read more</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyrxr56czu7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyrxr56czu7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If we doubled direct effort on this problem, what fraction of the remaining problem would we expect to solve? <a href=\"https://80000hours.org/articles/problem-framework/#how-to-assess-how-solvable-a-problem-is\">Read more</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm2gd4m4lke\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm2gd4m4lke\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We've made an initial evaluation of this problem by speaking to advisors and looking at preliminary research.</p></div></li></ol>", "user": {"username": "Robert_Wiblin"}}, {"_id": "FYALSpiiJAoLoKaxb", "title": "Doing Good Better - Introduction through to the end of Chapter 3. ", "postedAt": "2015-09-01T23:00:00.000Z", "htmlBody": "", "user": {"username": "Jesse Rothman"}}, {"_id": "KsxJMkNXiE2Ju398C", "title": "Scott Alexander \u2013 Axiology, Morality, Law", "postedAt": "2016-08-28T15:00:02.241Z", "htmlBody": "<h3>I.</h3>\n<p>Philosopher Amanda Askell <a href=\"http://www.rationalreflection.net/can-we-offset-immorality/\">questions the practice of moral offsetting</a>.</p>\n<p>Offsetting is where you compensate for a bad thing by doing a good thing, then consider yourself even. For example, an environmentalist takes a carbon-belching plane flight, then pays to clean up the same amount of carbon she released.</p>\n<p>This can be pretty attractive. If you\u2019re really environmentalist, but also really want to take a vacation to Europe, you could be pretty miserable not knowing whether your vacation is worth the cost to the planet. But if you can calculate that it would take about $70 to clean up more carbon than you release, that\u2019s such a small addition to the overall cost of the trip that you can sigh with relief and take the flight guilt-free.</p>\n<p>Or use offsets <a href=\"https://slatestarcodex.com/2015/09/23/vegetarianism-for-meat-eaters/\">instead of becoming vegetarian</a>. An typical person\u2019s meat consumption averages 0.3 cows and 40 chickens per year. <a href=\"https://animalcharityevaluators.org/charity-review/mercy-for-animals/\">Animal Charity Evaluators</a> believes that donating to a top animal charity this many animals\u2019 lives for less than $5; others note <a href=\"https://medium.com/@harrisonnathan/the-actual-number-is-almost-surely-higher-92c908f36517\">this number is totally wrong and made up</a>. But it\u2019s hard to believe charities could be less cost-effective than just literally buying the animals; this would fix a year\u2019s meat consumption offset price at around $500. Would I pay between $5 and $500 a year not to have to be a vegetarian? You bet.</p>\n<p>Askell is uncomfortable with this concept for the same reasons <a href=\"https://slatestarcodex.com/2015/01/04/ethics-offsets/\">I was when I first heard about it</a>. Can we kill an enemy, then offset it with enough money to save somebody else\u2019s life? Smash other people\u2019s property, then give someone else enough money to buy different property? Can Bill Gates nuke entire cities for fun, then build better cities somewhere else?</p>\n<p>She concludes:</p>\n<blockquote>\n<p>There are a few different things that the harm-based ethicist could say in response to this, however. First, they could point out that as the immorality of the action increases, it becomes far less likely that performing this action and morally offsetting is the best option available, even out of those options that actualists would deem morally relevant. Second, it is very harmful to undermine social norms where people don\u2019t behave immorally and compensate for it (imagine how terrible it would be to live in a world where this was acceptable). Third, it is \u2013 in expectation \u2013 bad to become the kind of person who offsets their moral harms. Such a person will usually have a much worse expected impact on the world than someone who strives to be as moral as they can be.</p>\n<p>I think that these are compelling reasons to think that, in the actual world, we are \u2013 at best \u2013 morally permitted to offset trivial immoral actions, but that more serious immoral actions are almost always not the sorts of things we can morally offset. But I also think that the fact that these arguments all depend on contingent features of the world should be concerning to those who defend harm-based views in ethics.</p>\n</blockquote>\n<p>I think Askell gets the right answer here \u2013 you can offset carbon emissions but not city-nuking. And I think her reasoning sort of touches on some of the important considerations. But I also think there\u2019s a much more elegant theory that gives clear answers to these kinds of questions, and which relieves some of my previous doubts about the offsetting idea.</p>\n<h3>II.</h3>\n<p>Everything below is taken from vague concepts philosophers talk about all the time, but which I can\u2019t find a single good online explanation of. I neither deserve credit for anything good about the ideas, nor can avoid blame for any mistakes or confusions in the phrasing. That having been said: consider the distinction between axiology, morality, and law.</p>\n<p>Axiology is the study of what\u2019s good. If you want to get all reductive, think of it as comparing the values of world-states. A world-state where everybody is happy seems better than a world-state where everybody is sad. A world-state with lots of beautiful art is better than a world-state containing only featureless concrete cubes. Maybe some people think a world-state full of people living in harmony with nature is better than a world-state full of gleaming domed cities, and other people believe the opposite; when they debate the point, they\u2019re debating axiology.</p>\n<p>Morality is the study of what the right thing to do is. If someone says \u201cdon\u2019t murder\u201d, they\u2019re making a moral commandment. If someone says \u201cPirating music is wrong\u201d, they\u2019re making a moral claim. Maybe some people believe you should pull the lever on the trolley problem, and other people believe you shouldn\u2019t; when they debate the point, they\u2019re debating morality.</p>\n<p>(this definition elides a complicated distinction between individual conscience and social pressure; fixing that would be really hard and I\u2019m going to keep eliding it)</p>\n<p>Law is \u2013 oh, come on, you know this one. If someone says \u201cDon\u2019t go above the speed limit, there\u2019s a cop car behind that corner\u201d, that\u2019s law. If someone says \u201cmy state doesn\u2019t allow recreational marijuana, but it will next year\u201d, that\u2019s law too. Maybe some people believe that zoning restrictions should ban skyscrapers in historic areas, and other people believe they shouldn\u2019t; when they debate the point, they\u2019re debating law.</p>\n<p>These three concepts are pretty similar; they\u2019re all about some vague sense of what is or isn\u2019t desirable. But most societies stop short of making them exactly the same. Only the purest act-utilitarianesque consequentialists say that axiology exactly equals morality, and I\u2019m not sure there <em>is</em> anybody quite that pure. And only the harshest of Puritans try to legislate the state law to be exactly identical to the moral one. To bridge the whole distance \u2013 to directly connect axiology to law and make it illegal to do anything other than the most utility-maximizing action at any given time \u2013 is such a mind-bogglingly bad idea that I don\u2019t think anyone\u2019s even considered it in all of human history.</p>\n<p>These concepts stay separate because they each make different compromises between goodness, implementation, and coordination.</p>\n<p>One example: axiology can\u2019t distinguish between murdering your annoying neighbor vs. not donating money to save a child dying of parasitic worms in Uganda. To axiology, they\u2019re both just one life snuffed out of the world before its time. If you forced it to draw some distinction, it would probably decide that saving the child dying of parasitic worms was more important, since they have a longer potential future lifespan.</p>\n<p>But morality absolutely draws this distinction: it says not-murdering is obligatory, but donating money to Uganda is supererogatory. Even utilitarians who deny this distinction in principle will use it in everyday life: if their friend was considering not donating money, they would be a little upset; if their friend was considering murder, they would be horrified. If they themselves forgot to donate money, they\u2019d feel a little bad; if they committed murder in the heat of passion, they\u2019d feel awful.</p>\n<p>Another example: <a href=\"https://slatestarcodex.com/2014/12/19/nobody-is-perfect-everything-is-commensurable/\">Donating 10% of your income to charity</a> is a moral rule. Axiology says \u201cWhy not donate all of it?\u201d, Law says \u201cYou won\u2019t get in trouble even if you don\u2019t donate any of it\u201d, but at the moral level we set a clear and practical rule that meshes with our motivational system and makes the donation happen.</p>\n<p>Another example: \u201cDon\u2019t have sex with someone who isn\u2019t mature enough to consent\u201d is a good moral rule. But it doesn\u2019t make a good legal rule; we don\u2019t trust police officers and judges to fairly determine whether someone\u2019s mature enough in each individual case. A society which enshrined this rule in law would be one where you were afraid to have sex with anyone at all \u2013 because no matter what your partner\u2019s maturity level, some police officer might say your partner seemed immature <em>to them</em> and drag you away. On the other hand, elites could have sex with arbitrarily young people, expecting police and judges to take their side.</p>\n<p>So the state replaces this moral rule with the legal rule \u201cdon\u2019t have sex with anyone below age 18\u201d. Everyone knows this rule doesn\u2019t perfectly capture reality \u2013 there\u2019s no significant difference between 17.99-year-olds and 18.01-year-olds. It\u2019s a useful hack that waters down the moral rule in order to make it more implementable. Realistically it gets things wrong sometimes; sometimes it will incorrectly tell people not to have sex with perfectly mature 17.99-year-olds, and other times it will incorrectly excuse sex with immature 18.01-year-olds. But this beats the alternative, where police have the power to break up any relationship they don\u2019t like, and where everyone has to argue with everybody else about whether their relationships are okay or not.</p>\n<p>A final example: axiology tells us a world without alcohol would be better than our current world: ending alcoholism could avert millions of deaths, illnesses, crimes, and abusive relationships. Morality only tells us that we should be careful drinking and stop if we find ourselves becoming alcoholic or ruining our relationships. And the law protests that it tried banning alcohol once, but it turned out to be unenforceable and gave too many new opportunities to organized crime, so it\u2019s going to stay out of this one except to say you shouldn\u2019t drink and drive.</p>\n<p>So fundamentally, what is the difference between axiology, morality, and law?</p>\n<p>Axiology is just our beliefs about what is good. If you defy axiology, you make the world worse.</p>\n<p>At least from a rule-utilitarianesque perspective, morality is an attempt to triage the infinite demands of axiology, in order to make them implementable by specific people living in specific communities. It makes assumptions like \u201cpeople have limited ability to predict the outcome of their actions\u201d, \u201cpeople are only going to do a certain amount and then get tired\u201d, and \u201cpeople do better with bright-line rules than with vague gradients of goodness\u201d. It also admits that it\u2019s important that everyone living in a community is on at least kind of the same page morally, both in order to create social pressure to follow the rules, and in order to build the social trust that allows the community to keep functioning. If you defy morality, you still make the world worse. And you feel guilty. And you betray the social trust that lets your community function smoothly. And you get ostracized as a bad person.</p>\n<p>Law is an attempt to formalize the complicated demands of morality, in order to make them implementable by a state with police officers and law courts. It makes assumptions like \u201cpeople\u2019s vague intuitive moral judgments can sometimes give different results on the same case\u201d, \u201csometimes police officers and legislators are corrupt or wrong\u201d, and \u201cwe need to balance the benefits of laws against the cost of enforcing them\u201d. It also tries to avert civil disorder or civil war by assuring everybody that it\u2019s in their best interests to appeal to a fair universal law code rather than try to solve their disagreements directly. If you defy law, you still get all the problems with defying axiology and morality. And you make your country less peaceful and stable. And you go to jail.</p>\n<p>In a healthy situation, each of these systems reinforces and promotes the other. Morality helps you implement axiology from your limited human perspective, but also helps prevent you from feeling guilty for not being God and not being able to save everybody. The law helps enforce the most important moral and axiological rules but also leaves people free enough to use their own best judgment on how to pursue the others. And axiology and morality help resolve disputes about what the law should be, and then lend the support of the community, the church, and the individual conscience in keeping people law-abiding.</p>\n<p>In these healthy situations, the universally-agreed priority is that law trumps morality, and morality trumps axiology. First, because you can\u2019t keep your obligations to your community from jail, and you can\u2019t work to make the world a better place when you\u2019re a universally-loathed social outcast. But also, because you can\u2019t work to build strong communities and relationships in the middle of a civil war, and you can\u2019t work to make the world a better place from within a low-trust defect-defect equilibrium. But also, because in a just society, axiology <em>wants</em> you to be moral (because morality is just a more-effective implementation of axiology), and morality wants you to be law-abiding (because law is just a more-effective way of coordinating morality). So first you do your legal duty, then your moral duty, and then if you have energy left over, you try to make the world a better place.</p>\n<p>(Katja Grace has some really good writing on this kind of stuff <a href=\"https://meteuphoric.wordpress.com/2015/12/02/friendship-is-utilitarian/\">here</a>)</p>\n<p>In unhealthy situations, you can get all sorts of weird conflicts. Most \u201cmoral dilemmas\u201d are philosophers trying to create perverse situations where axiology and morality give opposite answers. For example, the <a href=\"https://en.wikipedia.org/wiki/Trolley_problem#The_fat_man\">fat man version of the trolley problem</a> sets axiology (\u201cit\u2019s obviously better to have a world where one person dies than a world where five people die\u201d) against morality (\u201cit\u2019s a useful rule that people generally shouldn\u2019t push other people to their deaths\u201d). And when morality and state law disagree, you get various acts of civil disobedience, from people hiding Jews from the Nazis all the way down to Kentucky clerks refusing to perform gay marriages.</p>\n<p>I don\u2019t have any special insight into these. My intuition (most authoritative source! is never wrong!) says that we should be very careful reversing the usual law-trumps-morality-trumps-axiology order, since the whole point of having more than one system is that we <em>expect</em> the systems to disagree and we want to suppress those disagreements in order to solve important implementation and coordination problems. But I also can\u2019t deny that for enough gain, I\u2019d reverse the order in a heartbeat. If someone told me that by breaking a promise to my friend (morality) I could cure all cancer forever (axiology), then f@$k my friend, and f@$k whatever social trust or community cohesion would be lost by the transaction.</p>\n<h3>III.</h3>\n<p>With this framework, we can propose a clearer answer to the moral offsetting problem: you can offset axiology, but not morality.</p>\n<p>Emitting carbon doesn\u2019t violate any moral law at all (in the stricter sense of morality used above). It does make the world a worse place. But there\u2019s no unspoken social agreement not to do it, it doesn\u2019t violate any codes, nobody\u2019s going to lose trust in you because of it, you\u2019re not making the community any less cohesive. If you make the world a worse place, it\u2019s perfectly fine to compensate by making the world a better place. So pay to clean up some carbon, or donate to help children in Uganda with parasitic worms, or whatever.</p>\n<p>Eating meat doesn\u2019t violate any moral laws either. Again, it makes the world a worse place. But there aren\u2019t any bonds of trust between humans and animals, nobody\u2019s expecting you not to eat meat, there aren\u2019t any written or unwritten codes saying you shouldn\u2019t. So eat the meat and offset it by making the world better in some other way.</p>\n<p>(the strongest counterargument I can think of here is that you\u2019re not betraying animals, but you might be betraying your fellow animals-rights-activists! That is, if they\u2019re working to establish a social norm against meat-eating, the sort of thing where being spotted with a cheeseburger on your plate produces the same level of horror as being spotted holding a bloody knife above a dead body, then your meat-eating is interfering with their ability to establish that norm, and this is a problem that requires more than just offsetting the cost of the meat involved)</p>\n<p>Murdering someone does violate a moral law. The problem with murder isn\u2019t just that it creates a world in which one extra person is dead. If that\u2019s all we cared about, murdering would be no worse than failing to donate money to cure tropical diseases, which also kills people.</p>\n<p>(and the problem isn\u2019t just that it has some knock-on effects in terms of making people afraid of crime, or decreasing the level of social trust by 23.5 social-trustons, or whatever. If that were all, you could do what 90% of you are probably already thinking \u2013 \u201cJust as we\u2019re offsetting the murder by donating enough money to hospitals to save one extra life, couldn\u2019t we offset the social costs by donating enough money to community centers to create 23.5 extra social-trustons?\u201d There\u2019s probably something like that which would work, but along with everything else we\u2019re crossing a <a href=\"http://lesswrong.com/lw/ase/schelling_fences_on_slippery_slopes/\">Schelling fence</a>, <a href=\"http://lesswrong.com/lw/24o/eight_short_studies_on_excuses/\">breaking rules</a>, and weakening the whole moral edifice. The cost isn\u2019t infinite, but it\u2019s pretty hard to calculate. If we\u2019re positing some ridiculous offset that obviously outweighs any possible cost \u2013 maybe go back to the example of curing all cancer forever \u2013 then whatever, go ahead. If it\u2019s anything less than that, be careful. I <em>like</em> the metaphor of these three systems being on three separate tiers \u2013 rather than two Morality Points being worth one Axiology Point, or whatever \u2013 exactly because we don\u2019t really know how to interconvert them)</p>\n<p>This is more precise than Askell\u2019s claim that we can offset \u201ctrivial immoral actions\u201d but not \u201cmore serious\u201d ones. For example, suppose I built an entire power plant that emitted one million tons of carbon per year. Sounds pretty serious! But if I offset that with environmental donations or projects that prevented 1.1 million tons of carbon somewhere else, I can\u2019t imagine anyone having a problem with it.</p>\n<p>On the other hand, consider spitting in a stranger\u2019s face. In the grand scheme of things, this isn\u2019t so serious \u2013 certainly not as serious as emitting a million tons of carbon. But I would feel uncomfortable offsetting this with a donation to my local Prevent Others From Spitting In Strangers\u2019 Face fund, even if the fund worked.</p>\n<p>Askell gave a talk where she used the example of giving your sister a paper cut, and then offsetting that by devoting your entire life to helping the world and working for justice and saving literally thousands of people. Pretty much everyone agrees that\u2019s okay. I guess I agree it\u2019s okay. Heck, I guess I would agree that murdering someone in order to cure cancer forever would be okay. But now we\u2019re just getting into the thing where you bulldoze through moral uncertainty by making the numbers so big that it\u2019s impossible to be uncertain about them. Sure. You can do that. I\u2019d be less happy about giving my sister a paper cut, and then offsetting by preventing one paper cut somewhere else. But that seems to be the best analogy to the \u201cemit one ton of carbon, prevent one ton of carbon\u201d offsetting we\u2019ve been talking about elsewhere.</p>\n<p>I realize all this is sort of hand-wavy \u2013 more of a \u201chere\u2019s one possible way we could look at these things\u201d rather than \u201chere\u2019s something I have a lot of evidence is true\u201d. But everyone \u2013 you, me, Amanda Askell, society \u2013 seems to want a system that tells us to offset carbon but not murder, and when we find such a system I think it\u2019s worth taking it seriously.</p>\n", "user": {"username": "tessa"}}, {"_id": "YmHTpM9FpChtPo6w5", "title": "Minding Our Way \u2013 Conviction without self-deception", "postedAt": "2016-05-08T13:56:16.841Z", "htmlBody": "<p>\"Believe in yourself\" is perhaps the most common trope to be found in self-help books and motivational texts. It appears in fiction (especially children's books), film (especially sports films), and motivational posters. Coaches of sports teams labor to inspire. Low morale is the bane of teams.</p>\n<p>The model, I think, goes something like this. Imagine two people who are about to swim in a 10-person race. They're both standing on the diving platform. The first one knows how their performance compares to the performance of their opponents, and knows that they have (statistically) a 17% chance of winning. The second one ignores all the odds, and has psyched themselves up, and is <em>certain</em> that they're going to win. They can feel the conviction in their body; their mind shuts down any hint of a thought to the contrary; they are pumped, excited, and filled with that sense of certainty.</p>\n<p>Common wisdom says that the second person is going to push themselves harder (and, thus, increase their relative chances of winning). The first person seems less able to tap into their reserves, and more liable to despair when they fall behind. The second person seems more likely to give the race everything they've got.</p>\n<p>What gives? Is epistemic virtue (in the form of acknowledging a mere 17% chance of victory) harming the first swimmer? Is their respect for truth causing them to be worse at swimming?</p>\n<p>The fledgeling rationalist might say \"Yes, and this is why rationality is not for everyone: Rationality is only for people who care more about knowing true things than about succeeding at physical tasks like swim races.\"</p>\n<p>The intermediate rationalist might say \"No, that's wrong; rationality is not about <em>having true beliefs</em>, it is about <a href=\"http://mindingourway.com/desire-is-the-direction-rationality-is-the-magnitude/\">winning</a>. Having true beliefs is regularly useful, but when it stops being useful, stop doing it. If you race better by becoming certain you're going to win, then throw epistemics out the window, and become certain you're going to win the race.\"</p>\n<p>This is a better sentiment, but still, I think, misguided.</p>\n<p>What I say is: <em>stop conflating feelings with beliefs</em>.</p>\n<p>The conviction that the second swimmer feels is not a belief. It's a feeling. It's a mental stance; a way of thinking. Maybe they're feeling excited. Maybe their heart rate is higher. Maybe they have a tingling feeling throughout their body. Maybe their thoughts are more focused and singular. But none of those things are <em>beliefs</em>; they are not <em>statements about the world</em>. I say, learn to detach the conviction from the statistics: You can still enter the <em>mental state</em> we refer to as \"certainty that you're going to win\", without in fact predicting victory with high credence.</p>\n<p>Our language and our culture and our <a href=\"http://mindingourway.com/ephemeral-correspondance/\">poorly designed brains</a> make it very easy to conflate feelings with beliefs. For instance, there's this feeling that <em>correlates</em> with extreme confidence that we <em>call</em> \"certainty,\" and thus, it's easy to imagine that that state is only accessible to people with extremely high confidence in some relevant proposition. But the feeling and the credence don't have to come in lockstep \u2014 the two can be disentangled.</p>\n<p>How? I find that the answer is very different for different feelings (that common wisdom says are linked to strange epistemic states), and also very different for different people. I personally get pretty far by simply \"getting out of my own way\": Where a straw rationalist might feel the beginning glimmers of conviction or excitement or hope, and then squash it with a thought such as \"but statistically this is very likely to fail\", I simply... don't squash the glimmer. Because \"statistically this is likely to fail\" just <em>doesn't bear on the feeling</em>, from my perspective. I'm allowed to feel hopeful about a thing even while being well-calibrated on its chances of success. I'm allowed to feel conviction before a race, even if I'm well-calibrated about my odds. I don't need to lose the useful mental stances, simply because I'm better-calibrated.</p>\n<hr>\n<p>When I was a kid, I got into a number of arguments with my brother. I remember various distinct feelings that I had in different types of arguments. Sometimes, I'd be uncertain, but pretty sure he was reasoning poorly. In those situations I'd feel a sense of caution, an impulse to deflect, and that impulse that has you wanting to raise your hands and say \"look, I'm not sure myself, but...\". Other times, I'd know that I was wrong, but I'd be unwilling to lose face. In those situations, I'd feel defiant or trapped, and I'd have impulses to escape or lash out. Other times, I'd have very high confidence in my own beliefs, and I'd feel a strong sense of certainty and conviction, which gave rise to feelings of frustration, or righteousness, or solidity.</p>\n<p>Nowadays, through various methods, I've done some rewiring on which feelings correspond to which epistemic states. Throughout that rewiring, I've endeavored not to throw the baby out with the bathwater.</p>\n<p>I don't know if you associate \"certainty\" with a feeling, but I still can \u2014 I associate it quite strongly with that feeling that I'd have in particular arguments as a kid, and with parts of what I imagine the second swimmer feeling when they're \"certain they're going to win.\"</p>\n<p>I suspect that many rationalists, upon learning that <a href=\"https://en.wikipedia.org/wiki/Cromwell%27s_rule\">one can never be certain</a>, simultaneously lose access to both the epistemic state of certainty, and to the <em>feeling</em>. They say, \"well, I can never be certain of anything,\" and they start managing their beliefs differently, and they become less prone to overconfidence, and they become more amenable to evidence, which is all great. But simultaneously, I suspect many start finding the feeling that we label \"certainty\" to be repulsive.</p>\n<p>The feeling is not the belief! For me, that feeling was strongly correlated with cognitive flaws in my youth (let's just say that I was not a very well-calibrated 8-year-old), and therefore I definitely treat it with some suspicion in similar contexts. But the feeling can still be useful in other contexts.</p>\n<p>For example, it is useful to the swimmer.</p>\n<p>I think it's important to <em>tease apart feelings from beliefs</em>. If you're standing on that diving platform, I think it's important to simultaneously know you have a 17% chance of victory, <em>and</em> fill yourself with the excitement, focus, and confidence of the second swimmer. Become able to tap into conviction, without any need for the self-deception.</p>\n<p>One of the most common objections to truth-seeking that I have found is \"if I believed the truth, then I wouldn't be able to feel [X] anymore, and my life would get worse,\" for values of [X] including \"hope,\" \"happiness,\" and \"conviction.\" So I say: disentangle the feelings from the beliefs. <a href=\"https://forum.effectivealtruism.org/posts/FHveQK2g3iAuzDFnD/detach-the-grim-o-meter\">Detatch the grim-o-meter</a>. Be a little <a href=\"https://forum.effectivealtruism.org/posts/PfF5FRt38MXpoirfy/recklessness\">reckless</a>. Just because we call the feeling \"certainty\" doesn't mean that you're only allowed to feel it when your confidence is unreasonably high.</p>\n", "user": {"username": "tessa"}}, {"_id": "D7g3ScevNGEnyfqiL", "title": "Minding Our Way \u2013 Dive In", "postedAt": "2016-06-13T14:52:43.075Z", "htmlBody": "<p>I often bump into people who want to do something big, interesting, or important, but who utterly lack the ability to commit themselves to a particular action (often because they lack the ability to convince themselves that something is worth doing).</p>\n<p>My suggested remedy comes in three parts. First, become able to feel <a href=\"https://forum.effectivealtruism.org/posts/YmHTpM9FpChtPo6w5/conviction-without-self-deception\">conviction even in the face of high uncertainty</a>. Second, <a href=\"https://forum.effectivealtruism.org/posts/uGwxsTvyphKGG3qbu/deliberate-once\">learn how to weigh your options so hard that by the time you've picked the best available action, no part of you has an urge to go back into \"deliberation mode\" until you encounter new evidence or ideas that would have changed the result of your deliberation</a>. Third, come to realize that <em>you find good things to do by getting your hands dirty, not by sitting on the sidelines and bemoaning how no task seems worthy of your conviction</em>.</p>\n<p>Imagine two high school students, Alice and Bob, both of whom want to make a big impact on the world. Alice says (in an excited, breathless voice) \"I'm going to change the world by starting to work for company X, which will give me skill Y, which will make me attractive to company Z, and in company Z I'll be able to work my way up the ranks until I have the clout to get onto a committee at the United Nations, and then I'll work my way up to president of the United Nations, and from there I'll be able to make a real difference!\"</p>\n<p>Bob says, \"That plan sounds stupid and will never work. Also, even if it did, the UN has no real power. I also want to change the world, but I'm not going to do it via a long convoluted hopeless plan. I'm just going to generally improve my ability to change the world and wait for a better plan to appear.\"</p>\n<p>Bob is correct, in that Alice's plan is in fact hopeless. But I still have my money on Alice doing more good than Bob, in the long run.</p>\n<p>Yes, her plan is bad. It's complicated, it has too many steps, and it's built on a poor model of how to change the world. But I'd still place my money on Alice.</p>\n<p>Why? Because Alice is going to be out there bumping into the world, and Bob's going to be staying inside wishing he had actions he could commit to. Alice is going to have dozens of opportunities to realize that her plan is bad as she struggles to work her way up the ranks with her eyes set on the presidency of the United Nations. If she's sufficiently good at updating in response to evidence and truly changing her mind, then she'll realize that her original plan was silly, and she'll find better plans. And because she'll have been out there in the wild, bumping into social constraints, running into other enthusiastic people, and stumbling upon new opportunities, she'll have more chances than Bob to put herself on a good course of action.</p>\n<p>Bob won't get that feedback loop when he sits at home waiting for better plans to appear.</p>\n<p>The only way to get a good model of the world inside your head is to <a href=\"http://mindingourway.com/what-sort-of-thing-a-brain-is/\">bump into the world</a>, to let the light and sound impinge upon your eyes and ears, and let the world carve the details into your world-model. Similarly, the only method I know of for finding actual good plans is to take a bad plan and <em>slam it into the world</em>, to let evidence and the feedback impinge upon your strategy, and let the world tell you where the better ideas are.</p>\n<p>As an example, consider Elie Hassenfeld and Holden Karnofsky, co-founders of <a href=\"http://www.givewell.org/\">GiveWell</a> in 2007. GiveWell originally focused on evaluating short-term interventions on global poverty and global health. Nowadays, those two are also heading up the <a href=\"http://www.openphilanthropy.org/\">Open Philanthropy Project</a> looking at <a href=\"https://forum.effectivealtruism.org/posts/badD656AemGYnraMt/hits-based-giving\">riskier</a> and (I think) much higher impact interventions. In 2006, when Elie and Holden were considering staring a charity evaluator that focused specifically on evaluating global poverty/health charities, you could have gone to them and said \"Actually, I'm not sure that's literally the best thing you could be doing. There are better ways to do good than just helping people out of poverty, we have animal suffering and existential risks and other such things to worry about.\" And you may well have been <em>right</em>. But it wouldn't have been <em>helpful</em> (and, knowing those two, you wouldn't have gotten very far). The way that Elie and Holden got to where they are is not by agonizing over whether a global poverty charity evaluation was literally the best possible thing he could be doing. They got to where they are by jumping directly into the fray and doing <em>something</em>. They saw that charity evaluations were crap, and jumped into the space head-first. Because of this, over the years, it was Elie and Holden \u2014 and not the person who said \"hey wait this might not be literally the best available choice\" \u2014 who learned the ins and outs of the charity landscape, made important connections, gained visible credibility, built a team of brilliant people around them, repeatedly encountered new evidence and changed their beliefs, and ended up at the helm of a massive collaborative project with <a href=\"http://www.goodventures.org/\">Good Ventures</a>. By now they've had a huge positive impact on the world, and are poised to continue that streak.</p>\n<p>In my experience, the way you end up doing good in the world has very little to do with how good your initial plan was. Most of your outcome will depend on luck, timing, and your ability to actually get out of your own way and <em>start somewhere</em>. The way to <em>end up</em> with a good plan is not to <em>start</em> with a good plan, it's to start with some plan, and then slam that plan against reality until reality hands you a better plan.</p>\n<p>It's important to possess a minimal level of ability to update in the face of evidence, and to <a href=\"http://lesswrong.com/lw/ik/one_argument_against_an_army/\">actually change your mind</a>. But by far the most important thing is to just <em>dive in</em>.</p>\n<hr>\n<p>How, then, do you dive in? Which fray should you leap into, and how? Unfortunately, I don't have a great answer to this question. I attempted to leap into many different frays many different times, and most of the time, I bounced off. One day I'll figure out how to transmit more of the lessons I learned, but for now, the best I can say is this: It helps to have a concrete plan (even if that plan is crazy).</p>\n<p>Maybe the plan is \"I'm going to befriend my senator, and become an aide, and work my way up the ranks, and eventually become a senator myself, and then I'll have a shot at becoming the president.\" Maybe the plan is \"I'm going to get a biology PhD so that I can start my own CRISPR lab so that I can be on the forefront of human intelligence enhancement.\" Maybe the plan is \"I'm going to become a project manager at DARPA, and put in a lot of effort into figuring out who the real decision-makers are and where the real power comes from, and then I'm going to follow that trail.\" Maybe the plan is \"I'm going to read the AI papers from all the separate subfields, and have a better picture of the field than everybody else, and figure out how to ensure that the first AGI humanity builds is aligned using my <em>own bare hands</em>.\"</p>\n<p>The idea doesn't have to be <em>good</em>, and it doesn't have to be <em>feasible</em>, it just needs to be the best <em>incredibly concrete</em> plan that you can come up with at the moment. Don't worry, it will change rapidly when you start slamming it into reality. The important thing is to come up with a concrete plan, and then start executing it as hard as you can \u2014 while retaining a reflective state of mind updating in the face of evidence.</p>\n<p>If it becomes clear that power within DARPA is nepotistic or otherwise well-defended, maybe you'll switch tactics, and maybe you'll have a much better idea of where you could actually make a difference, now that you have more connections inside DARPA and a better understanding of the landscape. If you realize that you won't be able to understand what all the disparate AI subfields are doing just by reading their papers, maybe you'll decide to shift tactics and apprentice under as many people as possible. Your second plan doesn't need to be good or feasible either, of course \u2014 the important thing is that you (1) start with a plan; (2) get out there and start operating; and (3) get better plans as you get more information.</p>\n<p>You're still going to need a lot of luck, and you need to be prepared for many of your plans not working. Also, don't get me wrong, it <em>helps</em> to start with a good/feasible plan. But \"quality of the initial plan\" is much less important than many people expect.</p>\n<p>(If you want help putting your initial plan together (or building your initial network), I suggest applying to a <a href=\"http://rationality.org/\">Center for Applied Rationality</a> workshop, they're good at that sort of thing.)</p>\n<p>The important thing is to stop waiting on the sidelines for better options to appear, and to start leaping in there. Make a crazy detailed plan, and dive into the fray.</p>\n", "user": {"username": "tessa"}}, {"_id": "uGwxsTvyphKGG3qbu", "title": "Minding Our Way \u2013 Deliberate Once", "postedAt": "2016-05-23T13:47:58.760Z", "htmlBody": "<p>Here's a question I get asked pretty regularly:</p>\n<blockquote>\n<p>OK, I'm sold on this whole <a href=\"http://mindingourway.com/best-you-can/\">\"do the best you can\"</a> thing, but how do you actually commit? When I look at my available options, none of them look great. I can take the one that seems best (despite its flaws), but then I keep doubting myself the whole time, and wondering whether there isn't something better I could be doing. For example, I'm currently [doing a PhD|running a startup|earning to give|working at an EA org], and I keep wondering whether I should instead be [switching majors|running a startup|earning to give|working at a different org], and I can't access conviction or resolve. How is it that you actually commit to what you're doing?</p>\n</blockquote>\n<p>My solution is fairly simple: Deliberate once, and then don't deliberate again until you get new information that would have changed the result of your deliberation.</p>\n<p>If you have a big decision to make, set aside some time to do some serious thinking. Ask people you trust for the <a href=\"https://forum.effectivealtruism.org/posts/4r75LjNbsbBYDnRdN/obvious-advice\">obvious advice</a>, and then actually do it. Set some five-minute timers, spend time describing the problem before you spend time describing possible solutions, brainstorm a wide variety of solutions. Figure out where you're highly uncertain, find the places where your decision turns on a critical piece of information you're missing, and add actions like \"run thus-and-such an experiment\" to your set of available actions. Build models. Make your best guesses about the probabilities of various outcomes in response to your actions, make your best guesses about how good those outcomes are, multiply out an expected value calculation, throw the numbers out, and consult your updated intuitions. And so on.</p>\n<p>Whatever your \"think seriously\" process is, run it, and then pick the best action available to you (given the information you currently have, having taken into account the incompleteness of your situation). Then do that. Then <em>don't go back into deliberation mode until you encounter information that would have actually changed your answer</em>.</p>\n<p>If you do it right, there's no need to go back to worrying about which thing you should be doing unless you encounter new evidence or information. When you do, deliberate again \u2014 or, better, plug the new evidence into the model you built when you deliberated the first time, instead of re-doing all that work. Half the reason for really deliberating hard the first time is to build a versatile model, anyway. (Though of course, evidence and experience will also cause you to update that model as you go.)</p>\n<hr>\n<p>Many people seem plagued by \"am I doing the right thing?\" thoughts which get in the way of them committing themselves to <em>any</em> action. I think there are at least three different types of \"am I doing the right thing?\" thoughts, each worth treating differently.</p>\n<p>First are the \"but I am uncertain!\" thoughts, which I refer to as \"commitment-aversion thoughts\". They appear when someone isn't confident in what they're doing in an absolute sense, regardless of whether they see any good alternatives, and the above advice is more or less advice geared towards shifting you into a mode where you aren't getting pinged with \"but I am still uncertain!\" thoughts all the time. However, it's important to distinguish the commitment-aversion thoughts (which I think are often in error) from two other similar-sounding thoughts which are quite important.</p>\n<p>Next are the \"I just got new evidence that we're doing the wrong thing!\" thoughts, which I refer to as \"abort notices.\" Perhaps you decided to complete your econ PhD, and, three months in, you observe that you've been demotivated the whole time and that you've made almost no progress (which comes as a surprise to you). Or perhaps your friend from silicon valley tells you how high programmer salaries are these days, and that information would have changed your decision if you had know it six months ago, and you think \"maybe it should also change my decision now.\" Or perhaps you read a blog post on the internet about a cognitive bias that you notice was distorting your decisions, and you are pretty sure that, without the bias, you'd be doing something else instead. Dealing with these sorts of abort notices is easy, so long as you don't quash them: Simply change your action, or (if you need to) go back into \"heavy deliberation\" mode. In these cases, be especially wary of the <a href=\"https://en.wikipedia.org/wiki/Sunk_costs\">sunk cost fallacy</a> and consider taking <a href=\"https://forum.effectivealtruism.org/posts/r4ewWDs4pqBSzEGBQ/be-a-new-homunculus\">measures to avoid it</a>.</p>\n<p>Third are the \"I have a vague sense that I deliberated incorrectly but I can't articulate it yet\" thoughts, which I refer to as \"confusion pings.\" These thoughts are very important, and they can be pretty hard to distinguish from commitment-aversion thoughts. <em>Make sure that you don't steamroll your confusions in an attempt to squash commitment-aversion thoughts</em>. Noticing and respecting confusion pings is a <a href=\"http://lesswrong.com/lw/if/your_strength_as_a_rationalist/\">core rationalist skill</a>, and if some part of you feels an ineffable hard-to-articulate concern with what you're doing, then the solution is not to shove it under the rug, the solution is to pay very close attention to it. <a href=\"http://lesswrong.com/lw/o4/leave_a_line_of_retreat/\">Leave yourself a line of retreat</a>, ask yourself what thoughts you're not allowing yourself to think and what actions you're not allowing yourself to consider, and ask yourself questions like \"If I changed the course of my life and looked back on this moment in two years, what would I think I was missing?\" Human deliberation is a flawed and fallible process, and confusion pings are sometimes the only hint you ever get of a giant gaping blind spot in your deliberation process. In my experience, handling confusion pings (and learning to articulate your inarticulable concerns) is a skill that requires practice, and it's a skill that I have found very valuable.</p>\n<p>On my current best model of where the \"am I doing the right thing?\" thoughts come from, they tend to come from one of two places. Either (a) some part of you is concerned that your deliberation was dangerously biased, flawed, or otherwise invalid; or (b) some part of you is not yet comfortable in the face of uncertainty. On this model, I suggest a two-pronged method for dealing with commitment-aversion thoughts. First, treat them as if they might be confusion pings: Learn to inspect them and extract content from them, especially content of the form \"I think I have been ignoring factor X\" or \"I think I have been under-weighing concern Y.\" Second, learn to be comfortable in the face of high uncertainty, and develop a deliberation procedure of your own that deserves <a href=\"https://forum.effectivealtruism.org/posts/LNHevqSgGbMvn58WK/confidence-all-the-way-up\">appropriate meta-confidence</a>.</p>\n<p>Operating under uncertainty is the norm, not the exception. No matter how hard you deliberate, your deliberation procedure is going to be flawed and biased, and there are going to be considerations you're missing and evidence you failed to take into account. It is very hard to predict the consequences of your actions, and you aren't going to get a perfect answer. However, some answers are still better than others, and you can construct a process for choosing between actions that leaves you comfortable that you're doing the best you can do with the information and time available.</p>\n<hr>\n<p>When you've committed yourself to a new action, and you start wondering whether you're really doing the right thing, the relevant question is not \"were my thoughts when I chose this action perfectly unbiased?\" Nor is it \"did I find literally the best available action?\" No, the relevant question is, \"am I in a better position <em>now</em> to pick a good action than I was <em>then</em>?\", or alternatively, \"could I do a significantly better job picking an action this time around than I did last time around, enough so to make up for the opportunity cost of deliberating again?\"</p>\n<p>The only information that affects your actions is information that changes which action you would think is better after deliberating. This is true regardless of how much uncertainty you have. Think hard, pick the best action, and then don't worry about what action you're doing until you see something that would have changed the result of the hard thinking. If you understand this on a gut level, and if you're one of the people who has found my advice helpful in the past, then I predict that this mindset is one where you won't experience constant doubt about whether you're doing the right thing. Deliberate well once, and then don't deliberate again until you come across new information that would have changed your answer.</p>\n", "user": {"username": "tessa"}}, {"_id": "yGEAiRYZnrQ8kejQ9", "title": "Worldview Diversification", "postedAt": "2016-12-13T14:15:41.581Z", "htmlBody": "<p>In principle, we try to find the best giving opportunities by comparing many possibilities. However, many of the comparisons we\u2019d like to make hinge on very debatable, uncertain questions.</p>\n<p>For example:</p>\n<ul>\n<li>Some people think that animals such as chickens have essentially no moral significance compared to that of humans; others think that they should be considered comparably important, or at least 1-10% as important. If you accept the latter view, <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a> looks like an extraordinarily outstanding cause, potentially to the point of dominating other options: billions of chickens are treated incredibly cruelly each year on factory farms, and we estimate that <a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms\">corporate campaigns</a> can spare over 200 hens from cage confinement for each dollar spent. But if you accept the former view, this work is arguably a poor use of money.</li>\n<li><a href=\"http://www.openphilanthropy.org/blog/moral-value-far-future\">Some have argued</a> that the majority of our impact will come via its effect on the long-term future. If true, this could be an argument that reducing <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risks</a> has overwhelming importance, or that <a href=\"https://www.openphilanthropy.org/focus/scientific-research\">accelerating scientific research</a> does, or that improving the overall functioning of society via <a href=\"https://www.openphilanthropy.org/focus/us-policy\">policy</a> does. Given how difficult it is to make predictions about the long-term future, it\u2019s very hard to compare work in any of these categories to <a href=\"http://www.givewell.org/charities/top-charities\">evidence-backed interventions serving the global poor</a>.</li>\n<li>We have additional uncertainty over how we should resolve these sorts of uncertainty. We could try to quantify our uncertainties using probabilities (e.g. \u201cThere\u2019s a 10% chance that I should value chickens 10% as much as humans\u201d), and arrive at a kind of <a href=\"https://en.wikipedia.org/wiki/Expected_value\">expected value</a> calculation for each of many broad approaches to giving. But most of the parameters in such a calculation would be very poorly grounded and non-<a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">robust</a>, and it\u2019s unclear how to weigh calculations with that property. In addition, such a calculation would run into challenges around <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf\">normative uncertainty</a> (uncertainty about morality), and it\u2019s quite unclear how to handle such challenges.</li>\n</ul>\n<p>In this post, I\u2019ll use \u201cworldview\u201d to refer to a set of highly debatable (and perhaps impossible to evaluate) beliefs that favor a certain kind of giving. One worldview might imply that <a href=\"http://www.givewell.org/charities/top-charities\">evidence-backed charities serving the global poor</a> are far more worthwhile than either of the types of giving discussed above; another might imply that <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a> is; another might imply that <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risk reduction</a> is. A given worldview represents a combination of views, sometimes very difficult to disentangle, such that uncertainty between worldviews is constituted by a mix of empirical uncertainty (uncertainty about facts), <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf\">normative uncertainty</a> (uncertainty about morality), and methodological uncertainty (e.g. uncertainty about how to handle uncertainty, as laid out in the third bullet point above). Some slightly more detailed descriptions of example worldviews are in a footnote.<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-1\" id=\"fnref-J89KkqwmPsBwpagTk-1\">[1]</a></sup></p>\n<p>A challenge we face is that we consider multiple different worldviews plausible. We\u2019re drawn to multiple giving opportunities that some would consider outstanding and others would consider relatively low-value. We have to decide how to weigh different worldviews, as we try to do as much good as possible with limited resources.</p>\n<p>When deciding between worldviews, there is a case to be made for simply taking our best guess<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-2\" id=\"fnref-J89KkqwmPsBwpagTk-2\">[2]</a></sup> and sticking with it. If we did this, we would focus exclusively on animal welfare, <em>or</em> on global catastrophic risks, <em>or</em> global health and development, <em>or</em> on another category of giving, with no attention to the others. However, that\u2019s not the approach we\u2019re currently taking.</p>\n<p>Instead, we\u2019re practicing <strong>worldview diversification: putting significant resources behind <em>each</em> worldview that we find highly plausible.</strong> We think it\u2019s possible for us to be a transformative funder in each of a number of different causes, and we don\u2019t - as of today - want to pass up that opportunity to focus exclusively on one and get rapidly diminishing returns.</p>\n<p>This post outlines the reasons we practice worldview diversification. In a nutshell:</p>\n<ul>\n<li>I will first discuss the case <em>against</em> worldview diversification. When seeking to maximize <a href=\"https://en.wikipedia.org/wiki/Expected_value\">expected</a> positive impact, without being worried about the \u201crisk\u201d of doing no good, there is a case that we should simply put all available resources behind the worldview that our best-guess thinking favors.</li>\n<li>I will then list several reasons for practicing worldview diversification, in situations where (a) we have high uncertainty and find multiple worldviews highly plausible; (b) there would be strongly diminishing returns if we put all our resources behind any one worldview.\n<ul>\n<li>First, under a set of basic assumptions including (a) and (b) above, worldview diversification can maximize expected value.</li>\n<li>Second, if we imagined that different worldviews represented different fundamental values (not just different opinions, such that one would ultimately be \u201cthe right one\u201d if we had perfect information), and that the people holding different values were trying to reach agreement on common principles behind a <a href=\"https://en.wikipedia.org/wiki/Veil_of_ignorance\">veil of ignorance</a> (explained more below), it seems likely that they would agree to some form of worldview diversification as a desirable practice for anyone who ends up with outsized resources.</li>\n<li>Practicing worldview diversification means developing staff capacity to work in many causes. This provides option value (the ability to adjust if our best-guess worldview changes over time). It also increases our long-run odds of having large effects on the general dialogue around philanthropy, since we can provide tangibly useful information to a larger set of donors.</li>\n<li>There are a number of other practical benefits to working in a broad variety of causes, including the opportunity to use lessons learned in one area to improve our work in another; presenting an accurate public-facing picture of our values; and increasing the degree to which, over the long run, our expected impact matches our actual impact. (The latter could be beneficial for our own, and others\u2019, ability to evaluate how we\u2019re doing.)</li>\n</ul>\n</li>\n<li>Finally, I\u2019ll briefly discuss the key conditions under which worldview diversification seems like a good idea, and give some rough notes on how we currently implement it in practice.</li>\n</ul>\n<p>Note that worldview diversification is simply a broad term for putting significant resources behind multiple worldviews - it does not mean anything as specific as \u201cdivide resources evenly between worldviews.\u201d This post discusses benefits of worldview diversification, without saying exactly how (or to what degree) one should allocate resources between worldviews. In the future, we hope to put more effort into reflecting on - and discussing - which specific worldviews we find most compelling and how we weigh them against each other.</p>\n<p>Also note that this post focuses on deciding how to allocate resources between different plausible already-identified causes, not on the <a href=\"https://www.openphilanthropy.org/research/our-process#Exploring_potential_focus_areas\">process</a> for identifying promising causes.</p>\n<h2>The case against worldview diversification</h2>\n<p>It seems likely that if we had perfect information and perfect insight into our own values, we\u2019d see that some worldviews are <em>much</em> better guides to giving than others. For a relatively clear example, consider <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell\u2019s top charities</a> vs. <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">our work so far on farm animal welfare</a>:</p>\n<ul>\n<li>\n<p>GiveWell <a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets#HowcosteffectiveisLLINdistribution\">estimates</a> that its top charity (Against Malaria Foundation) can prevent the loss of one year of life for every $100 or so.</p>\n</li>\n<li>\n<p>We\u2019ve <a href=\"https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms\">estimated</a> that corporate campaigns can spare over 200 hens from cage confinement for each dollar spent. If we roughly imagine that each hen gains two years of 25%-improved life, this is equivalent to one hen-life-year for every $0.01 spent.</p>\n</li>\n<li>\n<p>If you value chicken life-years equally to human life-years, this implies that corporate campaigns do about 10,000x as much good per dollar as top charities. If you believe that chickens do not suffer in a morally relevant way, this implies that corporate campaigns do no good.<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-3\" id=\"fnref-J89KkqwmPsBwpagTk-3\">[3]</a></sup></p>\n</li>\n<li>\n<p>One could, of course, value chickens while valuing humans <em>more</em>. If one values humans 10-100x as much, this still implies that corporate campaigns are a far better use of funds (100-1,000x). If one values humans astronomically more, this still implies that top charities are a far better use of funds. It seems unlikely that the ratio would be in the precise, narrow range needed for these two uses of funds to have similar cost-effectiveness.</p>\n</li>\n</ul>\n<p>I think similar considerations broadly apply to other comparisons, such as <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">reducing global catastrophic risks</a> vs. <a href=\"https://www.openphilanthropy.org/focus/us-policy\">improving policy</a>, though quantifying such causes is much more fraught.</p>\n<p>One might therefore imagine that there is some \u201cbest worldview\u201d (if we had perfect information) that can guide us to do far more good than any of the others. And if that\u2019s right, one might argue that we should focus exclusively on a \u201cbest guess worldview\u201d<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-4\" id=\"fnref-J89KkqwmPsBwpagTk-4\">[4]</a></sup> in order to maximize how much good we do in <a href=\"https://en.wikipedia.org/wiki/Expected_value\">expected value</a> terms. For example, if we think that one worldview seems 10,000x better than the others, even a 1-10% chance of being right would still imply that we can do much more good by focusing on that worldview.</p>\n<p>This argument presumes that we are \u201crisk neutral\u201d: that our goal is only to maximize the expected value of how much good we do. That is, it assumes we are comfortable with the \u201crisk\u201d that we make the wrong call, put all of our resources into a misguided worldview, and ultimately accomplish very little. Being risk neutral to such a degree often seems strange to people who are used to investing metaphors: investors rarely feel that the possibility of doubling one\u2019s money fully compensates for the possibility of losing it all, and they generally use diversification to reduce the <em>variance</em> of their returns (they aren\u2019t just focused on expected returns). However, we don\u2019t have the same reasons to fear failure that for-profit investors do. There are no special outsized consequences for \u201cfailing to do any good,\u201d as there are for going bankrupt, so it\u2019s a risk we\u2019re happy to take as long as it\u2019s balanced by the possibility of doing a great deal of good. The Open Philanthropy Project aims to be risk neutral in the way laid out here, though there are some other reasons (discussed below) that putting all our eggs in one basket could be problematic.</p>\n<h2>The case for worldview diversification</h2>\n<p>I think the case for worldview diversification largely hinges on a couple of key factors:</p>\n<p><strong>Strong uncertainty</strong> about which worldviews are most reasonable. We recognize that any given worldview might turn out to look misguided if we had perfect information - but even beyond that, we believe that any given worldview might turn out to look misguided <em>if we reflected more rationally on the information that is available</em>. In other words, we feel there are multiple worldviews that each might qualify for \u201cwhat we should consider the best worldview to be basing our giving on, and the worldview that conceptually maximizes our expected value, if we thought more intelligently about the matter.\u201d We could imagine someday finding any of these worldviews to be the best-seeming one. We feel this way partly because we see intelligent, reasonable people who are aware of the arguments for each worldview and still reject it.</p>\n<p>Some people recognize that their best-guess worldview might be wrong, but still think that it is clearly the best to bet on in expected-value terms. For example, some argue that focusing on the far future is best even if there is a &gt;99% chance that the arguments in favor of doing so are misguided, because the value of focusing on the far future is so great if the arguments turn out to be valid. In effect, these people seem to be leaving open no realistic possibility of changing their minds on this front. We have a different kind of uncertainty, that I find difficult to model formally, but that is probably something along the lines of <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">cluster thinking</a>. All things considered - including things like our uncertainty about our fundamental way of modeling expected value - I tend to think of the different plausible worldviews as being in the same ballpark of expected value.</p>\n<p><strong>Diminishing returns</strong> to putting resources behind any given worldview. When looking at a focus area such as <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a> or <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from advanced AI</a>, it seems to me that giving in the range of tens of millions of dollars per year (over the next decade or so) can likely fund the best opportunities, help relevant fields and disciplines grow, and greatly improve the chances that the cause pulls in other sources of funding (both private donors and governments). Giving much <em>more</em> than this would hit strongly diminishing returns. For causes like these, I might roughly quantify my intuition by saying that (at the relevant margin) giving 10x as much would only accomplish about 2x as much. (There are other causes where this dynamic does not apply nearly as much; for example, we don\u2019t see much in the way of diminishing returns when it comes to supporting <a href=\"http://www.givewell.org/charities/give-directly\">cash transfers to the global poor</a>.)</p>\n<p>With these two factors in mind, there are a number of arguments for worldview diversification.</p>\n<h3>Expected value</h3>\n<p>When accounting for strong uncertainty and diminishing returns, worldview diversification can maximize expected value even when one worldview looks \u201cbetter\u201d than the others in expectation. One way of putting this is that if we were choosing between 10 worldviews, and one were 5x as good as the other nine, investing all our resources in that one would - at the relevant margin, due to the \u201cdiminishing returns\u201d point - be worse than spreading across the ten.<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-5\" id=\"fnref-J89KkqwmPsBwpagTk-5\">[5]</a></sup></p>\n<p>I think this dynamic is enhanced by the fact that there is so much we don\u2019t know, and any given worldview could turn out to be much better or much worse than it appears for subtle and unanticipated reasons, including those related to <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>.<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-6\" id=\"fnref-J89KkqwmPsBwpagTk-6\">[6]</a></sup></p>\n<p>It isn\u2019t clear to me how much sense it makes to think in these terms. Part of our uncertainty about worldviews is our uncertainty about moral values: to a significant degree, different worldviews might be incommensurate, in that there is no meaningful way to compare \u201cgood accomplished\u201d between them. Some explicit frameworks have been proposed for dealing with uncertainty between incommensurate moral systems,<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-7\" id=\"fnref-J89KkqwmPsBwpagTk-7\">[7]</a></sup> but we have significant uncertainty about how useful these frameworks these are and how to use them.</p>\n<p>Note that the argument in this section only holds for worldviews with reasonably similar overall expected value. If one believes that a particular worldview points to giving opportunities that are orders of magnitude better than others\u2019, this likely outweighs the issue of diminishing returns.</p>\n<h3>The ethics of the \u201cveil of ignorance\u201d</h3>\n<p>Another case for worldview diversification derives from, in some sense, the opposite approach. Rather than thinking of different worldviews as different \u201cguesses\u201d at how to do the most good, such that each has an expected value and they are ultimately compared in the same terms, presume that different worldviews represent the perspectives of different people<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-8\" id=\"fnref-J89KkqwmPsBwpagTk-8\">[8]</a></sup> with different, incommensurable values and frameworks. For example, it may be the case that some people care as deeply about animals as they do about people, while others don\u2019t value animal welfare at all, and that no amount of learning or reflection would change any of this. When choosing between worldviews, we\u2019re choosing which sorts of people we most identify and sympathize with, and we have strong uncertainty on the matter.</p>\n<p>One way of thinking about the ethics of how people with different values should interact with each other is to consider a kind of <a href=\"https://en.wikipedia.org/wiki/Veil_of_ignorance\">veil of ignorance</a>: imagine the agreements such people would come to about how they should use resources, if they were negotiating before knowing how much resources each of them would individually have available.<sup class=\"footnote-ref\"><a href=\"#fn-J89KkqwmPsBwpagTk-9\" id=\"fnref-J89KkqwmPsBwpagTk-9\">[9]</a></sup> One such agreement might be: \u201cIf one of us ends up with access to vastly more resources than the others, that person should put some resources into the causes most important to each of us - up to some point of diminishing returns - rather than putting all the resources into that person\u2019s own favorite cause.\u201d Each person might accept (based on the diminishing returns model <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification#The_case_for_worldview_diversification\">above</a>) that if they end up with vastly more resources than the others, this agreement will end up making them worse off, but only by 50%; whereas if someone else ends up with vastly more resources, this agreement will end up making them far better off.</p>\n<p>This is only a rough outline of what an appealing principle might look like. Additional details might be added, such as \u201cThe person with outsized resources should invest more in areas where they can be more transformative, e.g. in more neglected areas.\u201d</p>\n<p>We see multiple appealing worldviews that seem to have relatively few resources behind them, and we have the opportunity to have a transformative impact according to multiple such worldviews. Taking this opportunity is the ethical thing to do in the sense that it reflects an agreement we would have made under a \u201cveil of ignorance,\u201d and it means that we can improve the world greatly according to multiple different value sets that we feel uncertain between. I think that considering and putting weight on \u201cveil of ignorance\u201d based ethical concerns such as this one is a generally good heuristic for <a href=\"http://blog.givewell.org/2013/04/04/deep-value-judgments-and-worldview-characteristics/\">consequentialists</a> and non-consequentialists alike, especially when one does not have a solid framework for comparing \u201cexpected good accomplished\u201d across different options.</p>\n<h3>Capacity building and option value</h3>\n<p>Last year, we <a href=\"http://www.openphilanthropy.org/blog/should-open-philanthropy-project-be-recommending-morelarger-grants#Sec2\">described our process of capacity building</a>:</p>\n<p><em>\u201cOur goals, and our efforts, have revolved around (a) selecting focus areas; (b) hiring people to lead our work in these areas (see our most recent update); (c) most recently, working intensively with new hires and trial hires on their early proposed grant recommendations.</em></p>\n<p><em>Collectively, we think of these activities as capacity building. If we succeed, the end result will be an expanded team of people who are (a) working on well-chosen focus areas; (b) invested (justifiably) with a great deal of trust and autonomy; (c) capable of finding many great giving opportunities in the areas they\u2019re working on.\u201d</em></p>\n<p>In addition to building internal capacity (staff), we are hoping to support the growth of the fields we work in, and to gain knowledge over time that makes us more effective at working in each cause. Collectively, all of this is \u201ccapacity building\u201d in the sense that it will, in the long run, improve our ability to give effectively at scale. There are a number of benefits to building capacity in a variety of causes that are appealing according to different worldviews (i.e., to building capacity in <a href=\"https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform\">criminal justice reform</a>, <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a>, <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">biosecurity and pandemic preparedness</a> and more).</p>\n<p>One benefit is <strong>option value</strong>. Over time, we expect that our thinking on which worldviews are most appealing will evolve. For example, I recently discussed <a href=\"http://www.openphilanthropy.org/blog/three-key-issues-ive-changed-my-mind-about\">three key issues I\u2019ve changed my mind about</a> over the last several years, with major implications for how promising I find different causes. It\u2019s very possible that ten years from now, some particular worldview (and its associated causes) will look much stronger to us than the others - and that it won\u2019t match our current best guess. If this happens, we\u2019ll be glad to have invested in years of capacity building so we can quickly and significantly ramp up our support.</p>\n<p>Another long-term benefit is that we can <strong>be useful to donors with diverse worldviews.</strong> If we worked exclusively in causes matching our \u201cbest guess\u201d worldview, we\u2019d primarily be useful to donors with the same best guess; if we do work corresponding to all of the worldviews we find highly compelling, we\u2019ll be useful to any donor whose values and approach are broadly similar to ours. That\u2019s a big difference: I believe there are many people with fundamentally similar values to ours, but different best guesses on some highly uncertain but fundamental questions - for example, how to value reducing <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/\">global catastrophic risks</a> vs. <a href=\"https://www.openphilanthropy.org/focus/scientific-research\">accelerating scientific research</a> vs. <a href=\"https://www.openphilanthropy.org/focus/us-policy\">improving policy</a>.</p>\n<p>With worldview diversification, we can hope to appeal to - and be referred to - any donor looking to maximize the positive impact of their giving. Over the long run, I think this means we have good prospects for making many connections via word-of-mouth, helping many donors give more effectively, and affecting the general dialogue around philanthropy.</p>\n<h3>Other benefits to worldview diversification</h3>\n<p>Worldview diversification means working on a variety of causes that differ noticeably from each other. There are a number of practical benefits to this.</p>\n<p><strong>We can use lessons learned in one area to improve our work in another.</strong> For example:</p>\n<ul>\n<li>Some of the causes we work in are very neglected and \u201cthin,\u201d in the sense that there are few organizations working on them. Others were chosen for reasons other than neglectedness, and have many organizations working on them. Understanding the latter can give us a sense for what kinds of activities we might hope to eventually support in the former.</li>\n<li>Some of the causes we work on involve very long-term goals with little in the way of intermediate feedback (this tends to be true of efforts to <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">reduce global catastrophic risks</a>). In other causes, we can more expect to see progress and learn from our results (for example, <a href=\"https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform\">criminal justice reform</a>, which we selected largely for its <a href=\"https://www.openphilanthropy.org/focus\">tractability</a>).</li>\n<li>Different causes have different cultures, and by working in a number of disparate ones, we work with a number of <a href=\"https://www.openphilanthropy.org/about/team\">Program Officers</a> whose different styles and approaches can inform each other.</li>\n</ul>\n<p><strong>It is easier for casual observers (such as the press) to understand our values and motivations.</strong> Some of the areas we work in are quite unconventional for philanthropy, and we\u2019ve sometimes come across people who question our motivations. By working in a broad variety of causes, some of which are easier to see the case for than others, we make it easier for casual observers to discern the pattern behind our choices and get an accurate read on our core values. Since media coverage affects many people\u2019s preconceptions, this benefit could make a substantial long-term difference to our brand and credibility.</p>\n<p><strong>Over the long run, our actual impact will better approximate our expected impact.</strong> Our <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based giving</a> approach means that in many cases, we\u2019ll put substantial resources into a cause <em>even though we think it\u2019s more likely than not that we\u2019ll fail to have any impact.</em> (<a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">Potential risks from artificial intelligence</a> is one such cause.) If we put all our resources behind our best-guess worldview, we might never have any successful grants <em>even if</em> we make intelligent, high-expected-value grants. Conversely, we might \u201cget lucky\u201d and appear far more reliably correct and successful than we actually are. In either case, our ability to realistically assess our own track record, and learn from it, is severely limited. Others\u2019 ability to assess our work, in order to decide how much weight they should put on our views, is as well.</p>\n<p>Worldview diversification lessens this problem, to a degree. If we eventually put substantial resources into ten very different causes, then we can reasonably hope to get one or more <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">\u201chits\u201d</a> even if each cause is a long shot. If we get no \u201chits,\u201d we have some evidence that we\u2019re doing something wrong, and if we get one or more, this is likely to help our credibility.</p>\n<p>We\u2019re still ultimately making a relatively small number of \u201cbets,\u201d and there are common elements to the reasoning and approach we bring to each, so the benefit we get on this front is limited.</p>\n<p><strong>Morale and recruiting.</strong> Working in a variety of causes makes our organization a more interesting place to work. It means that our work remains exciting and motivating even as our views and our \u201cbest guesses\u201d shift, and even when there is little progress on a particular cause for a long time. It means that our work resonates with more people, broadening the community we can engage with positively. This point wouldn\u2019t be enough by itself to make the case for worldview diversification, but it is a factor in my mind, and I\u2019d be remiss not to mention it.</p>\n<h2>When and how should one practice worldview diversification?</h2>\n<p>As discussed <a href=\"https://www.openphilanthropy.org/blog/worldview-diversification#The_case_for_worldview_diversification\">above</a>, the case for worldview diversification relies heavily on two factors: (a) we have high uncertainty and find multiple worldviews highly plausible; (b) there would be strongly diminishing returns if we put all our resources behind any one worldview. Some of the secondary benefits discussed in the previous section are also specific to a public-facing organization with multiple staff. I think worldview diversification makes sense for relatively large funders, especially those with the opportunity to have a transformative impact according to multiple different highly appealing worldviews. I do not think it makes sense for an individual giving $100 or even $100,000 per year. I also do not think it makes sense for someone who is highly confident that one cause is far better than the rest.</p>\n<p>We haven\u2019t worked out much detail regarding the \u201chow\u201d of worldview diversification. In theory, one might be able to develop a formal approach that accounts for both the direct benefits of each potential grant and the myriad benefits of worldview diversification in order to arrive at conclusions about how much to allocate to each cause. One might also incorporate considerations like \u201cI\u2019m not sure whether worldviews A and B are commensurate or not; there\u2019s an X% chance they are, in which case we should allocate one way, and a Y% chance they aren\u2019t, in which case we should allocate another way.\u201d But while we\u2019ve discussed these sorts of issues, we haven\u2019t yet come up with a detailed framework along these lines. Nor have we thoroughly reflected on, and explicitly noted, which specific worldviews we find most compelling and how we weigh them against each other.</p>\n<p>We will likely put in more effort on this front in the coming year, though it won\u2019t necessarily lead to a complete or satisfying account of our views and framework. For now, some very brief notes on our practices to date:</p>\n<p><strong>Currently, we tend to invest resources in each cause up to the point where it seems like there are strongly diminishing returns, <em>or</em> the point where it seems the returns are <em>clearly</em> worse than what we could achieve by reallocating the resources - whichever comes first.</strong> A bit more specifically:</p>\n<ul>\n<li>In terms of staff capacity, so far it seems to me that there is a huge benefit to having one full-time staffer working on a given cause, supported by 1-3 other staff who spend enough time on the cause to provide informed feedback. Allocating additional staff beyond this seems generally likely to have rapidly diminishing returns, though we are taking a case-by-case approach and allocating additional staff to a cause when it seems like this could substantially improve our grantmaking.</li>\n<li>In terms of money, so far we have tried to <a href=\"http://blog.givewell.org/2015/11/25/good-ventures-and-giving-now-vs-later/#Benchmark\">roughly benchmark potential grants against direct cash transfers</a>; when it isn\u2019t possible to make a comparison, we\u2019ve often used heuristics such as \u201cDoes this grant seem reasonably likely to substantially strengthen an important aspect of the community of people/organizations working on this cause?\u201d as a way to very roughly and intuitively locate the point of strongly diminishing returns. We tend to move forward with any grant that we understand the case for reasonably well and that seems - intuitively, heuristically - strong by the standards of its cause/associated worldview (and appears at least reasonably likely, given our high uncertainty, to be competitive with grants in other causes/worldviews, including cash transfers). For causes that seem particularly promising, and/or neglected (such that we can be particularly transformative in them), we use the lower bar of funding \u201creasonably strong\u201d opportunities; for other causes, we tend more to look for \u201cvery strong\u201d opportunities. This approach is far from ideal, but has the advantage that it is fairly easy to execute in practice, given that we currently have enough resources to move forward with all grants fitting these descriptions.</li>\n</ul>\n<p>As noted above, we hope to put more thought into these issues in the coming year. Ideas for more principled, systematic ways of practicing worldview diversification would be very interesting to us.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-J89KkqwmPsBwpagTk-1\" class=\"footnote-item\"><p>One might fully accept total utilitarianism, plus the argument in <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">Astronomical Waste</a>, as well as some other premises, and believe that <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">work on global catastrophic risks</a> has far higher expected value than work on other causes.\nOne might accept total utilitarianism and the idea that the <a href=\"http://blog.givewell.org/2014/07/03/the-moral-value-of-the-far-future/\">moral value of the far future</a> overwhelms other considerations - but also believe that our impact on the far future is prohibitively hard to understand and predict, and that the right way to handle radical uncertainty about our impact is to instead focus on improving the world in measurable, robustly good ways. This view could be consistent with a number of different opinions about which causes are most worth working on.\nOne might put some credence in total utilitarianism and some credence in the idea that we have special duties to persons who live in today\u2019s society, suffer unjustly, and can benefit tangibly and observably from our actions. Depending on how one handles the \u201cnormative uncertainty\u201d between the two, this could lead to a variety of different conclusions about which causes to prioritize.\nAny of the above could constitute a \u201cworldview\u201d as I\u2019ve defined it. Views about the moral weight of animals vs. humans could additionally complicate the points above. <a href=\"#fnref-J89KkqwmPsBwpagTk-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-2\" class=\"footnote-item\"><p>Specifically, our best guess about which worldview or combination of worldviews is <em>most worth operating on</em> in order to accomplish as much good as possible. This isn\u2019t necessarily the same as which worldview is <em>most likely to represent a set of maximally correct beliefs, values and approaches</em>; it could be that a particular worldview is only 20% likely to represent a set of maximally correct beliefs, values, and approaches, but that if it does, following it would lead to &gt;100x the positive impact of following any other worldview. If such a thing were true (and knowable), then this would be the best worldview to operate on. <a href=\"#fnref-J89KkqwmPsBwpagTk-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-3\" class=\"footnote-item\"><p>(<a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">Bayesian adjustments</a> should attenuate this difference to some degree, though it\u2019s unclear how much, if you believe - as I do - that both estimates are fairly informed and reasonable though far from precise or reliable. I will put this consideration aside here.) <a href=\"#fnref-J89KkqwmPsBwpagTk-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-4\" class=\"footnote-item\"><p>Specifically, our best guess about which worldview is <em>most worth operating on</em> in order to accomplish as much good as possible. This isn\u2019t necessarily the same as which worldview is <em>most likely to represent a set of maximally correct beliefs, values and approaches</em>; it could be that a particular worldview is only 20% likely to represent a set of maximally correct beliefs, values, and approaches, but that if it does, following it would lead to &gt;100x the positive impact of following any other worldview. If such a thing were true (and knowable), then this would be the best worldview to operate on. <a href=\"#fnref-J89KkqwmPsBwpagTk-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-5\" class=\"footnote-item\"><p>Specifically, say X is the amount of good we could accomplish by investing $Y in any of the nine worldviews other than the \u201cbest\u201d one, and imagine that $Y is around the point of diminishing returns where investing 10x as much only accomplishes 2x as much good. This would then imply that putting $Y into each of the ten worldviews would have good accomplished equal to 14X (5X for the \u201cbest\u201d one, X for each of the other nine), while putting $10*Y into the \u201cbest\u201d worldview would have good accomplished equal to 10X. So the diversified approach is about as 1.4x as good by these assumptions. <a href=\"#fnref-J89KkqwmPsBwpagTk-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-6\" class=\"footnote-item\"><p>For example, say we return to the above hypothetical (see previous footnote) but also imagine that our estimates of the worldviews\u2019 value includes some mistakes, such that an unknown one of the ten actually has 1000X value and another unknown one actually has 0 value at the relevant margin. (The diminishing returns continue to work the same way.) Then putting $Y into each of the ten worldviews would have good accomplished equal to at least 1008X while putting $10<em>Y into the \u201cbest\u201d worldview would have good accomplished equal to about 208X (the latter is 2</em>(10%*1000X + 10%*8 + 80%*5X)). While in the previous case the diversified approach looked about 1.4X as good, here it looks nearly 5x as good. <a href=\"#fnref-J89KkqwmPsBwpagTk-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-7\" class=\"footnote-item\"><p>For example, see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf\">MacAskill 2014</a>. <a href=\"#fnref-J89KkqwmPsBwpagTk-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-8\" class=\"footnote-item\"><p>I\u2019m using the term \u201cpeople\u201d for simplicity, though in theory I could imagine extending the analysis in this section to the value systems of animals etc. <a href=\"#fnref-J89KkqwmPsBwpagTk-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-J89KkqwmPsBwpagTk-9\" class=\"footnote-item\"><p>I recognize that this setup has some differences with the well-known \u201cveil of ignorance\u201d proposed by Rawls, but still think it is useful for conveying intuitions in this case. <a href=\"#fnref-J89KkqwmPsBwpagTk-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "HoldenKarnofsky"}}, {"_id": "dNnSwXgohied3GKym", "title": "Efforts to Improve the Accuracy of Our Judgments and Forecasts", "postedAt": "2016-10-25T13:13:13.819Z", "htmlBody": "<p>Our grantmaking decisions rely crucially on our uncertain, subjective judgments \u2014 about the quality of some body of evidence, about the capabilities of our grantees, about what will happen if we make a certain grant, about what will happen if we <em>don\u2019t</em> make that grant, and so on.</p>\n<p>In some cases, we need to make judgments about relatively tangible outcomes in the relatively near future, as when we have supported <a href=\"https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform/justleadershipusa-close-rikers-campaign\">campaigning work for criminal justice reform</a>. In others, our work relies on speculative forecasts about the much longer term, as for example with <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence\">potential risks from advanced artificial intelligence</a>. We often try to quantify our judgments in the form of probabilities \u2014 for example, the former link estimates a <a href=\"https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform/justleadershipusa-close-rikers-campaign#Direct_impact\">20% chance of success for a particular campaign</a>, while the latter estimates a 10% chance that a particular sort of technology will be developed in the next 20 years.</p>\n<p>We think it\u2019s important to improve the accuracy of our judgments and forecasts if we can. I\u2019ve been working on a project to explore whether there is good research on the general question of how to make good and accurate forecasts, and/or specialists in this topic who might help us do so. Some preliminary thoughts follow.</p>\n<p>In brief:</p>\n<ul>\n<li>\n<p>There is a relatively thin literature on the science of forecasting.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-1\" id=\"fnref-nnx4n4K5YiWppKxsE-1\">[1]</a></sup> It seems to me that its findings so far are substantive and helpful, and that more research in this area could be promising.</p>\n</li>\n<li>\n<p>This literature recommends a small set of \u201cbest practices\u201d for making accurate forecasts that we are thinking about how to incorporate into our process. It seems to me that these \u201cbest practices\u201d are likely to be useful, and surprisingly uncommon given that.</p>\n</li>\n<li>\n<p>In one case, we are contracting to build a simple online application for credence calibration training: training the user to accurately determine how confident they should be in an opinion, and to express this confidence in a consistent and quantified way. I consider this a very useful skill across a wide variety of domains, and one that (it seems) can be learned with just a few hours of training. (Update: This calibration training app is <a href=\"https://www.openphilanthropy.org/blog/new-web-app-calibration-training\">now available</a>.)</p>\n</li>\n</ul>\n<p>I first discuss the last of these points (credence calibration training), since I think it is a good introduction to the kinds of tangible things one can do to improve forecasting ability.</p>\n<h2>Calibration training</h2>\n<p>An important component of accuracy is called \u201ccalibration.\u201d If you are \u201cwell-calibrated,\u201d what that means is that statements (including predictions) you make with 30% confidence are true about 30% of the time, statements you make with 70% confidence are true about 70% of the time, and so on.</p>\n<p>Without training, most people are not well-calibrated, but instead <em>overconfident</em>. Statements they make with 90% confidence might be true only 70% of the time, and statements they make with 75% confidence might be true only 60% of the time.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-2\" id=\"fnref-nnx4n4K5YiWppKxsE-2\">[2]</a></sup> But it is possible to \u201cpractice\u201d calibration by assigning probabilities to factual statements, then checking whether the statements are true, and tracking one\u2019s performance over time. In a few hours, one can practice on hundreds of questions and discover patterns like \u201cWhen I\u2019m 80% confident, I\u2019m right only 65% of the time; maybe I should adjust so that I report 65% for the level of internally-experienced confidence I previously associated with 80%.\u201d</p>\n<p>I recently attended a calibration training webinar run by <a href=\"http://www.hubbardresearch.com/\">Hubbard Decision Research</a>, which was essentially an abbreviated version of the classic calibration training exercise described in <a href=\"http://www.sciencedirect.com/science/article/pii/0030507380900525\">Lichtenstein &amp; Fischhoff (1980)</a>. It was also attended by two participants from other organizations, who did not seem to be familiar with the idea of calibration and, as expected, were grossly overconfident on the first set of questions.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-3\" id=\"fnref-nnx4n4K5YiWppKxsE-3\">[3]</a></sup> But, as the training continued, their scores on the question sets began to improve until, on the final question set, they both achieved perfect calibration.</p>\n<p>For me, this was somewhat inspiring to watch. It isn\u2019t often the case that a cognitive skill as useful and domain-general as probability calibration can be trained, with such objectively-measured dramatic improvements, in so short a time.</p>\n<p>The research I\u2019ve reviewed broadly supports this impression. For example:</p>\n<ul>\n<li>\n<p><a href=\"http://www.tandfonline.com/doi/full/10.1080/08850600490273431\">Rieber (2004)</a> lists \u201ctraining for calibration feedback\u201d as his first recommendation for improving calibration, and summarizes a number of studies indicating both short- and long-term improvements on calibration.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-4\" id=\"fnref-nnx4n4K5YiWppKxsE-4\">[4]</a></sup> In particular, decades ago, Royal Dutch Shell began to provide calibration for their geologists, who are now (reportedly) quite well-calibrated when forecasting which sites will produce oil.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-5\" id=\"fnref-nnx4n4K5YiWppKxsE-5\">[5]</a></sup></p>\n</li>\n<li>\n<p>Since 2001, Hubbard Decision Research trained over 1,000 people across a variety of industries. Analyzing the data from these participants, Doug Hubbard reports that 80% of people achieve perfect calibration (on trivia questions) after just a few hours of training. He also claims that, according to his data and at least one controlled (but not randomized) trial, this training predicts subsequent real-world forecasting success.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-6\" id=\"fnref-nnx4n4K5YiWppKxsE-6\">[6]</a></sup>\nI should note that calibration isn\u2019t sufficient by itself for good forecasting. For example, you can be well-calibrated on a set of true/false statements, for which about half the statements happen to be true, simply by responding \u201cTrue, with 50% confidence\u201d to every statement. This performance would be well-calibrated but not very <em>informative</em>. Ideally, an expert would assign high confidence to statements that are likely to be true, and low confidence to statements that are unlikely to be true. An expert that can do so is not just well-calibrated, but also exhibits good \u201cresolution\u201d (sometimes called \u201cdiscrimination\u201d). If we combine calibration and resolution, we arrive at a measure of accuracy called a \u201c<a href=\"https://en.wikipedia.org/wiki/Scoring_rule\">proper scoring rule</a>.\u201d<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-7\" id=\"fnref-nnx4n4K5YiWppKxsE-7\">[7]</a></sup> The calibration trainings described above sometimes involve proper scoring rules, and likely train people to be well-calibrated while exhibiting at least some resolution, though the main benefit they seem to have (based on the research and my observations) pertains to calibration specifically.</p>\n</li>\n</ul>\n<p>The primary source of my earlier training in calibration was a <a href=\"http://acritch.com/credence-game/\">game</a> intended to automate the process. The Open Philanthropy Project is now working with developers to create a more extensive calibration training game for training our staff; we will also make the game available publicly.</p>\n<h2>Further advice for improving judgment accuracy</h2>\n<p>Below I list some common advice for improving judgment and forecasting accuracy (in the absence of strong causal models or much statistical data) that has at least <em>some</em> support in the academic literature, and which I find intuitively likely to be helpful.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-8\" id=\"fnref-nnx4n4K5YiWppKxsE-8\">[8]</a></sup></p>\n<ol>\n<li>\n<p><strong>Train probabilistic reasoning</strong>: In one especially compelling study (<a href=\"http://journal.sjdm.org/16/16511/jdm16511.html\">Chang et al. 2016</a>), a single hour of training in probabilistic reasoning noticeably improved forecasting accuracy.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-9\" id=\"fnref-nnx4n4K5YiWppKxsE-9\">[9]</a></sup> Similar training has improved judgmental accuracy in some earlier studies,<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-10\" id=\"fnref-nnx4n4K5YiWppKxsE-10\">[10]</a></sup> and is sometimes included in calibration training.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-11\" id=\"fnref-nnx4n4K5YiWppKxsE-11\">[11]</a></sup></p>\n</li>\n<li>\n<p><strong>Incentivize accuracy</strong>: In many domains, incentives for accuracy are overwhelmed by stronger incentives for other things, such as incentives for appearing confident, being entertaining, or signaling group loyalty. Some studies suggest that accuracy can be improved merely by providing sufficiently strong incentives for accuracy such as money or the approval of peers.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-12\" id=\"fnref-nnx4n4K5YiWppKxsE-12\">[12]</a></sup></p>\n</li>\n<li>\n<p><strong>Think of alternatives</strong>: Some studies suggest that judgmental accuracy can be improved by prompting subjects to consider alternate hypotheses.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-13\" id=\"fnref-nnx4n4K5YiWppKxsE-13\">[13]</a></sup></p>\n</li>\n<li>\n<p><strong>Decompose the problem</strong>: Another common recommendation is to break each problem into easier-to-estimate sub-problems.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-14\" id=\"fnref-nnx4n4K5YiWppKxsE-14\">[14]</a></sup></p>\n</li>\n<li>\n<p><strong>Combine multiple judgments</strong>: Often, a weighted (and sometimes \u201cextremized\u201d<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-15\" id=\"fnref-nnx4n4K5YiWppKxsE-15\">[15]</a></sup>) combination of multiple subjects\u2019 judgments outperforms the judgments of any one person.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-16\" id=\"fnref-nnx4n4K5YiWppKxsE-16\">[16]</a></sup></p>\n</li>\n<li>\n<p><strong>Correlates of judgmental accuracy</strong>: According to some of the most compelling studies on forecasting accuracy I\u2019ve seen,<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-17\" id=\"fnref-nnx4n4K5YiWppKxsE-17\">[17]</a></sup> correlates of good forecasting ability include \u201cthinking like a fox\u201d (i.e. eschewing grand theories for attention to lots of messy details), strong domain knowledge, general cognitive ability, and high scores on \u201cneed for cognition,\u201d \u201cactively open-minded thinking,\u201d and \u201ccognitive reflection\u201d scales.</p>\n</li>\n<li>\n<p><strong>Prediction markets:</strong> I\u2019ve seen it argued, and find it intuitive, that an organization might improve forecasting accuracy by using <a href=\"https://en.wikipedia.org/wiki/Prediction_market\">prediction markets</a>. I haven\u2019t studied the performance of prediction markets yet.</p>\n</li>\n<li>\n<p><strong>Learn a lot about the phenomena you want to forecast</strong>: This one probably sounds obvious, but I think it\u2019s important to flag, to avoid leaving the impression that forecasting ability is more cross-domain/generalizable than it is. Several studies suggest that accuracy can be boosted by having (or acquiring) domain expertise. A commonly-held hypothesis, which I find intuitively plausible, is that calibration training is especially helpful for improving calibration, and that domain expertise is helpful for improving resolution.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-18\" id=\"fnref-nnx4n4K5YiWppKxsE-18\">[18]</a></sup>\nAnother interesting takeaway from the forecasting literature is the degree to which - and consistency with which - some experts exhibit better accuracy than others. For example, tournament-level bridge players tend to show reliably good accuracy, whereas TV pundits, political scientists, and professional futurists seem not to.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-19\" id=\"fnref-nnx4n4K5YiWppKxsE-19\">[19]</a></sup> A famous recent result in comparative real-world accuracy comes from a series of <a href=\"https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity\">IARPA</a> forecasting tournaments, in which ordinary people competed with each other and with professional intelligence analysts (who also had access to expensively-collected classified information) to forecast geopolitical events. As reported in Tetlock &amp; Gardner\u2019s <em><a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Superforecasting</a></em>, forecasts made by combining (in a certain way) the forecasts of the best-performing ordinary people were (repeatedly) more accurate than those of the trained intelligence analysts.</p>\n</li>\n</ol>\n<h2>How commonly do people seek to improve the accuracy of their subjective judgments?</h2>\n<p>Certainly many organizations, from financial institutions (e.g. see <a href=\"http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118006739.html\">Fabozzi 2012</a>) to sports teams (e.g. see <em><a href=\"http://books.wwnorton.com/books/978-0-393-05765-2/\">Moneyball</a></em>), use sophisticated quantitative models to improve the accuracy of their estimates. But the question I\u2019m asking here is: In the absence of strong models and/or good data, when decision-makers must rely almost entirely on human subjective judgment, how common is it for those decision-makers to explicitly invest substantial effort into improving the (objectively-measured) accuracy of those subjective judgments?</p>\n<p>Overall, my impression is that the answer to this question is \u201cSomewhat rarely, in most industries, even though the techniques listed above are well-known to experts in judgment and forecasting accuracy.\u201d</p>\n<p>Why do I think that? It\u2019s difficult to get good evidence on this question, but I provide some data points in a footnote.<sup class=\"footnote-ref\"><a href=\"#fn-nnx4n4K5YiWppKxsE-20\" id=\"fnref-nnx4n4K5YiWppKxsE-20\">[20]</a></sup></p>\n<h2>Ideas we\u2019re exploring to improve accuracy for GiveWell and Open Philanthropy Project staff</h2>\n<p>Below is a list of activities, aimed at improving the accuracy of our judgments and forecasts, that are either ongoing, under development, or under consideration at GiveWell and the Open Philanthropy Project:</p>\n<ul>\n<li>As noted above, we have contracted a team of software developers to create a calibration training web/phone application for staff and public use. (Update: This calibration training app is <a href=\"https://www.openphilanthropy.org/blog/new-web-app-calibration-training\">now available</a>.)</li>\n<li>We encourage staff to participate in prediction markets and forecasting tournaments such as <a href=\"https://www.predictit.org/\">PredictIt</a> and <a href=\"https://www.gjopen.com/\">Good Judgment Open</a>, and some staff do so.</li>\n<li>Both the Open Philanthropy Project and GiveWell recently began to make probabilistic forecasts about our grants. For the Open Philanthropy Project, see e.g. our forecasts about recent grants to <a href=\"http://www.openphilanthropy.org/giving/grants/university-pennsylvania-philip-tetlock-forecasting#Internal_forecasts\">Philip Tetlock</a> and <a href=\"http://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/CIWF-USA-general-support#Internal_forecasts\">CIWF</a>. For GiveWell, see e.g. forecasts about recent grants to <a href=\"http://www.givewell.org/evidence-action/march-2016-grant#Internalforecasts\">Evidence Action</a> and <a href=\"http://www.givewell.org/international/charities/ipa/may-2016-grant#Risksofthegrantandinternalforecasts\">IPA</a>. We also make and track some additional grant-related forecasts privately. The idea here is to be able to measure our accuracy later, as those predictions come true or are falsified, and perhaps to improve our accuracy from past experience. So far, we are simply encouraging predictions without putting much effort into ensuring their later measurability.</li>\n<li>We\u2019re going to experiment with some forecasting sessions led by an experienced \u201cforecast facilitator\u201d - someone who helps elicit forecasts from people about the work they\u2019re doing, in a way that tries to be as informative and helpful as possible. This might improve the forecasts mentioned in the previous bullet point.</li>\n</ul>\n<p>I\u2019m currently the main person responsible for improving forecasting at the Open Philanthropy Project, and I\u2019d be very interested in further ideas for what we could do.</p>\n<h2>Notes</h2>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-nnx4n4K5YiWppKxsE-1\" class=\"footnote-item\"><p>Technically, the scientific study of forecasting goes back to at least the 1940s, and arguably earlier. However, I am most interested in studies which do all of the following: collect forecasts about phenomena for which there aren\u2019t strong models and/or lots of data; assess the accuracy of those forecasts using a proper scoring rule; relative to the accuracy achieved by some reasonable baseline or control group; and which don\u2019t have other well-known but common limitations, such as failing to adjust for multiple comparisons.\nIn attempting to learn what I can from the forecasting literature, I haven\u2019t relied <em>exclusively</em> on studies which have all the features listed above, but my hope is that this list of features helps to clarify which types of studies I\u2019ve tried hardest to find and learn from. It is in this sense that the science of forecasting is a \u201cthin literature,\u201d even though there are thousands of published papers about forecasting, stretching back to the 1940s and earlier. <a href=\"#fnref-nnx4n4K5YiWppKxsE-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-2\" class=\"footnote-item\"><p><a href=\"http://eprints.lse.ac.uk/27324/\">Lichtenstein et al. (1982)</a>; <a href=\"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP002487.html\">Bazerman &amp; Moore (2013)</a>, ch. 2. <a href=\"#fnref-nnx4n4K5YiWppKxsE-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-3\" class=\"footnote-item\"><p>I had previously practiced calibration using an online <a href=\"http://acritch.com/credence-game/\">game</a> intended to give a form of automated calibration training. <a href=\"#fnref-nnx4n4K5YiWppKxsE-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-4\" class=\"footnote-item\"><p>From <a href=\"http://www.tandfonline.com/doi/full/10.1080/08850600490273431\">Russo (2004)</a>: \u201cEvidence indicates that the calibration of judgment can be substantially enhanced through feedback about one\u2019s own probability judgments. In one experiment, participants working at computers were asked general knowledge questions to which they gave their answers as well as their subjective probabilities. After each session of 200 items, the participants received a summary of their performance, which they discussed with the experimenter. Most participants were poorly calibrated before the training; of these, all substantially improved. Moreover, although the subjects participated in eleven training sessions, \u201call of the improvement came between the first and second round of feedback.\u201d Since the first training session lasted about an hour, with another forty-five minutes for preliminary instruction, it appears that with intensive feedback calibration can be dramatically improved in approximately two hours.\nP. George Benson and Dilek Onkal report similarly large gains after calibration feedback in a forecasting task. In this study, the improvement also occurred in one step, but it was between the second and third training sessions. Likewise, Marc Alpert and Howard Raiffa report that after calibration feedback the number of 98 percent confidence-range questions missed by their Harvard MBA students \u201cfell from a shocking 41 percent to a depressing 23 percent.\u201d While 23 percent is far more than the ideal 2 percent, it nevertheless represents a large improvement from 41 percent and it, too, was achieved after only one round of practice.\nWhile these results occurred in a laboratory setting, some evidence shows that calibration training in the workplace can be effective as well. The energy firm Royal Dutch/Shell successfully implemented a training program to improve the calibration of its geologists in finding oil deposits. Prior to the training, the geologists had been markedly overconfident, assigning a 40 percent confidence to locations that yielded oil less than 20 percent of the time. Predicting the location of oil deposits is clearly very different from predicting international events, with fewer variables and more reliable data. But, it may be analogous to imagery intelligence.\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-5\" class=\"footnote-item\"><p><a href=\"http://www.palgraveconnect.com/esm/doifinder/10.1057/9781137294678.0505\">Russo &amp; Schoemaker (2014)</a>: \u201c[The geologists] were given files from their archives containing many factors affecting oil deposits, but without the actual results. For each past case, they had to provide best guesses for the probability of striking oil as well as ranges as to how much a successful well might produce. Then they were given feedback as to what had actually happened. The training worked wonderfully: now, when Shell geologists predict a 30 per cent chance of producing oil, three out of ten times the company averages a hit\u2026\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-6\" class=\"footnote-item\"><p>From <a href=\"http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119085292.html\">Hubbard &amp; Seiersen (2016)</a>, ch. 7: \u201cSince [2001], Hubbard and his team at Hubbard Decision Research have trained well over 1,000 people in calibration methods and have recorded their performance, both their expected and actual results on several calibration tests, given one after the other during a half-day workshop\u2026\nTo determine who is calibrated we have to allow for some deviation from the target, even for a perfectly calibrated person. Also, an uncalibrated person can get lucky. Accounting for this statistical error in the testing, fully 80% of participants are ideally calibrated after the fifth calibration exercise. They are neither underconfident nor overconfident. Their 90% [confidence intervals] have about a 90% chance of containing the correct answer.\nAnother 10% show significant improvement but don\u2019t quite reach ideal calibration. And 10% show no significant improvement at all from the first test they take\u2026\n\u2026But does proven performance in training reflect an ability to assess the odds of real-life uncertainties? The answer here is an unequivocal yes. Hubbard tracked how well-calibrated people do in real-life situations on multiple occasions, but one particular controlled experiment done in the IT industry still stands out. In 1997, Hubbard was asked to train the analysts of the IT advisory firm Giga Information Group (since acquired by Forrester Research, Inc.) in assigning odds to uncertain future events. Giga was an IT research firm that sold its research to other companies on a subscription basis. Giga had adopted the method of assigning odds to events it was predicting for clients, and it wanted to be sure it was performing well.\nHubbard trained 16 Giga analysts using the methods described earlier. At the end of the training, the analysts were given 20 specific IT industry predictions they would answer as true or false and to which they would assign a confidence. The test was given in January 1997, and all the questions were stated as events occurring or not occurring by June 1, 1997 (e.g., \u201cTrue or False: Intel will release its 300 MHz Pentium by June 1,\u201d etc.). As a control, the same list of predictions was also given to 16 of their chief information officer (CIO) clients at various organizations. After June 1 the actual outcomes could be determined. Hubbard presented the results at Giga World 1997, their major IT industry symposium for the year\u2026\n\u2026the analysts\u2019 results\u2026 were very close to the ideal confidence, easily within allowable error\u2026\nIn comparison, the results of clients who did not receive any calibration training (indicated by the small triangles) were very overconfident\u2026 All of these results are consistent with what has typically been observed in a number of other calibration studies over the past several decades.\u201d\nI haven\u2019t seen the details of Hubbard\u2019s study, and in any case it suffers from multiple design limitations \u2014 for example, the treatment (calibration training) wasn\u2019t assigned randomly. <a href=\"#fnref-nnx4n4K5YiWppKxsE-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-7\" class=\"footnote-item\"><p>A proper scoring rule, applied to a set of probabilistic judgments or forecasts, awards points for both calibration and resolution, and does so in a way that incentivizes judges to report their probabilities honestly.\nMeasures like this should be assessed with respect to an appropriate benchmark. <a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Tetlock &amp; Gardner (2015)</a> explain this point in the context of assessing forecasts for accuracy using a proper scoring rule called a Brier score, which ranges from 0 to 1, with lower numbers representing better scores (ch. 3):\n\u201cLet\u2019s suppose we discover that you have a Brier score of 0.2. That\u2019s far from godlike omniscience (0) but a lot better than chimp-like guessing (0.5), so it falls in the range of what one might expect from, say, a human being. But we can say much more than that. What a Brier score means depends on what\u2019s being forecast. For instance, it\u2019s quite easy to imagine circumstances where a Brier score of 0.2 would be disappointing. Consider the weather in Phoenix, Arizona. Each June, it gets very hot and sunny. A forecaster who followed a mindless rule like, \u201calways assign 100% to hot and sunny\u201d could get a Brier score close to 0, leaving 0.2 in the dust. Here, the right test of skill would be whether a forecaster can do better than mindlessly predicting no change. This is an underappreciated point. For example, after the 2012 presidential election, Nate Silver, Princeton\u2019s Sam Wang, and other poll aggregators were hailed for correctly predicting all fifty state outcomes, but almost no one noted that a crude, across-the-board prediction of \u201cno change\u201d \u2014 if a state went Democratic or Republican in 2008, it will do the same in 2012 \u2014 would have scored forty-eight out of fifty, which suggests that the many excited exclamations of \u201cHe called all fifty states!\u201d we heard at the time were a tad overwrought. Fortunately, poll aggregators are pros: they know that improving predictions tends to be a game of inches.\nAnother key benchmark is other forecasters. Who can beat everyone else? Who can beat the consensus forecast? How do they pull it off? Answering these questions requires comparing Brier scores, which, in turn, requires a level playing field. Forecasting the weather in Phoenix is just plain easier than forecasting the weather in Springfield, Missouri, where weather is notoriously variable, so comparing the Brier scores of a Phoenix meteorologist with those of a Springfield meteorologist would be unfair. A 0.2 Brier score in Springfield could be a sign that you are a world-class meteorologist. It\u2019s a simple point, with a big implication: dredging up old forecasts from newspapers will seldom yield apples-to-apples comparisons because, outside of tournaments, real-world forecasters seldom predict exactly the same developments over exactly the same time period.\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-8\" class=\"footnote-item\"><p>In the footnotes that follow each piece of \u201ccommon advice\u201d I list for this post, I do not provide a thorough evaluation of the evidence supporting each claim, but merely provide some pointers to the available evidence. I have skimmed these and other studies only briefly, and my choices for which pieces of advice to include here relies as much on my intuitions about what seems likely to work \u2014 given my studies of forecasting, psychology, and other fields, as well as my general understanding of the world \u2014 as it does on an evaluation of the specific evidence I point to. In fact, I suspect that upon closer examination, I would find some of the primary studies listed or cited in these footnotes to be deeply flawed and unconvincing.\nMy list of common advice is not an exhaustive one. For additional suggestions, see e.g. <a href=\"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP002487.html\">Bazerman &amp; Moore (2013)</a> ch. 12, <a href=\"http://www.tandfonline.com/doi/full/10.1080/08850600490273431\">Rieber (2004)</a>, and <a href=\"https://books.google.com/books?id=XwjsCgAAQBAJ&amp;lpg=PA1&amp;pg=PA924#v=onepage&amp;q&amp;f=false\">Soll et al. (2016)</a>. <a href=\"#fnref-nnx4n4K5YiWppKxsE-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-9\" class=\"footnote-item\"><p><a href=\"http://journal.sjdm.org/16/16511/jdm16511.html\">Chang et al. (2016)</a> describe the training module randomly assigned to some participants in the Good Judgment Project forecasting tournaments:\n\u201cTraining evolved from year 1 to 4, but was never designed to take more than an hour. Common probabilistic reasoning principles included the understanding and use of event base-rates, basic principles of belief updating in a way that reflected the probative value of new evidence, the value of averaging independent evidence, the difference between calibration and resolution in Brier scoring, the pros and cons of using statistical-mathematical models to inform forecasts, and a discussion of common biases in probability judgment.\n\u2026Training in year 1 consisted of two different modules: probabilistic reasoning training and scenario training. Scenario-training was a four-step process: 1) developing coherent and logical probabilities under the probability sum rule; 2) exploring and challenging assumptions; 3) identifying the key causal drivers; 4) considering the best and worst case scenarios and developing a sensible 95% confidence interval of possible outcomes; and 5) avoid over-correction biases. The principles were distilled into an acronym QUEST: Question views, Use plausible worst-case and best-case scenarios, Explore assumptions, Several assumptions should be considered, Take heed of biases\u2026 Scenario training was designed in a way very similar to analytic training already used by the intelligence community, encouraging trainees to think critically about assumptions, potential futures, and causal mechanisms that could be at play on a given forecasting question.\nProbabilistic reasoning training consisted of lessons that detailed the difference between calibration and resolution, using comparison classes and base rates (Kahneman &amp; Tversky, 1973; Tversky &amp; Kahneman, 1981), averaging and using crowd wisdom principles (Surowiecki, 2005), finding and utilizing predictive mathematical and statistical models (Arkes, 1981; Kahneman &amp; Tversky, 1982), cautiously using time-series and historical data, and being self-aware of the typical cognitive biases common throughout the population. The training encouraged forecasters to remember the principles by the acronym CHAMP (Table 2)\u2026\nIn year 2, probabilistic reasoning and scenario training were combined into a single module. Graphics and more checks on learning were added.\nYear 3 expanded on year 1 and year 2 training by delivering the content in a graphical format (online via commercial software) and adding a letter S to CHAMP, as well as a new political science content module described by the acronym KNOW. The additional S encouraged forecasters to select the right questions to answer and seek out subjects where they have a comparative advantage. The additional KNOW module encouraged forecasters to understand the dynamics involving key political players (Bueno De Mesquita &amp; Smith, 2005; Waltz, 2001), determine the influence of norms and international institutions (Finnemore &amp; Sikkink, 1998; Keohane, 2005), seek out other political perspectives and be aware of potential wildcard scenarios (Taleb, 2010). The original CHAMP guidelines were also slightly modified based on lessons learned and observation of the best forecasters, together forming the revised guidelines under the acronym CHAMPS KNOW (Table 3). Additional checks on learning (i.e., short quizzes) were integrated into this version of the training as well\u2026\nYear 4 training was very similar to year 3 training. The probabilistic reasoning training was delivered via a customized web platform. Almost all information conveyed was illustrated with graphical examples or pictures. The main CHAMPS KNOW framework remained intact \u2014 save for the revision of the S guideline from \u201cSelect the right questions to answer\u201d to \u201cSelect the right level of effort to devote to each question,\u201d which provided a sharper and clearer description of performing cognitive triage on the forecasting question pool\u2026\nTraining yielded significant improvements in Brier score across all four tournament years (Figure 1). In year 1, both probability-trained forecasters (n = 119, MStd Brier Score = -0.05, SD = 0.24) and scenario-trained forecasters (n = 113, MStd Brier Score = -0.06, SD = 0.23) outperformed control forecasters (n = 152, M Std Brier Score = +0.07, SD = 0.28), F(2, 381) = 12.1, p &lt; .001. Accuracy did not differ between probability-trained and scenario-trained forecasters. The improvement in mean Brier scores from probability-training and scenario-training was 10% and 11%, respectively, relative to control forecasters.\nIn year 2, training increased accuracy, with probability-trained individuals (n = 205, MStd Brier Score = \u20130.10, SD Std = 0.25) outperforming control individuals (n = 194, M Std Brier Score = +0.05, SD Std = 0.25), t(395) = 5.95, p &lt; .001, a 12% score improvement. In year 3, training was associated with better performance (trained n = 97, MStd Brier Score = \u20130.08, SD Std = 0.27, control n = 116, MStd Brier Score = 0.00, SD Std = 0.28), t(207) = 2.32, p = .021, with trained individuals again achieving greater accuracy than controls, a 6% score improvement. Finally, in year 4, training was also significant, (trained n = 131, M Std Brier Score = \u20130.01, SD Std = 0.26, control n = 102, MStdBrierScore = \u20130.08, SD Std = 0.24), t(225) = 2.20, p = .028, a 7% score improvement. Additionally, as reported elsewhere, training improved the calibration and resolution of forecasters by reducing overconfidence (Mellers et al., 2014; Moore et al., 2016). Overall, the individual forecasters with probability-training consistently outperformed controls across all four years (Table 4).\u201d\nSection 1 of this paper also provides a succinct and up-to-date review of past work on debiasing and judgmental accuracy training.\nMy judgment that Chang et al. (2016) is an \u201cespecially compelling\u201d study comes substantially (but not entirely) from the fact that it overcomes some of the limitations of past work, as summarized by the authors:\n\u201cA number of studies have shed light on how probability estimates and judgments can be improved\u2026 However, past work suffers from at least six sets of limitations: 1) over-reliance on student subjects who are often neither intrinsically nor extrinsically motivated to master the task\u2026; 2) one-shot experimental tasks that limit both subjects\u2019 opportunities to learn and researchers\u2019 opportunities to assess whether experimentally induced gains were sustainable over time or whether they just briefly cued better thinking\u2026; 3) brief training modules, often as short as 10\u201315 minutes, that afforded few opportunities for retesting\u2026 and exploring the potential interactive effects of training and deliberate practice\u2026; 4) debiasing interventions that are narrowly tailored to a single bias (e.g., over-confidence, hindsight) and not designed to help with problems that activate multiple biases\u2026; 5) multifaceted and lengthy educational interventions, such as statistics courses, that are high in ecological validity but lose the internal validity advantages that accrue from random assignment\u2026; and 6) limited study of the moderating effects of individual differences beyond cognitive ability\u2026\nWe set out to overcome many of these problems. Our study uses a highly diverse cross-section of the population that, based on the effort expended for compensation provided, is almost certainly more intrinsically motivated than the standard undergraduate sample. The research went on for four years, tested lengthier debiasing methods, and investigated individual-difference moderators. Our study also represents one of the most rigorous tests of debiasing methods to date. The open-ended experimental task, forecasting a wide range of political and economic outcomes, is widely recognized as difficult (Jervis, 2010; Tetlock, 2005)\u2026 Our work does not correct all of the aforementioned conceptual and methodological problems, but we can address a significant fraction of them.\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-10\" class=\"footnote-item\"><p>For example, individual components of the training module from Chang et al. (2016) have been tested in earlier studies, as noted by Chang et al. (2016):\n\u201cConsidering base rates can also improve judgmental accuracy (Kahneman &amp; Tversky, 1973; Tversky &amp; Kahneman, 1981). [And] teaching people reference-class forecasting reduces base-rate neglect more than calling attention to the bias itself (Case, Fantino &amp; Goodie, 1999; Fischhoff &amp; Bar-Hillel, 1984; Flyvbjerg, 2008; Kahneman &amp; Tversky, 1977; Lovallo, Clarke &amp; Camerer, 2012).\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-11\" class=\"footnote-item\"><p>For example, the Doug Hubbard training I attended included some training in probabilistic reasoning, which in part was necessary to ensure the participants understood how the calibration training was supposed to work. <a href=\"#fnref-nnx4n4K5YiWppKxsE-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-12\" class=\"footnote-item\"><p>Presumably, strong monetary incentives are the primary reason why most financial markets are as efficient as they are, and strong monetary and/or reputational inventives explain why prediction markets work as well as they do (<a href=\"http://www.ingentaconnect.com/content/aea/jep/2004/00000018/00000002/art00006\">Wolfers &amp; Zitzewitz 2004</a>).\nRelatedly, <a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Tetlock &amp; Gardner (2015)</a> remark:\n\u201cIt is quite remarkable how much better calibrated forecasters are in the public IARPA tournaments than they were in [Tetlock\u2019s] earlier anonymity-guaranteed EPJ tournaments. And the evidence from lab experiments is even more decisive. Public tournaments create a form of accountability that attunes us to the possibility we might be wrong. Tournaments have the effect that Samuel Johnson ascribed to the gallows: they concentrate the mind (in the case of tournaments, on avoiding reputational death). See P. E. Tetlock and B. A. Mellers, \u201cStructuring Accountability Systems in Organizations,\u201d in Intelligence Analysis: Behavioral and Social Scientific Foundations, ed. B. Fischhoff and C. Chauvin (Washington, DC: National Academies Press, 2011), pp. 249\u201370; J. Lerner and P. E. Tetlock, \u201cAccounting for the Effects of Accountability,\u201d Psychological Bulletin 125 (1999): 255\u201375.\u201d\n<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2703011\">Kahan (2015)</a> summarizes an emerging literature on monetary incentives for accuracy in the context of politically motivated reasoning:\n\u201cIn an important development, several researchers have recently reported that offering mon-etary incentives can reduce or eliminate polarization in the answers that subjects of diverse political out-looks give to questions of partisan import (Khanna &amp; Sood 2016; Prior, Sood &amp; Gaurav 2015; Bullock, Gerber, Hill &amp; Huber 2015) ...\nIf monetary incentives do meaningfully reverse identity-protective forms of information processing in studies that reflect the PMRP [politically-motivated reasoning paradigm] design, then a plausible inference would be that offering rewards for \u201ccorrect answers\u201d is a sufficient intervention to summon the truth-seeking information-processing style that (at least some) subjects use outside of domains that feature identity-expressive goals. In effect, the incentives transform subjects\u2019 from identity-protectors to knowledge revealers (Kahan 2015a), and activate the corresponding shift in information-processing styles appropriate to those roles.\nWhether this would be the best understanding of such results, and what the practical implications of such a conclusion would be, are also matters that merit further empirical examination.\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-13\" class=\"footnote-item\"><p>In his review of \u201cdebiasing\u201d strategies, <a href=\"https://books.google.com/books?id=s73eYl1DRHUC&amp;lpg=PA316&amp;ots=ndQzlLpnen&amp;lr&amp;pg=PA316#v=onepage&amp;q&amp;f=false\">Larrick (2004)</a> summarized the evidence for the \u201cthink of alternatives\u201d strategy this way:\n\u201cBy necessity, cognitive strategies tend to be context-specific rules tailored to address a narrow set of biases, such as the law of large numbers or the sunk cost rule. This fact makes the simple but general strategy of \u201cconsider the opposite\u201d all the more impressive, because it has been effective at reducing overconfidence, hindsight biases, and anchoring effects (see Arkes, 1991; Mussweiler, Strack, &amp; Pfeiffer, 2000). The strategy consists of nothing more than asking oneself, \u201cWhat are some reasons that my initial judgment might be wrong?\u201d The strategy is effective because it directly counteracts the basic problem of association-based processes \u2013 an overly narrow sample of evidence \u2013 by expanding the sample and making it more representative. Similarly, prompting decision makers to consider alternative hypotheses has been shown to reduce confirmation biases in seeking and evaluating new information.\nSoll and Klayman (2004) have offered an interesting variation on \u201cconsider the opposite.\u201d Typically, subjective range estimates exhibit high overconfidence. Ranges for which people are 80 percent confident capture the truth 30 percent to 40 percent of the time. Soll and Klayman (2004) showed that having judges generate 10th and 90th percentile estimates in separate stages \u2013 which forces them to consider distinct reasons for low and high values \u2013 increased hit rates to nearly 60 percent by both widening and centering ranges.\n\u201cConsider the opposite\u201d works because it directs attention to contrary evidence that would not otherwise be considered. By comparison, simply listing reasons typically does not improve decisions because decision makers tend to generate supportive reasons. Also, for some tasks, reason generation can disrupt decision-making accuracy if there is a poor match between the reasons that are easily articulated and the actual factors that determine an outcome (Wilson &amp; Schooler, 1991). Lastly, asking someone to list too many contrary reasons can backfire \u2013 the difficulty of generating the tenth \u201ccon\u201d can convince a decision maker that her initial judgment must have been right after all\u2026\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-14\" class=\"footnote-item\"><p><a href=\"https://books.google.com/books?id=XwjsCgAAQBAJ&amp;lpg=PA12&amp;pg=PA182#v=onepage&amp;q&amp;f=false\">Moore et al. (2016)</a> summarize a few of the studies on problem decomposition and judgment accuracy:\n\u201cResearchers have devoted a great deal of effort to developing ways to reduce overprecision [a type of overconfidence]. Most of the research has revolved around three main approaches\u2026 [one of which is] decomposing the response set or alternatives into smaller components and considering each one of them separately\u2026\n\u2026[This approach] capitalizes on support theory\u2019s subadditivity effect (Tversky &amp; Koehler, 1994). It suggests counteracting overprecision by taking the focal outcome and decomposing it into more specific alternatives. Fischhoff, Slovic, and Lichtenstein (1978) found that the sum of all probabilities assigned to the alternatives that make up the set is larger than the probability assigned to the set as a whole. Thus, when estimating likelihoods for a number of possible outcomes, the more categories the judge is assessing (and the less we include under \u201call others\u201d) the less confident they will be that their chosen outcome is the correct one. Decomposition of confidence intervals has also achieved encouraging results. Soll and Klayman (2004) asked participants to estimate either an 80% confidence interval or the 10th and 90th fractiles separately (the distance between which should cover 80% of the participant\u2019s probability distribution). They found that the consideration of the high and low values separately resulted in wider and less overprecise intervals.\nOne elicitation method combines both the consideration of more information and the decomposition of the problem set into more specific subsets. The SPIES method (short for Subjective Probability Interval Estimates) (Haran, Moore, &amp; Morewedge, 2010) turns a confidence interval into a series of probability estimates for different categories across the entire problem set. Instead of forecasting an interval that should include, with a certain level of confidence, the correct answer, the participant is presented with the entire range of possible outcomes. This range is divided into bins, and the participant estimates the probability of each bin to include the correct answer. For example, to predict the daily high temperature in Chicago on May 21, we can estimate the probability that this temperature will be below 50\u00b0F, between 51\u00b0F and 60\u00b0F, between 61\u00b0F and 70\u00b0F, between 71\u00b0F and 80\u00b0F, between 81\u00b0F and 90\u00b0F, and 91\u00b0F or more. Because these bins cover all possible options, the sum of all estimates should amount to 100%. From these subjective probabilities we can extract an interval for any desired confidence level. This method not only produces confidence intervals that are less overprecise than those produced directly but it also reduces overprecision in subsequent estimates when participants switch back to the traditional confidence interval method (Haran, Moore et al., 2010). This reduction, however, does not seem to stem from the generalization of a better estimation process. Rather, the most pronounced improvements in estimates after a SPIES practice period seem to be when the SPIES task turns judges\u2019 attention to values previously regarded as the most unlikely (Haran, 2011). It may be possible, then, that when people are made aware of the possibility that their knowledge is incomplete (by directly estimating likelihoods of values which they completely ignored before), they increase caution in their confidence intervals.\u201d\n<a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Tetlock &amp; Gardner (2015)</a>, on the basis of the forecasting tournaments reported in that book, list problem decomposition as one of their \u201cTen Commandments for Aspiring Superforecasters\u201d:\n(2) Break seemingly intractable problems into tractable sub-problems.\nChannel the playful but disciplined spirit of Enrico Fermi who \u2014 when he wasn\u2019t designing the world\u2019s first atomic reactor \u2014 loved ballparking answers to head-scratchers such as \u201cHow many extraterrestrial civilizations exist in the universe?\u201d Decompose the problem into its knowable and unknowable parts. Flush ignorance into the open. Expose and examine your assumptions. Dare to be wrong by making your best guesses. Better to discover errors quickly than to hide them behind vague verbiage.\nSuperforecasters see Fermi-izing as part of the job. How else could they generate quantitative answers to seemingly impossible-to-quantify questions about Arafat\u2019s autopsy, bird-flu epidemics, oil prices, Boko Haram, the Battle of Aleppo, and bond-yield spreads.\nWe find this Fermi-izing spirit at work even in the quest for love, the ultimate unquantifiable. Consider Peter Backus, a lonely guy in London, who guesstimated the number of potential female partners in his vicinity by starting with the population of London (approximately six million) and winnowing that number down by the proportion of women in the population (about 50%), by the proportion of singles (about 50%), by the proportion in the right age range (about 20%), by the proportion of university graduates (about 26%), by the proportion he finds attractive (only 5%), by the proportion likely to find him attractive (only 5%), and by the proportion likely to be compatible with him (about 10%). Conclusion: roughly twenty-six women in the pool, a daunting but not impossible search task.\nThere are no objectively correct answers to true-love questions, but we can score the accuracy of the Fermi estimates that superforecasters generate in the IARPA tournament. The surprise is how often remarkably good probability estimates arise from a remarkably crude series of assumptions and guesstimates.\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-14\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-15\" class=\"footnote-item\"><p><a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Tetlock &amp; Gardner (2015)</a>, ch. 4, describe \u201cextremizing\u201d this way:\n\u201cWhen you combine the judgments of a large group of people to calculate the \u201cwisdom of the crowd\u201d you collect all the relevant information that is dispersed among all those people. But none of those people has access to all that information. One person knows only some of it, another knows some more, and so on. What would happen if every one of those people were given all the information? They would become more confident \u2014 raising their forecasts closer to 100% or zero. If you then calculated the \u201cwisdom of the crowd\u201d it too would be more extreme. Of course it\u2019s impossible to give every person all the relevant information \u2014 so we extremize to simulate what would happen if we could.\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-16\" class=\"footnote-item\"><p><a href=\"https://books.google.com/books?id=XwjsCgAAQBAJ&amp;lpg=PA1&amp;pg=PA924#v=onepage&amp;q&amp;f=false\">Soll et al. (2016)</a> summarize some of this literature briefly:\n\u201cWhen judgments are provided by many people, an extremely effective way to combine them is to weight them equally, such as by taking the simple average or applying majority rule (e.g., Clemen, 1989; Hastie &amp; Kameda, 2005). The idea of harnessing the \u201cwisdom of crowds\u201d has been applied to a wide variety of contexts, ranging from sports prediction markets to national security (Surowiecki, 2004). For quantity estimates, averaging provides benefits over the average individual whenever individual guesses bracket the truth (i.e., some guesses on both sides), so that high and low errors will cancel out (Larrick, Mannes, &amp; Soll, 2012).\u201d\n<a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Tetlock &amp; Gardner (2015)</a> also report several \u201cwisdom of crowds\u201d effects in Tetlock et al.\u2019s forecasting tournaments:\n\u201cTeams of ordinary forecasters beat the wisdom of the crowd by about 10%. Prediction markets beat ordinary teams by about 20%. And superteams beat prediction markets by 15% to 30%.\nI can already hear the protests from my colleagues in finance that the only reason the superteams beat the prediction markets was that our markets lacked liquidity: real money wasn\u2019t at stake and we didn\u2019t have a critical mass of traders. They may be right. It is a testable idea, and one worth testing. It\u2019s also important to recognize that while superteams beat prediction markets, prediction markets did a pretty good job of forecasting complex global events.\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-17\" class=\"footnote-item\"><p>I refer to Tetlock\u2019s forecasting tournaments, both those reported in <a href=\"http://press.princeton.edu/titles/7959.html\">Tetlock (2005)</a> and especially those reported in <a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Tetlock &amp; Gardner (2015)</a>. <a href=\"#fnref-nnx4n4K5YiWppKxsE-17\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-18\" class=\"footnote-item\"><p>There is relatively little (compelling) literature on this hypothesis, and I haven\u2019t evaluated that literature carefully, but my understanding is that what literature exists tends to support the hypothesis, including Tetlock\u2019s work, which I find unusually convincing (due to the strength of the study designs).\nNon-Tetlock work on this is reviewed (for example) in the section on previous literature in <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/for.2274/full\">Legerstee &amp; Franses (2014)</a>, where the authors write:\n\u201c[One] kind of feedback is task properties feedback, which is sometimes also called environmental feedback. It involves providing the forecaster with statistical information on the variable to be forecast. It can encompass data characteristics or statistical model forecasts. Note that it might be argued that this is not genuine feedback as it is provided before the judgmental forecast is given and is not feedback on the performance of the judgmental forecaster (see Bj\u00f6rkman, 1972). This task properties feedback has received most attention in research on feedback on judgmental forecasting (see Sanders, 1992; Remus et al., 1996; Welch et al., 1998; Goodwin and Fildes, 1999). In all cases it is found to improve forecast accuracy and in general it is found to be the most effective form of feedback (Lawrence et al., 2006).\nIntuitively, it seems plausible that \u201ctask properties feedback\u201d \u2014 which in other words is simply information about the phenomenon to be forecast, before the forecast is made \u2014 should especially improve the <em>resolution</em> of one\u2019s forecasts, while feedback on one\u2019s forecasting <em>performance</em> should improve one\u2019s <em>calibration</em>. This hypothesis is (weakly) supported by e.g. <a href=\"http://www.sciencedirect.com/science/article/pii/S0749597800929108\">Stone &amp; Opel (2000)</a>:\nBenson and Onkal (1992) suggest that environmental feedback [i.e. task properties feedback], unlike performance feedback, should be effective for improving people\u2019s discrimination skill [i.e. resolution], since environmental information provides information about the event to be judged. Only a small amount of work, however, has examined the impact of environmental feedback isolated from other types of feedback on judgmental accuracy. Lichtenstein and Fischhoff (1977, Experiment 2) trained participants to discriminate between European and American handwriting by providing them with samples of each type of handwriting. This handwriting training served as a type of environmental feedback, as it provided the participants with task information. As predicted, those participants who underwent the training procedure achieved higher discrimination scores than did those who received no such training.\nIf calibration and discrimination are psychologically distinct concepts, then providing domain-specific information (environmental feedback) should have no impact on calibration. In fact, Lichtenstein and Fischhoff did find an improvement in calibration scores resulting from the training in their study. However, they concluded that this improvement did not reflect a true improvement in calibration skill, but instead resulted from the hard\u2013easy effect (cf. Lichtenstein et al., 1982; Suantak, Bolger, &amp; Ferrell, 1996), whereby difficult questions (those answered correctly 50\u201370% of the time) produce overconfidence, easy questions (those answered correctly 80\u2013100% of the time) produce underconfidence, and those of moderate difficulty (those answered correctly 70\u201380% of the time) produce the best calibration. Since improvements in discrimination reflect gains in substantive knowledge on a topic, it would be expected that gains in discrimination would be accompanied by an increased number of questions answered correctly. Indeed, those participants who underwent the handwriting training answered 71% correctly while those who did not undergo the training answered only 51% correctly. Thus, on the basis of the increase in percentage of items answered correctly alone, the improvement in calibration could be attributed to the hard\u2013easy effect rather than to a true improvement in calibration skill.\n\u2026\nThe previous review suggests that, within the domains studied, performance feedback improves calibration and that environmental feedback improves discrimination. There is also reason to believe that performance feedback does not affect discrimination and that environmental feedback does not affect calibration; however, these conclusions are more equivocal, in that past findings have been open to multiple interpretations. The primary goal of the present study, then, was to demonstrate this dissociation\u2026\n\u2026\nThe results [of the new experiment reported in this study] strongly supported this hypothesis. Additionally, we found two unexpected effects: (1) the impact of feedback was greater for hard slides than for easy slides, and (2) environmental feedback led to increased overconfidence for easy slides.\u201d\nIn the large forecasting tournaments described in <a href=\"http://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/\">Tetlock &amp; Gardner (2015)</a>, there were many important correlates of forecasting accuracy, and several of them were related to domain knowledge: \u201cpolitical knowledge,\u201d \u201caverage number of articles shared,\u201d \u201caverage number of articles checked,\u201d and others \u2014 see Table 3 of <a href=\"http://pps.sagepub.com/content/10/3/267.short\">Mellers et al. (2015)</a>. But this, too, is relatively weak evidence, as domain knowledge was not manipulated experimentally. <a href=\"#fnref-nnx4n4K5YiWppKxsE-18\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-19\" class=\"footnote-item\"><p>Weather forecasters are commonly cited as a group that exhibits good accuracy (e.g. see <a href=\"http://www.penguin.com/book/the-signal-and-the-noise-by-nate-silver/9781594204111\">Silver 2012</a>, ch. 4), but they do not provide an example of accurate judgment in the <em>absence</em> of reasonably strong models and plentiful data.\nOn bridge players, see <a href=\"http://www.sciencedirect.com/science/article/pii/0749597887900471\">Keren (1987)</a>. On TV pundits and political scientists, see ch. 2 of <a href=\"http://www.penguin.com/book/the-signal-and-the-noise-by-nate-silver/9781594204111\">Silver (2012)</a>. For reviews of relevant judgmental forecasting literature, see e.g. <a href=\"http://www.sciencedirect.com/science/article/pii/S0749597800929108\">Stone &amp; Opel (2000)</a> and <a href=\"http://www.sciencedirect.com/science/article/pii/S0169207006000501\">Lawrence et al. (2006)</a>.\nAs for professional futurists: I\u2019m currently investigating the track record of long-range forecasting, much of which has been performed by professional \u201c<a href=\"https://en.wikipedia.org/wiki/Futurist\">futurists</a>.\u201d I might change my mind later, but so far my impression is that the accuracy of long-range (\u226510 year) forecasts by the most-respected, best-resourced professional futurists of the 50s-90s has not been very good. This shouldn\u2019t be a surprise: as far as I can tell, professional futurists of this period rarely if ever engaged in probability calibration training, and forecasting the long-term future is no doubt more difficult than forecasting short-term outcomes. (Of course it\u2019s possible that contemporary futurists are more accurate than those from the 50s-90s, but we\u2019ll have to wait for time to pass before we can evaluate the accuracy of their long-range forecasts.)\nThus far, I\u2019ve published only one finding from my ongoing investigation of the track record of long-range forecasting, concerning <a href=\"https://www.openphilanthropy.org/evaluation-some-technology-forecasts-year-2000\">some technology forecasts from a book called The Year 2000</a>. <a href=\"#fnref-nnx4n4K5YiWppKxsE-19\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-nnx4n4K5YiWppKxsE-20\" class=\"footnote-item\"><p>Here is an incomplete list of data points that informed my impression:</p>\n<p>At least \u201cseveral\u201d companies have invested in explicit probability calibration training, e.g. Royal Dutch Shell and Doug Hubbard\u2019s clients.\nThe jacket cover of 1989\u2019s <em><a href=\"https://books.google.com/books?id=5IhFAAAAYAAJ\">Decision Traps</a></em>, which includes calibration training as one of its key recommendations (pp. 96-102), claims that the authors \u201chave improved the decision-making skills of thousands of Fortune 500 executives with the program described in this book. Their clients come from such companies as General Motors, Royal Dutch/Shell, IBM, [and others].\u201d A 2001 book by the same authors (<em><a href=\"http://www.penguinrandomhouse.com/books/159138/winning-decisions-by-jedward-russo-and-paul-j-h-schoemaker/\">Winning Decisions</a></em>) says, in the acknowledgments, that \u201cJohn Oakes teamed up with us in the mid-1990s to design a management training program based on our book <em>Decision Traps</em>.\u201d\nAs late as March 2014, and shortly before the publication of the major papers describing IARPA\u2019s forecasting tournaments, <a href=\"http://cradpdf.drdc-rddc.gc.ca/PDFS/unc142/p538628_A1b.pdf\">Mandel et al. (2014)</a> claimed that \u201cthe work described later in this report is\u2026 to the best of our knowledge, the first systematic, long-term evaluation of the quality of analytic forecasts extracted from real intelligence reports, using [a proper scoring rule].\u201d\nI asked Robin Hanson, creator of the <a href=\"https://en.wikipedia.org/wiki/Prediction_market#History\">first corporate prediction market</a> and a leading advocate for the use of prediction markets, for his impression about how commonly firms make use of prediction markets. His emailed reply was that \u201cThe fraction using prediction markets is quite tiny, surely far smaller than the use of statistics etc. I\u2019d be surprised if there are 100 firms using [prediction markets] at any one time.\u201d\n<a href=\"http://journal.sjdm.org/16/16511/jdm16511.html\">Chang et al. (2016)</a> reports that \u201cfew organizations have embraced the debiasing methods that have been developed (Croskerry, 2003; Graber et al., 2012; Lilienfeld et al., 2009).\u201d <a href=\"#fnref-nnx4n4K5YiWppKxsE-20\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "lukeprog"}}, {"_id": "jx58vi5FcpCKr6xz8", "title": "Three Key Issues I\u2019ve Changed My Mind About", "postedAt": "2016-09-06T13:10:36.207Z", "htmlBody": "<p>Philanthropy - especially <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">hits-based</a> philanthropy - is driven by a large number of judgment calls. At the Open Philanthropy Project, we\u2019ve explicitly designed our process to put major weight on the views of individual leaders and program officers in decisions about the strategies we pursue, causes we prioritize, and grants we ultimately make. As such, we think it\u2019s helpful for individual staff members to discuss major ways in which our personal thinking has changed, not only about particular causes and grants, but also about our background <a href=\"http://blog.givewell.org/2013/04/04/deep-value-judgments-and-worldview-characteristics/\">worldviews</a>.</p><p>I recently wrote up a relatively detailed discussion of how my personal thinking has changed about three interrelated topics: (1) the importance of <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from advanced artificial intelligence</a>, particularly the <a href=\"https://www.openphilanthropy.org/research/cause-reports/ai-risk#Loss_of_control_of_advanced_agents\">value alignment problem</a>; (2) the potential of many of the ideas and people associated with the <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> community; (3) the properties to look for when assessing an idea or intervention, and in particular how much weight to put on metrics and \u201cfeedback loops\u201d compared to other properties. My views on these subjects have changed fairly dramatically over the past several years, contributing to a significant shift in how we approach them as an organization.</p><p>I\u2019ve posted my full writeup as a <a href=\"https://docs.google.com/document/d/1hKZNRSLm7zubKZmfA7vsXvkIofprQLGUoW43CYXPRrk/\">personal Google doc</a>. A summary follows.</p><h1>Changing my mind about potential risks from advanced artificial intelligence</h1><p>I first encountered the idea of <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from advanced artificial intelligence</a> - and in particular, the <a href=\"https://www.openphilanthropy.org/research/cause-reports/ai-risk#Loss_of_control_of_advanced_agents\">value alignment problem</a> - in 2007. There were aspects of this idea I found intriguing, and aspects I felt didn\u2019t make sense. The most important question, in my mind, was \u201cWhy are there no (or few) people with relevant-seeming expertise who seem concerned about the value alignment problem?\u201d</p><p>I initially guessed that relevant experts had strong reasons for being unconcerned, and were simply not bothering to engage with people who argued for the importance of the risks in question. I believed that the <a href=\"http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/#Arguments\">tool-agent distinction</a> was a strong candidate for such a reason. But as I got to know the AI and machine learning communities better, saw how <a href=\"https://smile.amazon.com/dp/B00LOOCGB2/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1\">Superintelligence</a> was received, heard reports from the <a href=\"http://futureoflife.org/2015/10/12/ai-safety-conference-in-puerto-rico/\">Future of Life Institute\u2019s safety conference in Puerto Rico</a>, and updated on a variety of other fronts, I changed my view.</p><p>I now believe that there simply is no mainstream academic or other field (as of today) that can be considered to be \u201cthe locus of relevant expertise\u201d regarding potential risks from advanced AI. These risks involve a combination of technical and social considerations that don\u2019t pertain directly to any recognizable near-term problems in the world, and aren\u2019t naturally relevant to any particular branch of computer science. This is a major update for me: I\u2019ve been very surprised that an issue so potentially <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Importance\">important</a> has, to date, commanded so little attention - and that the attention it has received has been significantly (though not exclusively) due to people in the effective altruism community.</p><p><a href=\"https://docs.google.com/document/d/1hKZNRSLm7zubKZmfA7vsXvkIofprQLGUoW43CYXPRrk/edit#heading=h.709ahkk1jg0e\"><i>More detail on this topic</i></a></p><h1>Changing my mind about the effective altruism (EA) community</h1><p>Note: This section focuses on the parts of the effective altruist community that I did <i>not</i> initially encounter as people donating to, or spreading the word about, GiveWell and its top charities.</p><p>I\u2019ve had a longstanding interest in the <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> community. I identify as part of this community, and I share some core values with it (in particular, the goal of doing as much good as possible). However, for a long time, I placed very limited weight on the views of a particular subset of the people I encountered through this community. (This was largely because they seemed to have a tendency toward reaching very unusual conclusions based on seemingly simple logic unaccompanied by deep investigation. I had the impression that they tended to be far more willing than I was to \u201caccept extraordinary claims without extraordinary evidence\u201d in some sense, a topic I\u2019ve written about several times (<a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">here</a>, <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">here</a> and <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">here</a>).</p><p>A number of things have changed.</p><ul><li>Potential risks from advanced AI, discussed above, is one topic I\u2019ve changed my mind about: I previously saw this as a strange preoccupation of the EA community, and now see it as a major case where the community was early to highlight an important issue.</li><li>More generally, I\u2019ve seen the outputs from a good amount of <a href=\"http://www.openphilanthropy.org/research/our-process\">cause selection work</a> at the Open Philanthropy Project. I now believe that the preponderance of the causes that I\u2019ve seen the most excitement about in the effective altruism community are outstanding by our <a href=\"https://www.openphilanthropy.org/focus\">criteria</a> of importance, neglectedness and tractability. These causes include <a href=\"http://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare\">farm animal welfare</a> and <a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">biosecurity and pandemic preparedness</a> in addition to <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from advanced artificial intelligence</a>. They aren\u2019t the only outstanding causes we\u2019ve identified, but overall, I\u2019ve increased my estimate of how well excitement from the effective altruism community predicts what I will find promising after more investigation.</li><li>I\u2019ve seen EA-focused organizations make progress on galvanizing interest in effective altruism and growing the community. I\u2019ve seen some effects of this directly, including more attention, donors, and strong employee candidates for GiveWell and the Open Philanthropy Project.</li><li>I\u2019ve gotten to know some community members better generally, and my views on some general topics (below) have changed in ways that have somewhat reduced my skepticism of the kinds of ideas effective altruists pursue.</li></ul><p>I now feel the EA community contains the closest thing the Open Philanthropy Project has to a natural \u201cpeer group\u201d - a set of people who consistently share our basic goal (doing as much good as possible), and therefore have the potential to help with that goal in a wide variety of ways, including both collaboration and critique. I also value other sorts of collaboration and critique, including from people who question the entire premise of doing as much good as possible, and can bring insights and abilities that we lack. But people who share our basic premises have a unique sort of usefulness as both collaborators and critics, and I\u2019ve come to feel that the effective altruism community is the most logical place to find such people.</p><p>This isn\u2019t to say I support the effective altruism community unreservedly; I have <a href=\"http://effective-altruism.com/ea/pf/why_the_open_philanthropy_project_isnt_currently/5ie\">concerns and objections</a> regarding many ideas associated with it and some of the specific people and organizations within it. But I\u2019ve become more positive compared to my early impressions.</p><p><a href=\"https://docs.google.com/document/d/1hKZNRSLm7zubKZmfA7vsXvkIofprQLGUoW43CYXPRrk/edit#heading=h.bbmitqz3r4zo\"><i>More detail on this topic</i></a></p><h1>Changing my mind about general properties of promising ideas and interventions</h1><p>Of the topics discussed here, this one is the hardest to trace the evolution of my thinking on, and the hardest to summarize.</p><p>I used to think one should be pessimistic about any intervention or idea that doesn\u2019t involve helpful \u201cfeedback loops\u201d (trying something, seeing how it goes, making small adjustments, and trying again many times) or useful selective processes (where many people try different ideas and interventions, and the ones that are successful in some tangible way become more prominent, powerful, and imitated). I was highly skeptical of attempts to make predictions and improve the world based primarily on logic and reflection, when unaccompanied by strong feedback loops and selective processes.</p><p>I still think these things (feedback loops, selective processes) are very powerful and desirable; that we should be more careful about interventions that don\u2019t involve them; that there is a strong case for preferring charities (such as GiveWell\u2019s top charities) that are relatively stronger in terms of these properties; and that much of the effective altruism community, including the people I\u2019ve been most impressed by, continues to underweight these considerations. However, I have moderated significantly in my view. I now see a reasonable degree of hope for having strong positive impact while lacking these things, particularly when using logical, empirical, and scientific reasoning.</p><p>Learning about the <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">history of philanthropy</a> - and learning more about history more broadly - has been a major factor in changing my mind. I\u2019ve come across many cases where a philanthropist, or someone else, seems to have had remarkable prescience and/or impact primarily through reasoning and reflection. Even accounting for <a href=\"https://en.wikipedia.org/wiki/Survivorship_bias\">survivorship bias</a>, my impression is that these cases are frequent and <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">major</a> enough that it is worth trying to emulate this sort of impact. This change in viewpoint has both influenced and been influenced by the two topics discussed above.</p><p><a href=\"https://docs.google.com/document/d/1hKZNRSLm7zubKZmfA7vsXvkIofprQLGUoW43CYXPRrk/edit#heading=h.fzecg7qn3yie\"><i>More detail on this topic</i></a></p><h2>Conclusion</h2><p>Over the last several years, I have become more positive on the cause of <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from advanced AI</a>, on the effective altruism community, and on the general prospects for changing the world through relatively speculative, long-term projects grounded largely in intellectual reasoning (sometimes including reasoning that leads to \u201cwacky\u201d ideas) rather than direct feedback mechanisms. These changes in my thinking have been driven by a number of factors, including by each other.</p><p>One cross-cutting theme is that I\u2019ve become more interested in arguments with the general profile of \u201csimple, logical argument with no clear flaws; has surprising and unusual implications; produces reflexive dissent and discomfort in many people.\u201d I previously was very suspicious of arguments like this, and expected them not to hold up on investigation. However, I now think that arguments of this form are generally worth paying serious attention to until and unless flaws are uncovered, because they often represent positive innovations.</p><p>The changes discussed here have caused me to shift from being a skeptic of supporting work on potential risks from advanced AI and effective altruism organizations to being an advocate, which in turn has been a major factor in the Open Philanthropy Project\u2019s taking on work in these areas. As discussed at the top of this post, I believe that sort of relationship between personal views and institutional priorities is appropriate given the work we\u2019re doing.</p><p>I\u2019m not certain that I\u2019ve been correct to change my mind in the ways described here, and I still have a good deal of sympathy for people whose current views are closer to my former ones. But hopefully I have given a sense of where the changes have come from.</p><p>More detail is available here:</p><p><a href=\"https://docs.google.com/document/d/1hKZNRSLm7zubKZmfA7vsXvkIofprQLGUoW43CYXPRrk/\">Some Key Ways in Which I\u2019ve Changed My Mind Over the Last Several Years</a><br>&nbsp;</p>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "zmBnKnpECErWsCiYq", "title": "Some Background on Open Philanthropy's Views Regarding Advanced Artificial Intelligence", "postedAt": "2016-05-16T13:08:03.353Z", "htmlBody": "<p>We\u2019re <a href=\"https://www.openphilanthropy.org/blog/our-progress-2015-and-plans-2016\">planning</a> to make <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from advanced artificial intelligence</a> a major priority in 2016. A future post will discuss why; this post gives some background.</p>\n<p>Summary:</p>\n<ul>\n<li>I first give our <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1\">definition of \u201ctransformative artificial intelligence,\u201d</a> our term for a type of potential advanced artificial intelligence we find particularly relevant for our purposes. Roughly and conceptually, transformative AI refers to potential future AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution. I also provide (below) a more detailed definition. The concept of \u201ctransformative AI\u201d has some overlap with concepts put forth by others, such as \u201csuperintelligence\u201d and \u201cartificial general intelligence.\u201d However, \u201ctransformative AI\u201d is intended to be a more inclusive term, leaving open the possibility of AI systems that count as \u201ctransformative\u201d despite lacking many abilities humans have.</li>\n<li>I then discuss the question of whether, and when, we might expect transformative AI to be developed. This question has many properties (long timelines, relatively vague concepts, lack of detailed public analysis) I associate with developments that are nearly impossible to forecast, and I don\u2019t think it is possible to make high-certainty forecasts on the matter. With that said, I am comfortable saying that I think there is a <em>nontrivial likelihood</em> (at least 10% with moderate robustness, and at least 1% with high robustness) of transformative AI within the next 20 years. I can\u2019t feasibly share all of the information that goes into this view, but I try to outline the general process I have followed to reach it.</li>\n<li>Finally, I briefly discuss whether there are other potential future developments that seem to have similar potential for impact on similar timescales to transformative AI, in order to put our interest in AI in context.</li>\n</ul>\n<p>The ideas in this post overlap with some arguments made by others, but I think it is important to lay out the specific views on these issues that I endorse. Note that this post is confined in scope to the above topics; it does not, for example, discuss potential risks associated with AI or potential measures for reducing them. I will discuss the latter topics more in the future.</p>\n<h2>Defining \u201ctransformative artificial intelligence\u201d (transformative AI)</h2>\n<p>There are many ways to classify potential advanced AI systems. For our purposes, we prefer to focus in on the particular classifications that are most relevant to AI\u2019s potential impact on the world, while putting aside many debates that don\u2019t relate to this (for example, whether and when an AI system might have human-like consciousness and emotions). \u201cTransformative AI\u201d is our term for a particular classification we find important. In this section, after some basic background, I will give two definitions of the term as we\u2019re using it: one is a relatively simple, rough sketch of the concept we\u2019re trying to capture, and the other is a more specific (though still far from precise) definition meant to help give a more detailed picture of what I would and wouldn\u2019t include in this classification.</p>\n<p>One of the main things we seek to assess about any given cause is its <a href=\"https://www.openphilanthropy.org/research/our-process#Exploring_potential_focus_areas\">importance</a>: how many people are affected, and how deeply? All else equal, we\u2019re more interested in artificial intelligence developments that would affect more people and more deeply. And consistent with our <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">philosophy of hits-based giving</a>, we think it\u2019s productive for any given cause to ask: \u201cWhat\u2019s the highest imaginable impact here? What are the most extreme scenarios, importance-wise, even if they\u2019re unlikely?\u201d</p>\n<p>When <a href=\"https://docs.google.com/document/d/1DTl4TYaTPMAtwQTju9PZmxKhZTCh6nmi-Vh8cnSgYak/edit\">asking these sorts of questions for US policy</a>, we\u2019ve discussed potential policy changes whose impact could be equivalent to hundreds of billions of dollars per year in economic value. These are high-impact, low-probability scenarios. In some contexts, however, I think it is appropriate to think about changes on an even larger scale. When asking just how significant a development could be, I think it\u2019s worth starting with the question: \u201cWhat were the most significant developments in <em>history,</em> and could a development in cause X compare?\u201d I think the answer will usually be \u201cno\u201d (I feel this way about most issues we work on), but when it is \u201cyes,\u201d it would be a mistake not to consider such a scenario.</p>\n<p>When thinking of the most significant developments in history, my thinking jumps to the <a href=\"https://en.wikipedia.org/wiki/Neolithic_Revolution\">agricultural (neolithic) revolution</a> and the <a href=\"https://en.wikipedia.org/wiki/Industrial_Revolution\">industrial revolution</a>, both of which I believe brought about fundamental, permanent changes in the nature of civilization.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-1\" id=\"fnref-zbHkDovJeJs3jmz9j-1\">[1]</a></sup> I believe that there is a serious possibility that progress in artificial intelligence could precipitate a transition comparable to (or more significant than) these two developments. One way in which this could happen would be if future AI systems became very broad in their abilities, to the point of being able to outperform humans in a wide array of jobs and thus fundamentally (and possibly quickly) transforming the economy. Another way would be if future AI systems proved capable of making major contributions to science and/or engineering, in one field or many, and thus caused a dramatic, possibly unexpected acceleration in the development of some transformative technology. (A <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec3\">later section of this post</a> lists some potential transformative technologies that I believe are possible in principle, though this is based only on my own impressions; I expect others to have different lists of the most potentially transformative technologies, but I expect fairly wide agreement on the basic point that <em>some</em> advance(s) in science and/or engineering could be transformative.)</p>\n<p>More broadly, I note that the agricultural and industrial revolutions both seem to have been driven largely by the discovery and proliferation of new technologies and ideas, developed through applied human intelligence. And with two such epochal events in the last ~10,000 years (one within the past ~300), I think it would be mistaken to dismiss such a dramatic transition as unprecedented or impossible.</p>\n<p>With this in mind, we define transformative AI as follows:</p>\n<p><em>Definition #1:</em> Roughly and conceptually, <strong>transformative AI is AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.</strong></p>\n<p><em>Definition #2:</em> Since definition #1 leaves a great deal of room for judgment, I provide a more detailed definition that I feel would likely (though not certainly) satisfy the first. Under this more detailed definition, transformative AI is anything that fits one or more of the following descriptions:</p>\n<ul>\n<li>AI systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems <em>could</em> accomplish such a thing unaided by humans doesn\u2019t mean they <em>would</em>; it\u2019s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction from the types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.</li>\n<li>AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.</li>\n<li>Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)</li>\n</ul>\n<p>Definition #2 is far from precise, and still leaves plenty of room for individual judgment. And neither of the two definitions strictly implies the other. That said, my view is that anything meeting definition #2 would quite likely meet definition #1, and definition #2 provides more clarity regarding what sorts of developments would and would not seem sufficiently different from today\u2019s technology to qualify as transformative AI from our perspective. Definition #2 is also intended to help a person imagine how they might judge whether (by their own judgment) transformative AI as I envision it has been developed at a given point in the future. The remainder of this post will be proceeding from definition #2 as it discusses comparisons and predictions where more detail is helpful.</p>\n<p>Note that these definitions of transformative AI are agnostic to many possible comparisons between AI systems and human minds. For example, they leave open the possibility of AI systems that count as \u201ctransformative\u201d despite not having human-like consciousness or emotions. They also leave open the possibility of AI systems that count as \u201ctransformative\u201d despite lacking many abilities humans have - it is necessary only that such systems have <em>sufficient</em> ability to bring about major changes in the world.</p>\n<h3>Relationship to some other AI-related terms</h3>\n<p>It\u2019s worth addressing the relationship of the \u201ctransformative AI\u201d concept to some other terms for potential high-impact advanced AI:</p>\n<ul>\n<li>\n<p>In <em><a href=\"http://smile.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/\">Superintelligence</a></em>, Nick Bostrom defines a superintelligence as \u201cany intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest.\u201d<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-2\" id=\"fnref-zbHkDovJeJs3jmz9j-2\">[2]</a></sup></p>\n</li>\n<li>\n<p><a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\">Artificial general intelligence (AGI)</a> is currently defined, according to Wikipedia, as \u201cthe intelligence of a (hypothetical) machine that could successfully perform any intellectual task that a human being can.\u201d</p>\n</li>\n<li>\n<p><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines#What_are_we_trying_to_forecast\">High-level machine intelligence</a> refers to an AI system \u201cthat can carry out most human professions at least as well as a typical human.\u201d</p>\n</li>\n</ul>\n<p>We intend \u201ctransformative AI\u201d to be, for the most part, a less restrictive term than any of these. Anything fitting one of the above three descriptions would likely meet at least the second condition of the <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#TransformativeAIDefinition\">detailed definition of transformative AI</a>, and it would therefore likely (though not definitely) meet the broader, more conceptual definition we gave.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-3\" id=\"fnref-zbHkDovJeJs3jmz9j-3\">[3]</a></sup> But it is possible to imagine transformative AI that does <em>not</em> qualify as superintelligence, AGI or high-level machine intelligence, because its impact comes from a relatively small number of domains in which AI systems achieve better-than-human performance. To give a couple of illustrative examples:</p>\n<ul>\n<li>Future AI systems might prove capable of analyzing scientific literature, generating new hypotheses, and designing experiments to test these hypotheses, resulting in a speedup of scientific progress comparable to what happened in the <a href=\"https://en.wikipedia.org/wiki/Scientific_revolution\">Scientific Revolution</a> and/or a speedup of technological progress comparable to what happened in the Industrial Revolution.</li>\n<li>Future AI systems might prove capable of accelerating progress on particular, highly important areas of science, even if they are limited in other areas.</li>\n<li>Future AI systems might bring about a dramatic leap in surveillance capabilities, e.g. by reducing the labor necessary to interpret large amounts of data. (Whether this would count as \u201ctransformative\u201d in our sense would depend on the details of how it played out.)</li>\n</ul>\n<p>It\u2019s important to us to include these sorts of possibilities in \u201ctransformative AI,\u201d for two reasons. First, the potential benefits and risks could be different from those posed by e.g. superintelligence, while still being highly worthy of our consideration. Second, below I discuss my views on when we might expect transformative AI to be developed, and it\u2019s important to my views that there are a large number of possible paths to transformative AI - not all of which require replicating all (or even most) of the functions of human brains.</p>\n<h2>When should we expect transformative artificial intelligence?</h2>\n<p>So far, I have discussed the in-principle possibility of an extremely powerful technology that could bring about extremely important changes. But I haven\u2019t addressed the thornier, and very important, question of whether there is any way to anticipate <em>how soon</em> we might expect such a development.</p>\n<p>I think the only defensible position on this question is one of very high uncertainty. I\u2019ve seen no signs of data or arguments that should give us confidence about timelines to transformative AI.</p>\n<p>However, having thought hard about this question and put a fair amount of time into investigating it over the last year, one claim I <em>am</em> comfortable making is that <strong>I think there is a nontrivial likelihood of transformative AI within the next 20 years.</strong> Specifically, when I say \u201cnontrivial likelihood,\u201d I mean:</p>\n<ul>\n<li>I believe the probability to be at least 10%, and consider this view to be <em>moderately</em> <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking\">robust</a> and stable. I\u2019m fairly (not highly) confident that a maximally thorough investigation would result in an estimate of at least 10%.</li>\n<li>I am <em>highly</em> confident that a maximally thorough investigation would put the probability at at least 1%.</li>\n</ul>\n<p>This view is important to my stance on the importance of potential risks from advanced artificial intelligence. If I did not hold it, this cause would probably still be a <a href=\"https://www.openphilanthropy.org/focus\">focus area</a> of the Open Philanthropy Project, but holding this view is important to prioritizing the cause <a href=\"https://www.openphilanthropy.org/blog/open-philanthropy-project-progress-2015-and-plans-2016\">as highly as we\u2019re planning to</a>.</p>\n<p>I recognize that it is an extremely difficult claim to evaluate. And my current view is based on a large number of undocumented conversations, such that I don\u2019t think it is realistic to aim for being highly convincing on this point in this post. However, I will attempt to lay out the general process I\u2019ve followed to reach my current views.</p>\n<p>I also note that we are doing work on several fronts to further refine our thinking about likely timelines. These include working toward a broader discussion of relevant technical issues (discussed below) and continuing an ongoing survey of the literature on forecasting, particularly the work of Philip Tetlock (whom we have <a href=\"https://www.openphilanthropy.org/giving/grants/university-pennsylvania-philip-tetlock-forecasting\">funded</a>), as well as seeking to understand the performance of past long-term predictions about technology.</p>\n<h3>Expert surveys and trend extrapolations</h3>\n<p>When asking how likely transformative AI is to be developed in the next 20 years, I think a natural first approach is to ask: (a) what do relevant experts believe about the likelihood, and (b) can we learn anything from extrapolation of relevant trends?</p>\n<p>We\u2019ve done our best to examine the available information about these questions. Findings are summarized by Luke Muehlhauser <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines\">here</a>, and I discuss the takeaways below. Unfortunately, <strong>I believe that these lines of inquiry give relatively little to go on,</strong> and they do not represent the only (or even primary) inputs into my thinking. With that said, the information we <em>do</em> have along these lines reinforces the view that there is a <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Nontrivial\">nontrivial likelihood</a> of transformative AI within the next 20 years.</p>\n<p>I\u2019ve chosen to present this information first because I think many readers would instinctively expect it to be the most useful information, even though - with matters as they stand - I have ended up putting much more weight on the arguments presented in <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Another_approach_to_forecasting_transformative_AI\">later sections</a>.</p>\n<p><strong>Expert surveys</strong>: Some prominent AI researchers have made public statements seemingly implying that certain kinds of AI are a long way away. For instance, in the context of discussing potential risks, <a href=\"http://www.outerplaces.com/science/item/8268-a-stanford-artificial-intelligence-researcher-on-why-we-don-t-need-to-worry-about-evil-robots\">Andrew Ng</a> has said, \u201cI don\u2019t work on preventing AI from turning evil for the same reason that I don\u2019t work on combating overpopulation on the planet Mars\u2026 Maybe hundreds of years from now, maybe thousands of years from now\u2014I don\u2019t know\u2014maybe there will be some AI that turn evil, but that\u2019s just so far away that I don\u2019t know how to productively work on that.\u201d As mentioned elsewhere in this post, I think it\u2019s plausible that he is right to think that certain forms of advanced AI are hundreds or thousands of years away. However, I haven\u2019t been able to identify systematic arguments for this view, and (as discussed below) I believe there are many researchers with relevant expertise who disagree. When assessing expert opinion, I am inclined to attempt to rely on surveys rather than on a small number of brief public statements.</p>\n<p>Most attempts to survey relevant experts have had major methodological issues. For reasons laid out in <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines#Expert_elicitation\">Luke\u2019s writeup</a>, I believe the most useful available survey is the \u201cTOP100\u201d survey from <a href=\"http://sophia.de/pdf/2014_PT-AI_polls.pdf\">M\u00fcller and Bostrom 2014</a>, which asked a number of researchers for the year by which they estimated a 10%, 50% and 90% chance of high-level machine intelligence (HLMI, defined in the previous section). Taking either the mean or median of responses implies a 10% probability within 20 years of today (median 2024, mean 2034); also note that the survey (again based on the mean and median of responses) implies a <em>90%</em> chance of high-level machine intelligence well within the next 200 years. And as discussed above, I feel that \u201chigh-level machine intelligence\u201d is mostly a more restrictive concept than \u201ctransformative AI.\u201d</p>\n<p>However, I have major reservations about all the surveys that have been done, including the one just cited. Most importantly, <strong>I believe that the people surveyed are in many, if not all, cases giving essentially off-the-cuff responses, with little or no attempt to make detailed models of key factors or break the question into smaller pieces that can then be investigated.</strong> I think that these practices are generally accepted as important for difficult forecasting challenges,<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-4\" id=\"fnref-zbHkDovJeJs3jmz9j-4\">[4]</a></sup> and my personal experience supports this; for example, I\u2019ve found the <a href=\"http://www.givewell.org/international/technical/criteria/cost-effectiveness/cost-effectiveness-models\">practice of doing cost-effectiveness analysis</a> to be important in raising crucial considerations for <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell\u2019s top charities</a>.</p>\n<p>With this point noted, I have several further reservations about the forecasts made in surveys, some of which derive from this high-level point. Most echo <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines#Expert_elicitation\">Luke\u2019s</a>. A few seem particularly worth highlighting:</p>\n<ul>\n<li>\n<p>I am concerned that the people surveyed may be biased toward shorter timelines, because the fact that they engage in these questions may indicate that they\u2019re unusually enthusiastic about the relevant technologies.</p>\n</li>\n<li>\n<p>I fear that those surveyed are not accounting for growth in the relevant fields. The conversations I\u2019ve had with machine learning researchers, as well as with <a href=\"https://www.openphilanthropy.org/about/team/daniel-dewey\">Daniel Dewey</a>, have led me to believe that - partly due to excitement over relatively recent results (discussed below) - there is a fairly rapid influx of researchers into AI- and machine-learning-related work. For example, attendance of the Conference on Neural Information Processing Systems (NIPS) <a href=\"http://thoughtsonir.github.io/machinelearning/A-reader-digest-to-NIPS/\">appears to have been growing rapidly over the past decade</a>. I note that multiple major, heavily funded AI labs have been started since 2010: <a href=\"https://en.wikipedia.org/wiki/Google_DeepMind\">DeepMind (acquired by Google for $500 million)</a>, <a href=\"https://en.wikipedia.org/wiki/Google_Brain\">Google Brain</a>, <a href=\"https://research.facebook.com/ai\">Facebook AI Research</a> (<a href=\"https://www.facebook.com/yann.lecun/posts/10151728212367143\">announced in 2013</a>), <a href=\"http://research.baidu.com/silicon-valley-ai-lab/\">Baidu Silicon Valley AI Lab</a>, <a href=\"https://en.wikipedia.org/wiki/Vicarious_(company)\">Vicarious</a> (which has <a href=\"https://www.crunchbase.com/organization/vicarious-systems-inc#/entity\">reportedly raised $72 million</a>),<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-5\" id=\"fnref-zbHkDovJeJs3jmz9j-5\">[5]</a></sup> and <a href=\"https://openai.com/blog/introducing-openai/\">OpenAI</a> (whose funders have committed $1 billion).<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-6\" id=\"fnref-zbHkDovJeJs3jmz9j-6\">[6]</a></sup> <a href=\"http://lukemuehlhauser.com/how-much-recent-investment-in-ai/\">One prominent AI researcher has stated</a>, \u201cIndustry [has probably invested] more in the last 5 years than governments have invested since the beginning of the field [in the 1950s].\u201d Someone giving an off-the-cuff projection of progress in AI research might be extrapolating from past progress, without accounting for the far greater interest and funding in the field now.</p>\n</li>\n<li>\n<p>I think it\u2019s quite possible that the path to transformative AI will involve many technical challenges, and that different challenges will be best addressed by different fields and different intellectual traditions. As long as our main source of information is off-the-cuff estimates rather than detailed discussions, I fear there could be distortions introduced by people\u2019s intuitions about fields they are relatively unfamiliar with. These distortions could cause a bias toward shorter timelines, if people are over-optimistic about fields they aren\u2019t familiar with or are extrapolating their own field\u2019s progress to that of other fields that could prove both necessary and substantially slower than their own. There could also be a bias toward longer timelines, if people are overlooking the fact that many of the problems that look difficult to them could prove more tractable to other approaches. (In particular, if a small number of approaches look like they may be highly general and could prove sufficient to develop transformative AI, a survey <em>average</em> will miss this dynamic by counting estimates from people working on these approaches the same way it counts estimates from everyone else.)</p>\n</li>\n</ul>\n<p>Some of the above considerations would imply that surveys underestimate how far we are from developing transformative AI; some would imply that they overestimate it. I think the issues in both directions are significant and seriously undermine the idea of relying on this data.</p>\n<p><strong>Trend extrapolation</strong>: In general, I believe it is often useful to look for relevant trends in quantifiable data when making predictions.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-7\" id=\"fnref-zbHkDovJeJs3jmz9j-7\">[7]</a></sup> Unfortunately, I believe there is little to go on in this category. The most relevant-seeming trend-extrapolation-based work seems to be the various attempts to answer the question, \u201cWhen will affordable [by various definitions] computers be capable of matching the processing power of a human brain?\u201d <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines#Trend_extrapolation\">Luke\u2019s review of this work</a> implies (in my view) that this capability may already have been reached, and in any case has a reasonable chance of being reached in the next 20 years, while noting a variety of reasons that it has very limited relevance for forecasting overall AI capabilities.</p>\n<h3>Another approach to forecasting transformative AI</h3>\n<p>This section will discuss a separate case I see for expecting a nontrivial probability of transformative AI in the next 20 years. This case is based on reasoning through - with a small set of technical advisors - the details of relatively recent progress in AI, and attempting to inventory the most crucial technical challenges that will need to be addressed in order to develop transformative AI.</p>\n<p>The technical advisors I have spoken with the most on this topic are close friends I\u2019ve met through GiveWell and effective altruism: <a href=\"https://www.linkedin.com/in/dario-amodei-3934934\">Dario Amodei</a>, <a href=\"http://colah.github.io/about.html\">Chris Olah</a> and <a href=\"http://cs.stanford.edu/~jsteinhardt/\">Jacob Steinhardt</a>. They are all relatively junior (as opposed to late-career) researchers; they do not constitute a representative sample of researchers; there are therefore risks in leaning too heavily on their thinking. With that said, talking to them has brought the advantage of being able to conduct - and listen in on - a large number of very detailed discussions, and I consider all three to be clearly on the cutting edge of various aspects of AI and machine learning research.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-8\" id=\"fnref-zbHkDovJeJs3jmz9j-8\">[8]</a></sup> It\u2019s possible that there are a large number of other researchers having similar discussions - even that similar discussions have informed the survey responses discussed above - but the only discussions along these lines I have access to are the ones these technical advisors have been having. I feel fortunate to have good enough relationships with relevant researchers to have access to these sorts of discussions, and as I\u2019ve <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">written previously</a>, I don\u2019t think it would be advisable to discard our observations simply because they are friends and/or not a representative set. As discussed below, we have made some attempts to supplement their thinking with outside perspectives, and hope to do more on this front. (We have also discussed our high-level conclusions with a significant number of AI and ML researchers. Conversations were in confidence and often time-constrained, but we saw few signs that our take is clearly unreasonable.)</p>\n<p>The rest of this section will discuss:</p>\n<ul>\n<li>The basic question of whether AI research is likely to proceed via a very large number of highly specialized insights, or whether there may turn out to be a few broadly applicable AI approaches that lead to rapid progress on an extremely wide variety of intellectual tasks. At this point, this seems to be very much an open question.</li>\n<li>Recent progress in AI and ML (particularly deep learning), which has provided some suggestive evidence for the idea of broadly applicable breakthroughs.</li>\n<li>Work that the technical advisors mentioned above have been doing to summarize \u201ccore open problems\u201d in AI - types of intellectual reasoning that seem important, but that researchers have not yet had success in reproducing - and potential research paths that could imaginably lead to progress on these problems. Based on this work, it is easy to imagine (though far from certain) that headway on a relatively small number of core problems could prove broadly applicable, and could lead to AI systems equalling or surpassing human performance in a very large number of domains. Discussions of this possibility, and the subjective estimates of the technical advisors involved, are important to my view that there is a <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Nontrivial\">nontrivial</a> probability of transformative AI in the next 20 years.</li>\n</ul>\n<h4>How diverse are necessary AI advances?</h4>\n<p><em>In what follows, I will distinguish between (a) \u201cintellectual functions\u201d - human intellectual activities as we would generally describe them, such as \u201cstudying physics\u201d or \u201cwriting a memo\u201d; and (b) \u201cunderlying algorithms\u201d - the specific mechanistic manipulations that are performed on raw data</em><sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-9\" id=\"fnref-zbHkDovJeJs3jmz9j-9\">[9]</a></sup>_ to perform intellectual functions._<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-10\" id=\"fnref-zbHkDovJeJs3jmz9j-10\">[10]</a></sup></p>\n<p>Many of the people I\u2019ve spoken with seem to instinctively assume that AI research is likely to proceed via a very large number of highly specialized insights, and that there will be few or no breakthroughs that lead to rapid progress on many fronts at once. It seems to me that this view is usually correlated with (though not the same as) an intuition that the human brain has an extraordinarily complex and varied architecture, and that each of the many intellectual functions humans perform requires fundamentally different underlying algorithms. For example, in order to build a computer system that can conduct scientific research at the level of human experts, the algorithm and training procedure used for reading existing scientific literature might need to be fundamentally different from the algorithm and training procedure for identifying important scientific questions. Other relevant tasks like experimental design, manipulating objects, writing quality expositions, etc., might all require fundamentally different algorithmic approaches.</p>\n<p>Others have a different intuition. There may turn out to be a few broadly applicable AI approaches that lead to rapid progress on an extremely wide variety of intellectual tasks. This intuition seems correlated with (though again, not the same as) an intuition that the human brain makes repeated use of a relatively <em>small</em> set of underlying algorithms, and that by applying the processes, with small modifications, in a variety of contexts, it generates a wide variety of different predictive models, which can end up looking like very different intellectual functions.</p>\n<p>My impression is that the current state of both neuroscience<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-11\" id=\"fnref-zbHkDovJeJs3jmz9j-11\">[11]</a></sup> and AI research is highly compatible with both possibilities (and a range of things in between). With respect to AI research, I believe that much historical progress has come from relatively specialized approaches and has thus looked more like what would be expected under the first hypothesis - but that <em>recent</em> progress has provided more evidence of broadly applicable breakthroughs. The next section discusses this recent progress.</p>\n<h4>Recent progress in AI and ML</h4>\n<p>Certain areas of AI and machine learning, particularly related to deep neural networks and other deep learning methods, have recently experienced rapid and impressive progress. In some cases this progress has been surprisingly fast relative to what practitioners 10 years ago likely would have expected, has been strongly applicable to real-world problems, or both.</p>\n<p><a href=\"https://www.openphilanthropy.org/about/team/daniel-dewey\">Daniel Dewey</a> provides the following overview list. These examples of progress were chosen based on our technical advisors\u2019 impressions of how impressive, significant, and/or surprising they were to academic AI researchers. We excluded some examples that might seem significant to non-researchers, but that did not meet these criteria (for example, recent progress in self-driving cars and IBM Watson\u2019s <em>Jeopardy!</em> win).</p>\n<ul>\n<li>\n<p><strong>Computer vision</strong> has improved significantly over the last 5 years, and now matches or exceeds human performance in some tasks. For example, in the <a href=\"http://www.image-net.org/challenges/LSVRC/\">ImageNet Large Scale Visual Recognition Challenge</a>\u2019s image recognition task, the best team\u2019s top-5 error<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-12\" id=\"fnref-zbHkDovJeJs3jmz9j-12\">[12]</a></sup> dropped from 28.2% in 2010 (before the adoption of deep learning) to 3.6% in 2015, beating a trained human\u2019s error of 5.1%.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-13\" id=\"fnref-zbHkDovJeJs3jmz9j-13\">[13]</a></sup> In 2012, a breakthrough in training deep convolutional networks achieved a 9.4% improvement over the previous year (typical year-over-year improvements were closer to 3%).<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-14\" id=\"fnref-zbHkDovJeJs3jmz9j-14\">[14]</a></sup></p>\n</li>\n<li>\n<p><strong>Speech recognition</strong> has shown similar progress, again largely due to adoption of and advances in deep learning. Benchmarks are more varied in speech recognition, but a few illustrative cases are: the drop from ~24% transcription error for Gaussian mixture models in 2011 on the \u201cSwitchboard\u201d data set to ~16% error for deep neural network models that same year;<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-15\" id=\"fnref-zbHkDovJeJs3jmz9j-15\">[15]</a></sup> the decrease to ~8% error on the same task by 2015<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-16\" id=\"fnref-zbHkDovJeJs3jmz9j-16\">[16]</a></sup>; and most recently the development of a distributed deep learning system, Deep Speech 2, that outperformed trained humans by 1.5%-3% on 3 out of four transcription tasks from the WSJ and Librispeech data sets in late 2015.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-17\" id=\"fnref-zbHkDovJeJs3jmz9j-17\">[17]</a></sup></p>\n</li>\n<li>\n<p><strong>Go:</strong> In October 2015, the Go-playing system AlphaGo defeated a professional in 8 out of 10 games; in March 2016, an improved version of the system defeated top-tier professional <a href=\"https://en.wikipedia.org/wiki/Lee_Sedol\">Lee Sedol</a> in 4 out of 5 games. It is <a href=\"http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress\">debatable</a> whether this performance should have been considered \u201csurprising,\u201d or a major leap relative to previous capabilities, when accounting for the high level of investment and hardware involved. However, it was another case in which deep learning seems to have made a major contribution to surpassing performance of top humans in a particular domain.</p>\n</li>\n<li>\n<p><strong>Expanding applicability of deep learning:</strong> more generally, there has been a proliferation of work applying existing deep learning methods to an expanding set of tasks. For example, new ideas in deep Q-learning and increased R&amp;D and hardware investment resulted in 2015 in a deep-learning-based system that achieved human-like performance across many Atari games without specialized game-by-game tuning;<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-18\" id=\"fnref-zbHkDovJeJs3jmz9j-18\">[18]</a></sup> sequence-to-sequence learning, encoder-decoder networks, attention models, and multimodal embeddings have enabled progress in using deep learning to perform tasks like sentence-level image description,<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-19\" id=\"fnref-zbHkDovJeJs3jmz9j-19\">[19]</a></sup> phrase translation,<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-20\" id=\"fnref-zbHkDovJeJs3jmz9j-20\">[20]</a></sup> and image generation;<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-21\" id=\"fnref-zbHkDovJeJs3jmz9j-21\">[21]</a></sup> and neural turing machines, memory networks, and other architectures augmenting deep neural networks with external memory have been proposed as ways of applying deep learning to question-answering<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-22\" id=\"fnref-zbHkDovJeJs3jmz9j-22\">[22]</a></sup> and learning algorithms from examples.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-23\" id=\"fnref-zbHkDovJeJs3jmz9j-23\">[23]</a></sup> Unlike the previous three examples, this expansion of deep learning beyond traditional classification tasks has not often yielded human-comparable performance, but it does suggest that deep learning may be broadly applicable to many problems.\n<a href=\"https://en.wikipedia.org/wiki/Deep_learning\">Deep learning</a> is a general approach to fitting predictive models to data that can lead to automated generation of extremely complex non-linear models. It seems to be, conceptually, a relatively simple and cross-domain approach to generating such models (though it requires complex computations and generates complex models, and hardware improvements of past decades have been a key factor in being able to employ it effectively). My impression is that the field is still very far away from exploring all the ways in which deep learning might be applied to challenges in AI. In light of the excitement over recent progress (and increased investment, as <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Investment\">noted above</a>), there will be increasing attempts to do such exploration.</p>\n</li>\n</ul>\n<p>As an aside, deep learning \u2014 like most modern approaches to machine learning, which rely heavily on statistics and approximations \u2014 produces systems with strengths and weaknesses that don\u2019t fit some common popular stereotypes of AI systems. They are often strong on activities commonly associated with \u201cintuition\u201d (playing Go, recognizing images),<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-24\" id=\"fnref-zbHkDovJeJs3jmz9j-24\">[24]</a></sup> but my understanding is that symbolic and logical reasoning have proven difficult to deeply and satisfyingly integrate into such systems.</p>\n<p>In my view, there is a live possibility that with further exploration of the implications and applications of deep learning - and perhaps a small number (1-3) of future breakthroughs comparable in scope and generality to deep learning - researchers will end up being able to achieve better-than-human performance in a large number of intellectual domains, sufficient to produce transformative AI. As stated above, I don\u2019t believe existing knowledge about either AI or neuroscience can rule out this possibility; a key question is how plausible it looks following thorough discussion and reflection by people well-positioned to understand the strengths and weaknesses of deep learning and other established and emerging approaches to AI.</p>\n<h4>Core open problems</h4>\n<p>Over the past several months, the technical advisors mentioned above have been working on a document summarizing \u201ccore open problems\u201d in AI - types of intellectual reasoning that seem important, and that humans seem to be able to do, but that researchers have not yet had success in reproducing. They have also been discussing potential research paths that could imaginably lead to progress on these problems.</p>\n<p>In order to get wider input, we organized a meeting at our offices that included Dario, Jacob and Chris as well as three other early-career academic and industry researchers at leading institutions, and since then the six of them have been collaborating on refining the document summarizing core open problems.</p>\n<p>We haven\u2019t yet determined whether and when there will be public output from these discussions; it will ultimately be the choice of the people involved. My hope is that the researchers will make a document public and start a conversation in the wider community. For now, however, their views are the best available (to us) approximation of what kind of picture we might get from maximally informed people.</p>\n<p>I don\u2019t intend to go into the details of individuals\u2019 views. But broadly speaking, based on these conversations, it seems to me that:</p>\n<ul>\n<li>It is easy to imagine (though far from certain) that headway on a relatively small number of core problems could lead to AI systems equalling or surpassing human performance in a large number of domains.</li>\n<li>The total number of core open problems is not <em>clearly</em> particularly large (though it is highly possible that there are many core problems that the participants simply haven\u2019t thought of).</li>\n<li>Many of the identified core open problems may turn out to have overlapping solutions. Many may turn out to be solved by continued extension and improvement of deep learning methods.</li>\n<li>None appear that they will <em>clearly</em> require large numbers of major breakthroughs, large (decade-scale) amounts of trial and error, or further progress on directly studying the human brain. There are examples of outstanding technical problems, such as unsupervised learning, that could turn out to be very difficult, leading to a dramatic slowdown in progress in the near future, but it isn\u2019t clear that we should confidently expect such a slowdown. I note that this situation is in contrast to many challenges in life sciences - such as <a href=\"https://www.openphilanthropy.org/research/cause-reports/animal-product-alternatives#Cultured_ground_meat\">producing meat from stem cells</a> - where it seems that we can identify multiple challenging steps, some of which would involve clear significant lags due to time-consuming experiments and regulatory processes.</li>\n<li>An aggregated picture of the subjective views of the people who have been working on the \u201ccore open problems\u201d document would point to a 10% or greater probability of transformative AI in the next 20 years.</li>\n</ul>\n<p>In discussing this work with technical advisors, I\u2019ve sometimes informally shared my own intuitions about which sorts of intellectual functions seem most challenging, mysterious, or impressive, and therefore likely hard to replicate. I\u2019ve generally found - unsurprisingly - that my thoughts (and common assumptions I hear from others) on these matters are far behind that of the technical advisors. For example, <em>creativity</em> in problem solving (e.g. generating novel ideas) may seem mysterious and very \u201chuman\u201d at first blush, but my understanding is that when an AI system has a strong model of what problem it is trying to solve and how to evaluate potential solutions, coming up with ideas that can be called \u201ccreative\u201d does not remain a major challenge. (As an example, commentators have remarked on the <a href=\"http://kottke.org/16/03/our-creative-beautiful-unpredictable-machines\">creativity of AlphaGo\u2019s play</a>.) I say all of this because I believe a common reaction to speculation about AI is to point to particular human modes of thought that seem hard to replicate, and I believe it\u2019s worth noting that I think AI researchers carry out unusually sophisticated versions of this exercise.</p>\n<p>Ideally, we would continue investigation on this topic by involving leading researchers from a diverse set of AI- and machine-learning-related fields, facilitating truly extensive discussion, and perhaps eventually using a <a href=\"https://en.wikipedia.org/wiki/Delphi_method\">Delphi method</a> (or similar approach) to arrive at forecasts. But getting such a group to participate in such a time-consuming process might not be feasible at all, and might take years if it were. (We have also discussed our high-level conclusions with a significant number of AI and ML researchers. Conversations were in confidence and often time-constrained, but we saw few signs that our take is clearly unreasonable.)</p>\n<p>At the moment, I feel we have gotten as far as we will for some time, in terms of assembling people who can combine (a) very strong knowledge of the cutting edge of AI and machine learning research with (b) a willingness to engage in the process - time-consuming, intellectually demanding, and highly speculative such that it is unlikely to lead directly to career advancement - of laying out and analyzing core open problems and potential research paths; (c) making probability estimates that are informed both by this analysis and by a general familiarity with probability-based forecasts.</p>\n<h3>Some notes on past \u201cfalse alarms\u201d and the burden of argumentation</h3>\n<p>When discussing the topics in this post, I\u2019ve sometimes encountered the claim that we should heavily discount the analysis of today\u2019s researchers, in light of the history of \u201cfalse alarms\u201d in the past - cases where researchers made overconfident, overly aggressive forecasts about how AI would develop. Luke Muehlhauser has looked into the history of past AI forecasts, and written up takeaways at some length <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/what-should-we-learn-past-ai-forecasts\">here</a>.</p>\n<p>Based on this work, I think there has indeed been at least one past period during which researchers overestimated how quickly AI would improve, and I think there\u2019s a substantial chance that we would have bought into the over-aggressive forecasts at that time. However, I see little evidence of similar dynamics after that period. In fact, since the 1970s, it appears that researchers have been fairly circumspect in their forecasts. (And the forecasts prior to the mid-1970s may well have been rational, if ultimately inaccurate, forecasts.) Overall, I see good reason to expect researchers to be overenthusiastic about their field, and thus to discount their claims to some degree, but I don\u2019t think that the history of past forecasts gives us much additional reason, and I certainly don\u2019t think there have been enough \u201cfalse alarms\u201d to provide strong evidence against my <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Nontrivial\">view about timelines to transformative AI.</a></p>\n<p>Another argument against my view would be to claim that it should face a very high burden of argumentation, since transformative AI would be such an extreme development. I think this is true to some degree; however,</p>\n<ul>\n<li>Developments of this magnitude are not unprecedented. The events that I\u2019ve used as reference points for transformative AI - the <a href=\"https://en.wikipedia.org/wiki/Neolithic_Revolution\">agricultural (neolithic) revolution</a> and the <a href=\"https://en.wikipedia.org/wiki/Industrial_Revolution\">industrial revolution</a> - both occurred within the past 10,000 years, and the industrial revolution occurred within the past 300. As each increased the pace of innovation and growth, each arguably raised the background likelihood of further, comparable transitions.</li>\n<li>As noted <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Expert_surveys_and_trend_extrapolations\">above</a>, expert surveys seem to imply a 90%+ chance of transformative AI well within the next 200 years, and this seems consistent with other conversations I\u2019ve had: I\u2019ve encountered very few people who seem to think that transformative AI is highly unlikely within the next few centuries.</li>\n</ul>\n<p>With all of the above points in mind, I do think one should discount the views of researchers to some degree, and I do think one should consider transformative AI forecasts to face a reasonably high burden of argumentation. However, I don\u2019t think one should discount researchers\u2019 views to an extreme degree, or have an overwhelmingly strong prior against transformative AI in the medium term.</p>\n<h3>Bottom line</h3>\n<p>There\u2019s no solid basis for estimating the likelihood of transformative AI in the coming decades. Trying to do so may be an entirely futile exercise; certainly it has many properties (long timelines, relatively vague concepts, lack of detailed public analysis) I\u2019d associate with developments that are nearly impossible to forecast.</p>\n<p>That said:</p>\n<ul>\n<li>At this point we\u2019ve tried to examine the problem from every angle we can, and further improvement in our picture of the situation would be quite time-consuming to obtain. (Nonetheless, we do hope to pursue such improvement.)</li>\n<li>Machine learning (particularly deep learning) is a dynamic field: it has seen impressive progress and major growth in researchers and resources over the last few years. It isn\u2019t clear where, if anywhere, the limitations of deep learning lie, and the highest-quality discussion we\u2019ve been able to participate in on this topic has not led to identifying any clear limitations or obstacles. It is consistent with a real possibility that only a small number of major breakthroughs will be needed for AI systems to achieve human-level or better performance on large numbers of intellectual functions.</li>\n<li>All the information we\u2019ve collected - from surveys of experts, available analysis based on trend extrapolation, and from more detailed, analytical discussions with technical advisors - is consistent with this possibility. All of these categories of information have major flaws and limitations.</li>\n<li>Assigning <em>less than</em> 10% probability to \u201ctransformative AI within the next 20 years\u201d would not seem supported by any of these classifications of evidence, and would have no solid justification I can identify at this time. I don\u2019t expect to come across such a justification within the next 6 months, though with enough effort we might encounter one within the next year.</li>\n</ul>\n<p>There have been <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/what-should-we-learn-past-ai-forecasts\">vivid false projections of AI in the past</a>, and we\u2019re aware that today\u2019s projections of future AI could look misguided in retrospect. This is a risk I think we should accept, given our <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">philosophy of hits-based giving</a>. Overall, I think it is appropriate to act as though there is at least a 10% chance of \u201ctransformative AI\u201d within the next 20 years; as though this \u201c10%\u201d figure is <em>somewhat</em> <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">stable/robust</a>; and as though we can be quite confident that the probability is at least 1%.</p>\n<p>In the past, I have <a href=\"http://blog.givewell.org/2010/06/29/singularity-summit/#comment-155806\">argued against investing resources based on arguments that resemble \u201cPascal\u2019s Mugging\u201d</a> (and I\u2019ve done so specifically in the context of reducing potential risks from advanced artificial intelligence). If one has little information about a debate, and no idea how to assign a probability to a given proposition, I don\u2019t think it\u2019s appropriate to make arguments along the lines of \u201cWe should assign a probability above X simply because our brains aren\u2019t capable of confidently identifying probabilities lower than that.\u201d Put differently, \u201cI can\u2019t prove that the probability is very low\u201d is not sufficient to argue \u201cThe probability is reasonably high.\u201d But I think the arguments I\u2019ve given above present a markedly different situation:</p>\n<ul>\n<li>\n<p>We have put a great deal of effort into becoming informed about the relevant issues, and feel that we\u2019ve explored essentially all of the available angles.</p>\n</li>\n<li>\n<p>There are many outputs of our investigations that could have led - but did not, in fact, lead - me to assign a &lt;10% probability of transformative AI in the next 20 years. For example, if the number of <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Core_open_problems\">core open problems</a> identified by technical advisors appeared larger and more diverse, or if the technical advisors working on identifying these problems had given different subjective bottom lines about the odds of transformative AI in the next 20 years, I could assign a much lower probability. Secondarily, despite the reservations I\u2019ve expressed about <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Expert_surveys_and_trend_extrapolations\">expert surveys and trend extrapolations</a>, I would be thinking about this issue quite differently if the expert surveys pointed to a much longer \u201c10% probability\u201d timeline or if trend extrapolations made it seem clear that we are still several decades from having computers that can match available estimates of the raw processing power of a human brain.</p>\n</li>\n<li>\n<p>My best-guess probability for \u201ctransformative AI within the next 20 years\u201d (&gt;=10%) is within the range where I feel mentally capable of distinguishing between different probabilities. I would assign a 5% or lower probability to many statements that might, in the absence of our investigations, look about as likely as \u201ctransformative AI within the next 20 years.\u201d For example, I\u2019d assign a &lt;=5% probability to the proposition that <a href=\"https://www.openphilanthropy.org/research/cause-reports/animal-product-alternatives#Cultured_ground_meat\">meat derived from stem cells</a> will be competitive (in the sense of real-world demand) with traditional meat within the next 20 years, or that a broad-spectrum, extremely effective (comparable with <a href=\"https://en.wikipedia.org/wiki/Imatinib\">Gleevec</a>) cancer drug (as in pill or protein) will come to market within the next 20 years.<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-25\" id=\"fnref-zbHkDovJeJs3jmz9j-25\">[25]</a></sup></p>\n</li>\n<li>\n<p>I think it\u2019s very unlikely that my best-guess probability for \u201ctransformative AI within the next 20 years\u201d could fall below 10% without a great deal more investigation or new information. Specifically, I think it would take at least 6-12 months for this to occur.</p>\n</li>\n</ul>\n<p>I believe there are <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">important differences</a> between probabilities assigned out of sheer guesswork and avoidance of overconfidence, vs. probabillities assigned as the result of substantive investigation that it would be hard to improve on. At this point, I have a view about the likelihood of transformative AI that - while the thinking behind it has many limitations and sources of uncertainty - fits more in the second category.</p>\n<h3>How convincing should this be to a reader outside the Open Philanthropy Project?</h3>\n<p>As stated above, my view is based on a large number of undocumented conversations, such that I don\u2019t think it is realistic to aim for being highly convincing in this post. Instead, I have attempted to lay out the general structure of the inputs into my thinking.</p>\n<p>For further clarification, I will now briefly go over which parts of my argument I believe are well-supported and/or should be uncontroversial, vs. which parts rely crucially on information I haven\u2019t been able to fully share.</p>\n<ul>\n<li><strong>Definition and in-principle feasibility of transformative AI.</strong> I believe that my view that <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1\">transformative AI</a> is possible in principle would not be straightforward to vet because it would require access to either deep expertise or conversations with experts on a variety of subjects. However, I believe such conversations would lead relatively straightforwardly to the idea that transformative AI seems possible in principle.</li>\n<li><strong>Expert surveys and trend extrapolations.</strong> I believe that our <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Expert_surveys_and_trend_extrapolations\">take on available data from expert surveys and trend extrapolation</a> is on relatively solid ground. Our methodology hasn\u2019t been as systematic as it could have been, but I would be quite surprised if it turned out that there were relevant, high-value-added data in these categories that we haven\u2019t considered, or that our take on the strengths and weaknesses of such data were missing crucial considerations. I believe an interested reader could perform their own searches and analysis to verify this. However, I don\u2019t believe this analysis alone would justify the view I\u2019m arguing, for reasons given above.</li>\n<li><strong>Possibility that a small number of broadly applicable insights might be sufficient for transformative AI.</strong> I believe that the above <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#How_diverse_are_necessary_AI_advances\">discussion of whether AI research is likely to proceed by a large number of specialized insights or a smaller number of more general ones</a>, including my statement that \u201cthe current state of both neuroscience and AI research is highly compatible with both possibilities (and a range of things in between),\u201d is on similar ground to the ideas in the first bullet point. It would not be straightforward to verify because it would require access to either deep expertise or conversations with experts on a variety of subjects, but I believe my relatively agnostic take on this topic would be fairly uncontroversial among such experts.</li>\n<li><strong>Recent progress in deep learning.</strong> Daniel Dewey\u2019s <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#Recent_progress_in_AI_and_ML\">summary of recent progress</a> should give a general sense that there have been major advances recently in a fairly broad array of domains, though technical background or technical advisors would be necessary to verify the claims. I think it is uncontroversial that there have been major advances, though just <em>how</em> major and impressive is a question with a great deal of room for debate.</li>\n<li><strong>Core open problems in AI and their implications for likely timelines.</strong> I haven\u2019t shared the specifics of the core open problems we\u2019ve discussed, and even if I did, it would be very hard to get a sense for whether these core problems have been reasonably chosen, how likely various research paths are to lead to progress on these problems, and how significant progress on these problems would be. My views on this question rely on a variety of beliefs about the particular technical advisors we\u2019ve worked with to understand these issues: that these advisors are among the stronger researchers in their fields, that they reflect intelligently and reasonably on what intellectual functions seem challenging for AI (and for what reasons), that they have enough connections and engagement with the rest of the community to notice most directly relevant major insights from other researchers, that they accurately report which beliefs are commonly held in the field vs. held by some researchers vs. commonly rejected, etc. I have formed my read on these advisors through a large amount of interaction. I don\u2019t expect outside readers to come to the same views I have on the nature of core open problems in AI; the only way I can think of to do so would be to form their own high-trust relationships with technical advisors (or to become technical experts themselves).</li>\n</ul>\n<h2>What other possible future developments might deserve similar attention?</h2>\n<p>It generally seems to me that since philanthropy is well-suited to <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">long-term, low-probability-of-success work</a> that aims to benefit the world as a whole rather than a particular organization or interest, it is a good idea for philanthropists to ask \u201cWhat are the most dramatic worldwide changes that could be in the relatively near future?\u201d We\u2019ve put a fair amount of informal effort into doing so,<sup class=\"footnote-ref\"><a href=\"#fn-zbHkDovJeJs3jmz9j-26\" id=\"fnref-zbHkDovJeJs3jmz9j-26\">[26]</a></sup> and at this point we feel the possibility of transformative AI stands out.</p>\n<p>To give some more context on this view, I will first list a few potential developments that seem to me to be particularly strong candidates for being transformative in principle, then briefly discuss which I find most relevant in the relatively near (~20 years) future.</p>\n<p>Some possible developments:</p>\n<ul>\n<li>Progress in biology to the point of having dramatically better ability to understand, and modify, the functions of a human body and/or brain.</li>\n<li>Development of extremely cheap, clean, scalable energy sources.</li>\n<li>Development of radically improved manufacturing methods (<a href=\"https://www.openphilanthropy.org/research/cause-reports/atomically-precise-manufacturing\">atomically precise manufacturing</a> is one possibility that we have written about, though far from the only one in this category).</li>\n<li>Progress in social science to the point of being able to design institutions and interventions that dramatically change the way people think and live.</li>\n<li>A variety of <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks\">global catastrophic risks</a>, particularly <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">pandemics</a> and worst-case <a href=\"https://www.openphilanthropy.org/research/cause-reports/anthropogenic-climate-change\">climate change</a>.</li>\n<li>Dramatic shifts in geopolitical relations.</li>\n<li>Dramatic shifts in cultural values.</li>\n</ul>\n<p>There are many of these possibilities we haven\u2019t investigated to my satisfaction. However:</p>\n<ul>\n<li>I don\u2019t believe the bulk of these are - or would seem after further investigation - much <em>more</em> likely over the next 20 years than the development of transformative AI. (The major exception is <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity\">pandemics</a>, a focus area of ours; the last two points are also possible for certain definitions of \u201cdramatic.\u201d) The combination of (a) a dynamic field with striking recent progress; (b) a live hypothesis for how this progress might be extended, with a relatively small number of further breakthroughs, to transformative technology; (c) a lack of concrete, clearly relevant \u201cobstacles\u201d one can name for this hypothesis seems fairly unique to AI, relative to the other possibilities, at this time.</li>\n<li>I believe that unexpectedly strong progress on AI could lead to unexpectedly fast progress on almost any of the above developments, and possibly to unexpectedly fast progress on several of these fronts at once.</li>\n<li>I think there are additional reasons - largely related to <a href=\"https://www.openphilanthropy.org/focus\">neglectedness and tractability</a> - that AI is especially in need of philanthropic attention. I will discuss these in a future post.</li>\n</ul>\n<p>(1) A randomized controlled trial conducted by Barbara Mellers and colleagues as part of a recent large forecasting tournament, reported in <a href=\"https://www.openphilanthropy.org/blog/%E2%80%9Dhttp://psycnet.apa.org/journals/xap/21/1/1/%E2%80%9D\">Mellers et al. (2015)</a>, found that \u201ctraining in probabilistic reasoning,\u201d among other factors, improved forecasting success (see the paper for details). They describe the training this way: \u201cForecasters were taught to consider comparison classes and take the \u201coutside\u201d view. They were told to look for historical trends and update their beliefs by identifying and extrapolating persistent trends and accounting for the passage of time. They were told to average multiple estimates and use previously validated statistical models when available. When not available, forecasters were told to look for predictive variables from formal models that exploit past regularities. Finally, forecasters were warned against judgmental errors, such as wishful thinking, belief persistence, confirmation bias, and hindsight bias. This training module was informed by a large literature that investigates methods of debiasing\u2026\u201d It is impossible to know how much of the measured improvement in forecasting accuracy was due to training on how to use comparison classes and trend extrapolations in particular (apart from the effects of the rest of the training), but it seems plausible that the training on comparison classes and trend extrapolation had some effect. The experiment\u2019s training materials are provided in the supplemental materials for <a href=\"https://www.openphilanthropy.org/blog/%E2%80%9Dhttp://pss.sagepub.com/content/25/5/1106.short%E2%80%9D\">Mellers et al. (2014)</a>.\n(2) The most exhaustive retrospective analysis of historical technology forecasts we have yet found, <a href=\"https://www.openphilanthropy.org/blog/%E2%80%9Dhttp://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA568107%E2%80%9D\">Mullins (2012)</a>, categorized thousands of published technology forecasts by methodology, using eight categories including \u201cmultiple methods\u201d as one category. Of all these methodology categories, quantitative trend analyses had the highest success rate (see Table 3). The authors summarize this finding, and add some important caveats, on page 18: \u201cFor success rates, quantitative trend analysis\u2026 outperforms all other methodologies across all time frames except for models\u2026 in the medium term. Forecasts generated from quantitative trend analysis do have a significantly high percentage of predictions about computer technologies (the technology area with the highest statistically significant success rate), with 46% of quantitative trend analysis forecasts falling into this technology area tag, compared to 21% for expert analysis methods, the methodology with the second highest percentage of forecasts about computer technologies. However, when comparing success rates for methodologies solely within the computer technology area tag, quantitative trend analysis performs slight below average, indicating the predominance of forecasts about computer technologies is not influencing the success rates of associated forecast methodologies. When quantitative trend analysis was compared to all other methodologies while correcting for technology area tag and time frame, it did not demonstrate a statistically better success rate. This is due in part to the small sample size of quantitative forecasts that did not project over the short term or make predictions about computer technologies.\u201d\nWe have not yet evaluated either of these studies closely, but suspect the view that it is often useful to look for relevant trends in quantifiable data when making predictions would be uncontroversial among forecasting experts.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-zbHkDovJeJs3jmz9j-1\" class=\"footnote-item\"><p>I believe this view is relatively uncontroversial among people who have studied the matter, and would be supported by most literature on the topic. I may put out some content in the next couple of months that gives a sense of the literature I\u2019ve found most informative for this view. <a href=\"#fnref-zbHkDovJeJs3jmz9j-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-2\" class=\"footnote-item\"><p>Start of chapter 2. <a href=\"#fnref-zbHkDovJeJs3jmz9j-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-3\" class=\"footnote-item\"><p>The way in which a superintelligence, artificial general intelligence, or high-level machine intelligence might <em>not</em> meet my more detailed definition would be if it failed to be cost-competitive with employing humans. I think this is a fairly minor discrepancy in the scheme of things, since computing technology has generally tended to decrease fairly quickly in cost over time. <a href=\"#fnref-zbHkDovJeJs3jmz9j-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-4\" class=\"footnote-item\"><p>For example, the following is the 2nd of the \u201cTen Commandments for Aspiring Superforecasters\u201d in Philip Tetlock\u2019s <a href=\"http://smile.amazon.com/gp/product/B00RKO6MS8/\">Superforecasting</a>:</p>\n<p>(2) Break seemingly intractable problems into tractable sub-problems.</p>\n<p>Channel the playful but disciplined spirit of Enrico Fermi who\u2014when he wasn\u2019t designing the world\u2019s first atomic reactor\u2014loved ballparking answers to head-scratchers such as \u201cHow many extraterrestrial civilizations exist in the universe?\u201d Decompose the problem into its knowable and unknowable parts. Flush ignorance into the open. Expose and examine your assumptions. Dare to be wrong by making your best guesses. Better to discover errors quickly than to hide them behind vague verbiage.</p>\n<p>Superforecasters see Fermi-izing as part of the job. How else could they generate quantitative answers to seemingly impossible-to-quantify questions about Arafat\u2019s autopsy, bird-flu epidemics, oil prices, Boko Haram, the Battle of Aleppo, and bond-yield spreads.</p>\n<p>We find this Fermi-izing spirit at work even in the quest for love, the ultimate unquantifiable. Consider Peter Backus, a lonely guy in London, who guesstimated the number of potential female partners in his vicinity by starting with the population of London (approximately six million) and winnowing that number down by the proportion of women in the population (about 50%), by the proportion of singles (about 50%), by the proportion in the right age range (about 20%), by the proportion of university graduates (about 26%), by the proportion he finds attractive (only 5%), by the proportion likely to find him attractive (only 5%), and by the proportion likely to be compatible with him (about 10%). Conclusion: roughly twenty-six women in the pool, a daunting but not impossible search task.</p>\n<p>There are no objectively correct answers to true-love questions, but we can score the accuracy of the Fermi estimates that superforecasters generate in the IARPA tournament. The surprise is how often remarkably good probability estimates arise from a remarkably crude series of assumptions and guesstimates. <a href=\"#fnref-zbHkDovJeJs3jmz9j-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-5\" class=\"footnote-item\"><p>Note that <a href=\"http://www.goodventures.org/about-us/press/vicarious-announces-15m-series-a-funding-led-by-good-ventures\">Good Ventures is an investor.</a> <a href=\"#fnref-zbHkDovJeJs3jmz9j-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-6\" class=\"footnote-item\"><p>Additional recent investments in AI include <a href=\"http://www.nytimes.com/2015/11/06/technology/toyota-silicon-valley-artificial-intelligence-research-center.html\">Toyota\u2019s</a> $1 billion commitment to AI R&amp;D over the next 5 years, <a href=\"http://www.nature.com/news/south-korea-trumpets-860-million-ai-fund-after-alphago-shock-1.19595\">South Korea\u2019s</a> announced $863 million fund to support AI R&amp;D over the next 5 years, <a href=\"https://www.crunchbase.com/organization/rethink-robotics/funding-rounds\">Rethink Robotics\u2019</a> $100+ million raised since November 2010, <a href=\"https://www.crunchbase.com/organization/genetic-finance/funding-rounds\">Sentient Technologies\u2019</a> $130+ million raised since April 2010, and <a href=\"http://www.businessinsider.com/goldman-sachs-investing-in-artificial-intelligence-2016-1\">Goldman Sachs\u2019</a> planned investments in AI. <a href=\"http://www.analysisgroup.com/uploadedfiles/content/insights/publishing/ag_full_report_economic_impact_of_ai.pdf\">One report</a> (that we have not vetted) estimates 2015 venture capital investments in AI at $1.9 billion (see table 1). <a href=\"#fnref-zbHkDovJeJs3jmz9j-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-7\" class=\"footnote-item\"><p>Two data points on the utility of trend extrapolation as one input to successful forecasting, provided by Luke Muehlhauser: <a href=\"#fnref-zbHkDovJeJs3jmz9j-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-8\" class=\"footnote-item\"><p>Dario works at Google Brain, and previously <a href=\"http://arxiv.org/abs/1512.02595\">co-authored a paper at Baidu Silicon Valley AI lab</a> that <a href=\"https://www.technologyreview.com/s/600766/10-breakthrough-technologies-2016-conversational-interfaces/\">MIT Technology Review</a> listed as a top breakthrough of 2015. Chris works at Google Brain, and <a href=\"http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html\">co-authored a paper last year</a> that drew <a href=\"https://www.google.com/search?q=inceptionism&amp;oq=inceptionism&amp;aqs=chrome.0.69i59j0l5.1110j1j4&amp;sourceid=chrome&amp;ie=UTF-8\">significant popular attention</a> for introducing a way of visualizing the \u201cthought process\u201d of neural networks. Jacob is a PhD student at Stanford who has published 9 papers in prominent computer science conferences and is lead organizer of a workshop at <a href=\"http://www.icml.cc/\">ICML</a> this year. <a href=\"#fnref-zbHkDovJeJs3jmz9j-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-9\" class=\"footnote-item\"><p>By \u201craw data\u201d, I mean sensory inputs, e.g. light and sound. Most humans use eyes and ears to receive light and sound; computers can receive light and sound using cameras and microphones or (more commonly and simply) via files. <a href=\"#fnref-zbHkDovJeJs3jmz9j-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-10\" class=\"footnote-item\"><p>The terms \u201cintellectual functions\u201d and \u201cunderlying algorithms\u201d map fairly well to <a href=\"https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis\">David Marr\u2019s</a> \u201ccomputational level\u201d and \u201calgorithmic/representational level,\u201d respectively. I use different terms only in an attempt to be clear for readers not familiar with those terms.\nNote that underlying algorithms can be opaque to the person carrying them out. For example, I can\u2019t describe mechanistic rules that my brain uses to perform the intellectual functions of looking at a person and determining, from the pattern of light I perceive plus background knowledge, whether I know them. <a href=\"#fnref-zbHkDovJeJs3jmz9j-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-11\" class=\"footnote-item\"><p>See <a href=\"http://www.openphilanthropy.org/sites/default/files/Adam_Marblestone_4-13-2016_%28public%29.pdf\">notes from an email exchange with Adam Marblestone</a> for more on relevant neuroscience specifically. <a href=\"#fnref-zbHkDovJeJs3jmz9j-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-12\" class=\"footnote-item\"><p>\u201cTop-5 error\u201d is the percentage of images where the correct label did not appear in the system\u2019s top 5 predicted labels. It\u2019s worth noting that these classifiers generally do not perform well when presented with images that are very different from ImageNet photos (such as random noise), and so it would be inaccurate to say that they perform at human-like levels for general image recognition. <a href=\"#fnref-zbHkDovJeJs3jmz9j-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-13\" class=\"footnote-item\"><p>See <a href=\"http://arxiv.org/abs/1409.0575v3\">Russakovsky et al. 2015</a> pages 19-21 for 2010-2014 performance, <a href=\"http://image-net.org/challenges/LSVRC/2015/results\">this page</a> for 2015 results (task 2a, team MSRA). <a href=\"http://arxiv.org/abs/1512.03385v1\">He et al. 2015</a> is the paper underlying the latter. <a href=\"http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet\">This blog post</a> has a comparison to human performance. <a href=\"#fnref-zbHkDovJeJs3jmz9j-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-14\" class=\"footnote-item\"><p>See <a href=\"http://arxiv.org/abs/1409.0575v3\">Russakovsky et al. 2015</a> pages 19-21 for 2010-2014 performance. More on the 2012 breakthrough: <a href=\"http://arxiv.org/abs/1207.0580v1\">Hinton et al. 2012</a>, <a href=\"http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf\">Krizhevsky, Sutskever and Hinton 2012</a>, <a href=\"http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\">Srivastava et al. 2014</a>. <a href=\"#fnref-zbHkDovJeJs3jmz9j-14\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-15\" class=\"footnote-item\"><p><a href=\"http://research.microsoft.com/pubs/153169/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf\">Seide, Li and Yu 2011.</a> <a href=\"#fnref-zbHkDovJeJs3jmz9j-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-16\" class=\"footnote-item\"><p><a href=\"http://arxiv.org/abs/1505.05899v1\">Saon et al. 2015.</a> <a href=\"#fnref-zbHkDovJeJs3jmz9j-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-17\" class=\"footnote-item\"><p><a href=\"http://arxiv.org/abs/1512.02595v1\">Amodei et al. 2015</a> In the fourth task, trained humans outperformed the system by less than 1%. <a href=\"#fnref-zbHkDovJeJs3jmz9j-17\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-18\" class=\"footnote-item\"><p><a href=\"http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\">Mnih et al. 2015.</a> Note that a separate instance of the system was trained and evaluated on each game, instead of a single instance being trained and evaluated on every game; the latter would be an extremely impressive result. Also note that different games have different numbers of input actions, and that some games have a \u201clife counter\u201d displayed; DQN instances were modified to make use of that many actions for each game, and were given the number of lives remaining in games where a life counter was available, but according to the paper no other modifications were made between games. <a href=\"#fnref-zbHkDovJeJs3jmz9j-18\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-19\" class=\"footnote-item\"><p>E.g. <a href=\"http://cs.stanford.edu/people/karpathy/cvpr2015.pdf\">Karpathy and Li 2015</a>, <a href=\"http://arxiv.org/abs/1502.03044v2\">Xu et al. 2015</a>. <a href=\"#fnref-zbHkDovJeJs3jmz9j-19\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-20\" class=\"footnote-item\"><p>E.g. <a href=\"http://arxiv.org/abs/1406.1078v3\">Cho et al. 2014.</a> <a href=\"#fnref-zbHkDovJeJs3jmz9j-20\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-21\" class=\"footnote-item\"><p>E.g. <a href=\"http://arxiv.org/abs/1502.04623v2\">Gregor et al. 2015</a> <a href=\"#fnref-zbHkDovJeJs3jmz9j-21\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-22\" class=\"footnote-item\"><p>E.g. <a href=\"http://arxiv.org/abs/1506.02075v1\">Bordes et al. 2015.</a> <a href=\"#fnref-zbHkDovJeJs3jmz9j-22\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-23\" class=\"footnote-item\"><p>E.g. <a href=\"http://arxiv.org/abs/1506.03134v1\">Vinyals, Fortunato and Jaitly 2015</a>, <a href=\"http://arxiv.org/abs/1511.06392v3\">Kurach, Andrychowicz, and Sutskever 2016</a>. <a href=\"#fnref-zbHkDovJeJs3jmz9j-23\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-24\" class=\"footnote-item\"><p>E.g. Geoff Hinton has <a href=\"http://www.macleans.ca/society/science/the-meaning-of-alphago-the-ai-program-that-beat-a-go-champ/\">said</a>, \u201cThe really skilled players just sort of see where a good place to put a stone would be. They do a lot of reasoning as well, which they call reading, but they also have very good intuition about where a good place to go would be, and that\u2019s the kind of thing that people just thought computers couldn\u2019t do. But with these neural networks, computers can do that too. They can think about all the possible moves and think that one particular move seems a bit better than the others, just intuitively. That\u2019s what the feed point neural network is doing: it\u2019s giving the system intuitions about what might be a good move. It then goes off and tries all sorts of alternatives. The neural networks provides you with good intuitions, and that\u2019s what the other programs were lacking, and that\u2019s what people didn\u2019t really understand computers could do.\u201d <a href=\"#fnref-zbHkDovJeJs3jmz9j-24\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-25\" class=\"footnote-item\"><p>Excluding cases where advances in AI are a primary driver behind these developments\u2019 coming about. <a href=\"#fnref-zbHkDovJeJs3jmz9j-25\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-zbHkDovJeJs3jmz9j-26\" class=\"footnote-item\"><p>We\u2019ve had many informal conversations on these topics, including with scientific advisors. Below are some sources I\u2019ve read or skimmed and that others might find useful in getting a sense for the broader space of potential future developments:\n<em><a href=\"http://smile.amazon.com/Global-Catastrophes-Trends-Fifty-Years/dp/0262518228/\">Global Catastrophes and Trends: The Next 50 Years</a></em> by Vaclav Smil\n<em><a href=\"http://www.dni.gov/files/documents/GlobalTrends_2030.pdf\">Global Trends 2030: Alternative Worlds</a></em> by the U.S. National Intelligence Council\n<em><a href=\"http://smile.amazon.com/Anticipating-2025-radical-changes-whether-ebook/dp/B00L2EAUP8/\">Anticipating 2025</a></em> by David Wood, Mark Stevenson and others\n<em><a href=\"http://smile.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrophic Risks</a></em> edited by Nick Bostrom and Milan Cirkovic\nWikipedia\u2019s <a href=\"https://en.wikipedia.org/wiki/List_of_emerging_technologies\">list of emerging technologies</a>\n<a href=\"https://www.edge.org/contributors/what-do-you-consider-the-most-interesting-recent-scientific-news-what-makes-it\">Edge.org\u2019s 2016 question on interesting/important recent scientific news</a> (also see <a href=\"http://www.scottaaronson.com/blog/?p=2612\">Scott Aaronson\u2019s summary</a>)\nA <a href=\"https://www.quora.com/What-are-the-biggest-ways-in-which-the-world-20-years-from-now-will-probably-be-different-from-today-What-are-the-biggest-X-factors-changes-that-are-not-probable-but-are-possible-and-could-be-huge\">Quora contest we sponsored</a> around the question, \u201cWhat are the biggest ways in which the world 20 years from now will probably be different from today? What are the biggest \u201cX factors\u201d (changes that are not probable, but are possible and could be huge)?\u201d\nA variety of other sources that I haven\u2019t reviewed, but were skimmed by Luke Muehlhauser in his search for literature on this general topic: <em><a href=\"http://smile.amazon.com/Abundance-Future-Better-Than-Think/dp/145161683X/\">Abundance</a></em>, <em><a href=\"http://smile.amazon.com/Next-Fifty-Years-Science-Twenty-first/dp/0375713425/\">The Next Fifty Years</a></em>, <em><a href=\"http://smile.amazon.com/Future-Brain-Essays-Leading-Neuroscientists/dp/069116276X/\">The Future of the Brain</a></em>, <em><a href=\"http://www3.weforum.org/docs/WEF_GAC15_Technological_Tipping_Points_report_2015.pdf\">Deep Shift: Technology Tipping Points and Social Impact</a></em>, <em><a href=\"http://smile.amazon.com/2052-Global-Forecast-Forty-Years/dp/1603584218/\">2052: A Global Forecast for the Next Forty Years</a></em> <em><a href=\"http://smile.amazon.com/Next-100-Years-Forecast-Century/dp/0767923057\">The Next 100 Years</a></em>, and the timeline at <a href=\"http://FutureTimeline.net\">FutureTimeline.net</a>, starting with <a href=\"http://www.futuretimeline.net/21stcentury/2010-2019.htm\">the present decade</a>. <a href=\"#fnref-zbHkDovJeJs3jmz9j-26\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "HoldenKarnofsky"}}, {"_id": "T7e42LXSDCkoF33Mt", "title": "Potential Risks from Advanced Artificial Intelligence: The Philanthropic Opportunity", "postedAt": "2016-05-06T12:55:19.421Z", "htmlBody": "<p>We\u2019re <a href=\"https://www.openphilanthropy.org/blog/our-progress-2015-and-plans-2016\">planning</a> to make <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence\">potential risks from artificial intelligence</a> a major priority this year. We feel this cause presents an outstanding philanthropic opportunity \u2014 with extremely high importance, high neglectedness, and reasonable tractability (<a href=\"https://www.openphilanthropy.org/focus\">our three criteria for causes</a>) \u2014 for someone in our position. We believe that the faster we can get fully up to speed on key issues and explore the opportunities we currently see, the faster we can lay the groundwork for informed, effective giving both this year and in the future.</p>\n<p>With all of this in mind, we\u2019re placing a larger \u201cbet\u201d on this cause, this year, than we are placing even on other <a href=\"https://www.openphilanthropy.org/focus\">focus areas</a> \u2014 not necessarily in terms of funding (we aren\u2019t sure we\u2019ll identify very large funding opportunities this year, and are more focused on laying the groundwork for future years), but in terms of senior staff time, which at this point is a scarcer resource for us. Consistent with our <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">philosophy of hits-based giving</a>, we are doing this not because we have confidence in how the future will play out and how we can impact it, but because we see a risk worth taking. In about a year, we\u2019ll formally review our progress and reconsider how senior staff time is allocated.</p>\n<p>This post will first discuss why I consider this cause to be an outstanding philanthropic opportunity. (My views are fairly representative, but not perfectly representative, of those of other staff working on this cause.) It will then give a broad outline of our <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Sec5\">planned activities</a> for the coming year, some of the <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Sec6\">key principles</a> we hope to follow in this work, and some of the <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Sec8\">risks and reservations</a> we have about prioritizing this cause as highly as we are.</p>\n<p>In brief:</p>\n<ul>\n<li>It seems to me that artificial intelligence is currently on a very short list of the most dynamic, unpredictable, and potentially world-changing areas of science. I believe there\u2019s a nontrivial probability that <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Transformative\">transformative AI</a> will be developed within the next 20 years, with enormous global consequences.</li>\n<li>By and large, I expect the consequences of this progress \u2014 whether or not transformative AI is developed soon \u2014 to be positive. However, I also perceive risks. Transformative AI could be a very powerful technology, with potentially <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks#Our_basic_framework\">globally catastrophic</a> consequences if it is misused or if there is a major accident involving it. Because of this, I see this cause as having extremely high importance (one of our <a href=\"https://www.openphilanthropy.org/focus\">key criteria</a>), even while accounting for substantial uncertainty about the likelihood of developing transformative AI in the coming decades and about the size of the risks. I discuss the nature of potential risks below; note that I think they do not apply to today\u2019s AI systems.</li>\n<li>I consider this cause to be highly neglected in important respects. There is a substantial and growing field around artificial intelligence and machine learning research, but most of it is not focused on reducing potential risks. We\u2019ve put substantial work into trying to ensure that we have a thorough landscape of the researchers, funders, and key institutions whose work is relevant to potential risks from advanced AI. We believe that the amount of work being done is well short of what it productively could be (despite recent media attention); that philanthropy could be helpful; and that the activities we\u2019re considering wouldn\u2019t be redundant with those of other funders.</li>\n<li>I believe that there is useful work to be done today in order to mitigate future potential risks. In particular, (a) I think there are important <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#TechnicalChallenges\">technical problems that can be worked on today</a>, that could prove relevant to reducing accident risks; (b) I preliminarily feel that there is also considerable scope for analysis of potential strategic and policy considerations.</li>\n<li>More broadly, the Open Philanthropy Project may be able to help support an increase in the number of people \u2013 particularly people with strong relevant technical backgrounds - thinking through how to reduce potential risks, which could be important in the future even if the work done in the short term does not prove essential. I believe that one of the things philanthropy is best-positioned to do is provide steady, long-term support as fields and institutions grow.</li>\n<li>I consider this a challenging cause. I think it would be easy to do harm while trying to do good. For example, trying to raise the profile of potential risks could contribute (and, I believe, has contributed to some degree) to non-nuanced or inaccurate portrayals of risk in the media, which in turn could raise the risks of premature and/or counterproductive regulation. I consider the Open Philanthropy Project relatively well-positioned to work in this cause while being attentive to pitfalls, and to deeply integrate people with strong technical expertise into our work.</li>\n<li>I see much room for debate in the decision to prioritize this cause as highly as we are. However, I think it is <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">important that a philanthropist in our position be willing to take major risks</a>, and prioritizing this cause is a risk that I see as very worth taking.</li>\n</ul>\n<p>My views on this cause have evolved considerably over time. I will discuss the evolution of my thinking in detail in a future post, but this post focuses on the case for prioritizing this cause today.</p>\n<h2>Importance</h2>\n<p>It seems to me that AI and machine learning research is currently on a very short list of the most dynamic, unpredictable, and potentially world-changing areas of science.<sup class=\"footnote-ref\"><a href=\"#fn-sGWbhzSRPX5GrR9Ct-1\" id=\"fnref-sGWbhzSRPX5GrR9Ct-1\">[1]</a></sup> In particular, I believe that this research may lead eventually to the development of <strong>transformative AI</strong>, which we have roughly and conceptually defined as AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution. I believe there is a nontrivial likelihood (at least 10% with moderate robustness, and at least 1% with high robustness) that transformative AI will be developed within the next 20 years. For more detail on the concept of transformative AI (including a more detailed definition), and why I believe it may be developed in the next 20 years, see our <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence\">previous post</a>.</p>\n<p>I believe that today\u2019s AI systems are accomplishing a significant amount of good, and by and large, I expect the consequences of further progress on AI \u2014 whether or not transformative AI is developed soon \u2014 to be positive. Improvements in AI have enormous potential to improve the speed and accuracy of medical diagnosis; reduce traffic accidents by making autonomous vehicles more viable; help people communicate with better search and translation; facilitate personalized education; speed up science that can improve health and save lives; accelerate development of sustainable energy sources; and contribute on a huge number of other fronts to improving global welfare and productivity. As I\u2019ve <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">written before</a>, I believe that economic and technological development have historically been highly beneficial, often despite the fact that particular developments were subject to substantial pessimism before they played out. I also expect that if and when transformative AI is very close to development, many people will be intensely aware of both the potential benefits and risks, and will work to maximize the benefits and minimize the risks.</p>\n<p>With that said, I think the <a href=\"https://www.openphilanthropy.org/research/cause-reports/ai-risk\">risks</a> are real and important:</p>\n<ul>\n<li><strong>Misuse risks.</strong> One of the main ways in which AI could be transformative is by <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#Sec1\">enabling/accelerating the development of one or more enormously powerful technologies</a>. In the wrong hands, this could make for an enormously powerful tool of authoritarians, terrorists, or other power-seeking individuals or institutions. I think the potential damage in such a scenario is nearly limitless (if transformative AI causes enough acceleration of a powerful enough technology), and could include long-lasting or even permanent effects on the world as a whole. We refer to this class of risk as \u201cmisuse risks.\u201d I do not think we should let misuse scenarios dominate our thinking about the potential consequences of AI, any more than for any other powerful technology, but I do think it is worth asking whether there is anything we can do today to lay the groundwork for avoiding misuse risks in the future.</li>\n<li><strong>Accident risks.</strong> I also believe that there is a substantial class of potential \u201caccident risks\u201d that could rise (like misuse risks) to the level of <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks#Our_basic_framework\">global catastrophic risks</a>. In the course of many conversations with people in the field, we\u2019ve seen substantial (though far from universal) concern that such risks could arise and no clear arguments for being confident that they will be easy to address. These risks are difficult to summarize; we\u2019ve described them in more detail <a href=\"https://www.openphilanthropy.org/research/cause-reports/ai-risk#Loss_of_control_of_advanced_agents\">previously</a>, and I will give only a basic outline here. <br>\nAs goal-directed AI systems (such as reinforcement learning systems) become more capable, they will likely pursue the goals (e.g. as implied by a loss function) assigned to them in increasingly effective, unexpected, and hard-to-understand ways. Among these unexpected behaviors, there could be harmful behaviors, arising from (a) mismatches between the goals that programmers conceptually intend and the goals programmers technically, formally specify; (b) failures of AI systems to detect and respond to major context changes (I understand context change to be an area that many currently-highly-capable AI systems perform poorly at); (c) other technical problems. (See <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#RewardChannel\">below</a> for a slightly more detailed description of one possible failure mode.) It may be difficult to catch undesirable behaviors when an AI system is operating, in part because undesirable behaviors may be hard to distinguish from clever and desirable behaviors. It may, furthermore, be difficult and time-consuming to implement measures for confidently preventing undesirable behaviors, since they might emerge only in particular complex real-world situations (which raise the odds of major context changes and the risks of unexpected strategies for technically achieving specified goals) rather than in testing. If institutions end up \u201cracing\u201d to deploy powerful AI systems, this could create a significant risk of not taking sufficient precautions. <br>\nThe result could be a highly intelligent, autonomous, unchecked system or set of systems optimizing for a problematic goal, which could put powerful technologies to problematic purposes and could cause significant harm. I think the idea of a <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks#Our_basic_framework\">globally catastrophic</a> accident from AI only makes sense for certain kinds of AI - not for all things I would count as transformative AI. My rough impression at this time is that this sort of risk does not have a high overall likelihood (when taking into account that I expect people to take measures to prevent it), though it may have a high <em>enough</em> likelihood to be a very important consideration given the potential stakes. In conversations on this topic, I\u2019ve perceived very large differences of opinion on the size of this risk, and could imagine changing my view on the matter significantly in the next year or so.</li>\n<li><strong>Other risks.</strong> Some risks could stem from changes that come about due to widespread use of AI systems, rather than from a particular accident or misuse. In particular, AI advances could dramatically transform the economy by leading to the automation of many tasks - including driving and various forms of manufacturing - currently done professionally by many people. The effects of such a transformation seem hard to predict and could be highly positive, but there are risks that it could greatly exacerbate inequality and harm well-being by worsening employment options for many people. We are tentatively less likely to focus on this type of risk than the above two types, since we expect this type of risk to be (a) relatively likely to develop gradually, with opportunities to respond as it develops; (b) less extreme in terms of potential damage, and in particular less likely to be a <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks#Our_basic_framework\">global catastrophic risk</a> as we\u2019ve defined it, than misuse or accidents; (c) somewhat less neglected than the other risks. But this could easily change depending on what we learn and what opportunities we come across.</li>\n</ul>\n<p>The above risks could be amplified if AI capabilities improved relatively rapidly and unexpectedly, making it harder for society to anticipate, prepare for, and adapt to risks. This dynamic could (though won\u2019t necessarily) be an issue if it turns out that a <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#Another_approach_to_forecasting_transformative_AI\">relatively small number of conceptual breakthroughs turn out to have very general applications</a>.</p>\n<p>If the above reasoning is right (and I believe much of it is highly debatable, particularly when it comes to <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence\">my previous post\u2019s arguments</a> as well as the importance of accident risks), I believe it implies that this cause is not just important but something of an <em>outlier</em> in terms of importance, given that we are <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">operating in an expected-value framework and are interested in low-probability, high-potential-impact scenarios</a>.<sup class=\"footnote-ref\"><a href=\"#fn-sGWbhzSRPX5GrR9Ct-2\" id=\"fnref-sGWbhzSRPX5GrR9Ct-2\">[2]</a></sup> The underlying stakes would be qualitatively higher than those of any issues we\u2019ve explored or taken on under the <a href=\"https://www.openphilanthropy.org/focus/us-policy\">U.S. policy category</a>, to a degree that I think more than compensates for e.g. a \u201c10% chance that this is relevant in the next 20 years\u201d discount. When considering <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#Sec3\">other possible transformative developments</a>, I can\u2019t think of anything else that seems equally likely to be comparably transformative on a similar time frame, while also presenting such a significant potential difference between best- and worst-case imaginable outcomes.</p>\n<p>One reason that I\u2019ve focused on a 20-year time frame is that I think this kind of window should, in a sense, be considered \u201curgent\u201d from a philanthropist\u2019s perspective. I see philanthropy as being well-suited to <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">low-probability, long-term investments.</a> I believe there are many past cases in which it took a very long time for philanthropy to pay off,<sup class=\"footnote-ref\"><a href=\"#fn-sGWbhzSRPX5GrR9Ct-3\" id=\"fnref-sGWbhzSRPX5GrR9Ct-3\">[3]</a></sup> especially when its main value-added was supporting the gradual growth of organizations, fields and research that would eventually make a difference. If I thought there were negligible probability of transformative AI in the next 20 years, I would still consider this cause important enough to be a <a href=\"https://www.openphilanthropy.org/focus\">focus area</a> for us, but we would not be prioritizing it as highly as <a href=\"https://www.openphilanthropy.org/blog/our-progress-2015-and-plans-2016\">we plan to this year</a>.</p>\n<p>The above has focused on potential risks of <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Transformative\">transformative AI</a>. There are also many potential AI developments short of transformative AI that could be very important. For example:</p>\n<ul>\n<li><a href=\"https://www.openphilanthropy.org/research/cause-reports/autonomous-vehicles\">Autonomous vehicles</a> could become widespread relatively soon.</li>\n<li>Continued advances in computer vision, audio recognition, etc. could dramatically alter what sorts of surveillance are possible, with a wide variety of potential implications; advances in robotics could have major implications for the future of warfare or policing. These could be important whether or not they ended up being \u201ctransformative\u201d in <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Transformative\">our sense</a>.</li>\n<li>Automation could have major economic implications, again even if the underlying AI systems are not \u201ctransformative\u201d in <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Transformative\">our sense</a>.</li>\n</ul>\n<p>We are interested in these potential developments, and see the possibility of helping to address them as a potential benefit of allocating resources to this cause. With that said, my <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence\">previously expressed views</a>, if correct, would imply that most of the \u201cimportance\u201d (as we\u2019ve defined it) in this cause comes from the enormously high-stakes possibility of transformative AI.</p>\n<h2>Neglectedness</h2>\n<p>Both artificial intelligence generally and potential risks have received increased attention in recent years.<sup class=\"footnote-ref\"><a href=\"#fn-sGWbhzSRPX5GrR9Ct-4\" id=\"fnref-sGWbhzSRPX5GrR9Ct-4\">[4]</a></sup> We\u2019ve put substantial work into trying to ensure that we have a thorough landscape of the researchers, funders, and key institutions in this space. We will later be putting out a landscape document, which will be largely consistent with the <a href=\"https://www.openphilanthropy.org/research/cause-reports/ai-risk#Who_else_is_working_on_this\">landscape we published last year</a>. In brief:</p>\n<ul>\n<li>There is a substantial and growing field, with a significant academic presence and significant corporate funding as well, around artificial intelligence and machine learning research.</li>\n<li>There are <a href=\"https://www.openphilanthropy.org/research/cause-reports/ai-risk#Who_else_is_working_on_this\">a few organizations</a> focused on reducing potential risks, either by pursuing particular technical research agendas or by highlighting strategic considerations. (An example of the latter is Nick Bostrom\u2019s work, housed at the Future of Humanity Institute, on <a href=\"http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/\">Superintelligence</a>.) Most of these organizations are connected to the <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> community. Based on conversations we\u2019ve had over the last few months, I believe some of these organizations have substantial <a href=\"http://www.givewell.org/international/technical/criteria/scalability\">room for more funding</a>. There tends to be fairly little intersection between the people working at these organizations and people with substantial experience in mainstream research on AI and machine learning.</li>\n<li>Ideally, I\u2019d like to see leading researchers in AI and machine learning play leading roles in thinking through potential risks, including the associated technical challenges. Under the status quo, I feel that these fields - culturally and institutionally - do not provide much incentive to engage with these issues. While there is some interest in potential risks - in particular, some private labs have expressed informal interest in the matter, and many strong academics applied for the <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-life-institute-artificial-intelligence-risk-reduction\">Future of Life Institute request for proposals that we co-funded last year</a> - I believe there is room for much more. In particular, I believe that the amount of dedicated technical work focused on reducing potential risks is relatively small compared to the extent of open technical questions.</li>\n<li>I\u2019d also like to see a larger set of institutions working on key questions around strategic and policy considerations for reducing risks. I am particularly interested in frameworks for minimizing future misuse risks of transformative AI. I would like to see institutions with strong policy expertise considering different potential scenarios with respect to transformative AI; considering how governments, corporations, and individual researchers should react in those scenarios; and working with AI and machine learning researchers to identify potential signs that particular scenarios are becoming more likely. I believe there may be nearer-term questions (such as how to minimize misuse of advanced surveillance and drones) that can serve as jumping-off points for this sort of thinking.</li>\n<li>Elon Musk, the majority funder of the Future of Life Institute\u2019s 3-year grant program on robust and beneficial AI, is currently focusing his time and effort (along with significant funding) on <a href=\"https://openai.com/about/\">OpenAI</a> and its efforts to mitigate potential risks. (OpenAI is an AI research company that operates as a nonprofit.) We\u2019re not aware of other similarly large private funders focused on potential risks from advanced artificial intelligence. There are government funders interested in the area, but they appear to operate under heavy constraints. There are individual donors interested in this space, but it appears to us that they are focused on different aspects of the problem and/or are operating a smaller scale.</li>\n</ul>\n<p>Bottom line - <strong>I consider this cause to be highly neglected</strong>, particularly by philanthropists, and I see major gaps in the relevant fields that a philanthropist could potentially help to address.</p>\n<h2>Tractability</h2>\n<p>It\u2019s been the case for a long time that I see this cause as important and neglected, and that my biggest reservation has been tractability. I see <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Transformative\">transformative AI</a> as very much a future technology \u2013 I\u2019ve argued that there is a nontrivial probability that it will be developed in the next 20 years, but it is also quite plausibly more than 100 years away, and even 20 years is a relatively long time. Working to reduce risks from a technology that is so far in the future, and about which so much is still unknown, could easily be futile.</p>\n<p>With that said, this cause is not as unique in this respect as it might appear at first. I believe that one of the things philanthropy is best-positioned to do is provide steady, long-term support as fields and institutions grow. This activity is necessarily slow. It requires being willing to support groups based largely on their leadership and mission, rather than immediate plans for impact, in order to lay the groundwork for an uncertain future. I\u2019ve written about this basic approach <a href=\"https://www.openphilanthropy.org/blog/how-approach-policy-oriented-philanthropy\">in the context of policy work</a>, and I believe there is ample precedent for it in the <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">history of philanthropy</a>. It is the approach we favor for several of our <a href=\"https://www.openphilanthropy.org/focus/us-policy\">other focus areas</a>, such as <a href=\"https://www.openphilanthropy.org/focus/us-policy/immigration-policy\">immigration policy</a> and <a href=\"https://www.openphilanthropy.org/focus/us-policy/macroeconomic-policy\">macroeconomic stabilization policy</a>.</p>\n<p>And I have come to believe that there is potentially useful work to be done today that could lay the groundwork for mitigating future potential risks. In particular:</p>\n<p><strong>I think there are important technical challenges that could prove relevant to reducing accident risks.</strong></p>\n<p><em>Added June 24: for more on technical challenges, see <a href=\"https://www.openphilanthropy.org/blog/concrete-problems-ai-safety\">Concrete Problems in AI Safety.</a></em></p>\n<p>I\u2019ve previously put significant weight on an argument along the lines of, \u201cBy the time <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Transformative\">transformative AI</a> is developed, the important approaches to AI will be so different from today\u2019s that any technical work done today will have a very low likelihood of being relevant.\u201d My views have shifted significantly for two reasons. First, as discussed <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence\">previously</a>, I now think there is a nontrivial chance that transformative AI will be developed in the next 20 years, and that the above-quoted argument carries substantially less weight when focusing on that high-stakes potential scenario. Second, having had more conversations about open technical problems that could be relevant to reducing risks, I\u2019ve come to believe that there is a substantial amount of work worth doing today, regardless of how long it will be until the development of transformative AI.</p>\n<p>Potentially relevant challenges that we\u2019ve come across so far include value learning (designing AI systems to learn the values of other agents through e.g. inverse reinforcement learning); problems having to do with making reinforcement learning systems and other AI agents less likely to behave in undesirable ways (designing reinforcement learning systems that will not try to gain direct control of their rewards, that will avoid behavior with unreasonably far-reaching impacts, and that will be robust against differences between formally specified rewards and human designers\u2019 intentions in specifying those rewards); reliability and usability of machine learning techniques (including transparency, understandability, and robustness against or at least detection of large changes in input distribution); formal specification and verification of deep learning, reinforcement learning, and other AI systems; better theoretical understanding of desirable properties for powerful AI systems; and a variety of challenges related to an approach laid out in a <a href=\"https://medium.com/ai-control\">series of blog posts by Paul Christiano</a>.</p>\n<p>Going into the details of these challenges is beyond the scope of this post, but to give a sense for non-technical readers of what a relevant challenge might look like, I will elaborate briefly on one challenge. A reinforcement learning system is designed to learn to behave in a way that maximizes a quantitative \u201creward\u201d signal that it receives periodically from its environment - for example, <a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\">DeepMind\u2019s Atari player</a> is a reinforcement learning system that learns to choose controller inputs (its behavior) in order to maximize the game score (which the system receives as \u201creward\u201d), and this produces very good play on many Atari games. However, if a future reinforcement learning system\u2019s inputs and behaviors are not constrained to a video game, and if the system is good enough at learning, a new solution could become available: the system could maximize rewards by directly modifying its reward \u201csensor\u201d to <em>always</em> report the maximum possible reward, and by avoiding being shut down or modified back for as long as possible. This behavior is a formally correct solution to the reinforcement learning problem, but it is probably not the desired behavior. And this behavior might not emerge until a system became quite sophisticated and had access to a lot of real-world data (enough to find and execute on this strategy), so a system could appear \u201csafe\u201d based on testing and turn out to be problematic when deployed in a higher-stakes setting. The challenge here is to design a variant of reinforcement learning that would not result in this kind of behavior; intuitively, the challenge would be to design the system to pursue some actual goal in the environment that is only indirectly observable, instead of pursuing problematic proxy measures of that goal (such as a \u201chackable\u201d reward signal).</p>\n<p>It appears to me that work on challenges like the above is possible in the near term, and could be useful in several ways. Solutions to these problems could turn out to directly reduce accident risks from transformative AI systems developed in the future, or could be stepping stones toward techniques that could reduce these risks; work on these problems could clarify desirable properties of present-day systems that apply equally well to systems developed in the longer-term; or work on these problems today could help to build up the community of people who will eventually work on risks posed by longer-term development, which would be difficult to do in the absence of concrete technical challenges.</p>\n<p><strong>I preliminarily feel that there is also useful work to be done today in order to reduce future misuse risks and provide useful analysis of strategic and policy considerations.</strong></p>\n<p>As mentioned above, I would like to see more institutions working on considering different potential scenarios with respect to transformative AI; considering how governments, corporations, and individual researchers should react in those scenarios; and working with machine learning researchers to identify potential signs that particular scenarios are becoming more likely.</p>\n<p>I think it\u2019s worth being careful about funding this sort of work, since it\u2019s possible for it to backfire. My current impression is that government regulation of AI today would probably be unhelpful or even counterproductive (for instance by slowing development of AI systems, which I think currently pose few risks and do significant good, and/or by driving research underground or abroad). If we funded people to think and talk about misuse risks, I\u2019d worry that they\u2019d have incentives to attract as much attention as possible to the issues they worked on, and thus to raise the risk of such premature/counterproductive regulation.</p>\n<p>With that said, I believe that potential risks have now received enough attention \u2013 some of which has been unfortunately exaggerated in my view \u2013 that premature regulation and/or intervention by government agencies is already a live risk. I\u2019d be interested in the possibility of supporting institutions that could provide thoughtful, credible, public analysis of <em>whether and when</em> government regulation/intervention would be advisable, even if it meant simply making the case against such things for the foreseeable future. I think such analysis would likely improve the quality of discussion and decision-making, relative to what will happen without it.</p>\n<p>I also think that technical work related to accident risks \u2013 along the lines discussed above \u2013 could be indirectly useful for reducing misuse risks as well. Currently, it appears to me that different people in the field have very different intuitions about how serious and challenging accident risks are. If it turns out that there are highly promising paths to reducing accident risks \u2013 to the point where the risks look a lot less serious \u2013 this development could result in a beneficial refocusing of attention on misuse risks. (If, by contrast, it turns out that accident risks are large and present substantial technical challenges, this makes work on such risks extremely valuable.)</p>\n<p><strong>Other notes on tractability.</strong></p>\n<ul>\n<li>I\u2019ve long worried that it\u2019s simply too difficult to make meaningful statements (even probabilistic ones) about the future course of technology and its implications. However, I\u2019ve gradually changed my view on this topic, partly due to reading I\u2019ve done on personal time. It will be challenging to assemble and present the key data points, but I hope to do so at some point this year.</li>\n<li>Much of our overarching goal for this cause, in the near term, is to <strong>support an increase in the number of people \u2013 particularly people with strong relevant technical backgrounds - thinking through how to reduce potential risks.</strong> Even if the specific technical, strategic and other work we support does not prove useful, helping to support a growing field in this way could be. With that said, I think we will accomplish this goal best if the people we support are doing good and plausibly useful work.</li>\n</ul>\n<p><strong>Bottom line.</strong> I think there are real questions around the extent to which there is work worth doing today to reduce potential risks from advanced artificial intelligence. That said, I see a reasonable amount of potential if there were more people and institutions focused on the relevant issues; given the importance and neglectedness of this cause, I think that\u2019s sufficient to prioritize it highly.</p>\n<h2>Some Open-Phil-specific considerations</h2>\n<h3>Networks</h3>\n<p>I consider this a challenging cause. I think it would be easy to do harm while trying to do good. For example:</p>\n<ul>\n<li>Trying to raise the profile of potential risks could contribute (and, I believe, has contributed to some degree) to non-nuanced or inaccurate portrayals of risk in the media, which in turn could raise the risks of premature and/or counterproductive regulation. In addition, raising such risks (or being perceived as doing so) could - in turn - cause many AI and machine learning researchers who oppose such regulation to become hostile to the idea of discussing potential risks.</li>\n<li>Encouraging particular lines of research without sufficient input and buy-in from leading AI and machine learning researchers could be not only unproductive but counterproductive. It could lead to people generally taking risk-focused research less seriously. And since leading researchers tend to be extremely busy, getting thorough input from them can be challenging in itself.</li>\n</ul>\n<p>I think it is important for someone working in this space to be highly attentive to these risks. In my view, one of the best ways to achieve this is to be as well-connected as possible to the people who have thought most deeply about the key issues, including both the leading researchers in AI and machine learning and the people/organizations most focused on reducing long-term risks.</p>\n<p>I believe the Open Philanthropy Project is unusually well-positioned from this perspective:</p>\n<ul>\n<li>We are well-connected in the <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> community, which includes many of the <a href=\"https://www.openphilanthropy.org/research/cause-reports/ai-risk#Who_else_is_working_on_this\">people and organizations</a> that have been most active in analyzing and raising awareness of potential risks from advanced artificial intelligence. For example, <a href=\"https://www.openphilanthropy.org/about/team/daniel-dewey\">Daniel Dewey</a> has previously worked at the Future of Humanity Institute and the Future of Life Institute, and has been a research associate with the Machine Intelligence Research Institute.</li>\n<li>We are also reasonably well-positioned to coordinate with leading researchers in AI and machine learning. Daniel has some existing relationships, partly due to his work on <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-life-institute-artificial-intelligence-risk-reduction\">last year\u2019s request for proposals from the Future of Life Institute</a>. As mentioned <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#More_detailed_analysis\">previously</a>, some of us also have strong relationships with several researchers at top institutions. We have recently been <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Sec7\">reaching out to many leading researchers to discuss our plans for this cause</a>, and have generally been within a couple of degrees of separation via our networks.</li>\n</ul>\n<h3>Time vs. money</h3>\n<p>One consideration that has made me hesitant about prioritizing this cause is the fact that I see relatively little in the way of truly \u201cshovel-ready\u201d giving opportunities. I list our likely priorities in the next section; I think they are likely to be very time-consuming for staff, and I am unsure of how long it will take before we see as many concrete giving opportunities as we do in some of our other <a href=\"https://www.openphilanthropy.org/focus\">focus areas</a>.</p>\n<p>By default, I prefer to prioritize causes with significant existing \u201cshovel-ready\u201d opportunities and minimal necessary time commitment, because I consider the Open Philanthropy Project to be <a href=\"https://www.openphilanthropy.org/blog/should-open-philanthropy-project-be-recommending-morelarger-grants\">short on capacity relative to funding at this stage in our development</a>.</p>\n<p>However, I think the case for this cause is compelling enough to outweigh this consideration, and I think a major investment of senior staff time this year could leave us much better positioned to find outstanding giving opportunities in the future.</p>\n<h2>Our plans</h2>\n<p>For the last couple of months, we have focused on:</p>\n<ul>\n<li>Talking to as many people as possible in the relevant communities, particularly leading researchers in AI and machine learning, in order to get feedback on our thinking, deepen our understanding of the relevant issues, and ensure that we have open channels of communication with them. Some high-level notes from these conversations are <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Sec7\">below</a>.</li>\n<li>Developing our communications strategy for this topic, including this series of blog posts.</li>\n<li>Investigating the few potential \u201cshovel-ready grants\u201d (by which I mean grants we can investigate and recommend with relatively low time investments) we\u2019re aware of. We will be publishing more about these later.</li>\n<li>Working with several technical advisors to begin to get a sense of what the most important concrete, known technical challenges are. Our hope is to get to the point of being able to offer substantial funding to support work on the most important challenges. We\u2019re beginning with close contacts and planning to broaden the conversation about the most important technical challenges from there.</li>\n<li>Working with close technical advisors to flesh out the <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#More_detailed_analysis\">key considerations around likely timelines to transformative AI</a>. We expect to continue this work, hopefully with an increasingly broad set of researchers engaging in the discussions.</li>\n<li>Having initial conversations about what sorts of misuse risks we should be most concerned about, and what sorts of strategic and policy considerations seem most important, in order to lay the groundwork for finding potential grantees in this category.</li>\n<li>Seeking past cases in which philanthropists helped support the growth of technical fields, to see what we can learn.</li>\n</ul>\n<p>Ultimately, we expect to seek giving opportunities in the following categories:</p>\n<ul>\n<li>\u201cShovel-ready\u201d grants to existing organizations and researchers focused on reducing potential risks from advanced artificial intelligence.</li>\n<li>Supporting substantial work on the most important technical challenges related to reducing accident risks. This could take the form of funding academic centers, requests for proposals, convenings and workshops, and/or individual researchers.</li>\n<li>Supporting thoughtful, nuanced, independent analysis seeking to help inform discussions of key strategic and policy considerations for reducing potential risks, including misuse risks.</li>\n<li>\u201cPipeline building\u201d: supporting programs, such as fellowships, that can increase the total number of people who are deeply knowledgeable about technical research on artificial intelligence and machine learning, while also being deeply versed in issues relevant to potential risks.</li>\n<li>Other giving opportunities that we come across, including those that pertain to AI-relevant issues other than those we\u2019ve focused on in this post (some such issues are listed <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#OtherIssues\">above</a>).</li>\n</ul>\n<p>Getting to this point will likely require a great deal more work and discussion \u2013 internally and with the relevant communities more broadly. It could be a long time before we are recommending large amounts of giving in this area, and I think that allocating significant senior staff time to the cause will speed our work considerably.</p>\n<h2>Some overriding principles for our work</h2>\n<p>As we work in this space, we think it\u2019s especially important to follow a few core principles:</p>\n<h3>Don\u2019t lose sight of the potential benefits of AI, even as we focus on mitigating risks</h3>\n<p>Our work is focused on potential risks, because this is the aspect of AI research that seems most neglected at the moment. However, as stated above, I see many ways in which AI has enormous potential to improve the world, and I expect the consequences of advances in AI to be positive on balance. It is important to act and communicate accordingly.</p>\n<h3>Deeply integrate people with strong technical expertise in our work</h3>\n<p>The <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-life-institute-artificial-intelligence-risk-reduction\">request for proposals we co-funded last year</a> employed an expert review panel for selecting grantees. We wouldn\u2019t have participated if it had involved selecting grantees ourselves with nontechnical staff. We believe that AI and machine learning researchers are the people best positioned to make many assessments that will be important to us, such as which technical problems seem tractable and high-potential and which researchers have impressive accomplishments.</p>\n<h3>Seek a lot of input, and reflect a good deal, before committing to major grants and other activities</h3>\n<p>As stated above, I consider this a challenging cause, where well-intentioned actions could easily do harm. We are seeking to be thoroughly networked and to seek substantial advice on our activities from a range of people, both AI and machine learning researchers and people focused on reduction of potential risks.</p>\n<h3>Support work that could be useful in a variety of ways and in a variety of scenarios, rather than trying to make precise predictions</h3>\n<p>I don\u2019t think it\u2019s possible to have certainty, today, about when we should expect transformative AI, what form we should expect it to take, and/or what the consequences will be. We have a preference for supporting work that seems robustly likely to be useful. In particular, one of our main goals is to support an increase in the number of people \u2013 particularly people with strong relevant technical backgrounds - dedicated to thinking through how to reduce potential risks.</p>\n<h3>Distinguish between lower-stakes, higher-stakes, and highest-stakes potential risks</h3>\n<p>There are many imaginable risks of advanced artificial intelligence. Our focus is likely to be on those that seem to have the very highest stakes, to the point of being potential <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks#Our_basic_framework\">global catastrophic risks</a>. In our view currently, that means misuse risks and accident risks involving transformative AI. We also consider neglectedness (we prefer to work on risks receiving less attention from others) and tractability (we prefer to work on risks where it seems there is useful work to be done today that can help mitigate them).</p>\n<h2>Notes on AI and machine learning researchers\u2019 views on the topics discussed here</h2>\n<p>Over the last couple of months, we have been reaching out to AI and machine learning researchers that we don\u2019t already have strong relationships with in order to discuss our plans and background views and get their feedback. We have put particular effort into seeking out skeptics and potential critics. As of today, we have requested 35 conversations along these lines and had 25. About three-fourths of these conversations have been with tenure-track academics or senior researchers at private labs, and the remainder have been with students or junior researchers at top AI and machine learning departments and private labs.</p>\n<p>We\u2019ve heard a diverse set of perspectives. Conversations were in confidence and often time-constrained, so we wouldn\u2019t feel comfortable attributing specific views to specific people. Speaking generally, however, it seems to us that:</p>\n<ul>\n<li>We encountered fewer strong skeptics of this cause than we expected to, given our previous informal impression that there are many researchers who are dismissive of potential risks from advanced artificial intelligence. That said, we spoke to a couple of highly skeptical researchers, and a few high-profile researchers who we think might be highly skeptical declined to speak with us.</li>\n<li>Most of the researchers we talked to did not seem to have spent significant time or energy engaging with questions around potential risks from advanced artificial intelligence. To the extent they had views, most of the people we talked to seemed generally supportive of the views and goals we\u2019ve laid out in this post (though this does not at all mean that they would endorse everything we\u2019ve said).</li>\n<li>Overall, these conversations caused us to update slightly positively on the promise of this cause and our plans. We hope to have many more conversations with AI and machine learning researchers in the coming months to deepen our understanding of the different perspectives in the field.</li>\n</ul>\n<h2>Risks and reservations</h2>\n<p>I see much room for debate in the decision to prioritize this cause as highly as we are. I have discussed most of the risks and reservations I see in this post and the ones preceding it. Here I list the major ones in one place. In this section, my goal is to provide a consolidated list of risks and reservations, but not necessarily to give my comprehensive take on each.</p>\n<ul>\n<li>As <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence\">discussed previously</a>, I assign a nontrivial probability (at least 10% with moderate <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">robustness</a>, at least 1% with high robustness) to the development of transformative AI within the next 20 years. I feel I have thought deeply about this question, with access to strong technical advisors, and that we\u2019ve collected what information we can, though I haven\u2019t been able to share all important inputs into my thinking publicly. I recognize that our information is limited, and my take is highly debatable.</li>\n<li>I see a risk that our thinking is distorted by being in an \u201cecho chamber,\u201d and that our views on the importance of this cause are overly reinforced by our closest technical advisors and by the <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> community. I\u2019ve written <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving#Anti-principles_for_hits-based_giving\">previously</a> about why I don\u2019t consider this a fatal concern, but it does remain a concern.</li>\n<li>I do not want to exacerbate what I see as an unfortunate pattern, to date, of un-nuanced and inaccurate media portrayals of potential risks from advanced artificial intelligence. I think this could lead to premature and/or counterproductive regulation, among other problems. We hope to communicate about our take on this cause with enough nuance to increase interest in reducing risks, without causing people to view AI as more threatening than positive.</li>\n<li>I think the case that this cause is <a href=\"https://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Neglectedness\">neglected</a> is fairly strong, but leaves plenty of room for doubt. In particular, the cause has received attention from some high-profile people, and multiple well-funded AI labs and many AI researchers have expressed interest in doing what they can to reduce potential risks. It\u2019s possible that they will end up pursuing essentially all relevant angles, and that the activities listed above will prove superfluous.</li>\n<li>I\u2019m mindful of the possibility that it might be futile to make meaningful predictions, form meaningful plans, and do meaningful work to reduce fairly far-off and poorly-understood potential risks.</li>\n<li>I recognize that it\u2019s debatable how important accident risks are. It\u2019s possible that preventing truly catastrophic accidents will prove to be relatively easy, and that early work will look in hindsight like a poor use of resources.</li>\n</ul>\n<p>With all of the above noted, I think it is <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving\">important that a philanthropist in our position be willing to take major risks</a>, and prioritizing this cause is one that I see as very worth taking.</p>\n<h2>Notes</h2>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-sGWbhzSRPX5GrR9Ct-1\" class=\"footnote-item\"><p>I\u2019m not in a position to support this claim very systematically, but we have done a substantial amount of investigation and discussion of various aspects of scientific research, as discussed in our <a href=\"https://www.openphilanthropy.org/blog/open-philanthropy-project-progress-2015-and-plans-2016\">recent annual review</a>. In a previous post, I <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#Sec3\">addressed</a> what I see as the most noteworthy other possible major developments in the next 20 years. <a href=\"#fnref-sGWbhzSRPX5GrR9Ct-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-sGWbhzSRPX5GrR9Ct-2\" class=\"footnote-item\"><p>Here I mean that it scores significantly higher by this criterion than the vast majority of causes, not that it stands entirely alone. I think there are a few other causes that have comparable importance, though none that I think have <em>greater</em> importance, as we\u2019ve defined it. <a href=\"#fnref-sGWbhzSRPX5GrR9Ct-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-sGWbhzSRPX5GrR9Ct-3\" class=\"footnote-item\"><p>We\u2019ve been accumulating case studies via our <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">History of Philanthropy project</a>, and we expect to publish an updated summary of what we know by the end of 2016. For now, there is some information available at our <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">History of Philanthropy page</a> and in a <a href=\"https://www.openphilanthropy.org/blog/hits-based-giving#The_basic_case_for_hits-based_giving\">recent blog post</a>. <a href=\"#fnref-sGWbhzSRPX5GrR9Ct-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-sGWbhzSRPX5GrR9Ct-4\" class=\"footnote-item\"><p>See <a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-potential-risks-advanced-artificial-intelligence#FieldGrowth\">our previous post</a> regarding artificial intelligence generally. See our <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/future-life-institute-artificial-intelligence-risk-reduction#The_Future_of_Life_Institute8217s_2015_Request_for_Proposals\">writeup on a 2015 grant to support a request for proposals</a> regarding potential risks. <a href=\"#fnref-sGWbhzSRPX5GrR9Ct-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "HoldenKarnofsky"}}, {"_id": "QXpxioWSQcNuNnNTy", "title": "The Copenhagen Interpretation of Ethics", "postedAt": "2016-11-04T07:00:00.000Z", "htmlBody": "<p>The Copenhagen Interpretation of quantum mechanics says that&nbsp;you can have a particle spinning clockwise and counterclockwise at the same time \u2013 until you look at it, at which point it definitely becomes one or the other. The theory claims that&nbsp;observing reality fundamentally changes it.</p><p>The Copenhagen Interpretation of Ethics says that when you observe or interact with a problem in any way, you can be blamed for it. At the very least, you are to blame for not doing <i>more</i>. Even if you don\u2019t make the problem worse, even if you make it slightly better, the ethical burden of the problem falls on you as soon as you observe it. In particular, if you interact with a problem and benefit from it, you are a complete monster. I don\u2019t subscribe to this school of thought, but it seems pretty popular.</p><hr><p>In 2010, <a href=\"https://web.archive.org/web/20210610041813/http://www.nytimes.com/2010/12/09/nyregion/09placebo.html?_r=2&amp;hpAlways=&amp;pagewanted=all\">New York randomly chose homeless applicants to participate in its Homebase program</a>, and tracked those who were not allowed into the program as a control group. The program was helping as many people as it could, the only change was explicitly labeling a number of people it wasn\u2019t helping as a \u201ccontrol group\u201d. The response?</p><blockquote><p><i>\u201cThey should immediately stop this experiment,\u201d said the Manhattan borough president, </i><a href=\"https://web.archive.org/web/20210610041813/http://topics.nytimes.com/top/reference/timestopics/people/s/scott_m_stringer/index.html?inline=nyt-per\"><i>Scott M. Stringer</i></a><i>. \u201cThe city shouldn\u2019t be making guinea pigs out of its most vulnerable.\u201d</i></p></blockquote><hr><p>On March 11th, 2012, the vast majority of people did nothing to help homeless people. They were busy doing other things, many of them good and important things, but&nbsp;by and large not improving the well-being of homeless humans in any way. In particular, almost no one was doing anything for the homeless of Austin, Texas. BBH Labs was an exception \u2013 they outfitted 13 homeless volunteers with WiFi hotspots and asked them to offer WiFi to SXSW attendees&nbsp;in exchange for donations. In return, they would be paid $20 a day plus whatever attendees gave in donations. Each of these 13 volunteers chose this over all the other things they could have done that day, and benefited from it \u2013 not a vast improvement, but significantly more than the 0 improvement that they were getting from most people.</p><p>The <a href=\"https://web.archive.org/web/20210610041813/http://www.wired.com/2012/03/the-damning-backstory-behind-homeless-hotspots-at-sxswi/\">response</a>?</p><blockquote><p><i>IT SOUNDS LIKE something out of a </i><a href=\"https://web.archive.org/web/20210610041813/http://nytsxsw.tumblr.com/post/19145988299/getting-a-decent-data-connection-at-sxsw-can-be-a\"><i>darkly satirical science-fiction dystopia</i></a><i>. But it\u2019s absolutely real \u2014 and a completely problematic treatment of a problem that otherwise probably wouldn\u2019t be mentioned in any of the panels at South by Southwest Interactive.</i></p></blockquote><p>There wouldn\u2019t be any scathing editorials if BBH Labs had just chosen to do nothing \u2013 but they&nbsp;did something helpful-but-not-maximally-helpful, and thus are open to judgment.</p><hr><p>There are times when it\u2019s almost impossible to get a taxi \u2013 when there\u2019s inclement weather, when a large event is getting out, or when it\u2019s just a very busy day. Uber attempts to solve this problem by introducing surge pricing \u2013 charging more when demand outstrips supply. More money means more drivers willing to make the trip, means more rides available. Now instead of having no taxis at all, people can choose between an expensive taxi or no taxi at all \u2013 a marginal improvement.&nbsp;Needless to say, Uber has been repeatedly lambasted for doing something instead of leaving the even-worse status quo the way it was.</p><hr><p>Gender inequality is a persistent, if hard to quantify, problem. <a href=\"https://web.archive.org/web/20210610041813/http://blog.jaibot.com/?p=392\">Last year I blogged</a> about how amoral agents could save money and drive the wage gap down to 0 by offering slightly less-sexist wages \u2013 while including some caveats about how it was probably unrealistic and we wouldn\u2019t see anything like that in reality.&nbsp;So of course <i><strong>less than a week after I wrote that&nbsp;</strong></i><a href=\"https://web.archive.org/web/20210610041813/http://www.theage.com.au/it-pro/business-it/evan-thornley-causes-stir-with-sexist-comments-at-sunrise-startup-conference-20140922-10kiku.html\">Evan Thornley says</a>&nbsp;:</p><blockquote><p><i>\u201cThere\u2019s a great arbitrage there, we would give [women] more responsibility and a greater share of the rewards than they were likely to get anywhere else and that was still often relatively cheap to someone less good of a different gender.\u201d</i></p><p><i>While Mr Thornley said he wasn\u2019t advocating that the gender pay gap should be perpetuated, he said it provided \u201can opportunity for forward thinking people\u201d.</i></p><p><i>A number of online commentators, as well as Australian start-up blogs, have since said Mr Thornley\u2019s comments were sexist.</i></p></blockquote><p>Mr. Thornley improved on the status quo \u2013 but in the process he interacted the problem and was thus caught up in it. This is a strategy which, if widely embraced, would practically eliminate&nbsp;many forms of wage discrimination overnight simply by harnessing something we have way too much of already: greed. So of course it was denounced.</p><hr><p>Last year the city of Detroit began to crack down on unpaid water bills, and thousands of poor people suddenly faced the prospect of having their water shut off. The vast majority of people did nothing to help them whatsoever. PETA did offer conditional help: If a family went vegan for 30 days, PETA would pay off their water bill, and throw in a basket of vegan food to boot. This was strictly more helpful than what 99.99999% of humanity was doing for Detroit residents at the time, as it didn\u2019t make anything worse and offered a trade for anyone who valued 30 days of not-being-vegan less than however much they owed on their water bill. For marginally improving he situation instead of ignoring it, they were <a href=\"https://web.archive.org/web/20210610041813/http://thinkprogress.org/economy/2014/07/24/3464033/peta-is-the-worst/\">denounced as \u201cthe worst\u201d</a>.</p><hr><p>Peter Singer has a famous thought experiment about a child&nbsp;drowning in a pond. I\u2019ll let <a href=\"https://web.archive.org/web/20210610041813/http://www.philosophybro.com/post/120721529553/peter-singers-drowning-child-argument\">Philosophy Bro</a> explain:</p><blockquote><p><i>Like, let\u2019s say I\u2019m on my way to a bitchin\u2019 party and I\u2019m looking fly as shit and I smell good because you already know, and I\u2019ve got a 30-rack of Natty because I\u2019ll be goddamned if I show up empty-handed to the house I\u2019m about to burn down. Once I get over this bridge, and turn the corner I\u2019ve arrived and so has the party. Except I hear a bunch of splashing and I look over the bridge into the river and \u2013 fuck me \u2013 there\u2019s a kid flailing around and calling for help, like he\u2019s drowning for some reason instead of handling his shit like an adult.</i></p><p><i>I should save his life, right?</i></p><p><i>Sometimes in philosophy we like to ask obvious questions and waggle our eyebrows suggestively, like maybe you don\u2019t exist after all, hmm? but bro, this is not one of those times. I should obviously jump in and SAVE THIS FUCKING CHILD\u2019S LIFE. So I ruin a Polo and I don\u2019t smell good anymore and a couple of the beers explode because I dropped them. Who gives a shit, right? A child was going to die.</i></p><p><i>\u2026[snip]\u2026</i></p><p><i>What if I told you that for $5, you could buy a life-saving vaccine for a child? Sure, he\u2019s far away, but we already agreed: who gives a shit, right? It\u2019ll still save his life, and it only costs you not having a fifth drink at the bar on a Thursday. Remember that $300 bar receipt you posted with the caption \u201cjust another Thursday night wearing matching plaid with my bros, we\u2019re special and impressive and are the ACTUAL six dudes with the biggest dicks, unlike all you OTHER overconfidences of bros who think that, well guess what, it\u2019s us?\u201d What you were really saying was \u201cI routinely pass up the chance to save two dozen lives with science so that I can black out and pretend that I like myself for a night.\u201d That\u2019s fucked up, bro.</i></p></blockquote><p>The difference is that the drowning child has been definitively noticed, and thus her moral weight bears down on us and we have to save her. But children thousands of miles away? Not noticed!</p><p>I think this might be where a lot of the discomfort with talking about things we can do to alleviate suffering comes from. If you implicitly believe in the Copenhagen Interpretation of Ethics, then to confront the scope of suffering in the world is to make it your fault, and then if you don\u2019t throw everything you have at the problem you\u2019re as \u201cbad\u201d as PETA or Mr. Thornley or Uber or BBH Labs.</p><p>But what if \u2013 what if noticing a problem didn\u2019t make it any worse? What if we could act on&nbsp;a problem and not feel horrible for making it just a little better, even if it was an action that benefited ourselves as well? What if we said that in these instances, these groups weren\u2019t evil \u2013 it\u2019s okay to notice a problem and only make it a little bit better. If everyone did that, the world would be a vastly better place. If everyone \u201cexploited\u201d opportunities where they could benefit and alleviate people\u2019s suffering at the same time, we\u2019d all be better off.</p>", "user": {"username": "jai"}}, {"_id": "TatQ2cfnhMyZerm9j", "title": "Differential Technological Development: Some Early Thinking", "postedAt": "2015-09-29T10:23:25.590Z", "htmlBody": "<p><i>Note: this post aims to help a particular subset of our audience understand the assumptions behind our work on </i><a href=\"https://www.openphilanthropy.org/blog/our-updated-agenda-science-philanthropy\"><i>science philanthropy</i></a><i> and </i><a href=\"https://www.openphilanthropy.org/blog/open-philanthropy-project-update-global-catastrophic-risks\"><i>global catastrophic risks</i></a><i>. Throughout, \u201cwe\u201d refers to positions taken by the Open Philanthropy Project as an entity rather than to a consensus of all staff.</i></p><p>Two priorities for the Open Philanthropy Project are our work on <a href=\"https://www.openphilanthropy.org/blog/our-updated-agenda-science-philanthropy\">science philanthropy</a> and <a href=\"https://www.openphilanthropy.org/blog/open-philanthropy-project-update-global-catastrophic-risks\">global catastrophic risks</a>. These interests are related because\u2014in addition to greatly advancing civilization\u2019s wealth and prosperity\u2014advances in certain areas of science and technology may be key to exacerbating <i>or</i> addressing what we believe are the largest global catastrophic risks. (For detail on the idea that advances in technology could be a driver, see \u201c\u2009\u2018Natural\u2019 GCRs appear to be less harmful in expectation\u201d in <a href=\"https://www.openphilanthropy.org/blog/potential-global-catastrophic-risk-focus-areas\">this post</a>.) For example, nuclear engineering created the possibility of nuclear war, but also provided a source of energy that does not depend on fossil fuels, making it a potential tool in the fight against climate change. Similarly, future advances in bioengineering, genetic engineering, geoengineering, computer science (including artificial intelligence), nanotechnology, neuroscience, and robotics could have the potential to affect the long-term future of humanity in both positive and negative ways.</p><p>Therefore, we\u2019ve been considering the possible consequences of advancing the pace of development of various individual areas of science and technology in order to have more informed opinions about which might be especially promising to speed up and which might create additional risks if accelerated. Following Nick Bostrom, we call this topic \u201cdifferential technological development.\u201d We believe that our views on this topic will inform our priorities in scientific research, and to a lesser extent, global catastrophic risks. We believe our ability to predict and plan for future factors such as these is highly limited, and we generally favor a <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">default presumption that economic and technological development is positive</a>, but we also think it\u2019s worth putting some effort into understanding the interplay between scientific progress and global catastrophic risks in case any considerations seem strong enough to influence our priorities.</p><p>The first question our investigation of differential technological development looked into was the effect of speeding progress toward advanced AI on global catastrophic risk. This post gives our initial take on that question. One idea we sometimes hear is that it would be harmful to speed up the development of artificial intelligence because not enough work has been done to ensure that when very advanced artificial intelligence is created, it will be safe. This problem, it is argued, would be even worse if progress in the field accelerated. However, very advanced artificial intelligence could be a useful tool for overcoming other potential global catastrophic risks. If it comes sooner\u2014and the world manages to avoid the risks that it poses directly\u2014the world will spend less time at risk from these other factors.</p><p>Curious about how to compare these two factors, I tried looking at a simple model of the implications of a <a href=\"http://www.fhi.ox.ac.uk/gcr-report.pdf\">survey of participants</a> at a <a href=\"http://www.global-catastrophic-risks.com/\">2008 conference</a> on global catastrophic risk organized by the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford University. I found that speeding up advanced artificial intelligence\u2014according to my simple interpretation of these survey results\u2014could easily result in reduced net exposure to the most extreme global catastrophic risks (e.g., those that could cause human extinction), and that what one believes on this topic is highly sensitive to some very difficult-to-estimate parameters (so that other estimates of those parameters could yield the opposite conclusion). This conclusion seems to be in tension with the view that speeding up artificial intelligence research would increase risk of human extinction on net, so I decided to write up this finding, both to get reactions and to illustrate the general kind of work we\u2019re doing to think through the issue of differential technological development.</p><p>Below, I:</p><ul><li>Describe our <a href=\"https://docs.google.com/spreadsheets/d/1fURedpAEckcfpKKqe7XTkn25dmOPWjN1daXKyw1YtVA/edit#gid=1228692427\">simplified model</a> of the consequences of speeding up the development of advanced AI on the risk of human extinction using a <a href=\"http://www.fhi.ox.ac.uk/gcr-report.pdf\">survey of participants</a> at a <a href=\"http://www.global-catastrophic-risks.com/\">2008 conference</a> on global catastrophic risk organized by the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford University.</li><li>Explain why, in this model, the effect of faster progress on artificial intelligence on the risk of human extinction is very unclear.</li><li>Describe several of the model\u2019s many limitations, illustrating the challenges involved with this kind of analysis.</li></ul><p>We are working on developing a broader understanding of this set of issues, as they apply to the areas of science and technology described above, and as they relate to the global catastrophic risks we focus on.</p><h1>A simple model of the consequences of faster advanced AI development</h1><p>\u201cArtificial intelligence is risky if we are not sufficiently prepared\u201d and \u201cArtificial intelligence will reduce other risks if developed safely\u201d are probably the two clearest considerations related to the impact of faster progress toward advanced artificial intelligence on global catastrophic risks, and how much weight to put on them depends on a number of highly uncertain questions, including:</p><ol><li>How large other global catastrophic risks are in comparison with risks from advanced artificial intelligence</li><li>How much advanced artificial intelligence would affect the other important risks</li><li>How much (if any) less prepared we\u2019d be for advanced artificial intelligence if the pace of progress in artificial intelligence were faster</li></ol><p>To illustrate what might be involved in reasoning about this kind of problem, I made a <a href=\"https://docs.google.com/spreadsheets/d/1fURedpAEckcfpKKqe7XTkn25dmOPWjN1daXKyw1YtVA/edit#gid=1228692427\">simplified model</a> using a <a href=\"http://www.fhi.ox.ac.uk/gcr-report.pdf\">survey of participants</a> at a <a href=\"http://www.global-catastrophic-risks.com/\">2008 conference</a> on global catastrophic risk organized by the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford University (where I worked before joining GiveWell). We do not know the details regarding the specific participants, but would guess that participation in this survey selects for unusually high levels of concern about global catastrophic risk.</p><p>The model makes the following assumptions:</p><ol><li>It uses the conference\u2019s estimates of the probability of extinction from various risks.</li><li>It assumes the risk from possible extinction events other than advanced AI is evenly distributed from 2015 to 2100.</li><li>It assumes that if advanced AI is developed safely, the remaining years of risks from other factors will become negligible.</li><li>It focuses exclusively on extinction risk, which we believe is the main type of risk that the people most concerned about advanced AI have in mind.</li></ol><p>The model outputs how much the extinction risk from advanced AI would have to increase (as a fraction of its initial value), as a result of arriving X years sooner, in order to offset the risk reduction from having X fewer years exposed to other risks. The model only considers what happens between 2015 and 2100.</p><p>The median participant in FHI\u2019s conference estimated that the probability of human extinction by 2100 was 19%, with 5 percentage points of that risk coming from advanced artificial intelligence. In my model, those numbers yield the following table:</p><figure class=\"table\" style=\"width:598px\"><table><tbody><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\" rowspan=\"2\">&nbsp;</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\" colspan=\"3\">Number of years advanced AI is sped up</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">5</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">10</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">Percentage point decrease in other risks</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">0.16%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">0.82%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1.65%</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">Fractional decrease in other risks</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1.18%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">5.88%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">11.76%</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">Fractional increase in risk from advanced AI required to offset decrease in other risk</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">3.29%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">16.47%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">32.94%</td></tr></tbody></table></figure><p>For example, this table says that if progress toward advanced AI were sped up by 5 years, it would reduce other risks by about 6% (or about 0.8 percentage points). Since the probability of extinction from advanced AI is lower than the total probability of extinction from other causes, this would require an increase of over 16% to outweigh those 0.8 percentage points. The figures for 5 and 10 years are just multiples of the figures for one year.</p><p>The model doesn\u2019t say anything about how large the increase in risk from advanced AI would actually be if advanced AI were sped up by a given number of years, largely because I couldn\u2019t think of any simple and reasonable way to model that. The model allows readers to check the implications of their own views on this topic.</p><p>One possible example for some context: if one believed that 20 additional years of research on risks from advanced AI would be needed in order to cut risk from advanced AI in half (from 5% to 2.5%), this would imply an average of 0.125 additional percentage points of risk from advanced AI per year of speedup. Since each year of speedup is estimated at ~0.164 percentage points of risk reduction from other causes, this would imply that speedup is safety-improving for the average year. (The picture becomes substantially more complicated if one does not evenly divide the relevant probabilities across years.)</p><p>Overall, under the assumptions of this model, the net effect of faster progress toward advanced AI on total global catastrophic risk seems unclear.</p><p>To a be a bit more general and illustrate the impacts of alternative assumptions, here\u2019s a table that looks at the relevant figures under different assumptions about the <i>ratio of risk from advanced AI to all other risks</i>. These are the proportional increases in risk from advanced AI required to offset X years of AI progress if you accept the above model and assume that the ratio of (risk from advanced AI:all other risks) is as specified. For example, the table says that if you think the ratio is 0.3:1, and you otherwise accept the model described above, then you should think that one year of faster progress toward advanced AI would have neutral consequences for the probability of human extinction before 2100 if faster progress proportionally increased the risk of advanced AI by about 4%.</p><figure class=\"table\" style=\"width:598px\"><table><tbody><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\" colspan=\"2\" rowspan=\"2\">&nbsp;</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\" colspan=\"3\">Number of years development of advanced AI is sped up</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">5</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">10</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\" rowspan=\"5\">Ratio of risk from advanced AI to all other risks</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">0.1</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">11.76%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">58.82%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">117.65%</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">0.3</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">3.92%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">19.61%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">39.22%</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1.18%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">5.88%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">11.76%</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">3</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">0.39%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1.96%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">3.92%</td></tr><tr><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">10</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">0.12%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">0.59%</td><td style=\"border-bottom:1px solid rgb(238, 238, 238);padding:10px;vertical-align:top\">1.18%</td></tr></tbody></table></figure><h1>Limitations to the model</h1><p>This model is extremely simplified, and limitations include the following:</p><ol><li>Risks from future technology are not likely to be evenly spread across the century. They seem small right now, and presumably will be largest during some future time when technology has advanced. They might decline after that as prevention/response mechanisms improve. And each might occur before or after advanced artificial intelligence, in which case the effect of additional progress toward advanced AI on each risk could be much greater or much smaller. For example, if <a href=\"https://www.openphilanthropy.org/research/cause-reports/global-catastrophic-risks/atomically-precise-manufacturing\">atomically precise manufacturing</a> would by default be developed in 2060 and advanced AI would by default be developed in 2050, then additional progress toward advanced AI would not have any effect on risks from atomically precise manufacturing.</li><li>It\u2019s extremely hard to say how much additional preparation for advanced AI would reduce risks from advanced AI. It\u2019s possible that years of preparation work will have increasing or diminishing returns, which could make later years more or less productive relative to earlier years. It\u2019s possible preparation efforts will be more effective in years when AI development is further along. It\u2019s possible that advanced AI is sufficiently distant, and preparation sufficiently subject to diminishing returns, that additional progress toward advanced AI relative to the status quo wouldn\u2019t diminish the level of preparation appreciably.</li><li>Artificial intelligence may not fully eliminate other risks to civilization, and it might vary by risk. For example, nuclear and other advanced weapons could still be used with great destructive potential. But biological weapons might pose much smaller risks.</li><li>The same action (e.g. publishing a series of important papers) might speed up progress toward advanced AI by a significantly greater amount of time if advanced AI is coming in relatively soon (e.g. 20 years), in comparison with potential scenarios where the arrival of advanced AI is much farther in the future (e.g. more than 100 years). Pushing forward progress toward advanced AI could have importantly different consequences in terms of reducing other risks or adequacy of preparations for advanced AI in scenarios where advanced AI is coming relatively soon in comparison with scenarios where advanced AI is coming relatively late. For example, if advanced AI is coming much later, I would guess that (i) it\u2019s more likely we\u2019ll be adequately prepared when advanced AI arrives, and (ii) there will already be other advanced technologies that pose risks to civilization when advanced AI arrives.</li><li>I have little confidence in the conference\u2019s estimates of the probability of various catastrophic events. Readers can try their own estimates in our model if they prefer, or refer to the second table above for a rough sense of what their views would imply.</li><li>The conference\u2019s 5% estimate for risk from advanced AI presumably includes some assumptions about when advanced AI is likely to arrive and how much preparation is likely to be in place before then. I haven\u2019t modeled this; I\u2019ve merely laid out the implications of various assumptions about the impact of marginal speedup on marginal risk increase.</li><li>The model does not include positive/negative outcomes other than extinction, such as global catastrophes or other positive/negative events with lasting consequences.</li><li>There are other important considerations related to the pace of progress in artificial intelligence that I did not consider. For example, the rate of progress\u2014and where the progress happens\u2014could affect:<ol><li>What country or organization obtains advanced capabilities first, which could positively or negatively affect overall risk. It could also affect the attitudes with which advanced AI is developed; for example, in today\u2019s relatively stable and peaceful geopolitical environment, people seem particularly likely to prioritize safety before deploying advanced AI, relative to a possible world where geopolitics is more unstable and the incentives favor deploying advanced AI as soon as possible.</li><li>How advanced available hardware and robotics is when advanced artificial intelligence is developed, which may affect \u201chardware overhang\u201d and how powerful advanced artificial intelligence is at early stages of its development.</li><li>How long it takes to move from it being possible to create advanced artificial intelligence to adequate safety measures being implemented (which could potentially be longer or shorter if advanced AI develops more quickly).</li></ol></li></ol><p>We are working on developing a broader\u2014though still relatively simple\u2014picture of how large various global catastrophic risks are, how those risks depend on progress in different areas of science and technology, and how progress in different areas of science and technology interact with each other.</p><h1>Acknowledgements</h1><p>Thanks to the following people for reviewing a draft of this post and providing thoughtful feedback (this of course does not mean they agree with the post or are responsible for its content): Stuart Armstrong, Nick Bostrom, Paul Christiano, Owen Cotton-Barratt, Daniel Dewey, Eric Drexler, Holden Karnofsky, Luke Muehlhauser, Toby Ord, Anders Sandberg, and Carl Shulman.</p>", "user": {"username": "Nick_Beckstead"}}, {"_id": "zixjTByRiSvxYjCyW", "title": "Why a New Study of the Mariel Boatlift Has Not Changed Our Views on the Benefits of Immigration", "postedAt": "2015-10-21T05:00:00.000Z", "htmlBody": "<p><em>By David Roodman</em></p>\n<p><em>Note: Before the launch of the Open Philanthropy Project Blog, this post appeared on the <a href=\"http://blog.givewell.org/\">GiveWell Blog</a>. Uses of \u201cwe\u201d and \u201cour\u201d in the below post may refer to the Open Philanthropy Project or to GiveWell as an organization. Additional comments may be available at the <a href=\"http://blog.givewell.org/2015/10/21/why-a-new-study-of-the-mariel-boatlift-has-not-changed-our-views-on-the-benefits-of-immigration/\">original post</a>.</em></p>\n<hr>\n<p><img src=\"http://blog.givewell.org/wp-content/uploads/2015/10/Mariel_Refugees1.jpg\" alt=\"alt_text\" title=\"image_tooltip\"></p>\n<p>As a consultant for the Open Philanthropy Project last year, I <a href=\"http://davidroodman.com/blog/2014/09/03/the-domestic-economic-impacts-of-immigration/\">reviewed</a> the research on whether immigration reduces employment or earnings for workers in receiving countries. I concluded that for natives the harm, if any, is small.</p>\n<p>Last month the prominent immigration researcher <a href=\"http://www.hks.harvard.edu/fs/gborjas/\">George Borjas</a> posted a <a href=\"https://web.archive.org/web/20151007165844/http://www.hks.harvard.edu/fs/gborjas/publications/working%20papers/Mariel2015.pdf\">challenge</a> to a seminal study in my review. His new paper contends that the Mariel boatlift, which brought some 60,000 Cuban refugees to Miami in 1980, <em>did</em> profoundly affect the labor market there, depressing wages for low-education men (ones with less than a high school education) by 10\u201330%.</p>\n<p>Borjas\u2019s work is especially significant because it seems to upend a <a href=\"http://davidcard.berkeley.edu/papers/mariel-impact.pdf\">study of the boatlift</a> published by <a href=\"https://www.econ.berkeley.edu/faculty/808\">David Card</a> 25 years ago, which found little impact of all that immigration on workers in Miami. Interestingly, Borjas, who emphasizes the harm of Cuban immigration, is himself a <a href=\"https://en.wikipedia.org/wiki/George_J._Borjas#Personal_life_and_education\">Cuban emigr\u00e9</a>.</p>\n<p>I probed this dispute, replicating and checking the results in the dueling papers. I ultimately found little cause to change my views. The main reasons:</p>\n<ul>\n<li>Of the two Census Bureau data sets that Borjas relies on, the one with larger samples shows smaller impacts.</li>\n<li>According to that data set, wages for women, which Borjas excludes, <em>rose</em>, if anything, after immigration spikes (especially after a second one in 1994\u201395).</li>\n<li>I see no sharp breaks from long-term trends of the sort that could be confidently attributed to the 1980 immigration surge. The Borjas analysis appears correct that wages for low-education Miami men (defined henceforth as those with less than a high school education) were lower on average in 1981\u201383 than in 1977\u201379\u2014with the drop being larger than in most other US cities. But the data argue more for a steady long-term decline than sudden drops after immigration surges. The Borjas analysis tends to obscure this distinction by aggregating or smoothing data over several years.</li>\n<li>The original study by David Card is one of 17 covered in my review, including three others exploiting natural experiments in mass migration. None of the studies is as compelling as a randomized trial, but the overall picture\u2014of at most modest harm from substantial immigration\u2014does not change if the Card study is removed.</li>\n</ul>\n<p>Details follow.</p>\n<h2>Background</h2>\n<p>In my <a href=\"http://davidroodman.com/blog/2014/09/03/the-domestic-economic-impacts-of-immigration\">evidence review</a> on the impacts of immigration in receiving countries, I wrote:</p>\n<p><em>\u201cThere is almost no evidence of anything close to one-to-one crowding out by new immigrant arrivals to the job market in industrial countries. Most studies find that 10 percentage point increase in the immigrant \u201cstock\u201d as a share of the labor force changes natives\u2019 earnings by between \u20132% and +2% (Longhi, Nijkamp, and Poot 2005, Fig 1; Peri 2014, Pg 1). Although serious questions can be raised about the reliability of most studies, the scarcity of evidence for great pessimism stands as a fact. The economies of destination countries largely appear flexible enough to absorb new arrivals, especially given time.\u201d</em></p>\n<p>I based that conclusion in substantial part on studies that exploit natural experiments, large and sudden movements of people that have the best hope of making major side-effects of immigration obvious. It was David Card who inaugurated this literature with his 1990 paper on the Mariel boatlift. I tersely summarized that paper this way in a <a href=\"http://davidroodman.com/blog/2014/09/03/the-domestic-economic-impacts-of-immigration#Tab2\">table at the end of my review</a>:</p>\n<p><em>\u201cNatural experiment not as perfect as randomized since other events could have offset Boatlift\u2019s effects. But zero change after 7% labor supply spike most easily explained as (non-)impact.\u201d</em></p>\n<p>(Earlier on, my review also discusses the paper more fully.)</p>\n<p>The great boatlift began in April 1980 when Fidel Castro opened the Cuban port of Mariel for exit. Americans, presumably including many Cuban-Americans, provided the boats. Wikipedia<a href=\"https://en.wikipedia.org/wiki/Mariel_boatlift\"> helpfully reviews</a> the local and global events leading to this unusual episode. The major pulse of people came in May and June, and the flow was halted in September. Some 125,000 Cubans came to the US, half settling in Miami (Card, p. 246).</p>\n<p>In addition, as Borjas documents, a second wave of Cuban immigration, about half as large, arrived in 1994\u201395 (Borjas, Figure 1).<sup class=\"footnote-ref\"><a href=\"#fn-MHS9Z3d3Wt5kqo5zw-1\" id=\"fnref-MHS9Z3d3Wt5kqo5zw-1\">[1]</a></sup></p>\n<p>Card\u2019s 1990 study analyzes the data two ways, somewhat informally. The first looks at differences over time\u2014at whether wages, employment, and unemployment rose or fell for various demographic groups in Miami over 1979\u201385.<sup class=\"footnote-ref\"><a href=\"#fn-MHS9Z3d3Wt5kqo5zw-2\" id=\"fnref-MHS9Z3d3Wt5kqo5zw-2\">[2]</a></sup> The second looks at differences-in-differences. In particular, Card constructs a comparison group of four cities and looks not at whether Miami\u2019s wages, employment, and unemployment rose or fell but at whether they did so more than in the comparison group. This strategy controls for national factors such as economic recession to the extent they affected Miami and the comparison group equally. The comparison cities are Atlanta, Houston, Los Angeles, and Tampa-St. Petersburg. The algorithm for forming the control group was informal:</p>\n<p><em>\u201cThese four cities were selected both because they had relatively large populations of blacks and Hispanics and because they exhibited a pattern of economic growth similar to that in Miami over the late 1970s and early 1980s.\u201d (Card, p. 249)</em></p>\n<p>Card generally does not find that things worsened for workers in Miami after 1980, at least not more so than in the comparisons cities. The many specific checks do include exceptions. For example, among blacks in Miami, unemployment jumped from 5.6% to 9.6% between 1980 and 1981, a statistically significant increase, while it held steady at 12.6% in the control group (Card, Table 4, rows 2 &amp; 6). Narrowing to low-education blacks, unemployment did not rise in Miami relative to the control cities until 1982, which Card (p. 253) views as too late for confident attribution to the 1980 boatlift.</p>\n<p>My <a href=\"http://davidroodman.com/david/Roodman%20Borjas-Card%20replication.zip\">data and code</a> replicate the Card numbers quite well\u2014some perfectly, the rest close enough to still support Card\u2019s inferences.</p>\n<p>Borjas\u2019s retort a quarter century later draws on the same underlying data. But it differs in many respects. It limits the sample to ages 25\u201359 instead of 16\u201361, excludes women and Hispanics, favors different cities for the control group, focuses more on low-education workers, and differs in other minor ways.</p>\n<p>The study\u2019s conclusion diverges too:</p>\n<p><em>\u201cThe drop in the relative wage of the least educated Miamians was substantial (10 to 30 percent).\u201d (Borjas, p. 1)</em></p>\n<p>And Borjas is laudably transparent: you can <a href=\"http://www.hks.harvard.edu/fs/gborjas/publications/journal/JEL2015Archive.zip\">download the Stata code</a> for most of his results from his website. (Underlying data are from <a href=\"https://cps.ipums.org/cps-action/variables/group\">IPUMS CPS</a> and <a href=\"http://www.nber.org/cps/\">NBER</a>. My own <a href=\"http://davidroodman.com/david/Roodman%20Borjas-Card%20replication.zip\">data and code package</a> also contains files needed to reproduce much of the Borjas analysis.)</p>\n<p>At first, I found the Borjas conclusion quite striking: in this extreme case of immigrant influx, it seemed that vulnerable people\u2014those with little education\u2014actually had suffered, and that this had been overlooked in Card\u2019s analysis. But as I probed more, I became less convinced.</p>\n<p>To explain my thinking, I\u2019ll touch on some key differences between the two studies, then show you some graphs of annual data.</p>\n<h2>Two data sources</h2>\n<p>To my knowledge, the US Census Bureau conducts two surveys that gather national information on people\u2019s work and earnings. Both are part of the Current Population Survey (CPS) family, which originated in efforts during the Great Depression to get a firmer fix on how many people were employed and how much they were making.</p>\n<p>According to this <a href=\"http://www.nber.org/data/morg.html\">FAQ page</a>, the CPS interviews 50\u201360,000 households every month. Once a household enters the survey sample, it is interviewed once a month for four months, then left alone for eight months, then interviewed again, in the same four calendar months as before. The last of each quartet of interviews includes questions about how much household members worked and earned in the previous week. Since this information comes from people on their way out of a survey cycle, it is referred to as \u201cOutgoing Rotation Group\u201d data. ORG data undergird the government\u2019s monthly announcements of job growth and unemployment rates.</p>\n<p>The other survey is the <a href=\"https://www.census.gov/did/www/saipe/data/model/info/cpsasec.html\">Annual Social and Economic Supplement</a>, which is folded into the CPS each March and asks people how much they earned <em>last year</em>. At least in Miami during the years of greatest interest here, this survey reached about half as many people.</p>\n<p>Where Card uses ORG data, Borjas works with both data sets, in parallel. Borjas also narrows his sample more than Card, as noted above. This table counts <strong>Miamians in the two Census Bureau data sets in the core Borjas analysis and the most comparable Card analysis</strong>:</p>\n<p><img src=\"http://blog.givewell.org/wp-content/uploads/2015/10/Borjas-ORG-and-ASEC-samples3.png\" alt=\"alt_text\" title=\"image_tooltip\"></p>\n<p>Because of the larger samples, analysis based on the ORG data seems more reliable. In addition, the ORG survey records recollections of earnings last week rather than last year, which may be more accurate; on the other hand, weekly data can be noisier since some people\u2019s earnings vary substantially week-to-week. Notably, the more dramatic results in the Borjas paper come from the smaller ASEC samples.</p>\n<h2>Whether to include women</h2>\n<p>Card includes women in his analysis. Borjas does not. Among the many differences between the two, this one matters. Borjas explains his focus this way:</p>\n<p><em>\u201cIt is tempting to increase sample size by including working women, but female labor force participation was increasing very rapidly in the 1980s, so that wage trends are likely to be affected by the selection that obviously marks women\u2019s entry into the labor market.\u201d</em></p>\n<p>I am not sure what this means. The idea seems to be that forces unrelated to the Cuban influx were driving trends in women\u2019s earnings, and these could mask the effect of that influx. [<em>Update, October 21: an email from Borjas appears to confirm this reading.</em>] But similar things might be said for low-education men, whose earnings would have been influenced by broad events such as recessions and de-industrialization. Analytical devices such benchmarking against control groups and focussing on changes <em>immediately</em> after the immigration influx should seemingly work as well for women as for men.</p>\n<p>If Borjas excludes women precisely because he knows in advance that their wage trend contradicts the pattern he is looking for, who is engaging in biased selection?</p>\n<p>So it seems to me that Borjas\u2019s methods will work as well or poorly for both genders, and that both merit attention.</p>\n<h2>Sudden or gradual changes?</h2>\n<p>A third differences is that while Card reports annual figures, Borjas groups data into longer periods, in various ways. His graphs show three\u2013year moving averages. Some regressions (Borjas, Tables 5 &amp; 6) compare 1977\u201379 to 1981\u201383 and subsequent three-year periods. Others combine 1981\u201386 into a single \u201cshort-run\u201d post-boatlift period.</p>\n<p>Combining multiple years of data makes it hard to discern if wages for low-education people dropped suddenly right after the boatlift, or not until a couple of years later, or merely continued a gradual decline.</p>\n<p>Put otherwise, Borjas appears readier than Card (or me) to attribute changes 3\u20135 years after the boatlift to the boatlift. I am unready because many forces buffeted the Miami economy circa 1980\u2013an oil shock, extreme interest rates, two recessions, and a debt crisis in Latin America. To be fair, they affected cities in the control group too, but perhaps not equally. A relative wage drop 3\u20135 years after the boatlift seems to admit many explanations.</p>\n<h2>Fresh plots</h2>\n<p>To pierce the confusion, I made some graphs. They are designed to closely parallel Borjas\u2019s statistical analysis\u2014the samples, the control group, the control variables, the formulas for variables such as earnings\u2013while breaking out the data by year.</p>\n<p>The first one plots the evolution of inflation-adjusted earnings for the group in the crosshairs of Borjas\u2019s microscope: low-education, non-Hispanic, male Miami workers aged 25\u201359. The data come from the ASEC survey, which, recall, yields some 20 data points a year circa 1980. (More explanation below the graph. Click or tap it for a larger version.)</p>\n<p><img src=\"http://blog.givewell.org/wp-content/uploads/2015/10/learnwkeRealTCMeanASEC21.png\" alt=\"alt_text\" title=\"image_tooltip\"></p>\n<p>In blue are average inflation-adjusted annual earnings for these Miami workers, expressed relative to the levels in the study\u2019s start year, 1977.<sup class=\"footnote-ref\"><a href=\"#fn-MHS9Z3d3Wt5kqo5zw-3\" id=\"fnref-MHS9Z3d3Wt5kqo5zw-3\">[3]</a></sup> So a value for some year of 75% means a 25% fall since the start. The smeared blue diamonds depict the 95% confidence ranges for these averages; this novel rendition is meant to remind you that probability is not spread uniformly within those ranges, but concentrated near their centers. (David Sparks <a href=\"https://dsparks.wordpress.com/2011/02/21/choropleth-tutorial-and-regression-coefficient-plots\">invented</a> this rendition. <a href=\"http://www.soz.unibe.ch/content/ueber_uns/jann/index_ger.html\">Ben Jann</a> brings it to life with his awesome <a href=\"https://ideas.repec.org/c/boc/bocode/s457686.html\">coefplot</a>.)</p>\n<p>The graph makes it look likely that earnings in this demographic fell between the late 1970s and the early 1980s.</p>\n<p>In red, in the same graph, are average wages after adjusting for the evolving the age composition of the group and systematic differences in earnings growth or decline across cities, as well as benchmarking against a Borjas-favored comparison group (Anaheim, Rochester, Nassau-\u00adSuffolk, and San Jose).<sup class=\"footnote-ref\"><a href=\"#fn-MHS9Z3d3Wt5kqo5zw-4\" id=\"fnref-MHS9Z3d3Wt5kqo5zw-4\">[4]</a></sup> All of these adjustments mimic the Borjas analysis. The cities in the Borjas comparison group look peculiar as a collective benchmark for Miami, but were chosen because they most nearly matched Miami\u2019s 1977\u201380 employment growth rate of 15.3% (Borjas, Table 3).</p>\n<p>At any rate, these adjustments make little difference. The red diamonds\u2014shifted right of the blue ones for legibility\u2014also show a wage decline from the late 1970s to the early 1980s, with an especially big drop in 1982.</p>\n<p>Last, the green diamonds show the result of grouping data into three year-periods, again in order to copy Borjas (Borjas, Table 5 &amp; 6). Comparing green dots, wages were clearly lower in 1981\u201383 than in 1977\u201379.</p>\n<p>Borjas too makes graphs from ASEC data; they show earnings for low-education men in Miami <a href=\"https://web.archive.org/web/20151007165844/http://www.hks.harvard.edu/fs/gborjas/publications/working%20papers/Mariel2015.pdf#page=44\">nosediving in the 1980s</a>. I see two problems with those graphs. First, as I mentioned, they plot three-year moving averages, which obscure the timing of the declines in a context where timing is of the essence. Second, unlike the graph above, Brojas\u2019s graphs include no confidence intervals, so they convey false certainty to the untrained eye.</p>\n<p>My next graph switches to the more plentiful ORG data set and exploits its coverage to carry forward to 2001, the last year in the Borjas computer code.<sup class=\"footnote-ref\"><a href=\"#fn-MHS9Z3d3Wt5kqo5zw-5\" id=\"fnref-MHS9Z3d3Wt5kqo5zw-5\">[5]</a></sup> A second vertical line is added for the 1994\u201395 migration surge:</p>\n<p>migration surge:</p>\n<p><img src=\"http://blog.givewell.org/wp-content/uploads/2015/10/learnwkeRealTCMeanORG21.png\" alt=\"alt_text\" title=\"image_tooltip\"></p>\n<p>The decline circa 1980 is still visible. But it looks a bit milder now. The trough in 1983 no longer approaches the 50%-drop line. And this flattening parallels Borjas\u2019s statistical results, which are milder for ORG data (Borjas, Table 5, row 1, vs. Table 6, row 1).</p>\n<p>Nevertheless, Borjas\u2019s finding of a wage decline after the 1980 boatlift still looks reasonable, if read literally.</p>\n<p>But I find it hard, viewing these graphs, to confidently attribute that fall to some shock in 1980. The decline appears gradual and long-term. Neither graph displays a sharp, statistically significant drop immediately after the boatlift.</p>\n<p>And switching to women reveals an opposite pattern. Their earnings <em>rose</em> after immigration surges:</p>\n<p><img src=\"http://blog.givewell.org/wp-content/uploads/2015/10/learnwkeRealTCMeanORG1.png\" alt=\"alt_text\" title=\"image_tooltip\"></p>\n<p>(In contrast, the <a href=\"http://blog.givewell.org/wp-content/uploads/2015/10/learnwkeRealTCMeanASEC12.png\">corresponding graph</a> with ASEC data still shows a slight, statistically insignificant drop among women in 1981.)</p>\n<p>Not surprisingly, combining the falling men and rising women smooths things out:</p>\n<p><img src=\"http://blog.givewell.org/wp-content/uploads/2015/10/learnwkeRealTCMeanORG31.png\" alt=\"alt_text\" title=\"image_tooltip\"></p>\n<p>(And here\u2019s the <a href=\"http://blog.givewell.org/wp-content/uploads/2015/10/learnwkeRealTCMeanASEC31.png\">corresponding graph</a> from ASEC data.)</p>\n<p>Notice that in 1977\u201382, the blue diamonds for average wages are statistically indistinguishable, as are the red ones for the adjusted means. It is only in 1983 that a larger drop emerges, and even it is transient, indistinguishable from noise. Meanwhile, wages rise, if anything, after the 1994\u201395 immigration surge.</p>\n<p>I believe these graphs provide a clearer picture than the Borjas paper of the underlying data. They do not make a compelling case for harmful side effects of immigration.</p>\n<p>Code and data replicating Borjas, Card, and my own analysis are <a href=\"http://davidroodman.com/david/Roodman%20Borjas-Card%20replication.zip\">here</a>. Also included is a larger set of graphs generated by that code, covering hourly wages, employment, and unemployment.</p>\n<p>All this graphical analysis is a bit loose, but the graphs come from formal regressions, numerical results from which are in <a href=\"http://davidroodman.com/david/Borjas%20replication%20tables.pdf\">two tables</a>. These check whether annual and hourly wages and, in the ORG data, employment and unemployment changed significantly in Miami from 1980 to 1981, 1982, or 1983, as well as from 1977\u201379 to 1981\u201383. They show changes with little significance by 1981 but more significance by 1982.</p>\n<p>Possibly the Mariel boatlift did swamp the Miami labor market, but with a two-year delay. But these results are also consistent with the continuation of long-term trends in Miami. I see no strong basis for implicating immigration.</p>\n<p>Perhaps I interpret the data differently because of a difference in priors. If I <em>assume</em> that immigration is a potentially leading force in labor markets, then I may be ready to blame the 1980 boatlift for the exceptional wage drop in Miami in the late 1980s, depicted above. But that comes close to assuming the conclusion. If I instead come to the data with more skepticism, mindful that many idiosyncratic forces impinge upon local labor markets, then graphs like that will not change my mind. Only a sharp drop soon after the boatlift would force me to reconsider.</p>\n<p>In its favor, Borjas\u2019s work has accentuated my appreciation of the limitations of Card\u2019s study. Even when looking short-term, over a year or so after a major immigration flow, many factors may disturb labor markets enough to obscure the effects of immigration. There is a lot of noise in the data. The appearance of no real effect is not conclusive.</p>\n<p>But my previous, terse summary still reads about right to me: \u201cNatural experiment not as perfect as randomized since other events could have offset Boatlift\u2019s effects. But zero change after 7% labor supply spike most easily explained as (non-)impact.\u201d More broadly, the overall impact of immigration on workers in the receiving country looks modestly negative at worst, with the most vulnerable group being <em>recent</em> immigrants. And there is little doubt that migrants from poor countries as a group <a href=\"https://web.archive.org/web/20150826064431/http://www.cgdev.org/files/1425376_file_Clemens_Economics_and_Emigration_FINAL.pdf\">gain immensely</a> by migrating.</p>\n<h2>Notes</h2>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-MHS9Z3d3Wt5kqo5zw-1\" class=\"footnote-item\"><p>Thus, as Borjas points out, \u201c<a href=\"https://web.archive.org/web/20151007200927/http://personal.psc.isr.umich.edu/yuxie-web/files/soc543-2004/Angrist-Krueger1999.pdf#page=53\">The Mariel Boatlift That Did Not Happen</a>\u201d actually did happen. For clarification of that cryptic statement, see <a href=\"http://davidroodman.com/blog/2014/09/03/the-domestic-economic-impacts-of-immigration/\">my discussion of Card (1990)</a>. <a href=\"#fnref-MHS9Z3d3Wt5kqo5zw-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MHS9Z3d3Wt5kqo5zw-2\" class=\"footnote-item\"><p>Unemployment is the fraction of those in the labor force\u2014defined as those who have or want work\u2014who are not working. Employment is the fraction of working-age adults who are working. <a href=\"#fnref-MHS9Z3d3Wt5kqo5zw-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MHS9Z3d3Wt5kqo5zw-3\" class=\"footnote-item\"><p>Results come from a regression of log inflation-adjusted annual earnings, expressed per week worked, on year dummies. Results are exponentiated for presentation, producing geometric averages. Copying Borjas, the sample is male, non-hispanic workers aged 25\u201359; \u201ctop-coded\u201d earnings are multiplied by 1.5; those with hourly wages below $1.50 or above $40 in 1980 dollars are excluded; Census Bureau\u2013supplied weights are incorporated; and standard errors are heteroskedasticity-robust. <a href=\"#fnref-MHS9Z3d3Wt5kqo5zw-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MHS9Z3d3Wt5kqo5zw-4\" class=\"footnote-item\"><p>Regressions are the same as before, but now including dummies for years, the four control cities, and 5-year age groups. <a href=\"#fnref-MHS9Z3d3Wt5kqo5zw-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MHS9Z3d3Wt5kqo5zw-5\" class=\"footnote-item\"><p>The dependent variable is the log of last week\u2019s earnings. Following Borjas, the sample is male, non-hispanic workers aged 25\u201359; \u201ctop-coded\u201d earnings are multiplied by 1.5; and those with hourly wages below $1.50 or above $40 in 1980 dollars are excluded. <a href=\"#fnref-MHS9Z3d3Wt5kqo5zw-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "EA Forum Archives"}}, {"_id": "6gRq6tguStfcAre6L", "title": "History of Philanthropy Case Study: The Founding of the Center for Global Development", "postedAt": "2016-06-15T15:48:22.913Z", "htmlBody": "<p>Ben Soskis, a consultant who has been working with us as part of our <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">History of Philanthropy project</a>, recently finished a case study on the founding and growth of the <a href=\"http://www.cgdev.org/\">Center for Global Development (CGD)</a>, a think tank that conducts research on and promotes improvements to rich-world policies that affect the global poor.</p><p>We are very impressed with CGD and its accomplishments. Earlier this year, we announced a <a href=\"https://www.openphilanthropy.org/focus/us-policy/miscellaneous/center-global-development-general-support-2016\">$3 million grant to CGD</a>. Our writeup includes a <a href=\"https://www.openphilanthropy.org/focus/us-policy/miscellaneous/center-global-development-general-support-2016#Track_record\">review of CGD\u2019s track record</a>, concluding that \u201cit seems reasonable to us to estimate that CGD has produced at least 10 times as much value for the global poor as it has spent, though we consider this more of a rough lower bound than an accurate estimate.\u201d</p><p>CGD appears to be an example of a <a href=\"https://www.openphilanthropy.org/blog/funder-initiated-startups\">funder-initiated startup</a>: philanthropist Ed Scott was the driving force behind the original high-level concept, and he recruited Nancy Birdsall to be CGD\u2019s leader.</p><p>There are a number of similarities between this case study and our <a href=\"https://www.openphilanthropy.org/blog/history-philanthropy-case-study-founding-center-budget-and-policy-priorities\">recent case study on the Center on Budget and Policy Priorities (CBPP).</a> In particular, both cases involved a funder exploring an extensive personal network, finding a strong leader for the organization they envisioned, committing flexible support up-front to that leader, and then trusting the leader with a lot of autonomy in creating the organization (while retaining a high level of personal involvement throughout the early years of the organization). However, there are a couple of major points of contrast: the crowdedness of the landscape and the amount of money committed.</p><p><strong>Amount committed</strong>: in the <a href=\"https://www.openphilanthropy.org/blog/history-philanthropy-case-study-founding-center-budget-and-policy-priorities\">case of CBPP</a>, the founding funder \u201ccommitted $175,000 in the first year [1981] and $150,000 in the second year, and asked CBPP to find other supporters so it could reduce its support after that point.\u201d In the case of CGD, the founding funder committed $25 million, to be disbursed over five years, in 2001.</p><p><strong>Crowdedness of the field:</strong> while CBPP appears to have had a unique vision from the start, the original vision for CGD overlapped significantly with the missions of multiple other global-development-oriented think tanks, including the Wolfensohn Center for Development at the Brookings Institution (founded in 2005 and closed in 2010) and the Overseas Development Council (established in 1969 and closed in 2000, a year before CGD started). There are a number of possible explanations that CGD appears to have had unusual success, including the fact that it was started at an opportune time (as general interest in relevant issues were increasing) and the fact that its leader (Nancy Birdsall) eventually developed a unique vision for the institution. Another possible explanation highlighted by Dr. Soskis is the generous amount committed from the start, with few strings attached:<br>&nbsp;</p><blockquote><p><i>Interviewees returned to this initial pledge on several occasions in highlighting what they believed to be a significant factor in CGD\u2019s rise. The commitment signaled that the new institution would have the resources to survive beyond the short-term, that it would have staying power, a crucial inducement in attracting top talent (especially considering ODC\u2019s demise). As CGD\u2019s Todd Moss explains, \u201ctrying to get top class people without having money in the bank can be difficult, because people are not going to leave a good job at the World Bank or a university if they think you might not be able to pay them in two years.</i></p></blockquote><p>Dr. Soskis also discusses specific contrasts with other think tanks in the same space that didn\u2019t have comparable funding, and experienced problems such as getting \u201c\u2009\u2018sucked into essentially acting like consulting firms\u2019 \u2026 bidding on short-term contracts from the UN, multilateral or governmental agencies\u201d rather than working on independent projects that could have outsized impact and pull in new donors.</p><p>Between this case study and our <a href=\"https://www.openphilanthropy.org/focus/us-policy/miscellaneous/center-global-development-general-support-2016#Track_record\">review of CGD\u2019s track record</a>, we\u2019ve now assembled a full story - spanning 16 years - of a philanthropist having outsized impact by making a long-term bet on a new institution.</p><p><a href=\"https://www.openphilanthropy.org/files/History_of_Philanthropy/CGD/Case_Study_CGD_Founding.pdf\">Read the full case study here (.pdf)</a></p>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "RavBn9FumPFyd3izB", "title": "History of Philanthropy Case Study: The Founding of the Center on Budget and Policy Priorities", "postedAt": "2016-05-20T15:30:25.853Z", "htmlBody": "<p>Suzanne Kahn, a consultant who has been working with us as part of our <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">History of Philanthropy project</a>, recently finished a case study on the founding and growth of the <a href=\"http://www.cbpp.org/\">Center on Budget and Policy Priorities</a> (CBPP), a well-regarded D.C. think tank that focuses on tax and budget policy with an aim of improving outcomes for low-income people.</p><p>We were interested in learning more about the history and founding of CBPP because:</p><ul><li>It appeared to be a successful institution that <a href=\"https://www.openphilanthropy.org/blog/funder-initiated-startups\">funders played a central role in helping to establish</a>.</li><li>Relative to many other think tanks, CBPP appears to have an <a href=\"http://files.givewell.org/files/conversations/Greenstein%204-3-14%20(public).pdf\">unusually clear and concrete track record</a>.</li><li>We\u2019ve <a href=\"http://www.openphilanthropy.org/focus/us-policy/macroeconomic-policy/center-budget-and-policy-priorities-full-employment-project\">supported</a> CBPP\u2019s Full Employment Project.</li></ul><p>The report\u2019s account of CBPP\u2019s history and founding mirrors, in many ways, that of <a href=\"http://www.cgdev.org/blog/reflections-and-what%E2%80%99s-ahead%E2%80%94cgd-founding-board-chair-ed-scott\">Center for Global Development</a> (the subject of another upcoming case study). In particular:</p><ul><li>It was a funder (in this case Dick Boone, Executive Director of the Field Foundation, along with his associate Robert Stein) who first proposed the basic idea of the new organization, and approached another person (in this case Bob Greenstein) to become the founder and leader of the organization.</li><li>The funder seems to have had little doubt about whom to approach: \u201cStein recalls that as they began to discuss providing seed funding for the new organization they had conceived, he and Boone never considered anyone but Greenstein for the position of executive director.\u201d As in the case of Center for Global Development, it appears that the funder was well-connected in policy circles, and that this was helpful in identifying the right person to approach.</li><li>The funder committed up-front support without formal restrictions and gave the founder wide latitude.</li><li>I have the general impression that the funder put a great deal of time and energy into the organization in its early years, despite ultimately leaving most important decisions in the hands of the founder.</li></ul><p>A couple of other things that struck me in this case study:</p><ul><li>The Field Foundation had previously worked with Bob Greenstein on a smaller project, and this was apparently an important input into its confidence in him. I think this is a good example of how grantmaking can improve one\u2019s networks and lay the groundwork for future work.</li><li>The report notes that the Field Foundation committed $175,000 in the first year and $150,000 in the second year, and asked CBPP to find other supporters so it could reduce its support after that point. I\u2019m struck by the relatively small size of these grants, given that they were for a new organization led by an accomplished, senior founder; I would be surprised if grants of this size (even adjusted for inflation) could have a similar effect today. (By contrast, <a href=\"http://www.insidephilanthropy.com/home/2014/4/22/more-bang-for-the-wonk-how-the-center-for-global-development.html#\">Center for Global Development had a $25 million commitment when it was started in 2001.</a>)</li></ul><p><a href=\"https://www.openphilanthropy.org/files/History_of_Philanthropy/CBPP/Case_Study_Center_on_Budget_and_Policy_Priorities.pdf\">Read the full case study here (.pdf)</a></p>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "LuErSN9R7t8PgcXrR", "title": "History of Philanthropy Case Study: The Impact of Philanthropy on the Passage of the Affordable Care Act", "postedAt": "2015-09-01T10:25:09.884Z", "htmlBody": "<p><i>Note: Before the launch of the Open Philanthropy Project Blog, this post appeared on the </i><a href=\"http://blog.givewell.org/\"><i>GiveWell Blog</i></a><i>. Uses of \u201cwe\u201d and \u201cour\u201d in the below post may refer to the Open Philanthropy Project or to GiveWell as an organization. Additional comments may be available at the </i><a href=\"http://blog.givewell.org/2015/09/02/history-of-philanthropy-case-study-the-impact-of-philanthropy-on-the-passage-of-the-affordable-care-act/\"><i>original post</i></a><i>.</i></p><p>Benjamin Soskis, who has been working for us on our <a href=\"https://www.openphilanthropy.org/research/history-of-philanthropy\">history of philanthropy project</a>, has completed a <a href=\"http://www.givewell.org/files/HofP/The_Impact_of_Philanthropy_on_the_Passage_of_the_Affordable_Care_Act.pdf\">case study</a> of philanthropy\u2019s impact on the 2010 passage of the Affordable Care Act (ACA).</p><p>The case study focuses first on the Atlantic Philanthropies\u2019 funding of Health Care for America Now! (HCAN), as well as on HCAN\u2019s activities and impact. The second part of the study surveys the activities of other funders involved in health care reform, such as the Robert Wood Johnson Foundation, the Kaiser Family Foundation, and the Commonwealth Fund.</p><p>The case study concludes that, as a whole, philanthropic spending had a critical, though not necessarily easily quantifiable, role in the passage of the ACA. In the following passage, Dr. Soskis quotes HCAN\u2019s Doneg McDonough:</p><blockquote><p><i>There\u2019s just no way health reform would have passed without the [philanthropically funded] outside efforts going on. No question about it. Beyond that, it gets a little fuzzy. How much of an impact did [any particular intervention] have and which things actually were critical to making the ACA happen?\u201d</i></p><p><i>This last statement, with its combination of broadly conceived certitude and localized indeterminacy, epitomizes one of this report\u2019s central findings regarding the claims of philanthropic impact. </i>(Case Study, Pg. 4)</p></blockquote><p>Dr. Soskis\u2019s study also examines the difficulty of disentangling the impact of any one funder from the impact of philanthropy as a whole. He writes:</p><blockquote><p><i>In fact, disaggregating the specific contributions of particular philanthropic funders and determining how to weigh them against each other proved one of the most significant challenges of this project. This would be an issue for any major policy initiative, but for national [health care reform], given the large number of funders involved and the efforts to coordinate activities between them, it proved even more challenging. This suggests one of the main paradoxes of evaluating the impact of philanthropy on the passage of health care reform legislation. Precisely those features which many considered essential to the passage of the ACA \u2013 the breadth, variety, and scale of philanthropic initiatives \u2013 also made it especially difficult to evaluate the contributions of any particular intervention. And the report highlights another paradox as well, one which presides over the entire study of policy impact evaluation: the more significant the legislative achievement, and the greater the impulse for various stakeholders involved to claim a definite degree of impact, the less likely it is that any determination of clear causal agency is actually possible. </i>(Case Study, Pg. 4)</p></blockquote><p><a href=\"http://www.givewell.org/files/HofP/The_Impact_of_Philanthropy_on_the_Passage_of_the_Affordable_Care_Act.pdf\">Read the full case study here (.pdf)</a></p>", "user": {"username": "Milan_Griffes"}}, {"_id": "tinYGn3ehLXQs6A9c", "title": "Conclusion of the Replacing Guilt series", "postedAt": "2016-02-28T08:00:00.000Z", "htmlBody": "<p>Today marks the end of my series on replacing guilt (<a href=\"http://mindingourway.com/guilt\">table of contents</a>).</p><p>I <a href=\"http://mindingourway.com/replacing-guilt/\">began the series</a> by discussing the \"restless guilt,\" that people feel when some part of them thinks they aren't doing what's important. I argued that it's possible to <a href=\"http://mindingourway.com/the-stamp-collector/\">care about things outside yourself</a>, and things <a href=\"http://mindingourway.com/caring-about-some/\">larger than yourself</a>, no matter what a nihilist tells you.</p><p>In the second arc of the series I implored readers to <a href=\"http://mindingourway.com/should-considered-harmful/\">drop their obligations</a> and ask themselves where they would put their efforts if there was nothing they felt they \"should\" be doing. If you can drop your sense of obligation and still <a href=\"http://mindingourway.com/on-caring/\">care hard</a> for something larger than yourself, you are well on your way to dispensing with guilt-based motivation.</p><p>In the third arc, I described techniques for building and maintaining a powerful intrinsic drive without the need to spur yourself with guilt. I point out that <a href=\"http://mindingourway.com/stop-before-you-drop/\">working yourself ragged is not a virtue</a>, and that the \"work too hard then rest a long time\" narrative is a <a href=\"http://mindingourway.com/rest-in-motion/\">dangerous narrative</a>. We can't always act as we wish we could: We're <a href=\"http://mindingourway.com/not-yet-gods/\">not yet gods</a>, and it's often easier to change our behavior by <a href=\"http://mindingourway.com/dont-steer-with-guilt/\">exploring obstacles with experimentation and creativity</a> instead of attempting to berate and guilt ourselves into submission. I plea for <a href=\"http://mindingourway.com/self-compassion/\">self compassion</a> and argue that <a href=\"http://mindingourway.com/there-are-no/\">there are no \"bad people\"</a>.</p><p>In the <a href=\"http://mindingourway.com/being-unable-to-despair/\">fourth arc</a>, I describe ways to draw on the fact that the world around you is broken as fuel for your intrinsic drive. If, when given the choice between \"bad\" and \"worse\" you can <a href=\"http://mindingourway.com/choose-without-suffering/\">choose \"bad\" without suffering</a>; if you can <a href=\"http://mindingourway.com/simply-locate-yourself/\">be content in your gambles</a> while <a href=\"http://mindingourway.com/have-no-excuses/\">having no excuses</a> and <a href=\"http://mindingourway.com/come-to-your-terms/\">coming to terms with the fact that you may fail</a>, then it becomes easy to <a href=\"http://mindingourway.com/transmute-guilt-i/\">transmute your guilt into resolve</a> and struggle hard to make the future as bright as you can make it.</p><p>In the <a href=\"http://mindingourway.com/stop-trying-to-try-and-try/\">fifth and final arc</a>, I describe mindsets and mental stances from which guilt seems an alien concept. Primary among them are \"<a href=\"http://mindingourway.com/confidence-all-the-way-up/\">confidence all the way up</a>\", the skill of believing in your capabilities while not being overly sure of anything; and <a href=\"http://mindingourway.com/desperation/\">desperate</a> <a href=\"http://mindingourway.com/recklessness/\">recklessness</a> <a href=\"http://mindingourway.com/defiance/\">defiance</a>, the three dubious virtues of those with strong intrinsic drive.</p><p>I conclude with a few words on <a href=\"http://mindingourway.com/how-we-will-be-measured\">how we will be measured</a>: When all is said and done, Nature will not judge us by our actions; we will be measured only by what <i>actually happens.</i> Our goal, in the end, is to ensure that the timeless history of our universe is one that is filled with whatever it is we're fighting for. For me, at least, this is the underlying driver that takes the place of guilt: Once we have learned our lessons from the past, there is no reason to wrack ourselves with guilt. All we need to do, in any given moment, is look upon the actions available to us, consider, and take whichever one seems most likely to lead to a future full of light.</p>", "user": {"username": "So8res"}}, {"_id": "6vQWXkbMq4MN26zHc", "title": "How we will be measured", "postedAt": "2016-02-21T06:00:00.000Z", "htmlBody": "<p>After nearly a year of writing, my \"replacing guilt\" sequence is coming to a close. I have just one more thing to say on the subject, by pointing out a running theme throughout the series.</p><p>When all is said and done, and Nature passes her final judgement, <a href=\"http://mindingourway.com/stop-before-you-drop/\">you will not be measured by the number of moments in which you worked as hard as you could</a>. You will not be judged by someone rooting around in your mind to see <a href=\"http://mindingourway.com/there-are-no/\">whether you were good or bad</a>. You will not be evaluated according to how <a href=\"http://mindingourway.com/have-no-excuses/\">unassailable your explanations are</a>, for why the things that you couldn't possibly have prevented the things that went wrong.</p><p>You will be measured only by what <i>actually happens,</i> as will we all.</p><p>That doesn't mean all of us are using the same measuring stick: Some people are working to ensure that our universe-history is one in which they in particular have a happy and fulfilled life; others are working to ensure that our universe-history is one in which their children never have to debase themselves to survive. Still others look wide, and see poverty and destitution and suffering, and work to ensure that those blemishes fade from our universe-history, in the places they can reach, near the time of their lives. Others look far forward, working to ensure that our universe-history is full of flourishing sentient civilizations and other nice things.</p><p>All it means is that the <i>type</i> of thing we're all trying to do, one way or another, is ensure that the actual history of our universe, the actual timeless structure of the place we're embedded, is as desirable as possible. That's the type of game we're playing: We manipulate universe-histories, for the sake of the future.</p><p>Some people have a listless guilt, thinking that nothing matters but feeling vaguely restless as they watch themselves spend their lives on things they think are pointless. Other people have a pointed guilt, thinking that <i>everything</i> matters, and berating themselves whenever they fall short of perfection. For me, the framing that <i>we act to determine the shape of our actual universe-history</i> is a framing that avoids both these pitfalls. Is there a way you want the completed, timeless story of our universe to go? Then act to ensure that the future is as good as you can make it. Are you wracked with guilt about your inability to act as you wish, or regret for the things you did in the past? Then act to ensure that the future is as good as you can make it. That's the sort of game we're playing: At all times, act to ensure that our future is bright.</p><hr><p>I think many people get a bit mixed up about what type of game we're playing. They get stuck playing a social game, measuring their accomplishments by comparison to the accomplishments of their neighbors; or they <a href=\"http://mindingourway.com/half-assing-it-with-everything-youve-got/\">mistake someone else's expectations for their preferences</a> and get stuck chasing <a href=\"http://lesswrong.com/lw/le/lost_purposes/\">lost purposes</a>; or someone slights them and their vision narrows as their sole objective becomes <i>retaliation.</i></p><p>I'm not saying social goals are intrinsically bad. Wealth and status are useful aids when it comes to determining the future; the accomplishments and expectations of your peers can provide useful measurements of your abilities. But there's a difference between pursuing social goals for the sake of determining the course of our universe-history, and forgetting entirely that success is measured in terms of what actually happens throughout the course of history.</p><p>I alluded to this when I described <a href=\"http://mindingourway.com/defiance/\">defiance</a> as \"choosing self-reliance.\" At the end of the day, each and every one of us is engaged in a personal struggle to determine the future. We are not <i>alone;</i> there are many around us who can be friends and allies and support us in our struggle. But the goal, in the end, is to use what resources we have at our disposal to ensure that the universe-history is filled with light, whatever our light may be. I hope yours includes friends and family and loved ones, but <i>making it happen</i> \u2014 that is your personal task. You are encouraged to draw on the support of friends and allies where possible; and ensuring that you have close connections may be one of the properties you're putting into the timeless history of our universe: But even then, the task of <i>ensuring our universe-history is one in which you have close connections</i> is your personal task.</p><hr><p>What we are doing, on this earth, is acting in such a way that our future is filled with light. From this framing, \"guilt-based motivation\" is a foreign concept: If you start to feel guilty, simply <a href=\"http://mindingourway.com/be-a-new-homunculus/\">look at your situation with fresh eyes</a>, and then act such that the future is filled with light. Our lives are not status competitions; the world is not a proving ground. We are participating in a <i>gambit for the future</i> (or, more likely, a gambit for the shape of the multiverse), and that is all.</p><p>When there are people who oppose us out of nothing save for petty spite; when there are obstacles that stand between us and something important to us which seem all but insurmountable; when we encounter personal limitations that prevent us from acting as we wish to; it is easy to confuse retaliation, overcoming adversity, and growing stronger, with our actual goals. But crossing those hurdles is not the final objective: those hurdles are only parameters in our calculations about how to affect the future; they are nothing but the state of the game board in a game with cosmic states.</p><p>In that game, some people have stronger positions than others, and more leverage with which to determine the timeless story of our universe. Life isn't fair. But all of us, one way or another, are here to make sure that our universe history is filled with light \u2014 whatever 'light' may be to each of us.</p><p>So find allies, find friends, find everything you need to improve your ability to ensure that our universe-history tells a story you like. Move towards whatever levers on our future you can find. And then <i>fill it with light.</i></p>", "user": {"username": "So8res"}}, {"_id": "Z4QD4gwWbhEYwZDof", "title": "Defiance", "postedAt": "2016-02-14T06:00:00.000Z", "htmlBody": "<p>The third dubious virtue is defiance. As with the other <a href=\"http://mindingourway.com/desperation/\">dubious</a> <a href=\"http://mindingourway.com/recklessness/\">virtues</a>, it can get you into trouble. <a href=\"http://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear\">Remember the law of equal and opposite advice</a>. Used correctly, it can play a key role in a healthy guilt-free motivation system.</p><p>I used to tell people that I'm roughly 90% defiance-fueled. The most common response was \"ha ha I guess you can be manipulated by reverse psychology, then\"; which led me to realize that I didn't yet know how to convey what I meant by \"defiance fuel,\" so I stopped saying it. Today, we see whether I can convey what I mean by \"defiance fuel\" yet.</p><p>Most people I talk to about defiance think of it as a mental stance adopted against some authority figure. Perhaps they imagine a parental figure saying \"finish your broccoli,\" and a child who hates broccoli with their jaw set and smolder in their eyes, who proceeds to eat with as much petulance as they can muster, plotting their revenge. The feeling we imagine in that child is perhaps the standard central example of \"defiance.\"</p><p>I claim that that child does possess defiance-the-virtue, but not in their petulance, and not in their opposition to an authority figure. Defiance-the-action is in the child chewing with their mouth open in an open refusal to submit; defiance-the-virtue is in the mental actions they make <i>before</i> they start chewing with their mouth open. It's in the internal steeling they do when deciding not to be ordered around. It's in their decision to be self-reliant, it's in their refusal to take orders lying down. If these automatic and subconscious mental motions were verbalized, they might be written \"I am my own person; and not beholden to your whims,\" or \"if you push me, I push back.\" But they aren't verbalized, because they aren't conscious. They're reflexive.</p><p>Defiance-the-virtue is about encountering a badness that's brewing in the world, and <i>reflexively</i> doing everything you can to throw a wrench in the works, to twist things in your favor. Defiance-the-virtue is about taking nothing lying down, and refusing to let badnesses in the universe slide.</p><p>Defiance isn't about acting petulantly without hesitation: A defiant child might bide their time, knowing that if they act rashly there will be harsh consequences. Defiance is about <i>resisting the default state of affairs</i> without hesitation: A defiant child might weigh their options and bide their time, but at no point do they wonder whether they should defy. They simply dislike the situation, and so rebel against it.</p><p>Defiance-the-virtue is about having <i>that</i> reaction, to something that's wrong in the world.</p><hr><p>Of course, there's an art to defying the right things. I do recommend <a href=\"http://mindingourway.com/the-value-of-a-life/\">defying death</a>; I don't recommend having the \"defiance\" reaction against people who tell you to do things in a stern and authoritative voice. People who order you around can either be ignored or obeyed according to the social context, but they aren't usually worth <i>defying,</i> except perhaps in situations where you legitimately need to demonstrate that you're not beholden to them, and where gentler reminders have failed.</p><p>As a rule of thumb, I suggest that it's usually healthy to have a defiance reaction towards <i>states of the world</i>, and usually unhealthy to have a defiance reaction towards <i>people.</i></p><p>To illustrate the difference, imagine you're Neo, twenty years after the first <a href=\"https://en.wikipedia.org/wiki/The_Matrix\">matrix movie</a>. The sequels never happened; instead you got trapped in the matrix while one by one, all your connections to the outside world died or disappeared. One day, you lost your grasp on your ability to control the matrix, your abilities slipping through your grasp like lucidity slipping away in a dream. Now you stand atop a skyscraper, looking across the gap at its twin, unable to quite recall what it was like to fly.</p><p>You stand there frozen, desperate to recall what you once knew, finding it evasive. Behind you, someone else enters the rooftop and shouts at you over the wind.</p><p>\"What the hell are you doing, you idiot?\" they cry. \"Get back from there! Now!\"</p><p>Defiance-against-a-person would be to feel a burning need to show this person up, show them that you're not beholden to their demands, and possibly do something rash.</p><p>Defiance-against-the-world would be to hear this person cry out, and use the impetus to remember what it was you used to know. You would say, \"Oh, right. I'm in the matrix.\" You would remember that the rules and customs of this place do not have dominion over you, no matter what illusions the people around you are taken in by. Your mind would snap back into focus. You would grab what you had forgotten how to grasp, and leap.</p><p>(And those with defiance-the-virtue deeply instilled in them don't need the impetus provided by another person to access the mental state \u2014 defiance is a property of the relationship between them and the state of the world that they can recall at will, not a property of the relationships between them and others.)</p><hr><p>This is the defiance I mean to talk about. It's related to <a href=\"http://mindingourway.com/deregulating-distraction-moving-towards-the-goal-and-level-hopping/\">level hopping</a> and skepticism about your limitations. It's related to the skill of measuring your progress not against others, but against what actually happens.</p><p>I've been writing a long sequence of posts on how to replace guilt-based motivation with something else. Many people have remarked to me that my writings on averting guilt seem inspired by Taoism. And: maybe. There are some parallels. But not here, not with defiance.</p><p>Defiance is not about coming to terms with the world. It's about looking looking at the world and having the same mental reflexes as the defiant child. It's about the reflexive impulse to say \"screw this\" and choose self-reliance over hopelessness in the face of problems that are crushingly large. It's about a deep-seated inability to go gently into that good night. It's about being able to look at the terrible social equilibria we're all trapped in and get <i>pissed off</i> \u2014 not because any individual is evil, but because almost nobody is evil and everything is broken anyway.</p><p>Above all, it's about seeing that the wold is broken, and <i>feeling</i> something akin to \"fuck these mortal constraints, I'm <i>fixing things.</i>\"</p><p>When the defiant child eats their vegetables with as much spite as is humanly possible, there was never a <i>thought that crossed their mind</i> about capitulating to their parents. Petulance was an <i>automatic response.</i> They weren't carefully weighing a decision about whether to spite their parents \u2014 at best, they may have carefully weighed a decision about whether to get their payback now, overtly; or later, subtly. The defiance was a <i>reflex;</i> the fact that they weren't going to submit quietly to authority was never in question.</p><p>Defiance-the-virtue is about having the same reflexive response, not towards an authority figure, but towards <i>the state of a </i><a href=\"http://mindingourway.com/the-value-of-a-life/\"><i>broken world</i></a>. It's about making the fact that you struggle to fix broken worlds <i>automatic and unspoken</i> \u2014 you might weigh your options and bide your time, but you spare no thought for <i>whether you will struggle.</i></p><p>I don't know how to teach defiance, but it's one of the keystones of my motivation system. If you want to build yourself a motivation system akin to mine, defiance is an important component.</p><p>So this is how I suggest motivating yourself in place of guilt: Let the wrongness of the world trigger something deep inside of you, such that the question stops being whether you will capitulate or lose hope, and becomes <i>how you will wrest the course of the future onto a different path.</i> See the current state of affairs as your adversary; see the future as the prize that hangs in the balance. Shake off the illusory constraints, set your jaw, and rebel. Defy.</p><p>Allow yourself to be a little <a href=\"http://mindingourway.com/recklessness/\">reckless</a>. Get a little <a href=\"http://mindingourway.com/desperation/\">desperate</a>. Let defiance of the way things are burn in you. Then <i>act.</i></p>", "user": {"username": "So8res"}}, {"_id": "PfF5FRt38MXpoirfy", "title": "Recklessness", "postedAt": "2016-02-02T06:00:00.000Z", "htmlBody": "<p>The second dubious virtue is recklessness. As with <a href=\"http://mindingourway.com/desperation\">desperation</a>, there are many bad ways to be reckless. There is a nihilistic recklessness, in those with a muted ability to feel and care, that is self-destructive. There is a social recklessness, when peers push each other towards doing something dangerous that none of them would do alone, in a demonstration of commitment that can become needlessly dangerous. And there is a fiery, destructive recklessness in those too quick to anger, which can lead people to actions they will regret for a lifetime. I caution against all these types of recklessness.</p><p>Nevertheless, there is a type of recklessness that is a virtue. This is <i>recklessness in the pursuit of an external goal,</i> and I have found it to be rather rare.</p><p>I get a lot of questions from people about how cautious they should be as they make changes in their lives. If they remove their guilt motivation, will they be able to do anything at all? If they really try to understand how screwed up the world is, on a gut level, will they break? If they devote their efforts to the pursuit of something larger than them, will they lose touch with their humanity, and with their ability to connect to other human beings?</p><p>And I tend to answer: You are not made of glass.</p><p>Dive in. Change things. Fix problems. If more problems crop up, fix those too.</p><p>Imagine that you look upon yourself, detect harmful guilt-based motivation, tear it out, and then notice that this leaves you with a Zen-like lack of drive, such that most of yourself is now happy to let days slip by but some small part of you is crying out that something is wrong. Recklessness-the-virtue is about being in that state and deciding to push <i>forward</i> rather than retreating; deciding to make a desperate effort to acquire a new drive, rather than panicking and retreating back towards guilt.</p><p>Recklessness is about ripping off the blinders that prevent you from <a href=\"http://mindingourway.com/see-the-dark-world/\">seeing the dark world</a> on a gut level, and knowing that if this happens to be debilitating then you'll find some new way to handle it, rather than being forced to retreat.</p><p>Always forward, never back. Be <a href=\"http://mindingourway.com/being-unable-to-despair/\">unable to despair</a>. Have <a href=\"http://mindingourway.com/confidence-all-the-way-up/\">confidence all the way up</a>. Think of all the people you know who are too stagnant, too cautious about breaking something important, to ever change at all.</p><p>You can recover from breaking a few parts of yourself, so long as you're modular rather than fragile. You can become able to roll with a few punches.</p><hr><p>(This seems like a good time to insert a heavy-handed reminder about the law of <a href=\"http://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear\">equal and opposite advice</a>! Many people would do well to gain a little recklessness, but many others need <i>less</i> recklessness and <i>more</i> caution. If you're in a particularly fragile mental state, consider disregarding this post entirely.)</p><hr><p>During my undergraduate education, I was the president of an entrepreneurship club. The first most common type of person who would drop by asking for advice was that young wannabe founder all full of na\u00efve excitement about some half-formed notion that they're about to make the next facebook. The second most common person was that competent programmer with an idea that wasn't half-bad \u2014 maybe they had some idea for an app that would let couples communicate in a way they couldn't yet easily do, six years ago \u2014 but, being tempered and level-headed and well aware of the na\u00efvety of the first folks, were entirely unable to <i>commit</i> to their idea.</p><p>Both sets of prospective entrepreneurs were doomed to failure. The first set, for all the obvious reasons \u2014 they'd focus too narrowly on writing code that no one would ever buy, or fail to find their first users, or fail to make a minimum testable product, or they'd dramatically misunderstand and underestimate the difficulty of the technical challenges, or whatever.</p><p>The second set would fail because they didn't really expect themselves to succeed. They could <i>make</i> themselves work on their idea, while reciting to themselves some story about being risk-loving, but they couldn't get their head <i>into</i> the idea, to the point where they were spending fourteen hours a day working feverishly while plans and paths and strategies dominated their waking thoughts.</p><p>There's a fugue state that successful entrepreneurs report entering, which the second set of people had rendered themselves unable to enter. Somehow, their realistic understating of their odds destroyed their ability to commit.</p><p>In one fashion, this makes some sense: they, knowing that great success is likely a lie, cannot fool their innermost self into believing in their own vision, which precludes them from entering the fugue state.</p><p>But in another fashion, is silly. What do the <i>odds</i> have to do with your <i>ability to commit?</i> Why is their <i>epistemic</i> state preventing them from entering the <i>emotional</i> state that would most help them succeed?</p><p>I think there are a few different skills it takes to be able to ender the fugue state even while knowing that your odds of success are low. One of them, I think, is the virtue of recklessness.</p><p>Recklessness is in the ability to say \"screw the odds, I'm going to push forward on this path as hard as I can until a better path appears.\" If the odds are low, a better path is more likely to appear sooner rather than later \u2014 but the reckless let that be a fact about the <i>paths</i>, and they don't <i>further</i> allow low odds to prevent them from pushing forward on the best path they can currently see, as fast as possible.</p><p>If you want to become a successful entrepreneur, or if you want to succeed at other very difficult tasks, it helps to be able to take the best from both types of hopeless entrepreneurs. Become the sort of person who can enter the fugue state and give an idea your all, while <i>also</i> being able to see and avoid all the common failure modes. The fact that you are unlikely to succeed is an <i>epistemic</i> fact, you do not need to give it dominion over your <i>motivation.</i> Be a little reckless.</p><hr><p>Recklessness, as a virtue, is about being able to throw caution to the wind. It's about being able to commit yourself fully to the best path before you, and then change your entire life at the drop of a pin as soon as a better path appears. It's about being free to act without worrying too much about what happens if you disrupt the status quo \u2014 too many people are already too stagnant, and we need to move faster.</p><p>So if you find yourself knowing what it is that you need to do next, but worried that doing so will break something else important\u2026</p><p>then I say, do it.</p><p>Act.</p><p>Try not to break anything vital, but if you do, fix it and keep moving.</p><p>Always forward, never back.</p><p>Be a little reckless.</p>", "user": {"username": "So8res"}}, {"_id": "7Cj7Dg94ZtDKDj9dt", "title": "Desperation", "postedAt": "2016-01-24T06:00:00.000Z", "htmlBody": "<p>The next three posts will discuss what I dub the three dubious virtues: desperation, recklessness, and defiance. I call them dubious, because each can easily turn into a vice if used incorrectly or excessively. As you read these posts, keep in mind the law of <a href=\"http://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear\">equal and opposite advice</a>. Though these virtues are dubious, I have found each of them to be a crucial component of a strong and healthy intrinsic motivation system.</p><p>The first of the three dubious virtues is <i>desperation</i>. There are bad ways to be desperate: visible desperation towards <i>people</i> can put you in a bad social position, strain your relationships, or otherwise harm you. Desperation towards a <i>goal</i>, on the other hand, is vital for a guilt-free intrinsic drive.</p><p>By \"desperation towards a goal\" I mean the possession of a goal so important to you that you can commit yourself to it fully, without hesitation, without some part of you wondering whether it's really worth all your effort. I mean a goal that you pursue with both reckless abandon and cautious deliberation in fair portions. I mean a goal so important that it does not occur to you to spare time wondering whether you can achieve it, but only whether <i>this</i> path to achieving it is better or worse than <i>that</i> path.</p><p>In my experience, the really powerful intrinsic motivations require that you're able to struggle as if something of incredible value is on the line. That's much easier if, on a gut level, you <a href=\"http://mindingourway.com/the-value-of-a-life/\">believe that's true</a>.</p><hr><p>Desperate people have a power that others lack: they have the <i>ability to go all out,</i> to put all their effort towards a task without reservation. Most people I have met don't have the ability to go all out for <i>anything,</i> not even in their imagination.</p><p>Ask yourself: is there anything <i>you</i> would go all out for? Is there anything some antagonist could put in danger, such that you would pull out all your stops? Is there any threat so dire that you would hold nothing back, in your struggle to make things right?</p><p>I have met many people who cannot honestly answer \"yes\" to this question, not even under imaginary circumstances. If I ask them to imagine their family being kidnapped, they say they would call the police and wait anxiously. If I ask them to imagine the world threatened by an asteroid, they say they would do their best to enjoy their remaining time. These are fine and prudent answers. Yet, even if I ask them to imagine strange scenarios where they and they alone can save the Earth at great personal cost, they often say they would do it only grudgingly.</p><p>For example, imagine that aliens that want to toy with <i>you in particular</i> have put a black hole on a collision course with Earth. Imagine that the only way to redirect it is using alien tech on an alien space ship that has been left on Earth and which can be piloted only by you and you alone \u2014 and that, to destroy the black hole, you must cross the event horizon, never to return. Would you save the world then? And if so, would you do it only grudgingly?</p><p>Would you do it if the spacecraft was sequestered atop Mt. Everest? How hard would you struggle to get to the ship, if it was at the bottom of the ocean? What if it could only be operated if you spoke fluent Mandarin, and you only had one year to learn?</p><p>Would you go all out to save the world, or would you put in a token \"best effort\", a token \"at least I tried\", and then go back to enjoying your remaining time?</p><p>And if you can't go all out even in incredible imaginary scenarios where everything depends on you, <i>what are you holding out for?</i></p><p>A common protest here goes \"I don't want to lose my friendships, my close connections, my comfort. That is too high a price to pay. If the struggle would be too brutal, then I would prefer to enjoy my remaining time instead.\" But if that were the case, then why couldn't someone get you to go all out by putting your friendships, connections, and comfort on the line? Would you fight with everything you have for <i>those</i>? And if not, <i>what are you holding out for?</i></p><p>Why are you stopping yourself from putting in a full effort, if there is no situation even in principle which could compel you to pull out all the stops? Why are you holding part of yourself back, if there is nothing <i>even in imagination</i> for which you would unbar all the holds? If there is nothing anyone could put on the line such that you'd struggle with all of your being, then <i>what are you holding out for?</i></p><hr><p>I'm not saying you need to be willing to go all out for something <i>real.</i> It may be that the only scenarios where you'd really struggle for all you're worth are fanciful or ridiculous. I'm saying that you need to be able to go all out in principle.</p><p>There's a certain type of vulnerability that comes with committing your whole self to something. Our culture has strong social stigmas against people who <i>really unabashedly care</i> about something.</p><p>I remember a classmate in gradeschool who <i>really really cared</i> about Pokemon, to the point that others felt embarrassed just to associate with him. The stereotypical stigma against \"nerds\" seems rooted at least partially in a stigma against caring too much. Derision among the intellectual elites towards people who get really interested in sports seems to draw at least partially on the same stigma.</p><p>Notice the negative connotations attached to words like \"cultist\", \"zealot\", and \"idealist\". Notice all the people who distance themselves from whatever social movement they're in; those people who loosely identify as \"effective altruists\" or \"rationalist\" or \"skeptics\" or \"atheists\" but feel a deep compulsion to make sure you know that they think the <i>other</i> EAs/rationalists/skeptics/atheists are naive, Doing It Wrong, and blinded by their lack of nuanced views. I think that this is, in part, an attempt to defend against the curse of Caring Too Much.</p><p>Caring hard is uncool. The stereotypical intellectual is a detached moral non-realist who understands that nothing really matters, and looks upon all those \"caring\" folk with cynical bemusement.</p><p>Caring hard is vulnerable. If you care hard about something, then it becomes possible to lose something very important to you. Worse, everyone around you might think that you're putting your caring into the <i>wrong</i> thing, and see you as one of the naive blind idealist sheeple, and curl their lips at you.</p><p>Desperation is about <i>none of that mattering.</i> It's about having a goal so important that the social concerns drop away, except exactly insofar as they're relevant to the achievement of your goal. It's about being willing to let yourself care more about the task at hand than about what everyone thinks about you, no matter how much they would deride you for fully committing.</p><hr><p>A common barrier to desperation is that it can be difficult to admit that you really, really care about something, because then that means you are vulnerable to the loss of something that's very, very important to you. If your desperation is visible in a hostile social environment, desperation can destroy your ability to bargain and put you at a social disadvantage. Being social creatures, I suspect that many of us have mental architectures that prevent us from feeling desperation, because if we felt it, we'd show it, and that would undermine our social standing. (In my experience, <a href=\"http://mindingourway.com/confidence-all-the-way-up/\">confidence all the way up</a> helps alleviate this effect.)</p><p>Thus, if you want to make desperation part of your intrinsic drive, you may need to practice becoming able to admit, to yourself, on a gut level, that you might lose something so terribly important that it's worth gaining a little desperation. You must first <i>allow yourself to become desperate.</i> (This is why I wrote about <a href=\"http://mindingourway.com/see-the-dark-world/\">seeing the dark world</a> and <a href=\"http://mindingourway.com/come-to-your-terms/\">coming to your terms</a> before writing about desperation.)</p><p>There is a common failure mode among those who succeed at becoming desperate, which is that they burn their resources too quickly, in their desperation. If you have to get yourself into an alien spacecraft at the bottom of the ocean, and it's going to take many months of training, social and political maneuvering, and monotonous searching, then you would be unwise to spend your first week all wound up at maximum stress levels simply because you think that that's what it means to \"go all out\" and \"hold nothing back.\" If you're going to pull out all the stops and unbar all the holds, you need to understand how to carry on a slow burn as well as a fast burn. (This is why I wrote about how to <a href=\"http://mindingourway.com/stop-before-you-drop/\">avoid working yourself ragged</a> and <a href=\"http://mindingourway.com/rest-in-motion/\">rest in motion</a> before writing about desperation.)</p><p>With these tools in hand, I suggest finding a way to <i>become able to become desperate.</i> Perform whatever thought experiments and meditations you have to to be able to <i>imagine a situation</i> where you would do everything in your power to achieve some outcome, without regard for the consequences (beyond their affect on the outcome). Figure out the circumstances under which you'd pull out all the stops and unbar all the holds and put <i>everything you have</i> into the struggle.</p><p>(If there is no situation, even in theory, where you would give everything you have into your efforts, then consider that there may be a part of yourself that you're holding back for nothing, a part of yourself that you're wasting.)</p><p>I'm not saying you need to become desperate <i>now</i>. That may be unnecessary. Maybe your life is going well enough, and your goals are well enough achieved, that the best way to continue achieving them is to strengthen your friendships and your connections and enjoy your comforts. If your family <i>is</i> kidnapped, you probably <i>would</i> do best to call the police and then wait anxiously. If Earth <i>is</i> threatened by an asteroid, most people <i>would</i> do best to leave it to the experts and enjoy what time they have. So be it not upon me to force desperation upon you if you're leading a comfortable life. Make sure you don't suffer from <a href=\"http://mindingourway.com/youre-allowed-to-fight-for-something/\">the listless guilt</a>, and make sure you can in principle <i>become</i> desperate, so as to ensure that you're not holding a part of yourself back for nothing, but save the actual desperation for times of need.</p><p>If, on the other hand, you are in a time of need, if you're the sort who sees every death as a tragedy, if you're otherwise <a href=\"http://mindingourway.com/caring-about-some/\">fighting for something larger than yourself</a>, then <i>get desperate now.</i></p><p>The first step is allowing yourself to become desperate in principle. It's allowing there to be at least one imaginary scenario where you'd let yourself commit fully to a task without hesitation. Once you are able to do this, imagine the feeling that would come over you when you first committed yourself to that crucial undertaking, come whatever may. Is there a sense of desperation you would feel, a grasping need to <i>change the future?</i> Sit with it, become familiar with the sensation of desperation and any other feelings associated with the imaginary commitment.</p><p>Once you've gained some familiarity with those feelings, look with fresh eyes at what you're fighting for, at what you have to protect, at what you value, and see if any of it is worthy of a little desperation.</p>", "user": {"username": "So8res"}}, {"_id": "LNHevqSgGbMvn58WK", "title": "Confidence all the way up", "postedAt": "2016-01-17T06:00:00.000Z", "htmlBody": "<p>I apparently possess some sort of aura of competence. Some say I'm confident, others say I'm arrogant, others remark on how I seem very certain of myself (which I have been told both as compliment and critique).</p><p>I was surprised, at first, by these remarks from friends and family \u2014 from my perspective, I'm usually the first person in the conversation to express uncertainty in the form of probability estimates and error bars. I'm often quick to brainstorm alternative explanations of the data I use to support my claims. And, of course, I'm certain of nothing.</p><p>In fact, I had a conversation with a friend about this phenomenon once, which went something like this:</p><blockquote><p><i><strong>Me:</strong> Hey, have you noticed how everyone thinks I have an aura of confidence and certainty, sometimes arrogance? I don't know how to shake it, nor how it works. What's up with that?</i></p></blockquote><blockquote><p><i><strong>Him:</strong> Well, you always seem to have a solid grasp on every situation. When you're explaining things, you answer questions quickly, deftly, and with precision.</i></p></blockquote><blockquote><p><i><strong>Me:</strong> I don't think that's it, though. I'm rarely confident in the claims I'm making, and I tend to highlight that fact. Earlier, when we were talking with [other friend] about tools society can use to break monopolies, I was very explicit about where my uncertainty lies, and what assumptions my models relied upon, and where they might be flawed.</i></p></blockquote><blockquote><p><i><strong>Him:</strong> Yeah, but even then you were confident in what you were saying \u2014 maybe not confident in any particular claim you made, but confident in your overall analysis.</i></p></blockquote><blockquote><p><i><strong>Me:</strong> I don't think that's it either. I'll be the first to admit that the probabilities I put on my propositions are pulled out of thin air, and I'll also be the first to admit that my hypothesis space is decrepit and that I'd be able to find better models if I could think better. In fact, I'm aware of a bunch of flaws in the ways I think, and I dedicate a decent amount of effort to improving my own reasoning methods.</i></p></blockquote><blockquote><p><i><strong>Him:</strong> \u2026</i></p></blockquote><blockquote><p><i><strong>Me:</strong> \u2026 I'm doing the thing right now, aren't I?</i></p></blockquote><blockquote><p><i><strong>Him:</strong> Yes, yes you are.</i></p></blockquote><p>There definitely is something of \"confidence\" to this pattern of speech and thinking, but it's not an empirical confidence. The confidence people notice in me isn't in the <i>content</i> of my claims, for I'm quick to couch my claims with probability estimates and error bars. Most of the confidence isn't in my analysis, either; I'm quick to note the ways my analyses could be flawed.</p><p>Some of the confidence <i>does</i> reside in the ways I reason; I do admit that I am much better equipped to answer questions of the form \"but why are you so much more confident in your own reasoning than their reasoning, when they actually have more credentials?\" than most. But even there, I can note plausible biases and judgement errors in my own reasoning processes with alacrity.</p><p>Why, then, do I come off as so confident? Why do I seem so self-assured while listing the ways I know my brain is flawed?</p><p>On reflection, I've concluded that (at least part of) the answer is something I call \"confidence all the way up\". Insofar as I'm uncertain of my content, I'm confident in my analysis \u2014 except, I'm not fully confident in my analysis. But insofar as I'm uncertain of my analysis, I'm confident in my reasoning procedures \u2014 except, I don't put faith there, either. But insofar as I'm uncertain of my reasoning procedures, I'm confident in my friends and failsafe mechanisms that will eventually force me to take notice and to update. Except, that's not quite right either \u2014 it's more like, every lack of confidence is covered by confidence one meta-level higher in the cognitive chain.</p><p>The result is something that reads socially as confidence regardless of how much empirical uncertainty I'm under.</p><hr><p>Where does it bottom out? Well, insofar as my friends and failsafe mechanisms aren't sufficient to raise errors to my attention, I expect to reason poorly in an irredeemable fashion and then fail to achieve my goals. It bottoms out at the point where I say \"yeah, if I'm <i>that far gone</i>, then I fail and die.\"</p><p>(And somehow, I'm able to say even this while maintaining my aura of self-assuredness and confidence).</p><p>I have encountered many people who seem paralyzed by their uncertainties. They hit a question (such as \"what methods can a society use to break up monopolies?\") and they are pretty sure that they won't be able to generate the <i>right</i> answers, and so they generate <i>no</i> answers.</p><p>And this may be a better failure mode than the failure mode of someone who has <i>too much</i> confidence and self-assuredness, who makes up a bunch of bad answers and then believes them with all their heart.</p><p>Someone with Confidence All The Way Up, though, can achieve the third alternative: generate a bunch of bad answers, understand why they're bad and where their limitations are, and use that information as best they can.</p><p>I have found this mindset to be very useful throughout my life. Confidence all the way up is what has me dive into the fray to try new things, while others stand on the sidelines bemoaning a high degree of uncertainty. It's part of the technique of <a href=\"http://mindingourway.com/where-coulds-go/\">treat recurring failures as data and training</a>, rather than as a signal that it's time to feel guilty. It's part of the technique of <a href=\"http://mindingourway.com/not-yet-gods/\">knowing you're deeply limited</a> without letting that interfere with your <a href=\"http://mindingourway.com/moving-towards-the-goal/\">progress towards the goal</a>. Of the top ten most competent people I've met in person (by my estimation), eight of them seem to have some variant of confidence all the way up running. If the mindset seems foreign to you, I suggest finding a way to practice it for a while.</p><hr><p>Confidence all the way up is about working with what you have. It's about <a href=\"http://mindingourway.com/not-yet-gods/\">knowing your limitations</a>. It's about knowing that you don't have perfect models of \"what you have\" nor \"your limitations\", and proceeding anyway, with an even stride.</p><p>It's about knowing that there are going to be curveballs, and trusting your ability to handle curveballs, but not all the time; and trusting your ability to get back up when you're knocked down by a curveball you couldn't handle, but not all the time; and <a href=\"http://mindingourway.com/come-to-your-terms/\">coming to terms</a> with the fact that you might be hurt so badly you can't get up.</p><p>Yes, we're limited. All humans are limited. There are important, decision-relevant facts that we don't know. Our reasoning processes run on <a href=\"https://en.wikipedia.org/wiki/List_of_cognitive_biases\">compromised hardware</a>. But <a href=\"http://lesswrong.com/lw/n6a/the_correct_response_to_uncertainty_is_not/\">the correct response to uncertainty is not to proceed at half speed</a>!</p><p>No matter how hard you try to justify your beliefs, if you're being honest with yourself, they won't ground out into \"and therefore, no matter what I do, everything is going to be OK.\" No matter how hard you try to justify your reasoning, the meta-reasoning tower does not terminate at \"and thus, eventually you will become capable of success.\" They terminate at \"I may be so wrong that I can never be corrected; I may fail and all value may be lost.\" You will find no objectively stable perch from which to launch your reasoning.</p><p>But you were <i>created already in motion.</i> You don't <i>need</i> to ground out all your beliefs and justify all your reasoning steps before you can start moving. You don't need to have plans for every contingency before you can act. You don't need to be highly confident in your analyses before you present a model. If you sit around awaiting certainty, you will be waiting a long while.</p><p>Better, I say, to cover each lack of confidence on one level with confidence on the next level, and to come to terms with the fact that if you're so irredeemable that even your best meta-reasoning cannot save you, then you've already lost.</p>", "user": {"username": "So8res"}}, {"_id": "73vhvomkqDEHC4ws4", "title": "The art of response", "postedAt": "2016-01-03T06:00:00.000Z", "htmlBody": "<p>Imagine two different software engineers in job interviews. Both are asked for an algorithm that solves some programming puzzle, such as \"identify all palindromes in a string of characters.\"</p><p>The first candidate, Alice, reflexively enters problem-solving mode upon hearing the problem. She pauses for a few seconds as she internalizes the problem, and then quickly thinks up a very inefficient algorithm that finds the answer by brute force. She decides to sketch this algorithm first (as a warm up) and then turn her mind to finding a more efficient path to the answer.</p><p>The second candidate, Bob, responds very differently to the same problem. He reflexively predicts that he won't be able to solve the problem. He struggles to quiet that voice in his head while he waits for a solution to present itself, but no solution is forthcoming. He struggles to focus as the seconds pass, until a part of his brain points out that he's been quiet for an uncomfortably long time, and the interviewer probably already thinks he's stupid. From then on, his thoughts are stuck on the situation, despite his attempts to wrest them back to the task at hand.</p><p>Part of what makes the difference between Alice and Bob might be skill: Alice might have more experience that lets her solve programming puzzles with less concerted effort, which helps her get to a solution before self-doubt creeps in. Self-confidence may also be a factor: perhaps Alice is simply less prone to self-doubt, and therefore less prone to this type of self-sabotage.</p><p>A third difference between Alice and Bob is their <i>response pattern.</i> Bob begins by waiting blankly for a solution to present itself; Alice begins by checking whether she can solve a simple version of the problem (\"can I solve it by brute force?\"). Bob is more liable to panic when no answer comes (\"I have been quiet for too long\"), Alice is more liable to break the problem down further if no solution presents itself (\"Can I divide and conquer?\").</p><p>This difference is also explained in part by experience: a more seasoned software engineer is more likely to <i>reflexively</i> notice that a problem can be solved with a simple recursion, and know which data structures to apply where. I don't think it's <i>only</i> experience, though. Imagine Alice and Bob both faced with a second problem, outside their usual comfort zone \u2014 say, a friend asks them for advice about how to handle a major life-changing event. It's easy to imagine Alice attempting to understand the situation better and asking clarifying questions that help her understand how her friend is thinking about the question. It's similarly easy to imagine Bob feeling profoundly uncomfortable, while he tries to give neutral advice and worries about the fact that he might give bad advice that ruins his friend's life.</p><p>One might call what Alice is doing \"confidence,\" but that doesn't tell us <i>how it's working.</i> And 'confidence' also comes with connotations that may not apply to Alice \u2014 she may well decide that she isn't in a position to give good advice, she may be working from a shaky understanding and thus doubt her own conclusions, even as she turns her thoughts to understanding the obstacle before her.</p><p>One of the big differences, as I see it, is the difference in the <i>response pattern</i> between Alice and Bob. Alice justs <i>gets down to addressing the obstacle before her</i>, Bob spends mental cycles floundering. Managing response patterns is something of an art: when confronted with an obstacle, does your brain switch into problem-solving gear or do you start to flail?</p><hr><p>Note that the art of response is <i>not</i> about immediately solving any problem placed before you. Sometimes, the best automatic response is to find some way to disengage or dodge. <a href=\"http://mindingourway.com/half-assing-it-with-everything-youve-got/\">You aren't obligated to solve every problem placed before you</a>. The goal of having appropriate response patterns is to <i>avoid flailing</i> and <i>avoid staring blankly</i>. The goal is to have your mind shift into the problem-solving gear.</p><p>Having effective responses prepared isn't necessarily a general skill. I'm a computer programmer at heart, and a few years ago I switched paths to math research. If I'm faced with a programming problem that I want to solve, I quickly and easily slip into effective-response-mode; I can often find solutions to problems reflexively, and when I can't, I reflexively examine the problem from many different viewpoints and start breaking it down. Yet, if you confront me with a math problem I want solved, there are still times when my reflexive response is to sit back and wait for someone else to solve it for me. (It doesn't help that I'm surrounded by brilliant mathematicians who can do so successfully.) That reflexive response \u2014 the one of blanking my mind, curious while I wait for someone else to find the answer \u2014 is not a very effective response.</p><p>Effective responses aren't about answering <i>quickly</i>, either. When paired with expertise and familiarity an effective response to an obstacle will often lead to a fast answer, but oftentimes the most effective response is to pause and think. Plenty of people have very ineffective response patterns that involve opening their mouths the moment you ask them to help you confront an obstacle. Some people reflexively start solving the wrong problem, others reflexively start making excuses for themselves, still others reflexively share personal anecdotes that paint them in a positive light. Effective response patterns are not about answering fast, they're about answering <i>well.</i></p><hr><p>The most competent people that I know are, almost universally, people who have very effective response patterns to obstacles in their areas of expertise. The good programmers I meet reflexively start breaking a problem down the moment they decide to solve it. The stellar mathematicians I know reflexively start prodding at problems with various techniques, or reflexively identify parts of the problem that they don't yet understand. The best businesspeople among my advisors are people who listen to me describe the choice before me, and reflexively describe the costs, constraints, and opportunities they observe. Each has acquired a highly effective response pattern to problems that fall within their area of expertise. This response pattern allows them to hit an obstacle and start taking it apart, with an Alice-like mindset, rather than flailing and doubting themselves as per Bob.</p><p>Confidence, practice, and talent all help develop these specific response patterns quite a bit. That said, you can often learn someone's <i>response patterns</i> with much less effort than it takes to learn their skills: you can start thinking in terms of incentives, opportunity costs, and markets long before you become a master economist (though reading a microeconomics textbook surely doesn't hurt). Competence isn't just about believing in your capabilities; it's also about having a pattern prepared that takes you directly to the \"break down the problem and gnaw on the parts\" stage without ever dumping you into the \"worry about how you've been silent for a long time and reflect on the fact that the interviewer probably thinks you're dumb\" zone.</p><p>Having an <i>explicit</i> pattern, such as a checklist, can help you switch from one pattern to the other. For example, imagine Bob in the example above had a checklist which read as follows:</p><blockquote><p><i>If I start dwelling on how likely I am to fail, I will do the following. (1) Say \"hmm, let me think for a few minutes\" aloud. (2) Verify that I understand the problem, and ask clarifying questions if I don't. (3) Check whether I could easily solve the problem by brute force. (3) Come up with a few simplifications of the problem. (4) Find a way to break off only one part of the problem or one of its simplified variants.</i></p></blockquote><p>then he may well be able to manually switch from a flailing response pattern to an effective one. This sort of manual switching is a good way to instill a new response pattern. The ultimate goal, though, is for efficient response patterns to become <i>reflexive</i>.</p><p>In fact, I think many people could benefit from developing efficient \"fallback\" response patterns, to handle new or surprising situations. Response patterns like \"verify that your observations were correct\" or \"find more data\" or \"generate more than one plausible explanation for the surprise\" and so on. As far as I can tell, there <i>is</i> a general skill of being able to smoothly handle surprising new situations and think on your feet, and I suspect this can be attained by developing good response patterns designed for surprising new situations.</p><hr><p>This advice is not new, of course. Lots of self-help advice will tell you to break down the problems before you into smaller parts, and to infuse your actions with intentionality, and to <a href=\"http://mindingourway.com/obvious-advice/\">reflexively do the obvious things</a>, and so on. So I won't say much more on how to <i>attain</i> the Alice-like mindstate as opposed to the Bob-like mindstate. The important takeaway is that sometimes people respond to obstacles by breaking them down and other times they respond by flailing, and one way or another, it's useful to develop reflexive responses that put you into the former mindstate.</p><p>The way that I do it is by monitoring the ways that I respond to new obstacles placed before me. I watch myself facing various situations and observe which ones lead be to reflexively get defensive, or to reflexively blank my mind and wait for someone else to answer, or to reflexively freeze in shock and act dumbfounded. Then I practice building better response patterns for those situations, by figuring out what the checklists to run are, and I do my best to replace those patterns with reflexive inquiry, curiosity, requests for clarification, and impulses to take initiative. Polished response patterns have proven useful to me, and I attribute much of my skill at math, programming, and running nonprofits to having sane responses to new obstacles.</p><p>Regardless of where you get your response patterns from, I suspect that honing them will do you well.</p>", "user": {"username": "So8res"}}, {"_id": "4r75LjNbsbBYDnRdN", "title": "Obvious advice", "postedAt": "2015-12-06T06:00:00.000Z", "htmlBody": "<p>This is a common scene at the MIRI offices: I have a decision to make, like <a href=\"https://intelligence.org/2015/12/01/miri-2015-winter-fundraiser/\">what sort of winter fundraiser to run</a>. Before making any choices, I take a few minutes to write down all the obvious things to do before making the decision: spend five minutes brainstorming options before weighting any pros or cons; talk to people who have run different types of fundraisers in similar situations; and so on. I can usually generate a handful of obvious things to do before making my decision. I write those things down, and then I describe my decision to one of my advisors and see if they have any advice. They say \"only the obvious,\" and then rattle off five more obvious things I hadn't thought of, all of them useful.</p><p>Sometimes, I wonder how successful a person would be if they just did all the obvious things in pursuit of their goals.</p><p>So with that in mind, allow me to offer some quite obvious pieces of advice, which have proven very useful for me:</p><p>Before carrying out any plan, <i>actually do the obvious things.</i></p><p>When you're about to make a big decision, pause, and ask yourself what obvious things a reasonable person would do before making this sort of decision. Would they spend a full five minutes (by the clock) brainstorming alternative options before settling on a decision? Would they consult with friends and advisors? Would they do some particular type of research?</p><p>Then, <i>actually do the obvious things.</i></p><p>A corollary to this advice is to also occasionally consider <i>not doing things the wrong way.</i> Imagine someone who's recently failed at an endeavor that was important to them. They're fraught with despair, and you attempt to console them by saying \"well, at least you learned something.\" They snap back, \"yeah, I learned never to try hard things ever again!\"</p><p>This may be just an emotional outburst, yes, but if they act upon this outburst \u2014 and withdraw, and become less curious, and become more bitter \u2014 then they are in solid need of the above corollary. In fact, the middle of an emotional outburst is one of the <i>best times</i> to use the corollary. I have often myself found it useful, mid-hasty-decision, to pause, reflect, and ask myself \"wait\u2026 is this a <i>terrible plan?</i>\"</p><p>(And then, if the answer is yes, I don't carry out the plan \u2014 a crucial step.)</p><hr><p>Both pieces of advice above \u2014 \"do the obvious preparation\", and \"don't execute bad plans\" \u2014 each get a lot more useful as you expand your notions of \"obvious preparation\" and \"bad plan\". In fact, quite a bit of rationalist-style advice is about expanding your notion of \"obvious thing\" and \"bad plan.\" Thus, this advice gets much more helpful if you make sure to do the obvious things.</p><p>(Not all the rationalist advice is of this form, of course; many of the most important rationalist skills are cognitive operations that happen in <a href=\"http://lesswrong.com/lw/5kz/the_5second_level/\">five seconds or less</a>. One example of a five-second-level skill is the skill of encountering a new problem and <i>reflexively starting to list obvious perparations</i> or noticing an emotional outburst and <i>reflexively taking a step back and checking whether your current plan is terrible.</i> More often than not, one of the goals of these blog posts is to install a five-second cognitive reflex of <i>deciding to apply a tool</i> by describing the tool itself. But I digress.)</p><p>For example, the cognitive reflex of \"enumerate obvious preparations\" becomes much more useful once you have concepts like \"brainstorm options before weighing pros and cons\" and \"set a five minute timer and actually think about the problem for the whole five minutes\" and \"consider the opportunity costs.\" And the cognitive reflex of \"check whether your current plan is terrible\" becomes more useful as you add concepts like \"rationalizing\" and \"blindly acting out a social role\" and so on.</p><p>So this week's advice is obvious advice, but useful nonetheless: find a way to gain a reflex to actually do all the obvious preparation, before undertaking a new task or making a big decision.</p><hr><p>It's surprising how often the advice that I give people who come to me asking for advice cashes out to some form of \"well, have you considered doing the obvious thing?\"</p><p>For example, when someone comes to me and says \"help, I have a talk I have to give and I'm going to be terribly nervous and I dread it, what do I do?\" it's often surprisingly helpful for me to ask, \"well, what sort of things would make you less nervous?\" Or someone comes to me and says \"I find myself just playing video games all day, how do I stop myself?\", I first ask, \"have you considered what sorts of things you'd rather do besides play video games all day?\"</p><p>In many cases, the obvious prompts aren't sufficient. But in a surprising number of cases, <i>they are.</i> I still often find this advice useful myself: when my attention slips, I am often helped by someone just asking me to consider the obvious \u2014 \"what would make the task less dreadful?\" or \"have you thought for five minutes about alternatives?\" or \"have you considered delegating this?\" and so on.</p><p>Much of my advice for how to manage guilt was generated by this very process, by me imagining feeling guilty, and then imagining which obvious things I'd try to do to engage with the feeling. I would ask myself questions like \"what is the cause of this feeling?\" and \"how is it being useful to me?\" and \"is there a better way I can achieve those goals?\" and I would spend time listening to myself and brainstorming options, because those are all the obvious ways to address the problem. Many of my early posts on guilt were a product of articulating that reflexive process. The types of obvious advice that I would generate \u2014 such as asking \"what is the cause and use of this feeling?\" \u2014 might be very different from the obvious advice that you would generate, and that's fine. The trick is to apply the obvious advice first.</p><p>Or imagine you have the problem of finding it difficult to use the \"do the obvious\" technique. Maybe you've been struggling to remember to consider the obvious whenever you encounter a hard decision. Instead of asking for advice, consider generating a list like the following, first:</p><ul><li>Spend five minutes generating examples of decisions you made in the past where it would have been helpful to do the obvious things first. Then spend five minutes examining those and looking for patterns.</li><li>Close your eyes and visualize yourself facing a new decision in as much concrete detail as you can, and practice thinking \"oh wait, let me list the obvious things before proceeding.\"</li><li>Train yourself to notice decision-points better by buying a <a href=\"http://smile.amazon.com/GOGO-Tally-Counter-Manual-Mechanical/dp/B001KX1VW2?sa-no-redirect=1\">tally counter</a> and tracking decisions and giving yourself positive reinforcement every time you do.</li></ul><p>Or imagine that you have tried to do all the obvious things, and you find that you're going into \"enumerate the obvious\" mode even for the most trivial tasks, and it's making your trivial tasks take way too long and the whole thing seems pretty foolish. Then, before complaining, consider trying the corollary, and consider whether applying \"try the obvious\" far too often is in fact a terrible plan.</p><p>Your list of obvious things will very likely look very different from my own \u2014 my friends and advisors <i>still</i> generate obvious-in-retrospect ideas that I myself was incapable of generating, even after spending a few minutes generating the sort of ideas I expected them to generate. Collecting tips and techniques from other people in your environment is a great way to expand your \"obvious things\" repertoire, and asking for advice from friends will likely continue to generate new obvious things for quite some time. It's OK for your lists to be very different; the trick is to do <i>some</i> of the obvious preparation before making a hard decision. It can often make a difference.</p><hr><p>The important thing, here, is to find a way to actually start doing the obvious things. This is the skill that's like footwork for a rationalist: remembering to actually do the obvious preparation is easy to learn and difficult to master; it's a skill to drill when you have spare mental energy in hopes that it comes naturally and easily whenever the going gets tough and the stakes get high.</p><p>I continue to wonder how powerful a person could become, if they simply managed to do all the obvious things in pursuit of their goals.</p>", "user": {"username": "So8res"}}, {"_id": "uZzzguxgMmhz7jfWT", "title": "There is no try", "postedAt": "2015-11-29T06:00:00.000Z", "htmlBody": "<figure class=\"image\"><img src=\"http://mindingourway.com/content/images/2015/11/yoda.jpg\"></figure><p>Ok, so \"try\" is actually a pretty useful concept; there's a reason we have a very short word for it in the English language. Nevertheless, I have found it quite useful to occasionally spend a few weeks refusing to use the word \"try\" or any of its synonyms, at least when talking about myself, and especially when thinking about myself to myself.</p><p>This is a quick and easy way to <a href=\"http://mindingourway.com/stop-trying-to-try-and-try/\">put success in the background</a>, as discussed last week. For example, compare these two responses to \"what are you doing?\"</p><blockquote><p><i>I'm trying to solve this math problem.</i></p></blockquote><p>versus</p><blockquote><p><i>I'm pursuing a promising line of inquiry on this math problem. If it doesn't lead anywhere, I have two others to pursue next. If all three are fruitless, I'll ask for help.</i></p></blockquote><p>For the first person, \"failure\" is either first or second on the list of things they expect to happen next: they're trying to solve the problem, and either they'll solve it, or they'll fail. If they fail, they can say \"well, I tried\", and move on. And because failing and moving on is such a prominent option, they must struggle against it each time they pause; they are like the person trying to sprint up and down a soccer field as much as they can, rather than the person playing soccer.</p><p>The second person, who does not have 'try' in their vocabulary, is forced to say what specific actions they are actually taking \u2014 and now, failure on the entire problem is much further down on the list of possible outcomes. Failure at this particular line of approach just drops them into the next line of approach. They're more like the person playing the soccer game, getting exercise (\"trying to solve the problem\") without that idea explicit in their mind. This sort of mindset, I find, is often helpful.</p><hr><p>Imagine that I'm in the middle of flossing my teeth, when someone knocks on the door and asks what I'm doing. I wouldn't answer \"trying to floss,\" I'd just answer \"flossing\" \u2014 unless I had been interrupted so many times that I was beginning to doubt my ability to complete the task. When we're sure of our ability to complete a task, we don't describe ourselves as \"trying\", we just <i>do it.</i> I don't get up every morning and try to dress myself, I just get up and dress myself.</p><p>Whenever you can honestly say that you are <i>doing</i>, rather than <i>trying</i>, then I suggest you do so \u2014 but often this is only honestly possible when you're quite confident in your own ability to succeed.</p><p>(Some self-help books and professionals advocate <i>always</i> saying that you are \"doing\" rather than \"trying,\" but this often seems dishonest to me: when I'm trying to win a race, and I'm currently in tenth place, and you ask me what I'm doing, I have a hard time saying \"winning a race\" with a straight face.)</p><p>When removing 'try' and its synonyms from your vocabulary, you may find that you can't honestly say you're \"solving a math problem,\" because you have no idea whether you'll succeed. And saying you're \"working on a math problem\" is only slightly better; it's mostly just using \"working\" as a synonym for \"trying.\"</p><p>In these cases, if you want to remove the word 'try', I suggest not finding a near synonym, but increasing the granularity of your descriptions. Don't say \"I'm trying to solve this math problem,\" say \"I'm transforming the problem into a programming problem so I can see it from a different angle\", or \"I'm gameifying the problem so that my intuitions can get a better handle on it,\" or \"I'm producing random algebraic manipulations of this equation in desperate hope that one of them happens to be the answer,\" or \"I'm staring at the problem waiting for my gut to say something for enough time to pass that I can give up without losing face.\" Describe what you're doing on the level of granularity where at each step you describe, it would be silly to say you were \"trying\" at that step, in the same way it would be silly to say that you wake up and try to dress yourself \u2014 describe your actions on a level of granularity where each step is definitely something you're <i>doing</i>, rather than <i>trying</i>.</p><p>Often, when I get down to the level of granularity where I'm doing rather than trying, I find that I'm doing something pretty silly \u2014 as in, I'll start out by saying \"I'm trying to write the opening paragraph of this paper\", and then I'll notice the word 'trying', and I'll introspect a bit and rephrase a bit and I'll eventually figure out that I was doing was \"sitting in front of a screen holding the subject of the paper in my head waiting for my gut to figure out what to write\" or something along those lines. With that description given, it's much easier for me to say \"aha, my gut doesn't know what to write first; I'll make an outline on a whiteboard or some other place that feels non-committal.\"</p><p>\"Try\" is a useful word, but saying that you're \"trying\" to do something is a <i>high level description</i>, and it can often hide some very silly behaviors, like \"sitting around staring at the problem waiting for enough time to pass that I can give up without losing face.\"</p><hr><p>Occasionally, I tell people who come to me for advice that \"try\" is a fine and useful word, but saying that you're trying is something that <i>other</i> people get to say about <i>you</i>, not a thing that you get to say about yourself. <i>Others</i> get to say \"they're trying to save that person's life,\" but <i>you</i> only get to say \"I'm performing chest compressions while thinking back to remember my CPR training.\"</p><p>This isn't always the most useful advice; there is, after all, a reason why 'try' is such a short word. There are many situations where it's quite useful to communicate something like \"I'm trying to prove this lemma; can you help?\", and there are many other cases where it can be useful to use the word 'try' even when thinking about yourself to yourself. Nevertheless, there is a helpful sentiment buried in the above advice, and I have often found it useful to cash out my \"try\"s.</p><p>As such, I recommend, as an exercise, spending a few weeks refusing to use the word 'try'. This can help you train yourself to notice the difference between \"trying\" as in taking intelligent, concrete, fruitful actions; versus \"trying\" as in waiting for enough time to pass that you can safely say \"well I tried.\"</p><hr><p>This probably isn't what Yoda <i>actually</i> meant by \"there is no try.\" Nevertheless, I like to imagine Luke nodding and saying \"Oh, right; there is no try. I will close my eyes, relax, let the force flow through me, focus my mind, concentrate on a mental image of my X-wing, and then will it to lift, with no regard for its actual mass.\" That's the level of granularity at which you can tell whether a cashed-out \"try\" is a pre-emptive <a href=\"http://mindingourway.com/have-no-excuses/\">excuse for failure</a> or an intelligent attempt to succeed.</p><figure class=\"image\"><img src=\"http://mindingourway.com/content/images/2015/11/xwing.jpg\"></figure>", "user": {"username": "So8res"}}, {"_id": "stQuvrBrqyREvnHLu", "title": "Stop trying to try and try", "postedAt": "2015-11-22T06:00:00.000Z", "htmlBody": "<p>Imagine a graduate student of mathematics as they interact with a professor, attempting to understand something in the professor's area of expertise. They're working hard to wrap their head around the basic formalism. They're in \"learning mode\" \u2014 they're a student in the presence of a master, expected to try to understand the math but not necessarily expected to succeed. Even if they're doing quite well, they're still reminded of how math is big and they are small; they encounter wide swaths of knowledge that they do not yet have, and often feel humbled. They use their tools tentatively, aware that they may be using them inappropriately, and wonder when they'll become a master.</p><p>Now imagine the same student tutoring an undergraduate in linear algebra, a topic they know quite well. Now they're in \"teaching mode.\" Math is still large; the graduate student is still small; but the context is very different. The focus is no longer drawn repeatedly to all the things they don't know yet \u2014 but it's not drawn to all the things they <i>do</i> know, either. The focus simply isn't on them, or their abilities. It's on the undergrad. The grad student, in the back of their mind is not thinking \"wow math is so large I don't know enough yet I'm not sure I'll ever know enough\", and they're <i>also</i> not thinking \"wow I know so much this is great!\" \u2014 they're thinking about how to help the undergrad understand a complex concept.</p><p>I think that many people who are in learning mode expect that mastery feels like learning mode, except that instead of feeling like they know very little, they feel like they know quite a bit. By contrast, I think mastery looks much more like teaching mode \u2014 it looks like someone operating in a context where their knowledge and their skills are not the focus, but are just unconscious assumptions in the background.</p><p>Consider the grad student in teaching mode. Their approach to answering questions in teaching mode is very different than their approach in learning mode. That's not because all the questions they encounter in teaching-mode are simple \u2014 if you've ever been a tutor you know that tutors are commonly asked questions they can't answer in the moment. Rather, they approach questions differently because context is different. When the professor asks them questions, they're Expected To Do Their Best; when the undergrad asks them questions, they're just expected to answer.</p><p>In the first case, they're expected to try; in the second case, they're assumed capable, an assumption that fades into the background.</p><p>I describe this model because I think there is an analog of these two modes when it comes to \"trying\" to achieve any task \u2014 and today, I'm going to talk about trying.</p><hr><p>My advice is simple: notice when you're expected to try, and consider reframing. It's much harder to solve a problem when you're Expected To Do Your Best than it is to solve a problem when you're immersed in various subtasks, with the assumption that you're going to solve the problem buried implicitly and unconsciously in the context.</p><p>For example, consider exercise. Many people find it much easier to exercise in a context where the exercise is in the background rather than the foreground. Imagine someone who plays recreational soccer, sprinting up and down the soccer field up till the brink of exhaustion. Now imagine them not playing soccer, but just trying to sprint up and down the field up to the brink of exhaustion. They probably push themselves a lot less in the latter case. If \"sprint up and down the field a lot\" is the main goal, then at each possible stopping point, part of them starts trying to convince the rest that they've exercised enough for the day, and they must spend willpower to continue. In a soccer match, by contrast, the focus is elsewhere. They aren't constantly pinging themselves with explanations of how they've done enough sprinting for today. They aren't generating reasons why it's OK to stop here. They're trying to <i>score a goal.</i> Getting exercise is a background assumption, not a conscious choice.</p><p>Switching contexts such that your actual goal is in the background rather than the foreground \u2014 such that pursuing it is not a conscious choice that you need to reaffirm every time you find a stopping point \u2014 is a powerful tool.</p><p>This is not novel advice, of course, but it is perhaps a generalization over a few different common types of advice. As another example, consider two people trying to become friends on purpose (perhaps for romantic reasons). I conjecture that it's much harder for people to become friends on purpose than to become friends accidentally while pursuing some other endeavor.</p><p>If they're trying to become friends on purpose, then they're constantly asking themselves, \"are we friends yet?\", and like the grad student asking themselves \"do I understand all of mathematics yet?\", the answer will never be an unresounding \"yes\". They would do better to switch to a context where they're not constantly checking whether they're friends yet, and are instead just <i>being friends.</i></p><p>This model suggests that it's much more effective to alter the context such that neither party is regularly checking the depth of the friendship, but such that a strengthening bond is the implicit background assumption. (This suggests one reason why online dating feels more socially awkward than going on a date with someone you met in some other context.)</p><p>For a third (somewhat silly) example, imagine that I woke up one morning and said \"I'll try to run MIRI well today.\" (MIRI, the <a href=\"http://mindingourway.com/stop-trying-to-try-and-try/intelligence.org\">Machine Intelligence Research Institute</a>, is an organization I run.) If I did this, I'd be in trouble. How does one run a research institute? What would my next actions be? Things that seem plausibly like what people-who-run-institutes-well would do? Things that seem defensible to the board of directors? I have no idea how to \"try to run MIRI.\"</p><p>Now imagine instead that I woke up and said \"I'm going to glance at my MIRI priority list, update it if today happens to be Monday, and then identify MIRI's biggest bottleneck and work on it directly.\" Now I'm in business, and might do something useful with my day.</p><p>Notice the difference. In the second case, I'm not asking myself whether I can run a research institute. I'm not asking myself <i>how</i> to run a research institute (though \"study the strategies of people who ran other successful institutes\" does occasionally get to the top of my priority list). I'm assuming myself capable \u2014 not consciously, but as a background assumption. I'm not assuming <i>success</i> \u2014 either I can run a research institute or I can't, the jury's still out on that one \u2014 but my capability is not the focus of my attention. I fret about much more practical things, like the tone to strike in a fundraiser announcement post, or how to prioritize paper-writing versus novel research. I'm never \"trying to run MIRI;\" I'm just working on the next top-priority task.</p><p>This, I think, is one of the main distinctions between \"trying to try\" and \"actually trying\".</p><p>Trying to try to run MIRI would <i>feel like</i> just trying to run MIRI \u2014 it would feel like thinking about what it takes to run an institute and reading books about running institutes and worrying whether the board of directors thought I was doing a good job and so on. From the inside, I'd probably think I was trying very hard to actually run an institute.</p><p>Actually trying to run MIRI feels very different from the inside. It doesn't feel like trying to make an institute run, it feels like trying to get all the most important emails handled while not letting administrative duties suck up my day. It feels like struggling to prioritize three important tasks that can't all be done. Actually trying to run MIRI does not feel like trying to run MIRI, it feels like a never-ending stream of smaller tasks.</p><hr><p>I think many people imagine the difference between trying to try and actually trying involves something like Additional Effort or Additional Willpower. It's easy to imagine someone trying to try to (say) cure aging. Maybe they flounder around a bit and talk about how they want to join a biology startup, or start a biology startup, or get a biology degree, all while really deeply wanting to find some way to cure aging. It's also easy to imagine that the person \"actually trying\" to cure aging is doing something similar, but with more determination and a bit of pixie dust that makes things work out. The actually-tryer does the same things, but for them, the startup works through dint of sheer willpower; or they get a biology degree while winning so many accolades that they get to set up their own laboratory.</p><p>This isn't how I imagine \"actually trying.\" It's not trying-to-try with extra gusto. Actually trying looks like solving small subproblems, with the more ambitious target no longer the focus of attention, but rather a background task. Actually trying to cure aging doesn't look like a person getting a biology degree with <i>especially grim determination</i>, it looks like Aubrey de Grey wading through a mountain of mundane tasks while scraping together enough money to keep <a href=\"http://www.sens.org/\">SENS</a> running.</p><p>(SENS is currently fundraising, by the way.)</p><p>If you want to solve hard problems, stop trying to solve the hard problem directly. Change the context such that that's a background assumption: all your actions are going to be pointed roughly in the direction of solving-the-problem; what next? What's the next thing that needs doing? Work on <i>that.</i></p><p>This is perhaps simple advice, but I myself have found it useful in the past. Many years ago, when I was in high school, a friend of mine came back from college having joined a fencing team. He wanted to show me some of the basics, so he tossed me a sabre, and we had at each other. We crossed swords a few times, and he said something along the lines of \"Nate, the goal isn't to hit my sword, the goal is to hit <i>me</i>.\"</p><p>It's an obvious thought, a simple thought, and a thought I had failed to think. After that, I wasn't trying to fence, I was trying to <i>hit him.</i></p><p>Or consider the scene in The Matrix where Morpheus tells Neo <a href=\"https://www.youtube.com/watch?v=5mdy8bFiyzY\">\"Come on, stop trying to hit me and hit me!\"</a> \u2014 at which point Neo's blows grow more intense, until he gets a fist past Morpheus' defenses. I suspect that many people watching that scene imagine Neo turning on the \"try harder,\" pouring more effort into his punches and harnessing his frustration. When I watch the scene, I imagine a little bit of that, but mostly I imagine a similar mental shift to my \"don't bang swords together; strike the enemy\" mental shift \u2014 I imagine Neo had mostly been throwing out a bunch of martial arts moves that had recently been uploaded into his brain, in attempts to see if any of them worked against Morpheus, and that when Morpheus said \"stop trying to hit me and hit me\" Neo thought \"oh yeah, I'm not supposed to be deploying martial arts moves and monitoring whether I'm fighting well enough, I'm supposed to be <i>hitting Morpheus</i>,\" and that his brain shifted from the \"expected to try\" gear to the \"competence assumed\" gear.</p><p>I think many people solve problems more effectively in the \"competence assumed\" gear,\" when they're not fretting about whether they can solve problems because they're too busy fretting about very specific actionable subproblems.</p><hr><p>So if you want to tackle big problems, my advice is this: If you ever find yourself saying \"I'm currently trying to solve [problem]\", be wary. This is doubly true if you're Expected To Do Your Best.</p><p>If you find yourself saying \"well I'm trying to solve aging, but it's a big problem, so I'll likely fail,\" then stop in your tracks. Not because of the underconfidence \u2014 aging <i>is</i> a big problem and you <i>will</i> likely fail to solve it \u2014 but because you're sprinting up and down the field when you'd be better off playing a game of soccer.</p><p>If you approach a big problem with Intent To Try, then at every plausible stopping point part of you will be trying to convince you that you've done enough. And thus, at every plausible stopping point, you'll need to spend willpower to continue. Find a soccer game instead \u2014 some way to focus your attention on useful object-level tasks, with the pursuit of the important goal turned into an implicit unconscious background assumption so deeply ingrained in your plan that you can hardly see it any more.</p><p>As for how you make or find the soccer games, that's a discussion for another day. For now, my generic suggestion is to (a) generalize from the above examples and (b) imagine someone who's \"playing soccer\" with respect to your task or problem, and ask yourself what they might be doing. The key is to make the pursuit of your goal implicit, and spend your focus on the subproblems.</p>", "user": {"username": "So8res"}}, {"_id": "XxQwh9GJ5tjJQ3Xe4", "title": "Dark, not colorless", "postedAt": "2015-11-16T06:00:00.000Z", "htmlBody": "<p>The last arc of posts has been about how to handle a dour universe. <a href=\"http://mindingourway.com/being-unable-to-despair/\">Become unable to despair</a>, learn to <a href=\"http://mindingourway.com/see-the-dark-world/\">see the darkness</a> rather than flinching from it, learn to <a href=\"http://mindingourway.com/choose-without-suffering/\">choose between bad and worse without suffering</a>. Learn to live in a grim world without <a href=\"http://mindingourway.com/detach-the-grim-o-meter/\">becoming grim yourself</a>, learn to <a href=\"http://mindingourway.com/simply-locate-yourself/\">hear bad news without suffering</a>, and <a href=\"http://mindingourway.com/have-no-excuses/\">stop needing to know your actions were acceptable</a>. <a href=\"http://mindingourway.com/come-to-your-terms/\">Come to terms with the fact you may lose</a>, <a href=\"http://mindingourway.com/transmute-guilt-i/\">use the darkness as a source of fuel</a>, and <a href=\"http://mindingourway.com/best-you-can/\">let go of dreams of total victory</a>. These are the tools I use to tap into intrinsic motivation, in a precarious world where the problems are larger than I am.</p><p>Where others see a hurting world and feel guilty for not doing enough to help it, I see a hurting world and feed my own resolve. Instead of feeling guilty for not working until I drop, I recognize the <a href=\"http://mindingourway.com/stop-before-you-drop/\">psychological impossibility</a> and resolve to do everything I can <i>within</i> my <a href=\"http://mindingourway.com/not-yet-gods/\">mortal constraints</a>. For me, at least, this internal drive is more robust and reliable than guilt motivation.</p><p>This brings us to the end of the penultimate arc of the \"replacing guilt\" series of posts, which I began many months ago, and takes us into the final arc. The <a href=\"http://mindingourway.com/replacing-guilt/\">first arc</a> was about addressing the listless guilt that comes from ignoring a part of yourself that wants to be doing something more. The <a href=\"http://mindingourway.com/should-considered-harmful/\">second arc</a> was about eliminating the feeling of obligation, and fighting for something you care about <i>only because you care about it.</i> The <a href=\"http://mindingourway.com/stop-before-you-drop/\">third arc</a> was about coming to terms with your limitations and learning to optimize <i>within</i> them, rather than feeling guilty because of them. This post concludes the <a href=\"http://mindingourway.com/being-unable-to-despair/\">fourth arc</a>, about living in a dark universe and tapping into resolve instead of guilt.</p><p>The fifth and final arc is about what you do next. Once you've removed guilt and replaced it with intrinsic drive \u2014 both cold resolve and hot desire to <i>make the future bright</i> \u2014 what do you do next? What thought patterns allow one to turn these feelings into <i>actions</i>, rather than feelings of frustration and impotence?</p><p>I'll explore some of my answers to those questions in the coming handful of posts. But before then, I have one reminder I'd like to pass along.</p><hr><p>Among all this talk of coming to terms with a dark and dour world, I ask you to remember that the world is <i>dark</i>, but it is not <i>colorless.</i></p><p>I have seen many a friend attempt to see the dark world and then despair (for they are too small and the problems too large), and then confuse their sense of hopelessness with a sense of <i>meaninglessness.</i></p><p>(The reasoning goes: \"If the universe is so large, how can I matter? If the world is in such deep trouble, how can I make a difference? If all this were true, nothing would matter.\")</p><p>So consider this a gentle reminder that a dark world is not a <i>lost</i> world. It is not a grey world, where everything is dead and there is nothing we can do. It is not a cold empty universe, from which nothing can be built. It is simply a <i>damaged</i> world, a <i>hurting</i> world, that is intolerable precisely because it could be so much better.</p><p>If you gazed upon a worthless universe, all cold and dead, the sight would likely not fill you with despair \u2014 because while there is no light, while there are no happy sapients living full lives, there is also no darkness: that universe is empty and dull. If you gaze upon our universe and despair, then, then that can only be because there is so much that is not right, but <i>could be.</i></p><p>While our world is dark, it is still filled with color, and indeed many spots of light and even brilliance. Children laugh. Lovers meet. Right now, someone is just understanding one of the deep secrets of how the universe works for the first time, and their mind is filling with awe. Right now, someone is building a close friendship for the first time in a decade. Every day bears witness to a billion acts of love and kindness. This world is dark, yes \u2014 150,000 people die every day \u2014 but it is not <i>lost.</i></p><p>So don't let despair or hopelessness weigh you down. Instead, let them be a reminder: those are feelings you can only get from something worth saving. There are things here that are worth fighting for. If you begin to despair, then let that feeling be a reminder of what could be, and let everything that this world <i>isn't</i> be your fuel.</p><p>The world may be dark, but it's not colorless.</p>", "user": {"username": "So8res"}}, {"_id": "wGKn97bsfqEJg32Lf", "title": "The best you can", "postedAt": "2015-11-10T06:00:00.000Z", "htmlBody": "<p>In fiction, protagonists narrow their focus until the difference between success and failure on their specific task seems like the difference between victory and defeat. Batman attempts to solve the mystery while ensuring that nobody dies; meanwhile, children in Africa suffer from Malaria. The crew in <a href=\"https://en.wikipedia.org/wiki/The_Martian_(Weir_novel)\">The Martian</a> spends billions of dollars worth of capital to save one man; capital that could have been spent curing diseases.</p><p>Real people run a risk of duplicating this error, if they try to find the very best action available.</p><hr><p>It's easy to paralyze yourself if you try to do the \"right thing.\" There's always more uncertainty to be had. There's always more information you could gather. It's hard to become confident that you're doing the right thing. This can lead to paralysis, and persistent inaction.</p><p>It's much easier, I think, to stop asking \"is this action the right action to take?\" and instead ask \"what's the best action I can identify at the moment?\"</p><p>Sometimes, the best action you can identify is \"search for more alternatives.\" Sometimes, it's \"study more\" or \"learn more.\" Sometimes, it's a specific action. The nice thing is that \"what's the best action I can find in the next five minutes?\" always has a concrete answer. If you search for that, instead, you won't get paralyzed.</p><hr><p>Spoiler alert: you can't find the \"actually best\" action. Insofar as there <i>is</i> an \"actually best\" sequence of motor outputs your brain could produce, it's a mad convoluted dance that leverages butterfly effects to reforge the world overnight. You're not going to find the \"best action.\" And the best action you <i>can</i> find is exactly what it sounds like \u2014 the best action you're able to find.</p><p>You never have enough information to make a fully informed choice. You never have enough time to consider all the possibilities, or weigh all the evidence. You are always biased; your brain is compromised. The problem before you is too hard, and no matter what you do, a billion more people are going to die.</p><p>No matter what gambles you take, no matter how risky or cautious you are, you're trading off some possible futures against other ones. You can't save them all.</p><p>All you can do is look at your actions, and take the best one you can find.</p><hr><p>It's easy for humans to zoom in to the game we think we're playing, and try to win <i>completely</i>, to solve the mystery without letting <i>anyone</i> die.</p><p>It's easier to remember to pick the best action you can find, rather than striving to do the \"right thing,\" if you remember that people have already died; that the threshold has already been crossed. That we're not playing for a \"total victory\" any more, that we've already missed our chance at a \"perfect score.\"</p><p>This is a battle we've already lost.</p><p>A hundred billion people have already died.</p><p>Rome fell. The barbarian hordes flooded through its gates. There were a thousand years of darkness.</p><p>We've already missed our shot at a total victory. Now we're just building the best future we can.</p><p>So don't get paralyzed looking for the right thing to do. Just find the best action you <i>can</i> find, and do that.</p>", "user": {"username": "So8res"}}, {"_id": "RangJzh8zkEEwtMMJ", "title": "Transmute guilt into resolve", "postedAt": "2015-11-01T05:00:00.000Z", "htmlBody": "<p>A friend of mine came to me and said that he cares about his immediate friends, and he cares about humanity in the abstract, but he has trouble caring for most people. They seemed too shallow, too bitter, too spiteful to be worth an effort.</p><p>He'd been a sixth grade teacher, so I asked, \"What about when they were eleven? Were they worth an effort then?\"</p><p>\"Yes,\" he answered adamantly. Or, most could still be salvaged at eleven, though there are some that you'd need to get to even earlier, if you wanted to save them from the shallowness and the learned helplessness and the death of curiosity.</p><p>\"So then we live in a world that mishandles its youth, that turns them from bright children full of potential into empty shells. What are your feelings about that process, and the people subjected to it?\"</p><p>His answer, more or less, was \"A bit of anger, a bit of nothing-I-can-do-about-it, and a bit of victim-blaming, which I don't endorse.\"</p><p>Those last two emotions are very interesting: Why assure yourself that there is nothing you can do about the problem, if you don't care about the people who are harmed? Why assure yourself that it is their fault, if you stop caring about people once they are lost?</p><p>These seem like defense mechanisms, to me \u2014 defense mechanisms my friend generated unconsciously, because it was too painful look at bitter shallow adults and see lost mistreated eleven-year-olds.</p><hr><p>Most of the time, if something is hurting you, I recommend making it stop. There is one exception, though.</p><p>Imagine walking past a beggar on the street. They're dirty and downtrodden; weathered but not much older than you. They ask you for change as you pass by.</p><p>This causes a certain type of pain in people \u2014 enough pain that most people develop some sort of coping mechanism. Some people pretend they didn't see or hear the beggar. Some give an apology, some make up an excuse about not having any money. Some shove their hands in their pockets and drag out some spare change, so that they may discharge their moral duty.</p><p>Other people cope with cynicism or bitterness \u2014 the sight of a beggar reminds them of the failings of the hated out group, the people who voted for the Wrong Political Party in the local elections. Still others cope with a wave of guilt, shorting out the pain, because the guilt seems easier to bear.</p><p>My suggestion, this week, is notice that impulse. Notice the impulse to look away, to ignore, to make an excuse, to assure yourself that there's nothing you can do, to blame the hated out-group.</p><p>Resist the impulse, and acknowledge the pain. Sit with the pain. Don't excuse yourself from it, don't tell yourself a story about how there's nothing you can do or about how your attention and effort can be better spent helping other people elsewhere. That may be true, but it's another coping mechanism, and it also shorts out the pain.</p><p>Instead, I suggest sitting with the pain, and transmuting it into resolve.</p><hr><p>There are many people for whom guilt is the right response, when ignoring a beggar. If you're not doing anything to leave the world nicer than it was when you found it, if you're not doing anything to help your fellow human beings, if you're not doing anything to shape the grand story of Humanity as it plays out all around you, and if you <i>want</i> to be helping, then guilt is a healthy reminder that you've betrayed some part of yourself.</p><p>This is why my \"replacing guilt\" series began by addressing the <a href=\"http://mindingourway.com/replacing-guilt/\">listless guilt</a>, all those months ago. Sometimes, guilt is a reminder that you're not doing what you think is right, and those reminders can be valuable.</p><p>But most of the guilt-motivated people I know don't match that pattern. Many of them are dedicating their lives to making the world a better place, and they can do far more good by focusing their attention on their work and their health than they can by worrying over one beggar in the street, or over a thousand starving families that they can do nothing to save. They berate themselves for not <a href=\"http://mindingourway.com/stop-before-you-drop/\">needing less rest</a>, for not being able to do the <a href=\"http://mindingourway.com/where-coulds-go/\">psychologically impossible</a>, for not being as smart or as productive or as wealthy or as kind as those around them.</p><p>I say: Yes, the beggar suffers. Yes, a thousand families starve. The world is hurting.</p><p>And yes, there are others who are doing more than you to help. Some are smarter, some are more productive, some were born wealthier, some are kinder, some are less psychologically fragile, some have a stronger will.</p><p>But none of these are reasons for guilt. <a href=\"http://mindingourway.com/dont-steer-with-guilt/\">Guilt was made for us, not us for it</a>. Guilt is useful only insofar as it helps you wrest yourself from the wrong path. If you're already walking the path you want to walk, if you're working on becoming kinder, or more generous, or psychologically stronger, or wealthier, or smarter, if you're already moving as fast as you can given your current constraints, then the fact that the world is still hurting and you aren't strong enough to fix things yet is no reason for guilt.</p><p>Rather, it's a reason for <i>anger,</i> at a world where nobody is evil but everything is broken. It's a reason for <i>resolve,</i> to push yourself as hard as is healthy and sustainable <a href=\"http://mindingourway.com/rest-in-motion/\">but no harder</a>.</p><p>But it is not a reason for guilt, once you are doing what you can, in full light of the fact that you are <a href=\"http://mindingourway.com/not-yet-gods/\">still only mortal</a>.</p><hr><p>There are dozens of opportunities to transmute guilt, or awkwardness, or not-my-problem into resolve, each day.</p><p>Notice the disabused middle-aged woman who has to sacrifice a part of her soul working a job at Starbucks in order to earn her right to survive. See the madman yelling across the street, while everyone else reflexively struggles to ignore or unsee him. See a morbidly obese person avoid the stares of onlookers as they struggle with self-loathing in a civilization that filled its cheapest foods with poisons that ravage bodies.</p><p>Some people ignore these painful parts of the world. Others try to unsee them. Others try to distance themselves, by poking fun at those who are deemed \"pathetic.\"</p><p>I suggest seeing them, and remembering. Remember that there may come a time when humanity will move the very stars to ensure that no mind suffers as much as a first-world beggar does today. Remember that, beneath all the mental callouses that allow you to write fellow human beings off as unsalvageable, the reason you won't help them is not because they aren't worth helping, but because there are too many other things that need doing first.</p><p>So notice your impulse towards guilt. Notice your impulse to ignore. Notice your impulse to distance yourself from people you don't want to acknowledge. Notice your impulse to assure yourself that it's not your fault, that there's nothing you can do, that you can't help them because it's <a href=\"http://givewell.org/\">cheaper to help other people suffering just as much abroad</a>.</p><p>Then stop following those impulses. <a href=\"http://mindingourway.com/see-the-dark-world/\">See the dark world</a>. Acknowledge the pain, and remind yourself that we live in a universe <i>worth changing.</i></p><p>Remind yourself that you're a part of the grand human story, and that when our children's children's children hear about the amount of suffering we had to pass over in combat of greater evils, they will shed tears.</p><p>The count of people we have to leave behind can be a persistent source of pain. But don't let it be a persistent source of <i>guilt.</i> Instead, let it be a reminder that the universe is vast and uncaring, and that our job here is unfinished.</p>", "user": {"username": "So8res"}}, {"_id": "d7S4deQwxtdpZPphQ", "title": "Come to your terms", "postedAt": "2015-10-26T05:00:00.000Z", "htmlBody": "<p>Once, a friend of mine decided to make a drastic career change by teaching themselves a bunch of new skills from scratch, (with occasional assistance from me). They ran into occasional difficulties along the way, one of them being that they could not consider the possibility of failure without feeling fear.</p><p>The possibility of failing \u2014 of investing months in the effort, with nothing to show for it, and then having nowhere left to turn \u2014 weighed heavily on them. It wore them down, it caused great stress, it induced panic attacks. Sometimes, they were incapacitated to the point that they could hardly think.</p><p>This wasn't completely unreasonable: they had no safety net and no margin for error, and they had good reasons to fear for their personal safety in the event of failure. The problem was not that their fears were irrational. The problem was that they <i>couldn't think them.</i></p><p>I encouraged them to try facing their fears, and they did, but they found that coming to terms with the worst was impossible. They <a href=\"http://mindingourway.com/being-unable-to-despair/\">buckled, rather than buckling down</a>. So consider that a content note: the exercise I describe in this post may not be possible or helpful for you.</p><p>But it has been very helpful for me, and I continue to think that if my friend had been able to truly come to terms with the worst case scenario they had in mind, to imagine it in detail and accept it as a possibility, then they would have had a much easier time managing that stress.</p><hr><p>So here's my advice: Think the unthinkable. Consider that which is painful to consider. Figure out what, exactly, is at stake. Weigh the consequences. Come to terms with them.</p><p>I'm <i>not</i> suggesting that you convince yourself the worst case actually wouldn't be that bad. I'm <i>not</i> suggesting that you tell yourself a story about how you could handle the worst. I'm saying, <i>come to terms with what could happen.</i> Imagine the worst case, in detail; learn to weigh it on your scales; accept that if you fail things could go very poorly; and then maybe those bad outcomes will loosen their grip on you.</p><p>If you ever notice yourself following the same pattern as my friend \u2014 if you ever notice an outcome <i>so terrible that you can't even consider it without panicking,</i> then I suggest that you pause, take a deep breath, and consider that outcome.</p><p>Visualize it in full detail. Don't need to excuse it. Don't tell yourself it wouldn't be your fault. Don't tell yourself it would be fine. Don't make up a story about how you'd handle it successfully. Just <i>imagine the worst.</i></p><p>People close to you might get hurt. You could die. Lots of people could die. If bad outcomes are in the possibility space, internalize that <i>now.</i> Come to terms with that terrible fact as soon as you can. You want to get into a mental state where if the bad outcome comes to pass, you will only nod your head and say \"I knew this card was in the deck, and I knew the odds, and I would make the same bets again, given the same opportunities.\" If you need to panic, panic once and get it over with. Otherwise, fear will strike again every time the bad outcome moves a millimeter closer, and that fear may debilitate you or incapacitate you at a crucial moment.</p><hr><p>It's the thoughts you can't think that control you most, and it's the outcomes you can't consider that weigh heaviest on your scales.</p><p>An outcome that you can't consider without panicking \u2014 failing a class, crashing a car, destroying the family business \u2014 weighs infinitely heavily in your considerations. You can't even <i>think in the direction</i> of allowing the bad thing to happen, without encountering a cloying fear that steers your thoughts away. It is as if the bad outcome has infinite weight on your scales. Your thoughts become censored; you become unable to rationally weigh the risks and gambles.</p><p>Once you've fully considered the terrible outcome, its weight on your scales becomes finite. It may remain heavy, it may be the overriding concern in your life, it may still dominate your actions. But once you've weighed the outcome, it can only dominate your actions if you decide that that's rational, after weighing the possibilities and tradeoffs.</p><p>And maybe, after seriously considering the terrible outcome, it will <i>stop</i> dominating your actions. Maybe it will seem less terrifying once you drag it into the light. Maybe it will seem more manageable after you consider how you'd <i>actually</i> manage it. Maybe you'll notice that the outcome wasn't as terrifying as it seemed at a distance.</p><hr><p>In my line of work, I occasionally find myself in conversations with powerful people in situations where the outcome of the conversation has some small chance of dramatically affecting the future of humanity and all earth-originating life. The first time I found myself in one of these conversations, I was fairly shaken afterwards.</p><p>During the conversation, there was a sensation not unlike the one I got as a young driver on the interstate, realizing that I could, with a trivial twist of my hands, steer the car into oncoming traffic. After the conversation, there was a fear that had a lingering effect on my thoughts. I was jumpier. My actions were less considered. I was flustered.</p><p>A friend of mine (who had been through this before) noticed, and asked me whether I'd ever really come to terms with the fact that I just might set into motion a chain of events that leads to the end of the world.</p><p>I said no.</p><p>But, amusingly enough, I <i>had</i> spent time coming to terms with the fact that I might ruin my <i>own</i> life, and die old and bitter and unaccomplished.</p><p>I remember <i>that</i> ritual quite well: I was 18 at the time, and I had (a few years prior) decided to dedicate my life to <a href=\"http://mindingourway.com/on-saving-the-world/\">changing the world</a> in a big way. I was aware of the odds stacked against me, and I was aware of the success rates, and I was fully aware of the fact that, in all likelihood, I was going to fail, and my ideas were going to prove defunct, and my plans were never going to come to fruition.</p><p>I imagined that I could well end up a bitter old man, bemoaning plans that should have worked, to people who only scoffed. Now, I also planned <i>not</i> to become that bitter old man \u2014 but in those days, I wasn't yet sure how much control I'd gain over my own mind, and I saw lots of bitter old men around me. I was wary that my plans to avoid bitterness would <i>also</i> fail, and I'd become bitter and old despite my best efforts.</p><p>As I attempted to get a few different schemes started, and I noticed myself holding back a part of myself, in case my plan was just too crazy, in case I would be too harshly judged for trying. Introspecting, I concluded that I was resisting because I was afraid of ruining my own life.</p><p>So, knowing that a chance of becoming a bitter old man with little money, no respect, and nothing to show for it was one of the prices I might need to pay, I decided to come to terms with that fact once and for all. I spent time imagining this outcome in detail. I didn't try to explain it to myself, I didn't try to tell myself stories about how I'd avoid the outcome, I didn't try to tell myself it would be OK. I just pictured what would happen, considered the cost, weighed the price, and deemed the possibility of failure a price worth paying.</p><p>I didn't convince myself it would be <i>OK</i>, but I did decide that a chance of a not-OK outcome was a price worth paying.</p><p>And then those fears released their grip on me.</p><p>So when I was shaken by that high-stakes conversation, and my friend asked whether I had ever come to terms with the fact that I might set into motion a chain of events that leads to the end of the world, I laughed, and said no, but that I had done something similar, and that I knew the ritual. It was a simple task to repeat it, to go through the same mental motions but with larger stakes in mind.</p><p>Now, I'm a bit harder to shake.</p><p>(I'm sure this was not the only way I could have gotten used to high-stakes conversations, and undoubtably exposure alone would have eventually had a similar effect. Nevertheless, this mental ritual sped up the process quite a bit, and I'm under the impression that it's helped me think more clearly when making high-stakes decisions across the board.)</p><hr><p>So, I say, if there are outcomes before you that seem unthinkably terrible, then come to your terms with them. Don't explain them, don't excuse them, don't tolerify them, simply <i>visualize</i> them, and come to terms with the prices that you might need to pay.</p><p>You may be hurt. People you love may be hurt. You might break things that can't be fixed. The world might actually end. The point is not to convince yourself that you could handle the worst if it came, because maybe you won't be able to handle it. The point is simply to <i>know what the worst case looks like.</i></p><p>If you know what it looks like, you can do your best to avoid it. The outcomes you can't consider control your actions. If you want to avoid the worst outcomes, you need to be able to weigh <i>all</i> the outcomes on the scales.</p><hr><p>(For those of you who are wondering, fear not; my friend ultimately succeeded in switching careers.)</p>", "user": {"username": "So8res"}}, {"_id": "ARgPRWHbx67NiwCZr", "title": "Have no excuses", "postedAt": "2015-10-19T05:00:00.000Z", "htmlBody": "<blockquote><p><i>Except in a very few [tennis] matches, usually with world-class performers, there is a point in every match (and in some cases it's right at the beginning) when the loser decides he's going to lose. And after that, everything he does will be aimed at providing an explanation of why he will have lost. He may throw himself at the ball (so he will be able to say he's done his best against a superior opponent). He may dispute calls (so he will be able to say he's been robbed). He may swear at himself and throw his racket (so he can say it was apparent all along he wasn't in top form). His energies go not into winning but into producing an explanation, an excuse, a justification for losing.</i></p></blockquote><p>\u2015 C. Terry Warner, <i>Bonds That Make Us Free</i><br>&nbsp;</p><p>Throughout high school and college, I noticed that many of my peers seemed like they were trying hard, but they weren't trying hard to learn content or pass classes \u2014 they were trying hard to make sure that they had good excuses and cover stories prepared for when they failed. Seeing this, I resolved that I would never excuse my own failures to myself \u2014 not even if I had a very good excuse. If you have an excuse prepared, you will be tempted to fall back on it. An excuse makes failure more acceptable, in some way. It's a license to fail.</p><p>If you really need to succeed on a task, then I suggest that you resolve to refuse to excuse your failure, in the event that you do fail. Even if the failure was understandable. Even if you failed for unfair reasons, due to things you couldn't have foreseen. Simply refuse to speak the excuse. <i>Understand</i> your errors, and learn from them, but if people demand to know why you failed, say only, \"I'm sorry. I wasn't good enough.\" You may add \"and I think I know what I did wrong, and I'll work to fix it, and I'll do better next time,\" but only if that's true.</p><p>Don't add anything else: if you want to play to win, you have to refuse to acknowledge excuses. If you were excused then you were helpless, and you couldn't have done better, and you can't learn to do better next time. Thus, I suggest that you become incapable of believing an excuse, lest you automatically slip into the game of making sure your failure will be explainable, rather than making sure you succeed.</p><hr><p>\"But sometimes bad luck just happens!\" the one protests. We can imagine a person who took a bet that pays out $1,000,000 nine times out of ten and costs $10,000 otherwise. We can imagine them losing. We can imagine them saying \"I should have gotten the money!\", and feeling upset, and complaining that the dice went against them, and cursing the fates. We can imagine them loudly trying to make sure that everybody present knows that the bet was worth taking, to make sure that their loss is excusable. And this person will be playing to ensure that their actions were acceptable; rather than playing to win.</p><p>I suggest, don't try to excuse bad luck. Don't call foul. Don't say that life was unfair. You're welcome to say \"I'm sorry, I made a bet and I lost. I'd make the bet again, though, knowing what I did then.\" Then you're still <i>owning the choice.</i> You're <i>owning the failure,</i> which is the important part. Only by owning the failure can you hope to adjust and do better next time: if you feel like you are allowed to curse the dice every time they go against you, and have your gabling excused as terrible luck by your peers (\"oh they're such an unlucky person it's not their fault...\") then you're never going to learn when to bet and when to abstain.</p><p>I suggest cultivating your mental habits such that it feels <i>bad</i> to check whether or not your failure will have an excuse. Refuse to have excuses. Refuse to cover your failures. Only then, without expected social protection, do you really start trying to figure out how to win.</p><hr><p>\"No really, sometimes unforeseen circumstances arise!\", the one protests again. We can imagine someone who was totally planning to get their paper done on time, but who got violently ill. It's true: unforeseen circumstances can wreck your plans. But you <i>know</i> about the <a href=\"https://en.wikipedia.org/wiki/Planning_fallacy\">planning fallacy</a> (or if you didn't, you do now). You've been a human being for a long time. You know the background rates on illnesses, and on unforeseen circumstances in general. Why didn't you work slack into your plans? Why couldn't you see those bullets coming in advance?</p><p>If you <i>did</i> work a lot of extra slack into your plans, and you still got burned anyway by extraordinary circumstances, then as before, you are welcome to answer \"I took a gamble and I lost, and I'd take the same gamble again at the same odds.\" You're welcome to calculate that the risk is worth the benefit, and then pay the price when your debts are called in.</p><p>If you <i>didn't</i> work in the necessary leeway, then you're allowed to say \"I'm sorry, I messed up.\" You're allowed to add \"and I learned something, and I will do better next time,\" <i>if that's true.</i></p><p>Will you <i>actually</i> ever learn to beat the planning fallacy, if you allow yourself to use excuses? Will you <i>actually</i> visualize the possible failures, and take an outside view, and learn to see the bullets coming before they hit you? Or will you simply expect extenuating circumstances to arise, and feel relieved when they do, because a plausible excuse has presented itself?</p><p>I have found that it's usually in the moment when I refuse to make excuses even if I do fail, that I start really trying to win in advance.</p><hr><p>\"But people <i>want</i> excuses. They're social creatures! They want to know what happened!\", the one protests.</p><p>Sometimes. Sometimes people really want you to provide them some excuse, or at least some explanation. But even here, be careful: I have noticed that my friends often help me try to excuse <i>myself</i>, for one reason or another, and I think that giving in to this pressure can be harmful.</p><p>Imagine someone who failed to exit an abusive relationship, despite three years of trauma. After they successfully exit, their friends are likely to be first in line with condolences along the lines of \"they were gaslighting you\" and \"there wasn't anything you could have done\" and \"how could you have known what to do?\"</p><p>They are providing excuses, and these are toxic. They rob you of your power. They rob you of your ability to say \"actually, I <i>could</i> have known, if I had been thinking more clearly. I <i>could</i> have acted differently, if I had known better. And that's the <i>good part</i>, because it means that I am not a helpless victim, because it means that I can learn how to become stronger. Because it means that I cannot be trapped in that sort of situation again.\"</p><p>Excuses rob you of your agency. Yes, many people will try to get excuses out of you, if they perceive you as putting too much pressure on yourself. <i>But that pressure is precisely the impetus to learn and adapt,</i> and if you can bear it, then I suggest you do.</p><hr><p>There are situations where failing to generate excuses will cost you socially, especially if you're in the presence of people who have recently been generating excuses for themselves. If three students give thin excuses for why they didn't finish their project on time, and you say only \"I'm sorry, I wasn't good enough, I think I know what I did wrong, I'll do better next time;\" then they are liable to glare at you. In refusing to generate an excuse when everyone else is doing so, you violate some unspoken pact of mediocrity.</p><p>Sometimes, other people need <i>you</i> to make excuses in order to help excuse the fact that <i>they</i> are making excuses, and if you violate this norm, they find themselves faced with their own shortcomings. This can lead to some uncomfortable situations, and the best advice I can offer you for those, is that they provide a wonderful opportunity for <a href=\"http://mindingourway.com/self-signaling-the-ability-to-do-what-you-want/\">self-signaling</a> that you will refuse to excuse your actions even under intense social pressures.</p><p>Note, too, that in many other situations, refusing to generate excuses <i>gains</i> you lots of social status. Yes, there are places where people view refusal to generate an excuse as a violation of the solemn pact of mediocrity, but I have found that the people I can gain most from dealing with, are by and large people who have a deep appreciation and respect for those who live up to their errors.</p><hr><p>Excuses have you looking out to the world to explain your failure, rather than revealing the weak points in yourself. Did the unexpected happen? Then learn how to expect better next time. Were you betrayed? Learn how to build tighter social bonds, and learn how to see betrayals coming sooner next time. Did the dice turn against you? Then own up to your bet and make sure you're only making worthwhile gambles.</p><p>For many, the mantra of \"find the failure in yourself, rather than in the world\" will be harmful and destructive. If you are motivated primarily by guilt or shame, then seriously consider ignoring this post's advice. If you are prone to <a href=\"http://mindingourway.com/being-unable-to-despair/\">buckling instead of buckling down</a>, then seriously consider ignoring this post's advice. If you are struggling with your self-image and your sense of self-worth, if you think <a href=\"http://mindingourway.com/there-are-no/\">some people are bad</a>, if you flinch away from <a href=\"http://mindingourway.com/see-the-dark-world/\">seeing the dark world</a>, then seriously consider ignoring this post's advice. Or if \"find the failure in yourself\" feels bad or destructive at the moment for any other reason, then please ignore this post.</p><p>But if you are done with guilt motivation, and comfortable with the fact that we are <a href=\"http://mindingourway.com/not-yet-gods/\">not yet gods</a>, and capable of <a href=\"http://mindingourway.com/detach-the-grim-o-meter/\">detaching the grim-o-meter</a>, then I strongly suggest that you have no excuses. Find the flaws inside yourself. Don't tolerify them. Accept them, and plan ways to address or route around them. If you can't see what you need to do better next time, then it's going to be tough to do better next time.</p><p>This is part of the toolset that I use to replace guilt motivation: <i>play to win.</i> Don't play to excuse your loss.</p><p>You don't need to win every time \u2014 but you do need to <i>learn</i> every time.</p><p>If you find yourself trying to proclaim circumstance unfair, explaining how you could not possibly have seen this coming, then stop in your tracks. An explanation of how you couldn't possibly have seen this coming is a social device, an attempt to ensure that others still think you are OK, that they think your previous actions were acceptable. It's fine to play that social game; social games occasionally need to be played. But first, <i>figure out how you could have actually seen that thing coming</i>, next time. That's the important part.</p><p>Excuses are a social artifact, a way to ensure that you don't lose face when you fail.</p><p>But we're not here to win a social game.</p><p>Despite what all the monkey instincts might tell you, you're not playing Life in a competition against all the other monkeys.</p><p>You're playing Life with the universe, and the stakes are the entire future.</p><p>In the end, you won't be measured by how good your excuses were for all the things that didn't turn out the way you wanted.</p><p>You'll be judged only by what actually happens (as will we all).</p><hr><p>\"It's not an excuse, it's an <i>explanation.</i>\"</p><p>Explanations are excuses.</p><p>Don't get me wrong, it's very important to <i>understand</i> your failures. Note, though, that there's a big difference between \"understanding\" that your stupid knee was acting up and the sun was in your eyes and luck turned against you, and understanding that you didn't train hard enough or anticipate adverse conditions well enough.</p><p>When trying to understand your failures, it's important to figure out what <i>you</i> could have done better, rather than generating a list of reasons you never could have won. If there were unforeseen circumstances, understand why you couldn't foresee them. If your knee was acting up, learn how to either address that next time or work it into your expectations.</p><p>(And be very wary, when figuring out what you could have done better, for hints of destructiveness and fatalism in your tone. Imagine someone who is betrayed, and shouts \"well I guess now I've learned to never trust anyone ever again forever!\" For all their guise of having learned, they are harming themselves. It seems to me that this self-harm has something in common with an excuse: it gives a false veneer of locating a problem internally (\"I am too kind and trusting\") while actually identifying the problem in the world (\"the world is bad\"). The right lesson to learn is likely never \"become completely unable to trust,\" it is likely more along the lines of \"learn how to build tighter friendships\" or \"learn how to read humans better.\" It can be often useful to check the advice you just gave yourself to see whether it was obviously destructive, before following it.)</p><p>The point of understanding your failure is to learn how to act better next time, and I recommend that you understand your failures whenever possible. But don't explain them away, and don't excuse them.</p><p>If you want to succeed, stop generating reasons why you never could have won, and play to win.</p>", "user": {"username": "So8res"}}, {"_id": "JkcEMP56YmvuuouSH", "title": "Simply locate yourself", "postedAt": "2015-10-11T05:00:00.000Z", "htmlBody": "<p>Imagine I offer you the following bet: I'll roll a fair ten-sided die. If it comes up 1-9, you win a million dollars. If it comes up 0, you lose $10,000. (If you're significantly richer or poorer than the median person, adjust the numbers up or down accordingly, such that winning is very great and losing hurts a lot, but is manageable.) Imagine that you take the bet, because those odds are ridiculously in your favor. Now imagine that I roll the die, and you watch it rolling, and rolling, and rolling, until it starts to settle, and then it settles\u2026 on 0.</p><p>Imagine the sinking feeling you might get, as you see the zero, and realize that you have to give me ten thousand dollars. Maybe you suddenly feel uncomfortable. Maybe you're unwilling to meet my gaze. Maybe you're angry, or slightly sick to your stomach. Maybe some part of you is pushing against reality, trying to deny it, willing the past to <i>change.</i></p><hr><p>Now imagine a second bet. This time, imagine a world that has figured out cloning and cryonics and space travel. The bet works as follows: I put you to sleep, and then I separate you into ten identical copies (none of which have any more claim to being the original than any other), and then I put them all into stasis. Your possessions are replicated ten ways, and the ten yous are put on ten ships to ten different (already-colonized) planets. On nine of those planets, the local you will be placed in a room with blue walls, and given your possessions along with a million extra dollars. On one of those planets, the local you will be placed in a room with red walls, and will have $10,000 removed from their possessions. Then all ten yous will be awoken. Thus, nine copies of you will gain a million dollars, and one copy of you will lose ten thousand dollars.</p><p>Imagine that you understand this procedure, and consent to it. You're put to sleep, and split into ten copies, put into stasis, sent to ten planets, and revived from stasis. You wake slowly, and haven't opened your eyes yet. You know that nine yous will wake in a blue room and find themselves rich, and one you will wake in a red room and find themselves poor, and you don't know which you you are. You open your eyes, and the walls are\u2026 red.</p><p>In one sense, you've lost exactly the same sort of bet as the first bet. But there's a very different way that you might be feeling. In the second bet, instead of feeling a sinking feeling and a desire to push against reality, you may simply nod, and say \"ah, I'm the me in the red room.\"</p><p>Instead of treating the red walls as an unwelcome message about reality failing to go the way you wanted, you might treat them as a simple indicator of <i>where you ended up.</i> Instead of feeling despair, you may simply feel like you've figured out which you you are.</p><hr><p>Most people seem to treat most of their observations as Bet 1 type observations: they treat their observations as information about how the universe <i>turned out to be,</i> which may be quite a bit worse than they were hoping it would turn out. They feel despair, or resistance, or victimized by an unfair universe. Part of them tries to <a href=\"http://mindingourway.com/see-the-dark-world/\">tolerify</a>, some part of them flinches away from facing reality, and so on.</p><p>There's another way to treat your observations. It's the Bet 2 way: treat them simply as information about <i>where you ended up.</i></p><p>Imagine, on the one hand, Bet 1 as described above. Now imagine the same bet, but with a special die that generates ten copies of you (in different branches of the multiverse that are identical except for the number this die shows, separated such that the universes within them can never interact), such that nine of them will win a million dollars and one will lose ten thousand dollars.</p><p>Notice how someone who loses the former bet may try to push against reality, while someone who loses the latter bet has a much easier time simply saying \"Huh, I guess I'm the one in the 0 branch. Such was the price for nine out of ten multiverse branches to have rich versions of me, and now I will pay it.\"</p><p>But these are, more or less, the same bet. Why do they feel so different?</p><p>I say, <i>always</i> treat your bets like the latter sort of bet. Stop struggling against the bad news. Treat it not as bad news about how reality went, but rather treat it as you would treat information about <i>where in the multiverse you ended up.</i> Try <a href=\"http://mindingourway.com/be-a-new-homunculus/\">being a new homunculus</a>. Look around you and figure out where you just landed, regardless of where past you thought they should have landed. Often, the place will be in worse shape than past-you was expecting, but that has little bearing on what you do next (aside from updating your current anticipations such that future-you is less wrong).</p><p>Imagine you're a new homunculus that has just landed in a branch of the multiverse where things were going poorly\u2014maybe you recently lost social status, or made a choice that had worse effects than you expected, just before the new homunculus teleported in. This is an uncomfortable place to find yourself in! What do you do next?</p><p>Would you immediately throw a fit? What's the point of that? You just teleported into this part of the multiverse; how is struggling against the past supposed to help you? This is part of what <a href=\"http://mindingourway.com/detach-the-grim-o-meter/\">detaching the grim-o-meter</a> is all about: if you found yourself in a grim part of the multiverse, what would you do? Would you go around frowning and being dour all day? No? Because that sounds silly? Then there's no need to do that here!</p><p>Your observations are not messages that the world is full of terrible unfair luck. Your observations are simply indicators as to <i>where you are.</i> They're the data that you need to locate yourself.</p><p>Spoiler alert, you're currently located in a fairly precarious portion of the multiverse, where sentient beings are suffering and dying, and the future is hanging by a thread. It's worth cleaning this place up a bit, I think. But don't suffer about the poor state of affairs! Consider: if you <i>were</i> teleported to a precarious branch of the multiverse, what would you do upon arriving? Would you make sure to have a good time anyway? Would you do whatever you could to help out? Well then you're in luck! You <i>did</i> just arrive at a precarious part of the multiverse, and those are both things that you can do here.</p><p>When you get bad news, don't suffer over it. It's not unfair, it's not passing judgement, it's not a signal that everything sucks, it's not making the future worse. It's just telling you where you live.</p><p>And recently, you've ended up in the same part of the multiverse as I have. It is fairly nice, as parts of the multiverse go: it supports life, and things are better now than they were in many of the past points along our timeline. Nevertheless, it does look a bit precarious, and it sure does need some tidying up.</p><p>So, let's get to work!</p>", "user": {"username": "So8res"}}, {"_id": "FHveQK2g3iAuzDFnD", "title": "Detach the grim-o-meter", "postedAt": "2015-10-05T05:00:00.000Z", "htmlBody": "<p>I'm betting that the <a href=\"http://mindingourway.com/being-unable-to-despair/\">last</a> <a href=\"http://mindingourway.com/see-the-dark-world/\">three</a> <a href=\"http://mindingourway.com/choose-without-suffering/\">posts</a> have given many readers an incorrect impression about my demeanor. It's easy to read those posts and conclude that I must be a grim, brooding character who goes around with his jaw set all day long.</p><p>Which is understandable, but silly. You don't need to carry a grim demeanor to draw strength from seeing the dark world. It's quite possible to deeply want the world to be different than it is, and tap into a deep well of cold resolve, and still also be curious, playful, and relaxed in turn.</p><p>This isn't a story, and we don't need to pretend to archetypes.</p><p>I've met many who are under the impression that when you realize the world is in deep trouble, you're obligated to respond by feeling more and more grim. Like a movie about a detective that's trying to save a kidnapped child: as the detective learns that the child is in more and more danger, they lock their jaw and become more and more grim and determined. Their respite comes only when the child is rescued.</p><p>That's narrative thinking, and we aren't in a narrative. You can break the trope. (In fact, I <i>encourage</i> you to break tropes as soon as you realize that you're acting them out.)</p><p>Many people seem to have this internal grim-o-meter which measures how grim the state of the world is, and they dutifully try to keep this calibrated. When they hear that they might be failing a class, they get a bit more grim, and this helps them buckle down. When they hear that there was an earthquake in Napal, they get a little more grim, and they maybe even feel guilty if they can't feel appropriately grim for appropriately long.</p><p>I say, it's good to have a grim-o-meter, but <i>stop calibrating it against the state of the world.</i> That's a terrible plan!</p><p>I mean, look at humanity at large. People are killing each other like it's going out of style, while millions die from disease each year and civilization careens towards self-destruction.</p><p>Now look at your grim-o-meter. It has, like, seven different settings. Maybe twelve, on a good day.</p><p>That detective in the movie about the kidnapped child might be able to faithfully use a twelve-setting grim-o-meter to track the grimness of their own situation.</p><p>But the real world? The one with billions of people each with rich inner lives, and astronomical future potential hanging by a pale blue thread in Time? There's no way you can justifiably connect a twelve-setting grim-o-meter to <i>that.</i></p><p>And what if you could? Would your grim-o-meter always be set to \"maximum grimness,\" at least until humanity makes it through the gauntlet? That doesn't sound very fun or useful. Would you rather calibrate the grim-o-meter so that it adequately captures the normal range of variance in the human condition over your lifetime? Because then your grimness is likely to fluctuate wildly in response to events that have little relevance to your daily life (such as aggregate demand shocks in China). That <i>also</i> doesn't sound very fun or useful.</p><p>Look: that's not what your grim-o-meter is <i>for.</i> It's not supposed to be attached to the global state of the world. Feeling grim or carefree in proportion to the aggregate disparity or well-being on the planet is difficult, impractical, <i>and</i> mostly useless.</p><p>Your grim-o-meter is designed for <i>local</i> occasions. You need to get more grim (and more buckled down) as the work <i>immediately in front of you</i> gets harder, and you need to get less grim (so that you can spend time recharging and relaxing) whenever you have the affordance to recharge and relax. That's the <i>point</i> of the grimness setting.</p><p>Remember, the grim-o-meter was made for you, not you for it. What's the point of grimness? The point is to be able to buckle down when down needs buckling. And buckling down is something you need to do occasionally, if you want to get things done. But so is being curious, and being playful, and being calm. <a href=\"http://mindingourway.com/not-yet-gods/\">You're still a monkey</a>, remember?</p><p>The world is dark and gritty, but that doesn't mean that you need to be dark and gritty to match. This isn't a book, and you can adopt whatever demeanor you need to adopt to get the job done.</p><p>You can look at the bad things in this world, and let cold resolve fill you \u2014 and then go on a picnic, and have a very pleasant afternoon. That would be a little weird, but you could do it! The resolve is a useful source of motivation, but you don't need to adopt a permanently grim demeanor in order to wield it. In fact, personal effectiveness is all about having the right demeanor at the right time.</p><p>I suggest a mix of playfulness, curiosity, relaxation, calm, and yes, grim determination.</p><p>I also personally recommend a healthy dose of dark humor. Everybody's dying, after all.</p>", "user": {"username": "So8res"}}, {"_id": "NXcdJWb8YFB4Dtarx", "title": "Choose without suffering", "postedAt": "2015-09-27T05:00:00.000Z", "htmlBody": "<p>Imagine Eve, who works a service industry job. Her manager tells her, at the last minute and without warning, that she has to staff an event tomorrow in a town a few hour's drive from where she lives, and she has to wake up at 5am to get there on time.</p><p>Let's further suppose that she's on shaky footing with her manager as it is, and so she is posed with the following choice: she can either wake up at 5am tomorrow and go to work, or she can lose her job.</p><p>Imagine Eve's demeanor, upon learning this fact. It's likely dour, to say the least. She's probably grumpy and annoyed and malcontent, and she's likely to vent and complain all evening. She'll likely spend a lot of cognitive effort <a href=\"http://mindingourway.com/see-the-dark-world/\">tolerifying the situation</a>, convincing herself either that it's not going to be that bad to wake up early, or that her manager is a terrible person.</p><p>This is a common occurrence, I think: if you give humans the choice between bad and worse, they get <i>grumpy.</i></p><p>When people find that none of their options cross a certain \"acceptability\" threshold, they get <i>frustrated</i>.</p><p>This, I think, is part of why tolerification is such a common human response to unfortunate situations. In an intolerable world, <i>none</i> of your options seem acceptable: so you tolerify, until at least one option (perhaps indignance, perhaps cynicism, perhaps doing nothing differently) passes the acceptablility threshold. Only then are you able to act.</p><p>This behavior won't do, for someone living in a dark world. If you're going to live in a dark world, then it's very important to learn how to choose the best action available to you without any concern for how good it is in an absolute sense.</p><p>When given a choice between bad and worse, you need to be able to choose \"bad\", without qualm.</p><hr><p>I think that one of the big reasons why people get annoyed when none of their options pass the \"acceptable\" threshold is they're often failing to see a hidden third alternative, and some part of them knows that this might be the case. In this setting, the frustration might even be <i>useful</i>, if it puts them in a mental state where they search more fervently for an escape hatch.</p><p>Furthermore, by acting flustered, people may well be able to draw other humans to their aid, and the additional assistance can often help make the situation better.</p><p>So frustration in the face of a choice between bad and worse may be a useful response in many situations. (At the least, it was useful enough to our ancestors.) Indeed, when you're offered the choice between bad and worse, the first thing to do is <i>look for a third option</i> and the second thing to do is <i>ask for help.</i> Find shortcuts. Try to cheat. Call in the cavalry, if you can.</p><p>But once you determine that you really have been offered a choice between bad and worse, and that there are no other options \u2014</p><p>Then it is useful to be able to choose \"bad,\" without suffering over it.</p><hr><p>The first step to being able to choose the best option available without suffering, is to simply understand the distinction. Next time you find yourself feeling flustered because none of your options pass an absolute acceptability threshold, pause and reframe, and look at the <i>relative</i> acceptability of your actions instead. Simply knowing the distinction and watching out for it in real life may well be enough.</p><p>For me, another useful tool for choosing without suffering is to ask a \"what if\" question about a hypothetical universe, before making a choice in the real world. Let's say I'm trying to eliminate extreme poverty, and none of my actions seem good. I might say to myself, \"imagine you lived in a world where all your choices led to bad outcomes; what would you do then?\" I can improve the lives of these three people, and then a million people will die of preventable disease anyway. Or I can try to alter the flow of politics, and then a million people will die of preventable disease anyway. Or I can put money into researching preventable diseases, and then a million people will die of preventable diseases anyway. No matter what I do, at least a million people will die of preventable diseases. What would I do in <i>that</i> world?</p><p>Clearly, the answer is \"whatever action saves the <i>most</i> lives.\" I sometimes find it easier to frame my real problems as if they were hypothetical, identify the answer <i>there</i>, and then apply that to the real world.</p><p>In the hypothetical worlds where there are no third alternatives and all the actions before you, it doesn't matter that all the actions lead to bad outcomes. The best choice is still quite clear: take the action that leads to the best outcome, and take it without remorse. In the hypothetical, confident that there are no alternatives, it's quite easy to imagine selecting the least bad option from a terrible lot. In fact, it's easy to imagine doing this without any impulse to complain or struggle, but instead only a grim resolve to do the best you can in a bad situation.</p><p>So in the real world, do the same. Notice when you're measuring your options against what you think <i>should</i> happen; notice when you're measuring the futures you can attain against the futures you <i>want</i> to attain; and treat that as a cue to reframe. Look at your actions available options again, and stop measuring them against an objective ideal, and start measuring them against each other. Look for cheats, look for third alternatives, look for ways out\u2026</p><p>\u2026and then, when you're done and you've considered all available options,</p><p>simply take the best action available.</p><p>Take it, without suffering, no matter how bad it is.</p><p>That is all there is to do.</p>", "user": {"username": "So8res"}}, {"_id": "Apsvzx5X5DWk5x9TG", "title": "See the dark world", "postedAt": "2015-09-20T05:00:00.000Z", "htmlBody": "<p>Consider fictional Carol, who has convinced herself that she doesn't need to worry about the suffering of people who live far away. She works to improve her local community, and donates to her local church. She's a kind and loving woman, and she does her part, and (she reasons) that's all anyone can be expected to do.</p><p>Now consider fictional Dave, who failed a job interview. When telling his friends the story, he emphasizes how the interviewers were biased against him, and how they asked stupid questions.</p><p>Meanwhile, driven by hunger, a fox tries to reach some grapes hanging high on the vine but is unable to, although he leaps with all his strength. As he goes away, he remarks \"<a href=\"https://en.wikipedia.org/wiki/The_Fox_and_the_Grapes\">Oh, you aren't even ripe yet! I don't need any sour grapes.</a>\"</p><p>All of these reactions \u2014 and many others \u2014 share a common kernel. Carol, Dave, and the fox are all inventing reasons why an unpleasant state of affairs is acceptable. They're not inventing reasons why the world is <i>good</i>, by any means; but they are putting forth cognitive effort to make it seem <i>tolerable</i>.</p><p>Carol would surely tell you that it's terrible that children are suffering abroad \u2014 but only after convincing herself that her duty to help them had been discharged.</p><p>The fox would tell you that the world is worse for being full of sour grapes \u2014 and yet, he still had to work hard to assure himself that he didn't live in a far worse world, where the grapes were both ripe and inaccessible.</p><p>There's a certain type of darkness in the world that most people simply cannot to see. It's not the abstract darkness: people will readily acknowledge that the world is broken, and explain how and why the hated out-group is responsible. And that's exactly what I'm pointing at: upon seeing that the world is broken, people experience an impulse to explain the brokenness in a way that relieves the tension. When seeing that the world is broken, people <i>reflexively</i> feel a need to explain. Carol can acknowledge that there is suffering abroad, but this acknowledgement comes part and parcel with an explanation about why she bears no responsibility. Dave can acknowledge that he failed to pass the interview, but his mind automatically generates reasons why this is an acceptable state of affairs.</p><p>This is the type of darkness in the world that most people cannot see: they cannot see a world that is <i>unacceptable.</i> Upon noticing that the world is broken, they reflexively list reasons why it is still tolerable. Even cynicism, I think, can fill this role: I often read cynicism as an attempt to explain a world full of callous neglect and casual cruelty, in a framework that makes neglect and cruelty seem natural and expected (and therefore tolerable).</p><p>I call this reflexive response \"tolerification,\" and if you watch for it, you can see it everywhere.</p><hr><p>The <a href=\"https://en.wikipedia.org/wiki/The_Fox_and_the_Grapes\">sour grapes fallacy</a> is a clear example of tolerification, but it's only one instance of the broader class. Tolerification occurs <i>any</i> time you see something bad in the world and feel an impulse to explain, especially if that explanation relieves pressure that would otherwise be placed on you.</p><p>Consider, for example, Alice and Bob in my <a href=\"http://mindingourway.com/the-value-of-a-life/\">allegory of the dragon</a>. Both have recently learned that the market value of a life is only a few thousand dollars. Both are uncomfortable with this, and they reflexively tolerify the information in different ways.</p><p>Bob denies the information, protesting that one can't make decisions by attaching dollar values to lives, because lives are sacred. This declaration of a sacred value allows Bob to deny the discrepancy entirely, reject the implied responsibility, and restore tolerability to the universe.</p><p>Alice, by contrast, accepts the data and denies the intuition that lives are sacred. She notes that if you act like lives are worth <i>more</i> than a few thousand dollars then you'll save fewer lives than you could, and thus anyone who acts otherwise and wants to save lives is inconsistent. Therefore, she concludes that she can't treat the intrinsic value of a life as worth any more than the market price, and grows cynical \u2014 not only are lives non-sacred, she realizes, but they're not worth that much more than a few thousand cans of coke. Now she can worry less about saving lives: they weren't worth as much as she thought, anyway. Tolerification successful.</p><p>Notice how their gazes slip to one side or the other, both of them failing to see the dark world \u2014 the one where lives are <i>both</i> nigh invaluable, <i>and</i> priced at $3000. The one where it's <i>reprehensible</i> to pretend that a life is worth only as much as a few thousand cans of coke, <i>and</i> this is how you have to price a life if you want to save as many lives as you can. The world with a grim gap between life's price and life's value. This is the world that both Alice and Bob both reflexively tolerify away from.</p><p>In me, tolerification is toxic to intrinsic motivation. If you want intrinsic drive, I suggest you train yourself to notice when your gaze slips to one side or the other. When that happens, focus, and stare directly at the dark world.</p><hr><p><i>Content note: the remainder of this post encourages you to contemplate and acknowledge significant difficulties in your own life. I assume that the reader is </i><a href=\"http://mindingourway.com/being-unable-to-despair/\"><i>resilient in the face of adversity</i></a><i>. If acknowledging adversity in your life is currently liable to harm you, consider skipping the rest of this post.</i></p><p>My favored tool for subverting the impulse to tolerify the intolerable (and thereby stare directly at the dark world) is to pose myself a \"what if\" question.</p><p>What if I lived in the world where it was <i>both</i> the case that lives are nigh invaluable, <i>and</i> it costs only a few thousand dollars to save a life?</p><p>What if I lived in the world where it was <i>both</i> the case that I failed the interview <i>and</i> it was because I lacked the requisite skill?</p><p>The default impulse, upon learning that I failed the interview, might be to tolerify. Someone prone to tolerification might automatically, reflexively, start listing ways that the interview was stacked against them, or reasons why the questions were stupid, or reasons why they didn't want the job anyway. Then they might jump directly into the next interview, with excuses already in hand for when they fail that one too. This illustrates one major way that tolerification can be harmful: it might prevent you from seeing what really needs to be done. The person who refuses to tolerify can seriously consider spending more time practicing, or switching careers. If necessary, they can acknowledge that they really need to get a job while still dramatically unqualified, and decide to play the numbers with full knowledge of what they're doing. If they tolerify, they have to act indignant when they fail. If they don't, they can face what needs to be done.</p><p>Refusing to tolerify in this situation can be <i>really really hard.</i> Saying \"It seems I am not yet be skilled enough to get a job in this field\" can be <i>tough</i>, especially when your livelihood depends upon the opposite being true (and double-especially if you think that past failures make you a \"<a href=\"http://mindingourway.com/there-are-no/\">bad person</a>\").</p><p>The nice thing about the \"what if\" question is that I don't need to <i>believe</i> that that's the actual world when pondering the \"what if\". I don't need to <i>acknowledge</i> that I am unqualified for the job, I can simply ask what <i>would</i> do if I were. This makes it easier to plan out what I would do if I could see the dark world, and having a plan often makes it easier to acknowledge that the world I'm living in is dark. (See also: <a href=\"http://lesswrong.com/lw/o4/leave_a_line_of_retreat/\">leaving yourself a line of retreat</a>.)</p><p>So, let's run through some what ifs.</p><p>What if we lived in the world where it was <i>both</i> the case that (a) unwanted pregnancies could ruin the lives of both mother and child <i>and</i> (c) unborn children were moral patients with a right to life? What would you do then?</p><p>What if we lived in the world where it was <i>both</i> the case that (a) people are living and dying in extreme poverty <i>and</i> (b) you really need a new car soon if you want to keep your job, but you could spare a few thousand dollars if you really had to. What would you do then?</p><p>What if we lived in the world where people do have souls, but they're implemented on brains made of meat that rots when you die?</p><p>What if we lived in the world where evolution built conscious predators, and conscious prey that suffers as it gets eaten alive?</p><p>What if almost nobody was evil, but almost everything was broken anyway? What if the hated out-groups <i>aren't</i> responsible for all the suffering?</p><p>I'm not claiming that these what-ifs are accurate. Rather, I offer this as a tool for staring the dark world directly in the face. Imagine the world that is as bad as it might be. Imagine the world were full of intolerable injustices. What would you do then?</p><p>Can you look upon those dark worlds and feel a sense of despair, of the world being harder to fix than seems acceptable? Do you get a feeling of bracing yourself for making terrible tradeoffs, because there are too many problems and you can't handle all of them? If so, that's good: that's what it feels like, to see the dark world.</p><p>The question is, what would you do <i>then?</i></p><hr><p>I'm not here to offer answers. Maybe your answer is \"well in that world I'd stop trying so hard and move to a cabin in the woods and try to forget how screwed up everything was.\" Or maybe your answer is \"in that case I'd rise to the challenge, no matter how terrible the odds.\" More likely, it's something else entirely. I'm not trying to feed you answers. I'm trying to help you refuse to tolerify, because there is a source of resolve that comes only when you see the dark world.</p><hr><blockquote><p><i>I have to believe this falsehood, because otherwise I would be unable to go on.</i></p></blockquote><p>This is something that I hear fairly frequently, either to my face, or in popular media. \"I have to believe in God; otherwise there would be no meaning in my life.\" Or \"It's a good thing humans are unrealistically optimistic; we wouldn't be able to handle reality.\" Or \"I have to believe that I'm going to get this job; otherwise I wouldn't be able to continue trying.\" Or,</p><blockquote><p><i>\"All right,\" said Susan. \"I'm not stupid. You're saying humans need... fantasies to make life bearable.\"</i></p><p><i>really? as if it was some kind of pink pill? no. humans need fantasy to be human. to be the place where the falling angel meets the rising ape.</i></p><p><i>\"Tooth fairies? Hogfathers? Little\u2014\"</i></p><p><i>yes. as practice. you have to start out learning to believe the little lies.</i></p><p><i>\"So we can believe the big ones?\"</i></p><p><i>yes. justice. mercy. duty. that sort of thing.</i></p><p><i>\"They're not the same at all!\"</i></p><p><i>you think so? then take the universe and grind it down to the finest powder and sieve it through the finest sieve and then show me one atom of justice, one molecule of mercy. and yet\u2014Death waved a hand. and yet you act as if there is some ideal order in the world, as if there is some...some rightness in the universe by which it may be judged.</i></p><p><i>\"Yes, but people have got to believe that, or what's the point\u2014\"</i></p><p><i>my point exactly.</i></p></blockquote><p>\u2014 Terry Pratchett, <i>Hogfather</i><br>&nbsp;</p><p>People say they <i>need</i> to tolerify, because otherwise they wouldn't be able to handle the intolerable world.</p><p>But that's false. Acknowledging that the world is unacceptable will not kill you; the world is <i>already</i> as unacceptable as it is. Remember the <a href=\"http://lesswrong.com/lw/id/you_can_face_reality/\">litany of Gendlin</a>.</p><p>So face the dark world. See the intolerable.</p><p>Take up the burden that is supposed to be unbearable. Don't excuse the world, don't come up with reasons why it's OK. <i>Let it be not OK.</i></p><p>What happens then? What do you feel then?</p><p>Is there a sense of despair or helpessness? Is there a sense of hot fury or cold resolve? Is there a sense of being tiny in the face of a problem that is large?</p><p>Live <i>there</i>, in the face of the intolerable. Don't struggle to make it acceptable, just live with the bad world, while <a href=\"http://mindingourway.com/being-unable-to-despair/\">buckling down rather than buckling</a>.</p><p>It is there, while staring the dark world in the face, that I find a deep well of intrinsic drive. It is there that my resolve and determination come to <i>me</i>, rather than me having to go hunting for them.</p><p>I find it amusing that \"we need lies because we can't bear the truth\" is such a common refrain, given how much of my drive stems from my response to attempting to bear the truth.</p><p>I find that it's common for people to tell themselves that they need the lies in order to bear reality. In fact, I bet that many of you can think of one thing off the top of your heads that you're intentionally tolerifying, because the truth is too scary to even consider. (I've seen at least a dozen failed relationships dragged out for months and months due to this effect.)</p><p>I say, if you want the intrinsic drive, drop the illusion. Refuse to tolerify. Face the facts that you feared you would not be able to handle. You are likely correct that they will be hard to bear, and you are likely correct that attempting to bear them will change you. But that change doesn't need to break you. It can also make you stronger, and fuel your resolve.</p><p>So see the dark world. See everything intolerable. Let the urge to tolerify it build, but don't relent. Just live there in the intolerable world, refusing to tolerate it. See whether you feel that growing, burning desire to <i>make the world be different</i>. Let parts of yourself harden. Let your resolve grow. It is here, in the face of the intolerable, that you will be able to tap into intrinsic motivation.</p>", "user": {"username": "So8res"}}, {"_id": "kCTtwQBsdyaktxrXS", "title": "Being unable to despair", "postedAt": "2015-09-13T05:00:00.000Z", "htmlBody": "<p>Content note: these next few posts are not going to be for the faint of heart.</p><p>Sometimes, when people see that their life is about to get a lot harder, they start buckling down. Other times, they start despairing, or complaining, or preparing excuses so that they can have one ready when the inevitable failure hits, or giving up entirely and then <a href=\"http://mindingourway.com/failing-with-abandon/\">failing with abandon</a>. These next few posts assume that you have the former demeanor, and they might not be helpful to people who are inclined to respond to new difficulties with despair. Remember the law of <a href=\"http://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/\">equal and opposite advice</a>! (For every person who needs a certain piece of advice, there is someone else who needs the opposite advice.)</p><p>With that said, I'm going to spend a few words giving some tips about how to have the former demeanor, if you want to. The first piece of pertinent advice is that the way you respond to challenges is context dependent; even if you've already been known to respond to some problems by despairing, there are likely other problems that you respond to by buckling down.</p><p>There is a specific mindset that, in my experience, makes it much easier to adopt the \"buckle down\" demeanor. This is the mindset where \"not doing anything\" doesn't seem like an available option in the action-space. I've written a bit before about how I think <a href=\"http://mindingourway.com/rest-in-motion/\">many people think there is a default \"rest state\"</a>, and this is a related concept: many people seem to think that there is a privileged \"don't do anything\" action, that consists of something like curling up into a ball, staying in bed, and refusing to answer emails. It's much easier to adopt the \"buckle down\" demeanor when, instead, curling up in a ball and staying in bed feels like <i>just another action</i>. It's just another way to respond to the situation, which has some merits and some flaws.</p><p>So this is my second piece of advice, if you want to be the sort of person who buckles down in the face of hardship: see the world in terms of possible responses. See curling up in bed and ignoring the world as just <i>one possible response</i>, rather than an escape hatch. Dispel the illusion that some actions are labeled \"do nothing,\" and notice that those, too, are responses. There is no privileged null choice.</p><p>(That's not to say that it's <i>bad</i> to curl up in a ball on your bed and ignore the world for a while. Sometimes this is exactly what you need to recover. Sometimes it's what <a href=\"http://mindingourway.com/rest-in-motion/\">the monkey is going to do</a> regardless of what you decide. The point is that when nature offers you a choice, there is no \"don't choose\" option. There are only the options that nature offers, and all you can do is pick the best of them.)</p><p>My third piece of advice is to remember that you <a href=\"http://mindingourway.com/residing-in-the-mortal-realm/\">reside in the mortal realm</a>. If you get new information or a new way of looking at the world and you start to feel despair, or hopelessness, or helpless, or impotent, then it is <i>perfectly OK</i> to respond by curling up in a corner and feeling sad and scared and small for a little while. That's a fine response. It doesn't mean that you're not up to the task. Nor does it mean that you are condemned to despairing forever. You're allowed to feel small sometimes, and then get back up and keep going, without any need to pretend that things are fine. We're monkeys. Feeling helpless happens.</p><p>Rising to the challenge doesn't mean never feeling helpless. It means pushing on <i>anyway</i>, even if you feel helpless sometimes.</p><p>In my experience, tapping into internal drive often requires tapping into a deep desire to <i>make the world be different</i>, in a world that's very large and very hurting and very hard to change. When trying to do this, it can be easy to get overwhelmed by the odds stacked against you \u2014 regardless of their scale. (In fact, I have often found that the cards stacked against me personally \u2014 when I feel isolated, lonely, or friendless \u2014 induce as much despair as the cards stacked against anyone who tries to change the world at large.)</p><p>In the next few posts, I'm going to talk about tapping into that internal drive, and this will entail trying to see the situation for what it really is: which means owning up to everything stacked against you. If you aren't careful, this might cause you to buckle. But if you do it right, it can cause you to buckle down instead, and provide a source of drive.</p>", "user": {"username": "So8res"}}, {"_id": "H4Hwwgn2AHYnyvhur", "title": "Residing in the mortal realm", "postedAt": "2015-09-06T05:00:00.000Z", "htmlBody": "<p>The last sevenish posts describe the main tools I have for removing guilt-based motivation. The common thread running through them can be summed up as follows: <i>Reside in the mortal realm.</i></p><p>Many people <a href=\"http://mindingourway.com/self-compassion/\">hold themselves to a very different standard than they hold others</a>. They hold themselves accountable for failing to do the <a href=\"http://mindingourway.com/where-coulds-go/\">psychologically impossible</a>. They fret over past mistakes and treat themselves as failed gods, rather than <a href=\"http://mindingourway.com/not-yet-gods/\">ambitious monkeys</a>. This condemning-of-the-self can lead to great guilt, with all its negative effects.</p><p>My suggestion for dealing with guilt, roughly speaking, is to first <a href=\"http://mindingourway.com/shifting-guilt/\">focus your guilt</a>, by dispelling the guilt that comes from <a href=\"http://mindingourway.com/half-assing-it-with-everything-youve-got/\">not doing what other people think you should</a> or from <a href=\"http://mindingourway.com/not-because-you-should/\">from false obligations</a>, and shifting all your guilt into guilt about the fact that you have not yet made the future how you want it to be. Then, once your guilt is focused there, remember that you are a denizen of the mortal realm.</p><p>In the past, you have failed to act as you wished to act. You have failed to make the best available choices. But these facts have little bearing on what you do next. They have <i>some</i> bearing, insofar as your memories still hold lessons that can teach you about how to <a href=\"http://mindingourway.com/dont-steer-with-guilt/\">better steer yourself to steer the world</a>, but they <a href=\"http://mindingourway.com/there-are-no/\">do not say anything about the color of your soul</a>. They are simply the background knowledge against which you move forwards, from here, <a href=\"http://mindingourway.com/be-a-new-homunculus/\">looking only towards the future</a>.</p><p>You are a mortal, who often struggles to follow their own will, and your actions set the course of the entire future. Instead of berating yourself for your shortcomings, figure out how to do the best you can <i>given</i> the shortcomings \u2014 sometimes by spending time and effort to fix them (mere willpower seldom suffices), and sometimes by taking them as given and working around them.</p><p>Be a mere mortal, and do the best you can anyway. Learn everything your can from your mistakes, and then forgive yourself your sins, and look only to how much better you can make the future (knowing what you know now about how you perform in different situations).</p><p>Guilt has no place among mortals: we already <i>know</i> we're fallible. We don't need to suffer over that fact: our failings provide only information about what to do next, if we want to steer the future.</p><hr><p>Over the last few months, three different people have informed me that I broke their motivation systems. In short, one found themselves less able to care about what they were working on, another found themselves unable to force themselves to work, another found themselves unable to continue spurring themselves on with guilt.</p><p>In part, this is working as intended: in the long run, I think that guilt-based motivation can be harmful. However, my goal is not to simply remove existing motivation systems: my goal is to replace guilt with something else.</p><p>So the question is, without guilt, what can you use for drive? And this brings us to the penultimate arc of my \"replacing guilt\" series of posts.</p><p>I've already given partial answers to the question \"whence internal drive?\", when talking <a href=\"http://mindingourway.com/on-caring/\">on caring</a>, or about <a href=\"http://mindingourway.com/the-value-of-a-life/\">the value of a life</a>, or about <a href=\"http://mindingourway.com/caring-about-some/\">caring about something larger than yourself</a>. Those posts are intended to inspire you and remind you that there's something <i>worth</i> fighting for, and that you can fight for it even if you lack a burning passion. That's not the whole picture, though, and in the upcoming arc, I'll touch upon a different aspect of intrinsic motivation.</p><p>I think many people are motivated by an intrinsic (often subconscious) desire to be virtuous, or perhaps by a strong aversion to \"being bad.\" I think many other people are motivated primarily by whatever obligations currently sit on their plate. They don't need to ask themselves what they are doing or why; they simply continue fulfilling the obligations in front of them so that life continues proceed. They fulfill obligations at school, they fulfill obligations at their jobs, they find a spouse, they start a family, they fulfill obligations to their family. The obligations keep flowing in a steady stream, and there is never any need to soul-search in a grand quest for some sort of deep intrinsic drive (except, perhaps, during the occasional \"midlife crisis,\" which is a fine distraction that they're expected to eventually overcome).</p><p>Yet here I stand, suggesting that you ditch the notion \"being bad\" and drop your obligations entirely, keeping only what remains. But dropping an existing framework is a far cry from creating a new one, and dropping guilt does not often reveal a blindingly virtuous non-obligation that you're supposed to pursue instead of what you were currently pursuing.</p><p>In fact, the new framework can't contain \"supposed tos\" at all. Obligations have been jettisoned.</p><p>So in the upcoming arc, I'm <i>not</i> going to give you something to pursue. Rather, I'm going to do my best to give you a different way of looking at the world. I'm going to describe a vantage point from which guilt motivation seems quaint, and something else \u2014 maybe cold resolve, maybe hot desire, maybe a different drive \u2014 guides your actions instead.</p><p>From that vantage point, guilt is alien \u2014 and it is only once it seems foreign (rather than evil) that it be fully replaced.</p>", "user": {"username": "So8res"}}, {"_id": "drQpgZ2gMjn376TE8", "title": "There are no \"bad people\"", "postedAt": "2015-08-30T05:00:00.000Z", "htmlBody": "<p>When I help friends debug their intrinsic motivation, here's a pattern I often bump into:</p><blockquote><p><i>Well, if I don't actually start working soon, then I'll be a bad person.</i></p></blockquote><p>Or, even more worrying:</p><blockquote><p><i>Well they wanted me to just buckle down and do the work, and I really didn't want to do it then, which means that either they were bad, or I was bad. And I didn't want to be the bad one bad, so I got angry at them, and\u2026</i></p></blockquote><p>I confess, I do not know what it would mean for somebody to be a \"bad person.\" I do know what it means for somebody to be bad at achieving the goals they set for themselves. I do know what it means for someone to be good at pursuing goals that I dislike. I have no idea what it would mean for a person to \"be bad.\"</p><p>I know what it means for a person to lack skill in a specific area. I know what it means for a person to be procrastinating. I know what it means for a person to be acting under impulses that they don't endorse, such as spite or disgust. I know what it means for someone to fail to act as they wish to act. I know what it means for someone to hurt other people, either on purpose or with a feeling of helpless resignation.</p><p>But I don't know what it would mean for a person to \"be bad.\" That fails to parse. People don't have a hidden stone deep inside their brain that is either green or red depending on whether they are good or bad. \"Badness\" is not a fundamental property that a person can have. At best, \"they're bad\" can be shorthand for either \"I don't want their goals achieved\" or \"they are untrained in a number of skills which would be relevant to the present situation\"; but in all cases, \"they are bad\" must be either shorthand or nonsense.</p><p>Asking whether a person is \"fundamentally good\" or \"fundamentally bad\" is a type error. Life is not a quest where you struggle to wind up \"good.\" That's not the sort of reality we find ourselves in.</p><p>Rather, we find ourselves embedded in a vast universe, with control over the future and a goal of making it wonderful. We find ourselves to be part of a grand deterministic pattern, and we're trying to make that pattern as beautiful as possible.</p><p>Step back and imagine history as a fixed path through the great crystal that is our universe over all time; the time-crystal that describes everything everywhere and everywhen; the time-crystal where you can look not only forwards and backwards, but beforewards and afterwards. Imagine the path of history that dances through configurations to the tune of physics. That same physics, according to which the line jigs and jags, is what implements you. In those jigs and jags is the pattern that is your mind. Some of the jigs compute your thoughts, some of the jags compute your choices, and your choices determine how the line dances in the afterwards direction past the event of your choice.</p><p>We aren't here to alter the color of the fundamental \"goodness\" stone buried within us; we're here to make the path through time be a good one.</p><p>Life is not a game of \"wind up good at the end\"; life is about steering the future.</p><p>Look not to whether you are good or bad. Look to where you are, and what you can do from there.</p><hr><p>Living this mindset does not mean that you lack regrets. It does not free you from the burdens of your wrongdoings. I, like anyone, suffer from recalling harms that I have done to others. But instead of treating those recollections as dark judgements on my soul, I treat them as <a href=\"http://mindingourway.com/staring-into-regrets/\">messages from my past</a>, information about what sorts of undesirable behavior the Nate-monkey is liable to execute if I am not careful.</p><p>I sometimes find myself unable to act as I wish; unresponsive to my own cajoling. I treat these not as evidence of my fundamental brokenness, but as evidence about <a href=\"http://mindingourway.com/where-coulds-go/\">how and when I can intervene on the world</a>.</p><p>While I often fail, I do not act under fear of being judged inadequate by the universe. I may <i>be</i> inadequate to the tasks I undertake, I may fail to steer the future as I wish to, but I cannot be \"fundamentally bad.\" That sentence does not parse.</p><p>There is something freeing about this: I may succeed; I may fail; but I will not be judged by someone who roots through my mind to see whether the stone is green or red.</p><p>I will be judged only by the path that the future takes; as will we all.</p><hr><p>By contrast, when I help friends debug their motivation, I often find them motivated by a desperate attempt to avoid \"being bad.\"</p><p>Where I can, I encourage them not to let that be at the core of what motivates them. It's well and good, when introspecting about why what you're doing is important, to get an answer from yourself that is of the form \"otherwise I'll be bad.\" That's a fine answer to get. But <i>don't let that be the end of things.</i> Don't pretend that that's the final answer. <i>Investigate.</i></p><p>Ask yourself, \"what do I mean by that?\" Say to yourself, \"I bet that's shorthand for something.\" <i>Unpack</i> the feeling of would-be-bad.</p><p>If someone wants you to do the laundry, and you don't want to do the laundry, and you get angry at them because you have a sense that if there is conflict then one of you must be bad and <i>you</i> don't want to be bad\u2014</p><p>\u2014then pause, and investigate further.</p><p>Focus, and ask yourself what bad thing would happen if you did do the laundry, and what bad thing would happen if you didn't.</p><p>Maybe you get an answer like \"if I don't do the laundry then it will strain my relationship with my friend, but if I do do the laundry then it will spend scarce energy and attention and I'm feeling really exhausted and don't want to force myself to do it.\"</p><p>That's great! (The answer doesn't need to be <i>comfortable</i>, it just needs to be <i>unpacked</i>. You may well reveal conflicting desires. You may well find that you were ignoring goals that you had but didn't endorse, such as preserving your own attention or energy.) This is a similar mental action to <a href=\"http://mindingourway.com/should-considered-harmful/\">unpacking a should</a>: if you find yourself compelled to do something because otherwise you'd \"be bad,\" then become curious, investigate, and unpack the feeling into it's component parts.</p><p>Ask yourself, \"I don't know what it would mean to be bad; can you elaborate?\"</p><p>Then, <i>listen to yourself.</i> Don't worry if your answers seem senseless! Often, I have watched people completely fail to figure out what is blocking them, because as soon as they get an answer from deep inside their mind, they declare that it's ridiculous, and then they struggle to dismiss it or cover it up or decry it as \"irrational.\"</p><p>Perhaps they ask themselves what they mean by \"then I'd be bad\" and find something like \"I apparently think that if I don't do the laundry then it's evidence that I can't do <i>anything</i>, and that means I'll lose my job and end up on the street and die cold and alone, and that's <i>stupid,</i> so\u2026\" at which point they start lecturing themselves about why their concerns are dumb, instead of <a href=\"http://mindingourway.com/productivity-through-self-loyalty/\">declaring self-loyalty</a> and standing by themselves. (If you find yourself doing this, I suggest taking your concerns seriously, and explaining your different beliefs earnestly, with the same respect you'd show an inquisitive child who wants to understand the world but has a few flaws in their understanding.)</p><p><a href=\"http://mindingourway.com/not-yet-gods/\">You're still a monkey</a>! You often have inconsistent, strange preferences. Parts of you often have beliefs that other parts of you don't endorse. That's <i>ok.</i> Decrying your own inconsistencies is no way to fix them: work with yourself.</p><hr><p>So don't settle for being motivated to do something because otherwise you'd \"be bad.\" Unpack the feeling of \"being bad,\" and figure out what outcomes you're aiming for. Figure out what you want to <i>do.</i> Figure out how you want the future to <i>be.</i></p><p>Because at the end of the day, a person \"being bad\" fails to parse. \"Goodness\" and \"badness\" are not properties of people. People can do terrible things; they can pursue horrible goals; they can watch with growing despair as they act against their own best interests; but they do not have a fundamental stone buried deep inside of them which measures their worth.</p><p>Life is not a game of \"wind up good at the end.\" Life is about steering the future.</p>", "user": {"username": "So8res"}}, {"_id": "vPceuASWY4Ea97vAg", "title": "Self compassion", "postedAt": "2015-08-25T05:00:00.000Z", "htmlBody": "<p>Imagine a time when you were feeling guilt-wracked. Maybe a time you <a href=\"http://mindingourway.com/steering-towards-forbidden-conversations/\">hurt a friend badly</a>. Maybe a time you tried to do get some important work done, and found you couldn't, and this kicked off a failure spiral leading to a deep depression. Maybe some other time: the important thing is to load into memory a time you felt guilt-wracked, and recall how you felt towards yourself in that case.</p><p>(When I do this, I get an internal sense of resistance, of not-wanting-to-look, of willing-the-past-to-be-different.)</p><p>Now imagine you have a child, who grows to the same age that you were then, who finds themselves in exactly the same situation. Maybe they, too, hurt somebody badly -- they didn't consciously realize how badly they were about to hurt a friend until one moment too late, and now they feel terrible. Or maybe they, too, tried to do something important, and found it hard, and started doubting themselves, and spiraled downwards into a depression that they now have trouble climbing out of.</p><p>Imagine what you might feel towards your child, in this scenario.</p><p>(When I do this, I get a sense of compassion, of protectiveness, and a desire to reassure them that this is what it looks like to learn hard lessons, for <a href=\"http://mindingourway.com/not-yet-gods/\">us monkeys</a>.)</p><p>I encourage you to simulate the feelings you would feel towards your child in this situation \u2014</p><p>\u2014 and then check whether you can <i>also</i> feel that way towards <i>yourself</i>.</p><p>When you think of your own failings, can you feel that compassion and protectiveness and impulse to reassure towards <i>you?</i></p><p>Many can't. Some don't feel compassion towards others in the first place (this post is not for them \u2014 if you want help feeling compassion towards your fellow humans, then maybe try <a href=\"http://mindingourway.com/caring-about-some/\">this post</a> and see if it works for you.) Others can as easily feel compassion for themselves as others. But many people I've spoken to experience a wide gulf between compassion for others and self-compassion \u2014 which is a shame, because self-compassion is an important part of self-loyalty and the mental toolset I'm trying to convey with these posts.</p><hr><p>To close the gap between compassion and self-compassion, I offer two tools. The first is a reminder that self-compassion is not the same thing as self-pity, and nor is it the same thing as making excuses for yourself. It is well possible to feel self-compassion even while thinking that you are not moving fast enough. It is perfectly possible to feel self-compassion even as you notice that you're completely failing to act as you wish to.</p><p>For example, imagine someone going through boot camp in World War II, filled with resolve and determination to become a soldier and defend the free world \u2014 except they are a small person, and a weak one. Imagine them working their heart out, trying as hard as they can, and failing anyway. Imagine them failing to make the cut. Now, can you imagine feeling compassion for them, feeling warmth towards them, and maybe feeling a hint of sadness for their loss, without feeling any sense of pity? Compassion for yourself can be similar, without any hint of pity.</p><p>Or imagine another person going through the same boot camp, who really wants to go defend the free world with all their peers (on some level), but who lacks the deep drive. They <i>want</i> to feel the same passion and fire as their diminutive counterpart, but instead they feel resistance and suffer from depression \u2014 and every day they drag themselves out of bed (slightly too late), and every day they force themselves through the obstacle courses (but not quickly enough), and they aren't going to make the cut, and they're sick with guilt about it. Can you imagine feeling compassion for them in their plight, while making absolutely no excuses for their performance? Again, self-compassion can be the same way. You don't need to make excuses for yourself, to take the outside view and feel the same warmth for a monkey that's <i>trying</i> to try, against the gradient of depression and doubt.</p><p>Now imagine someone else doing what you're trying to do. Imagine them working on hard problems, and putting in what effort they can muster \u2014 sometimes it is enough, sometimes it isn't; sometimes they are highly motivated, other times they are blocked by their own mind and unable to act as they wish. Look at them and see the fragile monkey trying to build a satisfactory life, trying to improve their world. See if you can feel compassion for them. You don't need to pity them, you don't need to make excuses for their failures, you don't need to find ways they could improve: simply see if you can feel some warmth, for a fellow lost monkey \u2014 and then shift your gaze to yourself, and see if you can feel a similar sort of warmth.</p><hr><p>The second tool I offer, to close the gap between compassion for others and compassion for yourself, is this: I recommend that you pinch yourself, and remember what you are. Practice <a href=\"http://lesswrong.com/lw/k7/original_seeing/\">original seeing</a> while looking upon yourself and your situation. What do you see?</p><p>I see bundles of proteins and lipids arranged in a giant colony of cells, their lives given over to the implementation of a wet protein computer that thinks it's a person.</p><p>I see fractal patterns that arise on precisely the right sort of planet when you pour sunlight into it for a billion years.</p><p>I see wiggles in the Sun's wake that struggle to understand the universe. Incomprehensibly large constructs made of atoms, which are unnoticeably small on the scale of galaxies.</p><p>Look at us, the first species among the animals that can figure out what the stars are, yet still tightly bound to impulse and social pressure. (Notice how silly it is, monkeys acting all serious and wise as they try to affect the course of history.)</p><p>Look at us: half monkey, half god; towering below the stars.</p><p>Look at whatever quest you've taken on, you who was forged by the death of your father's brothers and now claims dominion over the future. Acknowledge that what you're trying to do is difficult. Turn the <a href=\"http://mindingourway.com/caring-about-some/\">monkey sight</a> on yourself, and see the lost monkey who's trying to steer an entire universe\u2026</p><p>and say hello. Check in with the monkey. See how it's doing.</p><p>Steering the future is a difficult thing. The world is large beyond comprehension, and the monkey wasn't really built for this. The monkey isn't really used to this sort of thing, and it can be pretty hard to work with sometimes.</p><p>Let the monkey know that you have its back. Let it know that you'll still have its back, even if it gets ornery or difficult or depressed. Through thick and thin, let you know that you have your support; that even when you screw everything, you'll stand by yourself, and help you through the mess, and help you figure out how to do better in the future.</p><p>See if you can resolve to work with yourself. You can do powerful things, if you work together.</p>", "user": {"username": "So8res"}}, {"_id": "a8QWw4ejKRNhfyN9n", "title": "Jacqueline Fuller: The future of philanthropy", "postedAt": "2015-08-28T07:00:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=x6Hlepk9GD0\"><div><iframe src=\"https://www.youtube.com/embed/x6Hlepk9GD0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Jacqueline Fuller introduces the work of Google.org and looks at how companies can practice effective altruism through corporate&nbsp;giving.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "tFtMxhZMhcKLLBpiN", "title": "Tyler Alterman: Succeeding at EA Global", "postedAt": "2015-08-28T07:00:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ExSrmDkaqs0\"><div><iframe src=\"https://www.youtube.com/embed/ExSrmDkaqs0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Tyler Alterman advises EA Global attendees to make plans, update their plans, and punch their plans in the&nbsp;mouth.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "BQHWcGxH8EHqTfhuf", "title": "Will MacAskill: State of the effective altruism union", "postedAt": "2015-08-28T15:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=_rw4eaPTqhs\"><div><iframe src=\"https://www.youtube.com/embed/_rw4eaPTqhs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Will MacAskill discusses strategies for overcoming obstacles within effective altruism to the realization of an awesome future, where the majority of world leaders are dedicating their lives to maximizing happiness and minimizing&nbsp;suffering.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "gHEhn3MSjj2xCYCZ2", "title": "Jonathan Courtney, Ben Kuhn, Ajeya Cotra, Chris Jenkins, & Aaron Tucker: Running a chapter", "postedAt": "2015-08-28T15:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=V4cVVQ_IvZA\"><div><iframe src=\"https://www.youtube.com/embed/V4cVVQ_IvZA\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 panel, Jonathan Courtney, Ben Kuhn, Ajeya Cotra, Chris Jenkins, and Aaron Tucker discuss their experiences with running effective altruism groups, ideas for programs and activities, and advice for people interested in running groups of their&nbsp;own.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "5DH3miAWdh7JRR9Ep", "title": "Ben Hoffman: Rationality II", "postedAt": "2015-08-28T15:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=cHs1Xx0_OOc\"><div><iframe src=\"https://www.youtube.com/embed/cHs1Xx0_OOc\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Why is it scary to try things? In this EA Global: San Francisco 2015 talk, Ben Hoffman looks at methods for continuous improvement that enable us to overcome the twin dangers of getting overly attached to our (possibly wrong) beliefs or being trapped by&nbsp;indecision.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "AcguaSkwvwDCcc5Ae", "title": "Matt Fallshaw: Rationality I", "postedAt": "2015-08-28T15:25:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=K7x3u0VeADA\"><div><iframe src=\"https://www.youtube.com/embed/K7x3u0VeADA\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Matt Fallshaw presents paths and next actions on how to use experts, community, feedback, review, charity, and realism to develop your awesome future&nbsp;self.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "26P8XxHMqoXAGKwLG", "title": "Nick Beckstead: Prioritization research", "postedAt": "2015-08-28T15:25:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=b8-FPQocffs\"><div><iframe src=\"https://www.youtube.com/embed/b8-FPQocffs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Using nuclear weapons policy as a case study, Nick Beckstead demonstrates the process of 'shallow' cause&nbsp;evaluation.</i></p><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2015 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "8YqTiDB2X2u8aMvBa", "title": "Matthew Gentzel: Policy change III", "postedAt": "2015-08-28T15:25:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=T7IpwxtCwq8\"><div><iframe src=\"https://www.youtube.com/embed/T7IpwxtCwq8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Matthew Gentzel highlights opportunities within politics to create significant impact across effective altruism cause areas, and discusses the efforts of Effective Altruism Policy Analytics to overcome the challenges of creating political&nbsp;change.</i></p><p><i>In the future, we plan to post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "KMtCKz8cSyduzMHKL", "title": "Rajesh Mirchandani: Policy change II", "postedAt": "2015-08-28T15:25:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=4KhHf4BbOeM\"><div><iframe src=\"https://www.youtube.com/embed/4KhHf4BbOeM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Rajesh Mirchandani describes his journey from working in journalism to policy change, and discusses ways in which policy change at the global scale can create and improve opportunities to help the global&nbsp;poor.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "kJBxth9SSJei7H7kr", "title": "Holden Karnofsky: Open Philanthropy Project", "postedAt": "2015-08-28T08:37:38.031Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=AOvUQnCMwww\"><div><iframe src=\"https://www.youtube.com/embed/AOvUQnCMwww\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Holden Karnofsky talks about the Open Philanthropy&nbsp;Project.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "hupxnnrCXRNdGN9SM", "title": "Will MacAskill: Opening talk (2015)", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=3XoSqI1dZag\"><div><iframe src=\"https://www.youtube.com/embed/3XoSqI1dZag\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Will MacAskill opens EA Global: San Francisco 2015 with an introduction to effective altruism, why it matters, and the virtues and the history of&nbsp;EA.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "nbidXshxvdSAFnF8Y", "title": "Joshua Greene: Nature of altruism II", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=YmQicNmhZmg\"><div><iframe src=\"https://www.youtube.com/embed/YmQicNmhZmg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>What problem does morality solve? How does morality go wrong? How can we do better? In this EA Global: San Francisco 2015 talk, Joshua Greene presents discusses the neurological underpinnings of morality and offers a functional definition of its&nbsp;mechanisms.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "8mx9qxsDS7urkzPTG", "title": "Robin Hanson: Nature of altruism I", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=nIRvNxykvTQ\"><div><iframe src=\"https://www.youtube.com/embed/nIRvNxykvTQ\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Robin Hanson challenges members of the effective altruism community to consider whether altruism is actually anything more than social signalling, and to notice our social biases around our altruistic&nbsp;endeavors.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "tx9uBbibjodR55AYC", "title": "Kerry Vaughan: Movement development II", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ZCOOtc-sofM\"><div><iframe src=\"https://www.youtube.com/embed/ZCOOtc-sofM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>What could the effective altruism movement become? What are the 'existential risks' to the EA movement? What can be done to avoid these risks and ensure that EA is successful? In this EA Global: San Francisco 2015 talk, Kerry Vaughan addresses these questions and other strategic considerations for the EA&nbsp;movement.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "QpZQbPo9pYuo2ExP7", "title": "Rob Wiblin: Movement development I", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=TH4_ikhAGz0\"><div><iframe src=\"https://www.youtube.com/embed/TH4_ikhAGz0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Rob Wilblin steelmans the case against movement growth, considering the risks associated with growth and the cost of onboarding new&nbsp;members of the effective altruism movement.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "hf7eoP8oFKcfYffFA", "title": "Michael Faye: Global poverty III", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=RSAEdTV6H6E\"><div><iframe src=\"https://www.youtube.com/embed/RSAEdTV6H6E\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>How can we not just have the most impact today but also optimize for tomorrow's scale? How can we find or create effective opportunities? In this EA Global: San Francisco 2015 talk, Michael Faye explores the intervention ecosystem and argues for the importance of direct giving, both as a benchmark for effectiveness and an incredibly scalable opportunity for&nbsp;impact.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "92zpdgNK98tAtdAjp", "title": "Eva Vivalt: Global poverty II", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=MY_oC-Dxb4k\"><div><iframe src=\"https://www.youtube.com/embed/MY_oC-Dxb4k\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Eva Vivalt dicusses AidGrade's work in living meta-analyses of impact evaluations and outlines the challenges inherent in figuring out whether an intervention is truly&nbsp;effective.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "v9QsNHAmv7vju6D9e", "title": "Jeff Kaufman: Global poverty I", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ad7qb_EE66Y\"><div><iframe src=\"https://www.youtube.com/embed/ad7qb_EE66Y\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Making the case for global poverty, Jeff Kaufman emphasizes the marginal value of your dollar in the developing world and the important role of feedback loops in effective&nbsp;action.</i></p><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2015 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "GeXvY7JyGkZHqWdym", "title": "Liv Boeree: Effective altruism entrepreneurship III", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=UQNYiKOGmo8\"><div><iframe src=\"https://www.youtube.com/embed/UQNYiKOGmo8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Liv Boeree introduces her work with Raising for Effective Giving and discusses the challenges of introducing EA ideas to a very different&nbsp;community.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "AhFJkTn77GwhqDuRH", "title": "Spencer Greenberg: Effective altruism entrepreneurship I", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=LiVyRtS_d9o\"><div><iframe src=\"https://www.youtube.com/embed/LiVyRtS_d9o\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Is it possible to innovate on purpose? Arguing against the standard 'genius' and 'luck' models of innovation, Spencer Greenberg presents four frameworks for deliberately seeking new&nbsp;ideas.</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/AhFJkTn77GwhqDuRH/spencer-greenberg-effective-altruism-entrepreneurship-i?commentId=H5nmMeDZSWF6uyJfQ\">Notes by Nils</a>:&nbsp;</p><blockquote><p><strong>How to innovate on purpose</strong></p><ul><li>It\u2019s not useful to think it requires being a genius (Einstein)</li><li>It\u2019s not useful to think it\u2019s about luck (wonderful idea from within the bathtub)</li></ul><p><strong>Innovative ideas need to be ...</strong></p><p><strong>a) ... impossible until now</strong></p><p>I.e. looking at existing technology and thinking about using it in new ways; e.g. Machine Learning applied to cars \u2192 Google self-driving cars</p><p>To-do:</p><ul><li>Learn promising technology very well</li><li>Ask the question: \"What industries has this technology not been applied to yet?\"</li></ul><p><strong>b) ... undiscoverable to others</strong></p><p>I.e. find out what multiple topics / things you know, and thereby realising your potentially unique overlaps; e.g. programming language design + cloud computing \u2192 developing programming language specifically designed for cloud computing</p><p>To-do:</p><ul><li>Pick two unrelated topics you know a lot about</li><li>Ask the question: \u201cWhat does combining them make possible?\u201d</li></ul><p><strong>c) ... stupid sounding</strong></p><p>I.e. find the overlap between ideas that seem like a bad and a good idea at the same time, because you know some secret that makes the stupid idea seem like a good one; e.g., \u201clet\u2019s have anonymous random people argue to decide the facts on every topic\u201d and \u201clet\u2019s have everyone trust this information\u201d \u2192 Wikipedia</p><p>To-do:</p><ul><li>Ask the question: \u201cWhat's an important truth you know that few do?\u201d</li><li>Ask the question: \u201cWhat seems dumb unless you know this truth?</li></ul><p><strong>d) ... tiny at first</strong></p><p>I.e. find the overlap between things people care about and weird people like you care about; e.g. \u201cpeople can\u2019t easily find Bay Area events, I will send an email list to my friends\u201d and \u201cI will let other people post their events too\u201d \u2192 Craigslist</p><p>To-do:</p><ul><li>Tackle a problem had by a small group you understand</li><li>Slowly expand the features for a larger and larger group</li></ul><p><strong>Entrepreneurial success depends on:</strong></p><ul><li>Quality of idea</li><li>Time commitment</li><li>Ability to build the product</li><li>Propensity to learn from feedback</li><li>Skill at securing funds</li><li>Relentless pursuit of success</li><li>Sales and marketing skills</li><li>Leadership</li><li>Random chance</li></ul></blockquote><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2015 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "9Xt6AyxwM9iHiwxBa", "title": "Kate Donovan: Effective altruism and community II", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ng-9C6d_1LA\"><div><iframe src=\"https://www.youtube.com/embed/ng-9C6d_1LA\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2105 talk, Kate Donovan discusses the importance of diversity in the EA movement, the challenges to creating an inclusive and welcoming community, and possible solutions for us to&nbsp;consider.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "gGnkxjAriic6A2DY5", "title": "Eliezer Yudkowsky, Rob Reich, and Tim Urban: Communicating complex concepts", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=U2APNeyV6FM\"><div><iframe src=\"https://www.youtube.com/embed/U2APNeyV6FM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>From misunderstandings in effective altruism to strategies for bridging inferential gaps, Eliezer Yudkowsky, Rob Reich, and Tim Urban discuss their experiences and insights in effective&nbsp;communication.</i></p><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2015 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "rKo42Dixsny2tAnTL", "title": "Uma Valeti: Biology and technology II", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=SAPo_Jllaoc\"><div><iframe src=\"https://www.youtube.com/embed/SAPo_Jllaoc\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Uma Valeti introduces New Harvest's work to fund research aimed at making animal products obsolete and makes the case for the importance of biotechnology in reducing animal&nbsp;suffering.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "NB9TXuXDPHwh9ib8h", "title": "Ryan Phelan: Biology and technology I", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=hcfhHMSYGJI\"><div><iframe src=\"https://www.youtube.com/embed/hcfhHMSYGJI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Ryan Phelan discusses her work with the Long Now Foundation on the Revive and Restore project to enhance and protect biodiversity through genetic rescue. One possibility she considers is reversing the extinction of megafauna, such as the wooly mammoth, to mitigate the negative effects of global climate&nbsp;change.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "X56D5AQFkHo3tH2xr", "title": "Nick Bostrom: Astronomical stakes", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=fmQkLKLmfQU\"><div><iframe src=\"https://www.youtube.com/embed/fmQkLKLmfQU\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Nick Bostrom talks about the opportunity costs of delayed technological&nbsp;development.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "TmeCu2osPCdZjhFWy", "title": "Nick Cooney: Animal advocacy III", "postedAt": "2015-08-28T09:16:11.679Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=jlw1W6vxkMw\"><div><iframe src=\"https://www.youtube.com/embed/jlw1W6vxkMw\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Nick Cooney surveys the cost-per-animal impact of current efforts to end animal&nbsp;suffering.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "c9h3TrXv6fJhaZuEB", "title": "Jacy Reese: Animal advocacy I", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=52NrhuXtFEo\"><div><iframe src=\"https://www.youtube.com/embed/52NrhuXtFEo\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2015 talk, Jacy Reese presents the argument for animal advocacy as a hugely promising cause area and summarizes strategies for effective action in this&nbsp;area.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "J5Qucc498oEjWFJnb", "title": "Niel Bowerman: Why and how we should prioritise", "postedAt": "2015-08-28T08:37:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=VXhMuO6e6qE\"><div><iframe src=\"https://www.youtube.com/embed/VXhMuO6e6qE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Niel Bowerman talks about why we should prioritise altruistic causes and how we should do it. The talk includes a discussion on comparing effectiveness, factoring cost effectiveness, choosing a cause area, choosing an intervention within a cause area, choosing organisations which focus on desired interventions, and&nbsp;more.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "MKaaGJiehjc4Rm6Gq", "title": "Justin Oakley: Virtue ethics and effective altruism", "postedAt": "2015-08-28T09:21:15.552Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=96HHUDIJg7I\"><div><iframe src=\"https://www.youtube.com/embed/96HHUDIJg7I\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Justin Oakley explains how virtue ethics can support effective altruism and how effective altruism is supported by the Aristotelian virtue of&nbsp;justice.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "4kGwXDYqxTcDrS5Yy", "title": "Chris Rodley: The most good you can do online", "postedAt": "2015-08-28T09:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=qlx76akHcZs\"><div><iframe src=\"https://www.youtube.com/embed/qlx76akHcZs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>For many of us, our days are increasingly spent on the Internet. So what kind of good can we do there? This talk from EA Global: Melbourne 2015 examines the various activities that altruists are engaging in online, from tweeting and blogging to creative projects such as viral&nbsp;videos.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "BodqniGTNzXebeznY", "title": "Peter Singer: The most good you should do", "postedAt": "2015-08-28T09:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=xSXRYSBF91k\"><div><iframe src=\"https://www.youtube.com/embed/xSXRYSBF91k\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Peter Singer discusses how we can be more effective in our philanthropy and the various approaches to making a larger impact on the&nbsp;world.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "6P75x8s8KWZwCs4eZ", "title": "Kerry Vaughan: The future of effective altruism", "postedAt": "2015-08-28T09:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=JKx0ithQdHQ&amp;t=1s\"><div><iframe src=\"https://www.youtube.com/embed/JKx0ithQdHQ\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Kerry Vaughan discusses strategies to grow the effective altruism movement, including ways to avoid failure modes.&nbsp;</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "voL8J6wDemB9zjvxE", "title": "Jess Whittlestone: Rationality and effective altruism", "postedAt": "2015-08-28T17:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=kqK_w6DQ6v8\"><div><iframe src=\"https://www.youtube.com/embed/kqK_w6DQ6v8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Jess Whittlestone discusses rationality and why we should care about it. Two major aspects of rationality are covered: \"epistemic rationality\" (forming accurate beliefs) and \"instrumental rationality\" (the skill of advancing your goals given your resources). Jess then discusses what rationality isn't, and some common misconceptions. Further, Jess discusses three common components of rationality central to EA: 1) seeking truth (including intellectual honesty and open-mindedness), 2) questioning our intuitions (including thinking fast and slow), and 3) being \"effective\" (i.e., strategic instead of just responding to environmental pressures).</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "Rr4QWAd9KjwQn5Fo5", "title": "Hilary Greaves, Peter Singer, David Pearce, & Justin Oakley: Philosophy and effective altruism", "postedAt": "2015-08-28T17:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=UekTXpkRMOA\"><div><iframe src=\"https://www.youtube.com/embed/UekTXpkRMOA\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Panelists Hilary Greaves, Peter Singer, Justin Oakley, and David Pearce discuss what they believe are important philosophical aspects of the effective altruism movement \u2014 from practical philosophy we can use today to possible endpoints implied by various frameworks in applied ethics. The panelists navigate through a wide range of fascinating and important topics that aspiring effective altruists and anyone whom is philosophically inclined will find both useful and enjoyable.</i></p><p><i>In the future, we may post a transcript for this EA Global: Melbourne 2015 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "hRqhYvbmww8FMKwR5", "title": "Peter Singer: Non-human animal ethics", "postedAt": "2015-08-28T17:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=TgRoZVT6kYc\"><div><iframe src=\"https://www.youtube.com/embed/TgRoZVT6kYc\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melboune 2015 talk, Peter Singer discusses moral value of non-human animals \u2014 the history of moral progress around equality of human animals and how we ought to treat animals \u2014 from Judaism and Christianity, to Aristotle, to Bentham (father of modern utilitarianism). Singer highlights Benthan's view that the capacity for suffering/joy is the vital characteristic that entitles a being to moral consideration. He discusses why we should take non-human animal suffering seriously and what we can do to alleviate the suffering of non-human animals.&nbsp;</i></p><p><i>Slides are here: </i><a href=\"https://www.slideshare.net/adam_ford/...\"><i>https://www.slideshare.net/adam_ford/...</i></a></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "A8uqsMczBbFT4bDzC", "title": "Grace Hollister: Let's deworm the world", "postedAt": "2015-08-28T17:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=MsjUKbOGn8U\"><div><iframe src=\"https://www.youtube.com/embed/MsjUKbOGn8U\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Intestinal worms are debilitating, widespread, and under-treated. School-based deworming is safe, cost-effective and scale-able. What is the evidence base for the work that the Deworm the World Initiative undertakes?&nbsp;</i></p><p><i>Grace Hollister is responsible for strategic management and technical oversight of Evidence Action\u2019s Deworm the World Initiative. She leads fundraising and advocacy for Deworm the World, and works closely with country teams on delivery of cost-effective, evidence-based technical assistance. Grace also works with a variety of partners to expand opportunities for school-based deworming programs globally. Previously, Grace worked at the Millennium Challenge Corporation where she was involved in portfolio analytics, database design, and program operations due diligence and implementation in Ghana and Tanzania. Grace received her BA from Tufts University. She lives close to the beach and hopes to become an expert paddleboarder in the near future.</i></p><p><i>In the future, we may post a transcript for this EA Global: Melbourne 2015 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "K9eNviaPhqyYwW7Bz", "title": "Sam Deere: Giving what we can to eradicate global poverty", "postedAt": "2015-08-28T17:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=7PjQazjJCjI\"><div><iframe src=\"https://www.youtube.com/embed/7PjQazjJCjI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Sam Deere discusses effective giving to help eradicate global poverty; cause prioritization; and how far your donation can go by giving to certain charities that Giving What We Can supports.&nbsp;</i></p><p><i>Sam has worked as a political adviser and communications manager, including for former Federal Finance Minister Penny Wong, and was part of the team that won the \u2018unwinnable\u2019 2014 South Australian state election. He is currently Director of Communications at Giving What We Can.</i></p><p><i>In the future, we plan to post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "7JT7PDSGuQnooSMWB", "title": "Sam Deere: Effective policy change", "postedAt": "2015-08-28T17:21:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=gzaGJAjAL4Q\"><div><iframe src=\"https://www.youtube.com/embed/gzaGJAjAL4Q\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Sam discusses effective strategies to effect change in policy. This talk was aimed at the effective altruism community, though the basic principles apply widely. Sam has worked as a political adviser and communications manager, including for former Federal Finance Minister Penny Wong, and was part of the team that won the \u2018unwinnable\u2019 2014 South Australian state election. He is currently Director of Communications at Giving What We Can.&nbsp;</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "WD4bK3Rn2xxSDM28x", "title": "Leon Di Stefano: Christianity and effective altruism", "postedAt": "2015-08-28T16:14:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=dbAIE_R1MNE\"><div><iframe src=\"https://www.youtube.com/embed/dbAIE_R1MNE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Leon Di Stefano discusses similarities and tensions between Christianity and aspects of the effective altruism movement, and contrasts utilitarianism with virtue ethics.&nbsp;</i></p><p><i>Slides are here: </i><a href=\"https://www.slideshare.net/adam_ford/effective-altruism-christianity-by-leon-di-stefano\"><i>https://www.slideshare.net/adam_ford/effective-altruism-christianity-by-leon-di-stefano</i></a><i>&nbsp;</i></p><p><i>After this talk was a panel on the subject: </i><a href=\"https://www.youtube.com/watch?v=7iwiV...\"><i>https://www.youtube.com/watch?v=7iwiV...</i></a></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "sBhTuhJysTkiiPhqN", "title": "Kerry Vaughan: Be greedy for the most good you can do", "postedAt": "2015-08-28T16:14:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=-mTNASalGUg\"><div><iframe src=\"https://www.youtube.com/embed/-mTNASalGUg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Kerry Vaughan discusses the questions \"What is effective&nbsp;altruism?\", \"What is its history? \", and \"What isn't EA? \" He also covers ideas for succeeding at being an effective altruist.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "QpGrcdWBTQgr9LQew", "title": "David Pearce: Abolitionist bioethics", "postedAt": "2015-08-28T16:14:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=MyS7CRJNHlE\"><div><iframe src=\"https://www.youtube.com/embed/MyS7CRJNHlE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>David Pearce is the author of The Hedonistic Imperative (1995), which calls for the use of biotechnology to phase out suffering throughout the living&nbsp;world. In 1998, he co-founded with Nick Bostrom the World Transhumanist Association (H+). Transhumanists believe in using technology to overcome our biological limitations.</i></p><p><i>In the future, we may post a transcript for this EA Global: Melbourne 2015 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "cLLxHeLBtLcMaYM4P", "title": "Hilary Greaves: Repugnant interventions", "postedAt": "2015-08-28T16:14:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=QCoYq7kzcH0\"><div><iframe src=\"https://www.youtube.com/embed/QCoYq7kzcH0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, philosopher Hilary Greaves discusses child mortality and family planning, which are both (fairly) frequently cited as \u2018top picks\u2019 in global&nbsp;prioritisation.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "ureuixpbs93NgL3Dx", "title": "Niel Bowerman: Astronomical stakes", "postedAt": "2015-08-28T16:14:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=p0JPpRswZKY\"><div><iframe src=\"https://www.youtube.com/embed/p0JPpRswZKY\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: Melbourne 2015 talk, Niel Bowerman discusses two possible existential risks \u2014 pandemics, and superintelligence \u2014 and the astronomical stakes for humanity in the future.&nbsp;</i></p><p><i>Dr. Niel Bowerman is Assistant Director at the Future of Humanity Institute. Niel co-founded the Centre for Effective Altruism, the Global Priorities Project, climate policy think tank Climatio, and was on the 80,000 Hours founding team. Niel has a DPhil (PhD) in physics from Oxford University. He was a member of President Obama\u2019s Energy and Environment Policy Team, and was Climate Science Advisor to the President of the Maldives. Niel has delivered speeches at G8 Summits, the European Parliament, World Bank, UK Treasury and at the World Economic Forum in Davos. Niel was short-listed for 'Most inspirational young person' by UK Climate Week, was awarded the inaugural Oxford University Vice-Chancellor\u2019s Civic Award, and was named a 'Young Global Shaper' by the World Economic Forum.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "2yjcCkW8e5t2gn52x", "title": "Why \"scout mindset\" is crucial to good judgment (Julia Galef)", "postedAt": "2016-04-04T07:00:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=3MYEtQ5Zdn8\"><div><iframe src=\"https://www.youtube.com/embed/3MYEtQ5Zdn8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>In this talk, Julia introduces a metaphor of soldiers and scouts to describe two very different approaches to thinking. To illustrate these two mindsets in action, she tells the story of the Dreyfus Affair: how one man was wrongfully convicted of treason, and how another realized this fact and fought to exonerate him.&nbsp;</p><p>Julia Galef co-founded the Center for Applied Rationality, a nonprofit devoted to developing cognitive science-based strategies for reasoning and decision-making. For the last six years, she has hosted the Rationally Speaking podcast.</p><p>For more on scout mindset, see <a href=\"https://www.amazon.com/Scout-Mindset-Perils-Defensive-Thinking/dp/0735217556\">Julia's book on the subject.</a></p>", "user": {"username": "EA Fellowship"}}, {"_id": "zuLX4GxFXBjnoTLSC", "title": "Industrial farming is one of the worst crimes in history", "postedAt": "2015-09-25T07:00:00.000Z", "htmlBody": "<blockquote><p>Animals are the main victims of history, and the treatment of domesticated animals in <a href=\"https://www.theguardian.com/environment/2010/oct/25/farming-ecosystems-prince-charles\">industrial farms</a> is perhaps the worst crime in history. The march of human progress is strewn with dead animals. Even tens of thousands of years ago, our stone age ancestors were already responsible for a series of ecological disasters. When the first humans reached Australia about 45,000 years ago, they quickly drove to extinction 90% of its large animals. This was the first significant impact that Homo sapiens had on the planet\u2019s ecosystem. It was not the last.</p><p>About 15,000 years ago, humans colonised America, wiping out in the process about 75% of its large mammals. Numerous other species disappeared from Africa, from Eurasia and from the myriad islands around their coasts. The archaeological record of country after country tells the same sad story. The tragedy opens with a scene showing a rich and varied population of large animals, without any trace of Homo sapiens. In scene two, humans appear, evidenced by a fossilised bone, a spear point, or perhaps a campfire. Scene three quickly follows, in which men and women occupy centre stage and most large animals, along with many smaller ones, have gone. Altogether, sapiens drove to extinction about 50% of all the large terrestrial mammals of the planet before they planted the first wheat field, shaped the first metal tool, wrote the first text, or struck the first coin.</p><p>The next major landmark in human-animal relations was the agricultural revolution: the process by which we turned from <a href=\"https://www.theguardian.com/science/2015/may/14/early-men-women-equal-scientists\">nomadic hunter-gatherers</a> into farmers living in permanent settlements. It involved the appearance of a completely new life-form on Earth: domesticated animals. Initially, this development might seem to have been of minor importance, as humans only managed to domesticate fewer than 20 species of mammals and birds, compared with the countless thousands of species that remained \u201cwild\u201d. Yet, with the passing of the centuries, this novel life-form became the norm. Today, more than 90% of all large animals are domesticated (\u201clarge\u201d denotes animals that weigh at least a few kilograms).&nbsp;</p><p>Consider the chicken, for example. Ten thousand years ago, it was a rare bird that was confined to small niches of South Asia. Today, billions of chickens live on almost every continent and island, bar Antarctica. The domesticated chicken is probably the most widespread bird in the annals of planet Earth. If you measure success in terms of numbers, chickens, cows and pigs are the most successful animals ever.</p><p>Alas, domesticated species paid for their unparalleled collective success with unprecedented individual suffering. The animal kingdom has known many types of pain and misery for millions of years. Yet the agricultural revolution created completely new kinds of suffering, ones that only worsened with the passing of the generations.</p></blockquote><h3><a href=\"https://www.theguardian.com/books/2015/sep/25/industrial-farming-one-worst-crimes-history-ethical-question\"><strong>Read the rest of the article</strong></a></h3>", "user": {"username": "EA Fellowship"}}, {"_id": "tpSXehgdZsd79XaCN", "title": "Robert Wiblin: Making sense of long-term indirect effects", "postedAt": "2016-08-06T00:40:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=qV83HnxONQk&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=33\"><div><iframe src=\"https://www.youtube.com/embed/qV83HnxONQk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2016 talk, 80,000 Hours' </i><a href=\"http://www.robwiblin.com/\"><i>Robert Wiblin</i></a><i> argues that the indirect long-term effects of our actions are both very important and very hard to estimate. He also argues that the most promising interventions include targeted work to reduce existential risk, along with promotion of peace and&nbsp;wisdom.</i></p><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>This talk is about flow through effects and the effect of different actions on the very long term. I should give you a health warning first. This is the talk about flow through effects that your mother warned you about. This is not going to be super inspiring, necessarily. It can be a little bit demoralizing to think about how hard it can be to affect the long&nbsp;term.</p><p>I also have more questions, really, than answers here. I don't have a simple thesis that I'm just going to be pushing on you. I'm going to be describing some of the issues that exist here and some of the questions that are still open. It's not going to have a simple ending, necessarily. It can be quite hard to forecast what effects our actions are going to have. Things that initially seem bad can end up being good in the long run. We've probably all experienced this in our own lives as well. I think this isn't a reason to not bother thinking about the long-term effects of our actions, because if we can't predict what effect our actions are going to have, even on a balance of probability standard, then they're probably not very valuable to do in the first&nbsp;place.</p><p>So first I just want to define some terms here because there's a lot of different words that people use to describe flow through effects, and that was the initial name for this talk, but Toby Ord convinced me that we should do a bit of rebranding here. Get rid of the term flow through effect, which is a bit unnecessarily vague, and start talking about indirect effects, which I'm going to define as effects that affect someone other than the person you initially intended to target, and long-term effects, which are effects that occur after the present generation is dead, at least assuming we have normal human lifespans. In the long term, all effects are going to be&nbsp;indirect.</p><p>I'll just describe some of the hypotheses that are relevant to whether flow through effects matter very much. The first one is the astronomical stakes idea, which <a href=\"https://www.youtube.com/watch?v=fmQkLKLmfQU\">Nick Bostrom came up with it</a>, and he gave it that name, and in fact I'm stealing a lot of ideas from Bostrom in this talk. The idea here is that what matters most of all is what happens to the vast amount of energy in the universe. Currently there's an enormous number of stars out there, an enormous amount of matter and energy, but as far as we can tell, it's basically producing something like zero value. It's just hydrogen sitting there in deep space, the suns burning up. It's not something that we would regard as particularly valuable or particularly&nbsp;harmful.</p><p>But if we organized it in the right way, then it could be extremely valuable or extremely harmful. The scale of the potential value that the whole rest of the universe produces is trillions of trillions times larger than what we could do just on Earth with current technology. This seems to me to be a pretty likely hypothesis. It's obvious that this is a pretty compelling idea if you're a utilitarian, but even if you place just some probability on consequences being something that really matter, and creating new positive things being a valuable thing to do. Then, because the scale of the potential benefit is so enormous, trillions of times larger than the things that we can accomplish on Earth, then in expected value terms, the astronomical stakes stuff is going to dominate your calculation of what's most&nbsp;important.</p><p>This pretty naturally leads onto the long-term effects hypothesis, which is that the majority of the value of our actions is from their effects on people who don't yet exist. I think if you buy the astronomical stakes argument, you probably have to buy this one as well. How can we affect what things happen in a hundred years' time, or a thousand years' time, or a million years' time? You would have to do it indirectly, through an indirect series of cause and effect: one person affecting the next generation, affecting the next generation, and so on, so it has to be indirect. I think even if you don't buy the astronomical stakes hypothesis, if you think that there are more than ten generations of people yet to come, if humans are going to continue existing even for another 500 years or so, then I think the effect on future generations, generations after the present one, are likely to dominate the moral effects of your various&nbsp;actions.</p><p>You have to trade off the fact that your effects on future generations, generations after the present one, are more uncertain, but there's also going to be a whole lot more of them. I'm just ballparking it here, but I think if there's more than ten yet to come, then you would have to say that the long-term effects of your actions are more important. So, Bostrom, thinking about this, was wondering if the long term is really what matters, what should we be looking to&nbsp;do?</p><p>The first suggestion, <a href=\"https://nickbostrom.com/astronomical/waste.html\">from a paper in 2003</a>, was to minimize the risks that humanity faces. A good signpost for one day achieving astronomical gains would be to achieve an okay outcome today. The logic here is that so long as we don't permanently ruin things, either by going extinct or by having some horrible dictatorship from which we can never escape, then humanity survives and we still have the scientific method, and we still have our brains, and we can live to improve the world another day. We can correct our mistakes and do better. This sometimes appears with the name <a href=\"http://www.existential-risk.org/concept.pdf\">\"maxipok\"</a>, which is an idea that maximizing the probability of an okay outcome is what we should be aspiring to do in the short&nbsp;term.</p><p>How might we go about doing this? Approach one, which I think is the one that most effective altruists are implicitly taking, is they're buying the better position hypothesis, which is that faster human empowerment, so reducing poverty, having better economic growth, improving people's health, improving their education and their understanding of the world, this kind of thing reliably makes the future more promising, basically because it puts us in a better position to deal with future challenges. Whatever the threats are to humanity's success in the longer term, if we have a lot of wealth, and we're healthy, and we're educated, then we'll be in a good position to deal with those&nbsp;problems.</p><p>I think this makes complete sense if you think we live in a world where the main conflict is between humans and nature, people and nature, which is a classic story archetype, because empowerment helps us to deal, clearly, with natural disasters like supervolcanoes, asteroids, diseases and pandemics. If we're smarter we can come up with vaccines more quickly, and we can prevent them from spreading. Wild animal suffering is another thing we could deal with if humans were better empowered, and so on, and so on. So in our people versus nature world, I think this theory is very&nbsp;compelling.</p><p>What if that's not the kind of narrative that's going on in the universe? What if we've seen the enemy, and it's us? What if we live in a person vs person conflict story? In that case, the better position hypothesis is a whole lot less clear because education, say, puts us in a better position to both solve and create problems more quickly. It empowers both good and bad things that humanity can do. For example, if we're better educated and we have a better economy, we will invent nuclear weapons sooner, perhaps, but we're then in a better position to invent ways not to use them because we'll come up with game theory and mutually assured destruction, and we'll figure out a way to deal with nuclear weapons without killing&nbsp;ourselves.</p><p>As an example of how development can create risks, the Soviet Union in the late '20s through the late '40s went through an absolutely explosive period of economic growth, one of the most rapid modernization processes that we've ever seen, something like China in the modern era. Millions and millions of people moving off of very unproductive jobs in farms into factories. From a human empowerment point of view, this looked absolutely fantastic because you've got lots of people escaping poverty, improved health, improved education. And probably it was a positive thing, but it also created some risks because the fact that the USSR industrialized so quickly meant that they were able to develop nuclear weapons very soon after the US developed them, creating, again, the potential for world-destroying nuclear war that wouldn't have existed, say, if the US had been the only power. In addition to that, the USSR was controlled by Stalin, one of the most monomaniacal, totalitarian dictators ever. A really evil guy who became a lot more powerful because he had this enormous economy behind him. So the USSR developing wasn't an unmitigated positive thing. It created some risks to humanity as&nbsp;well.</p><p>So here I want to present the person versus person hypothesis, which is that most of the threats to the long term are human created. I think this is true, because except for pandemics, with most natural risks that would be very damaging, like volcanoes, supervolcanoes or asteroids, the annual risk is really, really low, and we can usually recover from most of those things. It\u2019s very, very hard for a supervolcano to go out and absolutely kill everyone. The Future for Humanity Institute has a paper forthcoming talking about this, how anthropogenic risk is significantly larger than the risk from nature, probably like 10 or 100 times&nbsp;higher.</p><p>To get an idea of how you would go about modeling whether human empowerment is positive or negative in a person versus person world, it's definitely not easy because you have to think what is the risk to human civilization proportional to. Is it a per year risk, like you'd have with asteroids, where just every year there's a risk that an asteroid or a comet is going to come by the earth? Or maybe some of the risks that we face are proportional to the annual rate of growth? Perhaps if we grow faster, then we have less time to prepare for changes, and so we're less well able to deal them when they arrive? In which case going more slowly will be better because we can have more potential for&nbsp;forethought?</p><p>If it's per year, with the nuclear weapons example, you've got a risky time between when you invent nuclear weapons and when you invent a technology that neuters nuclear weapons, like mutually assured destruction that stops us from using them. In that case, if it's just in a transition between two things, then going faster is fine, because you're shortening the time between when you invent the problem and when you solve it. So in that case, maybe you want to go fast. But basically the modeling gets tricky here pretty quickly, so it's hard to come up with an overwhelming argument one way or the&nbsp;other.</p><p>Another thing that might be relevant would be the ratio between human prudence versus our power, our technological ability. One idea that you might have would be we should only obtain technological capabilities once we're actually ready to deal with them. That's the thing that's going to limit the risks to humanity in the long term. We do want to have technologies, but only once we're able to use them safely. For example, we give kids scissors but we don't give them guns, because scissors are useful to a child, and the risks that they pose are not so large because they know that they're going to cut themselves, and even if they do it's not the end of the world. Well we don't give them guns because they don't understand them, and they don't yet know how to use them safely, necessarily, so we wait until they're somewhat older and more&nbsp;mature.</p><p>The question I want to pose to you is do we have the unity, the compassion, and the maturity to wield new technologies of mass destruction? I think the answer is pretty clearly no, but if you're not convinced then this genius can tell you&nbsp;why.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/5Dj3REpUNaMgIWmo4wAium/47ec04f0c9f275f15aeffa576e2dccf4/We_need_brain.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/5Dj3REpUNaMgIWmo4wAium/47ec04f0c9f275f15aeffa576e2dccf4/We_need_brain.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/5Dj3REpUNaMgIWmo4wAium/47ec04f0c9f275f15aeffa576e2dccf4/We_need_brain.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/5Dj3REpUNaMgIWmo4wAium/47ec04f0c9f275f15aeffa576e2dccf4/We_need_brain.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/5Dj3REpUNaMgIWmo4wAium/47ec04f0c9f275f15aeffa576e2dccf4/We_need_brain.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/5Dj3REpUNaMgIWmo4wAium/47ec04f0c9f275f15aeffa576e2dccf4/We_need_brain.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/5Dj3REpUNaMgIWmo4wAium/47ec04f0c9f275f15aeffa576e2dccf4/We_need_brain.png?w=1800&amp;q=70 1800w\"></figure><p>I think he's got a very strong case here, that we need to have more brains before we develop technologies that are extremely risky to us. He\u2019s a very trustworthy&nbsp;fellow.</p><p>So this would suggest a different approach to going about doing good, which would be differential speed-up. If you're trying to do differential speed-up, rather than just general human empowerment, then you'll want to be thinking, \"What things do we most need before other things? What is it beneficial to have first, and also what things seem least likely to backfire, at least if we get them immediately?\" This leads to a question. What are signposts for a good future? Here I'm basically stealing a bunch of this analysis, again, from Bostrom, <a href=\"https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom/\">and the talk that he gave in Oxford two years&nbsp;ago</a>.</p><p>He went through a whole lot of possible things that are proxies for good long-term outcomes that we could measure in the short term. An analogy is with a chess game. Early on in the game, obviously you want to be capturing your opponent's pieces, but you can't always see exactly how the game is going to go later on. You don't know how it's going to end, and that's not how good players figure out what the best next move is. They don't map it out to actually check mating their opponent. Instead they're looking for proxies in the short term, like do I control a lot of the board, for example, or am I capturing my opponent's pieces. That's the kind of thing that we're looking for here. Things that we can actually measure in the short term but are a reliable and a consistent guide to whether we're making the future more&nbsp;promising.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/60ZFnfZKaAya62s4GwqI0/71090152688fc5843995618349d89802/Signposts_for_a_good_future.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/60ZFnfZKaAya62s4GwqI0/71090152688fc5843995618349d89802/Signposts_for_a_good_future.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/60ZFnfZKaAya62s4GwqI0/71090152688fc5843995618349d89802/Signposts_for_a_good_future.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/60ZFnfZKaAya62s4GwqI0/71090152688fc5843995618349d89802/Signposts_for_a_good_future.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/60ZFnfZKaAya62s4GwqI0/71090152688fc5843995618349d89802/Signposts_for_a_good_future.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/60ZFnfZKaAya62s4GwqI0/71090152688fc5843995618349d89802/Signposts_for_a_good_future.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/60ZFnfZKaAya62s4GwqI0/71090152688fc5843995618349d89802/Signposts_for_a_good_future.png?w=1800&amp;q=70 1800w\"></figure><p>Here are a number of things that he suggested. I don't have time to go through all of them. You notice, for example, that economic growth has a bit of a question mark next to it because it's just not really clear whether faster economic growth is so good or bad in the long term. But here are some of the ones that did seem more promising as possible signposts to guide us into the future. One would be a biological cognitive enhancement to making people smarter in the hope that they'll be able to deal with future challenges in a more wise and prudent way than we are, something that works like education, but more so. International peace and cooperation to prevent conflict, because a large way in which technologies can go wrong is if they're used as, say, weapons of war, or used by people against others. Solutions to the AI alignment problem. Obviously if we're going to create machines that are smarter than humans, then we want them to be aligned with human interests. There are good reasons to think that if we don't make a special effort to do this, they're not going to be aligned with higher&nbsp;interests.</p><p>Then one that I'd add in is better moral attitudes, so trying to encourage people to care about the welfare of all. I think it's harder to see how that could backfire than to see how economic growth might backfire, though they both have some similar values. If we can get future generations to care about everyone equally, to be very compassionate and not just pursue their own selfish interests, I think that's a reasonably good signpost for improving the&nbsp;future.</p><p>An interesting thing to observe about this whole framing about differential speed-up is that it actually brings effective altruism somewhat close to traditional ideas about how you might do good. People sometimes say, \"Why are you just focused on curing malaria, like is that really the best way to change the world?\" I think sometimes people have a point that that just might not be the best way to go about it. More traditional ideas might focus on wise leadership of a country, capacity and institution building to deal with problems, improving people's moral attitudes, and also just being wary of rapid change in a way that many of us are not. It's mostly conservatism being worried that if everything is just up-ended and we totally change society overnight, maybe that's going to go wrong and we should be crossing the river by feeling the stones, so to&nbsp;speak.</p><p>What do I think are probably the most important causes? My guess is things like working on risks in biotechnology, AI value alignment, climate change, preventing war and promoting peace, and a sense that we ought to be cooperating with one another and avoiding conflict, improving intelligence within government, such as forecasting the future and making good collective decisions, and similar such things, which I think are probably a more reliable guide to improving the future than simply trying to increase GDP&nbsp;growth.</p><p>What about reducing poverty? Am I saying that reducing poverty isn't good? No, I don't think that that's the case because reducing poverty also raises global sanity through more education and people who are smarter, which leads to more cosmopolitan moral values, and leads to better government as well. People who've put a lot more thought into this than me generally think that it's probably good overall. All I'm saying is that it might not be as good as it initially appears to be, but I'm certainly not saying that it's neutral or negative. If you'd like to explore this more, a really good thesis is <a href=\"https://rucore.libraries.rutgers.edu/rutgers-lib/40469/\"><i>On The Overwhelming Importance of Shaping the Far Future</i></a>, where one of our trustees, Nick Beckstead, concludes that improving economic growth is probably a positive, just maybe not as positive as it might first look. Another one is <a href=\"https://scholar.harvard.edu/files/bfriedman/files/the_moral_consequences_of_economic_growth_0.pdf\"><i>The Moral Consequences of Economic Growth</i></a> by Benjamin Friedman, which talks about the changes that you get in a society when it stagnates economically, and how you often get quite rapid moral regression. People are reverting to more tribal values, being less likely to cooperate with one another, and being less empathetic. Quite an interesting book from about 2006, I&nbsp;think.</p><p>Something to know is that poverty really isn't that neglected in the scheme of things. It's not a terribly unusual cause. Which is one reason that we talk about it a lot, because it's easy to explain to people that it's good if you save lives and reduce poverty. Reducing poverty absorbs something like more than half, let's say, of all effort by effective altruists, certainly more than half of all donations, so it's a really large focus area, I think, maybe relative to the strength of the arguments. It's a significant fraction of all actions by the poor themselves, of billions of people who are in relative poverty or global poverty. They're trying to get out of poverty themselves, and this is something we should consider when thinking is this really a neglected opportunity. It's true they don't have necessarily the same resources, but if you add it all up, I think there's a lot of work going into trying to prevent poverty, plus the many foundations that are focused on this, including many of the largest, and quite a lot of government&nbsp;aid.</p><p>I think poverty is neglected relative to some things, but it's probably not among the most neglected problems in the world. If you compare that with, say, how many NGOs and foundations there are working on international coordination, new dangerous technologies, peace, improving forecasting, this kind of work is reasonably obscure by comparison. I think there might be more low hanging fruit here because fewer people are working on it. Lots of people say, \"I want to end poverty,\" but if you meet a 16-year-old who says \"I really want to improve forecasting ability within the intelligence services,\" that is not a common thing for teenagers to dream of doing with their&nbsp;career.</p><p>In addition, I think these other ways of doing good can be quite a good fit for us. Effective altruism is sometimes accused of being filled with elites. I think that is potentially quite problematic in some ways, in that we can be very out-of-touch, maybe we just haven't experienced poverty ourselves, so that could blinker us, but it also creates some opportunities potentially if we have a lot of connections with people in government or within academia. I think effective altruism, many people here would have an unusually good shot at guiding governance and public service, rising to the top levels within important institutions within society, guiding specific new technologies because we're particularly clued into what things are being developed in the next five or ten years, and think about how we can make sure that they're used in wise ways rather than risky&nbsp;ways.</p><p>We might be in a good position to improve society's moral values as well, though I think this is a bit more questionable. Many of us might be out of touch with a lot of people in society. I personally often don't feel like I'm super in-touch with a lot of people, and so maybe on the one hand we have potentially a large audience, but are we actually good at persuading? Are most people in society receptive to changing their moral values and caring more about people overseas? I think that that's an open&nbsp;question.</p><p>So the bottom lines here are these. The indirect effects are crucial, though they're really hard to estimate. I think peace and collective wisdom are somewhat underrated by people in this community. There's probably an excessive focus on economic growth and health relative to other cause areas that might be more reliable signposts to improving the future and be somewhat more neglected. It may be a clich\u00e9 to say, but I think further research really is needed this time because this could be one of the things that is causing us to actually not do things that are terribly&nbsp;valuable.</p><p>Of course, people have known about his problem for years. I started working at the Centre for Effective Altruism four years ago. This isn't a new concern, but because it's a bit demoralizing to think about how hard it can be to predict the long-term effects of your actions and how hard it is to have really good insights here, this topic just kind of goes a bit neglected in my view. I think it would be valuable to get more smart people really thinking seriously about this and putting in months or years of work. You know, coming up with their own ideas and their own models for how we can improve society in the long&nbsp;term.</p><p>All that said, given how hard it is to think about this topic, all above might be misguided. I wouldn't like it if too much stuck to any specific thing that I've said, but I think the overall issue is quite important, so I'd like to have great conversations here in the rest of the conference. Thanks so much. So can I take questions for five minutes? All right, go for&nbsp;it.</p><h1>Q&amp;A</h1><p><strong>Question:</strong> What are the most effective interventions you are aware of for increasing peace? Maybe if you could focus especially on the Middle&nbsp;East?</p><p><strong>Rob Wiblin:</strong> Right. Unfortunately, I'm not an expert on that, and the Middle East sounds particularly tricky. I think the thing that I'd be focusing on if I was working on peace would be trying to get China and the US to get along so they don't have a massive war. Trying to get greater understanding of the interests of the Chinese government and Chinese people in the US, and vice versa, so they don't accidentally come to blows. Potentially also things around nuclear security. Unfortunately, I haven't gotten to that level of thinking, exactly how would you do peace interventions. I think by changing values so that people just find war more abhorrent. That's something that's already happened and it's been very valuable. There are organizations like Ploughshares Foundation which do work on trying to improve cooperation and peace. Generally going into the Foreign Service, for example, and just having a very anti-war attitude, I think could be positive, but I haven't thought that much about it at that specific level.&nbsp;Yeah.</p><p><strong>Question:</strong> You suggested that biological cognitive enhancement could be a promising route forward, but you seem less confident, about, say, education? I was wondering if you could contrast&nbsp;those.</p><p><strong>Rob Wiblin:</strong> Yeah, you'll have to read Bostrom's paper on that because I was just copying off what he said there, and I'm actually not entirely sure what the logic is. They did do some modeling on the effects of cognitive enhancement as opposed to general improvement, and I think they thought there were some different effects. If you look at the Future of Humanity website or Nick Bostrom's website, I think you'll get the answer in&nbsp;there.</p><p><strong>Question:</strong> Yeah, kind of going off of that question, is there any focus on the distribution of technologies like cognitive enhancement and just any kind of biotech, because it seems like with the distribution of current technologies in healthcare and economic growth, that there seem to be flaws in the distribution, so it would be unequal, and would probably be centered around people who were also in EA, and going back to that elitism of who would get these technologies once they're made, and would it really help everyone, or would it really help those who can afford&nbsp;it?</p><p><strong>Rob Wiblin:</strong> Yeah. There are a lot of issues there. I think that\u2019s one reason to prefer increasing economic growth in the developing world rather than the rich world. Imagine increasing equality and ability to solve problems. Increasing income equality and education equality is probably a safer bet than improving the frontier where you're more likely to invent new, dangerous things with that education. However it's not completely obvious that having greater equality of access to all these new things would necessarily be positive. If something is dangerous and destabilizing, then maybe you want to just try it on a tiny number of people first before it's scaled up to lots of people. I think if you look at most technologies, initially they're very expensive and they're just used by an elite, but then they filter out to everyone gradually as it becomes cheap to produce them, like smart phones, that process is happening, and it's happening with other education technology as&nbsp;well.</p><p>Depending on the technology, it might be bad for everyone to have access to it. Like with nuclear weapons, I keep coming back to that example, it could have been good if the US was the only country that had nuclear weapons. There were a lot of negotiations, potentially, of all countries deciding not to have nuclear weapons, or the US considered just trying to maintain a complete advantage and being the only country that ever has them. It's possible to imagine that either of those scenarios could be more stable than what we have now, where one person could potentially destroy the entire world with the click of a button. So yeah, it's really&nbsp;complicated.</p><p><strong>Question:</strong> In your talk you mentioned about moral values and having more shared moral values. So if you could elaborate more in terms of what techniques or apparatus we would use to establish and propagate&nbsp;that?</p><p><strong>Rob Wiblin:</strong> Yeah, to some extent I think what we're doing this already. For example, I think the effective altruism movement as a whole encourages people to take a global perspective on things and gives significant weight to the effects on, say, people in other countries. I think most people when they're deciding what policies their government should implement just think how this affects the citizen of this country, which I can often find quite confronting, and people have that mindset. But we're saying, \"No, you've got to think about not just like healthcare in your own country, but like what could your country do to improve health globally?\" That's like one of those shifts in mindset that's very&nbsp;valuable.</p><p>We're trying to encourage people to be concerned about the welfare of animals, both in farms, where we treat them badly, or even animals in nature, as a more extreme case. We also try to get people to think long term, to think about impacts on future generations, not just their children's children, but far, far beyond that. I think those are potentially the most important moral shifts I think you can get. Basically this idea that we shouldn't just be focused on ourselves, or our family, or people who look like us, or sound like us, but thinking about the effects of our actions on everyone and all kinds of beings at all times. We do kind of push that, but I think we could potentially be more focused on changing moral values and reaching a lot of people with that message, if that was what we were explicitly trying to&nbsp;do.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "HYHaBsukLkoE72zTd", "title": "Andrew Critch: Logical induction \u2014 progress in AI alignment", "postedAt": "2016-08-06T00:40:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=lXm-MgPLkxA&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=32\"><div><iframe src=\"https://www.youtube.com/embed/lXm-MgPLkxA\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>The Machine Intelligence Research Institute (MIRI) is interested in reasoning about highly advanced AI systems before they exist, specifically for developing models of safety and control for such systems. In this EA Global: San Francisco 2016 talk, MIRI's Andrew Critch examines some criteria for \"ideal\" logical induction, a new algorithm for logical induction that satisfies many desirable properties, and some implications for what a very powerful AI system is able to learn.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "4DgeZPiiwntgWn95j", "title": "Will MacAskill: Closing session (2016)", "postedAt": "2016-08-06T00:40:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=D2SwES3r9Kg&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=31\"><div><iframe src=\"https://www.youtube.com/embed/D2SwES3r9Kg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Will MacAskill summarizes the EA Global: San Francisco 2016 conference and shares a call to action.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "h58kFi5h5uAoxkMRD", "title": "John Chisholm: Earning influence with thought leaders", "postedAt": "2016-08-06T00:40:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=_ffVmzzqU28&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=30\"><div><iframe src=\"https://www.youtube.com/embed/_ffVmzzqU28\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2016 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "hfNeDBcND6uTov99J", "title": "Julia Wise: Coordinating for a stronger community", "postedAt": "2016-08-06T00:40:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=jbpeMmBmt7I&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=29\"><div><iframe src=\"https://www.youtube.com/embed/jbpeMmBmt7I\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2016 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "JnPkBze4er9mHKdpE", "title": "Christine Peterson: Upstream altruism", "postedAt": "2016-08-06T00:40:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ICa4hSVgmNk&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=27\"><div><iframe src=\"https://www.youtube.com/embed/ICa4hSVgmNk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Christine Peterson discusses applying effective altruist principles to early-stage action in this EA Global: San Francisco 2016 talk.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "95bGN9oGKjCiyW8sx", "title": "Cass Sunstein: From behavioural economics to public policy", "postedAt": "2016-08-06T00:40:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=skS9ube2ov8&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=26\"><div><iframe src=\"https://www.youtube.com/embed/skS9ube2ov8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>How can we use insights from modern psychology to create effective policies? Cass Sunstein \u2014 author of </i>Nudge: Improving Decisions about Health, Wealth, and Happiness<i>; S</i>impler: The Future of Government<i>; and </i>Wiser: Getting Beyond Groupthink to Make Groups Smarter<i> \u2014 addresses that question in this EA Global: San Francisco 2016 talk.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "hTvoTbzLrkFvRFffv", "title": "Rachel Glennerster: Engaging developing country governments", "postedAt": "2016-08-06T00:40:47.589Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=hsOINhyL-gM&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=25\"><div><iframe src=\"https://www.youtube.com/embed/hsOINhyL-gM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>How can we leverage the massive opportunity of influencing government spending? Rachel Glennerster, the Executive Director of the Abdul Latif Jameel Poverty Action Lab (J-PAL), address that question in this EA Global: San Francisco 2016 talk.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "56nPm7cjns4tyMumm", "title": "Michael Page: Embracing the intellectual challenge of effective altruism", "postedAt": "2016-08-06T00:40:08.242Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=xj3RJi1n72s&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=23\"><div><iframe src=\"https://www.youtube.com/embed/xj3RJi1n72s\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>While it's easy to view the intellectual challenge of effective altruism as a liability, it is better to view it as an asset. In this talk from EA Global: San Francisco 2016, Michael Page lays out why effective altruism is hard, and how we can accept and appreciate that fact.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "NntxFAFnoz5d7i9tD", "title": "Tara Mac Aulay: The effective altruism ecosystem", "postedAt": "2016-08-05T09:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=3S0SJgKytHE&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=22\"><div><iframe src=\"https://www.youtube.com/embed/3S0SJgKytHE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we plan to post a transcript for this EA Global: San Francisco 2016 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "GpyZjWBbEjc6hJ2sF", "title": "Owen Cotton-Barratt, Robin Hanson, Jason Matheny, and Julia Galef: Forecasting", "postedAt": "2016-08-05T09:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=GAvq7B-_xzY&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=21\"><div><iframe src=\"https://www.youtube.com/embed/GAvq7B-_xzY\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>This EA Global: San Francisco 2016 panel explores how we might use forecasting technology to improve decision making.</i></p><p><i>In the future, we maypost a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "svFBXfRqhXJ3559Ta", "title": "Jason Matheny: Government-driven science and technology", "postedAt": "2016-08-05T09:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=PZ6yfPxp7gE&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=21\"><div><iframe src=\"https://www.youtube.com/embed/PZ6yfPxp7gE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>This EA Global: San Francisco 2016 is by Jason Gaverick Matheny, director of Intelligence Advanced Research Projects Activity (IARPA). Previously Jason worked for the Future of Humanity Institute at Oxford University, where his work focused on existential risk. He holds a PhD in Applied Economics and a Master's in Public Health from Johns Hopkins University, an MBA from Duke University, and a BA from the University of Chicago. He was the cofounder of New Harvest, which supports the development of new agricultural biotechnologies. His work was called one of the \"ideas of the year\" by The New York Times.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "pfDntBuCGbsvBWmiP", "title": "Sam Deere, Tara Mac Aulay, Alexandre Sevigny, Ken Strasma, Brett Thalmann, and Chris Wylie: Data-driven movement building", "postedAt": "2016-08-05T09:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=-JkbuDM8otI&amp;list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&amp;index=20\"><div><iframe src=\"https://www.youtube.com/embed/-JkbuDM8otI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>This EA Global: San Francisco 2016 panel discusses using modern data to inform decisions about social movements.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}]