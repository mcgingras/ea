[{"_id": "6c7HqoKS69L45EJk5", "postedAt": "2023-05-09T12:51:01.459Z", "postId": "CgeDuvedjqCj56HXZ", "htmlBody": "<p>I'm sorry you've had this experience :(&nbsp;<br><br>I know of some other people who've taken bad mental health damage from internalizing pessimistic beliefs about AI risk, as well.<br><br>I'm not sure what to do about this, because it seems bad to recommend 'try to not form pessimistic opinions about AI for the sake of your mental health, or remain very detached from them if you do form them', but being fully in touch with the doom also seems really bad. &nbsp;</p>", "parentCommentId": null, "user": {"username": "Amber"}}, {"_id": "e8WodKkzLKEtyTgWY", "postedAt": "2023-05-09T13:01:49.012Z", "postId": "CgeDuvedjqCj56HXZ", "htmlBody": "<p>To be clear, I'm not at all recommending changing one's beliefs here. My language of gut belief vs cognitive beliefs was probably too imprecise. I'm recommending that, for some people, particularly if one is able to act on beliefs one doesn't intuitively feel, it's better not to try to intuitively feel those beliefs.&nbsp;</p><p>For some people, this may come at a cost to their ability to form true beliefs, &nbsp;and this is a difficult tradeoff. For me, I think, all things considered, intuiting beliefs has made me worse at forming true beliefs.&nbsp;</p>", "parentCommentId": "6c7HqoKS69L45EJk5", "user": {"username": "Nathan_Barnard"}}, {"_id": "ZpaDybgBxEB7H7vje", "postedAt": "2023-05-09T17:03:07.563Z", "postId": "CgeDuvedjqCj56HXZ", "htmlBody": "<p>I'm sorry to hear about this, Nathan. As I <a href=\"https://forum.effectivealtruism.org/posts/3KAuAS2shyDwnjzNa/predictable-updating-about-ai-risk#5_2_An_aside_on_mental_health\">say in the post</a>, I do think that the question how to do gut-stuff right from a practical perspective is distinct from the epistemic angle that the post focuses on, and I think it's important to attend to both.</p>", "parentCommentId": null, "user": {"username": "Joe_Carlsmith"}}, {"_id": "c9HsjNbG57EHMhpLh", "postedAt": "2023-05-09T20:59:48.739Z", "postId": "CgeDuvedjqCj56HXZ", "htmlBody": "<p>I agree ideally one would do gut stuff right both practically and epistemically. In my case, the tradeoff of productivity loss and loss in general reasoning ability in exchange for some epistemic gains wasn't worth it.</p><p>&nbsp;I think it's plausible that for people in a similar situation to me - people who are good at making decisions based on just analytic reasoning and have reason to think that they might be vulnerable if they were to try to believe things on a gut level as well as an analytic one - should consider not engaging certain EA topics on a gut level (I don't restrict this to AI safety - I know people who've had similar reactions thinking about nuclear risk and I've personally made the decision not to think about s-risk or animal welfare on a gut level either.)</p><p>I do want to emphasise that there was a tradeoff here - I think I have somewhat better AI safety takes as a result of thinking about AI safety on a gut level. The benefit though was reasonably small and not worth the other costs from an impartial welfareist perspective.&nbsp;</p>", "parentCommentId": "ZpaDybgBxEB7H7vje", "user": {"username": "Nathan_Barnard"}}, {"_id": "zBcBFzvuqfyjiZSBQ", "postedAt": "2023-05-10T19:08:39.555Z", "postId": "CgeDuvedjqCj56HXZ", "htmlBody": "<p>I think it depends on what role you're trying to play in your epistemic community.</p><p>If you're trying to be a <a href=\"https://sci-hub.ru/10.1086/644786\">maverick</a>,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4v2o48xelzm\"><sup><a href=\"#fn4v2o48xelzm\">[1]</a></sup></span>&nbsp;you're betting on a small chance of producing large advances, and then you want to be capable of building and iterating on your own independent models without having to wait on outside verification or social approval at every step. Psychologically, the most effective way I know to achieve this is to act as if you're overconfident.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmewvo83o4gn\"><sup><a href=\"#fnmewvo83o4gn\">[2]</a></sup></span>&nbsp;If you're lucky, you could revolutionise the field, but most likely people will just treat you as a crackpot unless you already have very high social status.</p><p>On the other hand, if you're trying to specialise in giving advice, you'll have a different set of optima on several methodological trade-offs. On my model at least, the impact of a maverick depends mostly on the speed at which they're able to produce and look through novel ideas, whereas advice-givers depend much more on their ability to assign accurate probability estimates on ideas that already exist. They have less freedom to tweak their psychology to feel more motivated, given that it's likely to affect their estimates.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4v2o48xelzm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4v2o48xelzm\">^</a></strong></sup></span><div class=\"footnote-content\"><blockquote><p><i>\"We consider three different search strategies scientists can adopt for exploring the landscape. In the first, scientists work alone and do not let the discoveries of the community as a whole influence their actions. This is compared with two social research strategies, which we call the follower and maverick strategies. Followers are biased towards what others have already discovered, and we find that pure populations of these scientists do less well than scientists acting independently. However, pure populations of mavericks, who try to avoid research approaches that have already been taken, vastly outperform both of the other strategies.\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr4m8wxiufr\"><sup><a href=\"#fnr4m8wxiufr\">[3]</a></sup></span></p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmewvo83o4gn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmewvo83o4gn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm skipping important caveats here, but one aspect is that, as a maverick, I mainly try to increase how much I \"<a href=\"https://www.wikiwand.com/en/Alieve\">alieve</a>\" in my own abilities while preserving what I can about the fidelity of my \"beliefs\".</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr4m8wxiufr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr4m8wxiufr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'll note that simplistic computer simulations of epistemic communities that have been specifically designed to demonstrate an idea is very weak evidence for that idea, and you're probably better off thinking about it theoretically.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Elua"}}]