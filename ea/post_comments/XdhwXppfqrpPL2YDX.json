[{"_id": "GsapvgNWcDQgzDcsP", "postedAt": "2023-07-12T20:11:08.535Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>I think \"Changes in funding in the AI safety field\" was published by the Centre for Effective Altruism.</p>\n", "parentCommentId": null, "user": {"username": "Stefan_Schubert"}}, {"_id": "o7TycqBzAu5ewyTbF", "postedAt": "2023-07-12T20:52:30.489Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>Thanks for spotting that. I updated the post.</p>", "parentCommentId": "GsapvgNWcDQgzDcsP", "user": {"username": "Stephen McAleese"}}, {"_id": "MYRxEJxFreqsvpiAP", "postedAt": "2023-07-13T17:45:29.171Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<blockquote><p>AI safety is a field concerned with preventing negative outcomes from AI systems and ensuring that AI is beneficial to humanity.</p></blockquote><p>This is a bad definition of \"AI safety\" as a field, which muddles the water somewhat. I would say that AI safety is a particular <i>R&amp;D branch</i> (plus we can add here meta and proxy activities for this R&amp;D field, such as AI safety fieldbuilding, education, outreach and marketing among students, grantmaking, and platform development such as what apartresearch.com are doing), of the gamut of activity that strives to \"prevent the negative result of civilisational AI transition\".&nbsp;</p><p>There are also other sorts of activity that strive for that more or less directly, some of which are also <i>R&amp;D</i> (such as governance R&amp;D (cip.org), R&amp;D in cryptography, infosec, and internet decentralisation (trustoverip.org)), and others are not R&amp;D: good old activism and outreach to the general public (StopAI, PauseAI), good old governance (policy development, <a href=\"https://www.lesswrong.com/posts/xgXcZQd5eqMqpAw3i/consider-joining-the-uk-foundation-model-taskforce\">UK foundational model task force</a>), and various \"mitigation\" or \"differential development\" projects and startups, such as <a href=\"https://www.optic.xyz/\">Optic</a>, <a href=\"https://www.digitalgaia.earth/\">Digital Gaia</a>, <a href=\"https://ought.org/\">Ought</a>, social innovations (I don't know about any good examples as of yet, though), innovations in education and psychological training of people (I don't know about any good examples as of yet). See more details and ideas in <a href=\"https://www.lesswrong.com/posts/sTDfraZab47KiRMmT/views-on-when-agi-comes-and-on-strategy-to-reduce?commentId=HBrQNwjksyxXFwkFe\">this comment</a>.</p><p>It's misleading to call this whole gamut of activities \"AI safety\". It's maybe \"AI risk mitigation\". By the way, 80000 hours, despite properly calling \"<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#we-can-tackle-these-risks\">Preventing an AI-related catastrophe</a>\", also suggest that the only two ways to apply one's efforts to this cause is \"technical AI safety research\" and \"governance research and implementation\", which is wrong, as I demonstrated above.</p><p>Somebody may ask, isn't technical AI safety research more direct and more effective way to tackle this cause area? I suspect that it might not be the case for people who don't work at AGI labs. That is, I suspect that independent or academic AI safety research might be inefficient enough (at least for most people attempting it) that it would be more effective to apply themselves to various other activities, and \"mitigation\" or \"differential development\" projects of the likes that are described above. (I will publish a post that details reasoning behind this suspicion later, but for now <a href=\"https://www.lesswrong.com/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent?commentId=CFJMgThbkD5u2wr8b\">this comment</a> has the beginning of it.)</p>", "parentCommentId": null, "user": {"username": "Roman Leventov"}}, {"_id": "3EdeAzqxJS46fqtG9", "postedAt": "2023-07-13T18:03:39.097Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>FYI, if any readers want just a list of funding opportunities and to see some that aren't in here, they could check out <a href=\"https://forum.effectivealtruism.org/posts/DqwxrdyQxcMQ8P2rD/list-of-ea-funding-opportunities\">List of EA funding opportunities</a>.</p><p>(But note that that includes some things not relevant to AI safety, and excludes some funding sources from outside the EA community.)</p>", "parentCommentId": null, "user": {"username": "MichaelA"}}, {"_id": "ZBmEH3bpCTkx88pAS", "postedAt": "2023-07-13T21:06:42.476Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>Stephen - thanks for a very helpful and well-researched overview of the funding situation. It seems pretty comprehensive, and will be a useful resource for people considering AI safety research.</p><p>I know there's been a schism between the 'AI Safety' field (focused on reducing X risk) and the 'AI ethics' field (focused on reducing prejudice, discrimination, 'misinformation', etc.) But I can imagine some AI ethics research (e.g. on mass unemployment, or lethal autonomous weapon systems, or political deepfakes, or AI bot manipulation of social media) that could feed into AI safety issues, e.g. by addressing developments that could increase the risks of social instability, political assassination, partisan secession, great-power conflict, which could lead to increased X risk.&nbsp;</p><p>I imagine it would be much harder to analyze the talent and money devoted to those kinds of issues, and to disentangling them from other kinds of AI ethics research. But I'd be curious whether anyone else has any sense of what proportion of AI ethics work could actually inform our understanding of X risk and X risk amplifiers....</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "P8SddEmJXbk35antg", "postedAt": "2023-07-21T23:13:41.747Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>FYI: Our (GoodX\u2019s) project <a href=\"https://app.impactmarkets.io/\">AI Safety Impact Markets</a> is a central place where everyone can publish their funding applications, and AI safety funders can subscribe to them and fund them. We have <a href=\"https://forum.effectivealtruism.org/posts/Lna7SayJkyrKczH4n/play-regrantor-move-up-to-usd250-000-to-your-top-high-impact\">~$350k in total donation budget</a> (current updated number) from interested donors.</p><p>(If you\u2019re a donor interested in supporting early-stage AI safety projects and you\u2019re interested in this crowdsourced charity evaluator, <a href=\"https://bit.ly/donor-interests\">please sign here</a>.)</p>", "parentCommentId": null, "user": {"username": "Telofy"}}, {"_id": "xeHiwmbEvGe6govuc", "postedAt": "2023-08-26T10:12:11.084Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>Shouldn't the EMA calculation for 3 years be:<br><br>EMA = Current_year*2/(3+1) + Last_yearEMA*(1 - 2/(3+1))<br><br>EMA = Current_year*0.5 + Last_yearEMA*0.5<br><br>And EMA calculation for 2 years be (your google sheet formula):<br><br>EMA = Current_year*2/(2+1) + Last_yearEMA*(1 - 2/(2+1))<br><br>EMA = Current_year*0.66 + Last_yearEMA*0.33<br><br>However, in the google formula you weighted the previous year by 0.66 and the current year by 0.33, meaning that you gave the most recent data less weightings and the EMA is actually 2 years instead of 3?<br><br><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html</a><br><a href=\"https://www.investopedia.com/terms/e/ema.asp\">https://www.investopedia.com/terms/e/ema.asp</a><br>&nbsp;<br>A slight change to use a 3 year-smoothing of a 3 year EMA<br><br>EMA(3 years) = Fund(t)*0.571+EMA(t-1)*0.286 + EMA(t-2)*0.143</p><p>*0.571+0.286+0.143 = ~ 1</p><p>I.e. 3 years EMA should actually be:<br><br>&nbsp;</p><figure class=\"table\" style=\"width:0px\"><table><tbody><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2014</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">0</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">0</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2015</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">1186000</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">782760</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2016</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">6563985</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">3971904.795</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2017</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">43222473</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">25927931.53</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2018</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">4280392</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">10427474.64</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2019</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">63826400</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">43134826.36</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2020</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">19626363</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">25034342.48</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2021</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">90760985</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">65152624.55</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2022</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">118717429</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">90001213.56</td></tr><tr><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2023</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">30873035</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">52685675.37</td></tr></tbody></table></figure>", "parentCommentId": null, "user": {"username": "chanakin"}}, {"_id": "f7iuiXoWJpPwkJPQY", "postedAt": "2023-08-28T14:36:35.442Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>Thanks for pointing this out. I didn't know there was a way to calculate the exponentially moving average (EMA) using NumPy.</p><p>Previously I was using alpha = 0.33 for weighting the current value. When that value is plugged into the formula alpha = 2 / N + 1, it means I was averaging over the past 5 years.</p><p>I've now decided to average over the past 4 years so the new alpha value is 0.4.</p>", "parentCommentId": "xeHiwmbEvGe6govuc", "user": {"username": "Stephen McAleese"}}, {"_id": "GhGdvFkzmKYWGuxjJ", "postedAt": "2023-10-02T21:39:50.358Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>I think work on near-term issues like unemployment, bias, fairness and misinformation is highly valuable and the book The Alignment Problem does a good job of describing a variety of these kinds of risks. However, since these issues are generally more visible and near-term, I expect them to be relatively less neglected than long-term risks such as existential risk. The other factor is importance or impact. I believe the possibility of existential risk greatly outweighs the importance of other possible effects of AI though this view is partially conditional on believing in longtermism and weighting the value of the long-term trajectory of humanity highly.</p><p>I do think AI ethics is really important and one kind of research I find interesting is research on what Nick Bostrom calls the value loading problem which is the question of what kind of philosophical framework future AIs should follow. This seems like a crucial problem that will need to be solved eventually. Though my guess is that most AI ethics research is more focused on nearer-term problems.</p><p>Gavin Leech wrote an EA Forum post which I recommend named <a href=\"https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large\">The academic contribution to AI safety seems large</a> where he argues that the contribution of academia to AI safety is large even with a strong discount factor because academia does a lot of research on AI safety-adjacent topics such as transparency, bias and robustness.</p><p>I have included some sections on academia in this post though I've mostly focused on EA funds because I'm more confident that they are doing work that is highly important and neglected.</p>", "parentCommentId": "ZBmEH3bpCTkx88pAS", "user": {"username": "Stephen McAleese"}}, {"_id": "MjhSdvT8dyN46GT6a", "postedAt": "2023-10-02T22:02:04.734Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>Some information not included in the original post:</p><ul><li>In April 2023, the UK government <a href=\"https://www.gov.uk/government/news/initial-100-million-for-expert-taskforce-to-help-uk-build-and-adopt-next-generation-of-safe-ai\">announced</a> \u00a3100m in initial funding for a new AI Safety Taskforce.</li><li>In June 2023, UKRI awarded \u00a331m to the University of Southhampton to create a new responsible and trustworthy AI <a href=\"https://www.ukri.org/news/54m-to-develop-secure-ai-that-can-help-solve-major-challenges/\">consortium</a> named Responsible AI UK.</li></ul>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "CmX3W3gzcJY4mvT5h", "postedAt": "2024-01-10T02:49:50.246Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>As explored in this article here (https://forum.effectivealtruism.org/posts/tuMzkt4Fx5DPgtvAK/why-solving-existential-risks-related-to-ai-might-require), I as well as others I'm sure share the author's opinion that current AI safety approaches do not work reliably. This is due to such efforts being misaligned in ways that are often invisible. For this reason, more research is necessary to come up with new approaches. The author expressed the opinion that the free market tends to prioritize progress in AI over safety because existential risk is an \"externality\", and that consequently, philanthropic funding is useful for filling the funding gap.</p>", "parentCommentId": null, "user": {"username": "Andy E Williams"}}, {"_id": "9iDp4A4w3c4cWRrxn", "postedAt": "2024-03-27T17:32:40.415Z", "postId": "XdhwXppfqrpPL2YDX", "htmlBody": "<p>It seems like that this number will increase by 50% once FLI (Foundation) fully comes online as a grantmaker (assuming they spend 10%/year of their USD 500M+ gift)</p>\n<p><a href=\"https://www.politico.com/news/2024/03/25/a-665m-crypto-war-chest-roils-ai-safety-fight-00148621\">https://www.politico.com/news/2024/03/25/a-665m-crypto-war-chest-roils-ai-safety-fight-00148621</a></p>\n", "parentCommentId": null, "user": {"username": "jackva"}}]