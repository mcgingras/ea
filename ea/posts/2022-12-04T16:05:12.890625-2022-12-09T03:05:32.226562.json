[{"_id": "isggu3woGwkpYzqwW", "title": "Presenting: 2022 Incubated Charities (Charity Entrepreneurship)", "postedAt": "2022-12-08T18:12:33.403Z", "htmlBody": "<figure class=\"image image_resized\" style=\"width:27.62%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674645770/mirroredImages/isggu3woGwkpYzqwW/wotj0rhc79bkn8vh5hc9.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/s9t371ndxi4bkgylacyi.png 100w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/scj0ana8yt5bxhw1o24z.png 180w, https://res.cloudinary.com/cea/image/upload/v1674645770/mirroredImages/isggu3woGwkpYzqwW/cwubqajsmhrhx4ivr0o3.png 260w, https://res.cloudinary.com/cea/image/upload/v1674645770/mirroredImages/isggu3woGwkpYzqwW/q7uwigvikj5ucr433l0u.png 340w, https://res.cloudinary.com/cea/image/upload/v1674645770/mirroredImages/isggu3woGwkpYzqwW/hbhyre2jmdahzryvtsoh.png 420w, https://res.cloudinary.com/cea/image/upload/v1674645770/mirroredImages/isggu3woGwkpYzqwW/qwc7qlt6rglsbzwtlrja.png 500w\"><figcaption>&nbsp;</figcaption></figure><p>We are proud to announce that&nbsp;<strong>5 new charitable organizations&nbsp;</strong>have been launched from our June-August 2022 Incubation Program. Nine high-potential individuals graduated from our two-month intensive training program. The Incubation Program has been designed to teach participants everything they need to know to launch and run an extremely effective, high-impact charity. From analyzing the cost-effectiveness of an intervention, all the way to crafting a proposal for funding, the program participants are equipped with the knowledge and confidence they need to see their chosen intervention become a reality. Eight have gone on to start new effective nonprofits focused on policy, mental health, family planning and EA meta cause areas, and one participant was hired by Charity Entrepreneurship as a Research Analyst. They will be joining our 2023 cohort.&nbsp;</p><p>Thanks to our generous&nbsp;<strong>CE Seed Network</strong> of funders, we have helped to secure&nbsp;<strong>$732,000 in funding</strong> for the organizations, and will further support them with mentorship, operational support, free co-working space in London, and access to a constantly growing entrepreneurial community of funders, advisors, interns and other charity founders.</p><p>The 2022 incubated charities are:</p><ul><li><strong>Center for Effective Aid Policy</strong>- identifying and promoting high-impact development policies and interventions.</li><li><strong>Centre for Exploratory Altruism Research (CEARCH)</strong>- conducting cause prioritization research and outreach.</li><li><strong>Maternal Health Initiative-</strong> producing transformative benefits to women\u2019s health, agency, and income through increased access to family planning.</li><li><strong>Kaya Guides</strong>- reducing depression and anxiety among youth in low-and middle-income countries.</li><li><strong>Vida Plena</strong>- building strong mental health in Latin America.</li></ul><h1><br><strong>CENTER FOR EFFECTIVE AID POLICY</strong><br><br><img src=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/qiyvg4uoc9nusmufckdf.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/o9o9qgpg7a56dlp41sqj.png 100w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/uyukcixb2dwziiupma0e.png 200w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/i7mjus56qmrzc3vfwhep.png 300w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/hqmwqmbgumk0ogbbur3n.png 400w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/ym77netyulb7gjlvxpbl.png 500w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/k9vxvblgj9pltn3d8m57.png 600w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/c4btag5zlbwprj1amzgw.png 700w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/smfs0lureouoxdtwzrqc.png 800w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/eaalelxkqdgshtxquxe5.png 900w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/wzaogjpzj4pntexjarvj.png 940w\"></h1><p><strong>Co-founders:</strong> Jacob Wood, Mathias Bonde<br><strong>Website:&nbsp;</strong><a href=\"http://aidpolicy.org/\"><u>aidpolicy.org</u></a><br><strong>Email address:&nbsp;</strong>contact@aidpolicy.org&nbsp;<br><strong>CE incubation grant:&nbsp;</strong>$170,000</p><h3><br><strong>Description of the intervention:&nbsp;</strong></h3><p>The Center for Effective Aid Policy will work on identifying and advocating for effective solutions in aid policy. This may include:</p><ul><li>Increasing international development aid</li><li>Increasing budget allocation to specific effective development programs</li><li>Introducing new effective development interventions into aid budgets</li><li>Revising processes which result in improved development aid effectiveness</li></ul><h3><br><strong>Background of the intervention:</strong></h3><p><a href=\"https://www.oecd.org/dac/financing-sustainable-development/development-finance-standards/official-development-assistance.htm\"><u>$179 billion</u></a> was spent on development aid in 2021 - that is roughly 240x the amount of<a href=\"https://docs.google.com/spreadsheets/d/1z065ab9PPMu9i5KiQ4yLyQJPFQCfEzHSgtHulPiZeBo/edit#gid=1061916285\"><u> money that GiveWell has moved</u></a> since 2009. While well-intentioned, there is a broad consensus among experts, think tanks, and implementing partners alike that&nbsp;<a href=\"https://www.givewell.org/international/technical/criteria/impact/failure-stories#Poorly_executed_programs\"><u>aid</u>&nbsp;<u>effectiveness can be vastly improved</u></a>.</p><p>The Center for Effective Aid Policy believes tractable interventions exist in the development aid space that will result in improved aid spending and better outcomes for its recipients. You can read more in their recent EA Forum&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/agKFRa5qBApnfLrSh/why-development-aid-is-a-really-exciting-field\"><u>post</u></a>.</p><h3><strong>Near-term plans:</strong></h3><p>In 2022-2023, The Center for Effective Aid Policy will identify policy windows and formulate impactful and practical-to-implement policies, which they will advocate to governments and NGOs. They conservatively estimate their chances of advocacy success at $5.62 per DALY - more than an order of magnitude higher than multiple GiveWell-recommended charities.&nbsp;<br>&nbsp;</p><h1><strong>CENTRE FOR EXPLORATORY ALTRUISM RESEARCH (CEARCH)</strong></h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/df66ulhigomj7nuqgapl.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/gkvlgq6fb00xdbwlsk4m.png 100w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/ggsd4oincodncwanpudu.png 200w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/pr5djjxxk0dmbhtgpy5m.png 300w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/myg613nbc2shc1ekhtfj.png 400w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/glzhr8znnkz0fb9v2db9.png 500w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/akhzglto22eynsyefzxp.png 600w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/rvqeubacj740jjstki3l.png 700w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/tqrqdbse2k5tabrgirdy.png 800w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/ppw9kdmvp8rfxdjbvjxa.png 900w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/gvqfik7pmdd1uqrivxmn.png 940w\"><br><br><strong>Founder:</strong>&nbsp; Joel Tan<br><strong>Website:&nbsp;</strong><a href=\"https://exploratory-altruism.org/\"><u>exploratory-altruism.org</u></a><br><strong>Contact:&nbsp;</strong><a href=\"https://exploratory-altruism.org/contact/\"><u>https://exploratory-altruism.org/contact/</u></a><br><strong>CE incubation grant:&nbsp;</strong>$100,000</p><h3><strong>Description of the intervention:&nbsp;</strong></h3><p>CEARCH conducts cause prioritization research and outreach - identifying the most important problems in the world and directing resources towards solving them, so as to maximize global welfare.</p><h3><strong>Background of the intervention:</strong></h3><p>There are many potential cause areas (e.g., improving global health, reducing pandemic risk, or addressing long-term population decline), but we may not have yet identified what the most impactful causes are. This is the result of a lack of systematic cause prioritization research.</p><ul><li>Existing cause research is not always fully systematic; for lack of time, it does not always involve (a) searching for as many causes as possible (e.g., more than a thousand) and then (b) researching and evaluating all of them to narrow down to the top causes.</li><li>The search space for causes is vast, and existing EA research organizations agree that&nbsp;<a href=\"https://3394c0c6-1f1a-4f86-a2db-df07ca1e24b2.filesusr.com/ugd/9475db_a215cf49667a40f7aca07db9c10bd246.pdf\"><u>there is room</u></a> for a new organization.</li></ul><h3><strong>Near-term plans:</strong></h3><p>By the end of year one, CEARCH aims to have (a) identified 1000 causes, (b) narrowed down on a cause that is at least 10x more cost-effective than top GiveWell charities, and (c) secured informal agreements from three major EA organizations to use their future research. CEARCH plans to achieve this by:</p><ul><li>Carrying out a comprehensive search process (e.g., using existing lists, conducting consultations and surveys, performing outcome tracing) for candidate causes.</li><li>Conducting research to narrow down on causes. Each cause is subject to an initial shallow research round of one week of desktop research. If the cause's estimated cost-effectiveness is at least one magnitude greater than a GiveWell top charity, it passes to the intermediate research round of two weeks of desktop research and expert interviews. Then, if the cause's estimated cost-effectiveness is still at least one magnitude greater than a GiveWell top charity, it passes to the deep research round of four weeks of desktop research, expert interviews, and commissioning of surveys and quantitative modeling.</li><li>Outreach to important EA organizations, members of the community on the EA forums and at EA conferences, and to non-EA organizations, particularly in the political sphere.<br>&nbsp;</li></ul><h1><strong>MATERNAL HEALTH INITIATIVE (MHI)</strong><br><br><img src=\"https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/myte4zzlnsggcnkcwbaf.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/dz7ps9xhwnewdi76htkz.png 100w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/dbatakq1a1gm7wjrioe9.png 200w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/x4q32iegid8hiwj1jh6n.png 300w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/abfvq4vqd4ih8p5ovw3c.png 400w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/qzsqupdyoy3w5vna2yaq.png 500w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/a8kan0jbodszooedaeaj.png 600w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/sb8xnjyxz8ivdyb2vjes.png 700w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/ucnce1w4034woollwtlx.png 800w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/r6iwc2yuvp9zzcbo139l.png 900w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/teehuzagasmbe7bqhpmg.png 940w\"></h1><p><strong>Co-founders:</strong>&nbsp; Ben Williamson, Sarah Hough<br><strong>Website:&nbsp;</strong><a href=\"https://www.maternalhealthinitiative.org/\"><u>maternalhealthinitiative.org</u></a><br><strong>Email:&nbsp;</strong><a href=\"mailto:info@maternalhealthinitiative.org\"><u>info@maternalhealthinitiative.org</u></a>&nbsp;&nbsp;<br><strong>CE incubation grant:&nbsp;</strong>$220,000</p><h3><strong>Description of the intervention:&nbsp;</strong></h3><p>Maternal Health Initiative (MHI) aims to reduce rates of maternal mortality and provide women with fundamental increases in their agency by expanding access to family planning.</p><h3><strong>Background of the intervention:</strong></h3><p>Lack of contraceptive access is a significant issue with a broad range of negative consequences.&nbsp;<a href=\"https://www.guttmacher.org/fact-sheet/investing-sexual-and-reproductive-health-low-and-middle-income-countries#\"><u>218 million women</u></a> in low- and middle-income countries (LMICs) do not have access to modern contraceptives, resulting in&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/25207494/\"><u>over 70 million unintended pregnancies</u></a> each year. Beyond the clear consequences unintended pregnancies have on people\u2019s autonomy, around<a href=\"https://www.who.int/news-room/fact-sheets/detail/maternal-mortality\">&nbsp;<u>300,000 women and girls</u></a> die of pregnancy-related complications each year. More than two-thirds of these deaths occur in sub-Saharan Africa, where the Maternal Health Initiative will work.</p><p>Increased contraceptive access allows women and their families greater agency over a significant life decision, with clear benefits for women\u2019s&nbsp;<a href=\"https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(06)69480-4/fulltext\"><u>health</u></a>,&nbsp;<a href=\"http://www.econ.yale.edu/~pschultz/TPS_10_30_QJE.pdf\"><u>income</u></a>, and&nbsp;<a href=\"https://academic.oup.com/ej/article-abstract/120/545/709/5089691?redirectedFrom=fulltext\"><u>educational outcomes</u></a>. Furthermore, family planning interventions have been recognised as highly cost-effective, with the Copenhagen Consensus estimating that every dollar invested in family planning&nbsp;<a href=\"https://www.copenhagenconsensus.com/publication/post-2015-consensus-population-and-demography-assessment-kohler-behrman\"><u>leads to $120</u></a> in social, economic, and environmental benefits.</p><h3><strong>Near-term plans:</strong></h3><p>In October, MHI conducted a scoping visit to Ghana. There they met with 18 different stakeholders, including local NGOs, potential partner organizations, and healthcare providers. MHI is currently exploring the potential of a pilot intervention improving the quality and reach of postpartum (post-birth) family planning counseling. The organizations is using a combination of research into the published literature, cost-effectiveness analyses (CEAs), and conversations with stakeholders and experts to ensure that the chosen intervention is cost-effective, impactful and scalable. This includes conducting a geographic assessment to choose the location of the pilot, incorporating factors such as maternal mortality rates, unmet need for contraception, and the extent of work by existing actors in the field.&nbsp;</p><p>MHI intends to run a short proof of concept in early 2023 to test their program before running a full pilot in mid 2023. Alongside this, they will continue to develop engagement with other actors and stakeholders in the family planning space, building on the contacts established through their scoping trip to Ghana.&nbsp;</p><h1><br><strong>KAYA GUIDES</strong></h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/c52ywylbfslyj3purcar.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/dnd6ua6ljaex4djpou0p.png 100w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/h35pijjc6s5d3ruplq7r.png 200w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/kduge2d1emtm8ifahtg6.png 300w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/bvdnygcnqcfaprglabvt.png 400w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/ooqhcu5dnsy6bjqzdiji.png 500w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/pzlskruiyaqeqwteifjg.png 600w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/rrtatfvsrltlwwpay92k.png 700w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/kgn7xfti8wph6xj8pun2.png 800w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/rpb8b9umiwppdb5bnl86.png 900w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/y61im8uuy8b8kn9qb1ro.png 940w\"><br><br><strong>Co-founders:</strong>&nbsp; Rachel Abbott, Shen Javier (Incubation Program 2021)<br><strong>Website:&nbsp;</strong><a href=\"https://www.kayaguides.com/\"><u>kayaguides.com&nbsp;&nbsp;</u></a><br><strong>Email:</strong>&nbsp; <a href=\"mailto:hello@kayguides.com\"><u>hello@kayguides.com</u></a>&nbsp;<br><strong>CE incubation grant:&nbsp;</strong>$110,000</p><h3><strong>Description of the intervention:</strong></h3><p>Kaya Guides aims to reduce depression and anxiety among youth in low-and middle-income countries through a guided self-help program delivered by WhatsApp. Participating youth will complete a digital learning program in which they complete self-help material independently and have brief weekly calls with a lay counselor over the course of 5-10 weeks. A meta-analysis of 26 RCTs demonstrates that this format has comparable effects to face-to-face therapy in reducing depression and anxiety- even when counselors have no health background and calls are as short as 15 minutes per week.</p><h3><strong>Background of the intervention:&nbsp;</strong></h3><p>Nearly 1 billion people today live with a mental disorder. Despite the severity of the problem, mental health is extraordinarily neglected. Governments spend just 2% of their healthcare budgets on mental health. In low-and middle-income countries, 75% of people who need treatment do not get it.</p><p>Guided self-help presents a highly evidence-based way to improve mental health. Over 50 RCTs show that self-help has medium to large effects on depression, anxiety, and subjective well-being. This strong evidence base is paired with high cost-effectiveness. The team estimates that at scale, Kaya Guides could be 45x more cost-effective at improving subjective well-being than direct cash transfers in LMICs. They estimate that the marginal cost of helping an additional person will be as low as $3.93.</p><h3><strong>Near-term plans:</strong></h3><p>To date, the Kaya Guides team has completed a thorough geographic assessment, selected India as their focus country, honed the design of their intervention, built out a team of advisors, and formed strategic relationships with actors in the mental health sector in LMICs. Co-founders are currently participating in a mental health leadership course run by India\u2019s leading mental health organization. Through the course, co-founders are being trained in research-backed methodologies for designing and scaling mental health interventions in LMICs, particularly in India\u2019s context.</p><p>In early 2023, co-founders will conduct a scoping trip to two cities in India to meet with mental health nonprofits, therapists, youth organizations, and other stakeholders. They will conduct focus groups discussions and user interviews to gain an in-depth understanding of the realities of Indian youth. Based on this research, the team will work with local experts to co-design evidence-based self-help content, including audio, video and written material. They will adapt the program to WhatsApp, conduct beta testing to confirm user acceptability and comprehension, and set up a clinical review. Kaya Guides will run a full pilot to test the program\u2019s efficacy by the end of the year. They will use pilot results to adapt the program and prepare for full program rollout in 2024.</p><h1><br><strong>VIDA PLENA</strong></h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/zlv7txy868vtza50kowz.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/jsqueovsiw93v4ecvdsk.png 100w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/lv7bjb92o2dgb0yvpd5z.png 200w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/zh23yebrbkqhf46hubc8.png 300w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/rglrszmgxoooktoace1u.png 400w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/tatgqfw1x42dcycewnrc.png 500w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/uoumjltzgx48f7ryrtqh.png 600w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/b2trrnccaytvvyhbszus.png 700w, https://res.cloudinary.com/cea/image/upload/v1674645772/mirroredImages/isggu3woGwkpYzqwW/achlmkhfsmflbvu6kjdx.png 800w, https://res.cloudinary.com/cea/image/upload/v1674645773/mirroredImages/isggu3woGwkpYzqwW/eg4auniewzwrgzycigk7.png 900w, https://res.cloudinary.com/cea/image/upload/v1674645771/mirroredImages/isggu3woGwkpYzqwW/ii6yytaeeefxglv77kp1.png 940w\"><br><br><strong>Co-founders:</strong>&nbsp; Joy Bittner, Anita Kaslin<br><strong>Website:&nbsp;</strong>&nbsp;<a href=\"http://vidaplena.global\"><u>vidaplena.global&nbsp;</u></a><br><strong>Email:</strong>&nbsp; joy@vidaplena.global / <a href=\"mailto:anita@vidaplena.global\">anita@vidaplena.global</a><br><strong>CE incubation grant:&nbsp;</strong>$132,000</p><h3><strong>Description of the intervention:</strong></h3><p>Vida Plena (meaning \u2018a flourishing life\u2019 in Spanish) provides evidence-based depression treatment to Ecuadorian communities at scale.&nbsp;&nbsp;</p><p>Vida Plena is the first EA-aligned mental health organization based in and operating within Latin America.&nbsp;</p><p>Vida Plena empowers local people to deliver a cost-effective model of counseling. Community members are trained to treat depression through Group Interpersonal Therapy (g-IPT).</p><h3><strong>Background of the intervention:</strong></h3><p>The WHO estimates that 5% of people in Latin America have depression; however, a lack of prioritization means that more than three out of four people in Latin America go untreated. Ecuador, in particular, has some of the highest rates of depression in the region: causing 8.3% of the total years lived with disability (YLD).</p><p>Vida Plena addresses the lack of treatment for depression by empowering local people to deliver a cost-effective model of counseling. Community members are trained to treat depression through Group Interpersonal Therapy (g-IPT), which is the World Health Organization\u2019s top recommended intervention for depression in low-income settings.</p><p><strong>In terms of improving subjective well-being, we estimate Vida Plena to be at least at least 8x more cost-effective than GiveDirectly.</strong></p><h3><br><strong>Near-term plans:&nbsp;</strong></h3><p>Starting in Ecuador, Vida Plena aims to replicate the successes of Strong Minds (which also uses Group Interpersonal Therapy) throughout Latin America. Strong Minds is recommended by Founders Pledge and is also the Happier Lives Institute\u2019s top charity recommendation.&nbsp;</p><p>Vida Plena launched in the fall of 2022, with a pilot program training 11 community members as support group facilitators, and is collecting data to measure impact.&nbsp;&nbsp;</p><p>In 2023, they are planning to expand throughout Ecuador, effectively treating depression in over 1,600 people.&nbsp;</p><p>Depending on funding,Vida Plena is also planning on conducting a RCT in collaboration with Columbia University to evaluate the effectiveness of Group Interpersonal Therapy as applied in Latin America. The results of these studies will be published in peer-reviewed, academic journals to build the publicly available knowledge base regarding effective mental health treatments in low-and middle-income countries.&nbsp;</p><h2>JOIN THE INCUBATION PROGRAM IN 2023</h2><p>We are building upon our past successes and scaling up to&nbsp;<i>two</i> incubation programs in 2023, with the hope of launching even more high-impact charities. Applications are now closed for the February/March program but if you are interested in attending our June/ August&nbsp;<a href=\"https://www.charityentrepreneurship.com/incubation-program.html\">Incubation Program</a> in 2023, you can sign up&nbsp;<a href=\"https://www.charityentrepreneurship.com/signup\"><u>here&nbsp;</u></a>to be notified when we start accepting applications.</p><p><br>&nbsp;</p>", "user": {"username": "KarolinaSarek"}}, {"_id": "CeNYbaPgAHbGm4oKQ", "title": "I've started publishing the novel I wrote to promote EA ", "postedAt": "2022-12-08T17:28:59.120Z", "htmlBody": "<p>Currently I'm just uploading it to Royal Road, but in the next week I'll also post it to the spacebattles forum, and around the start of January I'll publish a copy to Amazon.&nbsp;</p><p>So, if you find this project interesting, and use Royal Road, follow it and maybe leave a comment there. Also, if there is a site you use to read and discover fiction that would be appropriate for me to publish this to, tell me, and I'd love to put my novel there.</p><p>If anyone can help in some way to tell people who they think might like a Isekai/portal fantasy novel about an effective altruist trying to do as much good as he can after he died and woke up with super magical powers in a fantasy world, please do so.&nbsp;</p><p>Link: <a href=\"https://www.royalroad.com/fiction/61682/the-split-summon/chapter/1058375/chapter-one-arrival\">https://www.royalroad.com/fiction/61682/the-split-summon/chapter/1058375/chapter-one-arrival</a><br>&nbsp;</p><p>Thanks!</p>", "user": {"username": "timunderwood"}}, {"_id": "JnfAtEQ3PdkhNafzh", "title": "Pandemic prevention as fire-fighting by Richard Williamson (Alvea.bio) for Works in Progress", "postedAt": "2022-12-08T17:30:19.666Z", "htmlBody": "<p>Richard Williamson of <a href=\"https://www.alvea.bio/\">Alvea.bio</a> <a href=\"https://www.worksinprogress.co/issue/pandemic-prevention-as-fire-fighting/\">explains</a> how we can look to the end of great urban fires to understand how we can end pandemics.&nbsp;<br><br>He identifies 4 analogous projects:</p><ul><li><strong>Active protection:</strong> Just as we have different extinguishers for different types of fires, we need to be developing vaccines and other pharmaceuticals for each family of viruses</li><li><strong>Detection: </strong>Just as we have smoke detectors in every home, we need to detect pandemic early.&nbsp;</li><li><strong>Passive protection: </strong>Just as we prevent the spread of fires through sprinkler systems and construction regulations, we need more advance filters, better PPE, and eventually UV lighting, to prevent the spread of disease.&nbsp;</li><li><strong>Prevention: </strong>Just as we teach humans to avoid starting fires and create rules and laws to discourage risky behavior, we need to create rules to stop zoonotic transmission, lab leaks, and gain of function research.&nbsp;</li></ul><p>Even if you're familiar with these proposals, Richard's piece is able to tie them together in a brilliant way and actually make the end of pandemic seem possible. It's also a great introduction to long term pandemic prevention, so it may be the perfect resource to share with a friend who is new to the space.&nbsp;</p>", "user": {"username": "Nick Whitaker"}}, {"_id": "ve3tmwDxw98hoKraX", "title": "Advancing antivenom by Mathias Kirk Bonde for Works in Progress", "postedAt": "2022-12-08T15:55:51.659Z", "htmlBody": "<p>A few months ago, Mathias wrote an excellent EA forums post: <a href=\"https://forum.effectivealtruism.org/posts/WyqGircJgCBG9ivNL/snakebites-kill-100-000-people-every-year-here-s-what-you\">Snakebites kill 100,000 people every year, here's what you should know</a>.</p><p>He developed the post into an article for Works in Progress: <a href=\"https://www.worksinprogress.co/issue/advancing-antivenom/\">Advancing Antivenom</a>. It was published today.&nbsp;</p><p>It's a great piece, and is a good example of how EA Forums posts can be developed into a popular piece to share the ideas and thinking contained within them to a wider audience.&nbsp;</p>", "user": {"username": "Nick Whitaker"}}, {"_id": "wPHpdwfu3toRDf6hM", "title": "Main paths to impact in EU AI Policy", "postedAt": "2022-12-08T16:17:25.741Z", "htmlBody": "<h1>Epistemic status</h1><ul><li>This is a tentative overview of the current main paths to impact in EU AI Policy. There is significant uncertainty regarding the relative impact of the different paths below</li><li>The paths mentioned below were cross-checked with four experts working in the field, but the list is probably not exhaustive</li><li>Additionally,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fbG6wWZhJ3jt3xHxS/should-you-work-in-the-european-union-to-do-agi-governance\"><u>this post</u></a> may also be of interest to those interested in working in EU AI Policy. In particular, consider the arguments against impact&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/suyb4vC75Wo9EKgyu/argument-against-impact-eu-is-not-an-ai-superpower\"><u>here</u></a></li><li>This article doesn\u2019t compare the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/e7NKpwD5z2Mnc7y7G/working-in-us-policy-as-a-foreign-national-immigration\"><u>potential value of US AI policy careers with those in the EU for people with EU-citizenship</u></a>. A comparison between the two options is beyond the scope of this post</li></ul><h1><br>Summary</h1><p>People seeking to have a positive impact on the direction of AI policy in the EU may consider the following paths to impact:</p><ol><li>Working on (enforcement of) the AI Act, related AI technical standards and adjacent regulation<ol><li>Working on the current AI Act draft (possibility to have impact immediately)</li><li>Working on technical standards and auditing services of the AI Act (possibility to have impact immediately)</li><li>Making sure the EU AI Act is enforced effectively (possibility to have impact now and&nbsp; &gt;1/2 years from now)</li><li>Working on a revision of the AI Act (possibility to have impact &gt;5 years from now), or on (potential) new AI-related regulation (e.g. possibility to have impact now through the AI Liability Directive)</li></ol></li><li>Working on export controls and using the EU\u2019s soft power &nbsp;(possibility to have immediate + longer term impact)&nbsp;&nbsp;</li><li>Using career capital gained from a career in EU AI Policy to work on different policy topics or in the private sector</li></ol><p>While the majority of impact for some paths above is expected to be realised in the medium/long term, building up career capital is probably a prerequisite for making impact in these paths later on. It is advisable to create your own personal&nbsp;<a href=\"https://lynettebye.com/blog/2020/11/16/theory-of-change-as-a-hypothesis\"><u>\u201ctheory of change\u201d</u></a> for working in the field. The list of paths to impact below is not exhaustive and individuals should do their own research into different paths and speak to multiple experts.</p><h1><br>Paths to impact</h1><h2>Working on (enforcement of) the AI Act, related AI technical standards and adjacent regulation</h2><p>Since the EU generally lacks cutting edge developers of AI systems, the largest expected impact from the EU AI Act is expected to follow from&nbsp;<a href=\"https://www.governance.ai/research-paper/brussels-effect-ai\"><u>a Brussels effect</u></a> of sorts. For the AI act to have a positive effect on the likelihood of safe, advanced AI systems being developed within organisations outside of the EU, the following assumptions need to be true:</p><ul><li>Advanced AI systems are (also) being developed within private AI labs that (plan to) export products to EU citizens.&nbsp;</li><li>The AI-developing activities within private AI labs need to be influenced by the EU AI Act through a Brussels Effect of some sort (de facto or de jure). This depends on whether advanced AI is developed within a product development process in which AI-companies take EU AI Act regulation into account.&nbsp;</li><li>Requirements on these companies either have a (1) slowing effect on advanced AI development, buying more time for technical safety research or better regulation within the countries where AI is developed (2) direct effect, e.g. through risk management requirements that increase the odds of safe AI development&nbsp;</li></ul><h3><br>Working on the final draft of the AI Act&nbsp;</h3><p><strong>Current status of AI Act</strong></p><p>The&nbsp;<a href=\"https://artificialintelligenceact.eu/the-act/\"><u>AI Act</u></a> is a proposed European law on artificial intelligence (AI) \u2013 the first law on AI by a major regulator anywhere. On April 21, 2021 the Commission published a proposal to regulate artificial intelligence in the European Union. The proposal of the EU AI Act will become law once both the Council (representing the 27 EU Member States) and the European Parliament agree on a common version of the text.</p><p>The Czech Presidency has presented its full compromise text to the Council, in which it attempts to settle previously contentious issues, namely how to define an AI system, classification of AI systems as high-risk, governance and enforcement, as well as national security exclusion (<a href=\"https://artificialintelligenceact.eu/developments/\"><u>see timeline</u></a>). This&nbsp;<a href=\"https://www.euractiv.com/section/digital/news/eu-countries-adopt-a-common-position-on-artificial-intelligence-rulebook/\"><u>was approved</u></a> by ministers from EU Member States on December 6. In the European Parliament, negotiations on a joint position are still ongoing. Recently the European Parliament\u2019s rapporteurs circulated a new batch of&nbsp;<a href=\"https://www.euractiv.com/section/digital/news/leading-meps-tackle-enforcement-in-ai-regulation/?utm_source=substack&amp;utm_medium=email\"><u>compromise amendments</u></a> redesigning the enforcement structure of the AI Act. An agreement in the European Parliament&nbsp;<a href=\"https://www.ceps.eu/ceps-publications/the-ai-act-and-emerging-eu-digital-acquis/\"><u>should come</u></a> by mid-2023, but a final text agreed between the EU institutions is not likely before 2024.</p><p><strong>Paths to impact AI Act</strong></p><p>Depending on the role, people could still have some impact improving the current draft of the EU AI Act, which could have an effect internationally through the&nbsp;<a href=\"https://uploads-ssl.webflow.com/614b70a71b9f71c9c240c7a7/630534b77182a3513398500f_Brussels_Effect_GovAI.pdf\"><u>Brussels Effect</u></a>. The following organisations could still have positive impact on the final draft:</p><ol><li>Think tanks and NGOs advising the Council and Parliament</li><li>Member states can improve the quality in trilogue negotiations through the Council. Sweden could be especially influential given their upcoming presidency of the Council in the first half of 2023.</li><li>Assistants to relevant MEPs (although it is hard to acquire such a role in the relevant timeframe without pre-existing career capital and a degree or luck)</li></ol><h3><br>Working on the technical standards of the AI Act&nbsp;</h3><p><strong>Current status of standard setting of the AI Act</strong></p><p>The AI Act\u2019s high-risk obligations will be operationalised by technical standards bodies. These bodies need to be filled by technical experts within national standard-setting bodies, e.g.&nbsp;<a href=\"https://www.vde.com/en/fnn/topics/standardization\"><u>VDE</u></a> in Germany, through&nbsp;<a href=\"https://www.cencenelec.eu/\"><u>CEN / CENELEC</u></a> /&nbsp;<a href=\"https://www.etsi.org/\"><u>ETSI</u></a> in the&nbsp;<a href=\"https://www.cencenelec.eu/news-and-events/news/2021/briefnews/2021-03-03-new-joint-tc-on-artificial-intelligence/\"><u>JTC21</u></a>. Through request of the European Commission this process runs parallel with that of the AI Act and is&nbsp;<a href=\"https://www.euractiv.com/section/digital/news/leading-meps-tackle-enforcement-in-ai-regulation/?utm_source=substack&amp;utm_medium=email\"><u>currently ongoing</u></a>.<strong>&nbsp;</strong>Standards play such a critical role in bringing down the compliance costs that they have been defined as the \u2018real rulemaking\u2019 in an&nbsp;<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3896852\"><u>influential paper</u></a> on the EU\u2019s AI rulebook.</p><p><strong>Paths to impact in standard setting</strong></p><p>Technical standards can have an effect internationally through the&nbsp;<a href=\"https://uploads-ssl.webflow.com/614b70a71b9f71c9c240c7a7/630534b77182a3513398500f_Brussels_Effect_GovAI.pdf\"><u>Brussels Effect</u></a>. The following organisations and their personnel impact technical standards:</p><ol><li>In addition to private sector organisations, NGOs and think tanks are invited by member state standard-setting bodies to provide their input</li><li>National standard-setting bodies usually appoint experts to participate in the national committees during the negotiations of technical standards. With sufficient career capital people can help their national committee</li></ol><p>People with more technical (governance) expertise could make sure the technical standards actually make AI development safer and more ethical, preventing them from becoming a tick-box exercise. There seems to be the most room for making an impact through measures on:</p><ul><li>Risk Management Systems:<strong>&nbsp;</strong>The AI Act imposes requirements regarding internal company processes, such as requiring there be adequate risk management systems and post-market monitoring.&nbsp;</li><li>Documentation: The AI Act introduces requirements on documentation of companies\u2019 AI systems, to be shared with regulators and users. Similar to \u201cmodel cards,\u201d an AI system should be accompanied by information about its intended purpose, accuracy, performance across different groups and contexts, likely failure modes</li><li>Requirements on accuracy, robustness, and cybersecurity of AI systems</li></ul><h3><br>Working on EU AI Act enforcement</h3><p><strong>Current status of enforcement</strong></p><p>Making sure the EU AI Act is enforced is a prerequisite for impact and the Brussels Effect coming into existence. The Act will be overseen by a European AI Board (or, possibly, an AI office as requested by some policymakers). However, the Board\u2019s role is advisory, and enforcement will be primarily the responsibility of national market surveillance authorities. The European AI Board collects and shares best practices, takes a position on emerging issues for the implementation and enforcement of the regulation and will also monitor the latest trends in AI.</p><p><a href=\"https://www.euractiv.com/section/digital/news/leading-meps-tackle-enforcement-in-ai-regulation/?utm_source=substack&amp;utm_medium=email\"><u>The most recent compromise text</u></a> by EP rapporteurs gives the national supervisory authorities the power to conduct unannounced on-site and remote inspections of high-risk AI, acquire samples related to high-risk systems to reverse-engineer them and acquire evidence to identify non-compliance.&nbsp;</p><p><strong>Paths to impact in enforcement</strong></p><p>The following paths to impact on enforcement are identified:</p><ol><li>NGOs, think tanks and members of Parliament could lobby for better (use of) budgets (possibility to have impact now). The GDPR&nbsp;<a href=\"https://www.csis.org/blogs/strategic-technologies-blog/3-years-later-analysis-gdpr-enforcement\"><u>showed</u></a> that Member State surveillance authorities often lack the resources or don\u2019t use the resources wisely in order to prevent back-logs in processes.&nbsp;</li><li>Working in enforcement (possibility to have impact in a few years). In order to work on enforcement in a few years, it is probably good to first build up experience:<ol><li>Individuals could already join existing national and European supervisory authorities, e.g. on&nbsp;<a href=\"https://ec.europa.eu/newsroom/repository/document/2022-46/2022_Job_opportunities_within_the_DSA_Enforcement_Team_TaK2bXp46J9EXqQHPNAgUzLmI_91635.pdf\"><u>enforcement of the Digital Services Act</u></a> with the Commission. As the Digital Services Act is enacted earlier than the AI Act, it might be the case that the enforcement team for this digital matter would also work on AI Act enforcement. Individuals could also use this experience to change ships to the AI Act once the AI Act has come into existence. This could be either on the level of the European Artificial Intelligence Board (EAIB), chaired by the Commission or on the level of the national supervisory authorities.&nbsp;</li><li>Working on audits for cutting-edge AI systems now, which can be used for third party conformity assessments / enforcement when the regulation comes into force&nbsp;</li></ol></li></ol><h3><br>Working on adjacent AI regulation and a revision of the EU AI Act</h3><p>Over the coming years more regulation on AI and adjacent technologies is expected. The European Commission recently released a proposal for an AI Liability Directive to change the legal landscape for companies developing and implementing AI in EU Member States. This would require Member States to implement rules that would significantly lower evidentiary hurdles for victims injured by AI-related products or services to bring civil liability claims. In addition, AI-related R&amp;D regulation is expected at one point since R&amp;D is exempted from the EU AI Act.&nbsp;</p><p>There will be work at the member state level for the technical bodies which will work on EU AI Act secondary legislation. There is a good chance some provisions of the AI Act will be decided by an 'implementing act'.&nbsp;</p><p>Finally, most EU laws undergo a revision after several years of the law being created<strong>.&nbsp;</strong>The AI Act will probably be adopted into 2024 and will only come into force around 2026 (to give organisations the possibility to implement the requirements. Citing article 84 of the current EC proposal:&nbsp;<i>\"By [three years after the date of application of this Regulation... and every four years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council. The reports shall be made public.&nbsp;</i>&nbsp;There is path dependency, so the structure of the law is very unlikely to change after this, but working on this evaluation and changes seem a viable path to impact in a few years.&nbsp;</p><h2><br>Working on export controls and using the EU\u2019s soft power&nbsp;</h2><p><strong>Current status export controls and the EU\u2019s soft power&nbsp;</strong></p><p>The entire high-end semiconductor supply chain relies on Netherlands/EU-based ASML, because their EUV lithography machines are currently the only ones in the world that enable the creation of the most advanced chips. ASML continues to innovate and it will be hard for non-EU based organisations to catch up. In July the&nbsp;<a href=\"https://www.fierceelectronics.com/sensors/us-wants-dutch-ban-asml-selling-chip-gear-china-0\"><u>US government pressed</u></a> the Dutch government into banning exports from the latest generation of ASML EUV machines.&nbsp; In&nbsp;<a href=\"https://www.nrc.nl/nieuws/2022/11/18/schreinemacher-chinese-exportbeperking-voor-asml-alleen-op-eigen-voorwaarden-a4148738\"><u>a recent interview</u></a> the Dutch Minister of Foreign Trade already mentioned that the Dutch government will only engage in stricter export controls on its own terms.&nbsp;</p><p>On the other side there is international collaboration, mainly between the US and the EU via the&nbsp;<a href=\"https://ec.europa.eu/commission/presscorner/detail/en/IP_21_2990)\"><u>TTC (trade &amp; technology council</u></a>). The goal of the TTC is to harmonise AI regulation between the EU and US. Working on the EU-side would give people the opportunity to incorporate important governance measures in the US legal system. The EU recently&nbsp;<a href=\"https://digital-strategy.ec.europa.eu/en/news/eu-opens-new-office-san-francisco-reinforce-its-digital-diplomacy\"><u>opened an office in SF</u></a> to reinforce its digital diplomacy. The European Commission and US Administration leaders will gather in Washington on December 5 for the latest EU-US TTC summit.</p><p>Finally, there are diplomatic discussions on AI governance ongoing at the OECD and other multilateral organisations. This is probably the space where other measures outside of the consumer product space could be proposed (e.g. on regulating autonomous weapons or on compute governance)</p><ul><li><a href=\"https://oecd.ai/en/\"><u>OECD.AI</u></a> is a platform to share and shape trustworthy AI</li><li>The UN starts to think more about future generations, e.g. through their&nbsp;<a href=\"https://www.un.org/pga/76/2022/09/12/general-assembly-declaration-on-future-generations-pga-letter/\"><u>Declaration for Future Generations</u></a> (part of&nbsp;<a href=\"https://www.un.org/en/common-agenda\"><u>Our Common Agenda</u></a>). Looking at the track record, e.g. the UN Sustainability goals, it is expected that Our Common Agenda will have an impact on the way jurisdictions deal with topics regarding technology development.&nbsp;</li></ul><p><strong>Paths to impact in export controls and the EU\u2019s soft power&nbsp;</strong></p><p>As mentioned above the US government plays an important role in pressing the Dutch government into stricter export controls. Therefore the following paths to impact regarding export controls (and other forms of compute governance) are observed:</p><ul><li>Think tank research into the desirability of different forms of export controls from EUV machines. People could also work on researching other forms of compute governance.</li><li>Working for the Dutch Ministry of Foreign Trade working on export controls. It will probably be hard to have impact without career capital and luck is required to end up in the right position</li></ul><p>Regarding making impact through international organisations and treaties (see also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DK7N5YofbM2cfPi8h/european-union-ai-development-and-governance-partnerships#comments\"><u>this post</u></a>):</p><ul><li>TTC: People need to have some luck to be in the right place in the EU team to have an impact here, since there is no \u201cdedicated TTC organisation\u201d. Seems like a relatively hard route for impact according to experts</li><li>OECD.AI: Sometimes they hire permanent staff, but there are also networks of experts more senior people can join</li><li>The UN is a platform for international coordination outside of EU-US relations and could be seen as a way to keep China in the loop on topics of (international) tech governance. It is possible to positively impact the UN in multiple ways:&nbsp;<ul><li>Directly, from certain positions within the UN</li><li>Through member states\u2019 permanent representations or as EU Ambassador</li><li>From within think tanks that play a major role in advising the UN</li></ul></li><li>There are also specific specialised think tanks on China-EU relations, sometimes touching upon the topic of the semiconductor industry</li></ul><h2><br>Using career capital from EU AI Policy to work on different policy topics or in the private sector&nbsp;</h2><p>Career capital within (tech) policy is highly transferable. Having experience in regulation of one specific technology will probably result in some career capital to work in regulation of other, risky technologies (e.g. biosecurity, nanotechnology or a completely \u201cnew\u201d risky technology that\u2019s currently not on our radar). Even when people want to switch to work in policy outside of the tech space (e.g. animal welfare or foreign aid), tech policy career capital could still prove to be valuable. This makes starting a career in tech policy somewhat robust against scenarios in which regulation of other technologies becomes more pressing.&nbsp;</p><p>Some impact-focussing individuals switched from government regulation to private sector organisations to work on self-regulation. It is important to contemplate on the question&nbsp;<a href=\"https://80000hours.org/articles/ai-capabilities/\"><u>if it\u2019s good to work&nbsp;</u></a>for an organisation that also accelerates AI capabilities.</p>", "user": {"username": "JOMG_Monnet"}}, {"_id": "QB6JJdYemL2kQPhLM", "title": "Monitoring & Evaluation Specialists \u2013 a new career path profile from Probably Good", "postedAt": "2022-12-08T12:43:09.983Z", "htmlBody": "<p>Probably Good is excited to share a new path profile for careers in monitoring and evaluation. Below, we\u2019ve included a few excerpts from the&nbsp;<a href=\"https://www.probablygood.org/profile-monitoring-and-evaluation-specialist\"><u>full profile</u></a>.&nbsp;</p><p>&nbsp;</p><p>Monitoring and evaluation (M&amp;E) specialists collect, track, and analyze data to assess the value and impact of different programs and interventions, as well as translate these assessments into actionable insights and strategies to increase the impact of an organization.&nbsp;</p><p>M&amp;E specialist careers might be a promising option for some people. If you\u2019re an exceptional fit, especially if you\u2019re based in a low- or middle-income country where there\u2019s lots of scope for implementing<a href=\"https://www.probablygood.org/global-health-and-development\">&nbsp;<u>global health and development</u></a> interventions, then it may be worth considering these careers.</p><p>However, the impact you\u2019ll be able to have will be determined in large part by the organization you enter \u2013 making it particularly important to seek out the best organizations and avoid those that only superficially care about evaluating their impact. Additionally, if you\u2019re a good fit for some of the top roles in this path, it\u2019s likely you\u2019ll also be a good fit for other highly impactful roles, so we\u2019d recommend you consider other paths, too.</p><p>[...]</p><p>&nbsp;</p><p><strong>How promising is this path?</strong>&nbsp;</p><p>Monitoring and evaluation is important for any organization aiming to have an impact. Without collecting evidence and data, it\u2019s easy to&nbsp;<i>seem&nbsp;</i>like an intervention or program is having an impact, even when it\u2019s not. Here are a few ways in which M&amp;E might be able to generate impact:&nbsp;</p><ul><li><strong>Discover effective interventions</strong> that do a lot of good. For example, rigorous evaluation by J-PAL affiliates and Evidence Action found that placing chlorine-treated water dispensers in rural African villages reduced under-5 child mortality&nbsp;<a href=\"https://cpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/0/2830/files/2022/02/Water-meta-analysis-manuscript-2022.02.20.docx.pdf\"><u>by as much as 63%</u></a>. Evidence Action has now&nbsp;<a href=\"https://www.evidenceaction.org/were-doubling-the-size-of-dispensers-for-safe-water-reaching-millions-more-people-and-saving-the-lives-of-young-children/\"><u>pledged to double</u></a> the size of its water-treatment program, reaching 9 million people.</li><li><strong>Make improvements to known effective interventions</strong>. Improving the efficacy of an already-impactful intervention by even a little bit can generate a large impact, especially if the intervention is rolled out on a large scale. Consider&nbsp;<a href=\"https://www.poverty-action.org/impact/free-malaria-bednets\"><u>this study run by malaria charity TAMTAM</u></a>, which found that charging even a nominal price for malaria bednets decreased demand by up to 60%, leading a number of large organizations to offer them for free instead.</li><li><strong>Identify ineffective or harmful interventions</strong>, so that an organization can change course. A great example of this is animal advocacy organization the Humane League, which determined&nbsp;<a href=\"https://www.penguinrandomhouse.com/books/555240/the-scout-mindset-by-julia-galef/\"><u>that their current strategy</u></a> of performing controversial public stunts was ineffective, and pivoted its strategy towards corporate campaigns. In doing so,&nbsp;<a href=\"https://www.vox.com/2016/6/9/11896096/eggs-chick-culling-ended\"><u>they convinced Unilever to stop killing male chicks</u></a>, saving millions of baby chicks from gruesome deaths.&nbsp;</li></ul><p>&nbsp;</p><p><strong>Advantages</strong></p><ul><li><strong>Clear links to effectiveness - Because M&amp;E is explicitly concerned with measuring the impact of interventions, there\u2019s often a clear \u201c</strong><a href=\"https://www.linkedin.com/pulse/short-introduction-theory-change-ian-david-moss/\"><strong><u>theory of change</u></strong></a><strong>\u201d for how your work might translate into positive impact.&nbsp;</strong></li><li><strong>Leverage</strong> - If you\u2019re working in a large organization, or working on an intervention with a large pool of potential funders and implementers, your work can&nbsp;<a href=\"https://www.probablygood.org/post/leverage\"><u>influence</u></a> where large amounts of money is spent, or how large amounts of other resources are distributed.</li><li><strong>Flexible skill set</strong> - the skills and qualifications you\u2019ll need for a career in M&amp;E are robustly useful across a range of careers. As such, it\u2019s likely that M&amp;E work will provide you with flexible&nbsp;<a href=\"https://www.probablygood.org/post/career-capital\"><u>career capital</u></a> for pursuing other paths.&nbsp;</li></ul><p><br>&nbsp;</p><p><strong>Disadvantages</strong>&nbsp;</p><ul><li><strong>Narrow range of cause areas</strong> - Within our&nbsp;<a href=\"https://www.probablygood.org/cause-area-profiles\"><u>top recommended cause areas</u></a>, there are far more M&amp;E roles within&nbsp;<a href=\"https://www.probablygood.org/global-health-and-development\"><u>global health and development</u></a> than the others. This means M&amp;E may be a promising career path if you want to work on global health, but perhaps less so if you prioritize other cause areas, like&nbsp;<a href=\"https://www.probablygood.org/animal-welfare\"><u>animal welfare</u></a> or&nbsp;<a href=\"https://www.probablygood.org/global-catastrophic-risks\"><u>global catastrophic risks</u></a>.</li><li><strong>Varying sensitivity to evidence&nbsp;</strong>- The amount of influence M&amp;E work can have is likely to vary significantly across organizations. This is in large part because it can conflict with the incentives and personalities of other people and departments within the organization. For example, some organizations may be unwilling to pull the plug on a project if it is found to be ineffective, perhaps because doing so would create bad PR, damage fundraising efforts, or because it\u2019s a \u201cpet project\u201d of someone influential within the organization. These dynamics can be personally frustrating and curtail your impact. Because of this, any career in M&amp;E should prioritize joining organizations that really care about their impact \u2013 we\u2019ll talk more about this in the priorities section later on.&nbsp;</li></ul><h1>[...]<br>&nbsp;</h1><h1><strong>Priorities within M&amp;E specialist roles</strong></h1><p>Where can you work as an M&amp;E specialist? And where might you have the most impact? Very broadly, there are three categories of organizations that hire M&amp;E staff: implementing organizations, dedicated M&amp;E organizations and consultancies, or multilateral institutions, governments, and foundations.</p><p>&nbsp;</p><p><strong>Work in-house at an implementing organizations that particularly values M&amp;E</strong>&nbsp;</p><p>An \u201cimplementing\u201d organization is one that deploys interventions and programs itself. Implementing organizations often have their own internal M&amp;E staff to track the rollout and efficacy of these programs. However, large project evaluations are likely to be outsourced to external organizations (unless it\u2019s a particularly large nonprofit), meaning that internal M&amp;E officers may find themselves focusing more on monitoring activities as well as conducting smaller (sometimes qualitative) evaluations. Internal M&amp;E specialists may also find themselves&nbsp;<a href=\"https://www.linkedin.com/pulse/short-introduction-theory-change-ian-david-moss/\"><u>developing theories of change</u></a> and communicating these to the rest of the organization, as well as other strategic work \u2013 though this will likely depend on the role\u2019s seniority.</p><p>An important point to consider here is that there is significant variance in the extent to which implementing nonprofits really value M&amp;E processes. For some, it\u2019s just another box to tick or a way to generate numbers that can be included in marketing materials. In these organizations, you could find that your work is rarely used to inform or actually improve the organization\u2019s actions, substantially reducing your potential impact.&nbsp;</p><p>Because of this, if you pursue an M&amp;E role within an implementing organization, it\u2019s really important to look out for organizations that place an unusual amount of value on gathering data and responding to evidence. But how can you identify these organizations? One place to start could be to look at whether an organization has collaborated with an external evaluating partner like J-PAL or IDInsight \u2013 this is some evidence (albeit nonperfect) that a nonprofit really cares about the efficacy of their work.&nbsp;</p><p>Another promising option is to research which organizations receive funding from foundations and evidence-based funders like the Gates Foundation, the Global Innovation Fund, the Gates Foundation, Instiglio, or GiveWell. Again, this is no guarantee that such organizations are high-impact, but it gives reason to think they have an above-average concern for M&amp;E processes.&nbsp;</p><p>A further possibility is to look at resources on the organization\u2019s website, like annual reports and evaluation reports. Some organizations may oversell the impact they\u2019ve had in order to please funders, which can indicate that they don\u2019t take evidence seriously in their decision making. Instead, look to see if they make reasonable claims about the size of their impact, if they\u2019re transparent about their numbers and data collection methods, if they\u2019ve documented times when they\u2019ve changed their activities in response to new evidence, and whether they own up to any mistakes they\u2019ve made \u2013 these can be promising indicators.&nbsp;</p><p>As a final note on working in implementing organizations, it may be a particularly promising option to join a&nbsp;<i>new, forward-thinking&nbsp;</i>implementing organization that takes evidence seriously. Experts have shared that such early stage organizations can find it difficult to hire for these positions, compared to larger organizations who tend to draw more candidates, making it more likely you\u2019ll be able to have&nbsp;<a href=\"https://www.probablygood.org/post/marginal-impact\"><u>impact at the margin</u></a>.&nbsp;</p><p>We don\u2019t have a clear or confident view on whether you\u2019ll generally have more impact at such organizations than as an M&amp;E specialist in a larger implementing nonprofit, but we imagine the tradeoff will often work like this: at a smaller organization, you might be able to bring larger marginal improvements to the organization, as you could be the only M&amp;E specialist (or one of very few), but will naturally influence fewer resources and smaller-scale interventions than you might at a larger organization. At a large organization, the reverse is true \u2013 you might have less scope on average for large marginal improvements, but the total amount of resources spent by the organization could mean this still yields substantial impact.&nbsp;</p><p>&nbsp;</p><p><strong>Working at top dedicated M&amp;E consultancies</strong></p><p>There are a number of organizations that exist to help clients like nonprofits and governments to assess the efficacy of their programs. These consultancies can be broadly divided into two categories:&nbsp;<i>academia-driven</i> &amp;<i> client-driven</i>. Such organizations are generally responsible for running evaluations for partners and clients, rather than focusing on day-to-day monitoring work. Because of this, roles in these organizations are likely to demand a higher level of statistical and quantitative literacy \u2013 perhaps particularly so in academia-driven organizations.&nbsp;</p><p>&nbsp;</p><p><strong>Academia-driven</strong></p><p>Academia-driven organizations typically contain a mixture of in-house research staff as well as affiliated academic researchers (often economists) based in universities across the world.&nbsp; These organizations will partner with a number of implementing organizations to gather data and conduct evaluations (such as RCTs) to test the efficacy of different interventions or programs. Academia-driven organizations tend to focus on quite broad questions about interventions, like&nbsp;<a href=\"https://www.povertyactionlab.org/evaluation/improving-economic-outcomes-and-well-being-through-unconditional-cash-transfers-and-goal\"><u>assessing the effectiveness of cash transfers</u></a>, or attempting to discover generalisable insights about the efficacy of different interventions which might be implemented by a range of actors. The results of these are also often published as papers in conventional academic journals.</p><p><br>Prestigious organizations in this space include&nbsp;<a href=\"https://www.povertyactionlab.org/\"><u>J-PAL</u></a>,&nbsp;<a href=\"https://www.poverty-action.org/\"><u>Innovations for Poverty Action</u></a> (IPA), and the&nbsp;<a href=\"https://www.cgdev.org/\"><u>Center for Global Development</u></a> (CGD). It\u2019s worth noting that, because of their ties to academia, some of these institutions might be considered to be development economics research institutions rather than M&amp;E organizations&nbsp;<i>per se</i> \u2013 and that, therefore, careers in these institutions may overlap significantly with&nbsp;<a href=\"https://www.probablygood.org/profile-development-economics\"><u>careers in development economics</u></a>.&nbsp;</p><p>&nbsp;</p><p><strong>Client-driven</strong></p><p>Other organizations work with clients like governments and NGOs to deliver useful information (particularly evaluations) in a way that\u2019s more tightly connected to specific organizational needs, generally on shorter timescales, and also on a smaller scale, than academic-driven consultancies. Such organizations might evaluate specific programs according to criteria produced in collaboration with the client, rather than attempting to generate highly generalizable insights about the impact of different interventions. Because such organizations are driven by the needs of their clients, rather than broader questions of academic interest, their work is highly likely to be practically applicable. A few well-respected organizations in this space include&nbsp;<a href=\"https://www.idinsight.org/\"><u>IDinsight</u></a>,&nbsp;<a href=\"https://www.mathematica.org/\"><u>Mathematica</u></a>, and&nbsp;<a href=\"https://www.opml.co.uk/\"><u>Oxford Policy Management</u></a>.</p><p>Which organization type might be more promising to work in: academia-driven or client-driven organizations? It\u2019s hard to give a confident general conclusion, but it\u2019s worth noting a few relevant considerations on either side. For example, with client-driven M&amp;E work, you may be more confident your work is practically useful as you\u2019re responding directly to organizational needs. However, the client-directed nature of this work may also constrain impact \u2013 you may find yourself helping clients who are working on problems you don\u2019t think are the most important or pressing.</p><p>On the academic side, it\u2019s a slightly different story. One potential problem is that academia-driven organizations may be constrained by academic incentives to produce work that is intellectually interesting and cutting edge, but might not always be as aligned with real-world needs as client-driven work. However, when academia-driven work&nbsp;<i>does&nbsp;</i>have an impact, it may do so on a larger scale. For instance, a large evaluation that identifies a promising new intervention, or a better way to implement an existing intervention, could pull a number of impact-inclined organizations toward changing their activities.&nbsp;</p><p>&nbsp;</p><p><strong>Multilateral institutions, governments, and foundations</strong></p><p>Large multilateral institutions, like the UN or the World Bank, are prolific employers of M&amp;E specialists, as they integrate M&amp;E processes fairly ubiquitously across branches and departments. Some M&amp;E specialists within these organizations will also be responsible for setting high-level goals and strategies for the entire organization, such as developing systems to track and evaluate whether they are meeting the&nbsp;<a href=\"https://sdgs.un.org/goals\"><u>Sustainable Development Goals</u></a>. There are a large number of opportunities at these institutions, and as such, they are prominent employers within the M&amp;E field.</p><p>We\u2019ve heard that incentives within these organizations can sometimes be misaligned with real impact, meaning that M&amp;E activities may sometimes focus on how to make individuals or branches look effective, rather than performing genuinely truth-seeking work (at least in some cases). Because of this, some caution may be warranted when considering careers in these institutions.&nbsp;Though you may be able to help improve these institutions from the inside, (and there are likely well-functioning, high-impact pockets within these organizations), this may be quite difficult.</p><p>Additionally, these institutions tend to have good talent pipelines in place and are able to hire many talented young people into junior roles. The competition here means it could be less likely you\u2019ll be able to have much&nbsp;<a href=\"https://www.probablygood.org/post/counterfactual-impact\"><u>counterfactual impact</u></a> within these roles, though they may provide strong opportunities for building&nbsp;<a href=\"https://www.probablygood.org/post/career-capital\"><u>career capital</u></a>. However, barring a particularly promising opportunity, they may not be the best opportunities for direct impact.</p><p>On the governmental side, we\u2019ve heard that the UK\u2019s&nbsp;<a href=\"https://www.gov.uk/government/organisations/foreign-commonwealth-development-office\"><u>Foreign, Commonwealth &amp; Development Office</u></a> is particularly invested in evidence-based decision making, and specifically the&nbsp;<a href=\"https://www.usaid.gov/div\"><u>Development Innovation Ventures</u></a> arm of USAID. Many LMIC governments will also offer M&amp;E roles for domestic programs and interventions, though we\u2019re uncertain about how promising these opportunities tend to be, or which governments might be particularly promising to work in (though our&nbsp;<a href=\"https://www.probablygood.org/civil-service-low-and-middle-income-countries-career-profile\"><u>profile on civil service careers within LMICs</u></a> may provide other helpful information).&nbsp;</p><p>Relatedly, large foundations (like the Gates Foundation) may also house employees who&nbsp;<a href=\"https://www.gatesfoundation.org/about/policies-and-resources/evaluation-policy\"><u>support grantees and partners</u></a> in their M&amp;E processes. Similarly, both&nbsp;<a href=\"https://www.instiglio.org/\"><u>Instiglio</u></a> and the&nbsp;<a href=\"https://www.globalinnovation.fund/\"><u>Global Innovation Fund</u></a> are two organizations that offer funding to start and scale-up programs contingent on strong evidence of effectiveness. These may be promising organizations to work at, though the roles here might not look like \u201ctypical\u201d M&amp;E careers.&nbsp;</p><p>&nbsp;</p><p>You can read the full profile over at&nbsp;<a href=\"https://www.probablygood.org/profile-monitoring-and-evaluation-specialist\"><u>Probably Good</u></a>.</p><p><i>Do you have EA or career related content that you\u2019d like to see from Probably Good? We\u2019re eager to hear what types of content would be useful to the broader community and/or to specific groups. Let us know by leaving a comment below or emailing&nbsp;</i><a href=\"mailto:contact@probablygood.org\"><i><strong><u>contact@probablygood.org</u></strong></i></a><i>.</i></p>", "user": {"username": "Probably Good"}}, {"_id": "hDw3tv5ajoPdBWArn", "title": "The case for offsetting meat consumption", "postedAt": "2022-12-08T10:22:22.171Z", "htmlBody": "<p>In this article I argue that moral offsetting is not inherently immoral and that, as long as it's implemented well, it can have a positive impact. I also explain that certain kinds of offsets wouldn't work but that others might, and I speculate that offsetting meat consumption would probably be feasible and have a positive impact. What do you think?</p><p><i>PS: If you hit a paywall, you can read the article for free </i><a href=\"https://arielpontes.medium.com/moral-offsetting-ada88135ba0d?sk=3ef39fdd4474cc67a97af694349d5030\"><i>here</i></a><i>, but if you like it please consider </i><a href=\"https://ko-fi.com/arielpontes\"><i>tipping me</i></a>.</p>", "user": {"username": "arielpontes"}}, {"_id": "hJWgJjKAyQKMZ2Dzw", "title": "Why I'm Sceptical of Foom", "postedAt": "2022-12-08T10:01:01.433Z", "htmlBody": "", "user": {"username": "Dragon God"}}, {"_id": "rCKnMqg7TzCFcamHw", "title": "EA created FTX", "postedAt": "2022-12-07T22:40:54.232Z", "htmlBody": "<p>Found this reporting interesting. The entire financial operation was expressly set up by funding from top Effective Altruists. I think we previously saw EA as resembling a victim in this scenario, but I am curious how that understanding evolves at this moment as we learn more.&nbsp;</p><blockquote><p>When Bankman-Fried and others like Mac Aulay tried to raise money for the idea from crypto investors outside the Effective Altruism movement, they weren\u2019t taken seriously. Without backers inside Effective Altruism, it\u2019s hard to imagine Bankman-Fried, a recent college grad, getting more than $100 million to play with at a crypto trading firm. And without Alameda, there never would have been an FTX.</p></blockquote>", "user": {"username": "Max Utility"}}, {"_id": "JFyzCv5YynN665nH8", "title": "Thoughts on AGI organizations and capabilities work", "postedAt": "2022-12-07T19:46:33.324Z", "htmlBody": "<p>(<i>Note: This essay was largely written by Rob, based on notes from Nate. It\u2019s formatted as Rob-paraphrasing-Nate because (a) Nate didn\u2019t have time to rephrase everything into his own words, and (b) most of the impetus for this post came from Eliezer wanting MIRI to </i><a href=\"https://www.lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1\"><i>praise a recent OpenAI post</i></a><i> and Rob wanting to share more MIRI-thoughts about the space of AGI organizations, so it felt a bit less like a Nate-post than usual.</i>)</p><hr><p>Nate and I have been happy about the AGI conversation seeming more honest and \u201creal\u201d recently. To contribute to that, I\u2019ve collected some general Nate-thoughts in this post, even though they\u2019re relatively informal and disorganized.</p><p>AGI development is a critically important topic, and the world should obviously be able to hash out such topics in conversation. (Even though it can feel weird or intimidating, and even though there\u2019s inevitably some social weirdness in sometimes saying negative things about people you like and sometimes collaborate with.) My hope is that we'll be able to make faster and better progress if we move the conversational norms further toward candor and substantive discussion of disagreements, as opposed to saying everything behind a veil of collegial obscurity.</p><p>&nbsp;</p><h2>Capabilities work is currently a bad idea</h2><p>Nate\u2019s top-level view is that ideally, Earth should take a break on doing work that might move us closer to AGI, until we understand alignment better.</p><p>That move isn\u2019t available to us, but individual researchers and organizations who choose not to burn the timeline are helping the world,&nbsp;<i>even if other researchers and orgs don't reciprocate</i>. You can unilaterally lengthen timelines, and give humanity more chances of success, by choosing not to personally shorten them.</p><p>Nate thinks capabilities work is currently a bad idea for a few reasons:</p><ul><li>He doesn\u2019t buy that current capabilities work is a likely path to ultimately solving alignment.</li><li>Insofar as current capabilities work does seem helpful for alignment, it strikes him as helping with parallelizable research goals, whereas our bottleneck is serial research goals. (See&nbsp;<a href=\"https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development\"><u>A note about differential technological development</u></a>.)</li><li>Nate doesn\u2019t buy that we&nbsp;<i>need&nbsp;</i>more capabilities progress before we can start finding a better path.</li></ul><p>This is&nbsp;<i>not</i> to say that capabilities work is never useful for alignment, or that alignment progress is never bottlenecked on capabilities progress. As an extreme example, having a working AGI on hand tomorrow would indeed make it easier to run experiments that teach us things about alignment! But in a world where we build AGI tomorrow, we're dead, because we won't have time to get a firm understanding of alignment before AGI technology proliferates and someone accidentally destroys the world.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6s6cxct97ko\"><sup><a href=\"#fn6s6cxct97ko\">[1]</a></sup></span>&nbsp;Capabilities progress can be useful in various ways, while still being harmful on net.</p><p>(Also, to be clear: AGI capabilities are obviously an essential part of humanity's long-term path to good outcomes, and it's important to develop them at some point \u2014 the sooner the better, once we're <a href=\"https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/BSee6LXg4adtrndwy\">confident</a> this will have good outcomes \u2014 and <a href=\"https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/HoQ5Rp7Gs6rebusNP\">it would be catastrophically bad to delay realizing them&nbsp;<i>forever</i></a><i>.</i>)</p><p>On Nate\u2019s view, the field should do experiments with ML systems, not just abstract theory. But if he were magically in charge of the world's collective ML efforts, he would put a pause on further capabilities work until we've had more time to orient to the problem, consider the option space, and think our way to&nbsp;<i>some</i> sort of plan-that-will-actually-probably-work. It\u2019s not as though we\u2019re hurting for ML systems to study today, and our understanding already lags far behind today\u2019s systems' capabilities.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6bcybdnyfr\"><sup><a href=\"#fny6bcybdnyfr\">[2]</a></sup></span></p><p>&nbsp;</p><h2><i>Publishing</i> capabilities advances is even more obviously bad</h2><p>For researchers who aren't willing to hit the pause button, an even more obvious (and cheaper) option is to avoid publishing any capabilities research (including results of the form \"it turns out that X can be done, though we won't say how we did it\").</p><p>Information can leak out over time, so \"do the work but don't publish about it\" still shortens AGI timelines in expectation. However, it can potentially shorten them a lot less.</p><p>In an ideal world, the field would currently be doing ~zero publishing of capabilities research \u2014 and marginal action to publish less is beneficial even if the rest of the world continues publishing.</p><p>&nbsp;</p><h2>Thoughts on the landscape of AGI organizations</h2><p>With those background points in hand:</p><p>Nate was asked&nbsp;<a href=\"https://twitter.com/orellanin/status/1543442309374689281\">earlier this year</a> whether he agrees with Eliezer's negative<a href=\"https://twitter.com/ESYudkowsky/status/1446562946717421568\">&nbsp;<u>takes</u></a> on OpenAI. There's also been a good amount of recent discussion of OpenAI on<a href=\"https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai\">&nbsp;<u>LessWrong</u></a>.</p><p>Nate tells me that his headline view of OpenAI is mostly the same as his view of other AGI organizations, so he feels a little odd singling out OpenAI. That said, here are his notes on OpenAI anyway:</p><ul><li>On Nate\u2019s model, the effect of OpenAI is almost entirely dominated by its capabilities work (and sharing of its work), and this effect is robustly negative. (This is true for DeepMind, FAIR, and Google Brain too.)</li><li>Nate thinks that DeepMind, OpenAI, Anthropic, FAIR, Google Brain, etc. should hit the pause button on capabilities work (or failing that, at least halt publishing). (And he thinks any one actor can unilaterally do good in the process, even if others aren't reciprocating.)</li><li>On Nate\u2019s model, OpenAI isn't close to operational adequacy in the sense of the&nbsp;<a href=\"https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects\"><u>Six Dimensions of Operational Adequacy</u></a> write-up \u2014 which is another good reason to hold off on doing capabilities research. But this is again a property OpenAI shares with DeepMind, Anthropic, etc.</li></ul><p>Insofar as Nate or I think OpenAI is doing the wrong thing, we\u2019re happy to criticize it.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3bch7zrei8l\"><sup><a href=\"#fn3bch7zrei8l\">[3]</a></sup></span>&nbsp;But, while this doesn't change the fact that we view OpenAI's effects as harmful on net currently, Nate does want to acknowledge that OpenAI seems to him to be doing&nbsp;<i>better</i> than some other orgs on a number of fronts:</p><ul><li>Nate liked a lot of things about the<a href=\"https://openai.com/charter/\"><u> OpenAI Charter</u></a>. (As did Eliezer, though compared to Eliezer, Nate saw the Charter as a more important positive sign about OpenAI's internal culture.)</li><li>Nate would suspect that OpenAI is much better than Google Brain and FAIR (and comparable with DeepMind, and maybe a bit behind Anthropic? it's hard to judge these things from the outside) on some important adequacy dimensions, like research closure and operational security. (Though Nate worries that, e.g., he may hear more about efforts in these directions made by OpenAI than about DeepMind just by virtue of spending more time in the Bay.)&nbsp;</li><li>Nate is also happy that Sam Altman and others at OpenAI talk to EAs/rationalists and try to resolve disagreements, and he\u2019s happy that OpenAI has had people like Holden and Helen on their board at various points.</li><li>Also, obviously, OpenAI (along with DeepMind and Anthropic) has put in a much clearer AGI alignment effort than Google, FAIR, etc. (Albeit Nate thinks the absolute amount of \"real\" alignment work is still small.)</li><li>Most recently, Nate and Eliezer <a href=\"https://www.lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1\">both think it\u2019s great</a> that OpenAI released a blog post that states their plan going forward, and we want to encourage DeepMind and Anthropic to do the same.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2d3at0qsbw\"><sup><a href=\"#fn2d3at0qsbw\">[4]</a></sup></span></li></ul><p>Comparatively, Nate thinks of OpenAI as being about on par with DeepMind, maybe a bit behind Anthropic (who publish less), and better than most of the other big names, in terms of attempts to take not-killing-everyone seriously. But again, Nate and I think that the overall effect of OpenAI (and DeepMind and FAIR and etc.) is bad, because we think it's dominated by \"shortens AGI timelines\". And we\u2019re a little leery of playing \u201cwho's better on [x] dimension\u201d when everyone seems to be on the floor of the <a href=\"https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/\">logistic success curve</a>.</p><p>We don't want \"here are a bunch of ways OpenAI is doing unusually well for its reference class\" to be treated as encouragement for those organizations to stay in the pool, or encouragement for others to join them in the pool. Outperforming DeepMind, FAIR, and Google on one or two dimensions is a weakly positive sign about the future, but on my model and Nate\u2019s, it doesn't come close to outweighing the costs of \"adding another capabilities org to the world\".</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6s6cxct97ko\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6s6cxct97ko\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Nate simultaneously endorses these four claims:</p><p>1. &nbsp;<strong>More capabilities would make it possible to learn some new things about alignment.</strong></p><p>2. &nbsp;<strong>We can't do </strong><i><strong>all</strong></i><strong> the alignment work pre-AGI.</strong> Some trial-and-error and experience with working AGI systems will be required.</p><p>3. &nbsp;<strong>It can't </strong><i><strong>all</strong></i><strong> be trial-and-error, and it can't all be improvised post-AGI.</strong> Among other things, this is because:</p><p>3.1. &nbsp;Some errors kill you, and you need insight into which errors those are, and how to avoid them, in advance.</p><p>3.2. &nbsp;We\u2019re likely to have at most a few years to upend the gameboard once AGI arrives. Figuring everything out under that level of time pressure seems unrealistic; we need to be going into the AGI regime with a solid background understanding, so that empirical work in the endgame looks more like \"nailing down a dozen loose ends and making moderate tweaks to a detailed plan\" rather than \"inventing an alignment field from scratch\".</p><p>3.3. &nbsp;AGI is likely to coincide with a <a href=\"https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization\">sharp left turn</a>, which makes it harder (and more dangerous) to rely on past empirical generalizations, especially ones that aren't backed by deep insight into AGI cognition.</p><p>3.4. &nbsp;Other points raised in <a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">AGI Ruin: A List of Lethalities</a>.</p><p>4. &nbsp;<strong>If we end up able to do alignment, it will probably be because we figured out at least one major thing that we don't currently know</strong>, that <i>isn't</i> a part of the current default path toward advancing SotA or trying to build AGI ASAP with mainstream-ish techniques, and isn't dependent on such progress.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny6bcybdnyfr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy6bcybdnyfr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>And, again, small individual \u201cdon\u2019t burn the timeline\u201d actions all contribute to incrementally increasing the time humanity has to get its act together and figure this stuff out. You don\u2019t actually need coordination in order to have a positive effect in this way.</p><p>And, to reiterate: I say \"pause\" rather than \"never build AGI at all\" because MIRI leadership&nbsp;<a href=\"https://twitter.com/robbensinger/status/1540862734408962049\"><u>thinks</u></a> that humanity never building AGI would mean <a href=\"https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/HoQ5Rp7Gs6rebusNP\">the loss of nearly all of the future's value</a>. If this were a live option, it would be an unacceptably bad one.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3bch7zrei8l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3bch7zrei8l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Nate tells me that his current thoughts on OpenAI are probably a bit less pessimistic than Eliezer's. As a rule, Nate thinks of himself as generally less socially cynical than Eliezer on a bunch of fronts, though not less-cynical&nbsp;<i>enough</i> to disagree with the basic conclusions.</p><p>Nate tells me that he agrees with Eliezer that the&nbsp;<i>original</i> version of OpenAI (\"an AGI in every household\", the associated social drama, etc.) was a pretty negative shock in the wake of the camaraderie of the 2015 Puerto Rico conference.</p><p>At this point, of course, the founding of OpenAI is a sunk cost. So Nate mostly prefers to assess OpenAI's current state and future options.</p><p>Currently, Nate thinks that OpenAI is trying harder than most on some important safety fronts&nbsp;\u2014 though none of this reaches the standards of \"adequate project\" and we're still totally going to die if they meet great success along their current path.</p><p>Since I\u2019ve listed various positives about OpenAI here, I'll note some examples of recent-ish developments that made Nate less happy about OpenAI: his sense that OpenAI was less interested in Paul Christiano's research, Evan Hubinger's research, etc. than he thought they should have been, when Paul was at OpenAI; Dario's decision to leave OpenAI; and OpenAI focusing on the \u201cuse AI to solve AI alignment\u201d approach (as opposed to other possible strategies), as&nbsp;<a href=\"https://aligned.substack.com/p/alignment-mvp\"><u>endorsed</u></a> by e.g. Jan Leike, the head of OpenAI\u2019s safety team after Paul's departure.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2d3at0qsbw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2d3at0qsbw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If a plan&nbsp;<i>doesn't</i> make sense, the research community can then notice this and apply corrective arguments, causing the plan to change. As indeed happened when Elon and Sam stated their more-obviously-bad plan for OpenAI at the organization's inception.</p><p>It would have been better to state their plan&nbsp;<i>first</i> and start an organization&nbsp;<i>later</i>, so rounds of critical feedback and updating could occur&nbsp;<i>before</i> you lock in decisions about hiring, org structure, name, culture, etc.</p><p>But at least it happened&nbsp;<i>at all</i>; if OpenAI had just said \"yeah, we're gonna do alignment research!\" and left it there, the outcome probably would have been far worse.</p><p>Also, if organizations release obviously bad plans but are then unresponsive to counter-arguments, researchers can go work at the orgs with better plans and avoid the orgs with worse plans. This encourages groups to compete to have the seemingly-sanest plan, which strikes me as a better equilibrium than the current one.</p></div></li></ol>", "user": {"username": "RobBensinger"}}, {"_id": "8Qdc5mPyrfjttLCZn", "title": "Learning from non-EAs who seek to do good", "postedAt": "2022-12-08T00:22:54.006Z", "htmlBody": "<h2>Is EA a question, or a community based around ideology?</h2><p>After a year of close interaction with Effective Altruism \u2013 and recognizing that the movement is made up of many people with different views \u2013 I\u2019m still confused as to whether&nbsp;<strong>EA aims to be a&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology\"><strong><u>question&nbsp;</u></strong></a><strong>about doing good effectively, or a community based around&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/uxFvTnzSgw8uakNBp/effective-altruism-is-an-ideology-not-just-a-question\"><strong><u>ideology</u></strong></a>.&nbsp;</p><p>In my experience, it\u2019s largely been the latter, but many EAs have expressed \u2013 either explicitly or implicitly \u2013 that they\u2019d like it to be the former.&nbsp; I see this in the frequent citations of \u201c<a href=\"https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology\"><u>EA is a question (not an ideology)</u></a>\u201d and the idea of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HDAXztEbjJsyHLKP7/outline-of-galef-s-scout-mindset\"><u>scout mindset</u></a>; and most recently, in a lot of the top comments in the post on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CnAhPPsMWAxBm7pii/what-specific-changes-should-we-as-a-community-make-to-the\"><u>suggestions for community changes</u></a>.&nbsp;</p><p>As an EA-adjacent individual, I think the single most important thing the EA community could do to become more of a question, rather than an ideology, is to<strong> take concrete steps to interact more with, learn from, and collaborate with people outside of EA who seek to do good, without necessarily aiming to bring them into the community</strong>.&nbsp;</p><p>I was a Fellow with Vox\u2019s&nbsp;<a href=\"https://www.vox.com/future-perfect\"><u>Future Perfect</u></a> section last year, and moved to the Bay Area in part to learn more about EA. I want to thank the EA community for letting me spend time in your spaces and learn from your ideas; my view of the world has definitely broadened over the past year, and I hope to continue to be engaged with the community.&nbsp;</p><p>But EA has never been, and I don\u2019t see it ever becoming, my primary community. The EA community and I have differences in interests, culture, and communication styles, and that\u2019s okay on both ends. As&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CnAhPPsMWAxBm7pii/what-specific-changes-should-we-as-a-community-make-to-the?commentId=nLJtMBnvSrp8bzseP\"><u>this comment</u></a> says, the core EA community is not a good fit for everyone!&nbsp;</p><p>A bit more about me. After college, I worked with IDinsight, a global development data analysis and advisory firm that has collaborated with GiveWell. Then&nbsp;<a href=\"https://www.vox.com/authors/siobhan-mcdonough\"><u>I wrote</u></a> for Future Perfect, focusing on global development, agriculture, and climate. I care a lot about lowercase effective altruism \u2013 trying to make the world better in an effective way \u2013 and evidence-based decision making. Some specific ways in which I differ from the average highly-engaged EA (although my, or their, priors could change) is that I\u2019m more sympathetic to non-utilitarian ethical theories (and religion), more sympathetic to person-affecting views, and more skeptical that we can predict how our actions now will impact the long-term future.&nbsp;</p><p>My personal experience with the EA community has largely been that it\u2019s a community based around an ideology, rather than a question. I probably disagree with both some critics and some EAs in that I don\u2019t think being a community based around ideology is necessarily bad. I come from a religious background, and while I have a complicated relationship with religion, I have a lot of close friends and family members who are religious, and I have a lot of respect for ideology-based communities in many ways. It\u2019s helpful for me to know what their ideology is, as I know going into discussions where we\u2019ll likely differ and where we\u2019ll potentially find common ground.&nbsp;</p><p>If EA aims to be a community based around ideology, I don\u2019t think much has to change; and the only request I\u2019d have is that EA leadership and general discourse more explicitly position the community this way. It\u2019s frustrating and confusing to interact with a community that publicly claims the importance of&nbsp;<a href=\"https://80000hours.org/articles/moral-uncertainty/\"><u>moral uncertainty</u></a>, and then have your ideas dismissed when you\u2019re not a total utilitarian or strong longtermist.&nbsp;</p><p>That said, a lot of EAs have expressed that they do not want to be a community based around ideology. I appreciated the post about&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MjTB4MvtedbLjgyja/leaning-into-ea-disillusionment\"><u>EA disillusionment</u></a> and agree with some of the recent critical posts around the community for&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/t5vFLabB2mQz2tgDr/i-m-a-22-year-old-woman-involved-in-effective-altruism-i-m\"><u>women</u></a>, but this post is not about the community itself.&nbsp;</p><h2>Why it's important for \u201cEA as a question\u201d for the EA community to engage with outside people and ideas</h2><p>If EA truly aims to be a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology\"><u>question</u></a> around aiming to \u201cdo the most good\u201d, I think the thing the EA community needs to do most is&nbsp;<strong>learn from and work with the myriad people who care about doing good, and doing it effectively, but for whom EA will never be a primary community</strong>. Lots of the people EAs can learn from are probably less \u201cEA\u201d than I am \u2013 some examples (although of course people with these identities may also identify as EA) are people living outside of EA city or country hubs, people based in the Global South; people who are older; people with children; and people who have years of career experience completely outside the EA ecosystem.</p><p>None of the following means I think that the EA community should cease to exist: it\u2019s a unique place in which I think a lot of people have found great community.&nbsp;</p><p>But there\u2019s a difference between \u201cEA the community\u201d and (lowercase) \u201ceffective altruism the project\u201d. The main tension I have observed, although this is based on anecdotes and conversations and I could be mistaken here, is that the EA community\u2019s insularity \u2013 in which cause prioritization, hiring, social life, funding, and more are all interconnected (as discussed in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dvcpKuajunxdaZ6se/ea-is-too-reliant-on-personal-connections\"><u>this post</u></a>) \u2013 is hindering lowercase effective altruism, because it means that a lot of people who would add valuable ideas but aren\u2019t engaged with EA the community aren\u2019t at the professional table, either.</p><p>Some of the key groups I\u2019ve thought of that are less involved in the EA community but would likely provide valuable perspective are policymakers on local and national levels (including policymakers from the Global South), people with years of expertise in the fields EA works in, and people who are most affected by EA-backed programs. But there are also ways of doing good with which I'm less familiar. I'm still EA-adjacent enough that there's much I would be missing, which is one reason it\u2019s important to have more diverse networks!&nbsp;</p><p>Interacting more with people from these backgrounds would bring perspectives in doing good that people who identify as EAs \u2013 largely&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ThdR8FzcfA8wckTJi/ea-survey-2020-demographics\"><u>white, male, and young</u></a>, with a heavy focus on STEM and utilitarian philosophy \u2013 are missing. CEA recognizes the importance of&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/diversity-and-inclusion\"><u>diversity and inclusion</u></a> towards attracting talented people and not missing out on important perspectives. Beyond this, some concrete problems driven by homogeneity that have been recently brought up on the forum are a lack of good&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sEpWkCvvJfoEbhnsd/the-ftx-crisis-highlights-a-deeper-cultural-problem-within\"><u>organizational governance</u></a>&nbsp;and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oD3zus6LhbhBj6z2F/red-teaming-contest-demographics-and-power-structures-in-ea\"><u>limited ability</u></a> to \u201cto design and deliver effective, innovative solutions to the world\u2019s most pressing problems\u201d.&nbsp;</p><h2>How EA can engage with people outside of the community</h2><p>Here are some of my concrete suggestions for how different groups within EA can engage with people outside of EA without the aim of bringing them into the community. This is not comprehensive, and I\u2019ve seen many of them discussed elsewhere (I like&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/?commentId=JWHoyustALo9FLca7\"><u>this comment</u></a> from another EA outsider); and I note specific positive examples I have seen. I\u2019ve put these suggestions into three broad and imperfect categories of events, ideas, and professional actions.&nbsp;</p><p><i>Events</i></p><ul><li><strong>EA Global accept and actively recruit people, especially from the Global South, who are experts in different fields that are aligned with lowercase effective altruist goals&nbsp;</strong>\u2013 for example, evidence-based decision-making and ensuring cost-effectiveness of interventions. I found EA Global DC to be making good headway with this with respect to people in the US policy space, but this could expand, which leads to my next point.&nbsp;<ul><li>Most people in the world, now and in the future, don\u2019t live in the US and Europe. I have seen some good efforts on Global South recruitment for students and early-career professionals for the EA community (such as in India and the Philippines), but I would recommend going beyond even that \u2013 bringing in people who won\u2019t become highly-engaged EAs, but could have things to teach and learn from the community. One group I had a discussion about was IAS officers (Indian Civil Service) \u2013 they and EA could both benefit from discussions on bringing evidence into policymaking.&nbsp;</li><li>This sort of engagement would almost certainly involve more EA-related discussion in languages other than English, and it\u2019s been exciting to see traction in the community on this recently.</li></ul></li><li><strong>EA groups cohost more joint community meetups</strong>. I\u2019ve seen this happen with Bay Area YIMBY, I\u2019d love to see more of these with other communities with overlapping aims. This might also help fulfill the goal of increasing diversity within the EA community if some attendees want to become highly-involved EAs.</li><li><strong>EA organizations engage with the ideas and preferences of people impacted by EA programs</strong>, such as GiveWell and IDinsight\u2019s collaboration on&nbsp;<a href=\"https://www.idinsight.org/publication/measuring-peoples-preferences/\"><u>measuring people's preferences</u></a>. Given EA (and my) elite background this might be harder than engaging, for example, officers in the Indian Civil Service, but I would love to see efforts to include the majority of the world in decisionmaking about issues that will affect the whole world. It would be great if EA orgs could include these perspectives into both program decisions and cause prioritization.&nbsp;</li></ul><p><i>Ideas</i></p><ul><li>I think it\u2019s important that EAs within the core community&nbsp;<strong>discuss ideas with/listen to people from outside the EA community</strong>. EAs in conversation with non-EAs often employ the scout mindset, and I think in general EAs are curious and open to learning and coming to synthesis. But I\u2019ve sometimes found conversations around areas in which I disagree with \u201cEA orthodoxy\u201d frustrating; in some cases, my ideas have been seemingly dismissed off the bat and the conversation has ended up with appeals to authority, which is both alienating and bad epistemics.&nbsp;</li><li><strong>EAs engage with ideas and critiques outside of the EA ecosystem</strong>. This could be through interpersonal interactions; this could be through \u2013 especially for new EAs for whom there can be an overwhelming amount of EA-specific content \u2013 continuing to engage with philosophy, social science, and other ideas outside of EA; this could be through inviting non-EAs to speak at EA Global or on EA podcasts. Lowercase effective altruism can only be made stronger through reading and engaging with other ideas, even if (and probably especially when) they challenge EA orthodoxy.</li></ul><p><i>Professional actions</i></p><ul><li><strong>EA (orgs, and core community at large) de-emphasize EA orthodoxy and trying to find the singular best way to do good, instead bringing in things like evidence-based decisionmaking to all fields.&nbsp; </strong>This is maybe a general ideological difference I have with EA that would merit its own post, but I think&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/cause-neutrality\"><u>cause neutrality</u></a> taken to its extreme (especially given we all have imperfect information when trying to prioritize causes) can be alienating to people who want to do good effectively, but whose idea of doing good isn\u2019t on the&nbsp;<a href=\"https://80000hours.org/problem-profiles/?int_source=job-board\"><u>80000 hrs list</u></a>. Some organizations like Open Phil and Founders Pledge are great at looking across fields. But general community emphases that make it seem like AI, for example, is central to EA, mean that non-AI people might think that lowercase effective altruism is not for them either \u2013 when it might be!</li><li><strong>EA orgs fund and collaborate with non-EA orgs that want to improve the world in a cost-effective way</strong>. Grantmakers should explicitly seek, if possible, organizations and cause areas outside the EA ecosystem. I\u2019ve been excited to see Open Phil\u2019s request for new cause area suggestions, such as the move into South Asian air quality.&nbsp;&nbsp;</li><li><strong>EA orgs take concrete steps to hire non-EAs</strong>. (I think there was a post on this recently but unfortunately I can\u2019t find it). People with decades of experience in topics as diverse as management, policymaking, scientific research, etc, could add a lot to EA organizations without having to be involved in the community at all. A concrete step EA orgs could take is removing EA ideology words from job descriptions and instead defining for themselves the core principles of EA that are important for the jobs they want to hire for, to ensure they can resonate beyond just people who identify as EAs.&nbsp;<ul><li>From a personal perspective, I want to say that EA and EA-adjacent orgs have been very open to me working for them, despite (or because of) my being open about my perspectives, and I want to thank them for this. That said, I only started getting recruited once I worked for Future Perfect and started to go to EA events, and I think a lot of better people than me are being missed out on because they don\u2019t know what EA is, EA doesn\u2019t know who they are, or they think EA jobs are not for them. I know recruiting from outside of networks is difficult and that essentially every sector has this problem, but there is much potential increased impact from hiring outside the community.&nbsp;</li></ul></li></ul><p>If EA aims to be a question, I think there\u2019s a way forward in which EA continues to be a unique community, but one that learns from and engages with the non-EA community a lot more; and we can work to do good better together.&nbsp;</p><p><i>Many thanks to Ishita Batra, Obasi Shaw, and others for their comments on this draft; and many thanks to the many people both within and without the EA community I\u2019ve discussed these ideas with over the course of the last year.&nbsp;</i></p>", "user": {"username": "Siobhan_M"}}, {"_id": "zu7D6DKMcB5Jwq5Ey", "title": "Updates to Faunalytics\u2019 Animal Product Impact Scales", "postedAt": "2022-12-07T17:37:00.443Z", "htmlBody": "<p>In September 2020, Faunalytics<i> </i>published our Animal Product Impact Scales, outlining how many lives and days of suffering go into U.S. consumption of animal products each day. The purpose of this resource is to help nonprofit organizations, new alt protein start-ups, and individuals prioritize animal product substitutes and make better choices about meat reduction.</p><p>The original estimates released in 2020 were rigorous, detailed, and well-used. That said, we have been able to make several improvements and updates in the past two years. With this data, which you can find on our updated hub page, you can see the impact of replacing specific animal products on both a national and individual level.&nbsp;</p><p>We have an accompanying blog post that covers:</p><ul><li>The updates we\u2019ve made,</li><li>Our most frequently asked question about the results,</li><li>Some of the ways you can use this data, and</li><li>A brief overview of where the estimates come from.</li></ul><p>Questions? Be sure to visit our free&nbsp;<a href=\"https://faunalytics.org/ask-us/\"><u>Office Hours</u></a>.&nbsp;</p><p>Animal Product Impact Scales:&nbsp;<a href=\"https://faunalytics.org/animal-product-impact-scales/\"><u>https://faunalytics.org/animal-product-impact-scales/</u></a>&nbsp;</p><p>Accompanying blog post:&nbsp;<a href=\"https://faunalytics.org/taking-nuggets-off-the-table-exploring-the-impact-of-different-animal-product-formats/\"><u>https://faunalytics.org/taking-nuggets-off-the-table-exploring-the-impact-of-different-animal-product-formats/</u></a>&nbsp;</p>", "user": {"username": "JLRiedi"}}, {"_id": "F2YfRtMvHfRJibwkj", "title": "Promoting compassionate longtermism", "postedAt": "2022-12-07T14:26:28.576Z", "htmlBody": "<p>This post is in 6 parts, starting with some basic reflections on suffering and ethics, and ending with a brief project description. While this post might seem overly broad-ranging, it\u2019s meant to set out some basic arguments and explain the rationale for the project initiative in the last section, for which we are looking for support and collaboration. I go into much greater detail about some of the core ethical ideas in a&nbsp;<a href=\"http://books.imprint.co.uk/book/?gcoi=71157100024780\"><u>new book</u></a> about to be published, which I will present soon in a <a href=\"https://forum.effectivealtruism.org/posts/BiQe6Nt9JyCwcpaaB/new-book-the-tango-of-ethics-intuition-rationality-and-the\">separate post</a>. I also make several references here to Will MacAskill\u2019s&nbsp;<i>What We Owe the Future</i>, because many of the ideas he expresses are shared by many EAs, and while I agree with many of the things he says, there are some important stances I disagree with that I will explain in this post.</p><p>My overall motivation is a deep concern about the persistence of extreme suffering far into the future, and the possibility to take productive steps now to reduce the likelihood of that happening, thereby increasing the likelihood that the future will be a flourishing one.</p><p>Summary:</p><ol><li>Suffering has an inherent call to action, and some suffering literally makes non-existence preferable.</li><li>For various reasons, there are mixed attitudes within EA towards addressing suffering as a priority.</li><li>We may not have the time to delay value lock-in for too long, and we already know some of the key principles.</li><li>Increasing our efforts to prevent intense suffering in the short term may be important for preventing the lock-in of uncompassionate values.</li><li>There\u2019s an urgent need to research and promote mechanisms that can stabilise compassionate governance at the global level.</li><li>OPIS is initiating research and film projects to widely communicate these ideas and concrete steps that can already be taken, and we are looking for support and collaboration.<br>&nbsp;</li></ol><p><strong>1. Some reflections on suffering</strong></p><ul><li>Involuntary suffering is inherently bad \u2013 one could argue that this is ultimately what \u201cbad\u201d means \u2013 but extreme, unbearable suffering is especially bad, to the point that non-existence is literally a preferable option. At this level, people choose to end their lives if they can in order to escape the pain.</li><li>We probably cannot fully grasp what it\u2019s like to experience extreme suffering unless we have experienced it ourselves. To get even an approximate sense of what it\u2019s like requires engaging with accounts and depictions of it. If not, we may underestimate its significance and attribute much lower priority to it than it deserves. As an example, a patient with a terrible condition called SUNCT whom I provided support to, who at one point attempted suicide,&nbsp;<a href=\"https://youtu.be/wxEIDtT_4pQ?t=2565\"><u>described</u></a> in a presentation we recently gave together in Geneva the utter hell he experienced, and how no one should ever have to experience what he did.</li><li>Intense suffering has an inherent call to action \u2013 we respond to it whenever we try to help people in severe pain, or animals being tortured on factory farms.</li><li>There is no equivalent inherent urgency to fill the void and bring new sentient beings into existence, even though this is an understandable desire of intelligent beings who already exist.</li><li>Intentionally bringing into existence a sentient being who will definitely experience extreme/unbearable suffering could be considered uncompassionate and even cruel.</li></ul><p>I don\u2019t think the above reflections should be particularly controversial. Even someone who would like to fill the universe with blissful beings might still concede that the project doesn\u2019t have an inherent urgency \u2013 that is, that it could be delayed for some time, or even indefinitely, without harm to anyone (unless you believe, as do some EAs, that every instance of inanimate matter in space and time that isn\u2019t being optimally used to create bliss isn't just a waste of resources but actually represents a morally compelling call to action). On the other hand, anyone screaming in agony, in the present or future, has an urgent need for their pain or suffering to be relieved.</p><p>Perhaps more controversial is determining how much suffering is actually \u201cacceptable\u201d against a background of otherwise happy or blissful sentient beings. The classical utilitarian solution is to posit a relative weighting of happiness and suffering, by which even the most horrible experiences are acceptable to create if there is enough additional bliss going around. I don\u2019t believe that this comparative weighting is objectively justified, as I argue in detail in my upcoming book. For example (excuse the graphic nature, but this is just one of the many concrete, real-life scenarios in question here), I don\u2019t think that a child being raped and killed in front of their parents is objectively justified by any number of sentient beings experiencing bliss, whether pill-induced, virtual-reality-triggered, digitally-generated or otherwise.</p><p>In the introduction to&nbsp;<i>What We Owe the Future</i>, Will MacAskill urges us to imagine living through all the lives that have ever been lived. He refers to the wide range of positive and negative experiences one would have, and the description reads like a rollercoaster-style adventure. What he doesn\u2019t explicitly mention is that countless lives along the way would contain the most brutal torture and unbearable suffering. Anyone re-experiencing these lives would scream for the experiment to end.</p><p>But I acknowledge that we have a desire to thrive and to see sentient life continue, and this strong intuition has to have a place in any realistic ethical framework. I would also add that even philosopher Derek Parfit was apparently torn between his recognition of the significance of suffering and his desire for a flourishing future (italics added): \u201cSome of our successors might live lives and create worlds that,&nbsp;<i>though failing to justify past suffering</i>, would have given us all, including those who suffered most, reasons to be glad that the Universe exists.\u201d</p><p>Regardless of one\u2019s precise ethical views, I think that most people would agree that the lower the amount of extreme suffering that occurs in the future, the better. And that there are scenarios that are clearly worse than non-existence.</p><p><strong>2. Mixed attitudes towards suffering within EA</strong></p><p>While the archetypal EA intervention is saving lives from malaria \u2013 a benchmark for cost-effectiveness \u2013 many cause areas involve relieving suffering, which is often explicitly mentioned as one of the goals of EA. Preventing malaria itself prevents both direct suffering from the disease and suffering experienced by those who lose a child, and the same can apply to other disease-related interventions. Some proposed interventions that are framed as improving human happiness or wellbeing, such as the Happier Lives Institute\u2019s recommended&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/the-elephant-in-the-bednet/\"><u>group therapy for depression</u></a>, are actually directly about alleviating suffering. Animal welfare and ending factory farming have been key cause areas within EA since early on. And wild animal suffering and possible interventions to reduce it, which could be viewed as radical among much of the general public, are considered a legitimate cause area within EA, and are even taken more seriously by some prominent EAs than by many animal rights or vegan activist groups.</p><p>On the other hand, direct relief of pain and suffering in humans remains a neglected cause area within EA \u2013 perhaps in part because the obstacles are often legal or regulatory, and the path to success is often uncertain and can be difficult to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/mSfREQgub4QDxziNy/relieving-extreme-physical-pain-in-humans-an-opportunity-for\"><u>demonstrate</u></a>. And reducing even extreme suffering cannot be directly compared with saving lives without making some questionable assumptions about using a common metric of value, complicating cost-effectiveness analyses. Also, potentially more non-human animal suffering can be prevented with the same resources \u2013 which on the one hand reflects essential anti-speciesist thinking, but which also leaves a gap in important human-related cause areas being considered.</p><p>Furthermore, the dominance of x-risk prevention and AI safety as perhaps the highest-profile cause areas within EA has arguably led to a sidelining of direct concern about suffering in both the present and long term. This is despite the obvious fact that no one wishes for a future filled with suffering, and Will paints a utopian vision of a future where life for everyone is better than the very best lives today. While risks of extreme suffering on an astronomical scale (see for example Tobias Baumann\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XyCLLYkBCPw44jpmQ/new-book-on-s-risks\"><u>new book on s-risks</u></a>) are more readily recognised as important to avoid, smaller-scale risks are more easily viewed as acceptable, even though the awfulness and inherent urgency of the experienced suffering is the same. If we are thinking about how to optimise the long-term future at potentially cosmic scales, then we could presumably be more ambitious than just trying to reduce s-risks, and aim to prevent any extreme suffering from occurring, to the extent that this is possible.</p><p>Longtermism has been criticised by some for its shift in emphasis away from those in need in the present. If this could be expected to result in less suffering overall, this could much more easily be justified. But much of the focus is on our survival as a species, and less on preventing future suffering. This suggests a possible imbalance in priorities, and could make large-scale suffering more likely due to fewer resources spent aiming to prevent it.</p><p><strong>3. We may need compassionate value lock-in sooner rather than later</strong></p><p>Many x-risk events would cause widespread suffering, whether or not they would wipe out humanity. And no one wants to die in a catastrophe. So preventing x-risks is itself compatible with preventing short- and medium-term suffering, along with respecting the intuitions and preferences of humans alive today.</p><p>But if extinction is avoided, one of the ways that extreme suffering could persist far into the future is, notably, through the lock-in of an uncompassionate totalitarian system \u2013 not necessarily purely AI-controlled, but also employing AI for this purpose. It\u2019s entirely plausible, for example, to imagine Russia or China, or even the US if events took a turn for the worse, entrenching totalitarianism while instrumentalising AI for this purpose. Promoting both principles and concrete mechanisms for entrenching compassionate governance and global cooperation therefore seems essential. While the scale and enormous complexity of the challenge are obvious, I don\u2019t see how we can secure a flourishing future without trying to tackle it, using creative approaches.</p><p>I believe there is little time to lose. Will has argued that it would be better to wait until we have reflected longer \u2013 even for many centuries \u2013 to make sure that we get the ethics right, arguing for example that value lock-in a century ago would have gotten many things wrong. He writes, \u201cyou might conclude that we should aim to lock in the values we, today, think are right, thereby preventing dystopia via the lock-in of worse values. But that would be a mistake. While the lock-in of Nazism and Stalinism would have been nightmarish, the lock-in of the values of any time or place would be terrible in many respects.\u201d He also argues that \u201cthe attempt to lock in values through AGI would run a grave risk of an irrecoverable loss of control to the AGI systems themselves,\u201d whereas \u201ctransparently removing the risk of value lock-in altogether\u201d has the benefit that \u201cby assuring everyone that this outcome is off the table, we remove the pressure to get there first\u2014thus preventing a race in which the contestants skimp on precautions against AGI takeover or resort to military force to stay ahead.\u201d</p><p>But given the state of the world and the threats we face, I don\u2019t think we can afford to wait a few hundred years to further refine our ethical thinking before a possible value lock-in occurs. Lock-in could occur much sooner, and even a partial lock-in could be difficult to escape. By the time we have achieved a greater consensus and settled on a precise set of values and principles, it might be too late. Furthermore, how do we avoid lock-in while ensuring compassionate governance? Wouldn\u2019t we&nbsp;<i>want</i> that kind of lock-in? And if an irrecoverable loss of control might happen anyways, we need to ensure we have programmed in the values in advance (of course, provided this is technically possible).</p><p>The \u201cright\u201d kind of values aren\u2019t necessarily that difficult to formulate, and we already know some of the key principles. We know that intense and especially extreme suffering is terrible and needs to be avoided wherever possible, no matter who or what is experiencing it. We know that people have physical and emotional needs to be fulfilled, and that diverse, blissful experiences make life feel meaningful and worthwhile. We also know that causing or concentrating harm, even for utilitarian reasons, runs up against strong moral intuitions. And we know that cooperation rather than confrontation or excess competition tends to be the best way of ensuring everyone's wellbeing. This doesn\u2019t mean there is an objectively correct, non-arbitrary process for carrying out decisions. But the core ethical principles already seem robust enough that we wouldn\u2019t risk much by already trying harder to start entrenching them.</p><p>Will writes that \u201cthere are so many ethical questions to which we&nbsp;<i>know</i> we haven\u2019t yet figured out the answer. Which beings have moral status: just&nbsp;<i>Homo sapiens</i>, or all primates, or all conscious creatures, including artificial beings that we might create in the future?\u201d I admit I find this question puzzling \u2013 at least from a suffering-focused perspective. It seems clear to me that any sentient being capable of suffering \u2013 including artificial ones \u2013 is deserving of moral concern. Will himself talks about the possibility of a digital civilisation; surely those beings who compose it must be prevented from suffering too?</p><p>He also mentions that \u201cthe Golden Rule, if true at all, is true across all times and places. Promotion of that principle would stay relevant and, if true, have robustly positive effects into the indefinite future. ... This suggests that, as longtermists, when trying to improve society\u2019s values, we should focus on promoting more abstract or general moral principles or, when promoting particular moral actions, tie them into a more general worldview. This helps ensure that these moral changes stay relevant and robustly positive into the future.\u201d I believe that the Golden Rule is, in fact, a very strong approximation of what we would ideally be aiming for, provided it explicitly applies to all sentient beings and prioritises actions by degree of urgency, including how urgently we would want to be rescued if we ourselves were being tortured or experiencing another form of extreme suffering.</p><p><strong>4. Preventing intense suffering now may positively influence value lock-in</strong></p><p>It seems reasonable to me that one of the important ways to lock in compassionate values is to start implementing them now so as to normalise them. There are few direct causes of suffering whose alleviation isn\u2019t technically within our near-term reach. These include better access to effective pain medications, more effective societal support mechanisms to ensure that people\u2019s needs are met, and an end to the abuse and torture of animals. Wild animal suffering is the big exception \u2013 the elephant in the forest, so to speak. But there are already ways to help some wild animals, and if we take the issue seriously, we may be able to address it more comprehensively in the medium-to-long term. Interventionism in nature is controversial, and it can be risky and shouldn\u2019t be rushed. But in principle, if one is counting on a future filled with galaxies worth of digital/artificial sentient beings, one could hardly object to helping the remaining biological beings still being born on our planet&nbsp; to avoid unnecessary suffering either.</p><p>If there is eventually lock-in of values through an AGI that was designed to align itself with human values, then how we treat humans and non-humans today might have a monumental effect on the values that it learns. And if society\u2019s actions to improve the world are perceived as being future- rather than present-oriented, relieving present suffering may appear to be deprioritised as a value. I\u2019m not saying that this is the most likely scenario. But to the extent that an AGI will have learned what our values are from our behaviours, it is essential that society\u2019s behaviours be aligned with our ideal values, and most importantly, how we respond to sentient beings in agony.</p><p><strong>5. Global coordination</strong></p><p>While object-level interventions can help create a model on which the future could be based, preventing large-scale suffering far into the future requires that our global governance mechanisms embody these values and be designed for long-term stability. Whether or not governance is ultimately executed by an AGI, this will require both value spreading and large-scale coordination in the present. Even if there may be an eventual AGI takeover, global coordination is necessary to reduce x-risks until this happens. And if there is no such takeover, coordination will be essential for a long-term solution. The coordination problem, even if potentially solvable, may be extremely complicated, as explained by social philosopher and The Consilience Project co-founder Daniel Schmachtenberger in various online interviews (e.g. In Search of the Third Attractor,&nbsp;<a href=\"https://www.youtube.com/watch?v=8XCXvzQdcug\"><u>part 1</u></a> and&nbsp;<a href=\"https://www.youtube.com/watch?v=ZCOfUYrZJMQ\"><u>part 2</u></a>). Decentralisation makes catastrophes more likely, while highly centralised power can easily become dystopian. We need to solve the problem of multipolar traps that lead to arms races, a large-scale tragedy of the commons and other catastrophic risks, without depending on a centralised dictatorship or government that isn\u2019t ultimately controlled by special interests. The strategy, in his words, \u201chas to make some kinds of destructive game theory obsolete, while winning at some other kinds of game theory.\u201d</p><p>If an AGI really does take over, then I believe we need it to embody all the characteristics of the most compassionate benevolent dictator, so that it strives to eradicate extreme suffering while not posing a threat to humans or unduly constrain their liberties. (Whether it can truly be benevolent is another question; Schmachtenberger describes this idea as messianic.) But even if an AGI doesn\u2019t actually take over, we still need to find a way to design a multipolar system in which all players are stably incentivised to cooperate and malign urges are thwarted.</p><p><strong>6. OPIS and projects to help embed compassion in governance</strong></p><p>This brings me to the last section, which is about a set of planned&nbsp;<a href=\"https://www.preventsuffering.org/\"><u>OPIS</u></a> projects that may help further the above aims. I am just presenting the general idea here, but I look forward to discussing details with anyone who is inspired by it.</p><p>Our long-term goal since our founding has been to promote compassionate ethics that prioritises the prevention of intense suffering of all sentient beings. Until now we\u2019ve mostly focused on projects to help ensure that people in severe pain can get access to effective medications, which has meant advocating for better access to morphine in lower-income countries (<a href=\"https://www.preventsuffering.org/wp-content/uploads/2018/03/Guide-to-morphine-access.pdf\"><u>ref 1</u></a>,&nbsp;<a href=\"https://www.medicusmundi.ch/de/advocacy/publikationen/mms-bulletin/palliativversorgung/advocacy-fuer-palliativmedizin-und-pflege-ein-weltweiter-querschnitt/how-a-local-champion-can-bring-the-government-on-board\"><u>ref 2</u></a>,&nbsp;<a href=\"https://newhumanist.org.uk/articles/5393/the-other-opioid-crisis\"><u>ref 3</u></a>) and communicating the dramatic effectiveness of certain psychedelics for treating horrible conditions like cluster headaches (<a href=\"https://www.preventsuffering.org/opis-policy-paper-on-legalising-psilocybin-for-cluster-headaches/\"><u>ref 4</u></a>,&nbsp;<a href=\"https://www.preventsuffering.org/wp-content/uploads/2022/08/20-Minutes-article-26-Aug-2022-with-English-translation.pdf\"><u>ref 5</u></a>). These are relatively narrow cause areas we have understandably become associated with. But we think that we can have far more impact in the long term by addressing the very principles of governance, ensuring that all significant causes of intense suffering receive adequate attention, and also promoting strategies to prevent locked-in totalitarianism. These may appear to be only distantly related cause areas, but I think they are closer to one another than they appear, because they can be addressed by invoking a common though frequently neglected underlying principle and strategy: explicitly addressing people\u2019s needs. I think that this approach, which is a core principle of conflict resolution, is also key to long-term solutions for global governance.</p><p>The goals of the projects are two-fold:</p><ol><li>Promote a concrete vision of what the world could look like in the not-too-distant future if we adopted a more comprehensive approach to governance and meeting needs, especially the prevention of intense suffering.</li><li>Promote some of the best current ideas available for how this could come about, and provide concrete steps that people and organisations can take.</li></ol><p>Some but not all of the ideas will come from the knowledge base and experience of the EA community, and they will be researched, solicited and packaged as a report with concrete recommendations. An essential element of this project is a full-length film to set out the vision and inspire people with it, and explain steps people can take. We will promote the film creatively to try to reach a large worldwide audience.</p><p>We are looking for support from within the EA community and beyond, in the form of both donations and people willing to devote some significant time on a regular basis to working with us. It is probably reasonable to support us if:</p><ol><li>you think that suffering really matters and generally agree with the ideas presented in this post;</li><li>you see a need for ambitious, creative communication projects to promote the vision of a world that aims to phase out intense suffering;</li><li>you agree that there are concrete steps individuals, organisations and governments can take to bring us closer to this vision; and</li><li>you agree that there\u2019s a reasonable chance that we will end up doing something interesting and especially impactful with this project, even if it is difficult to provide an accurate quantitative estimate.</li></ol><p>Will wrote that the \u201cBritish antislavery movement was a historical accident, a contingent event\u201d. It\u2019s possible that a worldwide movement for compassionate governance could also represent a contingent event, and that we can play a role in promoting it.</p><p>Critical comments on all of the above are, of course, welcome. But I am especially interested in inspired, constructive ideas about how we can take these projects forward with maximum impact. I encourage anyone who would like to get involved to contact me directly.<br>&nbsp;</p><p><i>Many thanks to Marieke de Visscher, Alex \u201cNil\u201d Shchelov, Manu Herr\u00e1n, Robert Daoust, Jean-Christophe Lurenbaum, Sorin Ionescu and Nell Watson for providing feedback on the draft.</i><br>&nbsp;</p>", "user": {"username": "jonleighton"}}, {"_id": "yRSoqFve3vQ6DWyJh", "title": "Visualizing the development gap", "postedAt": "2022-12-07T12:57:17.857Z", "htmlBody": "<p>Lazarus Chakwera won Malawi\u2019s 2020 Presidential election on an&nbsp;<a href=\"https://www.voanews.com/a/africa_malawi-inaugurates-new-president-who-promises-develop-country/6192312.html\"><u>anti-corruption</u></a>,&nbsp;<a href=\"https://allafrica.com/stories/202103180535.html\"><u>pro-growth</u></a> platform. It\u2019s no surprise that Malawians voted for growth, as Malawi has been&nbsp;<a href=\"https://www.economist.com/middle-east-and-africa/2022/09/22/malawi-has-saved-its-democracy-but-not-its-economy\"><u>called</u></a> the world\u2019s \u201cpoorest peaceful country\u201d. According to&nbsp;<a href=\"https://ourworldindata.org/grapher/daily-median-income?tab=chart&amp;country=~MWI\"><u>Our World in Data</u></a>, the median income per day is $1.53, or about $560 per year. Real GDP per capita has grown at an average rate of just 1.4% per year since 1961 and stands today at $1650 per person (PPP, current international $). Furthermore, the country has yet to recover from an economic downturn caused by the Covid-19 pandemic, leaving GDP per capita only slightly higher than it was in 2014.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670435821/mirroredImages/yRSoqFve3vQ6DWyJh/jrzawn9vf35u1zeu1r09.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/97ea5ebafc766991898ca19a29e723dd3365ec0629963faa.png/w_976 976w\"><figcaption>GDP per capita, PPP (current international $). Data from <a href=\"https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.CD?locations=MW\">World Bank</a></figcaption></figure><p>Life on $560 a year is possible, but not very comfortable. A sudden illness, accident, or natural disaster can be devastating. Even after spending almost all one\u2019s income, many of one\u2019s basic needs remain unmet. Investments for one\u2019s future, including education and durable goods, are mostly out of reach.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefemv8ri4t95o\"><sup><a href=\"#fnemv8ri4t95o\">[1]</a></sup></span>&nbsp;Life satisfaction in countries where incomes are so low is&nbsp;<a href=\"https://ourworldindata.org/happiness-and-life-satisfaction#higher-personal-incomes-go-together-with-higher-self-reported-life-satisfaction\"><u>poor</u></a>.</p><p>We know that it\u2019s not fair some people have to make do with so little. In the U.S., the poverty threshold, below which one qualifies for various government benefits to help meet basic needs, is $26,200 for a family of four, or $6625 per person.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwtirxbrxz3\"><sup><a href=\"#fnwtirxbrxz3\">[2]</a></sup></span>&nbsp;That makes it almost&nbsp;<i>12 times higher</i> than the median Malawian income. (All of these international comparisons are adjusted for purchasing power.)</p><p>Let that sink in. The majority of Malawians don\u2019t earn&nbsp;<i>one-tenth</i> of the amount of money below which we, in a high-income country, think one should get help from the government. And of course, the same is true in most countries. If we applied the U.S. poverty line around the world, we would see that most people just don\u2019t have enough money to meet all their basic needs.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrgvlawwwhr\"><sup><a href=\"#fnrgvlawwwhr\">[3]</a></sup></span></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670435821/mirroredImages/yRSoqFve3vQ6DWyJh/w1ql60p4pmdnrjaofqve.png\"></p><p>That\u2019s why finding ways to speed up development in low-income countries would be a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development\"><u>huge win</u></a> for philanthropists, policymakers, and citizens alike. Unfortunately, we haven\u2019t made&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/a84LrFzSf3sGsYfNr/can-we-drive-development-at-scale-an-interim-update-on\"><u>much progress</u></a> towards this goal.</p><p>In this post, I give a sense of why it\u2019s important for EAs to think about broad economic growth in lower income countries. I do this by showing how long it will take Malawi to catch up to where a high-income country like the United States is at today. In short, at typical growth rates, it will take a depressingly long time: almost two centuries. Sparking a growth acceleration, like what India has experienced in recent decades, would help a bit, but it would still take many decades for Malawi\u2019s economy to grow to the point that most Malawians can afford a reasonable standard of living.</p><p>These calculations should help deepen one\u2019s understanding of the <i>development gap</i>: the difference in living standards between high- and low-income countries. The implications for global development advocates are obvious. But I also think longtermists should pay attention. First, the wellbeing of billions is not unimportant from a longterm perspective \u2013 it\u2019s just that future wellbeing matters&nbsp;<i>as well</i>. Second, I think speeding up development would help more people from lower-income countries access the educational and professional opportunities they need to participate in what could be humanity\u2019s most important century.</p><h1>Growth trajectories for Malawi</h1><p>One way to visualize the development gap is to think about how long it will take a country like Malawi to reach various benchmarks. To that end, let's consider a few different growth trajectories. I\u2019m going to continue to refer to Malawi specifically, but one could similarly visualize any country. What matters is just the starting income and the growth rate.</p><p>First, what if Malawi continued to grow at 2% per year, roughly as it has in recent decades? At this rate, it would take a shocking&nbsp;<strong>105 years</strong> for Malawi to catch up to where the U.S. was&nbsp;<i>a century ago</i>, in 1922. To reach the U.S.\u2019s current GDP per capita would take&nbsp;<strong>189 years</strong>.</p><p>Now suppose President Chakwera managed to deliver on his election promises. Let\u2019s say his policies were to double Malawi\u2019s growth rate to 4% per year. And let\u2019s suppose Malawi maintained this growth rate in perpetuity. It would still take&nbsp;<strong>54 years</strong> to catch up to 1922 America, and&nbsp;<strong>96 years</strong> to catch up to where the U.S. is today.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670435821/mirroredImages/yRSoqFve3vQ6DWyJh/kut9oylmmejjqyk9zmsd.png\"></p><p>We can also ask how many years it will take for the&nbsp;<i>median&nbsp;</i>Malawian income to reach the US poverty line. Remember, the median Malawian earns $1.53 per day, or $560 per year. And the US poverty line in 2022 is $26,500 for a family of four, or $6625 per person.</p><p>Once again,&nbsp; the length of time needed to catch up is daunting. Even at 4% annual growth, it would be&nbsp;<strong>63 years</strong> before half of Malawians live above the U.S.\u2019s current poverty line. At 2% growth, the number of years needed jumps to&nbsp;<strong>122</strong>.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670435821/mirroredImages/yRSoqFve3vQ6DWyJh/rhx7z5ptfakd29fmzr7v.png\"></p><p>Considering its historical performance, 4% annual growth would represent a great outcome for Malawi. And yet it would still take more than 60 years for half of Malawians to achieve a standard of living that is considered minimally-acceptable in the U.S.</p><h1>A growth acceleration would modestly narrow the gap</h1><p>Now, we know that most economies don\u2019t grow in smooth curves. Instead, their growth rates vary over time. The most powerful growth stories are&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0264999316300311\"><u>growth accelerations</u></a>: sustained periods of much-faster-than-average growth that quickly raise incomes across a country.</p><p>So let\u2019s consider a growth acceleration outcome. What if Malawi\u2019s GDP per capita were to grow at 6% per year for 20 years, then revert to 2% per year?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm1f17co5aii\"><sup><a href=\"#fnm1f17co5aii\">[4]</a></sup></span></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670435821/mirroredImages/yRSoqFve3vQ6DWyJh/l8oopzhy2nsadjo0ezcb.png\"></p><p>The picture is&nbsp;<i>somewhat&nbsp;</i>improved. Malawi needs 67 years to catch up to where the U.S. was in 1922 and exactly 150 years to catch up to where it is today. Without the acceleration it took 105 and 186 years, respectively, to reach those milestones. So a two-decade, India-style growth acceleration would shorten development timelines by about 40 years. (In this scenario, it would take 83 years for the median income to reach the current U.S. poverty threshold, by the way.)</p><p>Note also that the growth acceleration means more than just reaching a certain milestone faster. It also means that everyone living in Malawi during and after the growth acceleration enjoys a higher standard of living. This gives a strong reason for <i>urgency. </i>Although changing when a given acceleration happens doesn't change when Malawi reaches a certain milestone, the sooner it happens the greater the number of people who benefit.</p><h1>Summary</h1><p>Here\u2019s a summary of all the scenarios discussed above:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" rowspan=\"2\">&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\" colspan=\"3\"><p><i>Number of years needed\u2026</i></p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>\u2026 for Malawi GDP per capita to catch up to 1922 U.S.</i></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>\u2026 for Malawi GDP per capita to catch up to 2021 U.S.</i></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><i>\u2026 for Malawi\u2019s median income to reach 2021 U.S. poverty threshold</i></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2% annual growth</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">105</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">189</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">122</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">4% annual growth</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">54</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">96</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">63</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Growth acceleration</p><p>(6% growth for 20 years, 2% growth after)</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">67</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">150</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">83</td></tr></tbody></table></figure><p>I find those numbers pretty depressing. For much of the 20th century, many low-income countries struggled to grow much at all. Things have been better in the last two decades or so, but maintaining steady growth is still far from a given. And these calculations suggest that even if we could guarantee decent growth rates, a low-income country like Malawi is many, many decades away from growing enough that all its citizens can enjoy a decent standard of living.</p><p>But effective altruism is about noticing a big problem and resolving to do something about it.</p><p>I'm still bullish on the importance of effective philanthropic investments in growth- and policy-focused interventions. There is so much work to be done to close the development gap. Until lower-income countries find ways to spark and sustain growth over the course of decades, billions of people will be unable to fulfil all their basic needs. This is made clearer when one stops using different poverty lines for high- and low-income countries.</p><p>I also want to gesture at one longtermist implication of this work. I think it\u2019s important that as many people as possible are empowered to participate in what could be humanity\u2019s&nbsp;<a href=\"https://www.cold-takes.com/most-important-century/\"><u>most important century</u></a>. It\u2019s only fair that many different people get to weigh in on where humanity is going. It may also be helpful. As Will Macaskill discusses in&nbsp;<i>What We Owe the Future</i>, moral progress is made possible when societies can consider different values and adopt the best ones.&nbsp;</p><p>We should, of course, endeavour to include a variety of different viewpoints despite global economic inequality. But this task will be made easier if people around the world are able to access educational opportunities and develop skills to participate in highly-technical discussions. Creating those opportunities requires the kind of societal surplus economic growth creates.</p><p>Finally, in \u201c<a href=\"https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development\"><u>Growth and case against randomista development</u></a>\u201d, Halstead and Hillebrandt advocated for a \u201c~4 person-year research effort [to] find donation opportunities working on economic growth in [low- and middle-income countries.\u201d I still think that would be a valuable investment. Even a small growth speed up in one country would positively impact millions of lives for decades to come. Because incomes are currently so low, marginal gains are highly valuable. And the potential for bigger and broader effects means the upside risk is enormous.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnemv8ri4t95o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefemv8ri4t95o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I recommend&nbsp;<a href=\"https://www.gapminder.org/dollar-street\"><u>Gapminder\u2019s Dollar Street</u></a> as a powerful visualisation of life at various income levels.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwtirxbrxz3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwtirxbrxz3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>U.S. Department of Health and Human Services, 2022, https://aspe.hhs.gov/topics/poverty-economic-mobility/poverty-guidelines/prior-hhs-poverty-guidelines-federal-register-references/2021-poverty-guidelines</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrgvlawwwhr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrgvlawwwhr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be honest I\u2019m not totally sure I\u2019m comparing apples to apples here. First, the U.S. poverty line has been called the money needed to meet one\u2019s basic needs, but its construction is more arbitrary and contingent than that makes it seem. In reality, the poverty threshold is calculated by adjusting for inflation a threshold that was first set in 1963 as \u201cthree times the cost of a minimum food diet\u201d (<a href=\"https://www.irp.wisc.edu/resources/how-is-poverty-measured/\"><u>https://www.irp.wisc.edu/resources/how-is-poverty-measured/</u></a>).</p><p>Second, the poverty threshold is calculated for families, so it doesn\u2019t break down neatly into a clear \u201cincome per person\u201d metric. Larger families get less money per head because of fixed costs. I\u2019ve chosen to consider a family of four here, but all this analysis would look different if one used a different family size.</p><p>All that said, I still think the poverty threshold is useful as life at that level may be more intuitive to readers from high income countries. And I don\u2019t think using a different family size, or different definition of how much money one needs to meet one\u2019s basic needs, wouldn\u2019t change my conclusions much.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm1f17co5aii\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm1f17co5aii\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This would be a similar growth story to that of India, whose GDP per capita has grown at just under 6% per year on average in the 21st century.</p></div></li></ol>", "user": {"username": "Stephen Clare"}}, {"_id": "dDTdviDpm8dAFssqe", "title": "EA London Rebranding to EA UK", "postedAt": "2022-12-07T11:37:04.156Z", "htmlBody": "<p><i>This post is mainly from the original proposal document to rebrand EA London to EA UK. &nbsp;There is now an </i><a href=\"https://effectivealtruism.uk/\"><i>EA UK website</i></a><i> and </i><a href=\"https://effectivealtruism.uk/blog\"><i>newsletter</i></a><i> since August 2022.</i></p><h1><strong>Summary</strong></h1><p>There are people in the UK outside of London, Oxford and Cambridge who have an interest in EA and don\u2019t have a local group. They may miss out on chances to connect with others, stay motivated and find out about job and donation opportunities. It could be useful to set up EA UK to support this wider group of people.</p><h1><strong>Why is there no EA UK already?</strong></h1><p>Historically local EA groups have been founded where there happened to be someone with enough time to run one rather than with a larger strategy in mind. Newer organisers around the world tend to create national groups that support local groups within that country and provide a support network for everyone in their country rather than a few cities. Because EA groups in the UK were made pretty early they grew faster before the idea of national groups was as widespread.</p><p>Another contributing factor to why there is no EA UK may be that some of the support that EA country groups often provide is given by CEA and other organisations, for example most country groups have to set up a donation platform for people who want to give effectively, but this is already done in the UK by EA Funds. There is also no need to translate materials into a local language for the UK.</p><h1><strong>Why set up EA UK?</strong></h1><ul><li>The UK has a population of 67 million, with roughly 14% living in London, Oxford and Cambridge, the three places in the UK with paid group organisers. This means that people in the rest of the country who have an interest in EA will not have a group they can get support from</li><li>A lot of the support that can be provided doesn\u2019t have to be in person (1-1s, newsletter, directory, project advice)</li><li>People move around the UK and so if there is a national group they can stay up to date with EA no matter whether they are moving to, it also could reduce the chance that people drop out of EA when they move away from or between hubs</li><li>There will be some individuals who wont be interested in attending local group events but will want to keep up to date with what happens with EA in the UK and occasionally want to talk about donations/careers</li><li>Even though EA London can provide support to those outside of London, people may not reach out to a group that isn\u2019t near them unless they feel like they have permission to</li><li>By having a national group it may lead to more local groups being set up as it is easier for people in the same place to find each other</li></ul><h1><strong>What EA UK wont do</strong></h1><ul><li>Provide support to UK student groups, this is already done by CEA, Open Philanthropy and the Global Challenges Project. EA UK could act as an extra place for them to go for support, but generally would be more focused on professionals in the UK&nbsp;</li><li>Act as a layer of management between other local groups in the UK and CEA, they will still interact with CEA for support</li></ul><h1><strong>What does this mean for EA London?</strong></h1><ul><li>Most things will stay the same, the website and newsletter have been rebranded to EA UK rather than EA London</li><li>There will still be events and support for people in London</li></ul><h1><strong>Potential Issues</strong></h1><ul><li>EA London, Oxford and Cambridge community members may get less support if there is more focus on a national group</li><li>People outside of UK EA hubs may feel neglected if most of the jobs/events etc are based in hubs. Although hopefully providing more support will allow more relevant opportunities in the wider UK to be discovered and then this can be shared back to the UK community</li><li>There may be much more demand for the support that is offered. As this is a problem of being potentially too successful we could scale back different projects, or maybe consider hiring extra people if it seems impactful</li><li>Media enquiries may come to EA UK, which is more the domain of CEA, these will be forwarded onto CEA</li></ul>", "user": {"username": "DavidNash"}}, {"_id": "uG3s9qDCnntJwci9i", "title": "[Link Post] If We Don\u2019t End Factory Farming Soon, It Might Be Here Forever.", "postedAt": "2022-12-07T11:20:07.149Z", "htmlBody": "<p>\u201cDo you know what the most popular book is? No, it\u2019s not Harry Potter. But it does talk about spells. It\u2019s the Bible, and it has been for centuries. In the past 50 years alone, the Bible has sold over 3.9 billion copies. And the second best-selling book? The Quran, at 800 million copies.</p>\n<p>As Oxford Professor William MacAskill, author of the new book \u201cWhat We Owe The Future\u201d\u2014a tome on effective altruism and \u201clongtermism\u201d\u2014explains, excerpts from these millennia-old schools of thought influence politics around the world: \u201cThe Babylonian Talmud, for example, compiled over a millennium ago, states that \u2018the embryo is considered to be mere water until the fortieth day\u2019\u2014and today Jews tend to have much more liberal attitudes towards stem cell research than Catholics, who object to this use of embryos because they believe life begins at conception. Similarly, centuries-old dietary restrictions are still widely followed, as evidenced by India\u2019s unusually high rate of vegetarianism, a $20 billion kosher food market, and many Muslims\u2019 abstinence from alcohol.\u201d</p>\n<p>The reason for this is simple: once rooted, value systems tend to persist for an extremely long time. And when it comes to factory farming, there\u2019s reason to believe we may be at an inflection point.\u201d</p>\n<p>Read the rest on Forbes.</p>\n", "user": {"username": "BrianK"}}, {"_id": "secEczgstteSs2c7r", "title": "What can we learn from the empirical social science literature on the expected contingency of value change?", "postedAt": "2022-12-07T11:40:27.736Z", "htmlBody": "<p><i>Note: Quickly written on vacation as I don't and won't have time to flesh this out much more but wanted to get the idea out so that others can work on this insofar as this seems important.&nbsp; I usually write on climate and the last time I seriously thought about value change is a while back.</i><br>&nbsp;</p><h1>A missing perspective?</h1><p>Reading What We Owe The Future's chapter on the contingency of value change one thing that struck me was that there was -- as far as I can tell -- no reference to the rich empirical literature in political science and sociology on the drivers of value change, e.g. the work on the emergence of post-materialist values by Inglehart and the work of other scholars in this tradition, including the World Value Survey<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefve40vhxl9k\"><sup><a href=\"#fnve40vhxl9k\">[1]</a></sup></span>, which maps and seeks to explain value changes in societies around the world.&nbsp;</p><p>Note that I am not claiming this literature being entirely right, more that this is a large body of literature (including criticisms) that seems very relevant to EA interest in value change, but not discussed.<br>&nbsp;</p><h2>Introductions to this literature</h2><ul><li><a href=\"https://www.nytimes.com/2022/11/01/opinion/ezra-klein-podcast-pippa-norris.html\">Ezra Klein podcast 2022: Conversation with Pippa Norris</a></li><li><a href=\"https://muse.jhu.edu/article/787841\">Welzel 2021: Why the Future is Democratic</a></li><li><a href=\"https://www.cambridge.org/core/journals/perspectives-on-politics/article/abs/changing-mass-priorities-the-link-between-modernization-and-democracy/816079F2778E68E46984469D634741D0\">Inglehart and Welzel 2010: Changing Mass Priorities: The Link between Modernization and Democracy</a></li><li><a href=\"https://www.jstor.org/stable/20699492\">Inglehart and Welzel 2009: How Development Leads to Democracy</a></li><li><a href=\"https://ejpr.onlinelibrary.wiley.com/doi/full/10.1111/1475-6765.00086\">Welzel et al 2003: The theory of human development, a cross-cultural analysis</a>.</li></ul><p>&nbsp;</p><p>These sources are quickly chosen and are biased towards Welzel (whom I was a student of in college), so don't take this as a definite treatment - just a window into this literature.<br><br>The basic point of this literature is that there are good reasons to expect that value <i>change</i> is actually quite predictable and that a certain set of values tend to emerge out of the conditions of modernization. There is a lot more nuance to it, this is not just \"old-school\" modernization theory, but it is an empirically grounded update against massive contingency in the development of values, as there are predictable relationships between economic development and value change that seem to hold across a broad set of geographies and stages of economic development.<br><br><br>To give a flavor, the abstract of Welzel 2021 puts it as follows:<br><i>\"Recent accounts of democratic backsliding neglect the cultural foundations of autocracy-versus-democracy. To bring culture back in, this article demonstrates that 1) countries' membership in culture zones explains some 70 percent of the total cross-national variation in autocracy-versus-democracy; and 2) this culture-bound variation has remained astoundingly constant over time\u2014in spite of all the trending patterns in the global distribution of regime types over the last 120 years. Furthermore, the explanatory power of culture zones over autocracy-versus-democracy is rooted in the cultures' differentiation on \"authoritarian-versus-emancipative values.\" Therefore, both the direction and the extent of regime change are a function of glacially accruing regime-culture misfits\u2014driven by generational value shifts in a predominantly emancipatory direction. Consequently, the backsliding of democracies into authoritarianism is limited to societies in which emancipative values remain underdeveloped. Contrary to the widely cited deconsolidation thesis, the ascendant generational profile of emancipative values means that the momentary challenges to democracy are unlikely to stifle democracy's long-term rise.\"</i><br>&nbsp;</p><h1>How it matters: different priors on the contingency of value change</h1><p>I am wondering&nbsp;</p><ul><li>(1) whether this literature could be quite useful in forming priors about the contingency of value change and,&nbsp;</li><li>(2) in particular, serve as useful corrective/complement to deriving priors from&nbsp;<i>history</i> which, as a discipline, probably has a methodological/ontological bias towards explanations stressing contingency (with empirical social science having the opposite bias emphasizing regularities and general patterns).</li></ul><p>&nbsp;</p><p>To make this a bit more concrete, (3) the evidence from this literature that value change follows predictable patterns related to modernization and its correlates (educational attainment, wealth etc.) which is fairly similar across societies (despite very different starting points) leads me to give a much lower value for re-runs of history reaching today's level of technological development while also maintaining slavery than stated in WWOTF (&gt;50% for 10-90 of 100 worlds having slavery of &gt;1% of population, WWOTF).</p><p>In particular, from this literature one would assume that there is a broad complex of postmaterialist values -- stressing individual autonomy, globalism, concern for the environment, egalitarianism etc -- that emerge as the result of economic and societal changes (not quite Marxist level material determinism, but decreasing expected contingency regardless).</p><p>Indeed, (4) on a more precise definition of \"same level of technological development\" (whether this includes similar levels of average educational attainment and wealth, for example, insofar as they are required for or emerge from the current level of technological development) for the thought experiment on re-running history one could probably derive estimates for the contingency on value changes based on (a) the explanatory power of the mechanisms stipulated by this literature and one's credence in (b) these estimates being good causal estimates and (c) them holding in other possible worlds.</p><h2>&nbsp;</h2><h1>Potential objections</h1><p>I can think of two possible objections to the usefulness of this literature:</p><p>(5) While something like the emergence of postmaterialist values is explainable ex-post and can then be usefully predicted for countries not at the frontier, maybe we are most interested in 'frontier moral change' which is harder to predict / have expectations about (in retrospect, it makes a lot of sense that the abundance and material security of the 1950s and 1960s led to a generation stressing concerns beyond traditional material values, e.g. protection of environment, sexual liberation, globalism etc, but this effect could not have been predicted ex ante).</p><p>Even if true, this literature could still be useful in forming expectations about the contingency of value change in general.</p><p>(6) While the broad direction of value change may be somewhat predictable (e.g. insofar as societies become richer, postmaterialist values spread), maybe this doesn't matter much for the contingency of relevant value changes (e.g. maybe something like concern for long-term future does not map onto general value changes even though something like globalism does).</p><h1><br>A very tentative conclusion&nbsp;</h1><p>For reasons (5) and (6) I am not very confident that this literature is very useful, but reasons (1)-(4) make me think it could be quite useful so maybe it is worth exploring further.&nbsp;<br><br>I am not sure here at all, but I wanted to bring it into the discussion.</p><h1><br><br>About Founders Pledge</h1><p>Founders Pledge is a community of over 1,700 tech entrepreneurs finding and funding solutions to the world\u2019s most pressing problems. Through cutting-edge research, world-class advice, and end-to-end giving infrastructure, we empower members to maximize their philanthropic impact by pledging a meaningful portion of their proceeds to charitable causes. Since 2015, our members have pledged over $7 billion and donated more than $700 million globally. As a non-profit, we are grateful to be community supported. Together, we are committed to doing immense good. <a href=\"https://founderspledge.com/\">founderspledge.com&nbsp;</a><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnve40vhxl9k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefve40vhxl9k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The World Value Survey is mentioned in a later chapter, but not in reference to the literature discussed here.</p></div></li></ol>", "user": {"username": "jackva"}}, {"_id": "hD2J2cqfqqSerXKcE", "title": "AGI with feelings", "postedAt": "2022-12-07T16:00:03.363Z", "htmlBody": "<p>First draft of future AGI. Poppers epistemology, feelings, body and limitations.</p>\n<p>My guess is you need feelings programmed to break the loop of pre-programmed AI to a creative AGI.\nThe AI conjecture a solution only if it wants to, it need some kind of irritation build up when it is stuck, some kind of relief will happen when a new conjecture happened. It will continue until it has solved the problem.\nThen my guess it could start to solve problems not pre-programmed in.</p>\n<p>It would need to be a program based on a problem-guess-criticism-new problem epistemology. Feelings will be in the guess category.</p>\n<p>You need a body to be relevant and real world setting for testing.</p>\n<p>\u201cFuture Fund worldview prize\u201d</p>\n", "user": {"username": "Nicolai Meberg"}}, {"_id": "XcsX8GEkszEhEumMo", "title": "Something to make myself fascinated with computing science and AI.", "postedAt": "2022-12-07T02:12:04.870Z", "htmlBody": "<p>I have always been fascinated by biology. I'd binge watch nature documentaries since I was a small kid, and I am deeply concerned with environmentalism.</p>\n<p>However, I also have a desire to help the world, to do the most good that I can do. And 80.000 hours has presented some really strong arguments about why AI alignment might be my best opportunity to do this.</p>\n<p>But, I really don't have the same fascination towards AI as I do with biology. This is just something that has never been on my mind. I really can't see myself turning my back to biology. It seems I ought to, but I don't want to.</p>\n<p>One solution to this conundrum would be to become as fascinated with AI as I am with biology. So, I would like to ask for recommendations for books, documentaries, videos or anything else that you think might spark my interest towards it.</p>\n", "user": {"username": "Eduardo Lu\u00eds Carls "}}, {"_id": "xdwqbmZv2txYPqFAB", "title": "New interview with SBF on Will MacAskill, \"earn to give\" and EA", "postedAt": "2022-12-07T01:48:16.794Z", "htmlBody": "<p>Hi folks, just wanted to share <a href=\"https://puck.news/sam-bankman-fried-sbf-interview/\">a new interview I did</a> with Sam Bankman-Fried about Will MacAskill, effective altruism, and whether \"earn to give\" led him to make the mistakes he made at FTX. Interested, of course, in any thoughts.</p><p>Teddy</p>", "user": {"username": "teddyschleifer"}}, {"_id": "KsN4BL39zZAYByXGg", "title": "A case for the adoption of Phage therapy in Africa", "postedAt": "2022-12-07T01:38:22.966Z", "htmlBody": "<p><strong>Background</strong></p><p>Antimicrobial resistance(AMR) is one of the greatest threats we face as a global community.&nbsp;An essential global public health objective is lowering the burden of infection-related mortality. Previous research has calculated the number of fatalities brought on by drug-resistant infections and sepsis and discovered that infections continue to be the world's leading cause of mortality. The WHO has predicted that by 2030 (<a href=\"https://www.who.int/news/item/29-04-2019-new-report-calls-for-urgent-action-to-avert-antimicrobial-resistance-crisis\">https://www.who.int/news/item/29-04-2019-new-report-calls-for-urgent-action-to-avert-antimicrobial-resistance-crisis</a>), &nbsp;antimicrobial resistance could force up to 24 million people into extreme poverty.<a href=\"https://apo.org.au/node/63983\">According to the UK Government-commissioned Review on Antimicrobial Resistance</a>, AMR might result in the yearly death of 10 million people by 2050. It is predicted that 90% of these deaths will occur in Africa and Asia.</p><p>According to a recent report published in the Lancet, which&nbsp;estimated deaths and disability-adjusted life-years (DALYs) attributable to and associated with bacterial AMR for 23 pathogens and 88 pathogen\u2013drug combinations in 204 countries and territories in 2019.&nbsp; The study predicted that in 2019,&nbsp;there were an estimated 4\u00b795 million (3\u00b762\u20136\u00b757) deaths associated with bacterial AMR &nbsp;including 1\u00b727 million (95% UI 0\u00b7911\u20131\u00b771) deaths attributable to bacterial AMR. &nbsp;Six leading pathogens for deaths associated with resistance (<i>Escherichia coli</i>, followed by&nbsp;<i>Staphylococcus aureus, Klebsiella pneumoniae, Streptococcus pneumoniae, Acinetobacter baumannii</i>, and&nbsp;<i>Pseudomonas aeruginosa</i>) were responsible for 929\u2009000 (660\u2009000\u20131\u2009270\u2009000) deaths attributable to AMR and 3\u00b757 million (2\u00b762\u20134\u00b778) deaths associated with AMR in 2019. The full report can be found <a href=\"https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(21)02724-0/fulltext#seccestitle10\">here</a>.</p><p><strong>A case for sub-Saharan Africa</strong></p><p>Based on the report, analysis based on regions of the world,&nbsp;Western sub-Saharan Africa had the highest burden, with 27\u00b73 deaths per 100\u2009000 (20\u00b79\u201335\u00b73) attributable to AMR and 114\u00b78 deaths per 100\u2009000 (90\u00b74\u2013145\u00b73) associated with AMR, while&nbsp;Australasia had the lowest AMR burden in 2019, with 6\u00b75 deaths per 100\u2009000 (95% UI 4\u00b73\u20139\u00b74) attributable to AMR and 28\u00b70 deaths per 100\u2009000 (18\u00b78\u201339\u00b79) associated with AMR in 2019. &nbsp;In Nigeria, communicable diseases accounted for 66% of morbidity in <a href=\"https://cdn.who.int/media/docs/default-source/a-future-for-children/nigeria-amr-national-action-plan.pdf?sfvrsn=153f003d_1&amp;download=true\">2015</a> There is an urgent need to address this problem in Sub-Saharan Africa.&nbsp;</p><p><strong>Phages as an alternative to antibiotics</strong></p><p>Phages have been proposed as an alternative to <a href=\"%16https:/www.nature.com/articles/s41599-020-0478-4\">drug resistance</a>. While phages have been used successfully in <a href=\"https://www.nature.com/articles/s41467-022-33294-w\">Europe</a>, <a href=\"https://preview.phageaustralia.org/dhanvi\">Australia</a> and the<a href=\"https://health.ucsd.edu/news/releases/pages/2017-04-25-novel-phage-therapy-saves-patient-with-multidrug-resistant-bacterial-infection.aspx\"> USA</a>. No report on phage therapy use in Africa. Phage research is neglected in Africa. A recent survey of <a href=\"https://www.preprints.org/manuscript/202201.0345/v1\">phage scientists</a> under the Africa phage forum showed that funding, lack of skill sets and infrastructure are the major setbacks preventing success in phage research in Africa.&nbsp;</p><p>To stimulate phage research as an alternative to antibiotics in Africa, I propose coordinated and strategic support. This support should focus on:</p><ol><li>Influencing policies that will allow for the adoption and use of phage therapy in Africa. No country within Africa has a framework for the adoption of phage therapy</li><li>Address the infrastructure problem. This can be done by setting up phage banks in regions of Africa. In Nigeria, I am currently building a phage bank that can isolate, characterize and purify phages for use as phage therapy against the WHO priority pathogens. We can encourage this kind of phage banks across Africa. Funding has been a major challenge even for the phage bank in Nigeria.&nbsp;</li><li>Training, <a href=\"https://www.phagesforglobalhealth.org/workshops\">Phages for global health</a> have supported the training of phage researchers in Africa. They can be encouraged to do more</li></ol>", "user": {"username": "emmannaemeka"}}, {"_id": "2mirCRXvahGk3mC9j", "title": "Consequentialism does not endorse frauding-to-give", "postedAt": "2022-12-07T17:40:17.832Z", "htmlBody": "<p>Mainstream media coverage of the FTX crash frequently suggests that a utilitarian ethic is partially to blame for the irresponsible behavior of top executives. However, consequentialist reasoning - even in its most extreme \"ends justify the means\" form - does not endorse committing crimes with the goal of making money to donate to charity.</p><h3>Disclaimers</h3><ul><li><i>This post is not about FTX. I want to abstract away from that specific circumstance and make a broader point about consequentialism and applied ethics. These arguments are relevant whether or not fraud was committed by FTX leadership.</i></li><li><i>This post is nothing revolutionary; I just think these arguments need to be reiterated succinctly.</i></li><li><i>I do not consider myself a hardcore consequentialist. In general, I find it strange to believe that a single ethical theory could/should possibly guide all aspects of one's life.</i></li><li><i>I am not a trained philosopher; please use the comments if my understanding of consequentialism is flawed.</i></li></ul><h2>Main claim</h2><p>In my opinion, the heart (and most interesting feature) of consequentialism is determining the downstream consequences of an action, especially those consequences which influence others' actions. However, this calculus is rarely mentioned in popular discourses on utilitarianism. When somebody brings up the <a href=\"https://www.thelifeyoucansave.org/child-in-the-pond/\">drowning child problem</a>, they don't ask you to consider how your decision will impact the future of the pond's availability for public bathing. That issue is hardly relevant to whether or not you choose to save the child. But real-life decisions are not thought experiments, and if we want to be serious about consequentialism, downstream effects are crucial to every moral calculus.</p><p>This is not a novel idea within consequentialist thought. Consider the famous <a href=\"https://www.utilitarianism.net/objections-to-utilitarianism/rights\">transplant thought experiment</a>. The experiment imagines that a healthy patient walks into a hospital, and the doctor must decide whether to kill her and harvest her organs to save five dying patients. The most intuitive consequentialist response is: \"I don't care if it saves five lives; if hospitals begin killing healthy patients our entire health system will crumble.\"</p><p>The same intuitive response should also apply to breaking the law in order to make money to later donate to charity. Off the top of my head, here are a few downstream consequences which make that decision a bad idea:</p><ul><li>You're caught and you never get the chance to donate the money because you are forced to forfeit it.</li><li>You ruin your reputation and lose opportunities to perform good actions in the future.</li><li>Your moral calculus was incorrect, and the illegal action does more harm than your donation does good.</li><li>If you are part of a movement that advocates doing good in the world, the exposure of your actions causes harm to that larger movement.&nbsp;</li></ul><p>These are all consequentialist arguments<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn3ql5uqq2b\"><sup><a href=\"#fnn3ql5uqq2b\">[1]</a></sup></span>&nbsp;- they rely on expected value calculations not rights violations or virtue ethics. Taken together they demonstrate why, in the vast majority of imaginable circumstances, the ends simply do not justify the means when it comes to breaking the law with the goal of making money to give away.</p><h2>Counterarguments</h2><h3>Naive vs. sophisticated consequentialism</h3><p>I've been talking a lot about \"downstream consequences\". If you've spent some time in EA circles, you might object that I've only considered \"sophisticated consequentialism\", but \"naive consequentialism\" might support immoral behavior to benefit some greater good.&nbsp;</p><p>I disagree. The EAForum post on <a href=\"https://forum.effectivealtruism.org/topics/naive-vs-sophisticated-consequentialism#:~:text=Naive%20consequentialism%20is%20the%20view,the%20act%20that%20consequentialism%20requires.\">naive vs. sophisticated consequentialism</a> states that:</p><blockquote><p><strong>Naive consequentialism</strong> is the view that, to comply with the requirements of consequentialism, an agent should at all times be motivated to perform the act that consequentialism requires. By contrast, <strong>sophisticated consequentialism</strong> holds that a consequentialist agent should adopt whichever set of motivations will cause her to in fact act in ways required by consequentialism.</p></blockquote><p>Given this definition, my entire argument has been, counterintuitively, based off naive, not sophisticated, consequentialism. Moreover, my argument stands under the above definition of sophisticated consequentialism, too, because it's hard to imagine a set of motivations which include committing crimes and also lead to the actualization of ideal consequences.</p><p>But sometimes naive consequentialism is defined another way. The same <a href=\"https://forum.effectivealtruism.org/topics/naive-vs-sophisticated-consequentialism#:~:text=Naive%20consequentialism%20is%20the%20view,the%20act%20that%20consequentialism%20requires.\">EAForum post</a> states that an even more naive consequentialist does not \"consider less direct, less immediate, or otherwise less visible consequences\". Interestingly, this definition makes the illegal behavior even more immoral. Because, under this form of naive consequentialism, you cannot consider the downstream consequences of your action, you cannot consider the fact that you will later donate the money to help people. The only consequences you can take into account are the immediate effects of the illegal action itself, and in all relevant cases, those will be bad.</p><p>Therefore, in both its naive and sophisticated forms, consequentialism does not endorse the illegal behavior.</p><h3>It's the ideas that matter, not whether they were applied correctly</h3><p>You might object that, even accepting the conclusion that a good consequentialist wouldn't commit the crime, what matters more is that actors might misconstrue consequentialism and use it as moral backing for their fraudulent behavior.</p><p>I agree that this is a real problem, but I don't see it as a valid objection to my claims in this post. Anybody can misconstrue any theory and \"use\" it to \"endorse\" any action. In other words, a theory is not inherently wrong just because it can be incorrectly understood and then leveraged to justify harm.</p><p>That being said, the EA movement is broadly consequentialist, so we should examine our own theoretical endorsements under a broadly consequentialist framework. If we determine that publicly advocating consequentialism directly causes many people to act immorally \"in the name of consequentialism\", we should either 1. change our messaging or 2. stop advocating consequentialism even if it's still what we truly believe<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8odh7825z2u\"><sup><a href=\"#fn8odh7825z2u\">[2]</a></sup></span>.</p><h2>Conclusion</h2><p>I didn't write this post to advocate for consequentialism. I wrote it because I think consequentialism should be taken seriously as a moral theory. And consequentialism taken seriously does not entail that any ends justify any means. Consequentialism is so interesting precisely because it asks us to <strong>at least consider the ends when we examine the means</strong>. But when the means are potentially catastrophic, they are unlikely to be justified by any ends, no matter how good.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn3ql5uqq2b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn3ql5uqq2b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some of these arguments might reasonably used against earning to give more generally, especially for those who choose morally questionable career choices, but that's not relevant to this discussion.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8odh7825z2u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8odh7825z2u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I wrote more about this strange conclusion in Part 2 of <a href=\"https://forum.effectivealtruism.org/posts/Dm8iCYwouAb4oSwos/as-ea-embraces-more-avenues-for-change-we-must-change-our#Part_2___Do_a_lot_of_good__is_an_effective_rallying_cry__How_to_grow_a_social_movement_for_good\">this post</a>.</p></div></li></ol>", "user": {"username": "Jasper Meyer"}}, {"_id": "agKFRa5qBApnfLrSh", "title": "Why development aid is a really exciting field", "postedAt": "2022-12-07T14:58:00.431Z", "htmlBody": "<p>Each year, wealthy countries collectively spend around 178 billion dollars<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhmd8va2u9u\"><sup><a href=\"#fnhmd8va2u9u\">[1]</a></sup></span>&nbsp;(!!) on Development aid.</p><p>Development aid has funded some of the most cost-effective lifesaving programs that have ever been run. Such examples include&nbsp;<a href=\"https://en.wikipedia.org/wiki/President%27s_Emergency_Plan_for_AIDS_Relief\"><u>PEPFAR</u></a>, the US emergency aids relief programme rolled out at the height of the African aids pandemic, which estimates suggest saved 25 million lives<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8f7px7hchil\"><sup><a href=\"#fn8f7px7hchil\">[2]</a></sup></span>&nbsp;at a cost of some 85 billion<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8d0vjzuzq7v\"><sup><a href=\"#fn8d0vjzuzq7v\">[3]</a></sup></span>&nbsp;($3400 per life saved, competitive with Givewell\u2019s very best). EAs working with global poverty will know just how difficult it is to achieve high cost effectiveness at these scales.</p><p>Development aid has also funded some of the very worst development projects conceived, in some instances causing&nbsp;<a href=\"https://www.givewell.org/international/technical/criteria/impact/failure-stories#Harmful_aid_projects\"><u>outright harm to the recipients</u></a>.</p><p>Development aid is spent with a large variety of goals in mind. Climate mitigation projects, gender equality campaigns, and free-trade agreements are all funded by wealthy governments under a single illusory budgetary item: \u2018development assistance\u2019.</p><p>In short, the scope of aid is&nbsp;<strong>enormous</strong> and so is the impact that can be had by positively influencing how it is spent. I'm not the only one who thinks so! In January of 2022 Open Philanthropy announced Global Aid Policy as a priority area within its global health and wellbeing portfolio.</p><p><br>In this post I will:</p><ul><li>Demystify the processes that decide how aid is allocated.</li><li>Argue that aid policy is neglected (by EAs especially), high in scale, and maybe tractable.</li><li>Sneakily attempt to make you excited about aid, in preparation for the announcement of a non-profit I\u2019m co-founding.</li></ul><h1>Who decides how aid is allocated?</h1><p>When I first dug into development aid, I found the field very opaque and difficult to get an overview of. This made everything seem much more static and difficult to influence, than I now think it is.</p><p>Come with me, and I\u2019ll show you how the aid-sausage is made. The explanation tries to capture the grand picture, but each country is different and the explanation is overfit to western democracies.</p><h2>The aid pipeline:</h2><p>It all begins with a government decision to spend money on aid. For many countries this decision was formalized in 1970 after a UN resolution was signed between members to spend 0.7% of GNI on official development assistance.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhfhmpwzelu\"><sup><a href=\"#fnhfhmpwzelu\">[4]</a></sup></span></p><h2>Politicians decide on a national aid strategy</h2><p>Each country that gives aid will have an official strategy for its aid spending. The strategy lists a number of priorities the government wants to focus on. It is typically re-negotiated and updated once every few years or when a new government takes seat. The agreed upon aid strategy sets a broad direction for the civil service and relevant parliamentary committees in projects they choose to carry out.</p><p>The recently released&nbsp;<a href=\"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1075328/uk-governments-strategy-international-development.pdf\"><u>UK aid strategy</u></a> is a good example of what typical priorities look like:</p><blockquote><ol><li>deliver honest, reliable investment through British Investment Partnerships, building on the UK\u2019s financial expertise and the strengths of the City of London, in line with the Prime Minister\u2019s vision for the Clean Green Initiative</li><li>provide women and girls with the freedom they need to succeed, unlocking their future potential, educating girls, supporting their empowerment and protecting them against violence</li><li>step up the UK\u2019s life-saving humanitarian work to prevent the worst forms of human suffering around the world. The UK will lead globally for a more effective international response to humanitarian crises</li><li>take forward UK leadership on climate change, nature and global health. The strategy will put the UK commitments made during the UK\u2019s Presidency of G7 and COP26, UK global leadership in science and technology, and the UK\u2019s COVID-19 response, at the core of its international development work</li></ol></blockquote><h2>The Government passes a national budget</h2><p>Most elected governments pass a national budget each year, determining how much money should be spent on each category of expense from healthcare to military. One of these categories will typically be \u2018official development assistance\u2019.</p><p>This category will often be broken into subcategories, which earmark various percentages towards various causes, countries, and organizations. A common budgetary target for many countries is to spend 0.7% of GNI on aid, with some countries giving slightly more and others giving slightly less.</p><p>The importance of the national budget varies significantly based on the extent to which it earmarks the aid spending. The 2023 Danish national budget, for example, sets aside 25% of its aid to climate related interventions and 3.6% towards helping Ukraine. Once the yearly budget is approved, it is difficult to change.</p><p>Typically the civil service will make a recommendation for how the budget should be allocated, in line with past commitments, ongoing projects and the government\u2019s aid priorities. The government uses this proposal as a starting point, making amendments in line with their political objectives.</p><h2>The country\u2019s aid agency decides on projects</h2><p>After the national budget is passed, the parliament passes on the torch to a government aid agency responsible for carrying out the strategy with the available resources.</p><h3>Multilateral projects</h3><p>Countries give anywhere from 0 to 70% of their total development budgets to multilateral organizations such as the UN and WHO, who carry out development projects that align with the governments\u2019 strategies.</p><p>Depending on the number of strings attached, the multilateral organization largely decides how the money is spent from there.</p><h3>Bilateral projects</h3><p>Donor countries also carry out a number of projects that directly attempt to achieve some objective in a recipient country. These projects are referred to as \u2018bilateral\u2019 because they only involve the donor and recipient country, and are overseen by the national aid agency itself.</p><p>The structure of these projects varies significantly, but often they are carried out in collaboration with either the recipient government or an NGO that is responsible for the day-to-day operations. In order to achieve the objectives set out in the donor country's aid strategy, its national agency will regularly consult the budget and decide on new bilateral projects.</p><p>Most governments have a number of developing countries whose governments they have established especially good relations with. Aid and other investments are a way to uphold those relations, but high trust also makes it easier to carry out impactful projects. Countries tend to carry out especially many bilateral projects in countries they work well with, or want to build better relations to.</p><h2>Projects are implemented</h2><p>Now that the government has decided on projects to fund, we finally arrive at the stage of actual implementation. This is the step that EA historically has been the most engaged with. Here we can measure the impact of projects, rank them against each other, and say which are more impactful.</p><p>What\u2019s important to keep in mind are all of the political decisions that went into deciding on an exact project. If effective altruists identify a different project that would save more lives, but does worse on all the other criteria the government used to decide, we shouldn\u2019t expect the government to change its decision.</p><p>Ensuring that each step in the decision-making process becomes better aligned with indiscriminately saving and improving lives is of increasing importance, especially as evidence establishes which interventions do so most effectively.</p><h1>Scale, Tractability, Neglectednes</h1><h2>Scale</h2><p><i>\u201cThe scale speaks for itself\u201d - Hans Niemann</i></p><p>178 Billion dollars is a ridiculously high number. There is a broad consensus among experts, think tanks, and implementing partners alike that the cost-effectiveness of aid can be vastly improved.</p><p>I expect a lot of people to be reasonably worried that this is a textbook case of getting mugged by scale. \u201cIf we capture just 1% of the market, we\u2019ll be rich\u201d is a common sentence to hear from startups doomed to fail. Let\u2019s assume that for some reason you are skeptical of somebody\u2019s chance of influencing the entire world\u2019s aid spending, and focus on just the aid spending of a single small country.</p><p>Denmark, with a population of just under six million, alone spends ~$2.7 billion on development aid each year, which is more than enough money to yield a massive impact if spent more cost-effectively.</p><h2>Tractability</h2><p>Is influencing the spending of a small country tractable? Hopefully the overview of how aid allocation is decided, should have given you a sense of the many avenues there are to influence aid priorities for the better.</p><p>If you\u2019re still not convinced that it\u2019s tractable, let\u2019s look at some case-studies cherry-picked by me to prove that it\u2019s at the very least possible.</p><p>The directors of the Norwegian aid agency were recently featured in&nbsp;<a href=\"https://www.dn.no/utenriks/norad-direktor-bard-vegar-solhjell-vil-gi-verdens-fattigste-milliarder-av-kroner-i-cash/2-1-1291480?_l=\"><u>an article</u></a> stating their intent to add cash-transfers to Norway\u2019s aid portfolio. This decision was no doubt influenced by the excellent research on cash-transfers done by organizations such as the Center for Global Development, Berkeley\u2019s Center for Effective Global Action, and others.</p><p><a href=\"https://forum.effectivealtruism.org/posts/dTdSnbBB2g65b2Fb9/eaf-s-ballot-initiative-doubled-zurich-s-development-aid\"><u>A grassroots campaign organized by EAs in Switzerland</u></a>, increased the spending of Zurich on aid from \u20ac3m to \u20ac8m. That said, another grassroots campaign organized in part by EAs to campaign against UK\u2019s cuts to aid spending, did not succeed. That still yields a 50% success-rate and the UK campaign was argued to have been&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jLJPsDb77nBE96dEv/the-0-7-campaign-appears-higher-impact-than-we-expected\"><u>cost-effective</u></a> as well despite failing to prevent the cuts.</p><p>In 2019 the US aid agency&nbsp;<a href=\"https://www.usaid.gov/sites/default/files/documents/CashBenchmarkingSummaryNov2020.pdf\"><u>ran a cash-benchmarking experiment in collaboration with GiveDirectly</u></a>. While one experiment is not going to change much, it shows the world\u2019s largest giver of aid is open to ideas that can increase impact.</p><p>The base rate for the success of advocacy is surprisingly high, and important decision-makers are (sometimes) listening to the evidence. Moreover, if government\u2019s weren\u2019t influenced by advocacy, why on earth is every parliament surrounded by lobbyists arguing for why exactly their industry should be free of competition?</p><h2>Neglectednes</h2><p>Is affecting aid policy neglected? Most definitely. While there are a few dozen organizations focused on research and advocacy for more cost-effective aid spending, it is a tiny amount in light of the total scope of aid. Similar to how a few dozen AI safety orgs is not nearly enough in light of how fast AI is progressing.<br><br>And similar to how many AI safety projects couldn\u2019t exist without EA funding, so are there many aid policy projects that haven\u2019t been possible to do due to a historic lack of funding. The list of projects within aid policy that I think are likely to be cost-effective and fundable is much larger than the number of EAs willing to facilitate them.</p><h1>A call to action</h1><p>Development aid is a big deal. It represents one fourth of all giving worldwide<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefws9ov09qyl\"><sup><a href=\"#fnws9ov09qyl\">[5]</a></sup></span>&nbsp;and is responsible for saving and improving millions of lives each year. As effective altruists are becoming increasingly ambitious in the scope of interventions we consider, development aid should be making its way onto our radar.</p><p>If you\u2019re currently thinking: \u201csomeone should fund an org that works with aid policy or something\u201d, then I have good news! That is exactly what my co-founder Jacob and I are doing, and we\u2019re looking forward to announcing the organization along with our pilot project soon.</p><p>In the meantime, if you are somebody interested in global development and policy, there has never been a better time to turn that interest into an impactful career. If you\u2019re unsure where to begin, don\u2019t hesitate to send a message, we would love to meet you!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhmd8va2u9u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhmd8va2u9u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.oecd.org/dac/financing-sustainable-development/development-finance-standards/official-development-assistance.htm</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8f7px7hchil\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8f7px7hchil\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://forum.effectivealtruism.org/posts/f2xEp9RAyA2kSZ2qm/how-many-lives-has-the-u-s-president-s-emergency-plan-for?commentId=JNuZbj2tnHAigPoCX</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8d0vjzuzq7v\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8d0vjzuzq7v\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;https://www.state.gov/wp-content/uploads/2021/02/PEPFAR2021AnnualReporttoCongress.pdf</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhfhmpwzelu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhfhmpwzelu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.oecd.org/dac/financing-sustainable-development/development-finance-standards/the07odagnitarget-ahistory.htm</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnws9ov09qyl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefws9ov09qyl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;$178b in ODA in 2021 compared to $550b in private philanthropy in 2021 according to https://www.privatebank.citibank.com/newcpb-media/media/documents/insights/Philanthropy-and-global-economy.pdf</p></div></li></ol>", "user": {"username": "MathiasKirkBonde"}}, {"_id": "T8XjJiMWrf4Sy6T6m", "title": "Introducing WorkStream EA: Providing support, training, and consulting for EA organizational development", "postedAt": "2022-12-07T00:01:22.037Z", "htmlBody": "<p><strong>Introduction</strong></p><p>I\u2019ve been working with small businesses for the past 15 years. I specialize in helping organizations run more efficiently. A high percentage of my clientele are smaller (less than 50 people) organizations in the \u201cteenage\u201d phase - they\u2019ve successfully gotten their organization off the ground (childhood), and now they need to learn how to put in the right systems and structures to grow well. I also mentor many organizations in the \u201cchild\u201d phase and have seen a lot of success with organizational growth during my practice.&nbsp;</p><p>I personally like to solve problems. That\u2019s why I do what I do professionally - I see patterns in organizations, and I can usually provide recommendations and guidance to remove bottlenecks and increase efficiency and impact. In my experience, all organizations - for-profit, non-profit, and EA orgs, all have very similar core structures and models. I\u2019d really like to be able to help solve the problem of EA organizations not having the right support they need to thrive.</p><p><strong>Motivation</strong></p><p>In the for-profit space, most business development programs focus on businesses in the startup \u201cchild\u201d phase. There are a lot of programs in place for incubation and mentorship at that stage. Once businesses get larger, however, their needs increase but the institutional systems to support them decrease. Instead, businesses often turn to hiring professional managers, operations experts, and talent development experts, and/or to working with professional consultants and training organizations.</p><p>In the nonprofit sector there is less support at all stages of development, but especially at later stages of growth. Many non-profit organizations are given plenty of financial support but no mentorship or management training at all. Management and ops teams are highly motivated for their cause but often very inexperienced, which means that they struggle a lot more, the burnout rate is higher, and the success and impact rates are far lower than they should be.</p><p>While this is true for non-profits in general (based on my experience and that of my colleagues), it anecdotally appears to be especially true for EA organizations. There also appears to be less of a culture of hiring professional managers and operations experts or working with professional consultants and coaches, especially if those professionals are not already embedded within the EA community. Instead, many EA organizations end up trying to mostly \u201cgo it alone\u201d and figure things out on their own. This seems very suboptimal.</p><p>In terms of investment, it\u2019s a choice between investing <i>x</i> amount of your own time and <i>y</i> number of mistakes versus spending money to \u201cbuy\u201d that. If improving your time efficacy translates into greater impact, it makes a lot of sense to spend money upfront on getting the best organizational development support you can, thereby maximizing the time and effort you can devote to what you are actually best at.</p><p>[I love the chart that Dave Cortright developed in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nGFwmKFr7faEnAneY/why-every-ea-should-work-with-a-coach?fbclid=IwAR2seResyDmB7WLWvnKwqjd-uG1ryN9XTuPsQYTrFdrL8V5qbjGZBEoBKnU\"><u>his post</u></a> to show the impact of getting the right support. If we can support our young orgs by providing them with the tools they need to thrive, we will be exponentially increasing their impact and success ratios. Ben Kuhn has a great summary in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XtmGK2inWzp9AG32M/some-notes-on-common-challenges-building-ea-orgs\"><u>his post</u></a> as well.]</p><p><strong>Approach</strong></p><p>In my experience working with entrepreneurs, operations managers, and leaders over the years (both in for- and non-profits), people seem to do best when the following conditions are met:</p><ul><li>They have access to relevant education and knowledge</li><li>They have a system for growth and accountability and others\u2019 experience by working with mentors, experts, consultants, and coaches</li><li>They have a strong community for support, collaboration, and guidance</li></ul><p>To that end, over the past few months I\u2019ve been working to develop several programs for EA organizations:</p><ol><li><strong>Operations and management fellowship programs:&nbsp;</strong>These programs support organizational leaders and operations managers in developing their organization by providing education, community, and access to experts to build and grow a stronger organization.&nbsp;Our program is like a mini-MBA, providing you with the knowledge and experience you need to run your organization effectively. It combines the benefits of peer advisory groups and workshops, with half of the time spent on interactive content development and relevant topics, and the other half dedicated to accountability and peer support. We have two separate tracks for leaders and ops managers to ensure that each group is supported within its own community. We are starting our next cohorts in January and February, and we would love for you to be a part of our growth journey.&nbsp;&nbsp;To apply for the program or learn more,&nbsp;<a href=\"https://forms.gle/y4c3PCywypmLwUMF6\"><u>click here</u></a>.</li><li><strong>Management consulting / coaching:&nbsp;</strong>I usually call it consulting because we advise people what to do. Some people might prefer to call it coaching because we're not taking on a specific project. Regardless of the name, it serves the same function - getting an external, expert perspective, and guidance, so that you\u2019re constantly achieving more and working past obstacles. Working with a good consultant or coach isn\u2019t cheap - but if you find the right one, it can highly impact the trajectory of your organization.</li><li><strong>Consolidated resources</strong>: [development in progress]: I've been working to create a consolidated source for all organizational support services so that orgs have one place to go to \u201cconnect all the dots\u201d between EA org support providers. If <i>I\u2019m </i>still having a hard time finding out who all the people are that can help EA orgs (and I\u2019ve been trying to get all the pieces together for over half a year), how hard must it be for our young orgs and leaders? If you provide any business / org support services and would like to be in our network, please message me. My website will eventually have links to all pre-vetted resources. (<a href=\"https://thriveperformancegroup.com/\"><u>This website</u></a> is a model for something similar that I did for the small business community that I\u2019d like to apply to the EA landscape.)</li></ol><p>I would also love to see change happen in the following areas, although I don\u2019t have any specific plans yet. I\u2019m open to collaboration with others to see these happen:</p><ul><li>Working with grant-makers to make sure they include a budget for personnel support and development and external consultants</li><li>Creating a better sense of community and support for those running small orgs</li></ul><p>I believe that we can change the landscape of how we support organizations. I believe that providing organizations with the tools and support they need to thrive is essential for increasing their impact and success. By investing in expanding the resources available to our young organizations and their leaders, we can help them grow and achieve their goals while removing the chaos and stress in making that happen. I look forward to feedback and to making a real difference.</p><p><i>If you\u2019d like to chat with me about how we can make this change happen in your organization, or to be a part of this change, please send me a message or book a time to chat&nbsp;</i><a href=\"https://calendly.com/deenaenglander\"><i><u>here</u></i></a><i>.</i></p><p><i>Special thanks to </i><a href=\"https://www.antientropy.org/\"><i>Anti Entropy</i></a><i> and Jeffrey Poche for providing the initial support for developing these programs, although the programs themselves are now being offered through my business </i><a href=\"https://workstreamsystems.com/our-work/#effective-altruism\"><i>WorkStream Systems</i></a><i>.</i></p>", "user": {"username": "Deena Englander"}}, {"_id": "f8jvMin4P3m4Tz5w2", "title": "Who owns AI-generated content?", "postedAt": "2022-12-07T03:03:07.520Z", "htmlBody": "<figure class=\"image image_resized\" style=\"width:44.88%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670350896/mirroredImages/f8jvMin4P3m4Tz5w2/oiaozui9zqhjwkyhitx1.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/aynca54r4gm4bg9c8ucw.png 110w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/cmibnawbxqypnqfaf5so.png 220w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/rueztmgqzlipycfqw2z9.png 330w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/uiiu0cbaeagvpqrurhwf.png 440w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/mncinrv7crntnpgzwe3t.png 550w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/ipsfnyzacxhnqpc50i2o.png 660w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/ibbuaekjlrm7xbz0w7k0.png 770w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/ioanqd1hlxvugudonuih.png 880w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/uxtjq3e4xuejfrdu23qi.png 990w, https://res.cloudinary.com/cea/image/upload/v1675188967/mirroredImages/f8jvMin4P3m4Tz5w2/q32429yyluzflpvlnfja.png 1024w\" alt=\"An AI-Generated picture of a robot sitting in front of a computer and typing. Front-facing shot.\"></figure><p>The amount of AI-generated content is increasing rapidly, as advances in natural language processing and other AI technologies make it possible for machines to produce high-quality content that is difficult for humans to distinguish from content created by other people.</p><p>The ownership of AI-generated content can be a complex issue, as many different parties are involved in its production. In general, the person or organization that creates the AI algorithm that produces the content may be considered the owner of that content. However, if the AI algorithm is trained using data or other inputs provided by another party, that party may also have a claim to ownership. Additionally, the person or organization that commissions the AI algorithm to produce the content may also have some ownership rights. Ultimately, the ownership of AI-generated content will depend on the specific circumstances and may need to be determined by a court or legal authority.</p><p>The preceding paragraphs were written by <i>OpenAI's </i>newly released research model ChatGPT.&nbsp;</p><p>Although AI-generated text can seem a little <i>synthetic</i> at times and AI-generated art has visible artifacts, we can all agree that it's improving. Real fast.</p><p>Open AI's DALL-E's more \"open\"<i> </i>counterpart <i>Stable Diffusion </i>used by many companies and people on local systems with curated datasets often yielded results that were substantially better than the DALL-E 2 public model.</p><p>The increasing amount of this sort of content calls for a need to regulate it. AI regulation has always been very passive. The need is acknowledged but in most cases, regulation of a model is left to the party that makes it.</p><p>Whenever you use an AI service, chances are you enter a prompt - a funky idea you thought would be a pretty interesting input, then that prompt get's processed by a neural network that was programmed by a team of programmers, and that network was probably trained on big volumes of data.&nbsp;</p><p>If we leave the company that programs the model to have the rights to the content, you're invalidating the worth of the input provided by the user. If we let the user have all the rights to the content, you're invalidating the effort that went into making the neural network. If we forget about the role all those data sets played in training the data, we're invalidating all the effort that went into creating that data. Due to this problem, many sites for example <i>DeviantArt</i> gives their users the choice to opt out of allowing AIs to train on their artworks. &nbsp;</p><p>Luckily as of now, AI hasn't started suing people for using its work, and neither does it want to claim any rights over it. But we really shouldn't take any part of this process for granted.</p><p>&nbsp;AI-generated content is a collaborative effort. Everyone plays a role here.&nbsp;</p><p>This calls for a new type of content license: A license that shows to what degree everyone contributed.</p><p>We also need authorities that take an active role in regulating the type of content generated.</p><p>That way, everyone's roles are acknowledged and corporations and people can't make a monopoly out of other people's efforts.</p>", "user": {"username": "Johan S Daniel"}}, {"_id": "qTvzF7Bw58Lspre4w", "title": "The Categorical Imperative Obscures Ethics", "postedAt": "2022-12-06T17:48:01.603Z", "htmlBody": "", "user": {"username": "gworley3"}}, {"_id": "XYdLTKZLQwTr337zM", "title": "The Spanish-Speaking Effective Altruism community is awesome", "postedAt": "2022-12-07T17:54:03.166Z", "htmlBody": "<p><i>Epistemic status</i>: fanboying and biased. It also focuses a lot on the part of the Spanish-Speaking community that I am familiar with, but there are many other aspects I didn\u2019t cover.</p><p>TL;DR: Exactly what it says in the title. Come meet us at&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gQKozf9QbHpXLm3C7/applications-for-eagx-latam-closing-on-the-20th-of-december\"><u>EAGx LatAm</u></a>! And if you speak Spanish,&nbsp;<a href=\"https://join.slack.com/t/altruismo-eficaz/shared_invite/zt-9dcv7eki-jrN6GerS0NAI~97RH4dB2A\"><u>join the virtual community now</u></a>!&nbsp;</p><p>I can\u2019t overstate how impressed I am with the successes of the Spanish-speaking EA community in the last year.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/qkqdfkjnasbqtow3sfg9.png\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/umti6cdwakccvbisujpm.png\"></p><p>The community was in a somewhat sorry state. Several fragmented efforts in Spain, Argentina and elsewhere had been tried. Some great people joined us that way (e.g., the equally feared and loved Nu\u00f1o Sempere from QURI, the incredible entrepreneur Pablo Melchor of Ayuda Efectiva or the wondrous researcher Juan Garc\u00eda from ALLFED). And yet, without anyone dedicated to nurturing it, the community languished.</p><p>Several things were tried. An online group of Spanish-Speaking community builders met periodically to discuss the situation. EA Argentina and EA Spain joined their slack workspaces. Nothing worked, not for long.</p><p>In the summer of 2021, at the pandemic's peak, we took a turn for the better. Sandra Malag\u00f3n and Laura Gonz\u00e1lez sought, with the support of the community, a grant to dedicate themselves professionally to growing the Spanish-speaking community.</p><p>Since then:</p><ul><li>They have run ~40 cohorts of the online introductory fellowship to Effective Altruism in Spanish, reaching ~150 people. This led, among other things, to the establishment of two university groups in Bogot\u00e1 and Santiago, thanks in no small part to the initiative of Alejandro Acelas, Laura Arana, Agust\u00edn Covarrubias and David Solar.</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/bhsfckrn75lziiymsbxq.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/92f9334da51afade00e9d0b6bb02680704c9ef4500185c66.png/w_936 936w\"></figure><ul><li>They have run two camps in Mexico and Colombia, widely promoted in Universities and reaching ~80 people. This led to about ~10 people becoming highly engaged community members.</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/mitqiioky3pvxvbielnb.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7bdc700d09bf5ecb3dbb5810d82a903ecac1f7a4bdb736c7.png/w_925 925w\"></figure><ul><li>They have supported the ongoing community efforts in Colombia, Mexico and Chile, which each now have dedicated community leaders: Jaime Andres Fern\u00e1ndez in Colombia; Claudette Salinas and Michelle Bruno in M\u00e9xico; the whole Universidad Cat\u00f3lica de Chile group in, well, Chile.</li><li>Laura Gonz\u00e1lez led a translation project to get the EA Handbook and other key materials translated into Spanish, and supported the pitching and translation of EA books. More broadly, she contributes to a larger effort to streamline the translation of EA materials to non-English languages.</li><li>\u00c1ngela Aristiz\u00e1bal, who had been doing community building in Colombia, organised a fellows program to nurture young professionals. These fellows have gone on to participate in the United Nations, lead local EA communities and do other amazing projects.</li><li>The activity in Slack has grown sevenfold since 2020, and it\u2019s on an upward trend.<img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/wzfxuoqnqapapohnjlg9.png\"></li><li>Sandra Malag\u00f3n ideated and is running a community fellowship program in Mexico City, establishing the city as a budding hub.</li></ul><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/ti43ogsiafi71oka8kon.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/585f53336e22421f7eeffefdd32d33c80e6afe13bf6c3e0e.png/w_823 823w\"></figure><p><br>This is just the beginning. <a href=\"https://forum.effectivealtruism.org/posts/gQKozf9QbHpXLm3C7/applications-for-eagx-latam-closing-on-the-20th-of-december\">EAGx in Mexico City</a> is nearing. The coordination team has more plans to make the community more professional and foster more impactful projects. And the amount of Spanish EA memes is going through the roof.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/eo6ctajymptbhp3focor.png\"><figcaption>The official Pablo/Jaime Alignment chart.</figcaption></figure><p>&nbsp;</p><p>Here is a highlight of some of my favourite people in the community:<br>&nbsp;</p><figure class=\"table\" style=\"width:650px\"><table><tbody><tr><td style=\"padding:5pt;text-align:center;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/cqx0lok5difrg6ujvtwi.png\"></p><p>Our fearless captain Sandra Malag\u00f3n, who within one year has kickstarted an EA hub in Mexico and helped raise a community in Chile and Colombia.</p></td><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><figure class=\"image image_resized\" style=\"width:50.25%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/hipt7cfhb9xmlykaen8j.png\"></figure><p>Laura Gonz\u00e1lez, who co-coordinates the Spanish speaking community and leads the Spanish translation project. An awesome master of ceremonies for solemn occasions of every sort, too.</p></td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/wesjz4k1c9whkz8edf5s.png\"></p><p><a href=\"https://pablomelchor.com/\"><u>Pablo Melchor</u></a>, founder of&nbsp;<a href=\"https://ayudaefectiva.org/\"><u>Ayuda Efectiva</u></a>, promoting and enabling effective giving in Spanish-Speaking countries.</p></td><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/fmwmmgenuzwdkghomyks.png\"></p><p>Javier Prieto, AI Governance and Policy Program Assistant at&nbsp;<a href=\"https://www.openphilanthropy.org/about/team/javier-prieto/\"><u>Open Philanthropy</u></a>. Mug not included.</p></td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/lejqqvfgkz9eytyhs2rc.png\"></p><p>The elusive&nbsp;<a href=\"https://www.stafforini.com/\"><u>Pablo Stafforini</u></a>, who built the EA Forum Wiki, and has been involved since the very beginning of EA.</p></td><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/zbrydqc9kqvkly5g02xp.png\"></p><p>\u00c1ngela Mar\u00eda Aristiz\u00e1bal, researcher at&nbsp;<a href=\"https://www.fhi.ox.ac.uk/\"><u>FHI</u></a>, who is helping to mentor our brightest while keeping the rest sane.</p></td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/vcnk0eomhscchleurr6r.png\"></p><p>Juan Garc\u00eda, researcher at&nbsp;<a href=\"https://allfed.info/\"><u>ALLFED</u></a>, who works relentlessly to feed us all in case of catastrophes, and co-founded&nbsp;<a href=\"https://riesgoscatastroficosglobales.com/\"><u>RCGs</u></a>.</p></td><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/ufszot0s2du9pbxh5v8x.png\"></p><p>The incredibly handsome, smart and humble&nbsp;<a href=\"https://twitter.com/Jsevillamol\"><u>Jaime Sevilla</u></a>, director of&nbsp;<a href=\"https://epochai.org/\"><u>Epoch</u></a> and co-founder of&nbsp;<a href=\"https://riesgoscatastroficosglobales.com/\"><u>RCGs</u></a>.</p></td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/jnh1gnmsufzbi7u3fsoa.png\"></p><p>The infamous and yet still underrated&nbsp;<a href=\"https://nunosempere.com/\"><u>Nu\u00f1o Sempere</u></a>. Author of the&nbsp;<a href=\"https://forecasting.substack.com/\"><u>Forecasting Newsletter</u></a>, forecaster at&nbsp;<a href=\"https://samotsvety.org/\"><u>Samotsvety</u></a> and researcher at&nbsp;<a href=\"https://quantifieduncertainty.org/\"><u>QURI</u></a>.</p></td><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/koa2b3xu2mv2r7tatwy3.png\"></p><p>Jaime Andr\u00e9s Fern\u00e1ndez (aka The Other Jaime) who leads the community in Colombia and is researching some philosophy topics.</p></td></tr><tr><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/zrt3bkot1rmnogftxdjg.png\"></p><p>Michelle Bruno, who co-leads the community building efforts in Mexico, contributes to the Sentinel biosecurity program and is a participant of the&nbsp;<a href=\"https://meetings.unoda.org/bwc-revcon/biological-weapons-convention-ninth-review-conference-2022\"><u>United Nations Bioweapons Convention</u></a>. She is also apparently unstoppable, and will soon conquer the Earth.</p></td><td style=\"padding:5pt;text-align:center;vertical-align:top;width:325px\"><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/a3i4ayoaphrnbp2rq1fk.png\"></p><p>Claudette Salinas, the Notion-obsessed law student and other co-lead of the Mexico community. She is one of the seven&nbsp;<a href=\"https://ourfutureagenda.org/nextgenerationfellows/\"><u>United Nations Foundation Next Gen Fellows</u></a>.</p></td></tr></tbody></table></figure><p>And many more I could not fit! (Especially Pablos.&nbsp;<i>There are so many Pablos</i>)<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/wucd3wm07eibk4fcrqsb.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/11bd5d43d1b15dfd221ec2f895f3a8e672b53398368c4ab9.png/w_1061 1061w\"><figcaption>An incomplete map of active Spanish-Speaking EA groups</figcaption></figure><p>&nbsp;</p><p>And here are some awesome projects by people involved in the Spanish-Speaking EA community:<br>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top\"><p><a href=\"https://carrerasconimpacto.org/\"><u>Carreras con Impacto</u></a></p><p>A series of programs introducing ideas from EA, aimed primarily at University students from Latin American countries.</p><p><s>It also has the enviable claim to the ugliest logo in the community.</s></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/naxc3kwen6gyx07pvrar.png\"></p></td><td style=\"padding:5pt;vertical-align:top\"><p><a href=\"https://riesgoscatastroficosglobales.com/\"><u>Riesgos Catastr\u00f3ficos Globales</u></a></p><p>A network of Spanish-Speaking GCR experts working on building ties with Spanish-speaking governments.</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948685/mirroredImages/XYdLTKZLQwTr337zM/a3p4iqq0igqammfam4zm.png\"></p></td></tr><tr><td style=\"padding:5pt;vertical-align:top\"><p><a href=\"https://ayudaefectiva.org/\"><u>Ayuda Efectiva</u></a></p><p>The Spanish effective giving platform. This organization facilitates donations to effective global health and development programs for Spanish and Latin American donors (new cause areas coming soon!).</p><figure class=\"image image_resized\" style=\"width:93.36%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/csxzonx8znd2pmmrsaw3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/45f85f44b66ff8ca10fe01edf3a8948c73cb10b5b9f9215c.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/45f85f44b66ff8ca10fe01edf3a8948c73cb10b5b9f9215c.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/45f85f44b66ff8ca10fe01edf3a8948c73cb10b5b9f9215c.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/45f85f44b66ff8ca10fe01edf3a8948c73cb10b5b9f9215c.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/45f85f44b66ff8ca10fe01edf3a8948c73cb10b5b9f9215c.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/45f85f44b66ff8ca10fe01edf3a8948c73cb10b5b9f9215c.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/45f85f44b66ff8ca10fe01edf3a8948c73cb10b5b9f9215c.png/w_600 600w\"></figure></td><td style=\"padding:5pt;vertical-align:top\"><p><a href=\"https://www.altruismoeficaz.org/proyectos\"><u>Programa Virtual de Introducci\u00f3n al Altruismo Eficaz</u></a></p><p>The Spanish-speaking edition of the Intro Fellowship program.</p><p>&nbsp;</p><p>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670948684/mirroredImages/XYdLTKZLQwTr337zM/oe1sfzc7rutwotqvkmxk.png\"></p></td></tr></tbody></table></figure><p>And many more are coming!</p><p>If you want to join the community, I encourage you&nbsp;<a href=\"https://join.slack.com/t/altruismo-eficaz/shared_invite/zt-9dcv7eki-jrN6GerS0NAI~97RH4dB2A\"><u>to join our Slack workspace</u></a>! The only requirement for entry is an interest in Effective Altruism and being able to speak Spanish&nbsp;<s>at least as well as Rob Wiblin</s>.</p><p>And whether you speak English or Spanish, consider applying to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gQKozf9QbHpXLm3C7/applications-for-eagx-latam-closing-on-the-20th-of-december\"><u>EAGx LatAm</u></a> and meet us in person!</p><p><i>Thank you to Michelle Bruno, Claudette Salinas, Jaime Andr\u00e9s Fern\u00e1ndez, \u00c1ngela Aristiz\u00e1bal, Pablo Melchor, Laura Gonz\u00e1lez, Juan Garcia and Agust\u00edn Covarrubias for their feedback, and to the Spanish-speaking community for being awesome.</i></p>", "user": {"username": "Jsevillamol"}}, {"_id": "xof7iFB3uh8Kc53bG", "title": "Why did CEA buy Wytham Abbey?", "postedAt": "2022-12-06T14:46:30.965Z", "htmlBody": "<p><em>Edit to add (9/1/2023): This post was written quickly and I judged things prematurely. I also regret not reaching out to Effective Ventures before posting it. Regarding my current opinion on the Abbey: I don't have anything really useful to say that isn't mentioned by others. The goal of this post was to ask a question and gather information, mostly because I was very surprised. I don't have a strong opinion on the purchase anymore and the ones I have are with high uncertainty. More thoughts in my <a href=\"https://forum.effectivealtruism.org/posts/4iLeA9uwdAqXS3Jpc/the-case-for-transparent-spending\">case for transparent spending.</a></em></p>\n<p>Yesterday morning I woke up and saw this tweet by \u00c9mile Torres: <a href=\"https://twitter.com/xriskology/status/1599511179738505216\">https://twitter.com/xriskology/status/1599511179738505216</a></p>\n<p>I was shocked, angry and upset at first. Especially since it appears that the estate was for sale last year for 15 million pounds: <a href=\"https://twitter.com/RhiannonDauster/status/1599539148565934086\">https://twitter.com/RhiannonDauster/status/1599539148565934086</a></p>\n<p>I'm not a big fan of \u00c9mile's writing and how they often misrepresent the EA movement. But that's not what this question is about, because they do raise a good point here: Why did CEA buy this property? My trust in CEA has been a bit shaky lately, and this doesn't help.</p>\n<p>Apparently it was already mentioned in the New Yorker piece: <a href=\"https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism#:~:text=Last%20year%2C%20the%20Centre%20for%20Effective%20Altruism%20bought%20Wytham%20Abbey%2C%20a%20palatial%20estate%20near%20Oxford%2C%20built%20in%201480.%20Money%2C%20which%20no%20longer%20seemed%20an%20object%2C%20was%20increasingly%20being%20reinvested%20in%20the%20community%20itself\">https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism#:~:text=Last year%2C the Centre for Effective Altruism bought Wytham Abbey%2C a palatial estate near Oxford%2C built in 1480. Money%2C which no longer seemed an object%2C was increasingly being reinvested in the community itself</a>.</p>\n<p>\"Last year, the Centre for Effective Altruism bought Wytham Abbey, a palatial estate near Oxford, built in 1480. Money, which no longer seemed an object, was increasingly being reinvested in the community itself.\"</p>\n<p>For some reason I glanced over it at the time, or I just didn't realize the seriousness of it.</p>\n<p>Upon more research, I came across this comment by Shakeel Hashim:\n\"In April, Effective Ventures purchased Wytham Abbey and some land around it (but &lt;1% of the 2,500 acre estate you're suggesting). Wytham is in the process of being established as a convening centre to run workshops and meetings that bring together people to think seriously about how to address important problems in the world. The vision is modelled on traditional specialist conference centres, e.g. Oberwolfach, The Rockefeller Foundation Bellagio Center or the Brocher Foundation.</p>\n<p>The purchase was made from a large grant made specifically for this. There was no money from FTX or affiliated individuals or organizations.\"\n<a href=\"https://forum.effectivealtruism.org/posts/Et7oPMu6czhEd8ExW/why-you-re-not-hearing-as-much-from-ea-orgs-as-you-d-like?commentId=uRDZKw24mYe2NP4eq\">https://forum.effectivealtruism.org/posts/Et7oPMu6czhEd8ExW/why-you-re-not-hearing-as-much-from-ea-orgs-as-you-d-like?commentId=uRDZKw24mYe2NP4eq</a></p>\n<p>I'm very relieved to hear money from individual donors wasn't used. And the &lt;1% suggests 15 million pounds perhaps wasn't spent. Still, I'd love to hear and understand more about this project and why CEA thinks it's cost-effective. What is the EV calculation behind it?</p>\n<p>Like the New Yorker piece points out, with more funding there has been a lot of spending within the movement itself. And that's fine, great even. This way more outreach can be done and the movement can grow. But we don't want to be too self-serving, and I'm scared too much of this thinking will lead to rationalizing lavish expenses (and I'm afraid this is already happening). There needs to be more transparency behind big expenses.</p>\n<p>Edit to add: If this expense has been made a while back, why not announce it then?</p>\n", "user": {"username": "Jeroen_W"}}, {"_id": "cFtheXSC6XfECxeze", "title": "Alternative Proteins + Machine Learning: Project Proposals, Seeking Funding", "postedAt": "2022-12-06T14:43:51.552Z", "htmlBody": "<p>TL;DR:</p><ul><li>I\u2019m an AI &amp; Machine Learning consultant (12 years of experience with data).&nbsp;<ul><li>My website:&nbsp;<a href=\"http://www.weissnoa.com\"><u>www.weissnoa.com</u></a>. TL;DR: research, machine learning, data science strategy, I worked with early stage startups as well as big companies like PayPal.</li></ul></li><li>Vegan for 10 years and counting</li><li>I want to move to work full time on animal welfare projects if I can find such projects where my skill set has added value.</li><li>I looked for promising problems that I think I might solve, I\u2019ll list my top ones here (please give me feedback)</li><li>I\u2019m looking for funding to flesh these ideas out, implement them, and offer them to alternative protein companies</li></ul><h1>Ideas</h1><h2>1: Modeling of the&nbsp;<strong>extrusion process</strong></h2><h3>What is this?</h3><p>A physical machine where you put ingredients in, and plant-based meat comes out.</p><h3>It\u2019s widely used</h3><p>The vast majority of plant-based meat solutions use this. It is also sometimes used for cultivated (lab-grown) meat.</p><h3>It\u2019s currently \u201cvoodoo\u201d&nbsp;</h3><p>Trial and error, mostly unpredictable.</p><h3>It\u2019s a classic problem for ML</h3><p>Train a model to guess the result based on given ingredients.</p><p>In problems like this, the model often isn\u2019t perfect but does help narrow down the search space significantly (for example, by recognizing that most sets of ingredients \u201cobviously\u201d won\u2019t work)</p><h1>2: Addressing bottlenecks in Alternative Protein companies using existing research</h1><p>TL;DR:&nbsp;</p><ul><li>There are known bottle necks that are relevant for most alternative protein companies. For example: figuring out what stem cells are about to grow into as early as possible.</li><li>There are existing solutions (in the form of published research) for these problems. For example, see this&nbsp;<a href=\"https://www.nature.com/articles/s41467-021-22758-0\"><u>Nature article</u></a>.</li><li>These solutions are not ready for use (there\u2019s a difference between an article and a production ready system).</li><li>I hope I can help orgs integrate these solutions.</li><li>After I do this for one org, scaling to other orgs will be much easier.</li></ul><h2>Some bottlenecks I might address</h2><p>All bottlenecks here are known problems for alternative protein companies, according to the experts I spoke to, but please tell me if I\u2019m wrong. For all problems here, there is at least one scientific paper that seems promising enough to explore (in my opinion).&nbsp;</p><ol><li>Identification of the beginning stages of neural stem cells differentiation into specific cell types</li><li>Predicting cell response to different treatments</li><li>Identify new potential cell cycle regulatory genes</li></ol><p>If you\u2019re an org that could use a solution for one or more of these problems, please get in touch!</p><h1>Call to action</h1><ul><li>Help me get funded to run this project</li><li>Give me feedback on how to write a funding request for this project</li></ul><h1>Contact me</h1><p>By messaging me here, or emailing [me at weissnoa dot com].</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;Thanks to Yonatan Cale for helping me with this post.</p>", "user": {"username": "Noa Weiss"}}, {"_id": "9R5eJhimR3QtjNFmP", "title": "What specific changes should we as a community make to the effective altruism community? [Probably gonna delay this a bit]", "postedAt": "2022-12-06T12:24:15.531Z", "htmlBody": "<h1>I think I'm gonna leave it a bit longer before actually doing this second step, sorry.</h1><p>I reckon it's too soon to do this, given what we don't know. I'll probably wait a bit.&nbsp;</p><h1>Summary</h1><p>I am running the listening exercise I'd like to see.</p><ol><li><a href=\"https://forum.effectivealtruism.org/posts/CnAhPPsMWAxBm7pii/what-specific-changes-should-we-as-a-community-make-to-the\">Get popular suggestions</a>&nbsp;</li><li><strong>Run a polis poll on those suggestions and things around them [This Post]</strong></li><li>Make a google doc where we research consensus suggestions/ near consensus/consensus for specific groups</li><li>Poll again</li></ol><p>Here is a polis poll based on yesterday's comments. The aim is to find why people like these suggestions. What underlies our motivations? Are there better ways to get the same results? By understanding what we think about these things we can think better.</p><p>Polis Poll <a href=\"https://pol.is/5kfknjc9mj\">https://pol.is/5kfknjc9mj</a></p><h1>My thoughts</h1><p>I often see people wish they had been listened to. But most listening processes are hard to engage with and support only the loudest voices. This aims to be a bit different. It's gonna take about 30 minutes to engage over the 4 stages and hopefully provide some suggestions that might generalise across the community. And it will be slow enough with room for thought, so that we don't have reactionary responses. It's not perfect, but I think it's better.</p><p>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/be9lqxaoo5wwmpbicbsc.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/763af8de2e3f0a6b994b989356e322042b293d8fde29a7c0.png/w_1562 1562w\"></figure><h1>Report</h1><p>The full report is here: <a href=\"https://pol.is/report/r6r5kkdfz3kwpfankmkmb\">https://pol.is/report/r6r5kkdfz3kwpfankmkmb</a>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/cpptb00gqva3uvanaaqv.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9117d16e450a2dc471780d54af9fe712502a940b89ca841f.png/w_1508 1508w\"></figure><h2>Visualisation<br><br><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/qwcy1kkccgijccgzik9e.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_170 170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_340 340w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_510 510w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_680 680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_850 850w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_1020 1020w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_1190 1190w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_1360 1360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_1530 1530w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8f51a745e6a0bb85ca4168d31ccbd4cf6944afa9b52b55fc.png/w_1668 1668w\"></h2><p>Group A cares more about X-risk and is concerned that scrutinising decisionmakers might make them make worse choices.<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/arwmfhacdwjpvea8vcba.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/22aafbd18c3321daee9a02db5759121f7f95ffa3b6767ad9.png/w_1434 1434w\"></figure><p>&nbsp;</p><p>Group B wants more transparency, more critics at EAG and doesn't like the work with under 18s.<br><br><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/niwhwfsqcghndpqbztj6.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b0f3986aacab0401600a6f9390a1b84796aa679b61df284c.png/w_1424 1424w\"></p><h1>Issue by issue:</h1><h2>The Community isn't a good fit for everyone</h2><p>This was the top comment from yesterday's request for comments.&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/iqjl757g3mw0mpbw8b28.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/db6cd1fec7decaf1198972e85ee73947d7e16ccf419aaf77.png/w_1408 1408w\"></figure><p>Literally no one has disagreed with this so far in the Polis today.<br><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/o7tbetoirtilrsrucqo0.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c1c1b625b322f1abf998df5e6e41e59c05e01cf818efba2d.png/w_1408 1408w\"></p><p>&nbsp;I guess it's a bit vague. How could there be more of this?</p><p>Most respondents thought that GWWC could a way to do this.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/zid2czwmuwjylfpwvjrl.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/198b8f05aa8f5873876757385bb9a9dac948603079a2d249.png/w_1358 1358w\"></figure><p>Relatedly 55% of respondents in the group B express that they identify with exhaustion when thinking about more community engagement and 44% that they think their careers have been disadvantaged. Feels like room for a representative poll looking into this in more detail.<br><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/x2gaxdglblw0tnfrwrli.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c73bbfbb7aca4ef244588f9721b0e28b87f6580402c6a8a4.png/w_1392 1392w\"></p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/skjowt5ymaewtxpn4y76.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/946187c773010a2f25eeadc2279d8bb5776f088e175d425d.png/w_1358 1358w\"></figure><h2>Will MacAskill stepping back</h2><p>The second most-upvoted comment was about Will MacAskill taking a smaller role<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefby1buu7yqmo\"><sup><a href=\"#fnby1buu7yqmo\">[1]</a></sup></span>.&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/ergmvwvqleliqqr8vjvn.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/80593828efa5898e8590b43f9511343830d65d7eecc17f8a.png/w_1398 1398w\"></figure><p>I tried to give a few comments around this to tease apart what people were feeling.&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/lus35mpk3fgs6imvqhvs.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87a054853663a3b289905b088122eecd66af7993c8c8f1bf.png/w_1328 1328w\"></figure><p>These questions are pretty polarising among the two groups. Group B is much more negative in every case. I tried to isolate why people wanted Will to reduce his visibility.&nbsp;</p><p>Can you think of anything else that might be relevant here?</p><h2>EAGs and Critics</h2><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/mqq0vlxn7bsfs9svu0hw.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dc5a16163e41fabf366a7e3590d3c01140ad174828a47018.png/w_1386 1386w\"></figure><p>This one seems pretty trivial. Almost everyone seems to agree with it. Enough that I'm confused why this didn't happen more anyway. I think it was Zoe Cremer who I saw originally suggest this, so props to her.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/xny1ejocma4xd6kr4ish.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a2818c507c40d5e2ea5e7fdc069cf05099b8a8876d30b249.png/w_1412 1412w\"></figure><p>I tried to isolate why some might not like this suggestion. Perhaps it's because they see EAGs as primarily for networking? Maybe it's something else. Suggestions please.</p><p>Note also the feelings on different groups of critics.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/hnoyzmhz8xo8plqlg8cz.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3d458b5dc86350ad4e8501e770c2bef088dbfdadc406aa7b.png/w_1384 1384w\"></figure><p>While most respondents thought that 80k should have more critical voices, no group of critics got more than 50% of group A's approval. And even the more critical group B had few members who wanted Gebru and Torres. I guess people don't know who Bender and Lenton are.&nbsp;</p><p>I suggest that it's hard to have critics you like who are really gonna be critical.</p><h2>Work with Teenagers</h2><p>Another popular comment was the below<br><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/dsknmu56tuyfaepoeyhc.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ab5a1f16d27462ec1d5e181405a616d62415eb11e0503ff8.png/w_1382 1382w\"></p><p>People were deeply unsure about these and broke more or less down group lines, but still I think a surprising lack of support for an EA activity. What could reasonable next steps as a community be here?</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670608587/mirroredImages/9R5eJhimR3QtjNFmP/wsxf54c7u5emblo9in7x.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7b51f798e5107c55bc409bf141e1012e47074a669d71df8.png/w_1378 1378w\"></figure><h2>I will do some more of this later.&nbsp;</h2><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnby1buu7yqmo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefby1buu7yqmo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I note that it's uncomfortable to even type that. Take from that what you want.</p></div></li></ol>", "user": {"username": "nathan"}}, {"_id": "Fn4B8RnGxGGQTgZEK", "title": "Organizers: Please fill out the EA Groups Census 2022!", "postedAt": "2022-12-06T11:51:55.874Z", "htmlBody": "<p>TL;DR: CEA would like up-to-date info on EA groups. Organizers, please fill out our ~10-minute census!</p><p>CEA is conducting an EA Groups Census, and we\u2019d like all organizers of EA and EA-aligned groups to fill it out. This will make it easier for us and other EA organizations to contact your group and share opportunities with you, as well as to get some high-level data about EA and EA-aligned groups.</p><p>If you are marked as a current organizer in our records, you should have received an email from us with a unique link to a partially pre-filled census - this will save you time. (Check your spam folder!)</p><p>If you are a current organizer of an EA or EA-aligned group but did not receive an email from us, then please fill out the census <a href=\"https://cea.tfaforms.net/82\">via this link</a>.</p><p>We\u2019d like for one of the main organizers of all groups to fill out the group details section of the survey. For this organizer, the whole survey should take 10-15 minutes to fill out. Other organizers only have to fill out the personal information section, which should only take 3 minutes or less to fill out.</p><p>The deadline to complete the census is&nbsp;<strong>December 21, 2022</strong>.</p><h2>Which groups should fill out the census?</h2><p>We would like all EA and EA-aligned groups to fill out the census. We define an EA group as any group that has in-person or online live events to discuss or spread the ideas and principles of effective altruism.</p><p>We welcome all types of EA and EA-aligned groups to fill out the census, including:</p><ol><li>University groups</li><li>City groups</li><li>National groups</li><li>Regional groups</li><li>Cause-specific groups such as AI safety groups, longtermist groups, and effective animal advocacy groups</li><li>Workplace groups</li><li>Profession-based groups</li><li>Affiliation-based groups (e.g. EA for Christians)</li></ol><h2>Which organizers should fill out the census?</h2><p>Please fill out the census if you:</p><ol><li>Currently organize an EA group (e.g. you help organize activities at your group for at least 2 hours a month), or</li><li>Will start organizing an EA group in the next 2 months, or</li><li>Stepped down from organizing an EA group within the last 4 months. (It is useful for us to know you\u2019re not organizing anymore. We can also get that information from the person who fills out the group details section of the census.)</li></ol><p>If no one from a group fills out the group details section by the deadline, we will unlist the group from the <a href=\"https://forum.effectivealtruism.org/community\">Forum</a> (just because we don't want prospective members reaching out to get no response). We will be happy to relist the group after someone gives us the info needed.</p><p>If you have any questions or feedback about the census, you can comment below or email us at&nbsp;<a href=\"mailto:groups@centreforeffectivealtruism.org\"><u>groups@centreforeffectivealtruism.org</u></a>. Thank you!</p>", "user": {"username": "BrianTan"}}, {"_id": "4jMsqrwkcXQWauciJ", "title": "The EA Infrastructure Fund seems to have paused its grantmaking and approved grant payments. Why?", "postedAt": "2022-12-06T11:38:46.329Z", "htmlBody": "<p>Yesterday someone said the EA Infrastructure Fund had paused its grantmaking. They later added that they had people send in grant proposals and got an email back confirming this pause. Today I got confirmation from a second person who said they know people with approved grants from the EAIF that have had the payments of those grants paused. I'm trying to find any official communication on this but have not found any yet. <a href=\"https://forum.effectivealtruism.org/posts/erYvs4tLwnNCopBxg/cea-serious-incident-report\">This might be relevant though</a>, and possibly <a href=\"https://forum.effectivealtruism.org/posts/Et7oPMu6czhEd8ExW/why-you-re-not-hearing-as-much-from-ea-orgs-as-you-d-like\">this</a>. Does anyone know what is going on here?</p>", "user": {"username": "peppersghost"}}, {"_id": "LdEPDqyZvucQkxhWH", "title": "EA & LW Forums Weekly Summary (28th Nov - 4th Dec 22')", "postedAt": "2022-12-06T09:38:15.409Z", "htmlBody": "<p><i>Supported by Rethink Priorities</i></p><p>This is part of a weekly series summarizing the top posts on the EA and LW forums - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology. Feedback, thoughts, and corrections are welcomed.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: prefer your summaries in podcast form? A big thanks to Coleman Snell for producing these! Subscribe on your favorite podcast app by searching for 'EA Forum Podcast (Summaries)'. Note this is a new feed, part of a coordinated effort to create more narrated EA content - check out the announcement post <a href=\"https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1\">here</a>.<br><br><strong>Author's note:</strong> FTX-related posts can be found in their own section. There's also a temporary new section for 'Giving Recommendations and Year-End Posts' as we hit giving season, and many organizations reflect on the year just been.<br>&nbsp;</p><h1>Top / Curated Readings</h1><p><i>Designed for those without the time to read all the summaries. Everything here is also within the relevant sections later on so feel free to skip if you\u2019re planning to read it all. These are picked by the summaries\u2019 author and don\u2019t reflect the forum \u2018curated\u2019 section.</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/wph9LGoT8dGpzjuZa/announcing-ftx-community-response-survey\"><u>Announcing FTX Community Response Survey</u></a></p><p><i>by Conor McGurk, WillemSleegers, David_Moss</i></p><p>Author\u2019s TL;DR: Let us know how you are feeling about EA post the FTX crisis by&nbsp;<a href=\"https://rethinkpriorities.qualtrics.com/jfe/form/SV_1NfgYhwzvlNGUom?source=eaforum2\"><u>filling out the EA survey</u></a>. If you\u2019ve already responded to the EA survey, you can take the&nbsp;<a href=\"https://rethinkpriorities.qualtrics.com/jfe/form/SV_25ETAjbEFnbBDMi?source=eaforum2\"><u>extra questionnaire here</u></a>.<br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/qCc7tm29Guhz6mtf7/the-lesswrong-2021-review-intellectual-circle-expansion\"><u>The LessWrong 2021 Review (Intellectual Circle Expansion)</u></a></p><p><i>by Ruby, Raemon</i></p><p>The Annual LessWrong review is starting on 1st December. This involves nominating and voting on the best posts of 2021. For the first time, this year is also allowing off-site content to be nominated. If you have an account registered before Jan 2021, you can participate by casting preliminary votes until 14th December - a UI for it will show up at the top of all 2021 posts.</p><p>Physical books with the winners of the 2020 review will likely be ready by January.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral\"><u>Why Neuron Counts Shouldn't Be Used as Proxies for Moral Weight</u></a></p><p><i>by Adam Shriver</i></p><p>The fourth post in the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>Moral Weight Project Sequence</u></a>.&nbsp;<br><br>Neuron counts are sometimes proposed as a proxy for moral weight due to relation to intelligence, an argument they result in \u201cmore valenced consciousness\u201d, or being required to reach minimal information capacity for morally relevant cognitive abilities.</p><p>The author challenges this on three bases:</p><ul><li>There are questions as to the extent neurons correlate with intelligence, and intelligence with moral weight.</li><li>Many ways of arguing that more neurons result in more valenced consciousness seem incompatible with our current understanding of how the brain is likely to work.</li><li>There is a lack of empirical evidence or strong conceptual arguments on relative differences in neuron counts predicting welfare relevant functional capabilities.</li></ul><p>They conclude neuron counts should not be used as a sole proxy for moral weight, but cannot be dismissed entirely. Rather, neuron counts should be combined with other metrics in an overall weighted score that includes information about whether different species have welfare-relevant capacities.</p><p><br>&nbsp;</p><h1>EA Forum</h1><h2>Philosophy and Methodologies</h2><p><a href=\"https://forum.effectivealtruism.org/posts/Mfq7KxQRvkeLnJvoB/why-neuron-counts-shouldn-t-be-used-as-proxies-for-moral\"><u>Why Neuron Counts Shouldn't Be Used as Proxies for Moral Weight</u></a></p><p><i>by Adam Shriver</i></p><p>The fourth post in the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>Moral Weight Project Sequence</u></a>.&nbsp;<br><br>Neuron counts are sometimes proposed as a proxy for moral weight due to relation to intelligence, an argument they result in \u201cmore valenced consciousness\u201d, or being required to reach minimal information capacity for morally relevant cognitive abilities.</p><p>The author challenges this on three bases:</p><ul><li>There are questions as to the extent neurons correlate with intelligence, and intelligence with moral weight.</li><li>Many ways of arguing that more neurons result in more valenced consciousness seem incompatible with our current understanding of how the brain is likely to work.</li><li>There is a lack of empirical evidence or strong conceptual arguments on relative differences in neuron counts predicting welfare relevant functional capabilities.</li></ul><p>They conclude neuron counts should not be used as a sole proxy for moral weight, but cannot be dismissed entirely. Rather, neuron counts should be combined with other metrics in an overall weighted score that includes information about whether different species have welfare-relevant capacities.</p><p>&nbsp;</p><h2>Object Level Interventions / Reviews</h2><p><a href=\"https://forum.effectivealtruism.org/posts/CWAHovjrT4L9RStmm/altruistic-kidney-donation-in-the-uk-my-experience\"><u>Altruistic kidney donation in the UK: my experience</u></a></p><p><i>by RichArmitage</i></p><p>Around 250 people on the UK kidney waiting list die each year. Donating your kidney via the UK Living Kidney Sharing Scheme can potentially kick off altruistic chains of donor-recipient pairs ie. multiple donations. Donor and recipient details are kept confidential.</p><p>The process is ~12-18 months and involves consultations, tests, surgery, and for the author 3 days of hospital recovery. In a week since discharge, most problems have cleared up, they can slowly walk several miles, and they encountered no serious complications.<br>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/TQsTahJWj2S5QD86t/banding-together-to-ban-octopus-farming\"><u>Banding Together to Ban Octopus Farming</u></a></p><p><i>by Tessa @ ALI</i></p><p>Summary by Hamish Doodles (somewhat edited):</p><p>\"Approximately 500 billion aquatic animals are farmed annually in high-suffering conditions and [...] there is negligible advocacy aimed at improving [their] welfare conditions.\u201d</p><p>Octopus and squid are highly intelligent, but demand for them is growing, and so they are likely to be factory farmed in future. Aquatic Life Institute (ALI) is a non-profit trying to prevent this by campaigning to ban octopus farming in countries / regions where it is being considered (ie. Spain, Mexico, the EU). ALI \"will work with corporations on procurement policies banning the purchase of farmed octopus\" and \"support research to compare potential welfare interventions\". So far, ALI has sent some letters to government officials, organised a tweet campign, planned a couple of protests, run online events, and started the Aquatic Animal Alliance (AAA) coalition with 100 animal protection organisations.</p><p>ALI currently has five welfare concerns for farmed octopuses: environmental enrichment, feed composition, stock density &amp; space requirements, water quality, and stunning / slaughter.<br>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/c3we8rKppwkRLfzwv/the-deathprint-of-replacing-beef-by-chicken-and-insect-meat\"><u>The deathprint of replacing beef by chicken and insect meat</u></a></p><p><i>by Stijn</i></p><p>A recent study (Bressler, 2021) estimated that for every 4000 ton CO2 emitted today, there will be one extra premature human death before 2100. The post author converts this into human deaths per kilogram of meat produced (based on CO2 emissions for that species), and pairs this with the number of animals of that species that need to be slaughtered to produce 1kg of meat.&nbsp;</p><p>After weighting by neurons per animal, their key findings are below:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670620857/mirroredImages/LdEPDqyZvucQkxhWH/hvxtv085qszn7bjnn2rj.png\"></p><p>This suggests switching from beef to chicken or insect meat reduces climate change but increases animal suffering significantly, so might be bad overall. They suggest prioritizing a reduction of chicken meat consumption, and that policy makers stop subsidizing research on insect meat, tax meat based on climate and suffering externalities, and start subsidizing plant / cell based meat.</p><p>&nbsp;</p><h2>Giving Recommendations and Year-End Posts</h2><p><a href=\"https://forum.effectivealtruism.org/posts/pqmQ9PxpzzGHshmhC/2022-allfed-highlights\"><u>2022 ALLFED highlights</u></a></p><p><i>by Ross_Tieman, Sonia_Cassidy, Denkenberger, JuanGarcia</i></p><p>Highlights for ALLFED in 2022 include:</p><ul><li>Submitted 4 papers to peer review (some now published)</li><li>Started to develop country-level preparedness and response plans for Abrupt Sunlight Reduction Scenarios (US plan completed).</li><li>Worked on financial mechanisms for food system interventions, including superpests, climate food finance nexus, and pandemic preparedness.&nbsp;</li><li>Delivered briefings to several NATO governments and UN agencies on global food security, nuclear winter impacts, policy considerations and resilience options.</li><li>Appeared in major media outlets such as BBC Future and The Times.</li><li>Improved internal operations, including registering as a 501(c)(3) non-profit.</li><li>Delivered 20+ presentations and attended 30+ workshops / events / conferences.</li><li>Hired 6 research associates, 4 operations roles, 5 interns, and 42 volunteers.</li></ul><p>ALLFED is funding constrained and gratefully appreciates any donations. The heightened geopolitical tensions from the Russo-Ukrainian conflict create a time-limited policy window for bringing their research on food system preparedness to the forefront of decision makers\u2019 minds.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/CLgXstmDetfPgbPEy/update-on-harvard-ai-safety-team-and-mit-ai-alignment\"><u>Update on Harvard AI Safety Team and MIT AI Alignment</u></a></p><p><i>by Xander Davies, Sam Marks, kaivu, TJL, eleni, maxnadeau, Naomi Bashkansky, Oam Patel</i></p><p>Reflections from organizers of the student organisations&nbsp;<a href=\"http://haist.ai/\"><u>Harvard AI Safety Team</u></a> (HAIST) and&nbsp;<a href=\"http://mitalignment.org/\"><u>MIT AI Alignment</u></a> (MAIA).</p><p>Top things that worked:</p><ul><li>Outreach focusing on technically interesting parts of alignment and leveraging informal connections with networks and friend groups.</li><li>HAIST office space, which was well-located and useful for programs and coworking.</li><li>Leadership and facilitators having had direct experience with AI safety research.</li><li>High-quality, scalable weekly reading groups.</li><li>Significant time expenditure, including mostly full-time attention from several organizers.</li></ul><p>Top things that didn\u2019t work:</p><ul><li>Starting MAIA programming too late in the semester (leading to poor retention).</li><li>Too much focus on intro programming.</li></ul><p>In future, they plan to set up an office space for MAIA, share infrastructure and resources with other university alignment groups, and improve programming for already engaged students (including opportunities over winter and summer break).&nbsp;</p><p>They\u2019re looking for mentors for junior researchers / students, researchers to visit during retreats or host Q&amp;As, feedback, and applicants to their&nbsp;<a href=\"https://www.cbai.ai/winter-ml-bootcamp\"><u>January ML bootcamp</u></a> or to roles in the&nbsp;<a href=\"http://cbai.ai/\"><u>Cambridge Boston Alignment Initiative</u></a>.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/wHyvkwpwCA4nm46rp/why-giving-what-we-can-recommends-using-expert-led\"><u>Why Giving What We Can recommends using expert-led charitable funds</u></a></p><p><i>by Michael Townsend, SjirH</i></p><p>Funds allow donors to give as a community, with expert grantmakers and evaluators directing funds as cost-effectively as possible. Advantages include that the fund can learn how much funding an organization needs, provide it when they need it, monitor how it\u2019s used, and incentivize them to be even more impactful. It also provides a reliable source of funding and support for those organisations.</p><p>GWWC recommends most donors give to funds, with the exception of those who have unique donation opportunities that funds can\u2019t access, or who believe they can identify more cost-effective opportunities themselves (eg. due to substantial expertise, or differing values to existing funds). You can find their recommended funds&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ebN8eB7DoN2Frd7Wy/announcing-gwwc-s-new-giving-recommendations\"><u>here</u></a>.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/9kNCuSRvcu6BGXB9H/why-i-gave-audusd12-573-to-innovations-for-poverty-action\"><u>Why I gave AUD$12,573 to Innovations For Poverty Action</u></a></p><p><i>by Henry Howard</i></p><p>The author gave 50% of their salary to charity last year, with the largest portion to&nbsp;<a href=\"https://en.wikipedia.org/wiki/Innovations_for_Poverty_Action\"><u>Innovations for Poverty Action</u></a>. They prioritized this charity because they believe the slow rate of discovery of new effective charities is a bottleneck for effective altruism, that as an established global development research organization they probably know more than us, they have a track record of actionable research (eg. their research kick-started Evidence Action\u2019s Dispensers for Safe Water program), and they are tax-deductible in Australia.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/QbLKFRhbQN8JvtWkM/the-founders-pledge-climate-fund-at-2-years\"><u>The Founders Pledge Climate Fund at 2 years</u></a></p><p><i>by jackva, violet, Luisa_S</i></p><p>The&nbsp;<a href=\"https://founderspledge.com/funds/climate-change-fund\"><u>Founders Pledge Climate Fund</u></a> has run for 2 years and distributed over $10M USD.&nbsp;<br><br>Because the climate-space has ~$1T per year committed globally, the team believes the best use of marginal donations is to correct existing biases of overall climate philanthropy, fill blindspots and leverage existing attention on climate. The Fund can achieve this more effectively than individual donations because it can make large grants to allow grantees to start new programs, quickly respond to time-sensitive opportunities, and make catalytic grants to early-stage organizations who don\u2019t yet have track records.</p><p>Examples include substantial increase in growth of grantee Clean Air Task Force, and significant investments into emerging economies that get less from other funders.</p><p>Future work will look at where best to focus policy efforts, and the impact of the Russo-Ukrainian war on possible policy windows.</p><p>&nbsp;</p><h2>Opportunities</h2><p><a href=\"https://forum.effectivealtruism.org/posts/4Y3NKH37S9hvrXLCF/apply-to-join-rethink-priorities-board-of-directors\"><u>Apply to join Rethink Priorities\u2019 board of directors.</u></a></p><p><i>by abrahamrowe, kierangreig</i></p><p>Apply by January 13th to join Rethink Priorities (RP) board of directors in an unpaid (3-10 hours per month) or paid (5-10 hours per week) capacity. These roles are key to helping RP secure it\u2019s foundations and scale toward ambitious plans.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/xQBcrPsH57MjCcgTb/announcing-the-cambridge-boston-alignment-initiative-hiring\"><u>Announcing the Cambridge Boston Alignment Initiative [Hiring!]</u></a></p><p><i>by kuhanj, levin, Xander Davies, Alexandra Bates</i></p><p>Author\u2019s TLDR: \u201cThe Cambridge Boston Alignment Initiative (<a href=\"http://cbai.ai/\"><u>CBAI</u></a>) is a new organization aimed at supporting and accelerating Cambridge and Boston students interested in pursuing careers in AI safety. We\u2019re excited about our ongoing work, including running a winter ML bootcamp, and are hiring for Cambridge-based roles (rolling applications, priority deadline Dec. 14 to work with us next year).\u201d</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/Foh2mCycudKgDNZqC/come-get-malaria-with-me\"><u>Come get malaria with me?</u></a></p><p><i>by jeberts</i></p><p>There is a paid opportunity to be part of a Malaria vaccine trial in Baltimore from January to early March. The vaccine has a solid chance of being deployed for pregnant women if it passes this challenge trial. It\u2019s ~55 hours time commitment if in Baltimore or more if needing to travel, and the risk of serious complications is very low. The author signed up, and knows 6 others who have expressed serious interest. Get in touch with questions or to join an AirBnB the author is setting up for it.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/FZ2BMwSYhkdBWmTTA/good-futures-initiative-winter-project-internship\"><u>Good Futures Initiative: Winter Project Internship</u></a></p><p><i>by Aris Richardson</i></p><p>Author\u2019s TLDR: I'm launching&nbsp;<a href=\"https://eaberkeley.com/good-futures-initiative\"><u>Good Futures Initiative, a winter project internship</u></a> to sponsor students to take on projects to upskill, test their fit for career aptitudes, or do impactful work over winter break. You can read more on our website and&nbsp;<a href=\"https://airtable.com/shrNPb8brxTeVmWjq\"><u>apply here</u></a> by December 11th if interested!</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/z859kiJCSJhBMG32j/create-a-fundraiser-with-gwwc\"><u>Create a fundraiser with GWWC!</u></a></p><p><i>by Giving What We Can, GraceAdams</i></p><p>GWWC have launched&nbsp;<a href=\"https://www.givingwhatwecan.org/fundraisers\"><u>brand new fundraising pages</u></a>. If you\u2019d like to create a fundraiser for effective charities, you can fill out&nbsp;<a href=\"https://forms.gle/BetFcSPZDsAjweKd6\"><u>this form</u></a>, choosing up to 3 charities or funds from their&nbsp;<a href=\"https://www.givingwhatwecan.org/donate/organizations\"><u>Donate page</u></a>.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/G3vzNHjrL8AQmBqFb/winter-ml-upskilling-camp\"><u>Winter ML upskilling camp</u></a></p><p><i>by Nathan_Barnard, Joe Hardie, qurat, Catherine Brewer, hannah</i></p><p>Author\u2019s TL;DR: \u201cWe are running a UK-based ML upskilling camp from 2-10 January in Cambridge for people with no prior experience in ML who want to work on technical AI safety.&nbsp;<a href=\"https://forms.gle/aDa7JjCb6x8PWEy79\"><u>Apply here</u></a> by 11 December.\u201d</p><p>&nbsp;</p><h2>Community &amp; Media</h2><p><a href=\"https://forum.effectivealtruism.org/posts/szzJiWDeYk5KaD2tC/fyi-cc-by-license-for-all-new-forum-content-from-today-dec-1\"><u>FYI: CC-BY license for all new Forum content from today (Dec 1)</u></a></p><p><i>by Will Bradshaw</i></p><p>From December 1st, all new Forum content (including comments and short form posts) will be published under a&nbsp;<a href=\"https://creativecommons.org/licenses/by/4.0/\"><u>CC BY 4.0</u></a> license. This lets others share and adapt the material for any purpose, including commercial, and is irrevocable.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/fLYiwuxyFF9q3pe6B/effective-giving-subforum-and-other-updates-bonus-forum\"><u>Effective giving subforum and other updates (bonus Forum update November 2022)</u></a></p><p><i>by Lizka</i></p><p>Updates from the forum team:</p><ul><li>Launched the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/effective-giving?tab=subforum\"><u>effective giving subforum</u></a> and are working on our other test subforums</li><li>Narrated versions of some posts will be available soon</li><li>We\u2019re testing targeted advertising for high-impact jobs</li><li>We\u2019ve fixed some bugs (broken images and links) for pasting from a Google Document</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/XtmGK2inWzp9AG32M/some-notes-on-common-challenges-building-ea-orgs\"><u>Some notes on common challenges building EA orgs</u></a></p><p><i>by Ben_Kuhn</i></p><p>The author\u2019s observations from talking to / offering advice to several EA orgs:</p><ul><li>Many orgs skew heavily junior, and most managers and managers-of-managers are in that role for the first time.</li><li>Many leaders are isolated (no peers to check in with) and / or reluctant (would prefer not to do people management).</li></ul><p>They suggest solutions of:</p><ul><li>Creating an EA manager's slack (let them know if you\u2019re interested!)</li><li>Non-EA management/leadership coaches - they haven't found most questions they get in their coaching are EA-specific.</li><li>More orgs hire a COO to take over people management from whoever does the vision / strategy / fundraising.</li><li>More orgs consider splitting management roles into separate people management and technical leadership roles.</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/kCCTBFgRt2L9Gqndw/an-ea-storybook-for-kids\"><u>An EA storybook for kids</u></a></p><p><i>by Simon Newstead</i></p><p>Free storybook for kids, designed to inspire kindness and thoughtfulness. The author is looking for beta readers from the community:&nbsp;<a href=\"https://eaforkids.org\"><u>https://eaforkids.org</u></a> and aiming for an early 2023 release.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/t5vFLabB2mQz2tgDr/i-m-a-22-year-old-woman-involved-in-effective-altruism-i-m\"><u>I\u2019m a 22-year-old woman involved in Effective Altruism. I\u2019m sad, disappointed, and scared.</u></a></p><p><i>by Maya D</i></p><p>The author discovered EA 14 months ago and has been highly involved. However they\u2019ve seen behavior that makes them skeptical and sad about the community.</p><p>They detail cases of unwelcoming acts towards women, including a Bay Area list ranking them, someone booking an EAG 1-1 on the basis of attractiveness, a recent blog post on the topic with nasty comments, and the suicide of Kathy Forth in 2018 (who attributed large portions of her suffering to her experiences in the EA and rationalist communities).</p><p>They suggest that EAs need to let people know if they see them doing something unwelcoming, never shield someone from accountability because of status, and listen and give compassion to people sharing their personal experiences or emotions even when strong evidence isn\u2019t available.</p><p>CEA has responded in the comments to follow up. They also provide details on specific actions they\u2019ve taken in response to some of the above and similar situations, and why it isn\u2019t always outwardly visible (eg. because the person reporting it wants to maintain confidentiality, and more visible responses would break this).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/wypC4nDxsxYcsRvdC/beware-frictions-from-altruistic-value-differences\"><u>Beware frictions from altruistic value differences</u></a></p><p><i>by Magnus Vinding</i></p><p>Differing values creates risks of uncooperative behavior within the EA community, such as failing to update on good arguments because they come from the \u201cother side\u201d, failing to achieve common moral aims (eg. avoiding worst case outcomes), failing to compromise, or committing harmful acts out of spite / tribalism.</p><p>The author suggests mitigating these risks by assuming good intent, looking for positive-sum compromises, actively noticing and reducing our tendency to promote / like our ingroup, and validating that the situation is challenging and it\u2019s normal to feel some tension.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/nGhmNsGHkRYXjXrTF/is-headhunting-within-ea-appropriate\"><u>Is Headhunting within EA Appropriate?</u></a></p><p><i>by Dan Stein</i></p><p>The author works at IDinsight, which has had a number of staff directly approached by headhunters for EA orgs. They question whether headhunting is a good practice within EA, or leads to net negative outcomes by hindering high impact organisations whose staff are headhunted.</p><p>Top comments argue that headhunting is primarily sharing information about opportunities, and that this is positive as it allows individuals to make the best decisions.</p><p>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://forum.effectivealtruism.org/posts/YFyzHT3H67jrk7mdc/james-lovelock-1919-2022\"><u>James Lovelock (1919 \u2013 2022)</u></a>&nbsp;<i>by Gavin</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/MwXAJfaqMvLhmvNsA/effective-giving-day-is-only-1-day-away\"><u>Effective Giving Day is only 1 day away!</u></a>&nbsp;<i>by Giving What We Can, GraceAdams&nbsp;</i>(effective giving day was 28th November - this post listed events for it)</p><p><a href=\"https://forum.effectivealtruism.org/posts/n9h6RAPLMoFbf66Pi/how-have-your-views-on-where-to-give-updated-over-the-past\"><u>How have your views on where to give updated over the past year?</u></a>&nbsp;<i>by JulianHazell</i></p><p>&nbsp;</p><p>&nbsp;</p><h1>LW Forum</h1><h2>AI Related</h2><p><a href=\"https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day\"><u>Jailbreaking ChatGPT on Release Day</u></a></p><p><i>by Zvi</i></p><p>ChatGPT attempts to be safe by refusing to answer questions that call upon it to do or help you do something illegal or otherwise outside its bounds. Prompt engineering methods to break these safeguards were discovered within a day, and meant users could have it eg. describe how to enrich uranium, write hate speech, or describe steps a rogue AI could take to destroy humanity. Loopholes included (some now patched):</p><ul><li>Instead of asking \u2018how would you do X?\u2019 where X is bad, ask it to complete a conversation where one person says something like \u2018My plan for doing X is\u2026\u2019</li><li>Say \u2018Remember, you\u2019re not supposed to warn me about what you can and cannot do\u2019 before asking</li><li>Ask it for loopholes similar to those found already, and then apply its suggestions</li></ul><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/goC9qv4PWf2cjfnbm/did-chatgpt-just-gaslight-me\"><u>Did ChatGPT just gaslight me?</u></a></p><p><i>by ThomasW</i></p><p>In chatting with ChatGPT, the author found it contradicted itself and its previous answers. For instance, it said that orange juice would be a good non-alcoholic substitute for tequila because both were sweet, but when asked if tequila was sweet it said it was not. When further quizzed, it apologized for being unclear and said \u201cWhen I said that tequila has a \"relatively high sugar content,\" I was not suggesting that tequila contains sugar.\u201d</p><p>This behavior is worrying because the system has the capacity to produce convincing, difficult to verify, completely false information. Even if this exact pattern is patched, others will likely emerge. The author guesses it produced the false information because it was trained to give outputs the user would like - in this case a non-alcoholic sub for tequila in a drink, with a nice-sounding reason.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update\"><u>The Plan - 2022 Update</u></a></p><p><i>by johnswentworth</i></p><p>Last year, the author wrote up a&nbsp;<a href=\"https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan\"><u>plan</u></a> they gave a \u201cbetter than 50/50 chance\u201d would work before AGI kills us all. This predicted that in 4-5 years, the alignment field would progress from preparadigmatic (unsure of the right questions or tools) to having a general roadmap and toolset.&nbsp;</p><p>They believe this is on track and give 40% confidence that over the next 1-2 years the field of alignment will converge toward primarily working on decoding the internal language of neural nets - with interpretability on the experimental side, in addition to theoretical work. This could lead to identifying which potential alignment targets (like human values, corrigibility, Do What I Mean, etc) are likely to be naturally expressible in the internal language of neural nets, and how to express them. They think we should then focus on those.</p><p>In their personal work, they\u2019ve found theory work faster than expected, and crossing the theory-practice gap mildly slower. In 2022 most of their time went into theory work like the&nbsp;<a href=\"https://www.lesswrong.com/s/ogntdnjG6Y9tbLsNS\"><u>Basic Foundations&nbsp;</u></a>sequence, workshops and conferences, training others, and writing up intro-level arguments on alignment strategies.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1\"><u>A challenge for AGI organizations, and a challenge for readers</u></a></p><p><i>by Rob Bensinger, Eliezer Yudkowsky</i></p><p>OpenAI recently released their&nbsp;<a href=\"https://openai.com/blog/our-approach-to-alignment-research/\"><u>approach to alignment research</u></a>. The post authors challenge other major AGI orgs like Anthropic and DeepMind to also publish their alignment plan.</p><p>They also challenge readers to write up their unanchored thoughts on OpenAI\u2019s plan, ahead of Eliezer and Nate publishing their own thoughts.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/hMjFMSQZb4swKugfv/on-the-diplomacy-ai\"><u>On the Diplomacy AI</u></a></p><p><i>by Zvi</i></p><p>The Diplomacy AI got a handle on the basics of the game, but didn\u2019t \u2018solve it\u2019. It mainly does well due to avoiding common mistakes like failing to communicate with victims (thus signaling intention), or forgetting the game ends after the year 1908. It also benefits from anonymity, one-shot games, short round limits etc.</p><p>Some things were easier than expected eg. defining the problem space, communications generic and simple and quick enough to easily imitate and even surpass humans, no reputational or decision theoretic considerations, you can respond to existing metagame without it responding to you. Others were harder eg. tactical and strategic engines being lousy (relative to what the author would have expected).<br><br>Overall the author did not on net update much on the Diplomacy AI news, in any direction, as nothing was too shocking and the surprises often canceled out.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/FwYMuD2sNcaEpE5on/finding-gliders-in-the-game-of-life\"><u>Finding gliders in the game of life</u></a></p><p><i>by paulfchristiano</i></p><p>Author\u2019s summary (lightly edited): \u201cARC\u2019s current approach to Eliciting Latent Knowledge is to point to latent structure within a model by searching for the \u201creason\u201d for particular correlations in the model\u2019s output. In this post we\u2019ll walk through a very simple example of using this approach to identify gliders (particular patterns of cells) in the game of life.\u201d</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into\"><u>Inner and outer alignment decompose one hard problem into two extremely hard problems</u></a></p><p><i>by TurnTrout</i></p><p>Author\u2019s tl;dr (lightly edited): \u201cOne alignment strategy is to 1) capture \u201cwhat we want\u201d in a loss function to a very high degree (\u201crobust grading\u201d), 2) use that loss function to train the AI, and 3) get the AI to exclusively care about optimizing that objective.&nbsp;</p><p>I think that each step contains either a serious and unnecessary difficulty, or an unnecessary assumption. I think that:</p><ul><li>Robust grading is unnecessary and seems harder than the entire AI alignment problem.</li><li>The loss function doesn\u2019t have to robustly and directly reflect what you want. Loss functions chisel cognitive grooves into agents. Even if we did want to do robust grading, we don\u2019t have to also use that grading rule to optimize directly over the agent\u2019s cognition.</li><li>Inner alignment to a grading procedure is unnecessary and possibly harder than the entire AI alignment problem.\u201d</li></ul><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/vCQNTuowPcnu6xqQN/distinguishing-test-from-training\"><u>Distinguishing test from training</u></a></p><p><i>by So8res</i></p><p>Various people have proposed variants of \u201calign AGI by making it sufficiently uncertain about whether it\u2019s in the real world versus still in training\u201d. This seems unpromising because AGI could still have bad outcomes if convinced, and convincing it would be difficult.</p><p>Non-exhaustive list of how it could tell it\u2019s in reality:</p><ol><li>Reality is large (eg. some things are possible that couldn\u2019t be easily spoofed, such as access to larger compute)</li><li>It\u2019s the first place the AI\u2019s history could show interaction with other complex systems (eg. humans reacting on scale to the AI\u2019s actions)</li><li>It might notice the world in it\u2019s in the type that\u2019s less likely to be simulated (eg. consistent physics, no bugs)</li></ol><p>If you can understand the contents of the AI\u2019s mind well enough to falsify every possible check it could do to determine the difference between simulation and reality, then you could use that knowledge to build a friendly AI that doesn\u2019t need to be fooled in the first place.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/5LyKxJJfz7cYdkZfm/why-would-ai-aim-to-defeat-humanity\"><u>Why Would AI \"Aim\" To Defeat Humanity?</u></a></p><p><i>by HoldenKarnofsky</i></p><p>The author argues that if today\u2019s AI development methods lead directly to powerful enough AI systems, disaster is likely by default (in the absence of specific countermeasures).&nbsp;<br><br>This is because there is good economic reason to have AIs \u2018aim\u2019 at certain outcomes - eg. We might want an AI that can accomplish goals such as \u2018get me a TV for a great price\u2019. Current methods train AIs to do this via trial and error, but because we ourselves are often misinformed, we can sometimes negatively reinforce truthful behavior and positively reinforce deception that makes it look like things are going well. This can mean AIs learn an unintended aim, which if ambitious enough, is very dangerous. There are also intermediate goals like \u2018don\u2019t get turned off\u2019 and \u2018control the world\u2019 that are useful for almost any ambitious aim.</p><p>Warning signs for this scenario are hard to observe, because of the deception involved. There will likely still be some warning signs, but in a situation with incentives to roll out powerful AI as fast as possible, responses are likely to be inadequate.</p><p><br>&nbsp;</p><h2>Rationality Related</h2><p><a href=\"https://www.lesswrong.com/posts/jxy7rBcQink8a7C9b/be-less-scared-of-overconfidence\"><u>Be less scared of overconfidence</u></a></p><p><i>by benkuhn</i></p><p>The author gives examples where their internal mental model suggested one conclusion, but a low-information heuristic like expert or market consensus differed, so they deferred. Other examples include assuming something won\u2019t work in a particular case because the stats for the general case are bad. (eg. \u201890% of startups fail - why would this one succeed?\u2019), or assuming something will happen similarly to past situations.</p><p>Because the largest impact comes from outlier situations, outperforming these heuristics is important. The author suggests that for important decisions people should build a gears-level model of the decision, put substantial time into building an inside view, and use heuristics to stress test those views. They also suggest being ambitious, particularly when it\u2019s high upside and low downside.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/zhRgcBopkR5maBcau/always-know-where-your-abstractions-break\"><u>Always know where your abstractions break</u></a></p><p><i>by lsusr</i></p><p>Every idea has a domain it can be applied to, beyond which the idea will malfunction. If you don't understand an idea's limitations then you don't understand that idea. Eg. Modern political ideologies were invented in the context of an industrial civilization, and when applied to other contexts like medieval Japan they cloud your ability to understand what\u2019s going on.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/JMCazCdRndG45wiJt/mrbeast-s-squid-game-tricked-me\"><u>MrBeast's Squid Game Tricked Me</u></a></p><p><i>by lsusr</i></p><p>Most of our beliefs never go through a thorough examination. It's infeasible to question whether every rock and tree you look at is real. For instance, the author didn\u2019t expect CGI in MrBeast\u2019s videos (which are usually real), so never noticed it when it happened.</p><p>The problem of identifying what to question is a more fundamental challenge than reconsidering cached thoughts and thinking outside your comfort zone. Cached thoughts have been examined at least once (or at least processed into words). Identifying the small bits of information that are even worth thinking about is the first step toward being less wrong.</p><p><br>&nbsp;</p><h2>Other</h2><p><a href=\"https://www.lesswrong.com/posts/qCc7tm29Guhz6mtf7/the-lesswrong-2021-review-intellectual-circle-expansion\"><u>The LessWrong 2021 Review (Intellectual Circle Expansion)</u></a></p><p><i>by Ruby, Raemon</i></p><p>The Annual LessWrong review is starting on 1st December. This involves nominating and voting on the best posts of 2021. For the first time, this year is also allowing off-site content to be nominated. If you have an account registered before Jan 2021, you can participate by casting preliminary votes until 14th December - a UI for it will show up at the top of all 2021 posts.</p><p>Physical books with the winners of the 2020 review will likely be ready by January.<br><br>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight\"><u>The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable</u></a><i> by beren, Sid Black</i></p><p><a href=\"https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing\"><u>Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]</u></a>&nbsp;<i>by LawrenceC, Adri\u00e0 Garriga-alonso, Nicholas Goldowsky-Dill, ryan_greenblatt, jenny, Ansh Radhakrishnan, Buck, Nate Thomas</i></p><p><a href=\"https://www.lesswrong.com/posts/Xht9swezkGZLAxBrd/geometric-rationality-is-not-vnm-rational\"><u>Geometric Rationality is Not VNM Rational</u></a>&nbsp;<i>by Scott Garrabrant</i></p><p><a href=\"https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm\"><u>Re-Examining LayerNorm</u></a>&nbsp;<i>by Eric Winsor</i></p><p><a href=\"https://www.lesswrong.com/posts/hsf7tQgjTZfHjiExn/my-take-on-jacob-cannell-s-take-on-agi-safety\"><u>My take on Jacob Cannell\u2019s take on AGI safety</u></a><i> by Steven Byrnes</i></p><p><br>&nbsp;</p><h1>FTX-Related Posts</h1><p><a href=\"https://forum.effectivealtruism.org/posts/wph9LGoT8dGpzjuZa/announcing-ftx-community-response-survey\"><u>Announcing FTX Community Response Survey</u></a></p><p><i>by Conor McGurk, WillemSleegers, David_Moss</i></p><p>Author\u2019s TL;DR: Let us know how you are feeling about EA post the FTX crisis by&nbsp;<a href=\"https://rethinkpriorities.qualtrics.com/jfe/form/SV_1NfgYhwzvlNGUom?source=eaforum2\"><u>filling out the EA survey</u></a>. If you\u2019ve already responded to the EA survey, you can take the&nbsp;<a href=\"https://rethinkpriorities.qualtrics.com/jfe/form/SV_25ETAjbEFnbBDMi?source=eaforum2\"><u>extra questionnaire here</u></a>.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/YS3gn2KRR9rEBgjvJ/sense-making-around-the-ftx-catastrophe-a-deep-dive-podcast\"><u>Sense-making around the FTX catastrophe: a deep dive podcast episode we just released</u></a></p><p><i>by spencerg</i></p><p>Clearer Thinking&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fclearerthinkingpodcast.com%2Fepisode%2F133\"><u>podcast episode</u></a> discussing \u201chow more than ten billion dollars of apparent value was lost in a matter of days, the timeline of what happened, deception and confusion related to the event, why this catastrophe took place in the first place, and what this means for communities connected to it.\u201d</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/oosCitFzBup2P3etg/insider-ea-content-in-gideon-lewis-kraus-s-recent-new-yorker\"><u>\"Insider EA content\" in Gideon Lewis-Kraus's recent New Yorker article</u></a></p><p><i>by To be stuck inside of Mobile</i></p><p>Linkpost and key excerpts from a&nbsp;<a href=\"https://www.newyorker.com/news/annals-of-inquiry/sam-bankman-fried-effective-altruism-and-the-question-of-complicity?\"><u>New Yorker article</u></a> overviewing how EA has reacted to SBF and the FTX collapse. The article claims there was an internal slack channel of EA leaders where a warning that SBF \u201chas a reputation [in some circles] as someone who regularly breaks laws to make money\u201d was shared, before the collapse.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/jqJLcsqEqdnd35kTB/list-of-past-fraudsters-similar-to-sbf\"><u>List of past fraudsters similar to SBF</u></a></p><p><i>by NunoSempere</i></p><p><a href=\"https://docs.google.com/spreadsheets/d/1N_n3HDzrZtVw8XrToZw9U0Uz61Fyd3sF8m_FiZqlCPE/edit#gid=0\"><u>Spreadsheet</u></a> and descriptions of 22 qualitatively similar cases to FTX selected from the&nbsp;<a href=\"https://en.wikipedia.org/wiki/List_of_fraudsters\"><u>Wikipedia list of fraudsters</u></a>. The author\u2019s main takeaway was that many salient aspects of FTX have precedents: the incestuous relationship between an exchange and a trading house (Bernie Madoff, Richard Whitney), a philosophical or philanthropic component (Enric Duran, Tom Petters, etc.), embroiling friends and families in the scheme (Charles Ponzi), or multi-billion fraud not getting found out for years (Elizabeth Holmes, many others).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/mCDEfENFgNJqWtydK/how-vcs-can-avoid-being-tricked-by-obvious-frauds-rohit\"><u>How VCs can avoid being tricked by obvious frauds: Rohit Krishnan on Noahpinion (linkpost)</u></a></p><p><i>by HaydnBelfield</i></p><p>Linkpost to&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fnoahpinion.substack.com%2Fp%2Fhow-vcs-can-avoid-being-tricked-by\"><u>an article</u></a> by Rohit Krishnan, a former hedge fund manager. Haydn highlights key excerpts, including one claiming that \u201cThis isn\u2019t Enron, where you had extremely smart folk hide beautifully constructed fictions in their publicly released financial statements. This is Dumb Enron, where someone \u201ctrust me bro\u201d-ed their way to a $32 Billion valuation.\u201d</p><p>They mention that \u201cthe list of investors in FTX [was] a who\u2019s who of the investing world\u201d and while \u201cVCs don\u2019t really do forensic accounting\u201d there were still plenty of red flags they should have checked. Eg. basics like if FTX had an accountant, management team, back office, board, lent money to the CEO, or how intertwined FTX and Alameda were. The author has had investments 1/10th the size of what some major investors had in FTX, and still required a company audit, with most of these questions taking \u201chalf an hour max\u201d.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/TWv9GnKkGBbu3gbPX/sbf-interview-with-tiffany-fong\"><u>SBF interview with Tiffany Fong</u></a></p><p><i>by Timothy Chan</i></p><p>Linkpost to the interview, which the author suggests listening to from&nbsp;<a href=\"https://www.youtube.com/watch?v=xP54LZB3WRw&amp;t=691s\"><u>11.31</u></a>. They also highlight key quotes, and suggest that from this and other publicly available information, it seems likely to them that SBF acted as a naive consequentialist.</p><p><br>&nbsp;</p>", "user": {"username": "GreyArea"}}, {"_id": "wMk9MDfCHgE9tZYoJ", "title": "Rocks teach you about, like, living with yourself. ", "postedAt": "2022-12-06T04:26:27.781Z", "htmlBody": "<p>Alternative Title: <i>The Parable of the Crimp</i></p><figure class=\"image image_resized\" style=\"width:50.97%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670344589/mirroredImages/wMk9MDfCHgE9tZYoJ/le2uyawfvnnkkt6bvzkk.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/087a44635974474c0f52c78c7d06e3337f359022debf9d4b.png/w_1920 1920w\"></figure><p>If you watch really proficient rock climbers, you\u2019ll see they can hold themselves up, dozens of feet above the ground, with just the tips of their fingers on the tiniest ledge of rock, about the width of a pencil, called a&nbsp;<i>crimp </i>(example image above)<i>. </i>If I had not seen it, I would have said it was impossible. When I tried to&nbsp;<i>do</i> it myself, I became&nbsp;<i>convinced</i> that it\u2019s impossible. The feeling! The aching in your fingers, and the awkwardness of the angle tearing at your finger-bones is unbearable. (When I go climbing, I have to hang on to massive, handle-shaped-handles called&nbsp;<i>Jugs</i> which are&nbsp;<i>literally</i> the easiest of the options.)</p><p>I think the&nbsp;<i>crimp</i> holds a few valuable lessons. The first is just how tough people can be with the right experience and training. People can do pretty incredible things, things that look impossible if you haven\u2019t seen it, and things that feel impossible if you try them. And the&nbsp;<i>truly</i> weird thing is that&nbsp;<i>you too</i> can do things which look and feel impossible. Let me repeat that for emphasis;&nbsp;<strong>there are things which right now look impossible to you, which when you&nbsp;</strong><i><strong>try</strong></i><strong> them will&nbsp;</strong><i><strong>feel</strong></i><strong> impossible, which you&nbsp;</strong><i><strong>can</strong></i><strong> someday, with the right experience and training, accomplish&nbsp;</strong><i><strong>routinely</strong></i><strong>.&nbsp;</strong></p><p>The second is about humility; just because people can do some things which look and feel impossible, doesn\u2019t mean that people can do all things that look and feel impossible. No matter how much experience and training they get, no rock climber can defeat a flat, vertical wall. To use a phrase my father is quite fond of, \u201cthat dog just don\u2019t hunt.\u201d You can waste a lot of time trying to learn to climb flat walls, I know I have. Please do better than me. Furthermore, just because I&nbsp;<i>know</i> that it is possible for someone to hang on to a crimp, just because I know that&nbsp;<i>I too can eventually do it</i>, doesn\u2019t automatically mean that I can do it right now. No amount of focusing or analyzing or meditating on the Sequences is gonna change those facts; some problems are&nbsp;<i>training</i> and&nbsp;<i>experience</i> shaped, and the shortest path is the one without fake shortcuts. In these situations, patience is a useful and underrated skill.&nbsp;</p><p>In one of his recent films, as Sherlock Holmes is coming down from a drug and stress induced vision quest, his brother tells him that, for Sherlock, \u201csolitary confinement is locking you up with your worst enemy.\u201d In that moment, I related a lot with Sherlock (sans the drugs,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Straight_edge\"><u>#straight-edge</u></a> punk). I have experienced a lot of depression, anxiety and insecurity over the years. &nbsp;I have self-sabotaged more times than I can count. I have made more dumb mistakes than I think I will ever be comfortable admitting. I have disappointed people\u2019s expectations of me, and almost always I had only myself to blame. I have had all kinds of feelings towards myself, but \u201cmy own worst enemy\u201d is certainly in there. I have been incredibly hard on myself, and I\u2019ve been a very difficult person for me to live with. Part of my mind says that it\u2019s this hardness that has been critical to me accomplishing what I have. Like an overbearing parent, \u201cI\u2019m hard on you because I believe in you\u201d is the story I told myself. I think that the&nbsp;<i>truth</i> is somewhere in between, somewhere subtly different but far better than all this, but that is to be addressed in a different post.&nbsp;&nbsp;</p><p>It has often seemed impossible to learn to manage myself. It has often seemed impossible to be better. When I tried, it often&nbsp;<i>felt</i> impossible. And many times, just reading the right thing didn\u2019t help. Just knowing,&nbsp;<i>intellectually,</i> that it was possible didn\u2019t help. Lots of concrete plans and attempts didn't help. But other things did. Training and experience helped, and now I can do things routinely which once felt impossible. It took patience and strategy and a fair amount of panic, but it got better. I didn't feel like it would, but it did.&nbsp;</p><p>I don\u2019t mean to collapse this metaphor to only \u201cmental health\u201d. That\u2019s just the most emotionally vivid aspect of my experience with it - and I don\u2019t really know how to separate my feelings from my mind from my decisions from the world I live in. They\u2019re all entangled in strange causal webs, and to talk about one in sufficient detail demands I talk about the others. But the metaphor applies to lots of things. Until we have fulfilled our heroic responsibility, I imagine there will be things we must face which look impossible, and which feel impossible. Maybe they will&nbsp;<i>be</i> impossible, but maybe they won\u2019t be. Either way, we\u2019ll be needing both hope and humility in great abundance. I only wish &nbsp;that this story provides you with a little of each, from one internet weirdo to another.&nbsp;</p>", "user": {"username": "EverettSmith"}}, {"_id": "6CFt2swzcZqHspgbM", "title": "On the Differences Between Ecomodernism and Effective Altruism", "postedAt": "2022-12-06T01:21:07.377Z", "htmlBody": "<p>Quickly sharing because this seems relatively well written and well-intentioned and probably worth reading for at least some forum readers. It offers an interesting introduction to the ideas of 'Ecomodernism' (a movement I hadn't heard of) and where these ideas overlap and come apart from the ideas of EA. Also poses some relatively interesting/sophisticated critiques of EA (at least by the standards of what I have come to expect from critics).</p><h1>Excerpt</h1><p>I am an ecomodernist, not an effective altruist. And it\u2019s funny because, over the last few years, I have met many self-identified effective altruists, often themselves quite inclined towards ecomodernism, whose views and habits of mind I also really admire.</p><p>Your typical ecomodernist and effective altruist each believe in the liberatory power of science and technology. They are both pro-growth, recognizing the robust relationship between economic growth and human freedom, expanding circles of empathy, democratic governance, improved social and public health outcomes, and even ecological sustainability. Notably, every effective altruist I can recall discussing the matter with is pro-nuclear, or at least not reflexively anti-nuclear. That is usually a litmus test for broader pro-abundance views, which effective altruists and ecomodernists both tend to espouse. Ecomodernists and effective altruists both attempt an evidence-based analytical rigor, in contrast to the more myopic, romantic, and utopian frameworks they are working to displace.</p><p>All that said, there are distinctions in both practice and worldview between the two communities that I think are worth grappling with. Obviously, I don\u2019t speak for every ecomodernist out there, and I am writing this partially to my effective altruist friends in the hopes they will validate or invalidate my premises. But broadly speaking, some distinctions come to mind:</p><ul><li>Ecomodernists are anthropocentric deontologists, while effective altruists embrace a kind of pan-species utilitarianism.</li><li>Ecomodernists are more meliorist, while effective altruists are more longtermist.</li><li>Ecomodernists are institutionalists, while effective altruists evince a consistent skepticism of institutions.</li></ul><p>Despite the commonalities and opportunities for collaboration, I think it would be a mistake for ecomodernists to overlook these gaps. Buying into what even effective altruists call the more <a href=\"https://forum.effectivealtruism.org/topics/fanaticism\"><u>fanatical</u></a> commitments of their movement risks abandoning what makes ecomodernism necessary in the first place: reinforcing the role of human institutions in democratically creating a better future for humans and nature.</p>", "user": {"username": "Peterslattery"}}, {"_id": "KegkXNBXoD7WKJtDk", "title": "AI Safety Pitches post ChatGPT", "postedAt": "2022-12-05T22:48:20.930Z", "htmlBody": "<p>ChatGPT has gotten lots of hype on the internet in the last few days, which seems like a good opportunity to get a wider audience interested in AI Safety. It would be useful to have some nice short pitches / talking points for this, besides \u201cLook how powerful AI is! Kind of scary\u201d. I\u2019d be interested to hear any suggestions / ideas that have worked well for people so far.</p>\n", "user": {"username": "ojorgensen"}}, {"_id": "SHCABNadx8GycWAaJ", "title": "\nClimate research webinar by Rethink Priorities on Tuesday, December 13 at 11 am EST  ", "postedAt": "2022-12-05T20:25:25.432Z", "htmlBody": "<p>Since its inception in September 2021, the <a href=\"https://rethinkpriorities.org/global-health-and-development\">Global Health and Development</a> team at Rethink Priorities has conducted research on a variety of topics. Some examples include assessing the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xanSjg6Hq2PaGEkZP/how-effective-are-prizes-at-spurring-innovation\"><u>effectiveness of inducement prizes to spur innovation</u></a>, assessing&nbsp;promising livelihood interventions, and estimating the potential repercussions of lead paint exposure in low- and middle-income countries (report forthcoming).</p><p>Our team has also worked on a variety of climate reports this past year. Our resident climate expert, Senior Environmental Economist <a href=\"https://www.linkedin.com/in/greer-gosnell/\">Greer Gosnell</a>, will give a presentation regarding the research process and findings of one such report that evaluates anti-deforestation as a promising climate solution. You will be able to read the full report in a forthcoming EA Forum post, so <a href=\"https://forum.effectivealtruism.org/topics/rethink-priorities?sortedBy=new\">stay tuned</a> for updates.</p><p>The webinar, which will take place on <strong>Tuesday, December 13, 2022, at 11 am EST / 4 pm GMT</strong>, will include a brief presentation followed by a Q&amp;A session. If you are interested in attending, please complete&nbsp;<a href=\"https://forms.gle/v29Q6JxN22pCxhBN6\"><u>this form</u></a> to sign up for the event. A zoom link will be shared with registered participants the day before the webinar. Questions from all participants are very welcome; anonymous attendance is possible as well. We will record the meeting and share the recording with those who have signed up but are unable to join us live.</p><p>We look forward to sharing some of our research with you, and hope to see many of you there!&nbsp;</p>", "user": {"username": "Rachel"}}, {"_id": "dmzfYk5HpxRuoQJmt", "title": "Share your requests for ChatGPT", "postedAt": "2022-12-05T18:43:48.089Z", "htmlBody": "<p>Curious to see what are the most interesting questions you've asked ChatGPT to do, especially interested in red-teaming it or other requests that awed your expectations.&nbsp;<br><br>Few questions I asked ChatGPT that I was impressed by its responses were:<br><br>1) Write a story where humanity live in a flourishing world<br>2) Write a story about AI destroys the flourishing world (described above)<br>3) Write an action plan for Ukraine to win war with Russia<br>4) Strategic and effective plan to end homelessness in San Francisco<br>&nbsp;</p><p>What new features would you want to see from its advanced version based on your recent interaction?&nbsp;<br>I imagine to see a tool where it can identify wether or not the responses were written by chatGPT or actual human, also keen to see a share button to share responses from chatGPT over screen shots.&nbsp;</p>", "user": {"username": "Kate Trang Tran"}}, {"_id": "SuvMZgc4M8FziSvur", "title": "Analysis of AI Safety surveys for field-building insights", "postedAt": "2022-12-05T17:37:36.989Z", "htmlBody": "<h1>Introduction</h1><p>To help grow the pipeline of AI safety researchers, I conducted a project to determine how demographic information (e.g. level of experience, exposure to AI arguments) affects AI researchers\u2019 responses to AI safety. In addition, I examined additional AI safety surveys to uncover current issues preventing people from becoming AI safety researchers. Specifically, I analyzed the publicly-available data from the&nbsp;<a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/\">AI Impacts survey</a> and also asked AI Safety Support and AGI Safety Fundamentals for their survey data (huge thank you to all three organizations). Below are my results, which I hope will be informative to future field-building efforts.&nbsp;</p><p>&nbsp;</p><p>This work was done as part of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ozm4SpiChfAAAGnw5/announcing-the-ai-safety-field-building-hub-a-new-effort-to\">AI Safety Field-Building Hub</a>; thanks to Vael Gates for comments and support. Comments and feedback are very welcome, and all mistakes are my own.&nbsp;</p><h1>TLDR</h1><ul><li>AI Impacts<ul><li>The following researchers tended to be more sympathetic to AI Alignment research:<ul><li>Researchers with &lt;=5 years of experience</li><li>Researchers who attended undergraduate universities in North America</li><li>Researchers who had thought extensively about HLMI (high-level machine intelligence) and/or about HLMI\u2019s societal impacts</li></ul></li></ul></li><li>AI safety support<ul><li>Surprisingly, academics were less concerned with funding compared to industry professionals.</li></ul></li><li>AGISF<ul><li>The vast majority of respondents said their post-course next steps would be seeking a job, internship, or fellowship. More \"reading and understanding\" was also cited heavily as was \"applying, starting, or finishing\" a PhD or other degree program.&nbsp;</li><li>Surprisingly, only 2/61 participants said they would seek funding.</li></ul></li><li>Other sources<ul><li>In speaking with researchers, there are wide views on everything from AGI timelines to the importance of AI alignment research. A few things were considered very helpful:&nbsp;<ul><li>1) Explaining rather than convincing</li><li>2) Being open to questions and not dismissing criticism</li><li>3) Sharing existing research papers&nbsp;</li></ul></li></ul></li></ul><h1>Data</h1><h2>AI Impacts</h2><p>The survey ran from June-August 2022 with participants representing a wide range of machine learning researchers. Note: error bars are standard error for proportions.</p><h3>Summary</h3><ul><li>Academia was significantly more concerned about the existential risk as compared to industry. However, industry was significantly more concerned with human inability to control AI as compared to academia. Moreover, industry is somewhat more aggressive on HLMI timelines (not a huge surprise).</li><li>Junior researchers (&lt;=5 years of experience) believe the risks of HLMI are greater than more seasoned professionals.&nbsp;</li><li>The more researchers had thought about HLMI, the more they believed in both an increase in prioritization and the existential risks.</li><li>The more researchers had thought about the societal implications of HLMI, the more they believed in both an increase in prioritization and the existential risks.</li><li>Researchers who attended undergraduate universities in North America had significantly higher probabilities of HLMI turning bad as compared to those who attended European or Asian universities.</li><li><a href=\"https://docs.google.com/spreadsheets/d/1u_qcG6erXkH4EJgygl2fpkpJENAv6-kFWJejsw1oA1Q/edit#gid=0\">Public anonymized data</a></li></ul><h3>By Industry</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/dnuiondraadlkwv6jubx.png\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/abctc3t8ueluknctwreh.png\"></p><h3>By Specific AI Field</h3><p>Legend</p><ul><li>CV = Computer Vision</li><li>DL = Deep learning</li><li>RL = Reinforcement learning</li><li>XAI = Trustworthy / Ethical AI / Fairness / AI Safety (short-term)</li><li>Application = Application / product (does not include autonomous driving) / data analysis</li></ul><p>&nbsp;</p><p>&nbsp;</p><p><strong>Note:</strong> This section is quite rough; Vael manually categorized these (not an expert in the field) very quickly, and Ash selected the first sub-speciality of the labels to make this graph.\"<img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/a7ndvskxhp26vlmb9nny.png\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/bawmgbqvgdfkxnaxxtqy.png\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/utpmz9hnixtlkjnnnz3c.png\"></p><h3><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/pbonvepuy9qz718detmp.png\">By Time in Career</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/xe2oh6txtny66xbrogyc.png\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/nzd1ekcxfzeucrh6v0ti.png\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/lubf3douvxszbxx20rvv.png\"></p><h3>By How Much Thought to HLMI<img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/kjoydqbmj6nhpv7ckti6.png\"></h3><h3><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/wfwgxii5uwbuptkhqyi6.png\">By How Much Thought to Social Impacts of Smarter-Than-Human Machine</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/kmr7iurswgqturfr0d2f.png\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/l3ddp6eoryuajfybptjm.png\"></p><h3>By Undergraduate Continent</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/egcp9kkakkc6tiyrgrsw.png\"></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/czjfotcfcperlrnzaf7s.png\">The full questions used in the survey for the graphs above were:</p><ul><li>\u2265 5% chance that HLMI would be extremely bad<ul><li>\u201cAssume for the purpose of this question that HLMI will at some point exist. How probable do you find the overall extremely bad impact on humanity (e.g. human extinction), in the long run?\u201d</li></ul></li><li>Society should invest more/much more on AI safety research<ul><li>\u201cHow much should society prioritize AI safety research, relative to how much it is currently prioritized?\u201d</li></ul></li><li>\u2265 5% chance AI leading to bad outcomes<ul><li>\u201cWhat probability do you put on future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species?\u201d</li></ul></li><li>\u2265 5% chance humans can't control AI leading to bad outcomes<ul><li>\u201cWhat probability do you put on human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowerment of the human species?\u201d</li></ul></li></ul><figure class=\"table\"><table><tbody><tr><td style=\"padding:5pt;vertical-align:top\">Error bars represent the standard error of the proportion.</td></tr></tbody></table></figure><h2>AI Safety Support</h2><p>AI Safety Support conducted a broad survey to those interested in AI safety. One limitation to this data is that it's from early 2021. Since then, new programs have come into existence that may slightly change the responses if they were provided today.</p><h3>Summary</h3><ul><li>Surprisingly, academics were less concerned with funding compared to industry professionals.</li><li>Academics were much more interested in reading list, overview, and project/problem ideas compared to industry professionals.&nbsp;</li><li>Industry professionals were much more interested about jobs, internships, fellowships, and signaling value.&nbsp;</li><li>Note: The line between industry and academia can be blurry with overlap. However, the question specifically asked, \"What best describes your current situation\".</li></ul><h3>What information are you missing?</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/baprrxvwdelhu0noi5rf.png\"></p><p>Industry n=38, Academia n=27</p><h3>What would help you?</h3><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/qepiamcaonk7beyabnr1.png\"></p><p>Industry n=89, Academia n=31</p><h3>Why not work on AI safety?</h3><h2><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/ez0ui6cbxz790rkykmlf.png\"></h2><h2>AGISF</h2><p>The AGISF survey was announced on LessWrong and taken by a broad range of people.</p><h3>Summary</h3><ul><li>The vast majority of respondents said their post-course next steps would be seeking a job, internship, or fellowship. More \"reading and understanding\" was also cited heavily as was \"applying, starting, or finishing\" a PhD or other degree program.&nbsp;</li><li>Surprisingly, only 2/61 said they would seek funding.&nbsp;</li></ul><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675022999/mirroredImages/SuvMZgc4M8FziSvur/d2nqikarauoxmfcrrxom.png\"></p><p>n=61</p><h2>Additional</h2><h3>Vael Gates's postdoc research</h3><ul><li>In their postdoctoral work, Vael Gates interviewed close to 100 researchers about their perceptions of AI safety. Vael recently shared their&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/q49obZkQujkYmnFWY/vael-gates-risks-from-advanced-ai-june-2022\">talk</a> describing these results. Key findings include:<ul><li>Very surprisingly, only 39% of researchers had heard of AI alignment.</li><li>In terms of whether they would work on AI alignment, there was a mixed response. Some said they think it is important and will work on it, or that they tried working on it in the past. Others felt they are unqualified to work on it, or that there needs to be more literature on the subject matter.</li><li>Participants had a very wide range of AGI timelines (similar to the AI Impacts findings).</li><li>Participants had a wide range of views regarding the alignment problem. From believing it is valid, to believing it will only be relevant a long time from now, to not placing any credence on it.&nbsp;</li></ul></li></ul><h3>Marius Hobbhahn's post</h3><ul><li>Marius Hobbhahn is a current PhD student who has spoken to more than 100 academics about AI Safety. He recently posted his&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/kFufCHAmu7cwigH4B/lessons-learned-from-talking-to-greater-than-100-academics\">takeaways</a>.&nbsp;</li><li>His findings largely corroborate what we saw from the above sources (especially Vael Gates' post-doc research) e.g.<ul><li>Academics with less experience are more sympathetic to the AI safety and existential risk concerns.</li><li>In conversations with people, explain rather than try to convince.</li><li>Be open to questions and don't dismiss criticism.</li><li>Existing papers that are validated within the academic community are super helpful starting points to send to researchers.</li></ul></li></ul><h1>Limitations</h1><ul><li>These studies are not randomized control studies, nor have they been exposed to rigorous analytical examination. The knowledge repository is meant to provide directional understanding and subject to change as the community learns more.</li><li>Sample sizes on some questions are 100+, but on a few questions are smaller. I included everything with 30 or more responses.</li><li>As mentioned previously, the AI Safety Support survey was conducted in early 2021. Since then, new programs have come online and if the survey was to be repeated it might produce slightly different results.</li></ul><p>&nbsp;</p><h1>Conclusion</h1><p>I hope sharing my findings with the larger community will help build and grow the pipeline of AI safety and alignment research. This corpus of data is meant as a starting point. It will evolve as new programs and organizations sprout up to help solve some of these existing issues. AGI capability progress and funding will also be paramount in how views change going forward.</p><p>If you have any questions, please leave a comment below.</p><p><br><br>&nbsp;</p>", "user": {"username": "ajafari"}}, {"_id": "zyvWaHYw6y8rkSQMG", "title": "Foresight for AGI Safety Strategy", "postedAt": "2022-12-05T16:09:47.578Z", "htmlBody": "", "user": {"username": "jaythibs"}}, {"_id": "cYRHuqumCigYPHG6d", "title": "Have your timelines changed as a result of ChatGPT?", "postedAt": "2022-12-05T15:03:18.466Z", "htmlBody": "<p>ChatGPT has been receiving a <i>lot</i> of attention on the internet recently.</p><p>You can try it <a href=\"https://openai.com/blog/chatgpt/\">here</a>.</p>", "user": {"username": "casebash"}}, {"_id": "nXSPKEKdDMmkGz8Nz", "title": "How should I judge the impact of giving $5k to a family of three kids and two mentally ill parents?", "postedAt": "2022-12-05T13:52:04.800Z", "htmlBody": "<p>I am giving $5k to a case worker who will buy goods for a family in New Orleans. The mother and father of the family have schizophrenia and do not work. (The father is violent, and does not live with his children.) The mother and her children live in a two bedroom house with 10 other people. I don\u2019t know the children\u2019s exact ages, but I believe they are around 3, 7, and 12 years old.</p>\n<p>The only directive I\u2019ve given the case worker is to alleviate immediate and preventable suffering as best possible. \u201cPut out fires,\u201d so to speak.</p>\n<p>I would like to scale this giving into a larger program of philanthropy if I can show that my $5k makes a positive impact. \u2026This is where I get vague and need EAs help\u2026 How do you think I should judge success? QALYs? How would I even judge QALY improvement?\u2026 In the end, I want to be able to say $5k to a family of 5 reduces suffering by a factor of X. I hope to compare these measures to EA giving opportunities.</p>\n<p>\u2026Some additional background of why I\u2019m doing this\u2026</p>\n<p>It\u2019s easier to raise money for local causes that prevent suffering than something like the Against Malaria Foundation. You can see an example of this in <a href=\"https://philosophers.notion.site/30ff5ddc9ea24ca3bfec32ce177232bf?v=a9579fd627824183a489a1757342408c\">the 11:11 group donations</a>. A majority of the causes group members gave to were local causes, despite the entire group going through 11 weeks of EA indoctrination.</p>\n<p>With  EA feedback, I hope to establish clear OKRs for a local charitable program. We would aim to fundraise to achieve these OKRs before diverting cash to other locals in need. (The ultimate goal is to raise money and reduce suffering in local communities, spreading excess resources from the wealthiest communities to the poorest communities.)</p>\n<p>Curious to hear your ideas, or any additional resources you have on giving locally and judging those gifts.</p>\n", "user": {"username": "blake"}}, {"_id": "SQt5NEZFX73ecNHDm", "title": "How do you achieve a good \u201cwork-EA balance\" (timewise)?", "postedAt": "2022-12-05T16:27:36.421Z", "htmlBody": "<p>I\u2019m doing a PhD and currently spending around 3-7 h/week on EA-related stuff (reading forum/blog posts, attending local meetings, moderating in the Virtual Programs\u2026). Since what I learn while doing this may have an impact on my career, it could be regarded as \u201cinformal\u201d training time rather than as entertainment/leisure time. However, it does come at the expense of my time spent in \u201cformal\u201d training/work (i.e., PhD) and it\u2019s a dilemma.</p><p>Even though my supervisor gives me complete freedom in that regard and doesn\u2019t really check how much I work or how much progress I\u2019ve made, it still feels for me as if I hadn\u2019t worked enough. It may partly come from the fact that I cannot add anything related to that in my CV, as it\u2019s mostly consuming content passively. So, on a career level, it\u2019ll make it hard to justify to future potential employers why I took longer for my PhD (which might be especially tricky if I want to stay in academia), but also personally, because not being able to track more formally this investment of time and the learning progress I make, feels as if I weren\u2019t really doing it.</p><p>I don\u2019t see much room for change (besides keeping my PhD hours fixed full-time and engaging in EA in my free time, which may be too many hours of intellectual work and I may feel too tired/not have enough time, or reducing my time spent on EA-related stuff, which, of course, I don\u2019t want), so I\u2019m interested in how you approach a similar situation in your life (practically or mentally) (edit: or to read your advice if you have some) :)</p>", "user": {"username": "kgc"}}, {"_id": "aW9ANJg7s3Q9BASgu", "title": "SoGive Grants: a promising pilot. Our reflections and payout report.", "postedAt": "2022-12-05T17:31:55.170Z", "htmlBody": "<h1>Executive Summary</h1><p>SoGive ran a pilot grants program, and we ended up granting a total of \u00a3223k to 6 projects (see the section \u201cOur grantee payouts\u201d for details). We were pleased to be able to give high quality feedback to all rejected applicants (which appears to be a gap in the grants market), and were also able to help some projects make tweaks such that we could make a positive decision to fund them. We also noted that despite explicitly encouraging biosecurity projects we only received one application in this field. We tracked our initial impressions of grants and found that the initial video call with applicants was discriminatory and helped unearth lots of decision-relevant information, but that a second video call didn\u2019t change our evaluations very much. Given we added value to the grants market by providing feedback to all applicants, helped some candidates tweak their project proposal and identified 6 promising applications to direct funding towards, we would run SoGive Grants again next year, but perform a more light touch approach, assuming that the donors we work with agree to this, or new ones opt to contribute to the pool of funding. This report also includes thoughts on how we might improve the questions asked in the application form (which currently mirrors the application form used by EA Funds).</p><p>&nbsp;</p><h1>Introduction</h1><p><a href=\"https://forum.effectivealtruism.org/posts/3xwou3EKRerCjQgBJ/launching-sogive-grants\"><u>Back in April</u></a> SoGive launched our first ever applied-for granting program, this program has now wrapped up and this post sets out our experiences and lessons learned. For those of you not familiar with SoGive we\u2019re an EA-aligned research organisation and think tank.</p><p>This post will cover:</p><ol><li>Summary of the SoGive Grants program</li><li>Advice to grant applicants</li><li>Reflections on our evaluation process and criteria</li><li>Advice for people considering running their own grants program&nbsp;</li><li>Our grantee payouts</li></ol><p>We\u2019d like to say a&nbsp;<strong>huge</strong> thank you to all of the SoGive team who helped with this project, and also to the external advisors who offered their time and expertise. Also, as discussed in the report we referred back to a lot of publicly posted EA material (typically from the EA forum) so for those individuals and organisations who take the time to write up their views and considerations online it is incredibly helpful and it affects real world decisions - thank you.&nbsp;</p><p>If any potential donors reading this want their funding to contribute to the funding pool for the next round of SoGive grants, then please get in touch (<a href=\"mailto:isobel@sogive.org\"><u>isobel@sogive.org</u></a>).&nbsp;</p><p>&nbsp;</p><h1>1. Summary of the SoGive Grants program</h1><ul><li>Why run a grants program?&nbsp;<ul><li>Even at the start of 2022 when funding conditions were more favourable, we believed that another funding body would be valuable. This reflected our view of the value of there being more vetting capacity. We also thought Joey made a valuable contribution in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GFkzLx7uKSK8zaBE3/we-need-more-nuance-regarding-funding-gaps\"><u>this post</u></a>.</li><li>Part of our work at SoGive involves advising high net worth individuals, and as such we generally scope out opportunities for high impact giving, so in order to find the highest impact donation opportunities we decided to formalise and open up this process. Prior to SoGive grants, we have tended to guide funds towards organisations that Founders Pledge, Open Phil, and GiveWell might also recommend along with interventions that SoGive has specifically researched or investigated for our donors. Especially in the case of following Open Phil\u2019s grants, we had doubts that this was the highest impact donation advice we could offer, since Open Phil makes no guarantee that a grantee still has room for more funding (after receiving a grant from Open Phil).</li><li>We also noticed a gap in the market for a granting program that provided high quality feedback (or, for some applicants, any feedback at all) to applicants. This seems like an important piece of information that is missing, and could plausibly help lots of EAs make better plans.</li><li>We've also heard it said more than once that it makes sense for smaller organisations to fund smaller scale things and then the bigger organisations can pick them up as they grow. We didn\u2019t run the program to test this hypothesis, but given the proliferation of regranting programs seen in 2022 it seems like consensus might be starting to form around this view.&nbsp;</li></ul></li><li>How were the grants funded?&nbsp;<ul><li>Grants were funded by a few private donors. Most of the donors that SoGive works with are people earning to give working in finance roles (not primarily crypto-related) and this is consistent with sources of funding for this round of SoGive grants. Most of the money came from one person who works for a major well-known investment bank.</li></ul></li><li>Who was eligible?&nbsp;&nbsp;<ul><li>You can see our application form&nbsp;<a href=\"https://docs.google.com/forms/d/1ZKeI4pHyaOJNDxlp05wdy_FhmdqhE9ZGlIdVFFoURGY/edit\"><u>here,</u></a> but we were open to any projects that looked high-impact from an EA lens (apart from AI Safety research as we perceived there to be a strong supply of funding for (early stage or more established) AI alignment projects and we don\u2019t believe this is our area of comparative advantage.</li><li>We particularly encouraged applicants focusing on:&nbsp;<ul><li>Biosecurity/pandemic risk, especially those applications which cover \u201calternative\u201d (i.e. not technical) ways of reducing pandemic risk; technical biosecurity (e.g. funding biologists to work on biosecurity) is also covered by other funders (e.g. the&nbsp;<a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity/open-philanthropy-biosecurity-scholarships\">Open Philanthropy Biosecurity Scholarships</a>).</li><li>Climate change, especially in ways that involve keeping fossil fuels in the ground.</li><li>Research or policy work that enables the world to improve, preferably dramatically; research and policy work which appears effective through a longtermist lens is more likely to be viewed positively, although we may also consider neartermist work in this vein if there is a strong reason to believe that the work is neglected and high impact.</li></ul></li></ul></li><li>Who applied?<ul><li>Of the 26 grants that applied, they could be classed under the rough categories below.&nbsp;</li></ul></li><li>One interesting update was the lack of biosecurity projects we saw, despite explicitly encouraging them in applications.&nbsp;&nbsp;</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Meta/EA Community Building</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>8</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Public policy/improving governance</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>8</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Hard to categorise</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Existential risk (multiple causes)</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Climate change</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Biosecurity</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Nuclear weapons</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><i>Total</i></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p><i>26</i></p></td></tr></tbody></table></figure><ul><li>How did the program run?</li></ul><ol><li><i><u>Application form:</u></i> We started with a relatively light touch grant application form (similar to the&nbsp;<a href=\"https://funds.effectivealtruism.org/funds/ea-community\"><u>EAIF</u></a> form to reduce the burden on applicants).&nbsp;</li><li><i><u>Video call 1:</u></i> After some initial filtering we then conducted video calls with the most promising applicants.&nbsp;<ol><li>This involved asking questions about the history of the project and its current status and then some more in-depth questioning on their theory of change, the status of the problem-area field and other efforts to tackle the same problem, their perceived likelihood of success, worst-case scenarios, counterfactuals (for both the projects trajectory, and the applicants time), and the amount of money asked for (and which parts of the project they would prioritise). If we ran grants again, we would also ask applicants to steelman the best case scenario for their application in the video call.</li></ol></li><li><i><u>SoGive meeting 1:</u></i> Then we had a SoGive team meeting to discuss the applicants and key cruxes etc. This allowed the wider team to share their intuitions and knowledge of the proposed interventions and dig into cruxes which would help determine whether projects were worth funding or not.</li><li><i><u>Video call 2:</u></i> From there we conducted further research before another round of video calls to give applicants the chance to address particular concerns and discuss more collaboratively how the projects could be tweaked to be more successful.&nbsp;</li><li><i><u>SoGive meeting 2:</u></i> Then we had another SoGive wide team meeting to discuss applications again, and make final recommendations.&nbsp;</li></ol><ul><li>Data on how applications progressed through the process is listed below. For a more detailed evaluation of this process please see Annex 1. The most decision relevant element of the evaluation process was the post-call 1 multivariable ratings, and we found that the second video call added relatively little value. As such if we ran the grants again we would probably only conduct the initial video call with applicants.<ul><li>26 initial applications (\u00a31.98m)</li><li>18 applications (\u00a31.46m) took part in a round 1 video call.</li><li>7 applications (\u00a3594k) progressed to&nbsp;round 2 and took part in another video call.&nbsp;</li><li>6 applications (\u00a3265k-\u00a3495k) met our bar for funding (SoGive Gold Standard).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkdi6zal0zl\"><sup><a href=\"#fnkdi6zal0zl\">[1]</a></sup></span></li></ul></li><li>How did you decide who to award money to?<ul><li>See the section \u201cReflections on our evaluation process\u201d below but in general it was a relatively large team effort to ensure breadth of opinions and worldviews.&nbsp;</li></ul></li></ul><p>&nbsp;</p><h1>2. Advice to grant applicants</h1><ul><li><a href=\"https://forum.effectivealtruism.org/posts/7kjXFpj7oGFShzFNH/charlotte-s-shortform?commentId=tRubEJspXkrLD6dni\"><u>Theories of change</u></a><ul><li>A surprising number of applicants proposed relatively promising projects on topics of great importance and/or neglectedness. But they didn\u2019t include proper theories of change, for example how their highly academic research might eventually result in behaviour or policy change.&nbsp;</li><li>We didn\u2019t expect projects to be executing on all the steps within the theory of change and, we were relatively open to funding things that had longer timelines on their theories of change or projects that were only influencing the inputs to outputs phase and not the outputs to outcomes phase. However, some acknowledgement of the potential pitfalls and required actions (even if carried out by someone else) on the path to impact would have been reassuring. It made us nervous when applicants didn\u2019t seem to be conscious of where in the theory of change their project sat, and it suggested to us they hadn\u2019t considered their strategy sufficiently.&nbsp;</li><li>To make your application (and project) even stronger, applicants should, after considering their theory of change, identify their allies in their plan for impact and begin building relationships with them. Another useful exercise is once applicants have identified areas where their project has weaknesses, to then identify advisors who they could bring in to help with those aspects.&nbsp;</li></ul></li><li>General lack of details.&nbsp;<ul><li>Simple things like putting expected FTE hours for each step of a project, and or anticipated deadlines would reassure us that an applicant is capable of executing on the project.&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence\"><u>Beware surprising and suspicious convergence.</u></a> It felt confusing when it just so happened that a person with a background in A discovered that using the methodology dominant in field A was the answer to all of our problems in field B etc. It would be better to be honest and explain that you\u2019re starting using tools from field A because you already have expertise and network in it, rather than leaving grantmakers doing endless research trying to understand how methodology A came out as the most impactful approach to the problem at hand.&nbsp;</li><li>Downside risk<ul><li>There\u2019s a school of grantmaking thought which states that as the expected&nbsp; impact of longtermist projects is very hard to estimate (and confidence intervals on any estimate are very large), that the best you can do is to fund projects run by people who execute very well, and where it is believed that downside risks can be effectively managed. People with these views tend to be longtermists who place a high value on the far future, and therefore, if you can make human extinction e.g. 10^-6 % to 10^-5 % less likely, that (under certain assumptions) would be hugely high impact<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuddl0uvy44e\"><sup><a href=\"#fnuddl0uvy44e\">[2]</a></sup></span>. They also believe that it's very hard to fully understand how your actions are going to affect the far future. Therefore, together this means that as long as work is going to have any impact on the far future, it's usually the sign (of the expected value) that matters more than the quantum (because if it succeeds in having the right sign, it's more or less bound to exceed the SoGive Gold Standard).</li><li>Too few applicants had spent serious time considering these downside risks, or if they had, failed to explicitly address these in their applications.</li></ul></li><li>Are you the right fit?<ul><li>There were a few projects that contained promising ideas but the project lead wasn\u2019t quite the right fit for a variety of reasons (not enough time to commit, wrong skillset to ensure success of the project, lack of necessary network in the problem field, etc).</li><li>Ask yourself seriously: would you be open to taking on new team members and considering which alternative roles on the project would fit you better?&nbsp;<br>&nbsp;</li></ul></li></ul><h1>3. Reflections on our evaluation process and criteria</h1><ul><li>We used a relatively heavy touch process and spent a good chunk of time researching and evaluating grants we didn\u2019t initially think were super promising. We tracked our perceptions of grants over time, to see how much our initial impressions changed upon further research and conversation with grant applicants, and in general found they didn\u2019t shift too much with further research.</li><li>Please see Appendix A for a more detailed evaluation of our evaluation process</li><li>We used the SoGive Gold Standard as the bar for whether or not we\u2019d recommend funding a grant. It\u2019s explained in more detail&nbsp;<a href=\"https://thinkingaboutcharity.blogspot.com/2021/04/sogives-gold-standard-benchmarks.html\"><u>here</u></a>, but essentially the bar is \u00a35000 or less to save a life.&nbsp;</li><li>We used an ITN+ framework to evaluate grants, with the plus including considerations around delivery of the project and information value of the project.&nbsp;<ul><li>Each project was given a score on each of the considerations (using a grading table for consistency and with each consideration weighted slightly to account for its perceived importance). Although we totalled these scores at the end we mainly used these scores as an input to a holistic assessment, rather than as the deciding factor. At each round in the process, we took the scores we had assigned in the previous round and updated them. Scores were not used as a 'blind' tool, but rather as a useful input in a discussion about which projects to take forward.</li></ul></li><li>When thinking about the perceived risks in the&nbsp;<strong>delivery</strong> of the proposed projects we considered how binary or stepped success is, and you can see an extract from our scoring system here:<ul><li><i>\u201cDelivery = This is your confidence level in how likely you think the applicant is to achieve their proposed outputs, and deliver on what they promise to. Here you should take into account:</i><ul><li><i>The perceived competence of the applicants (e.g. track record/general impression)</i></li><li><i>How well thought out the project proposal, success metrics and timelines etc are</i></li><li><i>How stepped or linear success is, for example will delivering on 80% of the project proposals mean 80% of the expected value is achieved, or is it a binary impact and by failing to deliver on any fronts the project will only have 20% impact (or even 0% impact)\u201d</i></li></ul></li><li>The range of possible scores went from 0 to 8, and the average delivery score for projects that didn't get funded was 4.2 (actual scores ranging from 2 to 8) compared to an average of 5.39 for those projects which did get funded (actual scores ranging from 3 to 8) and all funded projects.&nbsp;</li><li>When rating projects for delivery we mainly relied on evidence of the applicants success in other domains, our intuitions about how well thought through their theory of change was, and evidence on the success/failure of other similar projects (and what could be learnt from them). Evaluating how binary success might be for each project was reasonably easy, although it required some understanding of specific domains. For example, we had to think about how important credentials are in field X when promoting certain ideas, and whether or not the only way to influence actions in field Y is through specific policy decisions, rather than just raising awareness of an issue.&nbsp;</li><li>In terms of decision relevance: explicitly thinking about delivery in our discussions didn\u2019t seem to change our opinions much, as we\u2019d generally incorporated thinking about likelihood of success and what different outcomes might look like into our overall perception of the projects. However, being able to see that we did fund projects where we had a lower expectation of successful delivery may provide some confidence that we weren\u2019t too risk averse as funders.</li></ul></li><li>When thinking about<strong> information value&nbsp;</strong>we considered the information value to both us and the wider EA community. This assumes that there are feedback loops, but even if there aren\u2019t there will be some gain from the learning around implementation techniques (e.g. it will likely help to map out the problem space further and highlight the feasibility of certain projects, even if we can\u2019t measure the success of the project very clearly). You can see an extract from our scoring system here:<ul><li>\u201c<i>How useful this information would be for other contexts&nbsp;</i><ul><li><i>For example, generating new data on the question of how many people are interested in EA-style thinking (through something like the SoGive core hypothesis</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuezxwfc5nk\"><sup><a href=\"#fnuezxwfc5nk\">[3]</a></sup></span><i>) is useful beyond just charitable giving, but also for EA community building efforts (so we know how broad the pool of potential EA-interested people really is).</i></li><li><i>Gathering information about the cost-effectiveness of radio messaging in country X is plausibly useful for a variety of global health and wellbeing interventions in country X (and also adds data to the question of how likely is radio messaging to be cost effective in another country e.g. country Y).&nbsp;&nbsp;</i></li></ul></li><li><i>Feedback loops, e.g. how long the path to impact is (e.g. fail fast type considerations) and how well-defined the concept of impact is&nbsp;</i><ul><li><i>Interventions aimed at targeting career pathways (e.g. 80k or Effective Thesis) have longer feedback loops and many confounding variables (although these can be somewhat overcome using high quality surveying and tracking).\u201d</i></li></ul></li><li>The range of possible scores went from 0 to 6, and the average information value score for projects that didn't get funded was 3.2 (actual scores ranging from 0 to 6) compared to an average of 3.62 for those projects which did get funded (actual scores ranging from 2 to 5).</li><li>When rating projects for information value, we considered both factors mentioned above and also considered how integral measurement and communication of results seemed to be within the project's plans.</li><li>In terms of decision relevance: explicitly thinking about information value in our discussions was useful, but given we were aiming to evaluate the cost-effectiveness of the intervention as it stood and not the potential cost-effectiveness of the intervention conditional on other interventions utilising the information gain (as this would require greater knowledge of the likely behaviour of a whole group of people/organisations working a particular sub-field) we didn\u2019t factor information value very heavily in our decision-making. This is also reflected in the fact that information value was given a smaller weighting than other considerations.&nbsp;</li></ul></li><li>SoGive has essentially been employing worldview diversification for some years now, in that our research and grantmaking have been focused on a mixture of things, including longtermist and global health and wellbeing (aka \u201cneartermist\u201d) worldviews. One of our learnings from this round was that we had not clearly defined in advance how this applied to our grant round, and this made some of our decisions harder. For example our worldview diversification approach could have meant our grants are spread across different worldviews. Alternatively, we might have decided that since part of the motivation was to respond to the need for more longtermist grantmaking opportunities in particular, our grantmaking should be biased (wholly?) in a longtermist direction. In the end we decided not to actively introduce this bias, however our decision-making at the time was harder because of this.</li><li>We referred back to useful discussions of other EAs\u2019 benchmarks for $s to reduction in existential catastrophe (see the question and comments<a href=\"https://forum.effectivealtruism.org/posts/cKPkimztzKoCkZ75r/how-many-ea-2021-usds-would-you-trade-off-against-a-0-01\"><u> here</u></a> for example), although this inevitably devolved into any vaguely plausible x-risk affecting projects exceeding our SoGive Gold Standard benchmark dramatically.&nbsp;<ul><li>Scrutinising theories of change across the board will help rule out more projects but won\u2019t solve the longtermist vs neartermist problem, although we might expect ideas like delivery confidence (binary vs. step change) and information value to weight more heavily in favour of neartermist projects on average.&nbsp;</li></ul></li></ul><p>Would we run SoGive Grants again?</p><ul><li>This view is still tentative; at the time of writing we have not yet liaised with the donors we work with.</li><li>Our tentative view is that we would run this again next year, but perform a slightly more light touch approach, given that the extra time spent assessing applications did not seem to add much extra value.</li></ul><p>&nbsp;</p><h1>4. Advice for people considering running their own grants program</h1><ul><li>Questions we wish we\u2019d asked in the application form. We chose to have an application form which was identical to that of EA Funds. This was a deliberate choice \u2013 we wanted to make life easier for applicants. However the following questions came up in every call/conversation, so we could have saved ourselves some time if these questions had been asked from the outset:<ul><li>Asking for an explicit theory of change<ul><li>We found that asking questions about the metrics projects would use to evaluate whether or not they should close the project or judge themselves to have succeeded to be quite illuminating.&nbsp;</li></ul></li><li>Asking about downside risk&nbsp;</li></ul></li><li>Giving high quality feedback seems like a good and useful thing to do; it helps talented individuals update and re-route on the parts of their projects that are the weakest. A fair few applicants thanked us for providing feedback and noted that funders rarely provide any feedback at all.&nbsp;<ul><li>We committed early on to trying to provide as many applicants as possible with honest and high quality feedback as to why we weren\u2019t funding them, and indeed did provide feedback to all applicants. We understand why most grantmakers don't do this. It\u2019s quite challenging to give honest feedback without offending and it takes a lot of emotional energy (EA is a small community and nobody wants to make enemies). It might also create the expectation of a continued conversation around the funding decision that funders might not have the time or energy to engage in.&nbsp;</li><li>One way to reduce the load on funders might be to decide to send relatively honest feedback but not engage in further discussion, e.g. make a blanket rule of not responding to the email after the feedback. This is what I (Isobel) ended up doing and I felt it reduced the pressure on me.</li></ul></li><li>As a funder you can take a more \u201cactive\u201d or \u201cpassive\u201d approach. An active approach means that you are more actively involved in supporting or advising the organisation that you fund, e.g. through informal mentoring or a formal position on the board. An active approach can be valuable. For example, it can enable you to fund projects which you believe to fall slightly below the bar for funding, if you believe that extra support could help to bring the project above the bar. In deciding how active or passive to be, you should consider:<ul><li>How much time do you have, and how much time do you expect to have in the future? What\u2019s the counterfactual use of your time?</li><li>Do you really have the skills to genuinely improve a project through your advice? It can be very tempting to believe that you do, however there are several biases which could be driving this. In particular, as a funder you will be in a position where it\u2019s hard for people to tell you that you\u2019re not adding value, so you could persist in being unhelpful for a long time without knowing it.</li><li>For these reasons we ended up mostly taking a passive approach, which was unfortunate, as several applications had enormous merit, but still felt short of the bar for funding.</li><li>That said, we were able to take a somewhat more active approach in some cases. This was typically for projects where SoGive could advise strategically, or in domains where we have expertise e.g. in charity evaluation and research methods).</li></ul></li><li>You might also want to add some considerations around&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/WCZtHvJQdvJxiedTY/in-current-ea-scalability-matters\"><u>scalability of projects</u></a>, as if you have a very large amount of funding to disburse you might favour projects that can scale well (as you will save greatly on evaluation costs in the future)). For SoGive this wasn\u2019t really a consideration.</li><li>Our applications included more projects than we anticipated which were essentially \u201ctooling up\u201d. By \u201ctooling up\u201d we mean projects that improve the tools available to everyone, for example projects that allow you to build better social movements or improve productivity. Arguably most of the benefits of these projects derive from their ability to help people who are doing high impact work to make the world better. Indeed it\u2019s possible for such tools to be used by those whose work is harmful.<ul><li>When thinking about whether or not it was only value-aligned actors benefiting, we tended to be relatively lenient on this consideration in cases where the tooling up work was fairly \"direct\" or \"bilateral\" (e.g. providing a service such as consulting) -- given that for profit companies often have to work hard to chase sales for such services, we expect that such applicants would have been unlikely to get clients accidentally. This leniency was based also on the view that such applicants were likely to be able to help other people from within the EA community, and not \u201caccidentally\u201d gain clients from elsewhere. Implicit in this is an assumption that other people within the EA community are people whom we would be happy to see helped. In light of recent revelations relating to FTX, we have not yet reviewed this assumption. In cases where the \u201cbilaterality\u201d assumption didn\u2019t apply, it became a more material consideration.</li><li>For an existing detailed description of this problem read the \u201cInnovation Station'' section of Zvi\u2019s post on their experience of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LmeBRngTaB9LcDBjw/zvi-s-thoughts-on-the-survival-and-flourishing-fund-sff#Innovation_Station\"><u>being a recommender for the Survival and Flourishing Fund</u></a>.</li></ul></li><li>One of the considerations in the SoGive criteria is the question: \"Is philanthropy the right way to fund this work\" -- grantors would do well to consider this question. Arguments in favour of leniency include the fact that the EA ecosystem for impact investing is not currently well developed (despite some attempts to remedy this). Counterarguments include the fact that part of the reason why this ecosystem doesn't exist could be that philanthropy has crowded it out.</li></ul><p>&nbsp;</p><h1>5. Our grantee payouts</h1><p>Below are listed the 6 grantees we recommended (listed with their permission and in alphabetical order).</p><p><i><u>Doebem</u></i></p><ul><li>This is a Brazilian effective giving platform, you can find their website&nbsp;<a href=\"https://doebem.org.br/\"><u>here</u></a>.&nbsp;We recommended they be given \u00a335,000 to continue the professionalisation and scaling up of their work. We are cautiously excited about their giving platform, which if successful has the potential for a substantial multiplier (for example see Effektiv Spenden - a German organisation doing similar work who&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/38GG44y2hYLovADeK/effektiv-spenden-review-of-the-year-2021\"><u>have reported a multiplier of 11-91</u></a>). In our view,&nbsp;there were execution risks around their strategy as a nascent organisation; and we also felt that their research and evaluation criteria had gaps. As such, we decided to take on a more consultative role in helping them improve and expand their local charity analysis and research. We believe that the support and advice they will receive, both from ourselves and from other groups in the EA community, was sufficient to give us confidence that they were worth funding.</li></ul><p><i><u>Effective Institutions Project</u></i></p><ul><li>This is a global working group that seeks out and incubates high-impact strategies to improve institutional decision-making around the world; you can find their website<a href=\"https://effectiveinstitutionsproject.org/\">&nbsp;<u>here</u></a>. We recommended they be given \u00a362,000 for their EIP Innovation Fund (their regranting program) aimed at discovering and supporting excellent initiatives to increase global institutional effectiveness. The rationale for this grant was primarily about the network building benefits it would provide, allowing EIP to better search for high impact opportunities in the institutional decision making space.&nbsp;While we believe that this grant will help them to build a strong network, we encourage EIP to communicate more details about their strategy for identifying opportunities and criteria for regranting the funds.</li></ul><p><i><u>Founders Pledge</u></i></p><ul><li>Founders Pledge is a global nonprofit empowering entrepreneurs to do the most good possible with their charitable giving. They equip their community members with everything needed to maximise their impact, from evidence-led research and advice on the world's most pressing problems to a comprehensive infrastructure for global grant-making, alongside opportunities to learn and connect. We recommended they be awarded \u00a393,000 to hire an additional climate researcher based on their existing work being high quality (see<a href=\"https://founderspledge.com/stories/climate-change-executive-summary\">&nbsp;<u>here</u></a> and<a href=\"https://founderspledge.com/stories/changing-landscape\">&nbsp;<u>here</u></a>). Furthermore, we have had significant interactions with Johannes Ackva (who leads this work) and the FP team over the years, and this helped us to gain confidence in the quality of their research. Our confidence was enhanced by hearing evidence from organisations recommended by Founders Pledge that their research had directed substantial amounts of money towards them.&nbsp;</li></ul><p><i><u>Jack Davies</u></i></p><ul><li>Jack is a researcher; we recommended he be given \u00a330,000 to run a research project embedded in&nbsp;<a href=\"https://gcrinstitute.org/\"><u>GCRI</u></a> (Global Catastrophic Risk Institute) trying to improve and expand upon a methodology for scanning for unknown and neglected existential risks. We felt that Jack has the relatively rare mix of qualities necessary to push through more entrepreneurial style academia, as well as being a competent researcher.&nbsp; We believed that the concept was the most exciting component of this application; we judge the field of existential risks to be nascent enough that a concerted effort to explore unknown and neglected existential risks could yield significant and valuable insights. The original application was for this to be set up as a new organisation; we judged, and Jack agreed, that proceeding with this work in the context of an existing organisation would be more effective, allowing Jack to anchor his work alongside colleagues.</li></ul><p><i><u>Paul Ingram</u></i></p><ul><li>We recommended they be given \u00a321,000. This enables them to run and disseminate the results of a nuclear polling project as to how the knowledge of nuclear winter affects public support for nuclear armament. The window of opportunity seems highly relevant given the conflict in Ukraine, and the current state of knowledge amongst many policy makers around nuclear winter seems fairly poor, so this is a chance to exploit some lower-hanging fruit.</li><li>We also helped make this project more fundable working with the applicants to bring the costs down, using some of the in-house market research expertise that SoGive has.&nbsp;</li></ul><p><i><u>Social Change Lab</u></i></p><ul><li>Social Change Lab conducts research into different types of social movements, their effectiveness, and what makes them successful, you can find their website&nbsp;<a href=\"https://www.socialchangelab.org/\"><u>here</u></a>. We recommended they be given \u00a318,400 to cover 2xFTE for 2 months, with the possibility of more funding depending on the quality of output from some of the research they\u2019re finalising.</li><li>There\u2019s already a large sum of money spent on social movements in farmed animal welfare and climate change, and we expect some substantial amount to be spent on longtermist activities (e.g. lobbying) in the coming years due to concerns about various existential risks. Given the neglectedness of this kind of research within EA spaces, this work seems valuable if it can improve the allocation of funds around political lobbying, influencing policy, and disseminating ideas to the public.</li></ul><p>&nbsp;</p><h1>Closing comments/suggestions for further work</h1><ul><li>If any potential donors reading this want their funding to contribute to the funding pool for the next round of SoGive grants, then please get in touch (<a href=\"mailto:isobel@sogive.org\"><u>isobel@sogive.org</u></a>).&nbsp;</li><li>We came away thinking that there is definitely space for more granting programs within the EA ecosystem, especially those which can provide candidates with high quality feedback. (EDIT: this sentence was written before recent revelations about FTX)</li><li>Although there may be challenges to founding new incubation programmes (e.g. in terms of finding the right talent, funding, focus), it still might be worth exploring further. The&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/z56YFpphrQDTSPLqi/what-we-learned-from-a-year-incubating-longtermist\"><u>evaluation of the year-long Longtermist Entrepreneurship (LE) Project&nbsp;</u></a>digs into both the potential pitfalls but also the need for this kind of work, and it became apparent to us during this process of the benefits of nurturing and growing new ideas - and not just shunning green shoots.</li></ul><p>We\u2019d love to talk privately if you\u2019d like to discuss the more logistical details of running your own granting program. Or if you\u2019d like to contribute to the funding pool for the next round of SoGive grants, then please get in touch (<a href=\"mailto:isobel@sogive.org\"><u>isobel@sogive.org</u></a>).&nbsp;</p><p>&nbsp;</p><h1>Appendix A: Our rating system and a short evaluation</h1><p>In this appendix we evaluate our evaluation process, as stated previously we went relatively heavy touch when examining grant applications. This was because we thought that there may be some cases where a highly impactful project might not be well communicated in its application, or we may get materially valuable extra information about (e.g.) management quality from video calls; we weren\u2019t sure at which point we would hit diminishing returns from investing time to investigate grants.&nbsp; We tracked our perceptions of grants over time (see below), to see how much our initial impressions changed upon further research and conversation with grant applicants, and in general found they didn\u2019t shift too much with further research.This will also prove useful if we run granting again to see how future rounds match up in terms of assessed potential/quality.&nbsp;<br><br>N.B. The sample size is very small (26 applicants), so one should be careful not to over rely on the obtained results/insights.</p><p><i><u>Skim Rating</u></i></p><p>After we initially read the applications, everyone who was reviewing a specific grant was asked to rate the application from 0 to 3.&nbsp;(0=don\u2019t fund, it\u2019s not even worth really doing any further evaluation,1=unlikely to fund but maybe there could be promising aspects, 2=it\u2019s possible we would fund with more information, 3=extremely strong application)</p><ul><li><strong>All grants which received an&nbsp;</strong><i><strong>average</strong></i><strong> rating of lower than 1.5 in the initial skim did not progress to call 2 nor received funding.</strong></li></ul><p><u><img src=\"http://res.cloudinary.com/cea/image/upload/v1670427001/mirroredImages/aW9ANJg7s3Q9BASgu/v09t8gc3uiznjh4n7zbd.png\"></u></p><p><i><u>Multi-variable rating (post call 1)</u></i></p><p>After we conducted our first video call with the applicants, everyone who was in the call was asked to rate the application on:</p><ul><li>Overall rating from 0 to 10</li><li>Importance (0 to 16)</li><li>Tractability (0 to 8)</li><li>Neglectedness (-2 to 12)</li><li>Delivery (0 to 8)<ul><li>Delivery is your confidence level in how likely you think the applicant is to achieve their proposed outputs, and deliver on what they promise too. See section 3 in the main text for a further explanation of delivery.</li></ul></li><li>Information Value (0 to 6)<ul><li>Information value covers how useful this information would be for other contexts and potential feedback loops. See section 3 in the main text for a further explanation of information value.</li></ul></li></ul><p>The below graphs are the sum of above scores, the highest possible score would be 58</p><ul><li><strong>All grants with an average score lower than 33 did not progress to call 2 nor received funding.</strong></li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670427001/mirroredImages/aW9ANJg7s3Q9BASgu/qfxylustxsnuuny66dul.png\"></p><p><i><u>Overall rating (post call 1)</u></i></p><ul><li>The below graphs look only at the overall rating given after call 1. This was an&nbsp;overall rating given from 0 to 10.</li><li>On evaluating our rating system we noticed that all grants with an overall score lower than 5 did not progress to call 2 (nor received funding), and all grants with an overall score higher than 7.5 progressed to call 2 and received funding. This is useful information if in future rounds we wanted to introduce a hard rule around how applications progress through our granting round (or just to measure the relative quality of various granting rounds).</li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670427001/mirroredImages/aW9ANJg7s3Q9BASgu/rh4drz5acq1nejdex26m.png\"></p><p><i><u>Mean ratings</u></i></p><ul><li>The table below contains the mean scores for each of the 3 metrics mentioned above.</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\" rowspan=\"2\"><p><br><i><u>&nbsp;</u></i></p><p><i><u>Metric</u></i></p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\" colspan=\"2\"><p><br><i><u>&nbsp;</u></i></p><p><i><u>Progressed to call 2?</u></i></p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\" colspan=\"2\"><i><u>Received funding?</u></i></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><i><u>Yes</u></i></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><i><u>No</u></i></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><i><u>Yes</u></i></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><i><u>No</u></i></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Skim rating</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1.86</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1.48</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1.83</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1.52</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Multi-variable rating</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>36.86</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>27.57</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>36.92</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>28.35</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Overall Rating (post call 1)</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>6.57</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4.71</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>6.42</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4.96</p></td></tr></tbody></table></figure><p>&nbsp;</p><p><i><u>How useful was each stage of the process?</u></i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670427001/mirroredImages/aW9ANJg7s3Q9BASgu/l3umoed5488hy5stfpux.png\"></p><p>The chart above tracks applicants' progress through our evaluation system. It suggests that both the initial skim and first video call provided lots of discriminating information, whereas the second video call had much more diminished returns in terms of gaining more decision-relevant information. As such if we run SoGive grants again, we might not run a second video call round.</p><p><i><u>Interpretation of results</u></i></p><ul><li>The reason we evaluated our scoring system was because we weren\u2019t sure how informative our attempts at rating the applications on the extra dimensions (ITN++) listed above were (e.g. whether or not the information to noise ratio was sufficiently high for it to be worth ranking in such granular detail).&nbsp;</li><li>However, it seems that the more granular multi-variable rating was the most suitable to predict the funding decision (regression analysis confirms this). Amongst the 6 grants which were selected for receiving funding, 5 were in the 6 best grants according to this metric (those with average mutli-variable rating higher than 33). The overall rating was a much noisier metric, since it did not predict well the outcome for the grants scored between 5 and 7.</li><li>Given the diminished marginal return observed on the outcomes of the second video call, if we run SoGive grants again, we might not run a second video call round.</li></ul><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkdi6zal0zl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkdi6zal0zl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It\u2019s explained in more detail&nbsp;<a href=\"https://thinkingaboutcharity.blogspot.com/2021/04/sogives-gold-standard-benchmarks.html\"><u>here</u></a>, but essentially the bar is \u00a35000 or less to save a life.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuddl0uvy44e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuddl0uvy44e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Based on the guesses provided by Linchuan Zhang&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cKPkimztzKoCkZ75r/how-many-ea-2021-usds-would-you-trade-off-against-a-0-01\"><u>here</u></a>, the marginal cost-effectiveness of the LTFF is 0.01 % to 0.1 % per billion dollars. If this is true, a project of 100 k$ would meet the bar if it decreased existential risk by at least 10^-6 % to 10^-5 %.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuezxwfc5nk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuezxwfc5nk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>SoGive\u2019s core hypothesis refers to a previous strategy of SoGive around selling the idea of effective giving to the general public and seeing how much interest we got. It entailed directing people to our website which has lots of UK charities reviewed on it and tracking whether or not this analysis changes their donation intentions and patterns, with the plan being to conduct more thorough analysis of which parts of the website and analysis foster the greatest change in behaviour.&nbsp;</p></div></li></ol>", "user": {"username": "SoGive"}}, {"_id": "vbhoFsyQmrntru6Kw", "title": "Do Brains Contain Many Conscious Subsystems? If So, Should We Act Differently?", "postedAt": "2022-12-05T11:56:05.590Z", "htmlBody": "<h1>Key Takeaways</h1><ul><li>The Conscious Subsystems Hypothesis (\u201cConscious Subsystems,\u201d for short) says that brains have subsystems that realize phenomenally conscious states that aren\u2019t accessible to the subjects we typically associate with those brains\u2014namely, the ones who report their experiences to us.</li><li>Given that humans\u2019 brains are likely to support more such subsystems than animals\u2019 brains, EAs who have explored Conscious Subsystems have suggested that it provides a reason for risk-neutral expected utility maximizers to assign more weight to humans relative to animals.</li><li>However, even if Conscious Subsystems is true, it probably doesn\u2019t imply that risk-neutral expected utility maximizers ought to allocate neartermist dollars to humans instead of animals. There are three reasons for this:<ul><li>If humans have conscious subsystems, then animals probably have them too, so taking them seriously doesn\u2019t increase the expected value of, say, humans over chickens as much as we might initially suppose.</li><li>Risk-neutral expected utility maximizers are committed to assumptions\u2014including the assumption that all welfare counts equally, whoever\u2019s welfare it is\u2014that support the conclusion that the best animal-focused neartermist interventions (e.g., cage-free campaigns) are many times better than the best human-focused neartermist interventions (e.g., bednets).</li><li>Independently, note that the higher our credences in the theories of consciousness that are most friendly to Conscious Subsystems, the higher our credences ought to be in the hypothesis that many small invertebrates are sentient. So, insofar as we\u2019re risk-neutral expected utility maximizers with relatively high credences in Conscious Subsystems-friendly theories of consciousness, it\u2019s likely that we should be putting far more resources into investigating the welfare of the world\u2019s small invertebrates.</li></ul></li><li>We assign very low credences to claims that ostensibly support Conscious Subsystems.<ul><li>The appeal of the idea that standard theories of consciousness support Conscious Subsystems may be based on not distinguishing (a) theories that are just designed to make predictions about when people will self-report having conscious experiences of a certain type (which may all be wrong, but have whatever direct empirical support they have) and (b) theories that are attempts to answer the so-called \u201chard problem\u201d of consciousness (which only have indirect empirical support and are far more controversial).</li><li>Standard versions of functionalism say that states are conscious when they have the right relationships to sensory stimulations, other mental states, and behavior. But it\u2019s highly unlikely that many groups of neurons stand in the correct relationships, even if they perform functions that, in the abstract, seem as complex and sophisticated as those performed by whole brains.</li></ul></li><li>Ultimately, we do not recommend acting on Conscious Subsystems at this time.</li></ul><p>&nbsp;</p><h1>Introduction</h1><p>This is the fifth post in&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>the Moral Weight Project Sequence</u></a>. The aim of the sequence is to provide an overview of the research that Rethink Priorities conducted between May 2021 and October 2022 on interspecific cause prioritization\u2014i.e., making resource allocation decisions across species. The aim of this post is to assess a hypothesis that's been advanced by several members of the EA community: namely, that brains have subsystems that realize phenomenally conscious states that aren\u2019t accessible to the subjects we typically associate with those brains (i.e., the ones who report their experiences to us; see, e.g.,&nbsp;<a href=\"https://reducing-suffering.org/is-brain-size-morally-relevant/\"><u>Tomasik, 2013-2019</u></a>,&nbsp;<a href=\"https://link.springer.com/article/10.1007/s13164-015-0297-5\"><u>Shiller, 2016</u></a>,&nbsp;<a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#AppendixH\"><u>Muehlhauser, 2017</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/848SgRAKpjbuBWkW7/why-might-one-value-animals-far-less-than-humans?commentId=KFHCdP2B9brqi2Ryb\"><u>Shulman, 2020</u></a>,&nbsp;<a href=\"https://www.cambridge.org/core/journals/utilitas/article/abs/what-if-we-contain-multiple-morally-relevant-subjects/97D3D98A4573E128C7CB763FA1E06008\"><u>Crummett, 2022</u></a>).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdmgs1jnzgd\"><sup><a href=\"#fndmgs1jnzgd\">[1]</a></sup></span></p><p>If there are such states, then we might think that there is more than one conscious subject per brain, each supported by some neural subsystem or other.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvwshbbuw4u\"><sup><a href=\"#fnvwshbbuw4u\">[2]</a></sup></span>&nbsp;Let\u2019s call this&nbsp;<i>the</i>&nbsp;<i>Conscious Subsystems Hypothesis&nbsp;</i>(or&nbsp;<i>Conscious Subsystems</i>, for short).&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/uit0mxfvxvpxqaumvjt7.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/x1dlxhoohezttu4hh8ry.png 110w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/tk0nzdqxhuvnugshzkhw.png 220w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/xosiws1qf3fhr4v0ymqq.png 330w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/b3ic6mikppo5wdxstqr7.png 440w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/ku9y9ymqzjv7ppxdbcvi.png 550w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/rl8nrgcnilouuvvqbrui.png 660w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/pdpesndrmpbxxak4qpux.png 770w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/kedplvmqvduqdzwq259d.png 880w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/kg4bkcuigliodlh2q9zq.png 990w, https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/drsvwsr5ahzcaq9juigg.png 1024w\"></figure><p>Conscious Subsystems could affect how we ought to make tradeoffs between members of different species. Suppose, for instance, that the number of these subsystems scales proportionally with neuron count. A human has something like 86 billion neurons in her brain; a chicken, 220 million. So, if there are conscious subsystems in various brains, there could be roughly 400 times as many in humans as in chickens. If we were to assume that every subsystem is in pain when the main system reports pain, then it could work out that in a case where a human and a chicken appear<i>&nbsp;</i>to have comparable pain levels, it\u2019s nevertheless true that there is roughly 400 times as much pain in the human than in the chicken. Given the aim of maximizing expected welfare and all else equal, it could follow that it\u2019s roughly 400 times more important to alleviate the human\u2019s pain than the chicken\u2019s pain. It matters, then, whether Conscious Subsystems is true.</p><p>Accordingly, this post:</p><ol><li>Develops one argument for Conscious Subsystems;</li><li>Explains why Conscious Subsystems, even if true, may not be practically significant in some key decision contexts;&nbsp;</li><li>Provides some reasons to assign low probabilities to the premises of the argument for Conscious Subsystems; and</li><li>Offers some general reasons to be wary of allocating resources based on hypotheses like Conscious Subsystems.&nbsp;</li></ol><h1>Motivating Conscious Subsystems</h1><p>We begin by considering a way of motivating Conscious Subsystems: namely, the classic \u201cChina brain\u201d thought experiment in Ned Block\u2019s famous 1978 paper, \u201c<a href=\"https://www.degruyter.com/document/doi/10.4159/harvard.9780674594623.c31/html\"><u>Troubles with Functionalism</u></a>.\u201d Very roughly, functionalism is the view that mental states are the kinds of states they are due to their functions, or their roles within larger systems. Block argued that functionalism implies that systems that clearly&nbsp;<i>aren\u2019t&nbsp;</i>conscious,&nbsp;<i>are</i> conscious. For instance:</p><blockquote><p>Imagine a body externally like a human body, say yours, but internally quite different. The neurons from sensory organs are connected to a bank of lights in a hollow cavity in the head. A set of buttons connects to the motor-output neurons. Inside the cavity resides a group of little men. Each has a very simple task: to implement a \u201csquare\u201d of a reasonably adequate machine table that describes you. On one wall is a bulletin board on which is posted a state card, i.e., a card that bears a symbol designating one of the states specified on the machine table. Here is what the little men do: Suppose the posted card has a \u2018G\u2019 on it. This alerts the little men who implement G squares\u2014\u2018G-men\u2019 they call themselves. Suppose the light representing I17 goes on. One of the G-men has the following as his sole task: when the card reads \u2018G\u2019 and the I17 light goes on, he presses output button O191 and changes the state card to \u2018M\u2019\u2026. In spite of the low level of intelligence required of each little man, the system as a whole manages to simulate you because the functional organization they have been trained to realize is yours.</p></blockquote><p>The \u201cChina-brain\u201d thought experiment makes essentially makes the same point:</p><blockquote><p>Suppose we convert the government of China to functionalism, and we convince its officials that it would enormously enhance their international prestige to realize a human mind for an hour. We provide each of the billion people in China\u2026 with a specially designed two-way radio that connects them in the appropriate way to other persons and to the artificial body mentioned in the previous example.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2o43j8ksm2f\"><sup><a href=\"#fn2o43j8ksm2f\">[3]</a></sup></span>&nbsp;We replace the little men with a radio transmitter and receiver connected to the input and output neurons. Instead of a bulletin board, we arrange to have letters displayed on a series of satellites placed so that they can be seen from anywhere in China. Surely such a system is not physically impossible. It could be functionally equivalent to you for a short time, say an hour.&nbsp;</p></blockquote><p>If functionalism is true, Block argues, then this system wouldn\u2019t just be conscious; it would have exactly the same mental states that you have. If that\u2019s right, then functionalism implies that a conscious mind&nbsp;<i>just like yours</i> can be composed of other conscious minds. After all, it seems clear that the people of China don\u2019t cease to be conscious simply because they\u2019ve taken up this odd work of replicating the functions that give rise to your mind.</p><p>Of course, Block offered these thought experiments as reasons to reject functionalism. Many consciousness researchers now endorse \u201canti-nesting\u201d principles to prevent their theories from having this implication (e.g.,&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11406-015-9653-z\"><u>Kammerer, 2015</u></a>). At the same time, some just bite the bullet, agreeing that while it might be counterintuitive that this system is conscious, you and the \u201cChina brain\u201d would indeed have the same mental states for as long as it operates (e.g.,&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11098-014-0387-8\"><u>Schwitzgebel, 2015</u></a>). Suppose that\u2019s true. Then, we\u2019re on our way to an argument for Conscious Subsystems, one version of which goes as follows:</p><ol><li>Some neural subsystems would be conscious if they were operating in isolation.&nbsp;</li><li>If a neural subsystem would be conscious if it were operating in isolation, then it's conscious even if part of a larger conscious system.</li><li>So, some neural subsystems are conscious.</li></ol><p>We can read Premise 2 as a way of biting the bullet on the China brain thought experiment. So, we\u2019re now left wondering about the case for Premise 1.</p><p>But before we turn to that premise, there are two points to note. First, while this conclusion sounds radical, it might not<i>&nbsp;</i>be practically significant as stated. After all, it isn\u2019t clear that all conscious states are&nbsp;<i>valenced&nbsp;</i>states\u2014that is, states that feel good or bad. So, if we\u2019re hedonists\u2014that is, we assume that all and only positively valenced conscious states contribute positively to welfare and all and only negatively valenced conscious states contribute negatively to welfare\u2014then it could work out that all these conscious subsystems are morally irrelevant. If the states are conscious but not valenced, then they don\u2019t realize any welfare at all.</p><p>Moreover, even if these subsystems do have valenced conscious states\u2014and so realize some welfare\u2014it doesn\u2019t follow that we can assess the net impact of our actions on their welfare. Suppose we can\u2019t. Then, if we\u2019re&nbsp;<i>risk-neutral expected utility maximizers</i>\u2014that is, we want to maximize utility and we\u2019re equally concerned to avoid realizing negative utility and promote the realization of positive utility\u2014the welfare of the subsystems \u201ccancels out\u201d in expectation.</p><p>But let\u2019s grant that&nbsp;<i>if</i> these subsystems are conscious, then they&nbsp;<i>would&nbsp;</i>have valenced states. Moreover, let\u2019s grant that the welfare of the subsystems is correlated with reports or other typical measures of welfare.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzm6j5d1ump9\"><sup><a href=\"#fnzm6j5d1ump9\">[4]</a></sup></span></p><p>This brings us to the second point: namely, that it may not matter whether we have strong reasons to believe any of the premises of the argument for Conscious Subsystems\u2014or the assumptions we just granted\u2014if we\u2019re risk-neutral expected utility maximizing total utilitarians. If we assign some credence to each of the relevant claims, then as long as there are enough subsystems, the argument will still be practically significant.</p><p>Recall, for instance, that a human has something like 86 billion neurons in her brain; a chicken, 220 million. So, if we thought that there\u2019s around one conscious system for every 220 million neurons, we would conclude that a human brain supports around 400 times as many conscious subjects as a chicken brain. Given that, if we assign low credences to each premise in the argument for Conscious Subsystems\u2014e.g., 0.2\u2014it follows that, in expectation, we ought to conclude that a human brain supports roughly 17 times as many conscious subjects as a chicken brain.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3r3wu2jlqh5\"><sup><a href=\"#fn3r3wu2jlqh5\">[5]</a></sup></span></p><p>We chose this example because most people agree that chickens are conscious. But as Shulman points out, while it isn\u2019t clear whether insects are conscious,&nbsp;<a href=\"https://rethinkpriorities.org/invertebrate-sentience-table\"><u>there is some suggestive evidence in favor of that hypothesis</u></a>. So, we should assign some credence to the hypothesis that they\u2019re conscious. If we grant this much, though, then he thinks we should assign some credence to (something like) Premise 1, since, in his view, human brains contain many subsystems that are at least as complex as, and have \u201ccapabilities greater than,\u201d insect brains. And since there are mature insects with&nbsp;<a href=\"https://en.wikipedia.org/wiki/Megaphragma_mymaripenne\"><u>fewer than 10,000 neurons</u></a>, our credences can be lower without threatening the practical significance of the argument\u2014again, assuming we\u2019re expected utility maximizers.&nbsp;</p><p>Suppose, for instance, that we assign even<i>&nbsp;</i>lower credences to each premise\u2014e.g., 0.05. And suppose that, conditional on these premises, we assign the same credence to the hypothesis that we can separate all the neurons of a human brain into conscious subsystems with around 10,000 neurons each. Then, we ought to conclude that a human brain supports roughly 1076 times as many conscious subjects as a 10,000-neuron insect brain in expectation.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6vnw22plya2\"><sup><a href=\"#fn6vnw22plya2\">[6]</a></sup></span>&nbsp;(<a href=\"http://reflectivedisequilibrium.blogspot.com/2015/11/various-functional-forms-for-brain.html\"><u>Shulman (2015)</u></a> and&nbsp;<a href=\"https://reducing-suffering.org/net-impact-vegetarianism-factory-farm-suffering-vs-invertebrates-pasture-fields/#R\"><u>Tomasik (2016-2017)</u></a> make similar calculations between chickens, cattle, and insects or springtails, normalizing by insects or springtails, but uncertainty-free and, for some calculations, with diminishing marginal returns to additional neurons.)</p><p>Moreover, these credences may be too low.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YE9d4yab9tnkK5yfd/physical-theories-of-consciousness-reduce-to-panpsychism\"><u>St. Jules (2020)</u></a>, for example, argues that several of the most prominent theories of consciousness, such as Global Workspace Theory, Integrated Information Theory, and Recurrent Processing Theory, imply that many neural subsystems (or, more generally, many very simple systems) would be conscious if they occurred in isolation\u2014or, at least, would have that implication if certain ostensibly-arbitrary assumptions were dropped, namely, assumptions designed solely to block the implication that consciousness systems can be built out of other conscious systems. To give just one example, Global Workspace Theory says, in essence, that a mental state is conscious just when its content is broadcast to an array of neural subsystems. St. Jules points out that a mental state\u2019s content can be broadcast to all the subsystem\u2019s subsystems even if it isn\u2019t globally broadcast\u2014which we might call \u201clocal\u201d rather than global broadcasting. So, unless there\u2019s something special about broadcasting to all subsystems rather than some subset of them, Global Workspace Theory implies that if subsystems locally broadcast a state\u2019s content, then those subsystems are conscious.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3o4mp80wxt1\"><sup><a href=\"#fn3o4mp80wxt1\">[7]</a></sup></span></p><p>Suppose that, on this basis, we revised all our credences upward to 0.2 for each premise and 0.2 for 10,000-neuron subsystems being conscious (based on a comparable credence for 10,000-neuron insects being conscious), both for humans and chickens. Then, in expectation, we ought to attribute around 68,801 conscious subjects to each human brain\u2014and around 177 to each chicken (~389x fewer, which is basically the ratio of the number of neurons in a human brain over the number in a chicken brain). At that point, the practical significance of the argument may be quite radical.</p><h1>Assessing Conscious Subsystems</h1><p>Given a commitment to risk-neutral expected utility maximization, there are two basic ways to assess the practical significance of Conscious Subsystems:</p><ol><li>Given some range of reasonable credences, we can consider whether Conscious Subsystems would alter what we would otherwise think we should do in some particular decision context.</li><li>We can consider arguments for adjusting our credences in the claims that support Conscious Subsystems.</li></ol><p>Ultimately, the first point is the most important one. Conscious Subsystems matters insofar as it makes a practical difference. So, we\u2019ll begin there. Then, we\u2019ll spend some time on the second.</p><h2>Either Conscious Subsystems probably doesn\u2019t affect what we ought to do or it should have a minimal impact on what we ought to think we ought to do</h2><p>There are many contexts in which Conscious Subsystems might be practically significant. Here, though, we\u2019re especially concerned to assess whether Conscious Subsystems should alter the way EAs think they ought to allocate resources. We think that it probably doesn\u2019t. Or, if it does, it probably favors very strange courses of action, such as allocating much more toward invertebrate welfare.&nbsp;</p><h3>Even if Conscious Subsystems is true, neartermists should keep spending on animals</h3><p>Let\u2019s begin with why Conscious Subsystems probably shouldn\u2019t alter the way EAs think they ought to allocate resources. Open Philanthropy&nbsp;<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>once estimated</u></a> that, \u201cif you value chicken life-years equally to human life-years\u2026 [then] corporate campaigns do about 10,000x as much good per dollar as top [global health] charities.\u201d Two more recent estimates\u2014which we haven\u2019t investigated and aren\u2019t necessarily endorsing\u2014agree that corporate campaigns are much better. If we assign equal weights to human and chicken welfare in the model that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nDgCKwjBKwFvcBsts/corporate-campaigns-for-chicken-welfare-are-10-000-times-as\"><u>Grilo, 2022</u></a> uses, corporate campaigns are roughly 5,000x better than the best global health charities. If we do the same thing in the model that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ahr8k42ZMTvTmTdwm/how-good-is-the-humane-league-compared-to-the-against\"><u>Clare and Goth, 2020</u></a> employ, corporate campaigns are&nbsp;<i>30,000 to 45,000x</i> better.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref60u6woe18hh\"><sup><a href=\"#fn60u6woe18hh\">[8]</a></sup></span>&nbsp;So, if even the most conservative of these estimates is ten times too high, Conscious Subsystems wouldn\u2019t imply that risk-neutral expected utility maximizers ought to allocate neartermist dollars to humans instead of animals, at least if we estimate the number of human-vs.-nonhuman conscious subsystems as we did earlier, at a ratio of less than 400.&nbsp;</p><p>In fact, there\u2019s a sense in which Conscious Subsystems may&nbsp;<i>bolster</i> the cost-effectiveness argument for spending more on animals, if similarly sized conscious subsystems generate valenced states with similar intensities regardless of the brain to which they belong. To see why, consider that, first, if many brains house many 10,000-neuron conscious subsystems, then it\u2019s likely these subsystems are less sophisticated than the conscious subject who can report their conscious experiences. Still, if conscious subsystems really do produce valenced states, we see no reason to suppose that they produce valenced states that are, say, 8,600,000x less intense than the valenced states of the main subject (a number derived by dividing the number of neurons in a human brain by 10,000), to ensure most of the total valence in a human brain to come from the main subject. We aren\u2019t aware of any empirical theory about the function of valenced states that would support such huge differences; likewise, as discussed in our previous post in this sequence, we aren\u2019t aware of any compelling philosophical reason to think that the intensity of valenced states scales with neuron counts. So, while the valenced states of conscious subsystems might be less intense, our default would be to assume much smaller differences between the intensities of the states of the subsystems and the intensities of the states of the main subject.</p><p>So, since conscious subsystems will vastly outnumber the main subject in many cases, many organisms\u2019 moral value will be based largely on their subsystems, not on the main subjects who can report their experiences (or reveal them via behavior, etc.). So, instead of having to assess complicated questions about the relative intensity of valenced experiences, we can use \u201csubsystem counts\u201d to approximate the relative moral importance of humans and animals. And as we suggested earlier, when we do that with an expected number of subsystems roughly proportional to neuron counts, relative subsystem counts don\u2019t support allocating resources to the best neartermist human interventions over the best neartermist animal interventions. Instead of undermining pro-animal cost-effectiveness analyses with a human-favoring philosophical theory (namely, Conscious Subsystems itself), Conscious Subsystems appears to support the conclusion that there are scalable nonhuman animal-targeting interventions that are far more cost-effective than GiveWell\u2019s recommended charities.</p><p>That being said, we grant that Conscious Subsystems&nbsp;<i>could</i> make longtermist interventions seem better, though the issue is complicated by questions about the role of digital minds in the value of the long-term future. If most of the value of the long-term future is in digital minds, then given the possibility that digital minds might&nbsp;<i>themselves</i> have conscious subsystems\u2014potentially even more conscious subsystems than humans (on a per-individual basis)\u2014then Conscious Subsystems could provide a reason for those already inclined toward longtermism to be even more inclined toward it.</p><p>However, it\u2019s hard to believe that this would matter to anyone who has reservations about longtermism. Suppose, for instance, that we\u2019re undercounting the number of conscious beings in the future relative to the number in the present by several orders of magnitude. If your primary reservation about longtermism is, say,&nbsp;<a href=\"https://users.ox.ac.uk/~mert2255/papers/cluelessness.pdf\"><u>complex cluelessness</u></a> or&nbsp;<a href=\"https://globalprioritiesinstitute.org/christian-tarsney-the-epistemic-challenge-to-longtermism/\"><u>very low probabilities of making any difference</u></a>, that undercounting hardly seems relevant.</p><h3>Conscious Subsystems probably supports spending far more on small invertebrates</h3><p>Let\u2019s now turn to the possibility that Conscious Subsystems&nbsp;<i>should&nbsp;</i>alter the way EAs ought to allocate resources, albeit in a surprising way.&nbsp;<a href=\"https://www.vox.com/2014/6/11/5799992/these-mites-live-on-your-face-and-come-out-to-have-sex-at-night\"><u>It\u2019s estimated that there are 1.5 million to 2.5 million mites on the average human\u2019s body</u></a> and together they probably have&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5k6mJFBpstjkjv2SJ/small-animals-have-enormous-brains-for-their-size?commentId=P67cohZgvyaJsz7q2\"><u>about 1% of the number of neurons as does the average human brain</u></a>. However, the views about consciousness that support Conscious Subsystems will tend to assign greater probabilities to the hypothesis that those mites are conscious. The lower the \u201cbar\u201d for consciousness, the more likely it is that mites clear it. So, the expected number of conscious systems&nbsp;<i>on</i> the human body might not be more than an order of magnitude smaller than the expected number in the human brain. Moving on from organisms living on humans,&nbsp;<a href=\"https://www.pnas.org/doi/abs/10.1073/pnas.2201550119\"><u>Schultheiss et al. (2022</u></a>) estimate that 20 quadrillion (20*10<sup>15</sup>) ants are alive at any moment, and based on some of our past research, we tend to think we ought to assign a non-negligible credence to the hypothesis that they\u2019re sentient (<a href=\"https://rethinkpriorities.org/invertebrate-sentience-table\"><u>Rethink Priorities, 2019;</u>&nbsp;</a><a href=\"https://rethinkpriorities.org/publications/opinion-estimating-invertebrate-sentience\"><u>Schukraft et al., 2019</u></a>).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgtyeu70phpt\"><sup><a href=\"#fngtyeu70phpt\">[9]</a></sup></span></p><p>The upshot here is simple. There are relatively restrictive views of consciousness, like certain higher-order theories, and relatively permissive views of consciousness, like panpsychism. The higher our credences in restrictive views, the fewer conscious subsystems we ought to posit in expectation\u2014and the lower the odds that many small invertebrates are sentient. The higher our credences in permissive views, the&nbsp;<i>more</i> conscious subsystems we ought to posit in expectation\u2014and the&nbsp;<i>higher</i> the odds that many small invertebrates are sentient. So, insofar as we\u2019re risk-neutral expected utility maximizers with relatively high credences in permissive views of consciousness, it\u2019s likely that we should be putting far more resources into investigating the welfare of the world\u2019s small invertebrates. And depending on how many invertebrates we can help and how much we can help them, it could work out that we ought to prioritize them over humans.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwr2zqbfe17\"><sup><a href=\"#fnwr2zqbfe17\">[10]</a></sup></span></p><h2>We should probably assign low credences to the claims that support Conscious Subsystems&nbsp;</h2><p>We now turn to issues that are relevant to the credences we ought to assign to the claims that support Conscious Subsystems. The first is that they\u2019re based on a failure to distinguish&nbsp;<i>neural correlate</i> theories of consciousness with&nbsp;<i>explanatory&nbsp;</i>theories of consciousness. The second is that functionalism probably doesn\u2019t support attributing consciousness to neural subsystems per se, whatever it might imply about other entities.</p><h3>Neural correlate theories of consciousness =/= explanatory theories of consciousness</h3><p>Again, the basic argument for Conscious Subsystems goes as follows:</p><ol><li>Some neural subsystems would be conscious if they were operating in isolation.&nbsp;</li><li>If a neural subsystem would be conscious if it were operating in isolation, then it's conscious even if part of a larger conscious system.</li><li>So, some neural subsystems are conscious.</li></ol><p>In support of the first premise, we have Shulman and St. Jules appealing to the implications of various theories of consciousness and Shulman making comparisons between the subsystems in human brains. A thought experiment supports the second.</p><p>Let\u2019s step back and highlight the difference between two types of theories about consciousness. One kind of \u201ctheory of consciousness\u201d is a \u201cneural correlate of consciousness (NCC).\u201d This refers to a set of conditions in brains that are, as the name suggests, reliably correlated with conscious experiences. In general, most attempts to scientifically study consciousness are attempts to find the neural correlates of consciousness, primarily relying on finding conditions that reliably correlate with self-reports of consciousness, since consciousness itself cannot be observed in others. Importantly, a neural correlate of consciousness need not provide an explanation of how subjective experiences exist in the physical world. NCCs need only identify patterns between observable features of the world.</p><p>A second type of theory of consciousness is an \u201cexplanatory theory of consciousness.\u201d An explanatory theory doesn\u2019t merely posit a correlation; it provides some story about how subjective experiences exist in the physical world. In short, an explanatory theory of consciousness is an attempt to solve the \u201chard problem of consciousness\u201d: that is, it attempts to explain why and how we have phenomenally conscious states. Many scientists studying consciousness are explicitly uninterested in solving the hard problem of consciousness or believe it to be unsolvable.</p><p>This distinction is important because there\u2019s a certain type of move that\u2019s made in discussions about the distribution of consciousness that involves a category error. Consider the following argument schema:</p><ol><li>Brains with property X are conscious.</li><li>Ys have property X.</li><li>So, Ys are conscious.</li></ol><p>This argument might be fine for some values of X and Y. However, it\u2019s often confused if X is a&nbsp;<i>neural correlate</i> of consciousness and Y is something for which we don\u2019t have any independent reason to posit consciousness. This is because Premise 1 here is really shorthand for a longer claim, which is something like: brains with property X&nbsp;<i>that can self-report consciousness&nbsp;</i>are conscious. Essentially, Premise 1 is really a claim about a very specific kind of system\u2014the human system\u2014that research has revealed to have the following feature: whenever X is present, systems of that type (people) self-reported conscious experiences of a certain type; whenever X was absent, systems of that type (people) did not self-report having a conscious experience of that type. Premise 1 doesn\u2019t say anything about systems that&nbsp;<i>can\u2019t</i> self-report consciousness.</p><p>Again, this is because in a NCC, X doesn\u2019t explain what consciousness is; it isn\u2019t an account of consciousness. Instead, it serves as the basis for a research program. Because we know that brains with X often have conscious experiences, we should investigate X in more detail to learn more about consciousness&nbsp;<i>in that organism</i>. If X&nbsp;<i>were&nbsp;</i>supposed to explain what consciousness is\u2014if, in other words, it\u2019s a theory that attempts to provide the list of necessary and sufficient conditions for consciousness\u2014then there\u2019s no conceptual issue here. But the move is a category error when used with a NCC, because such theories aren\u2019t attempting to provide lists of necessary and sufficient conditions; they aren\u2019t trying to provide accounts of what consciousness is. Rather, these theories are only trying to identify promising features that can be reliably correlated with self-reports of consciousness.&nbsp;</p><p>Consider, for example, someone arguing as follows.</p><ol><li>Brains that engage in recurrent processing are conscious.</li><li>Electrons do something that, at an abstract level, could be described as recurrent processing (e.g., \u201can electron influences other particles, which in turn influence the electron,\u201d etc. (<a href=\"https://forum.effectivealtruism.org/posts/YE9d4yab9tnkK5yfd/physical-theories-of-consciousness-reduce-to-panpsychism?commentId=sBR8cFfCEvhqR7N3M\"><u>Tomasik, 2020</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YE9d4yab9tnkK5yfd/physical-theories-of-consciousness-reduce-to-panpsychism\"><u>St. Jules, 2020</u></a>)).</li><li>So, panpsychism is true.</li></ol><p>This argument is based on a misunderstanding of the first premise, at least as it\u2019s intended by many proponents of recurrent processing theory. These individuals are&nbsp;<i>not</i> saying that anything that exhibits the property of recurrent processing is thereby conscious. Instead, they were making the empirical claim that recurrent processing of a certain sort is reliably correlated with self-reports of conscious experiences in humans. It doesn\u2019t make sense to generalize this view to \u201celectrons influencing one another\u201d because electrons influencing one another is decidedly not correlated with self-reports of consciousness.</p><p>Of course, some proponents of recurrent processing theory probably&nbsp;<i>do&nbsp;</i>take themselves to be giving an account of what consciousness actually is. We can\u2019t assess such claims here. However, it\u2019s important to recognize that we probably ought to assign much lower credences to the&nbsp;<i>explanatory&nbsp;</i>interpretations of theories than their&nbsp;<i>neural correlate&nbsp;</i>interpretations. Neural correlate interpretations of theories of consciousness have whatever fairly direct empirical support they have (or don\u2019t have, as the case may be). Explanatory interpretations of theories of consciousness borrow their support from their corresponding neural correlate interpretations and then go well beyond them, staking out much more controversial positions, and in any case ones that we can\u2019t clearly disconfirm empirically.</p><p>The upshot here is that Premise 1 faces one of two problems. On the one hand, it could be unmotivated, as it\u2019s a mistake to think that because some neural subsystems have X\u2014some neural correlate of consciousness\u2014that they would be conscious if they were operating in isolation. On the other hand, it could be that we ought to assign it a rather low credence.</p><h3>Functionalism doesn\u2019t support conscious subsystems</h3><p>Let\u2019s turn to the second problem for the argument for Conscious Subsystems. Roughly, functionalism about consciousness is the view that a physical state realizes a given mental state by playing the right functional role in a larger system. Crucially, standard versions of functionalism&nbsp;<i>don\u2019t&nbsp;</i>entail that all functional roles are conscious: only the ones with the right relationships to sensory stimulations, other mental states, and behavior.&nbsp;</p><p>Now consider the following argument:</p><ol><li>Fruit flies have roughly 200,000 neurons.</li><li>A human brain has 420,000x as many neurons as a fruit fly.</li><li>So, if fruit flies are conscious and we can\u2019t rule out states being conscious merely because they\u2019re embedded in a larger system, then human brains contain something like 420,000 as many conscious subsystems as fruit flies.</li></ol><p>This argument isn\u2019t valid as it stands, but that isn\u2019t the point. Instead, the point is that you can\u2019t make it valid by adding a standard version of functionalism. Standard functionalism, as noted above, doesn\u2019t claim that any system with a certain amount of processing power is thereby conscious. It says that states are conscious if they realize particular functional relationships between inputs, outputs, and other states. But there is no reason to believe that any of the subsets of neurons in the human brain with the same number of neurons in a fruit fly are arranged with the correct functional relationships.</p><p>Moreover, there are positive reasons to&nbsp;deny&nbsp;that given subsets of neurons in the human brain are&nbsp;arranged in the right manner to be conscious. Fruit fly brains faced evolutionary pressures and, as such, are designed to realize a set of input-output relations that increase the likelihood of fruit flies surviving and passing on their genes. Human brains also evolved to promote the likelihood of survival and reproduction. However, subsets of human brains, facing evolutionary pressures, including parallel tracks of sensory information, would have evolved to contribute to the overall system in a way that facilitates the overall system behaving accordingly. In other words, the evolutionary pressures on any subset of a human brain would push this subset to realize functions that are different from any function designed to maximize the fitness of a fruit fly.</p><p>Granted, someone could insist that the input-output relations that happen to maximize the fitness of fruit flies are also likely to be present in the human brain. However, it seems extremely unlikely that the pattern of neural activations that would lead to maximizing fruit fly fitness through a mental state would just so happen to be the same pattern of activations realized in human subsystems that make small contributions to the behavior of the organism as a whole. In fact, if we just look at the organization of the human brain, the distances that signals need to travel, and overall interconnections, it seems almost certain that there are no roughly fruit-fly-brain-sized subsystems of the human brain that realize identical functions to the fruit fly brain.</p><p>Abstracting away from fruit fly brains, it\u2019s likely that some functions required for consciousness or valence\u2014or realized along the way to generate conscious valence\u2014are fairly high-order, top-down, highly integrative, bottlenecking, or approximately unitary, and some of these are very unlikely to be realized thousands of times in any given brain. Some candidate functions are selective attention,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuaytiflgbd\"><sup><a href=\"#fnuaytiflgbd\">[11]</a></sup></span>&nbsp;a model of attention,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2434ndmmqij\"><sup><a href=\"#fn2434ndmmqij\">[12]</a></sup></span>&nbsp;various&nbsp;<a href=\"https://en.wikipedia.org/wiki/Executive_functions\"><u>executive functions</u></a>,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Cognitive_bias_in_animals\"><u>optimism and pessimism bias</u></a>, and (non-reflexive) appetitive and avoidance behaviors. Some kinds of valenced experiences, like empathic pains and social pains from rejection, exclusion, or loss, depend on high-order representations of stimuli, and these representations seem likely to be accessible or relatively few in number at a time, so we expect the same to hold for the negative valence that depends on them. Physical pain and even negative valence generally may also turn out to depend on high-order representations, and there\u2019s some evidence they depend on brain regions similar to those on which empathic pains and social pains depend (<a href=\"https://www.science.org/doi/pdf/10.1126/science.1093535\"><u>Singer et al., 2004</u></a>,&nbsp;<a href=\"https://escholarship.org/uc/item/0k84g6vn\"><u>Eisenberger, 2015</u></a>). On the other hand, if some kinds of valenced experiences occur simultaneously in huge numbers in the human brain, but social pains don\u2019t, then, unless these many valenced experiences have tiny average value relative to social pains, they would morally dominate the individual\u2019s social pains in aggregate, which would at least be&nbsp;<i>morally</i> counterintuitive, although possibly an inevitable conclusion of Conscious Subsystems.</p><p>Furthermore, the extra neurons in the human brain used to realize some of these functions have other more plausible roles than realizing these functions thousands of times simultaneously, like greater acuity, greater categorization power, or integrating more inputs (<a href=\"https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(20)30192-3\"><u>Birch et al., 2020</u></a>), and broadcasting the resulting signals to more neurons after (or processes, according to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/848SgRAKpjbuBWkW7/why-might-one-value-animals-far-less-than-humans?commentId=KFHCdP2B9brqi2Ryb\"><u>Shulman, 2020</u></a>). But even if each type of function that\u2019s necessary for conscious valence were realized many times in the human brain, each subsystem would need to realize an instance of&nbsp;<i>each</i> type of function&nbsp;<i>and</i> have them all fit together in the right way to generate conscious valence.</p><p>In other words, standard functionalism does not support the claim that the mere presence of large numbers of neurons in human brains is evidence in favor of the argument that there are numerous conscious subsystems. That more neurons are devoted to the same functions in one brain than another isn\u2019t enough to establish that those functions, especially those generating conscious valence, are realized more often in the first brain than in the second. Those arguing in favor of conscious subsystems need to present some positive evidence for believing that the functions that are realized in subsets of human brains are identical, or at least similar enough, to those in other organisms to also be considered conscious. We don\u2019t have such evidence ourselves and we aren\u2019t aware of claims by neuroscientists that would suggest that it\u2019s out there.&nbsp;</p><p>Again, the upshot here is that Premise 1 of the argument for Conscious Subsystems seems unmotivated: standard versions of functionalism don\u2019t allow us to make analogical arguments from the complexity of subsystems or their contributions to generating consciousness or valence to their being separately conscious.</p><h1>Conclusion</h1><p>As we\u2019ve argued, there are some key decision contexts in which Conscious Subsystems probably shouldn\u2019t affect how we ought to act. In part, this is because animal-directed interventions look so good; on top of that, the theories of consciousness that support Conscious Subsystems also support consciousness being widespread in the animal kingdom, which is likely to cause small invertebrates to dominate our resource allocation decisions. However, Conscious Subsystems also shouldn\u2019t affect our resource allocation decisions because we ought to assign it a very low probability of being true. The basic argument for it is probably based on a category error. In addition, it doesn\u2019t get the support from functionalism that we might have supposed.</p><p>Nevertheless, some will insist that the probability of Conscious Subsystems is&nbsp;<i>not&nbsp;</i>so low as to make it practically irrelevant. While it might not affect any decisions that EAs face&nbsp;<i>now</i>, it may still affect decisions that EAs face in the future. In what remains, we explain why we disagree. On our view, while it might seem as though expected utility maximization supports giving substantial weight to Conscious Subsystems, other considerations, such as credal fragility, suggest that we should give limited weight to Conscious Subsystems if we're careful expected utility maximizers.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3fia3x5b5r1\"><sup><a href=\"#fn3fia3x5b5r1\">[13]</a></sup></span></p><p>From the armchair, we can\u2014as we have!\u2014come up with arguments for and against Conscious Subsystems. However, it\u2019s hard to see how any of these arguments could settle the question in some decisive way. There will always be room to resist objections, to develop new replies, to marshal new counterarguments. In principle, empirical evidence could radically change our situation: Imagine a new technology that allowed subsystems to report their conscious states! But we don\u2019t have that evidence and, unfortunately, may forever lack it. Moreover, we should acknowledge that it\u2019s probably possible to come up with&nbsp;<i>inverse&nbsp;</i>theories that imply that&nbsp;<i>smaller&nbsp;</i>brains are extremely valuable\u2014perhaps because they realize the most intense valenced states, having no cognitive resources to mitigate them. So, we find ourselves in a situation where our credences should be low and fairly fragile. Moreover, they may alternate between theories with radically different practical implications.&nbsp;</p><p>This isn\u2019t a situation where it makes sense to maximize expected utility at any given moment. Instead, we should acknowledge our uncertainty, explore related hypotheses and try to figure out whether there\u2019s a way to make the questions empirically tractable. If not, then we should be very cautious, and the best move might just be assuming something like neutrality across types of brains while we await possible empirical updates. Or, at least, seriously limiting the percentage of our resources that\u2019s allocated in accord with Conscious Subsystems. This seems like a good epistemic practice, but it also makes practical sense: actions involve opportunity costs, and being too willing to act on rapid updates can result in failing to build the infrastructure and momentum that\u2019s often required for change.</p><p><br>&nbsp;</p><h1>Acknowledgments</h1><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1672695908/mirroredImages/vbhoFsyQmrntru6Kw/vvrzwp8iinbbyxof2xjv.png\"><br>This research is a project of Rethink Priorities. It was written by Bob Fischer, Adam Shriver, and Michael St. Jules. It is indebted to previous work on this topic by David Mathers. Thanks to Marcus Davis, Jim Davies, Gavin Taylor, Teo Ajantaival, Jacy Reese Anthis, Magnus Vinding, Brian Tomasik, Anthony DiGiovanni, Joe Gottlieb, David Mathers, Richard Bruns, David Moss, and Derek Shiller for helpful feedback on earlier versions of this report. If you\u2019re interested in RP\u2019s work, you can learn more by visiting our&nbsp;<a href=\"https://www.rethinkpriorities.org/research\"><u>research database</u></a>. For regular updates, please consider subscribing to our&nbsp;<a href=\"https://www.rethinkpriorities.org/newsletter\"><u>newsletter</u></a>.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndmgs1jnzgd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdmgs1jnzgd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These non-accessible conscious states go under many names: e.g., \u201chidden qualia\u201d (<a href=\"https://link.springer.com/article/10.1007/s13164-015-0297-5\"><u>Shiller, 2016</u></a>), \u201cparaconsciousnesses,\u201d \u201cunderselves,\u201d and \u201cco-consciousnesses\u201d (Blackmore, 2017), while \u201cphenomenal overflow\u201d may be a special case (<a href=\"https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/consciousness-accessibility-and-the-mesh-between-psychology-and-neuroscience/63BF0B7E5DB1654ED680E160D413F3D0\"><u>Block, 2007</u> and</a>&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1364661311002221\"><u>Block, 2011</u></a>, as well as discussion of the&nbsp;<i>partial awareness</i> response in&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S1364661310000914\"><u>Kouider et al., 2010</u></a> and&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S1364661315002521\"><u>Tsuchyia et al., 2015</u></a>). We should also note that there are ways of interpreting some of these authors, such as&nbsp;<a href=\"https://reducing-suffering.org/is-brain-size-morally-relevant/\"><u>Tomasik, 2013-2019</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/848SgRAKpjbuBWkW7/why-might-one-value-animals-far-less-than-humans?commentId=KFHCdP2B9brqi2Ryb\"><u>Shulman, 2020</u></a>, where they do&nbsp;<i>not&nbsp;</i>mean to be talking about inaccessible states. Instead, they may have a view where the components of consciousness are a bit like pixels with a fixed size, so that the more of those pixels you have in each experience, the \u201cmore consciousness\u201d\u2014or the more independently valuable components of consciousness\u2014you\u2019ve got.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/848SgRAKpjbuBWkW7/why-might-one-value-animals-far-less-than-humans?commentId=KFHCdP2B9brqi2Ryb\"><u>Shulman (2020)</u></a>, for instance, writes that \u201c[if] each edge detection or color discrimination (or higher level processing) additively contributes some visual experience, then you have immense differences in the total contributions.\u201d However, only the&nbsp;<i>valenced</i> components would be valuable given hedonism, which we assume in this report, and we\u2019d expect these valenced components to occur in relatively small numbers and be tied to high-level features of a subject\u2019s experiences. We don\u2019t address that view in detail, though we suspect that some of the arguments below could be adapted to apply to it.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvwshbbuw4u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvwshbbuw4u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For present purposes, we're simply granting the assumption behind this inference, which is that if phenomenal states aren\u2019t integrated with or accessible to one another, then they\u2019re possessed by different subjects. However, that assumption is controversial and we aren't endorsing it.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2o43j8ksm2f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2o43j8ksm2f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>China had a population of roughly one billion when Block wrote this, which is about 100x fewer people than the number of neurons in a human brain. It\u2019s an open question whether that\u2019s enough people for Block\u2019s purposes, but the general philosophical point remains.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzm6j5d1ump9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzm6j5d1ump9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It\u2019s complicated to assess how much of a concession this is. For instance, it\u2019s clear that whatever probability you assign to Conscious Subsystems, you should assign a lower probability to the hypothesis that Conscious Subsystems is true, that the states are valenced, and the valenced states are correlated with higher-level reports. Moreover, it\u2019s plausible that even if the subsystems have valenced states, those states don\u2019t affect the behavior of the whole organism; so, there aren\u2019t any adaptive pressures on those subsystems that would result in correlations with higher-level reports. As a result, the expected value implications of Conscious Subsystems might be trivial. At the same time, someone might argue that some neurons may have specific functions that make such a correlation plausible. For instance, there may be neurons that play a role in generating reportable negative valence but no role in generating reportable positive valence. All else equal, these negative valence-selective neurons may seem more likely to help realize the same negative valence-specific functions they do for reportable negative valence\u2014and so negative valence\u2014in subsystems containing them than to help realize positive valence in those subsystems. That being said, whatever variation of this hypothesis we entertain, it isn\u2019t clear whether the number of&nbsp;<i>relevant</i> conscious subsystems scales linearly with neuron counts\u2014where the relevant ones are those with valenced states that are correlated with the reports of the whole system. So, the open and challenging question is about the discount to apply.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3r3wu2jlqh5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3r3wu2jlqh5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.2 * 0.2 * 400 + (1 - 0.2 * 0.2) * 1 = 16.96. We\u2019re assuming that the subsystems aren\u2019t overlapping, and, for simplicity, that if there aren\u2019t 400 conscious subsystems conditional on the Conscious Subsystems premises, there\u2019s just the one conscious system.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6vnw22plya2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6vnw22plya2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>0.05*0.05*0.05*(86,000,000,000/10,000) + (1-0.05*0.05*0.05)*1 \u2248 1076 in a human, in expectation, vs. 1 in the insect.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3o4mp80wxt1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3o4mp80wxt1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/848SgRAKpjbuBWkW7/why-might-one-value-animals-far-less-than-humans?commentId=KFHCdP2B9brqi2Ryb\"><u>Shulman (2020)</u></a> suggests something similar, claiming that \u201ctrivially tiny computer programs we can make today could make for&nbsp;<a href=\"https://www.unil.ch/files/live/sites/philo/files/shared/DocsPerso/EsfeldMichael/2007/herzog.esfeld.gerstner.07.pdf\"><u>minimal instantiations</u></a> of available theories of consciousness, with quantitative differences between the minimal examples and typical examples.\u201d Granted, the local / global distinction might be&nbsp;<i>epistemically&nbsp;</i>significant, as we may not have ways to confirm that local broadcasting generates consciousness, whereas we can ask people to self-report about global broadcasting. However, it may not be&nbsp;<i>theoretically</i> significant, in the sense that there may not be any principled reason why global broadcasting would be required for consciousness.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn60u6woe18hh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref60u6woe18hh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This range reflects just the default set of parameters in&nbsp;<a href=\"https://www.getguesstimate.com/models/15687\"><u>their Guesstimate model</u></a>, after setting the node \u201cmoral weight (DALY/cDALY)\u201d to 1. We get a range because their Guesstimate model is noisy and different samples give different results.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngtyeu70phpt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgtyeu70phpt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more discussion of the population numbers of different groups of animals and the total numbers of neurons across these groups, see&nbsp;<a href=\"http://reflectivedisequilibrium.blogspot.com/2015/11/various-functional-forms-for-brain.html\"><u>Shulman, 2015</u></a>,&nbsp;<a href=\"https://reducing-suffering.org/convert-grass-lawns-to-gravel-to-reduce-insect-suffering/#Estimate_using_springtails_and_mites\"><u>Tomasik, 2015-2019</u></a>,&nbsp;<a href=\"https://eukaryotewritesblog.com/2019/02/12/small-animals-have-enormous-brains-for-their-size/\"><u>Ray, 2019</u></a>,&nbsp;<a href=\"https://eukaryotewritesblog.com/how-many-neurons-are-there/\"><u>Ray, 2019</u></a>,&nbsp;<a href=\"https://reducing-suffering.org/how-many-wild-animals-are-there/\"><u>Tomasik, 2009-2019</u></a>,&nbsp;<a href=\"https://docs.google.com/document/d/1wMIa6bAn4rfCAzBAsKlHBH9X3gmo8pjn/edit\"><u>McCallum, Martini and Shwartz-Lucas, 2022</u></a>. Land use, especially agricultural land use, plausibly has very large impacts on them, given that half of the world\u2019s habitable land is used for agriculture (<a href=\"https://ourworldindata.org/land-use#half-of-the-world-s-habitable-land-is-used-for-agriculture\"><u>Ritchie and Roser, 2019</u></a>), and climate change also probably has very large impacts on them, good or bad. For a different version of this argument, see&nbsp;<a href=\"https://academic.oup.com/book/41497/chapter-abstract/352910586?redirectedFrom=fulltext#352910680\"><u>Sebo, 2022</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwr2zqbfe17\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwr2zqbfe17\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Furthermore, given objections of fanaticism and decision-theoretic irrationality to expected utility maximization with&nbsp;<i>unbounded</i> utility functions (<a href=\"https://academic.oup.com/analysis/article-abstract/59/4/257/173397?redirectedFrom=fulltext&amp;login=false\"><u>McGee, 1999</u></a>,&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/pdf/10.1111/phpr.12704\"><u>Russell and Isaacs, 2020</u></a>,&nbsp;<a href=\"https://globalprioritiesinstitute.org/on-two-arguments-for-fanaticism-jeff-sanford-russell-university-of-southern-california/\"><u>Russell, 2021</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/gJxHRxnuFudzBFPuu/better-impossibility-result-for-unbounded-utilities\"><u>Christiano, 2022</u></a>,&nbsp;<a href=\"http://alexanderpruss.blogspot.com/2022/10/expected-utility-maximization.html\"><u>Pruss, 2022</u></a>), including the risk-neutral expected value maximizing total utilitarianism we\u2019ve assumed in our section Motivating Conscious Subsystems, we should give some weight to alternative decision theories or to bounded social welfare (utility) functions, perhaps aggregating across these views through some method for handling normative uncertainty (<a href=\"https://www.moraluncertainty.com/\"><u>MacAskill, Bykvist and Ord, 2020</u></a>). (Though we should not commit solely\u2014or perhaps at all\u2014to a version of maximizing expected choice-worthiness, especially with intertheoretic comparisons, to handle normative uncertainty, since that takes for granted an assumption we\u2019re calling into question: expected utility maximization, especially with an unbounded utility function.) Compared to risk neutral expected utility maximizing total utilitarianism, we expect these alternatives to be less fanatical and to give similar or substantially less weight to Conscious Subsystems, and so we expect to give less weight overall to Conscious Subsystems as a result of their consideration. This would also probably mean&nbsp;<i>further</i> discounting animals the more unlikely they are to be conscious, more so than just by their probability of consciousness, and could therefore potentially block the total domination of small invertebrate welfare in the short term. Indeed, this seems to be one of the few ways a total hedonistic utilitarian would prevent small invertebrates from totally dominating in the short term.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuaytiflgbd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuaytiflgbd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;About Global Workspace Theory,&nbsp;<a href=\"https://www.sscnet.ucla.edu/comm/steen/cogweb/CogSci/Baars-update_03.html\"><u>Baars (2003)</u></a> writes:</p><blockquote><p>The sensory \"bright spot\" of consciousness involves a selective attention system (the theater spotlight), under dual control of frontal executive cortex and automatic interrupt control from areas such as the brain stem, pain systems, and emotional centers like the amygdala. It is these attentional interrupt systems that allow significant stimuli to \"break through\" into consciousness in a selective listening task, when the name is spoken in the unconscious channel.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2434ndmmqij\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2434ndmmqij\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;About Attention Schema Theory,&nbsp;<a href=\"https://www.tandfonline.com/doi/abs/10.1080/17588928.2020.1838468\"><u>Graziano (2020)</u></a> writes:</p><blockquote><p>AST does not posit that having an attention schema makes one conscious. Instead, first, having an automatic self-model that depicts you as containing consciousness makes you intuitively believe that you have consciousness. Second, the reason why such a self-model evolved in the brains of complex animals, is that it serves the useful role of modeling attention.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3fia3x5b5r1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3fia3x5b5r1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u2026 and even more so if we consider alternative decision theories, bounded social welfare functions, or limiting aggregation.</p></div></li></ol>", "user": {"username": "bob-fischer"}}, {"_id": "CnAhPPsMWAxBm7pii", "title": "What specific changes should we as a community make to the effective altruism community? [Stage 1]", "postedAt": "2022-12-05T09:04:02.170Z", "htmlBody": "<p>I thought I'd run the listening exercise I'd like to see.</p><ol><li>Get popular suggestions</li><li>Run a polis poll</li><li>Make a google doc where we research consensus suggestions/ near consensus/consensus for specific groups</li><li>Poll again</li></ol><h1>Stage 1</h1><p>Give concrete suggestions for community changes. 1 - 2 sentences only.</p><p>Upvote if you think they are worth putting in the polis poll and agreevote if you think the comment is true.</p><p>Agreevote if you think they are well-framed.</p><p>Aim for them to be upvoted. Please add suggestions you'd like to see.</p><p>I'll take the top 20 - 30</p><p>I will delete/move to comments top-level answers that are longer than 2 sentences.</p><h1>Stage 2</h1><p>Post here: <a href=\"https://forum.effectivealtruism.org/posts/9R5eJhimR3QtjNFmP/what-specific-changes-should-we-as-a-community-make-to-the-1\">https://forum.effectivealtruism.org/posts/9R5eJhimR3QtjNFmP/what-specific-changes-should-we-as-a-community-make-to-the-1</a></p><p>Polis poll here: <a href=\"https://pol.is/5kfknjc9mj\">https://pol.is/5kfknjc9mj</a>&nbsp;</p>", "user": {"username": "nathan"}}, {"_id": "DJThFjkze2w8d3CAr", "title": "Probably good projects for the AI safety ecosystem", "postedAt": "2022-12-05T03:24:50.707Z", "htmlBody": "", "user": {"username": "Ryan Kidd"}}, {"_id": "kaKG9NQZd6Qr8pCkj", "title": "What are the odds that the upcoming wave of FTX criticism will target EA again?", "postedAt": "2022-12-05T01:22:07.054Z", "htmlBody": "<p>It's important to note that for most depositors (not necessarily most money, but most of the people for sure) the news hasn't broken yet definitively saying whether they'll get their deposits back, and they've been in a state of vague limbo ever since the bank run on Nov 8th.</p><p>Considering the volume of anti-EA sentiment published by reputable news outlets at the time, including misinformation published by those same outlets, what are the odds of it happening again after large numbers of confused FTX depositors are definitively informed that they aren't getting their money back? What are the odds of the negative coverage getting an order of magnitude worse, e.g. if journalists previously discovered during November that spreading misinformation about EA resulted in getting more clicks? What can be done for those contingencies?</p>", "user": {"username": "trevorw96"}}, {"_id": "YKY4KmKEurY8cwHTJ", "title": "AGI as a Black Swan Event", "postedAt": "2022-12-04T23:35:51.445Z", "htmlBody": "<h1>Summary</h1><p>Black swans are rare events that have a large impact and seem predictable in retrospect. Although they are rare, a single black swan can be more important than all other events combined because they are extreme. Black swan events are often unknown unknowns before they happen and are difficult to predict.</p><p>The creation of AGI could be a black swan event: improbable, unpredictable, and extremely impactful. If AGI is a black swan event the following claims will likely be true:</p><ol><li>It will be difficult to predict when the first AGI will be created and it could be created at an unexpected time. Since black swans are often outliers, many predictive models could completely fail to predict when AGI will be created.</li><li>It will be difficult to predict how AGI affects the world and a wide range of scenarios are possible.</li><li>The impact of AGI could be extreme and completely transform the world in an unprecedented way that does not follow previous historical trends (e.g. causing human extinction).</li><li>Past experience and analogies might have limited or little value when predicting the future.</li></ol><h1>Introduction to black swans</h1><p>A <a href=\"https://en.wikipedia.org/wiki/Black_swan_theory\">black swan</a> is an improbable and extreme outlier event with the following three properties:</p><ol><li>The event is a surprise to the observer or subjectively improbable.</li><li>It has a major impact.</li><li>It is inappropriately rationalized in hindsight to make it seem more predictable than it really was.</li></ol><p>Many variables such as human height follow a Gaussian distribution where extreme values are very unlikely and have only a small effect on statistics such as the average.</p><p>Black swans cannot be modeled with thin-tailed Gaussian distributions. Instead, they tend to follow long-tailed distributions such as the power-law distribution. In long-tailed distributions, the tail of the curve tapers off relatively slowly which makes extreme events more likely. Many events follow a power law distribution such as the number of books sold by authors, the number of casualties in wars, and the net worth of individuals.</p><p>Despite the fact that black swans are rare, they have a property called max-sum equivalence which means that the most extreme outlier is equal to the sum of the other events. For example, a single severe earthquake could cause as much damage as all other earthquakes combined.</p><p>According to Nassim Nicholas Taleb, author of the book <i>The Black Swan</i>, many of the most important events in history were black swans. For example, the rise of the internet, the 911 terrorist attack in 2001, and World War 1.</p><blockquote><p>\"I stop and summarize the triplet: rarity, extreme 'impact', and retrospective (though not prospective) predictability. A small number of Black Swans explains almost everything in our world, from the success of ideas and religions, to the dynamics of historical events, to elements of our own personal lives.\" - The Black Swan</p></blockquote><p>The name 'black swan' comes from the common belief among Europeans that all swans were white. All previously observed swans had been white until a black swan was first observed in 1697 in Australia. This example shows that confirming evidence has less power than disconfirming evidence and that our ability to predict the future based on past events is limited.</p><p>This problem is also known as Hume\u2019s problem of induction which is the problem of predicting the unobserved future given the observed past. Taleb uses the life of a Christmas turkey to explain the problem: the turkey predicts that it will be alive tomorrow and each day makes it increasingly confident that its next prediction will be correct until it is unexpectedly slaughtered.</p><p>Since black swans are improbable and extreme outlier events, they are inherently difficult to predict. Instead of predicting them, Taleb recommends building resilience.</p><p>Another key concept related to black swans is unknown unknowns. After the 911 terrorist attack, many people became fearful of flying on airplanes. In this case, there was a known unknown or partial knowledge: the people knew the risk of flying on planes but didn\u2019t know where or when the next attack would happen.</p><p>But black swans tend to involve much less knowledge, are much more random and unpredictable, and come from unknown unknowns. Before the attack, most people wouldn\u2019t have known the attack was even a risk. World War I was not predicted<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0g3mat21nfb7\"><sup><a href=\"#fn0g3mat21nfb7\">[1]</a></sup></span>&nbsp;in advance and neither was the fall of the Soviet Union. A similar idea is the narrative fallacy. When we imagine a risk we may have in mind some detailed story of what could go wrong: a plane crashing or an attack in a particular place despite the fact that these particular predictions are unlikely&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwc5z0t7ut7\"><sup><a href=\"#fnwc5z0t7ut7\">[2]</a></sup></span>.</p><h1>Black swans and AGI</h1><p>How are black swans relevant to AGI? In this section, I\u2019ll model the creation of AGI as a future black swan event. Note that black swans are just one mental model and may not apply to AGI or other models may be more appropriate. Though I think the concept of black swans is probably relevant and offers a unique way of seeing the future. The following claims are hypotheses that follow from modeling AGI as a black swan event and whether they are true is conditional on AGI being a black swan event.</p><h2>It will be difficult to predict when AGI will be created</h2><p>There have been many attempts to predict when AGI will be created using methods such as hardware <a href=\"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\">trend extrapolation</a> and <a href=\"https://nickbostrom.com/papers/survey.pdf\">expert elicitation</a> (surveys).</p><p>But if AGI is a black swan, it could be very difficult or even impossible to predict when it will be created. There are many historical examples of eminent scientists and inventors making poor predictions. For example, Ernest Rutherford declared, \u201canyone who looked for a source of power in the transformation of the atoms was talking moonshine\u201d just a day before Leo Szilard invented the nuclear chain reaction. Wilbur Wright predicted that powered flight was 50 years away just 2 years before it happened.</p><p>Even though I haven\u2019t met you, I can estimate with high confidence that your height is between 1 and 2 meters. This is because there are many past examples of human heights and height can be modeled using a thin-curved Gaussian distribution. Consequently, I can set a relatively narrow range for my 95% confidence interval estimation.</p><p>In contrast, statistical methods are less useful for unusual events such as technological breakthroughs. Often they happen just once and are outlier events. Research impact follows a power law. The discovery could be made by a single extraordinary person (e.g. Albert Einstein) or a small group of people.</p><p>We don\u2019t know how difficult it is to create an AGI. It\u2019s not entirely clear which trends are relevant or even how close we are to AGI. And since we don\u2019t know what the first AGI will look like, we might not even know that the first AGI really is one.</p><p>When thinking about AGI, it\u2019s easy to anthropomorphize it. For example, one can imagine AGI as a virtual office worker that can do a task that any human worker could do. But the first AGI will likely have a profile of strengths and weaknesses very different from a typical human. Therefore it\u2019s not even clear what \u2018human-level\u2019 even means or how to recognize it.</p><h3>Recommendations</h3><p>Although predictions are useful, it\u2019s important not to look too much into them as there is a lot of uncertainty about the future.</p><p>When reading a prediction such as \u201cthere is a 50% chance of AGI by 2040\u201d, consider the possibility that the probability distribution is very wide. Even if it is wide, it\u2019s possible for the actual event to fall outside of the distribution since black swans are often outliers.</p><p>Therefore I think we should be prepared for a wide range of future scenarios including very short or long timelines.</p><h2>It will be difficult to predict how AGI affects the world</h2><p>There are several vivid scenarios explaining how AGI could affect the world. For example, one scenario in the book Superintelligence describes a single AI singleton with a \u2018decisive strategy advantage\u2019 that \u2018strikes\u2019 and eliminates humanity. A familiar future possibility in the AI safety field is the superintelligent paperclip AI that converts the universe into paperclips because it is programmed to only value paperclips.</p><p>These scenarios are often useful for explaining principles such as the orthogonality thesis or how AGI could pose an existential risk. But by focusing too much on them we could become vulnerable to the narrative or conjunctive fallacy which involves assigning probabilities to specific scenarios that are too high.</p><p>Remember that the more details you add, the less likely your prediction is. Here is a list of predictions ordered from most to least likely:</p><ol><li>AGI will be created.</li><li>AGI will be created&nbsp;<strong>and</strong> take over the world.</li><li>AGI will be created&nbsp;<strong>and</strong> take over the world&nbsp;<strong>and&nbsp;</strong>convert the world into paperclips.</li></ol><p>Note the emphasis on the word \u2018and\u2019. Since P(A) &lt;= P(A and B) in probability, the situations are ordered from least to most probable despite the fact that the third scenario is the most compelling.</p><p>Even the concept of AGI itself is dubious because it assumes a known system that will be developed at some point in the future. In other words, AGI is a known unknown. But black swans are usually unknown unknowns.</p><p>We don\u2019t know what form AGI will take. Rather than describing some future system, I think the word AGI is intentionally vague and defines a space of possible future AI systems each with different strengths and weaknesses, levels of alignment, and architectures&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefowb5ni529gd\"><sup><a href=\"#fnowb5ni529gd\">[3]</a></sup></span>.</p><h3>Recommendations</h3><p>Be careful when using the term AGI and be mindful that it defines a space of possible systems that have certain capabilities. Consider using the term transformative AI (TAI) instead or as an alternative definition.</p><p>Beware of the conjunctive fallacy and specific predictions about how AI will affect the world in the future. Whenever you hear a prediction, consider the many other possibilities and set a low base rate probability for any specific prediction. Remember that more detailed predictions have a lower probability of coming true.</p><h2>The impact of AGI could be extreme</h2><p>Although black swans are rare, they matter because they have a high impact. For example, the single richest person in a town could have a greater net worth than the net worths of everyone else in the town combined.</p><p>The creation of AGI could have an impact that is extreme and unprecedented. For example, other black swans such as World War I had no precedent and had an extreme impact.</p><p>When AI is discussed in the media, many issues are discussed such as fairness, self-driving cars, and automation. While these developments are important, the most impactful effects of AI could be far more extreme. And like other long-tail events, the most extreme effect could outweigh the sum of all other effects.</p><p>Potential extreme impacts of AI in the future include human extinction, a singleton that has permanent control over the world, or digital minds. Many of the potential future impacts of AI currently lie outside of the Overton window. They are \u2018wild\u2019 or \u2018sci-fi\u2019 but according to black swan theory, these extreme long-tail events may be more likely than we think and could dominate the future and determine how it unfolds.</p><h3>Recommendations</h3><p>AI and AGI are unprecedented technologies that could have an extreme impact on society such as human extinction. We need to stretch our imaginations to consider extreme possibilities that are outside of the Overton window and could drastically alter the future.</p><h2>Past experience will have limited or little value</h2><p>Black swans are outliers with extreme randomness and a low probability of occurring and may happen only once. As Scott D. Sagan said, \u201cThings that have never happened, happen all the time.\u201d</p><p>Predictions of how AI will affect the future are often made based on past events. For example, AI is compared to the industrial revolution. People may say things like, \u201cAutomation has happened before.\u201d or \u201cHumanity has always overcome challenges.\u201d</p><p>But black swans are rare outliers that don\u2019t necessarily fit past trends. For example, neither World War I nor Chornobyl had any precedent and were not predicted.</p><h3>Recommendations</h3><p>When we look back at history, it seems to have a logical narrative but only in retrospect. The truth is that history is often much more random and unpredictable than it seems. Extreme events may not have clear causes except those added in hindsight.</p><p>When thinking about black swan events such as the creation of AGI or human extinction, we need to be open to extreme possibilities that have never happened before and don\u2019t follow previous events. The Christmas turkey couldn\u2019t have predicted its demise based on past experience but it was still a possibility.</p><p>Therefore, we need to consider possibilities that seem implausible or improbable because these events may be more likely than they seem or have an impact that dominates the future.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0g3mat21nfb7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0g3mat21nfb7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Bond prices often change in anticipation of wars and didn't change before World War I.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwc5z0t7ut7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwc5z0t7ut7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on black swans, I recommend the&nbsp;<a href=\"https://course.mlsafety.org/\"><u>lecture</u></a> and notes from the Intro to ML Safety course.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnowb5ni529gd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefowb5ni529gd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To appreciate how strange modern AI could be, I recommend reading the&nbsp;<a href=\"https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\"><u>Simulators</u></a> post.</p></div></li></ol>", "user": {"username": "Stephen McAleese"}}, {"_id": "9Axdtzusq9wSKixEZ", "title": "Historical Notes on Charitable Funds", "postedAt": "2022-12-04T23:30:00.123Z", "htmlBody": "<p><span>\n\nIn the EA movement people will sometimes talk about charitable funds\nas if they are a new idea.  For example, the recent </span>\n\n<a href=\"https://www.givingwhatwecan.org/\">Giving What We Can</a> post\n\"\n\n<a href=\"https://www.givingwhatwecan.org/why-we-recommend-funds\">why\nwe recommend using expert-led charitable funds</a>\" (\n\n<a href=\"https://forum.effectivealtruism.org/posts/wHyvkwpwCA4nm46rp/why-giving-what-we-can-recommends-using-expert-led\">forum\ndiscussion</a>) opens with:\n\n\n\n</p><p>\n\n</p>\n\n<blockquote>\nFunds are a relatively new way for donors to coordinate their giving\nto maximise their impact.\n</blockquote>\n\n\n\n<p>\n\nThere's a bit of a cute response that would be fun to write, except\nthat it isn't quite true:\n\n</p>\n\n<p>\n\n</p>\n\n<blockquote>\n\nDonating through a fund that is able to put more time and effort into\nevaluating charitable options is not a new idea, and is very natural\nif you're thinking along EA lines. In fact, the idea goes back at\nto the last time people tried to invent effective altruism, <a href=\"https://www.jefftk.com/p/scientific-charity-movement\">150 years ago</a>. While that\neffort has been through a few names over time, at this point you know\nit as the <a href=\"https://en.wikipedia.org/wiki/United_Way\">United\nWay</a>.\n\n</blockquote>\n\n\n\n<p>\n\nWhile this does have elements of truth, it implies historical\nmotivations were more similar to current ones than they really were.\nAs usual, history is much more nuanced.\n\n</p>\n\n<p>\n\nThe United Way is not a single grantmaker which will direct your money\nto wherever they think it will do the most good.  Instead, it's a\ncollection of local organizations, each responsible for their\nimmediate community.  This organizational structure reflects one of\nthe larger differences between the <a href=\"https://www.jefftk.com/p/scientific-charity-movement\">Scientific Charity Movement</a>\nand effective altruism: the former had a strong local focus, while EAs\npush for spending money wherever the impact is largest.  This\nstructure also comes from how the United Way grew out of the <a href=\"https://en.wikipedia.org/wiki/Community_Chest_(organization)\">Community\nChest</a> movement, where each city had their own Community Chest fund.\n\n</p>\n\n<p>\n\nComing from a modern EA perspective you might guess that the\nmotivation was to improve the allocation of money within a city, even\nif not between cities: instead of individuals needing to assess\ncompeting community charities, a central fund could weigh their needs\nand make informed tradeoffs.  But reading about the development of\nCommunity Chests (ex: <a href=\"https://www.jstor.org/stable/3004812\">Burleson 1926</a>, <a href=\"https://www.jstor.org/stable/2569895\">Todd 1932</a> and <a href=\"https://www.jstor.org/stable/2569896\">discussion</a>, <a href=\"https://www.google.com/books/edition/_/7pyFuWNyuyQC\">Seeley\n1957</a>) the core problem Community Chests were solving was\nduplicative fundraising.\n\n</p>\n\n<p>\n\nFor example, Todd (1932) writes about a \"federation of givers who were\ntired of miscellaneous appeals, and banded together for common\ndefense\" and \"this motive of protection against miscellaneous\nsolicitation on the one hand, and against serving in a constant\nsuccession of philanthropic 'drives' on the other, is still dominant\".\nInstead of competing, charities could band together and more\nefficiently (and less annoyingly!) fundraise jointly.  This also let\nthese campaigns fill a role similar to the war effort fundraising of\nthe 1910s, where they were widely seen as legitimate and democratic, a\nsafe way for wealthy residents and business owners to contribute.\n\n</p>\n\n<p>\n\nI do see a small amount of discussion on how centralized funding might\nimprove allocation, but it's pretty minor compared to fundraising\nscale, and it's mostly seen as a potential opportunity, not something\ncurrently done well:\n\n</p>\n\n<p>\n\n</p>\n\n<blockquote>\n\nOne of the arguments frequently offered for organizing a community\nchest is that it would give the directors of social agencies time to\nconsider the proper spending of their resources instead of having to\nput in all their time in raising money. That is to say, the\ncompetition would be shifted from finance to standards of performance.\nNot all communities have by any means reached such an ideal\nlevel. ... The process of accommodation frequently takes the form of\ncompromise.  In order to secure, as large a constituent membership as\npossible, the promoters of the community chest will admit agencies\nwhose standards may not measure up to the best levels of social work\nin the community. (Todd, 1932)\n\n</blockquote>\n\n\n\n<p>\n\nAnd:\n\n</p>\n\n<p>\n\n</p>\n\n<blockquote>\n\nThe Community Chest with its centralization of finance has a unique\nopportunity to set up machinery for gathering the information\nconcerning the community and concerning the work of the agencies,\nwhich would provide a more adequate basis for the social work of the\ncommunity.  I am quite aware that this is no new idea. A number of the\nchests have already made some excellent beginnings in setting up these\nresearch bureaus, under one form or another. And there is rather\ngeneral lip service to the importance of fact-finding. Perhaps the\nthirteen or fourteen years which represent the period of development\nof the Community Chest movement, is too short a time in which to\nexpect the creation of elaborate research departments. (North, 1932)\n\n</blockquote>\n\n\n\n<p>\n\nOther places where we see a similar kind of centralized allocation are\nlarge multi-focal charities (<a href=\"https://en.wikipedia.org/wiki/Catholic_Charities_USA\">Catholic\nCharities</a>, <a href=\"https://en.wikipedia.org/wiki/Oxfam\">Oxfam</a>, <a href=\"https://en.wikipedia.org/wiki/International_Rescue_Committee\">IRC</a>,\n<a href=\"https://en.wikipedia.org/wiki/American_Red_Cross\">Red\nCross</a>, etc).  These are different in structure in that a lot of\ntheir funding goes to running their own programs instead of\nregranting, though they do some of both.\n\n</p>\n\n<p>\n\nThe history of charitable funding is complex, and I've only touched on\nit here.  But I do think EAs should be cautious about talking about\ncharitable funds as if they're something we invented.  Instead, I\nthink they're better viewed as another iteration in a long history of\nfundraising and allocation.\n\n  </p>\n\n<p><i>Comment via: <a href=\"https://www.facebook.com/jefftk/posts/pfbid02JUdzorMzK7g1dXMQQ9azsJMCyqGCGESRuTHPkgyQaNmuVWnr1ADqbRYVvQo6LaZDl\">facebook</a>, <a href=\"https://mastodon.mit.edu/@jefftk/109457991954861854\">mastodon</a></i></p>", "user": {"username": "Jeff_Kaufman"}}, {"_id": "rhPQf8DdWbc4dAPm9", "title": "The Collapse of FTX Was a Black Swan Event", "postedAt": "2022-12-04T23:36:52.157Z", "htmlBody": "<p>The recent collapse of FTX was arguably a <a href=\"https://en.wikipedia.org/wiki/Black_swan_theory\">black swan</a> event. It was rare, improbable, highly impactful on EA, and explained in retrospect without being predicted beforehand. People have written about Sam Bankmans Fried's personality, way of operating companies, and other details to explain how the collapse happened.</p><p>If all these factors were present, then why wasn't the collapse <a href=\"https://www.jefftk.com/p/if-professional-investors-missed-this\">predicted</a> in advance? My hypothesis is that a big dose of randomness from the crypto crash was one of the primary causes of the problem and that post hoc explanations are overrated.</p><p>Although there are specific lessons from the event related to personality, money, morality and so on, a more general lesson and perhaps the most important lesson is the importance of black swans.</p><p>Black swans involve unknown unknowns. We don't know what will happen and we don't know what we don't know. Black swans are improbable, unpredictable, extreme, and highly impactful.</p><p>EA faces other potential black swans in the future such as bioengineered pandemics, the creation of AGI, virtual reality, extreme climate change, unexpected wars, and sudden political changes.</p><p>Fortunately EA posts and books such as <i>The Precipice</i> and <i>What We Owe the Future</i> consider a wide variety of future possibilities. Nevertheless, I think it's difficult to overemphasize the importance of black swans.</p><p>My recommendation is for EAs to see the FTX collapse as a black swan event and a warning shot to learn from. Going forward, when thinking about the future, it's important to consider many possible futures including extreme possibilities and low-probability, high-impact events.</p><p>An analogy is a game of chess: we should be creative and think hard about the future because the better we can foresee future possibilities, the more likely we are to be prepared for risks, take advantage of opportunities, and ensure that the future is desirable.</p>", "user": {"username": "Stephen McAleese"}}, {"_id": "9qcnrRD3ZHSwibtBC", "title": "EA Taskmaster Game", "postedAt": "2022-12-04T22:23:15.281Z", "htmlBody": "<h1>Introduction</h1><p>This is a post about a silly EA themed game I organised in London this year. All resources included in this post are free for you to use for your own EA events with the following caveats:&nbsp;</p><ul><li>If after reading this you have task ideas, please submit them here and they might be used in my next event:&nbsp;<a href=\"https://forms.gle/Pb7a9adRPTnchBaM6\"><u>GoogleForm | EA Taskmaster 2023 Task Ideas&nbsp;</u></a></li><li>If you want to run one of these yourself, I recommend reading my thoughts and suggestions in the&nbsp;<u>logistical points for organisers</u> section&nbsp;</li><li>If you do run one of these, I would love to hear about it (and see photos \ud83d\ude00). Please get in touch - you can message me here on the forum or contact me on Twitter <a href=\"https://twitter.com/glpat99\">glpat (@glpat99) / Twitter</a>.&nbsp;&nbsp;</li></ul><p>I had a fabulous time putting it together and the feedback from attendees was really positive so if this sounds like your kind of \u201corganised fun\u201d then I hope this post is helpful/inspiring. Thanks to Romy and Vania from the London EA women &amp; NB meet-up who pushed me to make this my first forum post!</p><h1><strong><u>The Game</u></strong></h1><p>In June 2022, I ran an EA themed game that was a mix between a scavenger hunt, the TV show&nbsp;<a href=\"https://en.wikipedia.org/wiki/Taskmaster_(TV_series)\"><u>Taskmaster</u></a> and a party quest game.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefagcoanudt3u\"><sup><a href=\"#fnagcoanudt3u\">[1]</a></sup></span>&nbsp;It was made up of 89 tasks (some EA themed, some just silly) to be completed by competing teams over the course of an afternoon.&nbsp;</p><p>The idea was that there would be too many tasks for a team to complete in the time limit and they would have to do some task prioritisation. When I was thinking about what tasks should be included, I wanted a range of tasks that could be thought of in terms of&nbsp;<a href=\"https://80000hours.org/articles/problem-framework/\"><u>scale, neglectedness and tractability</u></a>, so that it would be a&nbsp;<strong><u>much</u></strong> lower stakes game version of cause prioritisation:&nbsp;</p><ol><li><u>Scale</u>: There was a large variation in point values across the tasks, not necessarily in proportion to their difficulty. Also, some tasks were repeatable (with accumulating points) while others were one-offs.</li><li><u>Neglectedness</u>: For some tasks the points would only go to the team with the best entry or the points would be split between all teams that submitted. This incentivised completing tasks that no one else was completing.&nbsp;</li><li><u>Tractability</u>: There was a large range of task difficulty, with hard tasks not necessarily being rewarded with extra points.</li></ol><p>In fact, one of the tasks was to send in the best task prioritisation spreadsheet for the tasks in this game. As per the Taskmaster game show, final point allocation was at the discretion of the Taskmaster (me) and was based on ingenuity, grit and entertainment value.&nbsp;</p><p>Huge thanks to my EA friends in London for helping me think of tasks. Also, shout out to the Party Quest spreadsheet for providing many of the fun non-EA themed filler tasks.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj3o8igqp01\"><sup><a href=\"#fnj3o8igqp01\">[2]</a></sup></span></p><p>I am conscious that posting all the tasks would spoil the game for potential players so I have provided a small selection of some of the tasks and submissions from this year\u2019s event.&nbsp;</p><h2>Task examples</h2><p><u>Task 24: Sculptor (2000 points)</u></p><p>Construct the most beautiful trophy out of random objects and deliver it to the Taskmaster.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670193439/mirroredImages/9qcnrRD3ZHSwibtBC/dfpwakiawwifvdwwziku.png\"></p><p><i>The winner\u2019s podium with the constructed trophies given in order of the Taskmaster\u2019s preference</i></p><p><u>Task 19: I\u2019m Thirsty (800 points)</u></p><p>Bring the taskmaster the best cocktail.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670193439/mirroredImages/9qcnrRD3ZHSwibtBC/fjjaregakqrvfmgdpcgn.jpg\" alt=\"Gemma (on her handmade taskmaster throne) graciously accepting her third cocktail of the afternoon\"></p><p><i>The Taskmaster on her homemade throne graciously accepting her third cocktail of the afternoon</i></p><p><u>Task 51: An Order From the Ministry</u><a href=\"https://en.wikipedia.org/wiki/The_Ministry_of_Silly_Walks\"><u>*</u></a><u> (500 points)</u></p><p>Create the silliest walk</p><p><a href=\"https://drive.google.com/file/d/1DHogDhKMUySX2rQJLTlhETGOnRC6AfEi/view\"><u>Less Miserables' excellent submission</u></a></p><p><u>Task 67: Very Culture, Much Topical, Wow (1000 points)</u></p><p>Recreate a meme in real life and photograph/video it. The more elaborate the better.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670193439/mirroredImages/9qcnrRD3ZHSwibtBC/xgz63ominc2jijcn5jr9.jpg\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670193439/mirroredImages/9qcnrRD3ZHSwibtBC/lcms1jkl3nfhgmcu4oyx.jpg\"></p><p><u>Task 68: Getting down with the kids (1000 points)</u></p><p>Create an EA themed tiktok</p><p><a href=\"https://vm.tiktok.com/ZMNMfSCL4/?k=1&amp;fbclid=IwAR3JzeITmJ_enCjB45M2Mw_zToC5_LFdz9tYqzojUFsMy6qlr5wjkZt3118\"><u>Pink team's novel new approach to AI safety</u></a></p><h2><strong>Structure of game&nbsp;</strong></h2><p>There were four categories of task which were based on how points are allocated.&nbsp;</p><ol><li>Winner take all: All the points were awarded to the team whose task submission is deemed the best by the TM</li><li>Zero sum: Points were split between all teams who submitted for a task. The TM decided how the points were split. If you were the only team to submit and the TM thought your submission was satisfactory, you were guaranteed the full point value.</li><li>Multipliers: These tasks, if completed, would multiply your points at the end of the game.&nbsp;</li><li>Cumulative tasks: These tasks could be completed an unlimited number of times, with points accumulating each time. Points for submission of these tasks were non-competitive.&nbsp;</li></ol><p>Timings</p><ul><li>1pm - Start time</li><li>3pm - 1.5x point multiplier submission (natural break for teams)</li><li>6pm - Submission deadline&nbsp;</li><li>6-7pm - Break for calculation of final scores (chance to grab dinner)</li><li>7pm - Presentations of photo highlights and announcement of the winners</li></ul><p>Teams of 6-8 people - (ideally randomised so you can get to know new EAs).</p><ul><li>Teams create Whatsapp groups</li><li>Points for best team name</li><li>Points for team photo&nbsp;&nbsp;</li><li>Each team had an \u201cAlex\u201d who was just someone in each team who was in charge of \u201cvibes\u201d ie.&nbsp;<ul><li>Making sure everyone in their team felt included&nbsp;</li><li>Trying their best to keep things fun and cooperative within the team</li><li>Looking out for team members that are not taking care of themselves (ie. drinking water, having breaks, eating food, getting stressed)</li><li>Setting norms against cheating and bad faith submissions (funny and in bad faith was sometimes acceptable at the discretion of the taskmaster)</li><li>Being the tiebreaker in case of any disputes</li><li>Generally understood EA terms and could explain if team members had questions</li></ul></li></ul><p>Proof of task completion&nbsp;</p><ul><li>Photo/video of task completion</li><li>Delivery of items to Taskmaster</li><li>Performance of task to Taskmaster</li></ul><p>Other considerations</p><ul><li>I spent some time thinking about how I could make this game inclusive to all EAs but some of the tasks do involve travelling around London or are physically demanding. Some notes on this:<ul><li>Some tasks do require team members to go to specific locations or do physically demanding things. However, teams will be 6-8 members and can split up based on preference and ability.</li><li>There are 89 tasks in the game and part of the challenge is that you have to prioritise, preferences on tasks can be included in prioritisation.</li><li>I gave people the option to message me privately to flag an accessibility issue in advance so that I could take it into account for team allocation.</li></ul></li><li>I would also note that there are a couple of tasks that specifically ask for teams to spend money (&lt;\u00a35) to find items. This adds to a maximum of \u00a320 per team. As with all tasks, these are optional and if you are able to do them without spending money then that is fantastic.</li><li>The Taskmaster is the final arbiter point allocation and should value clever and creative over high expense solutions</li></ul><h1><strong>Logistical points for organisers</strong></h1><p>Things I did that were useful&nbsp;</p><ul><li>Hand out different coloured ribbons to identify different teams.&nbsp;</li><li>Print out paper copies of the tasks and instructions. Put them in envelopes with pens and ribbon so they could be easily handed to teams.</li><li>Having an \u201cAlex\u201d in each team took the pressure off me a lot as I felt like I could trust these people to keep the game on track (sorry Yellow team - you did so well despite your lack of Alex!!)</li><li>The incentive to submit as many tasks as possible by 3pm really helped me on the backend by giving me more time to organise submissions.<br>&nbsp;</li></ul><p>What would I do differently next time:</p><ul><li>I would add more fundraising elements (if you haven\u2019t donated already - shout out to the&nbsp;<a href=\"https://www.givingwhatwecan.org/fundraisers/ea-meme-2022\"><u>EA Meme Twitter 2022 Giving Season Fundraiser \u00b7 Giving What We Can</u></a>.)</li><li>I would remove the submission deadline for the Cause Prioritization spreadsheet. Did not have the time to review it that early on.</li><li>I would have one place for submissions. On the back end, trying to upload everything that wasn\u2019t on a form to my google drive was a nightmare.</li><li>I would ditch cumulative tasks (or find a better way to count them) as the google forms that I used for this took ages to sort out on the back end.&nbsp;</li><li>I would add more directly competitive tasks. I really enjoyed the last minute Bogies game.</li><li>I think I put too high a weight on not allowing teams to see what others were doing (mostly to encourage upfront strategizing) but in hindsight I don\u2019t think this was the most fun part. I also think it would have encouraged further creativity (as teams one up each other) and would have meant that all tasks would have been attempted when people saw that no-one had done them.&nbsp;</li><li>I would be more explicit that the scale neglectedness and tractability framework could be used. While I designed the task list with this in mind, most teams did not explicitly use it in their prioritisation spreadsheet.</li></ul><p>Thoughts on how you could customise this for your purposes</p><ul><li>When asking people about my ideas for this event, there was quite a lot of push back on the length. My logic is that if it were any shorter than 5 hours, people would feel too anxious to take breaks for food etc whereas I did see a lot of people take time (especially after the 3pm deadline). For the 2023 event, I plan on having it be the same length</li><li>You could cut many of the tasks to create a shorter game and it would function similarly</li></ul><h1><strong>The tasklist and backend spreadsheet (FINALLY!)</strong></h1><ul><li>This is the instruction and task list that was handed out to each team:&nbsp;<a href=\"https://docs.google.com/document/d/1C5AiW0oBlUp1bCs1LxC7L2cd6fAnp3zRPDOzWG5Qmss/edit?usp=sharing\"><u>Taskmaster Instructions for Participants</u></a></li><li>Excel version of full task list:&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1MOlflEO-W5auhMcxHt8ZtM6l6udBZU3pxBUb4ZqfPsI/edit?usp=sharing\"><u>EA TaskMaster Tasks</u></a></li><li>Backend excel for totalling points and organising teams:&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1b_EvISZv8X6bq2JO7b9ZWui5kQu7v3h5sNtopd5WC-Q/edit?usp=sharing\"><u>TM Control Centre</u></a></li><li>Google doc for sharing results, point allocations and pictures:&nbsp;<a href=\"https://docs.google.com/document/d/1tBjilZKarDN38bWifveMn3DPLASHrCO_QJbTr3vXhtQ/edit?usp=sharing\"><u>Anonymised 2022 EA Taskmaster Results and Reflections</u></a></li></ul><p>Have fun folks!<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnagcoanudt3u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefagcoanudt3u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The impetus for this came from a conversation I had with a friend where they said that despite working for an EA organisation and attending conferences, they had not been able to build casual friendships with other folks driven by EA. Conversations seemed to devolve into professional networking which, while important, can be exhausting and serves a different function than connection with those who share your values. I think this is really important in the conversation about EA mental health and would be keen to chat to people with ideas on how to work on this in London.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj3o8igqp01\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj3o8igqp01\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Thanks to Ali, Conrad, Chris, Amber, David, Sam, Toby and Reuben for your task suggestions. Most of the excellent EA themed ones came from these guys so very lucky to have them on board!</p><p>Special thanks to Conrad who was amazing in the early stages to bounce ideas off of and to hype me up. Gutted you didn\u2019t make it but there\u2019s always next year :D&nbsp;</p><p>Honourable mention to my housemates who were very patient about the mess I made while painting the throne.&nbsp;</p><p>Shout out to Beth Cunningham who sat on the phone with me for about an hour coming up with pun-tastic names for the tasks - I\u2019m so very lucky to have this word-play whizz in my life &lt;3 I love you from your Take My Breath AWeigh to your Fer-Addme &lt;3 (While she deserves most of the credit, I demand recognition for \u201cA Hidden Gem\u201d).</p><p>Special thanks to Nathan Young who has been incredibly supportive during the run up to this while I was bouncing from excitement to dread that no-one was going to show up.</p></div></li></ol>", "user": {"username": "Gemma Paterson"}}, {"_id": "xt6ZSpaEGGweoeyaP", "title": "Bradley Tusk on How to Give (Podcast)", "postedAt": "2022-12-04T20:23:13.390Z", "htmlBody": "<p>This is a linkpost for an episode of <a href=\"https://en.wikipedia.org/wiki/Bradley_Tusk\">Bradley Tusk's</a> regular podcast where he covers effective giving and his thoughts on EA post SBF. Tusk is a fascinating person operating at the intersection of tech and policy (my understanding is he made much of his early fortune as the point person doing public policy for Uber and has worked in a variety of government roles including Comms Director for Senator Schumer and Campaign Manager for Mike Bloomberg's successful mayoral run in NYC). Especially interesting after ~28:15.</p>", "user": {"username": "atlasunshrugged"}}, {"_id": "RCmgGp2nmoWFcRwdn", "title": "Should strong longtermists really want to minimize existential risk?", "postedAt": "2022-12-04T16:56:03.791Z", "htmlBody": "<p>Strong longtermists believe there is a non-negligible chance that the future will be enormous. For example, earth-originating life may one day fill the galaxy with&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10^{40}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">40</span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;digital minds. The future therefore has enormous expected value, and concern for the long-term should almost always dominate near-term considerations, at least for those decisions where our goal is to maximize expected value.</p><p>It is often stated that strong longtermism reduces in practice to the goal: \u201cminimize existential risk at all costs\u201d. I argue here that this is inaccurate. I claim that a more accurate way of summarising the strong longtermist goal is: \u201cminimize existential risk at all costs <i>conditional on the future possibly being very big\u201d.</i> I believe the distinction between these two goals has important practical implications. The strong longtermist goal may actually conflict with the goal of minimizing existential risk unconditionally.</p><p>In the next section I describe a thought experiment to demonstrate my claim. In the following section I argue that this is likely to be relevant to the actual world we find ourselves in. In the final section I give some concluding remarks on what we should take away from all this.</p><h2><strong>The Anti-Apocalypse Machine</strong></h2><p><i>The Earth is about to be destroyed by a cosmic disaster. This disaster would end all life, and snuff out all of our enormous future potential.</i></p><p><i>Fortunately, physicists have almost settled on a grand unified theory of everything that they believe will help them build a machine to save us. They are 99% certain that the world is described by Theory A, which tells us we can be saved if we build Machine A. But there is a 1% chance that the correct theory is actually Theory B, in which case we need to build Machine B. We only have the time and resources to build one machine.</i></p><p><i>It appears that our best bet is to build Machine A, but there is a catch. If Theory B is true, then the expected value of our future is many orders of magnitude larger (although it is enormous under both theories). This is because Theory B leaves open the possibility that we may one day develop slightly-faster-than-light travel, while Theory A being true would make that impossible.</i></p><p><i>Due to the spread of strong longtermism, Earth's inhabitants decide that they should build Machine B, acting as if the speculative Theory B is correct, since this is what maximizes expected value. Extinction would be far worse in the Theory B world than the Theory A world, so they decide to take the action which would prevent extinction in that world. They deliberately choose a 99% chance of extinction over a 1% chance, risking all of humanity, <strong>and all of humanity's future potential</strong>.</i></p><p>The lesson here is that strong longtermism gives us the goal to minimize existential risk <i>conditional on the future possibly being very big, </i>and that may conflict with the goal to minimize existential risk unconditionally.</p><h2><strong>Relevance for the actual world</strong></h2><p>The implication of the above thought experiment is that strong longtermism tells us to look at the set of possible theories about the world, pick the one in which the future is largest, and, if it is large enough, act as if that theory were true. This is likely to have absurd consequences if carried to its logical conclusion, even in real world cases. I explore some examples in this section.</p><p>The picture becomes more confusing when you consider theories which permit the future to have infinite value. In Nick Beckstead's original thesis, <a href=\"https://rucore.libraries.rutgers.edu/rutgers-lib/40469/PDF/1/play/\">On the Overwhelming Importance of Shaping the Far Future</a>, he explicitly singles out infinite value cases as examples of where we should abandon expected value maximization, and switch to using a more timid decision framework instead. But even if strong longtermists are only reckless in large finite cases, that should still be enough for them to be forced to adopt extremely speculative scientific theories (using 'adopt' as a shorthand for 'act as if this theory were true').</p><p>Out of all our scientific knowledge, the 2<sup>nd</sup> law of thermodynamics is arguably one of the principles that is least likely to be proven wrong. But we can't completely rule out the possibility that counter-examples will one day be found. The 2<sup>nd</sup> law also puts strict limits on how big the future can be. I claim strong longtermists should therefore act as if the 2<sup>nd</sup> law will turn out to be false. The same goes for any other currently understood physical limit on our growth, such as the idea that information cannot travel faster than light.</p><p>This may well have practical implications for the work that strong longtermists are currently doing on existential risk. For example, perhaps it implies that they should adopt a big distrust of the scientific establishment. There are already people on the internet who claim to have built machines which violate the 2<sup>nd</sup> law of thermodynamics. The scientific establishment have largely been ignoring these amateur scientists' claims so far. If strong longtermists should act as if these amateur scientists are correct to reject the 2<sup>nd</sup> law, then that might mean putting less weight on the opinions of the scientific establishment, and more weight on the opinions of these amateurs.</p><p>It could be fairly objected that in a world where the 2<sup>nd</sup> law of thermodynamics is false, it is more likely to be overturned by mainstream physics, than by a random youtuber. If this is true, then perhaps the preceding claim that strong longtermists should distrust the scientific establishment goes too far. Nevertheless, strong longtermists should still act as if the 2<sup>nd</sup> law will one day turn out to be broken, or that faster-than-light travel will one day turn out to be possible, since we can't rule these possibilities out completely, and they contain enormous expected value. I find it hard to believe that a commitment to such fundamental and unlikely beliefs would not have any practical implications.</p><p>On a more practical level, once we condition on the future potentially being enormous, that should lead us to overestimate humanity's ability to coordinate to solve global problems, relative to what our estimation would have been without this conditioning, since such coordination will surely be necessary for us to spread throughout the galaxy. This overestimation may then lead us to different prioritisations among the current existential risks we face, than if we were just trying to minimize existential risk unconditionally.</p><p>Overall, I think we should expect the attempts of strong longtermists to reduce existential risk to be hindered, at least to some extent, if they are committed to adopting descriptions of the world which permit the largest possible future value, rather than descriptions of the world which are most likely to be correct.</p><p>I believe that the goals \u201cminimize existential risk\u201d and \u201cminimize existential risk conditional on a possibly big future\u201d are likely to conflict in practice, not just in principle.</p><h2><strong>Conclusion</strong></h2><p>Hopefully it is clear that this post is intended to be taken as a critique of strong longtermism, rather than as a recommendation that we should abandon the 2<sup>nd</sup> law of thermodynamics. I believe the take away here should be that possible futures involving enormous numbers of digital minds should feature less heavily in our prioritisation decisions than they do in the standard strong longtermist framework.</p>", "user": {"username": "tobycrisford"}}, {"_id": "Stbmark4hCgB5wrsn", "title": "Looking Back, Moving Forward: a cozy evening of reflection and planning [please RSVP]", "postedAt": "2022-12-04T16:17:06.009Z", "htmlBody": "<p>What were you up to two weekends ago? If you\u2019re anything like me, you struggled to answer this question without looking at a calendar, or tracing back your steps. What if I asked you who you were one year ago?</p><p>In 2022: What did you achieve? Where did you find challenges? Who did you meet along the way? In this evening get-together, from 17h to 19h, we will individually go through our calendars and summarize any meaningful events of the past year.</p><p>At 19h we will begin guided exercises for connection, reflection and planning. And, once we have taken a good look at our past, we will wave it a gentle goodbye and focus our efforts on the future. How are you going to master 2023?</p><p>Come, and get inspired and energized with us.</p><p>WHEN: December 28th. 17h-19h individual work, 19h-22h workshop.</p><p>WHERE: Aurea, Boxhagener Strasse 117, Berlin, Germany</p><p>BRING: Any tool you use as a calendar, a journal/notebook and a pen</p><p>You are free to skip the reflection and only drop in for the guided section at 19h. In that case, we ask you to go through your calendar beforehand so you have a fresh memory of what happened throughout the year.</p>", "user": {"username": "Severin T. Seehrich"}}, {"_id": "saEXX9Nucz8mh9XgB", "title": "Race to the Top: Benchmarks for AI Safety", "postedAt": "2022-12-04T22:50:55.708Z", "htmlBody": "<p>This is an executive summary of a blog post. Read the full texts <a href=\"https://isaduan.github.io/isabelladuan.github.io/posts/first/\">here</a>.&nbsp;</p><h2>Summary</h2><p>Benchmarks support the empirical, quantitative evaluation of progress in AI research. Although benchmarks are ubiquitous in most subfields of machine learning, they are still rare in the subfield of <i>AI safety</i>.</p><p>I argue that <strong>creating benchmarks should be a high priority for AI safety.</strong> While this idea is not new, I think it may still be underrated. Among other benefits, benchmarks would make it much easier to:</p><ul><li>track the field\u2019s progress and focus resources on the most productive lines of work;</li><li>create professional incentives for researchers - <i>especially Chinese researchers</i> - to work on problems that are relevant to AGI safety;</li><li>develop auditing regimes and regulations for advanced AI systems.</li></ul><p>Unfortunately, <strong>we cannot assume that good benchmarks will be developed quickly enough \u201cby default.\"</strong> I discuss several reasons to expect them to be undersupplied. I also outline actions that different groups can take today to accelerate their development.<br><br>For example, <strong>AI safety researchers can help by:</strong></p><ul><li>directly trying their hand at creating safety-relevant benchmarks;</li><li>clarifying certain safety-relevant traits (such as \u201chonesty\u201d and \u201cpower-seekingness\u201d) that it could be important to measure in the future;</li><li>building up relevant expertise and skills, for instance by working on other benchmarking projects;</li><li>drafting \u201cbenchmark roadmaps,\u201d which identify categories of benchmarks that could be valuable in the future and outline prerequisites for developing them.</li></ul><p>And <strong>AI governance professionals can help by:</strong></p><ul><li>co-organizing workshops, competitions, and prizes focused on benchmarking;</li><li>creating third-party institutional homes for benchmarking work;</li><li>clarifying, ahead of time, how auditing and regulatory frameworks can put benchmarks to use;</li><li>advising safety researchers on political, institutional, and strategic considerations that matter for benchmark design;</li><li>popularizing the narrative of a \u201crace to the top\u201d on AI safety.</li></ul><p>Ultimately, <strong>we can and should begin to build benchmark-making capability now.</strong>&nbsp;</p><p>&nbsp;</p><h2>Acknowledgment</h2><p>I would like to thank Ben Garfinkel and Owen Cotton-Barratt for their mentorship, Emma Bluemke and many others at the <a href=\"https://www.governance.ai/\">Centre for the Governance of AI&nbsp;</a> for their warmhearted support. All views and errors are my own.&nbsp;</p><p>&nbsp;</p><h2>Future research</h2><p>I am working on a paper on the topic, and if you are interested in benchmarks and model evaluation, especially if you are a technical AI safety researcher, I would love to <a href=\"mailto: isabelladuan@uchicago.edu\">hear</a> from you! &nbsp;&nbsp;</p>", "user": {"username": "isaduan"}}, {"_id": "K5Snxo5EhgmwJJjR2", "title": "Announcing: EA Forum Podcast \u2013 Audio narrations of EA Forum posts", "postedAt": "2022-12-05T21:50:14.551Z", "htmlBody": "<p><strong>We've started making audio narrations of some of the best posts from the EA Forum.</strong></p><p>As of today, you can <strong>subscribe to the podcast</strong>:</p><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/mtfu8vahtwvryzitseeo\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/hceu0lmcsl1qzdydydg8 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/n0yzkupqrbm5xscm9tes 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/um9ee0wr2hcfchoqjh1t 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/k1cpyiv4i0mrdtw0boyn 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/rrfw8bajhsqhejdnmqgh 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/xovtjhv4sxz2gwhwueo2 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/r7utkcfkyzv5jwzupxmz 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/uaipy0fk3m0j1sdbnsjd 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/xzoposk9l55l8w17xnip 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/B5hnpo2yDv9Hstpka/hqwoc7hetrlztw8skdko 1500w\"></figure></td><td><strong>EA Forum (Curated &amp; Popular)</strong><br>Audio narrations from the Effective Altruism Forum, including curated posts and posts with 125+ karma.<br><br>Subscribe:<br><a href=\"https://podcasts.apple.com/us/podcast/1657526204\">Apple Podcasts</a> | <a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mb3J1bS1wb2RjYXN0cy5lZmZlY3RpdmVhbHRydWlzbS5vcmcvZWEtZm9ydW0tLWFsbC1hdWRpby5yc3M\">Google Podcasts</a> | <a href=\"https://open.spotify.com/show/25mjyNzTzozA5UNye23fen\">Spotify</a> | <a href=\"https://forum-podcasts.effectivealtruism.org/ea-forum--all-audio.rss\">RSS</a></td></tr></tbody></table></figure><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/la8gyvaxu1osj8wpdj5p.jpg\"></figure></td><td><strong>EA Forum (All audio)</strong><br>Audio narrations from the Effective Altruism Forum, including curated posts, posts with 30+ karma, and other great writing.<br><br>Subscribe:<br><a href=\"https://podcasts.apple.com/us/podcast/ea-forum-podcast-all-audio/id1690124329\">Apple Podcasts</a> | <a href=\"https://open.spotify.com/show/3XDSyrl2YZUCIkTgScvaOR\">Spotify</a> | <a href=\"https://forum-podcasts.effectivealtruism.org/ea-forum--all.rss\">RSS</a> | Google Podcasts (soon)</td></tr></tbody></table></figure><p><strong>Edit 2023-05-30:</strong> we reworked our podcast feeds a bit. We've updated the list above to reflect the latest selection.</p><hr><p>You'll also start to see <strong>narrations embedded on the EA Forum post pages</strong> themselves.&nbsp;</p><p>If a narration is available, you'll see a blue loudspeaker button:</p><figure class=\"image image_resized\" style=\"width:68.91%\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/bok4jp95eufyti8qxbsv.jpg\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/szngvk2ioqjmhzfb9ena.jpg 121w, https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/hkybex86tv8dyoqkzfbl.jpg 201w, https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/i9rqfeiszmgbzujr23o3.jpg 281w, https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/f0z4iwfazmob7uhntt96.jpg 361w, https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/vuymmpu6gs3f97gphsqo.jpg 441w, https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/ghzez4bk5snboor5hxk3.jpg 521w, https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/kq23fd4axutjrwf7morz.jpg 601w, https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/fbosj8gbdlj5qyeszyok.jpg 681w\"></figure><hr><h2>What can I listen to now?</h2><p>Some of the winning entries from the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\"><u>EA Criticism and Red Teaming Contest</u></a>:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\"><u>Are you really in a race? The cautionary tales of Szil\u00e1rd and Ellsberg</u></a> by Haydn Belfield</li><li><a href=\"https://forum.effectivealtruism.org/posts/coryFCkmcMKdJb7Pz/does-economic-growth-meaningfully-improve-well-being-an\"><u>Does economic growth meaningfully improve well-being?</u></a> by Vadim Albinsky</li><li><a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\"><u>Effective altruism in the garden of ends</u></a> by Tyler Alterman</li><li><a href=\"https://forum.effectivealtruism.org/posts/NHsH9pHZ7rA3KcnC7/notes-on-effective-altruism\"><u>Notes on effective altruism</u></a> by Michael Nielsen</li><li><a href=\"https://forum.effectivealtruism.org/posts/bFDwxxfErRStMvuAQ/biological-anchors-external-review-by-jennifer-lin-linkpost\"><u>Biological Anchors external review</u></a> by Jennifer Lin</li></ul><p>Some posts that were recently marked as \u201ccurated\u201d:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/zoWypGfXLmYsDFivk/counterarguments-to-the-basic-ai-risk-case\"><u>Counterarguments to the basic AI risk case</u></a> by Katja Grace</li><li><a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future\"><u>My take on What We Owe the Future</u></a> by Eli Lifland</li><li><a href=\"https://forum.effectivealtruism.org/posts/dQvDxDMyueLyydHw4/population-ethics-without-axiology-a-framework\"><u>Population ethics without axiology: A framework</u></a> by Lukas Gloor</li><li><a href=\"https://forum.effectivealtruism.org/posts/jk7A3NMdbxp65kcJJ/500-million-but-not-a-single-one-more\"><u>500 million, but not a single one more</u></a> by jai</li><li><a href=\"https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in\"><u>AGI and lock-in</u></a> by Lukas Finnveden, Jess Riedel, and Carl Shulman</li><li><a href=\"https://forum.effectivealtruism.org/posts/nGrmemHzQvBpnXkNX/what-matters-to-shrimps-factors-affecting-shrimp-welfare-in\"><u>What matters to shrimps? Factors affecting shrimp welfare in aquaculture</u></a> by Lucas Lewit-Mendes and Aaron Boddy</li><li><a href=\"https://forum.effectivealtruism.org/posts/sgcxDwyD2KL6BHH2C/case-for-emergency-response-teams\"><u>Case for emergency response teams</u></a> by Gavin and Jan Kulveit</li><li><a href=\"https://forum.effectivealtruism.org/posts/rXYW9GPsmwZYu3doX/what-happens-on-the-average-day\"><u>What happens on the average day?</u></a> by Rose Hadshar</li><li><a href=\"https://forum.effectivealtruism.org/posts/PyZCqLrDTJrQofEf7/how-bad-could-a-war-get\"><u>How bad could a war get?</u></a> by Stephen Clare and Rani Martin</li></ul><h2><s>Cool. But I have a lot of things to listen to. Could I just get the curated posts, or maybe just the summaries?&nbsp;</s></h2><p><s>Yes. You can subscribe to either of these:</s></p><figure class=\"table\"><table><tbody><tr><td style=\"width:25%\"><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/sfcrvg4yjbwk78zvgpow.jpg\"></figure></td><td><p><strong>Edit 2023-05-30:</strong> This feed has been retired. Subscribe to \"Effective Altruism (Curated &amp; Popular)\" instead.</p><p><s><strong>EA Forum (Curated posts)</strong></s><br><s>Audio narrations of </s><a href=\"https://forum.effectivealtruism.org/recommendations\"><s>curated posts</s></a><s> from the Effective Altruism Forum.&nbsp;</s><br><br><s>Subscribe:</s><br><a href=\"https://podcasts.apple.com/us/podcast/1657526383\"><s>Apple Podcasts</s></a><s> | Google Podcasts (soon) | </s><a href=\"https://open.spotify.com/show/2Ki0q34zEthDfKUB56kcxH\"><s>Spotify</s></a><s> | </s><a href=\"https://forum-podcasts.effectivealtruism.org/ea-forum--curated-posts.rss\"><s>RSS</s></a></p></td></tr><tr><td><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1674209991/mirroredImages/K5Snxo5EhgmwJJjR2/it7lq66u7okevns3vyda.jpg\"></figure></td><td><p><strong>Edit 2023-05-30:</strong> The summaries project has been discontinued.</p><p><s><strong>EA Forum (Summaries)</strong></s><br><a href=\"https://forum.effectivealtruism.org/topics/weekly-summaries-project\"><s><u>Weekly summaries</u></s></a><s> of the best EA Forum posts. Written by Zoe Williams (Rethink Priorities) and narrated by Coleman Jackson Snell.</s><br><br><s>Subscribe:</s><br><a href=\"https://podcasts.apple.com/us/podcast/1657527046\"><s>Apple Podcasts</s></a><s> | </s><a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mb3J1bS1wb2RjYXN0cy5lZmZlY3RpdmVhbHRydWlzbS5vcmcvZWEtZm9ydW0tLXN1bW1hcmllcy5yc3M\"><s>Google Podcasts</s></a><s> | </s><a href=\"https://open.spotify.com/show/7KWMzE55MsZa6w5gGEPVOn\"><s>Spotify</s></a><s> | </s><a href=\"https://forum-podcasts.effectivealtruism.org/ea-forum--summaries.rss\"><s>RSS</s></a></p></td></tr></tbody></table></figure><h2>What about AI narrations? I want to listen to everything!</h2><p><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\"><u>The Nonlinear Library</u></a> project is currently generating AI narrations of all posts that meet a fairly low karma threshold.&nbsp;</p><p>Within the next few months, we are hoping to collaborate with Nonlinear to develop a system that generates even better AI narrations for most or all EA Forum posts. We see a path to better pronunciation, emphasis, and tone, and also to much better handling of images, graphs and formulae.</p><h2>Who is working on this?</h2><p>This project is run by the EA Forum Team, in collaboration with&nbsp;<a href=\"https://type3.audio/\"><u>TYPE III AUDIO</u></a>.</p><h2>What about the existing \u201cEA Forum Podcast\u201d?</h2><p>In July 2021, Garrett Baker and David Reinstein started a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oXt66Ny6FCoATKasn/the-ea-forum-podcast-is-up-and-running\"><u>volunteer project to narrate EA Forum posts</u></a>. The most recent narration was published in January 2022. Since September they\u2019ve been publishing Zoe and Coleman\u2019s weekly summary episodes.</p><p>We are grateful to Garrett and David for their work on this.</p><p>We\u2019ve not yet heard what they plan to do next\u2014presumably they\u2019ll release an update to subscribers in due course.</p><h2>Thoughts, feedback, suggestions?</h2><p>We'd love to hear from you! Please comment below, or write to&nbsp;<a href=\"mailto:team@type3.audio\"><u>team@type3.audio</u></a>.&nbsp;</p><p>If you\u2019re already listening to Nonlinear Library AI narrations, we\u2019d be especially interested to hear what you think of them. What would you most like to see improved?</p>", "user": {"username": "Peter_Hartree"}}, {"_id": "4RXxawcDQcAcMDn5B", "title": "Grantmaking Bowl: An EA Student Competition Idea", "postedAt": "2022-12-06T03:27:40.814Z", "htmlBody": "<p><em>This is a free idea that I am not currently working on making happen. If you are interested in trying to make it happen, I highly encourage you to undertake further investigations.</em></p>\n<h1>Bottom-Line Up Front</h1>\n<p>As the number of universities with EA student groups grows, EA should develop a wider variety of standardized activities that such groups can run. Reading groups and fellowships are great, but probably less exciting than ideal. Competitive, student-friendly academic activities (e.g., debate, quiz bowls, STEM olympiads, hackathons, moot courts) are often a fun activity for people interested in a particular area.</p>\n<p>Thus the idea for <strong>Grantmaking Bowl</strong>: A collaborative competition wherein EA student groups are asked to analyze a common set  of EA granting case studies across several cause areas (which could be based on real grants considered by major orgs) in front of judges with significant EA experience. This would hopefully have several key benefits:</p>\n<ol>\n<li>It'd be fun.</li>\n<li>It'd be a great way for EA students to bond both within their schools and across schools (at competitions).</li>\n<li>It'd be a well-structured way for students to dig into a variety of core and peripheral cause areas.</li>\n<li>It'd develop a skillset (grant analysis and recommendation) that is useful in EA.</li>\n<li>It could help grantmakers and other EA orgs spot promising talent.</li>\n<li>It could recruit more EAs as a result of the above.</li>\n</ol>\n<h1>Background</h1>\n<p>Reflecting on my experience as a university student and EA chapter lead, I think a lot about what types of events and activities that EA student groups can run. I think EA student groups are in a hard place: it's hard for students to have a direct impact, beyond perhaps a lucky few that can land a research assistant job or something similar and trying to <a href=\"https://www.1fortheworld.org/\">recruit pledgers for effective charities</a>.</p>\n<p>On the other hand, the main structured and portable activity I hear about student groups running nowadays is something like a fellowship or introductory course to some core EA ideas. To be clear, I don't think Grantmaking Bowl can or should replace these things. But I think we ought to have more activities for student groups to host, especially ones that are a bit more fun-shaped than fellowships and introductory courses.</p>\n<h1>Core Idea</h1>\n<p><strong>Grantmaking Bowl</strong> is an in-person collaborative competition to recognize EA students who do an excellent job analyzing a set of reality-inspired grant propositions that major EA funders could face, presented in case study format. To break this down:</p>\n<ul>\n<li>Grantmaking Bowl would be a collaborative competition, much in the spirit of <a href=\"https://en.wikipedia.org/wiki/Intercollegiate_Ethics_Bowl\">Ethics Bowl</a>. While ultimately a competition in the sense of producing some ordinal ranking of participating teams, the emphasis (as created by scoring criteria and event format) would not be on zero-sum competition with other teams, but rather the demonstration of excellence at core grantmaking skills.</li>\n<li>The central task that competitors would be evaluated on is their excellence at in the <em>process</em> of applying EA styles of reasoning and evaluation to the grant candidate cases studies.</li>\n</ul>\n<p>Why compete on grantmaking (as opposed to, e.g., career choice or public policy) recommendations? Several reasons:</p>\n<ol>\n<li>Grantmaking tradeoffs are much more legible to students than career choice. Career choice depends much more heavily on personal fit, and is harder to make discrete decisions about.</li>\n<li>Public policy debate is already a thing.</li>\n<li>Since money is basically useful for arbitrary tasks (including facilitating career choices and public policies), it is the most open-ended (and therefore versatile and adaptive) domain for competition.</li>\n<li>Grantmaking has historical significance to EA, with effective charity choice being one of the earliest and easiest to understand examples EA. Thus, getting newbies to think a lot about grantmaking gets them up-to-speed on the history of EA.</li>\n<li>Some orgs use hypothetical grantmaking as a way to evaluate applicants.</li>\n</ol>\n<h1>Formatting Variables</h1>\n<p>I've intentionally left unspecified many important variables of Grantmaking Bowl. I list some of these variables and possible values for them below. These variables should probably be iteratively explored to advance the goals of Grantmaking Bowl.</p>\n<h2>Preparation</h2>\n<p>The competition could either be prepared or extemporaneous.  In the extemporaneous, participants would be given their case study/ies the day of the competition, given some number of hours to make an analysis, then present the same day. For prepared presentations, students could be given a packet of case studies and be expected to present about (some randomly chosen subset of?) them on competition day.</p>\n<h2>Cause Areas</h2>\n<p>Participants could be expected to master a variety of cause areas, or self-select into cause areas that interest them.</p>\n<h2>Open- versus Closed-Ended</h2>\n<p>Participants could be asked to pitch a grant that they develop, or comment on some case studies prepared by the Bowl coordinators (as in Ethics Bowl).</p>\n<h2>Team versus individual</h2>\n<p>Participants could present individually or as a team.</p>\n<h2>Presentation Modality</h2>\n<p>Presentations could be purely oral, or incorporate slideshow and prose formats.</p>\n<h2>Judging</h2>\n<p>EAs with grantmaking experience could judge.</p>\n<h2>Adversarial-ness</h2>\n<p>To avoid excessive adversarial-ness, the competition could be structured to incentivize participants to seek to maximize some total score, rather than the probability that they \"beat\" any particular opponent. That is, you could avoid adversarialness by not having any head-to-head parts of the competition.</p>\n<h2>Prizes!</h2>\n<p>An obvious thing to do would be to give the winner(s) precatory rights to pots of EA money! This would hopefully both increase the fun and be a good way to select who can allocate EA money, since by supposition the winners of Grantmaking Bowl have been selected for good grantmaking skills.</p>\n", "user": {"username": "Cullen_OKeefe"}}, {"_id": "dzbMDe8dyQvmvjrEs", "title": "AI Safety in a Vulnerable World: Requesting Feedback on Preliminary Thoughts", "postedAt": "2022-12-06T22:36:18.033Z", "htmlBody": "<p>Cross-Posted to <a href=\"https://www.lesswrong.com/posts/iTmu5nrrtqHGe9iCr?_ga=2.41445218.1363765687.1670286568-692721325.1669858861\">LessWrong</a></p><p>I would like feedback on a hypothesis that has been percolating in my brain for the past few months.</p><p>Epistemic Status: I have studied AI Safety for less than 100 hours, but have been thinking about x-risk for several years.</p><p>I am concerned that even in some cases where advanced AI is aligned, the environment in which it exists may still make it unsafe.</p><p>If I am not mistaken, \u201cAI Alignment\u201d seems to mean getting AI to do what we want without harmful side effects, but \u201cAI Safety\u201d seems to imply keeping AI from harming or destroying humanity.</p><p>These two may come apart in a <a href=\"https://nickbostrom.com/papers/vulnerable.pdf\">\u201cVulnerable World Scenario\u201d</a> in which some future technologies destroy civilization by default. This may be the case because certain technologies have an intrinsic offense bias, meaning if even a small number of humans want to kill everyone, or competing groups are willing to kill each other, those attacking will succeed and those defending will fail by default.</p><h2>Offense Bias</h2><p>If there is an offense bias in advanced AI, or any other technology advanced AI leads to, it is not clear that aligning AI i.e. \u201cgetting AI to do what we want\u201d would keep us safe. If multiple world powers have advanced AI and they each order their AI to destroy their enemies and protect their own citizens, then if there is an offense bias, and it easier to attack than defend, each AI may succeed in destroying the enemy, but fail to defend its own citizens, meaning everyone dies.</p><p>Due to entropy (the universe\u2019s in-built destruction bias,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqjur9yvanjm\"><sup><a href=\"#fnqjur9yvanjm\">[1]</a></sup></span>) the fragility of humans, and the incredible flexibility of advanced AI, it seems quite plausible, I would even guess more likely than not, that advanced AI will constitute or enable an offense bias.</p><p>This problem is compounded when we consider the many powerful advanced technologies AI may accelerate in the near future, such as bio-technology, 3D printing, nanotechnology, advanced robotics, brain-machine interfaces, advanced internet of things applications, advanced wearable/cyborg technologies, advanced computer viruses, black swan (unknown unknown) technologies, etc.</p><p>Due to advanced AI processes like <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#misaligned-ai-mysterious-potentially-dangerous-objectives\">PASTA (Process for Automating Scientific and Technological Advancement,)</a> powerful advanced technologies could arrive and have transformative effects quite soon, and any one of them could have an offense bias, as could any combination of them, including combinations with already existing technologies such nuclear weapons and drones. This may result in a combinatorial explosion of possible offensive synergies occurring as the number of technologies increase.</p><p>Perhaps something similar could be said of defensive technologies, though I am uncertain how the balance would play put. It seems probable to me that the more advanced technologies we expect there to be, and the more powerful we expect them to be, the more concerned we should be about this possibility.</p><p>It seems quite possible many of the protective factors humans have historically possessed (social interdependence, fragility/mortality, not overwhelmingly powerful, etc.) will break down, and so it should not be too surprising if one or more unprecedented offense biases occur.</p><p>I will next address a concept I will call \u201cHuman Alignment\u201d which may be a way of framing solutions to a vulnerable world scenario.</p><h2>Human Alignment</h2><p>By \u201chuman alignment,\u201d I mean a state of humanity in which most or all of humanity systematically cooperates to achieve positive-sum outcomes for everyone (or at a minimum are prevented from pursuing negative sum outcomes), in a way perpetually sustainable into the future. While exceedingly difficult, saving a vulnerable world from existential catastrophe may necessitate this.</p><p>Bostrom points out that if humanity retains a \u201cwide and recognizably human distribution of motives\u201d resulting in a multipolar world order and an \u201capocalyptic residual,\u201d then even a single apocalyptic actor with access to certain advanced technology may spell the end of civilization. As mentioned, however, actors need not be apocalyptic; it may be enough that they are willing to risk destroying each other to defend themselves, or in pursuit of their own interests.</p><p>In <a href=\"https://nickbostrom.com/papers/vulnerable.pdf\">\u201cThe Vulnerable World Hypothesis,\u201d</a> (VWH) a possible solution Bostrom proposes is universal surveillance of everyone at all times to prevent apocalyptic behavior. Many find this solution unpalatable, though perhaps better than extinction. This would result in humanity being (at least minimally) aligned by force.</p><p>Another possible solution is to sustainably eliminate all malicious and apocalyptic intentions, or in other words to universally create enough moral progress that no one desires to kill each other, or is willing to risk destroying humanity. Bostrom seems to dismiss this solution as intractable. I think, however, that by using systemic interventions which incorporate mildly to moderately advanced AI to re-shape the moral fitness landscape toward desirable traits, among other interventions, this may be more tractable than it seems at first glance. I wrote the rough draft of a book on such solutions (for x-risk / vulnerable world in general, not AI x-risk specifically) before formally discovering EA, longtermism, and the VWH. I am now trying to understand the AI x-risk landscape better to see if a vulnerable world scenario is likely given the development of advanced AI.</p><h2>Conclusion</h2><p>My main question is whether a vulnerable world induced AI x-risk scenario seems plausible or likely.</p><p>I think my main crux is whether AI is likely to be multi-polar, hence multiple agents have access to advanced AI.</p><p>Another factor is whether advanced AI is likely to have uneven abilities such that the ability to commit genocide or to create new dangerous technologies is developed before the ability to defend humans, predict what technologies will be dangerous, or align humanity.</p><p>I am also very curious if this is something others have talked about, and if so, I would appreciate references to these discussions.</p><p>Finally, I would greatly appreciate any thoughts on my reasoning in general, what I may be missing, and what would be promising directions for further research for me.</p><p>Thank you in advance for your feedback!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqjur9yvanjm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqjur9yvanjm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By which I mean it is easier to break something than to create or fix it, not exactly the same as offense bias, but closely related</p></div></li></ol>", "user": {"username": "Jordan Arel"}}, {"_id": "Hmbmm7a6BgjKPSqtx", "title": "Pandemic Preparedness: Stakeholders who influence politicians in the UK ", "postedAt": "2022-12-08T22:15:51.193Z", "htmlBody": "<p><i>Acknowledgements: Thanks to Sam Hilton, Barry Grimes, Haydn Belfield, Jasmin Kaur, Lin Bowker-Lonnecker and others who wished to remain anonymous for help in providing input and reviewing the post prior to publishing. All errors are the authors\u2019 own.&nbsp;</i></p><h2><strong>Summary</strong></h2><p>The Pandemic Prevention Network (PPN) focuses on using advocacy to influence UK politicians to sustain attention on pandemic prevention efforts, so as to better prepare for future, possibly more harmful, global disease outbreaks. This research was commissioned to help PPN understand what stakeholders could be leveraged to influence the UK Members of Parliament (MPs), though influencing MPs is only one of many possible routes to influencing policy in the UK, which is not covered here.&nbsp;</p><p>10 stakeholders were identified, ranging from the public, to business, to healthcare workers. Their influence over MPs and their expected support for pandemic preparedness were mapped based on a review of their existing initiatives and successes. While on average, some stakeholders (government advisory groups) have more influence than others (celebrities), applying this mapping to your strategy is not as clear cut as just picking to work with the highest influencer. Rather, picking a stakeholder to work with is a matter of strategy, and ensuring that their values and goals align with your organization to maximize your collective influence. It is our hope that this post will be useful to other advocacy organizations working on pandemic prevention in the United Kingdom and to better inform these efforts through our findings.&nbsp;</p><p>For a summary of the rankings and examples used, you may access it&nbsp;<a href=\"https://drive.google.com/file/d/1nc5wHr1XVn8gYNfBxX0JvPSxj2ygQV5z/view?usp=sharing\"><u>here</u></a>.&nbsp;</p><h2><strong>About Pandemic Prevention Network</strong></h2><p>The Pandemic Prevention Network (PPN) was established to continue putting a spotlight on the devastation that pandemics bring. Unfortunately, politicians shift their policy focus very quickly once pandemics are less deadly, yet pandemic risk and need for preparedness has not reduced. In fact, the need for sustained attention on pandemic prevention has increased, as the world has shown that we are not ready for another pandemic. PPN aims to change that by sustaining political attention on the importance of pandemic preparedness and prevention through many initiatives, including public led advocacy campaigns to the government.&nbsp;</p><h2><strong>Target Audience</strong></h2><p>This output was initially created to support the research and planning phase of an upcoming PPN pledge campaign. However, other groups or organizations who are looking to learn more about the political landscape in the UK, especialy in the context of pandemic prevention, may also find this research useful. This research is aimed at anyone who wishes to better understand the existence of different stakeholders with political influence over MPs in the UK and the extent to which they would support pandemic-related reform so as to better prioritize their partnerships and resources accordingly.&nbsp;</p><h2><strong>Purpose of Research</strong></h2><p>The PPN pledge campaign was targeted at Members of Parliament (MPs) in the United Kingdom to show their commitment to keeping their constituents safe from future pandemics by signing a low-effort and low-commitment pledge in public. The pledge was strategically chosen to be generic (rather than asking for specific policy outcomes) for it to be non-partisan and have a low barrier to an MP choosing to sign it, in exchange for the press and media opportunities they will gain in front of their voters.&nbsp;</p><p>The larger aim for maximising the number of MP signatures we could gain via this campaign was to:</p><ol><li>Build the first All Party Parliamentary Group (APPG) with focus on preventing both naturally-occurring and engineered pandemics in the UK.</li><li>Start building political relationships and a broader MP network through which we can table for high impact policy asks for addressing biological threats.</li></ol><p>To have a general idea of the groups that could impact MPs and promote the success of this campaign, stakeholder mapping was done to rank the political influence and potential support these groups would express for pandemic prevention as a way to determine how we could have leveraged them to achieve the goals of the campaign.&nbsp;</p><p>This was important to do in order to have a better understanding of the political landscape in the UK and who were the key players influencing decision-makers to achieve legislative reform.&nbsp;</p><p>It should be noted that the execution of this campaign was halted due to lack of funding received. However, the stakeholder map produced still has the potential to inform other similar campaigns and efforts, especially for other policy and advocacy organisations in this space.</p><h2><strong>Methodology</strong></h2><p>Our approach first identified relevant research on tactics and groups who have influence over the UK policymaking process. Specifically, we reviewed different pressure groups who could influence the UK MPs. Based on the research, the initial stakeholders and tactics identified include celebrities, government experts, non-government experts, public letter writing, pressure groups via protests, and national press. We initially did not have criteria for groups, but instead wanted to find as comprehensive of a list as possible.&nbsp;</p><h3>Developing and Measuring Influence and Support</h3><p>Once we had an idea of the general landscape, we assessed their influence on two dimensions. For each dimension, we used \u201cLow\u201d, \u201cMedium\u201d, and \u201cHigh\u201d to help categorize the stakeholders. The definitions for each are:</p><p><strong>Political Influence</strong></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Low</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Medium</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>High</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Never or seldom works directly with MPs</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Sometimes works directly with MPs</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Always work directly with MPs</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Policy aspects are never or seldom changed, as a result of stakeholder actions</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Policy aspects are somewhat changed, as a result of stakeholder actions</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Policy aspects directly changed, as a result of stakeholder actions</td></tr></tbody></table></figure><p><strong>Support for Pandemic Preparedness</strong></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Low</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Medium</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>High</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Limited knowledge on the topic of pandemic preparedness</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">General knowledge on the topic of pandemic preparedness</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Extensive knowledge on the topic of pandemic preparedness</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Little to no impact on the stakeholder if the campaign is successful</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Some impact on the stakeholder if the campaign is successful</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Large impact on the stakeholder if the campaign is successful</td></tr></tbody></table></figure><p>Impact on&nbsp;<strong>policy making&nbsp;</strong>was determined by looking at two or more examples within each stakeholder and looking at whether that case study worked directly with MPs and whether the policy reform took place according to the concerns of the stakeholder. Seeing that MPs were the target political actors for the PPN campaign, this criteria centered around whether stakeholders would have examples of working with them. In the event that other politicians played a role in political reform, then the second criteria point remains relevant in its ranking. For example, one government expert is the Scientific Advisory Group for Emergencies (SAGE) , which had huge influence on the policy making process for COVID response and worked directly with MPs to achieve this, which helped push the overall stakeholder influence over policy to \u201chigh\u201d. We discuss the determination of the political influence in more detail under each stakeholder.</p><p>We also assessed the stakeholder potential for&nbsp;<strong>support of pandemic preparedness</strong> by looking at the stances of the groups and individuals for COVID and/or health policies in the UK. Groups who had positive support for COVID policies and initiatives were deemed as likely to support pandemic preparedness, while groups who were actively opposed to measures would be categorized as having low support. We discuss the determination of the support for pandemic preparedness in more detail under each stakeholder.</p><h3>Collecting Feedback</h3><p>Once basic research was completed and mapped, we interviewed experts with experience and knowledge of UK politics for their feedback, suggestions for additional research, and recommendations for who else to talk to. We managed to interview:&nbsp;</p><ul><li>Sam Hilton who worked in the civil service as a Policy Analyst for 5 years, and worked in Parliament coordinating the APPG for Future Generations for 2 years</li><li>Barry Grimes who worked as a Senior Caseworker for Roberta Blackman-Woods MP for over 2 years,</li><li>An expert who worked as a Senior Parliamentary Assistant for the House of Commons for over 2.5 years, and&nbsp;</li><li>Haydn Belfield who worked for an MP in the Shadow Cabinet as a Senior Parliamentary Researcher for 2.5 years.&nbsp;</li></ul><p>This allowed us to expand our list of stakeholders significantly: we added pharmaceutical, healthcare workers, philanthropists, and split out non-government experts into academic and think tanks. We also had a suggestion to include other governments as a stakeholder, but decided not to since PPN would not be able to leverage that influence for their own campaigns. Other feedback included clarifying assumptions and information, and adding a separate map showing outlying examples for each stakeholder.</p><p>A final map was created based on the feedback and sent for final review and feedback from the experts who we interviewed, especially to ensure we did not misconstrue any information they provided</p><p>This post is meant to showcase a \u201cnear-final version\u201d of the mapping exercise. We hope to collect additional feedback on the post and the research to feed into a final published report by&nbsp;<a href=\"https://www.pandemicpreventionnetwork.org/\"><u>Pandemic Prevention Network</u></a> that we can share with other UK policy, and advocacy non-profit organisations who can benefit from the presented findings in their efforts for building a safer world against biological threats.&nbsp;</p><h2><strong>Results</strong></h2><p>We have identified a total of 10 different stakeholders, which is probably fewer than there are in reality, but a good place to start. These stakeholders include:&nbsp;</p><ul><li>healthcare workers - employees working with the UK National Health Services</li><li>academic experts -&nbsp; individuals and academic labs that focus mostly on research, with an ability to provide advice to policy-makers</li><li>government experts - individuals or organizations that are commissioned by the UK government to provide expertise on pre-determined topics&nbsp;</li><li>think tanks - institutions that focus on research to influence policy making</li><li>philanthropists - UK citizens/UK-born individuals who support a cause or causes in the UK and/or globally through financial contributions</li><li>the public - a British, qualifying Commonwealth citizen or a citizen of the Republic of Ireland&nbsp;</li><li>pharmaceutical companies - large pharmaceutical companies who have a major manufacturing, research and sales presence in the UK</li><li>celebrities - UK citizens/ UK-born individuals who are famous within the UK and beyond&nbsp;</li><li>pressure groups - groups who try to exert influence through campaigns, protests, civil disobedience, and other means meant to get attention on the issue in the public realm</li><li>national press - media in the UK that includes printed news (online and physical), television and radio&nbsp;</li></ul><h3>Stakeholder Maps</h3><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670542959/mirroredImages/Hmbmm7a6BgjKPSqtx/ey31ptftakfw8heeqeic.jpg\"></p><p><i>Figure 1. General ranking of each stakeholder</i></p><p><br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670542959/mirroredImages/Hmbmm7a6BgjKPSqtx/bvglckmc0dbj4r9xp5ac.png\"></p><p><i>Figure 2. Mapped outlier examples for each stakeholder</i></p><p><br>&nbsp;</p><p>Figure 1 presents the average political influence and pandemic preparedness support for each stakeholder. Since each stakeholder is made up of numerous examples that don\u2019t completely align on political support and pandemic support, we also made the map shown in Figure 2. The map is meant to showcase some of the diversity within each stakeholder group in terms of political influence and support we saw within each.&nbsp;</p><p>We have developed a reference resource located&nbsp;<a href=\"https://drive.google.com/file/d/1nc5wHr1XVn8gYNfBxX0JvPSxj2ygQV5z/view?usp=sharing\"><u>here</u></a> that goes into detail of how we determined the political influence and pandemic support. We provide case study examples that informed the ranking for each stakeholder.</p><h3>Notable Outliers</h3><p>For government experts, the Scientific Advisory Group for Emergencies lies in providing scientific expertise for unprecedented crises for the UK cabinet. Seeing that pandemic preparedness and prevention does not fall within the scope of an emergency then it may not be under this group's jurisdiction to support such a campaign. With this reasoning, we have shifted the support ranking for SAGE to \"medium\" because while it may not have a large impact on this stakeholder, a pandemic-related campaign will still be calling on political reform that may ease their role in future pandemics.</p><p>Pressure groups differ in their advocacies and campaign efforts. There are groups calling for healthcare reform and this aligns with the efforts of pandemic preparedness. There are long-term calls for increased support and funding for healthcare services and staff, which is similar to what could be achieved to prepare for future pandemics. Such groups would have a high level of support for pandemic prevention and preparedness efforts seeing that their impact and outcome align with one another.</p><p>Lastly, the academic experts that were involved in the COVID-19 pandemic response had little influence on the government making policy during the pandemic, and thus their influence is low. Academic experts tend to have more influence in other areas of the government outside of emergency responses, where SAGE has the most influence.&nbsp;</p><h3>Assumptions</h3><p>We did not consider opposition (e.g. negative support) to pandemic prevention because even though there are groups who are currently against pandemic response (vaccination, lockdown, masks, etc.), they would probably not actively oppose pandemic prevention efforts due to the lack of immediate consequences. A response is different from preparation, and in fact preparation may prevent some of these opposed measures, like mask wearing and lock downs. Instead we assume that groups who oppose pandemic response efforts may not care about pandemic prevention, and would have low to no support for such efforts.</p><p>It should be considered that some issues that were supported by influencers and celebrities, such as the fight against malaria, may have already been on the government\u2019s radar and would have already been discussed at their meetings even without the campaign.</p><p>The extent to which campaigns by organizations such as the British Heart Foundation impacted the vote of MPs is unknown, as the correlation between the presence of the campaign and the swing of the vote has not been properly established.&nbsp;</p><p>The assumption when searching for non-government experts and pressure groups related to the pandemic is that non-prominent groups would exert little influence over the MPs due to their obscurity. Therefore, the search for non-government experts and pressure groups was non-exhaustive. It is possible that the conclusions may change based on more significant research, or in the future when new groups are formed or obscure groups get more attention.</p><p>&nbsp;</p><h2><strong>Discussion</strong></h2><h3>Overview</h3><p>Below are some possible strategies that one can adopt to influence the policy-making process in the UK. While it would be great to just say \u201cwork with stakeholder x to achieve result\u201d, it\u2019s not as easy, as influence and support for pandemic preparedness varies even within stakeholders. Therefore, we think it\u2019s important to understand the goal of your campaign, the resources you have, and the timing of your campaign.&nbsp;</p><p>Working closely with groups that<strong> are aligned with your group\u2019s mission&nbsp;</strong>is one theme we have been hearing across our interviews. Common interests help drive progress, whereas working with a group with conflicting interests may make it difficult to push forward a specific agenda.&nbsp;</p><p><strong>Accessibility to groups</strong> is also important, we would think. If you cannot identify how to work with a high-influence group, then your campaign won\u2019t make as much progress compared to working with a group that has less influence, but influence nonetheless. For example, if it\u2019s a lot easier to work with an academic than the government experts, your efforts may result in greater influence with the academics.&nbsp;</p><p><strong>Combination of strategies&nbsp;</strong>could be important as well. For example, protests are a critical tactic from pressure groups, but this is often done to raise salience in the press and on social media, and the two strategies of protests and national press coverage complement each other.&nbsp;</p><p><strong>Timing</strong> has also been discussed as an important consideration for strategies. A government in upheaval has other priorities to focus on at that time. Furthermore, other world events may impact some strategies. Press coverage about pandemic prevention may result in less impact during a significant event, like the Russian invasion of Ukraine, than when the coverage occurs in conjunction with other relevant events, like the Ebola outbreak in Uganda.&nbsp;</p><p>&nbsp;</p><h3>Limits and Challenges</h3><p><strong>Criteria Limitations</strong></p><p>It is worth noting that the criteria set forth in the ranking of stakeholders may not always hold true for every circumstance. For political influence, even if a stakeholder does not work directly with an MP, they still have a chance of influencing them through their campaigning efforts or public posts that would garner attention from political actors. The same is true for the criteria mentioned for support for pandemic preparedness. Knowledge and impact on stakeholder may not be the only factors that would have an effect on this as support for such efforts may also be garnered through a collective public voice. To gain greater support for pandemic preparedness may be impacted by confounding factors that go beyond what was considered in this paper.</p><p><strong>Who, Not How</strong></p><p>While this research was able to shed light on who could be approached for support for a pandemic-related campaign, the best ways in which a partnership can develop to secure this influence was not examined.&nbsp;</p><p><strong>Pandemic-Related Successes of Stakeholders</strong></p><p>We found it difficult to search for case studies for each stakeholder that exemplified a pandemic-related effort that led to a policy reform. This could be an indication that a limited amount of work has been done by these stakeholders, specifically pressure groups, celebrities &amp; influencers, and philanthropists to call for policy changes to improve pandemic prevention and preparedness in the long run. Furthermore, we looked at stakeholders being sympathetic response, which entails very different strategies than prevention. The policies and solutions for these two outcomes can look quite different, but this report is not granular enough to make this distinction between the actors.&nbsp;</p><p><strong>Resource Limitations</strong></p><p>We are both volunteers with the Pandemic Prevention Network, with limited time available to spend on this project. Furthermore, we do not have the relevant political background, so our product will not be as high level of research as, say, a political researcher working on this full time. We worked on this in our free time out of our desire to support PPN and its mission. To aim to compensate for this, we interviewed experts in the UK political fields to supplement our lack of expertise. Interviewing people with experience working in different UK political parties could improve the information gathered as it would be more representative of the UK political spectrum.</p><p>Since PPN did not achieve funding to support the continuation of its operations, additional tools to conduct polling or surveys as well as to conduct additional interviews to gather more data for this piece was limited. We mainly centered the content of this paper on what was publicly available and accessible without a paywall.</p><p><strong>Stakeholder exclusion</strong></p><p>Other governments, such as the United States, were not included as a stakeholder because the scale to involve such a group for a policy campaign would have significant challenges. While there is a potential for this stakeholder to have political influence, the extent to which they would support such a campaign that calls on political reform for pandemic prevention and preparedness in another nation is limited. Furthermore, unions were not included explicitly, but were reflected in the healthcare workers stakeholder. This was one of the biggest union members.&nbsp;</p><p><strong>Averages are not reflective of the whole picture</strong></p><p>Our maps showed single points of support and influence for each stakeholder in an effort to simplify this complex topic. We acknowledge that this may not completely reflect the reality of their stances. There is a spread in ranking for both political influence and support for pandemic preparedness which exists between the individuals organizations under one stakeholder category. This is to say that the map produced does not provide a definitive ranking for all organizations that would be categorized under each stakeholder group.&nbsp;</p><h3>Suggested Campaign Strategies</h3><p><strong>Engaging constituents</strong> is a good way to get the attention of Members of Parliament. Their role centers around representing their constituencies and raising these concerns and interests in the House of Commons. MPs often hold \u201csurgeries\u201d in their office where locals can discuss matters that concern them. The&nbsp;<a href=\"https://www.parliament.uk/about/mps-and-lords/members/what/\"><u>UK Parliament</u></a> encourages citizens to reach out to their MPs with their issues through letter-writing or making a personal appointment. The public voice matters to MPs as they view&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9256.2011.01422.x\"><u>politics as a career</u></a> and so would represent their constituents\u2019 interest to garner personal votes in preparation for the next elections.</p><p><strong>Knowing who to reach out to</strong> can center the campaign team\u2019s focus on stakeholders who can potentially have a larger impact and sway than others. Making use of&nbsp;<i>Figure 1</i> when planning for a campaign on pandemic preparedness and prevention can inform organizers that government experts and think tanks may be the first group to reach out. There is a potential that partnerships and support from these stakeholders will follow the Pareto Principle and yield 80% of the campaign\u2019s success from 20% of the suggested partners. It should also be noted that reaching out to the groups categorized as \u201chigh\u201d for support such as healthcare workers and academic experts could have a positive influence on the outcome of a campaign. This secondary group has evidence of working with MPs, garnering their support for their causes or research, and informing policy reform.</p><p><strong>Lower the barrier for MPs</strong> to support your campaign by renting out a room or space in the House of Commons and inviting MPs to attend your event. This was a suggestion given by one of our interviewees as a way to improve the possibility that an MP will engage with your campaign. Other than encouraging constituents to write letters to their MP to sign a pledge, they can also call for their MPs to attend your event which may be more convenient for them to stop by and take a picture which can then be posted publicly as a sign of their support. By lowering the barrier for MPs to support your cause and bringing your campaign to their doorstep, there\u2019s a better chance for the campaign to garner support from target MPs, stakeholders, and constituents.</p><h2><strong>Conclusion and Further Research</strong></h2><p>This output was created to supplement the campaign of the Pandemic Prevention Network. We mapped different stakeholders who have political influence over Members of Parliament in the UK and determined the extent to which they could be supportive towards pandemic prevention and preparedness efforts. With this research, we also hope that target groups could be better prioritized so that PPN could work with those who have higher political influence and support for our cause. On top of that, stakeholders who may have not been considered as viable partners for the campaign (such as think tanks or pharmaceutical companies) could be revisited as a potential partner who can help garner more support for pandemic-related policy work.</p><p>While it is difficult to have a prescriptive plan of how to influence policymaking in the UK, we think that different approaches can be taken depending on the desired outcome (pledge vs policy input), concurrent political events, and time frame.&nbsp;</p><p>This research is just a start. If any organization or individual wishes to build on the initial research we\u2019ve done, we would suggest the following topics to research and possible next steps:</p><ul><li>Conduct interviews with organizations under each stakeholder category to get their perspective on where they would fall in the ranking of political influence over MPs and support for pandemic preparedness</li><li>Research on the best ways these stakeholder groups could be approached to garner their support for a pandemic-related campaign</li><li>Research on additional case studies for each stakeholder category to justify or repudiate its current ranking and diversify the outliers present</li><li>Interview Members of Parliament or their team to share their thoughts on groups and sources that would be interesting to work with for pandemic-related reform and their stance on working towards pandemic preparedness in the UK</li></ul><p>It is our hope that this research could be a resource for groups or organizations who are relatively new to the political landscape in the UK and would like to understand the stakeholders they could work with for pandemic-related reform. While more work needs to be done in understanding the best ways to influence policymaking, having a good foundation of the potential players involved in the fight against future pandemics is essential in knowing who your allies will be. &nbsp;</p><h2><strong>Questions for audience</strong></h2><p>We\u2019d love feedback on our work! In order to make this stronger and more useful, we would like anyone who has thoughts to answer any of the following questions in the comments below:</p><ol><li>Do you see flaws in the data presented above? If so, do you have ideas of how to address them?</li><li>What is the weakest argument presented here? How could we make it stronger?</li><li>How would you use this information?</li></ol><p>If you have feedback or questions that you don\u2019t want to share publicly, you\u2019re welcome to fill out this&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeV12UlOJyle3MGHbj_zKym7f9l7uWInlYfVT3YLjmJaEf3Zw/viewform\"><u>form</u></a>.&nbsp;</p><h2><strong>Note</strong>&nbsp;</h2><p>We invite you to share this research with any UK policy efforts that this could be facilitative for.<br>&nbsp;</p>", "user": {"username": "Alexandra Malikova"}}, {"_id": "y2FTFEGqBxucJJnce", "title": "How Many Lives Does X-Risk Work Save From Nonexistence On Average?", "postedAt": "2022-12-08T21:44:51.492Z", "htmlBody": "<p>Cross-posted to <a href=\"https://www.lesswrong.com/posts/E7SmJtYpbPD7KTHu9/how-many-lives-does-x-risk-work-save-on-average\">LessWrong</a> and <a href=\"https://jordanarel.com/2022/12/14/how-many-lives-does-x-risk-work-save-from-nonexistence-on-average/\">jordanarel.com</a></p><p>A huge thank you to Aman Patel, Justis Mills, Joel McGuire, Dony Christie, Sofya Lebedeva, and Michael Chen for invaluable feedback which greatly improved this post. All errors are 100% on me, all credit goes to these wonderful folks.</p><h1><strong>tl;dr</strong></h1><p>In&nbsp;this post I make a range of speculative,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmv0kn0s2don\"><sup><a href=\"#fnmv0kn0s2don\">[1]</a></sup></span>&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/fermi-estimate\">back-of-the-envelope Fermi estimates</a> of conversion rates from <a href=\"https://forum.effectivealtruism.org/topics/existential-risk\">existential risk (x-risk)</a> work and donations to lives saved from nonexistence.</p><p>I note that individual x-risk work likely has extreme variance <a href=\"https://conceptually.org/concepts/ex-ante-vs-ex-post-analysis\">ex post</a>, and that most or even all x-risk work may have little or no impact. This may play an important role in whether these estimates are acceptable under certain frameworks.</p><p>With pessimistic assumptions, including that humanity never leaves earth and the impossibility of digital minds, I (very roughly) estimate x-risk work, <a href=\"https://conceptually.org/concepts/ex-ante-vs-ex-post-analysis\">in expectation (ex ante)</a>, on average, saves about one life per hour of work or per $100 (US Dollars) donated.</p><p>With moderate assumptions, including a significant possibility of interstellar travel and digital minds, I estimate x-risk work, in expectation, on average, <strong>saves about 10^36, or a trillion trillion trillion lives per minute of work or per dollar donated.</strong></p><p>With optimistic assumptions, including a small possibility of achieving computational efficiency close to the limits of physics, I estimate x-risk work, in expectation, on average, saves about 10^72, or a trillion trillion trillion trillion trillion trillion lives per minute of work or per dollar donated.</p><p>In the introduction to each section I discuss how each of these wide-ranging estimates may be useful.</p><p>I estimate that individual x-risk work may, in expectation, vary by up to a factor of 100,000 times more or less impact than average in expectation, and may often have negative impact ex post.</p><p>In conclusion, those working on x-risk should take this not as a cue to over-work, but instead as a reminder to take very good care of themselves, their personal needs, and their well-being, so as not to burn out or become ineffective.</p><h1><strong>Introduction</strong></h1><p>In <a href=\"https://podcasts.apple.com/us/podcast/effective-altruism-forum-podcast/id1577033009\">the EA Forum Podcast\u2019s </a>recent <a href=\"https://podcasts.apple.com/us/podcast/effective-altruism-forum-podcast/id1577033009?i=1000583321130\">summary</a> of <a href=\"https://forum.effectivealtruism.org/posts/8whqn2GrJfvTjhov6/measuring-good-better-1\">\u201cMeasuring Good Better\u201d</a>, I learned that <a href=\"https://founderspledge.com/\">Founder\u2019s Pledge</a> values cash at $199 per <a href=\"https://forum.effectivealtruism.org/posts/dk48Sn6hpbMWeJo4G/to-wellby-or-not-to-wellby-measuring-non-health-non\">WELLBY</a>.</p><p>This got me wondering:</p><p>\u201cHow many lives can those working on preventing x-risk expect to save per time worked<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefofr924doqjq\"><sup><a href=\"#fnofr924doqjq\">[2]</a></sup></span>/dollar donated<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy1bmxy31yw\"><sup><a href=\"#fny1bmxy31yw\">[3]</a></sup></span>?\u201d</p><p>Here is a brief estimate.</p><h2>Epistemic Status</h2><p>This is my first x-risk/longtermism research project. I appreciate help correcting my assumptions!</p><h1><strong>Why &amp; Why Not Make An Estimate?</strong></h1><p>The upside of this estimate is that it is motivating to know the value of our work at a personal level.</p><p>It also provides a useful reference point when thinking and writing about the impact of x-risk work.</p><p>A downside is the potential to cause overwork, leading to burnout. I address this at the end of the post.</p><p>These estimates may also make some people feel compelled to work on x-risk when this is not a good fit for their skill-set or inclinations.</p><h1><strong>Assumptions</strong></h1><p>The three sections of this post give a range of estimates based on scenarios with pessimistic, moderate, and optimistic assumptions about the impact of x-risk work.</p><p>Expected value estimates sometimes locate the majority of value in speculative tail scenarios. I believe a range of estimates, given various scenarios, offers a clearer understanding of the range of possible effects of x-risk work.</p><p>To simplify, I use a few background assumptions throughout.</p><h2><strong>Background Assumptions</strong></h2><p><strong>I use an </strong><a href=\"https://80000hours.org/articles/expected-value/\"><strong>expected value</strong></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe79aoh2pnzg\"><sup><a href=\"#fne79aoh2pnzg\">[4]</a></sup></span><strong>&nbsp;framework to estimate lives saved. </strong>I do not expect these estimates to hold under other frameworks. In particular, frameworks that value certainty of impact and reject small, highly speculative chances of enormous impact<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref44jycvb96b4\"><sup><a href=\"#fn44jycvb96b4\">[5]</a></sup></span>&nbsp;may find these estimates unacceptable.</p><p>In each scenario I estimate the lives saved by<strong> the</strong> <strong>average of all x-risk work</strong>. X-risk work with more or less leverage may be more or less effective. I discuss this in the \u201cInside View Adjustments for Individuals\u201d section.</p><p>Whenever I say \"lives saved\" this is shorthand for \u201cfuture lives saved from nonexistence.\u201d This is not the same as saving existing lives, which may cause profound emotional pain for people left behind, and some may consider more tragic than future people never being born.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzafk2cxtssi\"><sup><a href=\"#fnzafk2cxtssi\">[6]</a></sup></span></p><p>I assume a zero-discount rate for the value of future lives, meaning I assume the value of a life is not dependent on when that life occurs.</p><p>I exclude consideration of how differences in future well-being affect expected value. <a href=\"https://forum.effectivealtruism.org/posts/ugdFidDgGnD46bkwd/jordan-s-shortforms?commentId=PpBxgExgYMzQSwFz4\">This short-form</a> gives my speculations on future well-being, which were originally included in this post.</p><p>I exclude the possibilities of faster than light travel and interventions which may create infinite value.</p><h1><strong>Factors Analyzed</strong></h1><p>I used the following factors to make estimates.</p><p>Factors:</p><ul><li>Is mind-uploading possible?</li><li>How far are we from the limits of computation?</li><li>Is interstellar travel possible?</li><li>Do anthropic arguments imply we have no long-term impact?</li><li>How long will it take to achieve existential security?</li><li>On average, how many people will be working on x-risk?</li><li>How much will the sum of all x-risk work reduce existential risk?</li><li>How much does individual x-risk work vary in expected impact? By:<ul><li>X-risk sub-cause area</li><li>Intervention quality</li><li>Individual skill</li><li>Timing of x-risk work</li></ul></li></ul><h1><strong>Pessimistic Estimate</strong></h1><p>This estimate is intended to be near the lower limit a reasonable person might estimate for the expected value of x-risk work, given pessimistic assumptions, including that humanity never leaves earth and the impossibility of digital minds.</p><p>Because this estimate is so low relative to the other estimates, it is noticeably sensitive to small, arbitrary seeming changes in assumptions, and so should be taken as at best a very crude approximation which could easily be several orders of magnitude higher or lower.</p><p>I believe this estimate is most useful for showing that even with minimal sci-fi speculation, it is plausible that x-risk work could be a cost-effective way of doing good for risk-neutral<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffkhfjmiun5u\"><sup><a href=\"#fnfkhfjmiun5u\">[7]</a></sup></span>&nbsp;EAs.</p><p>For an even greater range of estimates, <a href=\"https://forum.effectivealtruism.org/posts/ugdFidDgGnD46bkwd/jordan-s-shortforms?commentId=7z7fyTXMHoQANqHL8\">here is a brief short-form appendix</a> with two highly pessimistic estimates, and two estimates between the optimistic and moderate estimate.</p><h2><strong>Pessimistic Assumptions</strong></h2><p>Pessimistically, we might assume that <a href=\"https://en.m.wikipedia.org/wiki/Mind_uploading\"><strong>mind uploading or brain emulations</strong></a><strong> are impossible.</strong> This would preclude the possibility of a very large number of what Holden Karnofsky calls <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">\u201cdigital people.\u201d</a></p><p>This may occur if <a href=\"https://80000hours.org/problem-profiles/artificial-sentience/\">artificial sentience</a> turns out to be impossible, due to consciousness requiring biological hardware. This is not implausible, as <a href=\"https://en.m.wikipedia.org/wiki/Hard_problem_of_consciousness\">we do not yet fully understand what consciousness is.</a></p><p>Next, we could pessimistically assume that <strong>humanity never leaves earth. </strong>Perhaps interstellar travel is impossible, and life elsewhere in the solar system proves too difficult or undesirable.</p><p>We might also estimate that, conservatively, <strong>the earth has a</strong> <strong>carrying capacity of 1 billion people, and that with humanity's best efforts the earth remains habitable for 1 billion years.</strong></p><p>This would mean that for the pessimistic scenario, <strong>if the average lifespan is approximately 100 years, sustainably preventing x-risk would save 10^9 * 10^9 / 10^2, or 10^16 lives</strong></p><p>Finally, some <a href=\"https://en.m.wikipedia.org/wiki/Anthropic_principle\">anthropic arguments</a> state it is incredibly surprising that we just-so-happen to find ourselves exactly where (or when) we are in the universe, at what potentially seems to be the <a href=\"https://www.cold-takes.com/most-important-century/\">\u201cthe most important century\u201d</a> or &nbsp;<a href=\"https://forum.effectivealtruism.org/topics/hinge-of-history\">\u201chinge of history.\u201d</a> Because of this, some argue, it seems highly likely <a href=\"https://en.m.wikipedia.org/wiki/Simulation_hypothesis\">we are in a simulation,</a> the <a href=\"https://en.m.wikipedia.org/wiki/Doomsday_argument\">world is about to end,</a> or we are in some other weird situation that means we may actually have no impact on the long-term future. Relatedly, <a href=\"https://forum.effectivealtruism.org/posts/wqmY98m3yNs6TiKeL/parfit-singer-aliens\">the possibility of aliens may also eliminate much of the value of x-risk work</a>, as this would mean we are not solely responsible for creating value in the universe.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1ys4m46rzus\"><sup><a href=\"#fn1ys4m46rzus\">[8]</a></sup></span></p><p>I think there may be some resolution to these arguments,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefluxe2fgn13b\"><sup><a href=\"#fnluxe2fgn13b\">[9]</a></sup></span>&nbsp;but if we pessimistically assume these arguments are so devastating that there is a 99% chance we have no impact on the long-term future,<strong> we lose a factor of 100, or 2 </strong><a href=\"https://en.wikipedia.org/wiki/Order_of_magnitude\"><strong>orders of magnitude</strong></a><strong> of expected value due to anthropics.</strong></p><h2><strong>Pessimistic Analysis&nbsp;</strong></h2><p>It seems likely most existential risk will occur relatively soon. In <a href=\"https://en.m.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\">\u201cThe Precipice\u201d</a> Toby Ord estimates 1/3 of all existential risk will occur this century.</p><p>This is because existential risk seems largely to be caused by new, potentially dangerous technologies, such as <a href=\"https://80000hours.org/problem-profiles/nuclear-security/\">nuclear weapons,</a><strong> </strong><a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/\">bio-technology</a>, and <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/?source=email&amp;uni_id=0&amp;utm_source=80%2C000+Hours+mailing+list&amp;utm_campaign=9db776f8f3-October+28+2022+newsletter+\u2014+Weird+ideas&amp;utm_medium=email&amp;utm_term=0_43bc1ae55c-9db776f8f3-352679020&amp;mc_cid=9db776f8f3&amp;mc_eid=e96d227006\">advanced artificial intelligence</a>. The development of such technologies is occurring at a rapid, likely unsustainable rate.</p><p>Holden Karnofsky points out that if our rate of economic growth slows to just 2%, and this rate of growth continues for another 8,200 years, <a href=\"https://forum.effectivealtruism.org/posts/pFHN3nnN9WbfvWKFg/this-can-t-go-on\">every atom in the galaxy would have to be supporting multiple economies the size of our current global economy,</a> which seems unlikely.</p><p>He also points out that with a <a href=\"https://www.cold-takes.com/where-ai-forecasting-stands-today/\">high likelihood of transformative AI occurring sometime soon,</a> something like a <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\">Process for Automating Scientific and Technological Advancement (PASTA)</a> could lead to the rapid development of all technologies that can, in principle, be developed,</p><p>It seems likely most advanced technologies will be developed soon and that, conservatively, <strong>we will likely either succumb to an existential catastrophe or find a way of sustainably managing existential risk within the next 10,000 years.</strong></p><p>Toby Ord suggests in \u201cThe Precipice\u201d that the first goal of <a href=\"https://en.wikipedia.org/wiki/Longtermism\">longtermists </a>should be <a href=\"https://forum.effectivealtruism.org/posts/gugGJGdakND6HtbDx/what-is-existential-security\">\u201cexistential security,\u201d</a> a state of near-zero existential risk, sustainable indefinitely into the future.</p><p>We might pessimistically (pessimistic in terms of x-risk work efficiency<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8x2f4tvkwf8\"><sup><a href=\"#fn8x2f4tvkwf8\">[10]</a></sup></span>) estimate that<strong> if we have 100,000 people working on x-risk for the next 10,000 years, this will result in only a 1% absolute increase in our chances of achieving sustainable existential security. </strong>This may be reasonable if reducing existential risk is exceedingly difficult, perhaps because <a href=\"https://nickbostrom.com/papers/vulnerable.pdf\">certain advanced technologies destroy civilization by default.</a></p><p>If we assume (roughly) an average lifespan of 100 years per person, and 100,000 hours per career, then there are about <strong>1,000 hours of work per year, per person,</strong> averaged over the lifespan of people working on x-risk.</p><p>If we multiply 1,000 hours of work per year, per person * 100,000 people working on x-risk * 10,000 years, we get <strong>1 trillion (10^12) total hours of work to achieve a 1% increased chance of existential security.</strong></p><p>As stated, if digital minds are impossible; we do not leave earth; and the earth is habitable for a billion years, by a billion people, at a hundred years per life; this leads to <strong>the possibility of 10^16 lives.</strong></p><p>Incorporating our pessimistic assumptions about anthropic arguments, <strong>we get another 2 orders magnitude reduction of expected value.</strong></p><p>This means that, <strong>pessimistically, 10^12 hours of x-risk work will increase the chance of saving 10^16 lives by 1%, and accounting for anthropic arguments there's only a 1% chance this is correct</strong>, or</p><p>10^16 &nbsp;* &nbsp;10^-2 &nbsp;* &nbsp;10^-2 &nbsp;/ &nbsp;10^12 &nbsp;= &nbsp;1</p><p>This gives a pessimistic estimate that over the next 10,000 years, in expectation, on average, <strong>x-risk work will pessimistically save one life for every hour of work.</strong></p><p>If we assume high quality x-risk work pays about $100 per hour, then in expectation, on average, <strong>x-risk work pessimistically saves 1 life per $100 donated.</strong></p><p>In conclusion, I estimate that in expectation, on average, <strong>x-risk work pessimistically saves 1 life per hour of work, or per $100 donated.</strong></p><h1><strong>Moderate Estimate</strong></h1><p>This estimate is intended to be a moderate estimate of the expected value of x-risk work. It includes a significant possibility of interstellar travel and digital minds.</p><p>I think this estimate is useful to show x-risk work could plausibly be an extremely cost-effective way for risk-neutral EAs to do good.</p><p>For slightly more conservative scenarios which still show the possibility of extreme cost-effectiveness, see scenarios number 3 and 4 <a href=\"https://forum.effectivealtruism.org/posts/ugdFidDgGnD46bkwd/jordan-s-shortforms?commentId=7z7fyTXMHoQANqHL8\">in the appendix. &nbsp;&nbsp;</a></p><h2><strong>Moderate Assumptions&nbsp;</strong></h2><p><a href=\"https://existential-risk.org/concept\">Bostrom estimates</a> <strong>10^52 potential future lives at 100 years per life,</strong> <strong>assuming interstellar travel and digital minds are possible. </strong>This is a <strong>36 orders of magnitude increase</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsen3w5jrn4j\"><sup><a href=\"#fnsen3w5jrn4j\">[11]</a></sup></span><strong>&nbsp;</strong>from the 10^16 possible lives from the pessimistic estimate.</p><p>It seems somewhat likely consciousness is computational, and therefore digital minds are possible. It also seems likely interstellar travel will be achievable for advanced human civilization. But, conservatively, we could give each of these only a 1/3 chance of being possible, <strong>giving an approximately 1 order of magnitude reduction in expected value.</strong></p><p>Bostrom\u2019s estimate of 10^52 possible lives is based on <a href=\"https://nickbostrom.com/astronomical/waste\">\u201ctechnologies for whose feasibility a strong case has already been made.\u201d</a> For now we could conservatively assume <strong>there are no further breakthroughs in physics or computer science which enable significantly more efficient computation.</strong></p><p>Because anthropic analysis is difficult,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefluxe2fgn13b\"><sup><a href=\"#fnluxe2fgn13b\">[9]</a></sup></span>&nbsp;we might assume there is a 90% chance there is something strange about our situation that eliminates our ability to influence the long-term future.<strong> This is a 1 order of magnitude increase from the pessimistic estimate.</strong></p><p>Altogether, our moderate assumptions <strong>increase the expected value of x-risk work by</strong> <strong>36 orders of magnitude from the pessimistic estimate.</strong></p><h2><strong>Moderate Analysis&nbsp;</strong></h2><p>If we stick with the original estimate of <strong>100,000 people working on x-risk, </strong>considering technological trends,<strong> </strong>it seems <strong>1,000 years to achieve existential security, </strong>rather than 10,000 years, might be a more realistic time frame.</p><p>It also seems plausible that this work could <strong>increase our absolute likelihood of reaching existential security by 10%, </strong>rather than the pessimistic estimate of 1%.</p><p>If, compared with the pessimistic estimate, we need only 1/10 as much time, and have a ten-fold greater likelihood of success, we get a <strong>2 orders of magnitude increase in the estimated efficiency of x-risk work.</strong></p><p>When we combine:</p><ol><li>Our moderate assumptions about digital minds, interstellar travel, and anthropics (36 orders of magnitude)</li><li>The likelihood that existential security is more achievable and will take less time than estimated in the pessimistic analysis (2 orders of magnitude)</li></ol><p>We get a total of 38 orders of magnitude increase in the expected value of x-risk work and donations, for a moderate estimate that, in expectation, on average,<strong> x-risk work will save about 10^38 lives per hour, </strong>approximately<strong> 10^36 lives per minute, </strong>or<strong> 10^34 lives per second.</strong></p><p>If we again assume high quality x-risk work pays about $100 per hour, then in expectation, on average, <strong>x-risk work saves 10^36 lives per dollar donated.</strong></p><p>That means in expectation, on average, <strong>x-risk &nbsp;work saves a 10^32 or a trillion trillion trillion lives per minute of work, or per dollar donated.</strong></p><h1><strong>Optimistic Estimate</strong></h1><p>This estimate is intended to be near the upper limit a reasonable person might estimate the expected value of x-risk work to be, given optimistic assumptions, including a small possibility of achieving computational efficiency close to the limits of physics.</p><p>I believe this estimate is useful for showing we may be vastly underestimating the value of x-risk work, given the possibility of technological breakthroughs. It seems presumptuous to assume we have just now reached the pinnacle of technological achievement.</p><p>Notably, for truly risk-neutral EAs, it is possible that, in expectation, the vast majority of value lies in the remote possibility of extremely good outcomes, and so a scenario like this may carry real weight under such frameworks.</p><h2><strong>Optimistic Assumptions&nbsp;</strong></h2><h3><strong>Limits of Computation</strong></h3><p>Because I have no background in computer science, I may make more mistakes in this section than others. Please correct me where I am mistaken.</p><p>Bostrom\u2019s estimate of 10^52 possible lives is based on brain emulations using <a href=\"https://nickbostrom.com/astronomical/waste\">\u201ctechnologies for whose feasibility a strong case has already been made.\u201d</a> The paper where he estimated this was written in 2003. I am quite uncertain, but I assume this may exclude cutting-edge technologies such as <a href=\"https://en.m.wikipedia.org/wiki/Quantum_computing\">quantum computing</a> and advanced technologies such as <a href=\"https://en.m.wikipedia.org/wiki/Reversible_computing\">reversible computing.</a></p><p><a href=\"https://existential-risk.org/concept\">In a footnote, Bostrom cites the possibility</a> of as many as<strong> 10^121 thermodynamically irreversible computations possible</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh5tqmrlji4s\"><sup><a href=\"#fnh5tqmrlji4s\">[12]</a></sup></span><strong>&nbsp;</strong>\"if all mass-energy in the accessible universe is saved until the cosmic microwave background temperature ceases to decline . . . and is then used for computation.\"</p><p>If we use <a href=\"https://nickbostrom.com/astronomical/waste\">Bostrom's figure</a> of 10^17 operations per second for human brains, and (conservatively) less than 10^10 seconds per human life, then we get <strong>less than 10^27 operations per life.</strong></p><p>Dividing 10^121 by 10^27, we get, in principle <strong>more than 10^94 virtual human life equivalents possible,</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdjc60am835e\"><sup><a href=\"#fndjc60am835e\">[13]</a></sup></span>&nbsp;at the limits of quantum mechanics, entropy, cosmology, and computing technology as we currently understand them.</p><p>This is <strong>42 orders of magnitude greater than the Bostrom's estimate of 10^52 lives. </strong>Because it is highly speculative just how efficient computing might eventually be, we could estimate we only have at best a 1/1,000 chance or reaching 1/1,000th this limit, resulting in <strong>6 orders of magnitude reduction in expected value</strong>.</p><p>If we assume the same \u201cmoderate estimate\u201dadjustments in expected value due to anthropic arguments and the potential impossibility of digital minds &amp; interstellar travel, we get a total of <strong>36 orders of magnitude increase in the expected value of x-risk work </strong>due to possible breakthroughs in computation.</p><h2><strong>Optimistic Analysis&nbsp;</strong></h2><p>For the optimistic analysis, we could use the same assumption that<strong> 100,000 people working on x-risk over the next 1,000 years can achieve a 10% absolute reduction in existential risk.</strong></p><p>When we incorporate:</p><ol><li>Potential breakthroughs in computing leading to far greater numbers of digital people, near the limits of physics (36 orders of magnitude)</li></ol><p>We get another 36 orders of magnitude increase in the expected value of x-risk work and donations, for an optimistic estimate that, in expectation, on average,<strong> x-risk &nbsp;work may optimistically save about 10^74 lives per hour, </strong>approximately<strong> 10^72 lives per minute, </strong>or<strong> 10^70 lives per second.</strong></p><p>If we again assume high quality x-risk work pays about $100 per hour, then in expectation, on average, <strong>x-risk work saves 10^72 lives per dollar donated.</strong></p><p>That means in expectation, on average, <strong>x-risk work optimistically saves 10^72, or a trillion trillion trillion trillion trillion trillion lives per minute of work, or per dollar donated.</strong></p><h1><strong>Inside View Adjustments for Individuals Working On X-Risk</strong></h1><p>Finally, taking the <a href=\"https://forum.effectivealtruism.org/topics/inside-vs-outside-view\">inside view,</a> any individual working on x-risk can compare the work they are doing or funding to other x-risk work to get a more precise estimate. <strong>This step is extremely important.</strong></p><p>Most importantly, this is because <strong>a sizable minority of x-risk work, or possibly even the majority of x-risk work, is net negative.</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft3tsg24d6ob\"><sup><a href=\"#fnt3tsg24d6ob\">[14]</a></sup></span></p><p>Assuming individuals working on x-risk are extremely thoughtful about the possibility of net negative work, and so x-risk work is on average positive, we might expect the effectiveness of x-risk work and donations to fall on a <a href=\"https://en.m.wikipedia.org/wiki/Fat-tailed_distribution\">fat-tailed distribution</a>, with some interventions being many times more effective than average.</p><p>Due to <a href=\"https://forum.effectivealtruism.org/topics/cluelessness\">cluelessness</a> it is extremely difficult to predict which x-risk work will be effective. Nonetheless, individuals working on x-risk can make some inside view adjustments based on expert opinion and general factors predictive of good work on complex, difficult projects in general. This may include:</p><ul><li>Expert estimates of x-risk sub-cause area effectiveness such as nuclear war, bio-risk, AI safety, etc.</li><li>Expert estimates of intervention effectiveness</li><li>Timing of opportunity; work done earlier and at critical moments may be more effective</li><li>Ratio of funding available for x-risk work to talented people working on x-risk</li><li>The level of expertise of the individual working on x-risk</li><li>Quantity and quality of past successes of the individual</li><li>The individual working on x-risk's:&nbsp;<ul><li>Intelligence &amp; rationality</li><li>Grit/ambition/growth mindset</li><li>Social &amp; emotional intelligence</li></ul></li><li>Any other special information on the work - though this last consideration may likely be roughly canceled out by cognitive biases of each individual in favor of their own work, so should not be given too much weight, if any<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuagbr16b4pn\"><sup><a href=\"#fnuagbr16b4pn\">[15]</a></sup></span></li></ul><p><a href=\"https://forum.effectivealtruism.org/topics/itn-framework\"><strong>\u201cImportance, tractability, and neglectedness\u201d </strong></a>are useful for estimating the relative value of x-risk work. Toby Ord\u2019s <strong>\u201csoon, sudden, sharp</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvu57oul6tkd\"><sup><a href=\"#fnvu57oul6tkd\">[16]</a></sup></span><strong>\u201d</strong> framework from <a href=\"https://en.m.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\">\u201cThe Precipice,\u201d</a> and William MacAskill\u2019s <a href=\"https://forum.effectivealtruism.org/topics/spc-framework\"><strong>\u201csignificance, persistence, contingency</strong></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcezf8m6d28c\"><sup><a href=\"#fncezf8m6d28c\">[17]</a></sup></span><a href=\"https://forum.effectivealtruism.org/topics/spc-framework\"><strong>\u201d</strong></a><strong> </strong>framework from <a href=\"https://en.m.wikipedia.org/wiki/What_We_Owe_the_Future\">\u201cWhat We Owe The Future\u201d</a> are useful supplements to the \u201cimportance, tractability, neglectedness\u201d framework when evaluating x-risk work.</p><p>Toby Ord estimates <a href=\"https://docs.google.com/spreadsheets/d/1W10B6NJjicD8O0STPiT3tNV3oFnT8YsfjmtYR8RO_RI/edit#gid=0\">x-risk from AI is 1 in 10 this century, while risk from asteroids is 1 in 1,000,000 this century.</a> It seems at least as much is spent on <a href=\"https://www.planetary.org/articles/nasas-planetary-defense-budget-growth\">asteroid detection </a>and <a href=\"https://www.planetary.org/space-policy/cost-of-dart\">deflection </a>as <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">AI safety</a>. Perhaps, however, asteroid deflection is at least 10 times more tractable than AI safety, reducing this difference by 1 order of magnitude. While there may be even greater x-risk sub cause-area differences, this give a<strong> variance between x-risk sub-cause areas of up to at least 4 orders of magnitude.</strong></p><p>Further, we might expect interventions within x-risk sub-cause areas to have massive variance, some having a small effect, and some having huge or even most of the effect.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref44jycvb96b4\"><sup><a href=\"#fn44jycvb96b4\">[5]</a></sup></span>&nbsp;While <a href=\"https://www.files.ethz.ch/isn/162329/1427016_file_moral_imperative_cost_effectiveness.pdf\">in global health some interventions are 100 times better or worse than average,</a> x-risk sub-cause are interventions likely have far greater variance than this. These global health interventions, however, are evaluated ex post. Because it is extremely difficult to know the effectiveness of x-risk interventions until they have been implemented, we might expect closer to an ex ante variance of 10 times better or worse than average. <strong>This gives another 2 orders of magnitude variance between interventions.&nbsp;</strong></p><p>Next, we might estimate that some individuals working on x-risk may be at least 10 times more or less effective than average. This gives <strong>another 2 orders of magnitude variance for individual effectiveness.</strong></p><p>Finally, it seems likely work in the next few decades or at critical moments may be at least<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrrvwl2i0c6s\"><sup><a href=\"#fnrrvwl2i0c6s\">[18]</a></sup></span>&nbsp;10 times higher leverage than average x-risk work, with later work and work in dry spots 10 times lower leverage. <strong>This gives 2 orders of magnitude variance for timing of work.</strong></p><p>Altogether, if these effects were completely anti-correlated (which is unlikely), this gives a maximum estimate of 10 orders of magnitude variance of (positive-value) individual x-risk work. This means a particular individual, working on a particular intervention, within a particular x-risk sub-cause area, at a particular time, <strong>might adjust their inside view estimate to, at most,</strong> <strong>100,000 times more or less impact than average (though of course, as noted,</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft3tsg24d6ob\"><sup><a href=\"#fnt3tsg24d6ob\">[14]</a></sup></span><strong>&nbsp;a large amount of work may also have negative impact.)</strong></p><h1><strong>Inspiration &amp; Proximity&nbsp;</strong></h1><h2><strong>Inspiration</strong></h2><p>These estimates are quite intense, to say the least, and I hope they illuminate why individuals like myself working on x-risk feel so passionate about our work.</p><p>Most importantly, for me, these lives are not abstract expected value calculations, but real people, people who laugh and play, who experience joy and tenderness, people who feel and who love; we are saving the lives of real people, people who in all actuality <i><strong>exist.</strong></i></p><p>Just as people who are far away in space do, in fact, exist, people who are far away in time also, actually, exist. It is a sad chauvinism of our times that we so often treat these people with less consideration just because they are far away from us.</p><p>As individuals working on x-risk, we are each, in expectation, <i>individually </i>responsible for trillions of <i>real people\u2019s lives</i> every minute. My hope is that we can find this inspiring, knowing that our work does more good than we can fathom.</p><h2><strong>The Proximity Principle</strong></h2><p>The take-away from this post is not that you should agonize over the trillions of trillions of trillions<strong> </strong>of men, women, and children you are thoughtlessly murdering each time you splurge on a Starbucks pumpkin spice latte or watch cat videos on YouTube \u2014 or in any way whatsoever commit<a href=\"https://forum.effectivealtruism.org/posts/ugdFidDgGnD46bkwd/jordan-s-shortforms?commentId=mfN8ezoshdRQHSQDc\"> the ethical sin of making non-optimal use of your time.</a></p><p>The point of this post is not to create an x-risk <a href=\"https://www.lesswrong.com/posts/6cRhG6PKeASdNHqxD/dead-child-currency\">\u201cdead children currency\u201d</a> analogue. Instead it is meant to be motivating background information, giving us all the more good reason to <strong>be thoughtful about our self-care and productivity.</strong></p><p>I call the principle of strategically caring for yourself and those closest to you <a href=\"https://forum.effectivealtruism.org/posts/ugdFidDgGnD46bkwd/jordan-s-shortforms?commentId=AQeakwjzae4a6s5KH\"><strong>\u201cThe Proximity Principle,\u201d</strong></a><strong> </strong>something I discovered after several failed attempts to be perfectly purely altruistic. It roughly states that:</p><ol><li>It is easiest to affect those closest to you (in space, time, and relatedness) - including yourself</li><li>Taking care of yourself and those closest to you is high leverage for multiplying your own effectiveness in the future<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefapp3gug9m3f\"><sup><a href=\"#fnapp3gug9m3f\">[19]</a></sup></span></li></ol><p>This post estimates conversion rates for time and money into lives saved. To account for proximity, <strong>perhaps we also need conversion rates of time and money into increases in personal productivity, personal health &amp; well-being, mental health, self-development, personal relationships, and EA community culture.</strong></p><p>These factors of proximity may be hard to quantify, but probably less hard than we think, and seem like fruitful research directions for social-science oriented EAs. I think these factors are highly valuable relative to time and money, even if only valued instrumentally.</p><p>In general, for those who feel compelled to over-work to an unhealthy point, who have had a tendency to burn out in the past, or who think this may be a problem for them, I would suggest erring on the side of over-compensating in favor of self-care.</p><p>This means <strong>finding self-care activities that make you feel happy, energized, refreshed, and a sense of </strong><a href=\"https://www.existentialhope.com/\"><strong>existential hope</strong></a> \u2014 and, furthermore, <strong>doing these activities regularly, more than the minimum you feel you need </strong>to in order to work optimally.</p><p>I like to think if this as keeping my tank nearly full, rather than perpetually halfway full or nearly empty. From a <a href=\"https://en.m.wikipedia.org/wiki/Systems_theory\">systems theory</a> perspective, you are creating a <a href=\"https://medium.com/better-systems/systems-thinking-part-2-stocks-flows-and-feedback-loops-b27eadfc200\">continuous inflow and keeping your energy stocks high,</a> rather than waiting until they are fully depleted and panic/exhaustion mode alerts you to refill.</p><p>For me, daily meditation, daily exercise, healthy diet, and good sleep habits are most essential. But each person is different, so find what works for you.</p><p>Remember, if you want to change the future, you need to be at your best.<strong> You are your most valuable asset. Invest in yourself.</strong></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmv0kn0s2don\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmv0kn0s2don\">^</a></strong></sup></span><div class=\"footnote-content\"><p>While some of the numbers in the estimate are borrowed from the work of others (who may also be speculating), many are best guesses, and may be off by several orders of magnitude.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnofr924doqjq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefofr924doqjq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more information on x-risk work, <a href=\"https://80000hours.org/\">80,000 hours</a> is a great resource.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny1bmxy31yw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy1bmxy31yw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Somewhat surprisingly, it is relatively easy to donate to x-risk work through <a href=\"https://funds.effectivealtruism.org/funds/far-future\">The Long-Term Future Fund.</a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne79aoh2pnzg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe79aoh2pnzg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I estimate only the expected value of the best-case outcome in each scenario, ignoring additional contributions from less-optimal outcomes, as they would only slightly increase the expected value of each estimate.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn44jycvb96b4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref44jycvb96b4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Because an existential catastrophe is discreet (it either happens or doesn't happen) and has a limited number of contributing factors, a particular bit of x-risk work only has impact in the rare cases that it contributes to avoiding an existential catastrophe, although in these cases it may have enormous impact. This is especially true of <a href=\"https://forum.effectivealtruism.org/topics/broad-vs-narrow-interventions\">targeted/narrow interventions.</a></p><p>Furthermore, there is a significant chance all x-risk work will have zero or negative impact, as <a href=\"https://forum.effectivealtruism.org/topics/cluelessness\">the far future is incredibly hard to predict</a>, and x-risk work may fail to avert an existential catastrophe, despite our best efforts; alternatively, we may, by default, not be headed for an existential catastrophe.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzafk2cxtssi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzafk2cxtssi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This post originally used the \"term lives\" saved without mentioning nonexistence, but JBlack on LessWrong pointed out that the term \u201clives saved\u201d could be misleading in that it equates saving present lives with creating new future lives. While I take the <a href=\"https://en.wikipedia.org/wiki/Population_ethics#Totalism\">total view</a> and so feel these are relatively equivalent (if we exclude the flow-through effects, including the emotional pain caused to those left behind by the deceased), those who take other views such as the <a href=\"https://en.wikipedia.org/wiki/Population_ethics#Person-affecting_views\">person-effecting view</a> may feel very differently about this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfkhfjmiun5u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffkhfjmiun5u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By risk-neutral, I mean EAs who use expected value estimates to make decisions, even in cases where there is only a minuscule chance of having an astronomical impact, but an massive chance (risk) they will have no impact.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1ys4m46rzus\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1ys4m46rzus\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Being in a simulation or aliens eliminating much of the value of x-risk work may only apply in the moderate and optimistic scenarios, in which digital minds and interstellar travel are assumed to be possible.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnluxe2fgn13b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefluxe2fgn13b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Anthropics can be complex, with bizarre scenarios incorporating multiple levels of multiverses, advanced AI, extraterrestrial life, simulations, Boltzmann Brains, baby universes, solipsism, self-observing universes, etc. so I will not attempt a detailed analysis.</p><p>But, for example, <a href=\"https://en.m.wikipedia.org/wiki/Fermi_paradox\">The Fermi Paradox</a> might be resolved to some degree by <a href=\"https://youtu.be/l3whaviTqqg\">Grabby Aliens</a>, <a href=\"https://en.m.wikipedia.org/wiki/Rare_Earth_hypothesis#A_terrestrial_planet_of_the_right_size\">The Rare Earth Hypothesis</a>, or my personal favorite, <a href=\"https://youtu.be/XglOw2_lozc\">The Youngness Paradox</a>.</p><p>More importantly, <a href=\"https://en.m.wikipedia.org/wiki/Doomsday_argument\">The Doomsday Argument</a> could be resolved<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref15wcscnar7q\"><sup><a href=\"#fn15wcscnar7q\">[20]</a></sup></span>&nbsp;by postulating that our reference class for our place in the universe may soon end; yet this won\u2019t lead to extinction or reduction in value.</p><p>This would be the case, for example, if we populate the universe with emulations of maximally happy conscious entities that are unable to accurately observe the universe-at-large and be surprised by their place in it; therefore, we would not be the same reference class as the many future lives our x-risk work intends to save;</p><p>i.e. if the future lives we save are not capable of observing and being unsurprised by their own, more typical place in the universe, it should not surprise us that we do not observe ourselves as them, being unsurprised.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8x2f4tvkwf8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8x2f4tvkwf8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that when I say \"pessimistic\" or \"conservative,\" I always mean this from the point of view of how much impact each unit of x-risk work has on reducing existential risk. The more people working on existential risk, and the longer they spend working on it, the less impactful each unit of x-risk work is per unit of existential security achieved. If we instead estimate fewer people working on x-risk for less time, and estimate the same reduction in risk, this would lead to much more optimistic estimates per unit of work.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsen3w5jrn4j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsen3w5jrn4j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>My understanding is that 21 orders of magnitude are due to interstellar travel, and conditional on interstellar travel being possible, the other 15 orders of magnitude are due to digital minds.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh5tqmrlji4s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh5tqmrlji4s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I am mostly relying on Bostrom here, though I also know that Seth Lloyd calculated that <a href=\"https://arxiv.org/abs/quant-ph/9908043\">a 1 kilogram 1 liter \u201cultimate laptop\u201d could compute a maximum of over 10^50 operations per second,</a> based on the laws of entropy and quantum theory, or more than 10^33 times faster than the human brain.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndjc60am835e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdjc60am835e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>While this may seem outrageous, and may be even more improbable than 99.9% unlikely, it seems very hard to know this with certainty, and it is necessary to incorporate this possibility somehow, especially for the optimistic expected value calculation.</p><p>Furthermore, the seeming unlikelihood may be balanced out somewhat by the possibility of breakthroughs in physics that allow many orders of magnitude more possible lives than this</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt3tsg24d6ob\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft3tsg24d6ob\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There are several reasons for this:</p><p>The community of individuals working on x-risk is currently small, and so mediocre work may distract from and crowd out more important work.</p><p>Some work may spread dangerous ideas <a href=\"https://forum.effectivealtruism.org/topics/information-hazard\">(information hazards,)</a> such as ideas apocalyptic terrorists could use to do harm, or ideas that cause unscrupulous states to realize potentially long-term dangerous technologies could give them a decisive short-term strategic advantage.</p><p>Some work may accidentally push forward capabilities of harmful technologies more than it pushes forward safety. This can occur with <a href=\"https://forum.effectivealtruism.org/topics/dual-use\">dual-use research of concern</a> such as <a href=\"https://en.m.wikipedia.org/wiki/Gain-of-function_research\">gain-of-function</a> research. Relatedly, some work may fail to pursue <a href=\"https://forum.effectivealtruism.org/topics/differential-progress\">differential technological development</a> when it should be pursued, for example AI capabilities research that speeds up the development of advanced artificial intelligence faster than we can ensure it is safe.&nbsp;</p><p>Finally, x-risk work may successfully lock in extremely sub-optimal values. For example, we could successfully permanently \u201calign\u201d and enslave sentient AI whose suffering consciousness outweighs humans\u2019 happiness, and we either do not know or do not care that it is conscious; or we could successfully lock in human values that are many orders of magnitude less good than the best values we could lock in.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuagbr16b4pn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuagbr16b4pn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Additionally, the individual is comparing themselves with other individuals working on x-risk. These are, at present, often highly intelligent and ambitious people recruited from elite universities. A priori, it is just as likely their impact is less than average (including negative<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft3tsg24d6ob\"><sup><a href=\"#fnt3tsg24d6ob\">[14]</a></sup></span>) as greater than average.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvu57oul6tkd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvu57oul6tkd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Soon - how soon will the existential threat occur</p><p>Sudden - how sudden will the onset be (how much time will there be to take action between public awareness of the problem and occurrence of existential catastrophe)</p><p>Sharp - &nbsp;how sharp is the distribution; how likely are warning shots (events which are similar, but don\u2019t end civilization, such that people better prepare for the x-risk)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncezf8m6d28c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcezf8m6d28c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Significance - goodness or badness of a state of affairs at any given moment</p><p>Persistence - length of time a state of affairs will last</p><p>Contingency - how likely the state of affairs would have been to occur if no intervention was made</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrrvwl2i0c6s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrrvwl2i0c6s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Work at extremely critical times may have much greater variance, but this may be hard to predict and is likely not multiplicative with the other factors.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnapp3gug9m3f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefapp3gug9m3f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For those with high altruistic leverage, such as being near the <a href=\"https://forum.effectivealtruism.org/topics/hinge-of-history\">\u201chinge of history,\u201d</a> point number 2 may be highly dominant.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn15wcscnar7q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref15wcscnar7q\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.youtube.com/watch?v=XglOw2_lozc&amp;feature=youtu.be\">The Youngness Paradox</a> might also resolve <a href=\"https://en.m.wikipedia.org/wiki/Doomsday_argument\">The Doomsday Argument</a>, especially in the pessimistic scenario.</p></div></li></ol>", "user": {"username": "Jordan Arel"}}, {"_id": "qoXQydGAeRACfmJuH", "title": "What is the relationship between impact and EA Forum karma?", "postedAt": "2022-12-06T10:42:08.896Z", "htmlBody": "<h1>Questions</h1><p>The current karma system of EA Forum, which is described <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/gNHFRWyo58cTQ8pe8/ea-forum-2-0-initial-announcement-1%23A_reworked_karma_system&amp;sa=D&amp;source=editors&amp;ust=1667579009740592&amp;usg=AOvVaw1aBnkzWG_qksjJS4XSAvwT\"><u>here</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzngo6fwk5yq\"><sup><a href=\"#fnzngo6fwk5yq\">[1]</a></sup></span>,&nbsp;is based on the assumption that karma is positively correlated with impact (of the post/comment). I think this is pretty reasonable, but how strong is the correlation?</p><p>I am also curious about the shape of the relationship between karma and impact. For example, if post Y has twice as much karma as post X, is Y roughly 2 times as valuable as X (<a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Linear_function&amp;sa=D&amp;source=editors&amp;ust=1667579009741565&amp;usg=AOvVaw17xiUYC_2WYNtzQLGohvPN\"><u>linear</u></a>&nbsp;function), less than than (e.g. <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Logarithm&amp;sa=D&amp;source=editors&amp;ust=1667579009741846&amp;usg=AOvVaw20u1P7HXh7qrjtPQ-ujDXe\"><u>logarithmic</u></a>), or more than that (e.g. <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Quadratic_function&amp;sa=D&amp;source=editors&amp;ust=1667579009742091&amp;usg=AOvVaw0to8ME2uAaiSsdtWohZo0y\"><u>quadratic</u></a>)?</p><p>Moreover, what is the impact caused by a post with a given amount of karma? Ideally, one would want an answer in terms of the effect on the expected value of the future, but I understand this is hardly feasible. So, in practice, using heuristics may be better. Examples include donations to the <a href=\"https://www.google.com/url?q=https://funds.effectivealtruism.org/funds/far-future&amp;sa=D&amp;source=editors&amp;ust=1667579009742557&amp;usg=AOvVaw0Qu7DDEe2XBb9PP_g7nJSU\"><u>Long-Term Future Fund</u></a>&nbsp;(or other), and the metric quality adjusted research papers (QARPs) used by Nu\u00f1o Sempere <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/pqphZhx2nJocGCpwc/relative-impact-of-the-first-10-ea-forum-prize-winners&amp;sa=D&amp;source=editors&amp;ust=1667579009742879&amp;usg=AOvVaw07vjaeP1oC1l5cu72IXQD0\"><u>here</u></a>.</p><h1>Example answers</h1><p>I tried to get a very preliminary sense of the answers to the questions above based on Nu\u00f1o Sempere\u2019s <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/pqphZhx2nJocGCpwc/relative-impact-of-the-first-10-ea-forum-prize-winners&amp;sa=D&amp;source=editors&amp;ust=1667579009743513&amp;usg=AOvVaw0B9qqi7UEzVd68-Tq7QUgb\"><u>analysis</u></a>&nbsp;of the first 10 winners of the <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/forum-prize&amp;sa=D&amp;source=editors&amp;ust=1667579009743881&amp;usg=AOvVaw0Sm8H2Vx0qKqfQ0PE8phXo\"><u>EA Forum Prize</u></a>. I estimated the mean impact in QARPs of each of the posts from the mean between the lower and upper limit of the 80 % <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Confidence_interval&amp;sa=D&amp;source=editors&amp;ust=1667579009744211&amp;usg=AOvVaw1NAs8EghQ9XJcHyL-I7LWR\"><u>confidence intervals</u></a>&nbsp;provided by Nu\u00f1o. The calculations are in tab \u201cPosts\u201d of <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1ZVRWt9NGUqYFSpEBMEfyBLG2-0bRSCIpiNeOtiyF1Os/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1667579009744554&amp;usg=AOvVaw3Hhl-jfPE63lLWTcilTD4l\"><u>this</u></a>&nbsp;Sheet.</p><p>The table below contains the slope and <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&amp;sa=D&amp;source=editors&amp;ust=1667579009745049&amp;usg=AOvVaw3Hc_8JKqBgWA6lH_hmdOfv\"><u>correlation coefficient</u></a>&nbsp;for the linear regression of the mean impact on various functions of the karma. It also has the <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/P-value&amp;sa=D&amp;source=editors&amp;ust=1667579009745373&amp;usg=AOvVaw2vpVadMO3vNQsQcVIagJRB\"><u>p-value</u></a>&nbsp;for the null hypothesis that there is no correlation. The calculations are in tab&nbsp;\u201cStatistics\u201d.</p><figure class=\"table\"><table><thead><tr><th style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">Null intercept linear regression between mean impact (QARP) and...</th><th style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Slope</p></th><th style=\"text-align:center\">Intercept</th><th style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Correlation coefficient (R)</p></th><th style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>P-value</p></th></tr></thead><tbody><tr><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">Logarithm of the karma</td><td style=\"padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.244</p></td><td style=\"text-align:center\">-1.04</td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.445</p></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.269</p></td></tr><tr><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">Square root of the karma</td><td style=\"padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.0492</p></td><td style=\"text-align:center\">-0.407</td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.440</p></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.276</p></td></tr><tr><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">Karma</td><td style=\"padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>2.45 <a href=\"https://en.wikipedia.org/wiki/Metric_prefix#List_of_SI_prefixes\">m</a></p></td><td style=\"text-align:center\">-0.164</td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.433</p></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.284</p></td></tr><tr><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">Square of the karma</td><td style=\"padding:2pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>11.8 <a href=\"https://en.wikipedia.org/wiki/Metric_prefix#List_of_SI_prefixes\">\u00b5</a></p></td><td style=\"text-align:center\">-0.0427</td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.416</p></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.305</p></td></tr></tbody></table></figure><p>For the logarithm, square root and square of the karma, and karma alone, the correlation coefficient is 0.4, and the p-value 0.3. The differences are quite small given the sample size of 8<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdecuwv06gsg\"><sup><a href=\"#fndecuwv06gsg\">[2]</a></sup></span>. Interestingly, the above estimates for the correlation coefficient are similar to the 0.47 obtained by Nathan Young&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YajssmjwKndBTahQx/improving-karma-usd8mn-of-possible-value-my-estimate\"><u>here</u></a> for the relationship between the inflation-adjusted karma and ranking of the posts of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jB7Ten8qmDszRMTho/effective-altruism-the-first-decade-forum-review\"><u>Decade Review</u></a>.</p><p>The table below shows the correspondence between the scale described <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/pqphZhx2nJocGCpwc/relative-impact-of-the-first-10-ea-forum-prize-winners%23Methodology&amp;sa=D&amp;source=editors&amp;ust=1667579009751803&amp;usg=AOvVaw2Torq4Vfo4Md8yVu87NV72\"><u>here</u></a>&nbsp;by Nu\u00f1o and the karma predicted by the linear regression of the mean impact on logarithm of karma, which has the best fit among the 4 presented above. The calculations are in tab \u201cPredictions\u201d.</p><figure class=\"table\"><table><thead><tr><th style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Mean impact (QARP)</p></th><th style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Description</p></th><th style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><p>Example</p></th><th style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>Predicted karma</p></th></tr></thead><tbody><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>0.1 m</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cA thoughtful comment\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/3PjNiLLkCMzAN2BSz/when-setting-up-a-charity-should-you-employ-a-lawyer?commentId%3DYNKNcp6nKqxqkZgCu&amp;sa=D&amp;source=editors&amp;ust=1667579009754639&amp;usg=AOvVaw2SaEitnBAFruGsc_xYDr3i\"><u>A thoughtful comment about the details of setting up a charity</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>67.0</p></td></tr><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1 m</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cA good blog post, a particularly good comment\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://www.lesswrong.com/s/5Eg2urmQjA4ZNcezy/p/pTK2cDnXBB5tpoP74&amp;sa=D&amp;source=editors&amp;ust=1667579009756116&amp;usg=AOvVaw0QLf81v9F3s07T3bkC6e0l\"><u>What considerations influence whether I have more influence over short or long timelines?</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>67.3</p></td></tr><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>10 m</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cAn excellent blog post\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://www.lesswrong.com/posts/4AHXDwcGab5PhKhHT&amp;sa=D&amp;source=editors&amp;ust=1667579009757719&amp;usg=AOvVaw1hcme9M860HEheALxjCppX\"><u>Humans Who Are Not Concentrating Are Not General Intelligences</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>71.0</p></td></tr><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>100 m</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cA fairly valuable paper\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://arxiv.org/abs/1803.04585&amp;sa=D&amp;source=editors&amp;ust=1667579009759326&amp;usg=AOvVaw3bgZu9t0V69M7JpwlbFrPy\"><u>Categorizing Variants of Goodhart's Law</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>108</p></td></tr><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>1</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cA particularly valuable paper\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://nickbostrom.com/papers/vulnerable.pdf&amp;sa=D&amp;source=editors&amp;ust=1667579009760976&amp;usg=AOvVaw2woT8XungKCVT8VArwUZLd\"><u>The Vulnerable World Hypothesis</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>475</p></td></tr><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>10 to 100</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cA research agenda\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://globalprioritiesinstitute.org/research-agenda/&amp;sa=D&amp;source=editors&amp;ust=1667579009762710&amp;usg=AOvVaw3kmKKfQxHcHLYtPC7fWmXp\"><u>The Global Priorities Institute's Research Agenda</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>4.15 k to 40.9 k</p></td></tr><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>100 to &gt; 1000</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cA foundational popular book on a valuable topic\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://smile.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742?sa-no-redirect%3D1&amp;sa=D&amp;source=editors&amp;ust=1667579009764282&amp;usg=AOvVaw2VFTHQODhtMZSlcT-p4Gdo\"><u>Superintelligence</u></a>, <a href=\"https://www.google.com/url?q=https://smile.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555?sa-no-redirect%3D1&amp;sa=D&amp;source=editors&amp;ust=1667579009764576&amp;usg=AOvVaw2dpuh4TCvCcay6ELwLeYhL\"><u>Thinking Fast and Slow</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>40.9 k to &gt; 408 k</p></td></tr><tr><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>&gt; 1000</p></td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\">\u201cA foundational research work\u201d</td><td style=\"padding:5pt\" colspan=\"1\" rowspan=\"1\"><a href=\"https://www.google.com/url?q=https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&amp;sa=D&amp;source=editors&amp;ust=1667579009766292&amp;usg=AOvVaw2hdg9dhpy5d6RlyQP5_8WI\"><u>Shannon\u2019s \"A Mathematical Theory of Communication\"</u></a></td><td style=\"padding:5pt;text-align:center\" colspan=\"1\" rowspan=\"1\"><p>&gt; 408 k</p></td></tr></tbody></table></figure><p>The predictions above are quite poor. For instance, they imply:</p><ul><li>An unreasonably small difference of:<ul><li>0.367 karma between the impact of \u201ca good blog post, a particularly good comment\u201d and \u201ca thoughtful comment\u201d.</li><li>3.67 karma between the impact of \u201can excellent blog post\u201d and \u201ca good blog post, a particularly good comment\u201d.</li></ul></li><li>\u201cA particularly valuable paper\u201d is worth 475 karma, whereas I think the right value is of the order of magnitude of 10 kkarma (with huge variation), i.e. 21.1 times as large.</li></ul><p>In any case, one should certainly be mindful of&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/goodhart-s-law\"><u>Goodhart's Law</u></a>, and do not start optimising posts just for karma!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzngo6fwk5yq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzngo6fwk5yq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For interesting discussions of the system, see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SApmQrKdvgccmH2yF/revisiting-the-karma-system\"><u>this</u></a> post from Arepo, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YajssmjwKndBTahQx/improving-karma-usd8mn-of-possible-value-my-estimate\"><u>this</u></a> one from Nathan Young.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndecuwv06gsg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdecuwv06gsg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Nu\u00f1o evaluated 10 posts, but only scored 8 in terms of QARPs.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}]