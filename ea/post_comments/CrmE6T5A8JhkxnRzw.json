[{"_id": "Rbwmc5WwbGfDboyo9", "postedAt": "2023-03-25T23:31:45.898Z", "postId": "CrmE6T5A8JhkxnRzw", "htmlBody": "<p>I really liked this edition! Best wishes for whatever is next.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "ezTQtDojwHiBcNjer", "postedAt": "2023-03-30T14:51:53.216Z", "postId": "CrmE6T5A8JhkxnRzw", "htmlBody": "<p>Thank you, Vasco.</p>", "parentCommentId": "Rbwmc5WwbGfDboyo9", "user": {"username": "Pablo_Stafforini"}}, {"_id": "zomn4jv9kr4JMw2uY", "postedAt": "2023-04-07T16:36:52.215Z", "postId": "CrmE6T5A8JhkxnRzw", "htmlBody": "<p>Thanks so much for this!</p><p>I don't suppose either of you have read Petra Kosonen's,&nbsp;<a href=\"https://globalprioritiesinstitute.org/tiny-probabilities-and-the-value-of-the-far-future-petra-kosonen/\"><u>Tiny probabilities and the value of the far future</u></a>? I found it a little hard to follow but am I right in thinking that one of the main arguments is the following?</p><p><i>Even if we think this 'time of perils' is here to stay and assume a </i>constant<i> existential risk per century of 1 in 6 from now on - and with no sci-fi space colonization or 'digital people' in the meantime - an individual's donations to AI safety efforts are </i>still<i> more cost-effective than donations to AMF, since biological humanity's expected lifespan on Earth only needs to be at least another ~250 years to make that the case.</i></p><p>(This assumes that current existential risk from AI is &gt;0.1% in the next 100 years, that an additional $1bn would reduce this probability by &gt;1% - i.e. from 0.1% to 0.099% - and that we should treat an individual's contribution to this $1bn as non-negligible just as we do with voting.)</p><p>That might not be enough to convince most people who have very high confidence in person-affecting ethics, but it could be persuasive for some others with broadly 'neartermist' leanings.</p>", "parentCommentId": null, "user": null}, {"_id": "ACqmKaL4bfs89Mjci", "postedAt": "2023-04-07T20:25:00.995Z", "postId": "CrmE6T5A8JhkxnRzw", "htmlBody": "<p>Hi Ubuntu,</p><p>I'm not sure if you are already aware of it, but we featured <a href=\"https://forum.effectivealtruism.org/s/Y7rCDmxRbrrKBT9Bo/p/5AzxpkNzgFWnjdqTf#Conversation_with_Petra_Kosonen\">a conversation</a> with Petra in an early issue of our newsletter, where she discusses some of these topics (including probability discounting and its implications for longtermism). I mention it in case it helps clarify some of the claims she makes in the paper.</p>", "parentCommentId": "zomn4jv9kr4JMw2uY", "user": {"username": "Pablo_Stafforini"}}, {"_id": "AHLf8j9pwKiQrb73b", "postedAt": "2023-05-09T10:37:50.433Z", "postId": "CrmE6T5A8JhkxnRzw", "htmlBody": "<p>[Update from Pablo &amp; Matthew]</p><p>As we reached the one-year mark of&nbsp;<i>Future Matters</i>, we thought it a good moment to pause and reflect on the project. &nbsp;While the newsletter has been a rewarding undertaking, we\u2019ve decided to stop publication in order to dedicate our time to new projects. Overall, we feel that launching&nbsp;<i>Future Matters</i> was a worthwhile experiment, which met (but did not surpass) our expectations. Below we provide some statistics and reflections.&nbsp;</p><h2>Statistics</h2><p>Aggregated across platforms, we had between 1,000\u20131,800 impressions per issue. Over time, an increasingly larger share came from Substack, reflecting our growth in subscribers on that platform and the absence of an equivalent subscription service on the EA Forum.<br>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/om7sypqwusnqxvc4ybef\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/s6koeb3b97u75vkd7inp 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/qecedpo0wylo8woll9ra 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/oasfq1kb6bax21t1ecww 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/soui4unvvl8mwjcmd6tx 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/nc6phnhxtfdn0rp2jffp 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/cfzg8hxltgac4cqeh4aq 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/tmi0kucd1gfrqee4eqc7 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/lkburxaj0ry93lii5qhv 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/gostbagfsvvmxsxdwjtg 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/i6y8u0hkn2cotarmpthk 1302w\"><figcaption>Impressions per issue</figcaption></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/vlqnqxse9xxlyy9hh4xn\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/efensivl1pxoymt99h8m 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/uzykjj5kab24ascj5mvw 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/mcw2qcdre9uhazgtrwhf 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/hpcimubesibft6vhbyjz 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/oycgqpldhxprdzzjl4va 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/vahoszjmm6x1vwnxjom4 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/hasbs5fedf3olinyyqpv 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/rgu5rskz4e854yzxmbkf 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/dl51fomtjmzycxffgnbc 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/mlojc1ycsvxyx4l13ocq 1476w\"><figcaption>Substack subscriptions</figcaption></figure><p>&nbsp;</p><p>A substantial fraction of our subscriptions came via other EA newsletters:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/axnije593dps0vuba0y6\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/bfawzw9yffbmyk0faubq 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/bihkbqmmnquvmpmeuyje 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/zlshawrbdt0orw8e4cri 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/qpbzhsuzik0e6vdiz9ki 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/b3t3jo9fptpishjwhhza 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/gcbi6vnylfnyz6rfcczg 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/lnyhufeywkbo58pdvfrg 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/bdnhcue86q2o0uz5krxc 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/d2fq3g7mxxw6ag6fbubh 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/AHLf8j9pwKiQrb73b/clr3hjxyhijqpnutgauc 1282w\"><figcaption>Top sources of Substack referrals</figcaption></figure><p>&nbsp;</p><h2>Reflections</h2><p><strong>Time investment.&nbsp;</strong>Writing the newsletter took considerably more time than we had anticipated. Much of that time involved two activities: (1) actively scanning&nbsp;<a href=\"https://twitter.com/i/lists/1532050255461621760\"><u>Twitter lists</u></a>,&nbsp;<a href=\"https://ea.news/\"><u>EA News</u></a>, email alerts and other sources for suitable content and (2) reading and summarizing this material. The publication process itself was also pretty time-consuming, but we were able to fully delegate it to a very efficient and reliable assistant. Overall, we each spent at least 2\u20133 days working on each issue.&nbsp;</p><p><strong>AI stuff.&nbsp;</strong>Over the course of the year, AI-related content began to dwarf other topics, to the point where&nbsp;<i>Future Matters</i> became mostly AI-focused.&nbsp;</p><p>.<img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/f3vfocf7s7spdbzuejkv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/jecpf9a1xwaduapaj1z8 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/lqxj6oayqylqmjch2lsl 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/mhouukdksr2r7cucjisj 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/hxhauumokfljdsfwtdzn 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/rgtgubztlhjodl73mdjh 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/dbrxzohkg6tcizuu53bq 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/hpdf8qoikubux572sdtz 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/wzgyugbyvwkmwkyl0kif 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/bipzdpohmuplhdlt4rmp 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/9XSxLfxzddZecFCa3/ve49ngic4ew9r84jjuo8 1200w\"></p><p>We feel like this shift in priorities was warranted \u2014 the recent pace of AI progress has been staggering, as has been the recklessness of certain AI labs. All the more surprising has been the receptiveness of the public and media to taking AI risk concerns seriously (e.g. the momentum behind measures to slow down AI progress).&nbsp;</p><p>In this context, it appears to us that the value of a newsletter focused on longtermism and existential risk generally is lower than it was when we started it, relative to a newsletter with a sole focus on AI risk. But we don\u2019t think we\u2019re the best people to run such a newsletter. There are already a number of good active AI newsletters out there, which have their own focuses:</p><ul><li><a href=\"https://importai.substack.com/\"><u>ImportAI</u></a> (Jack Clark)</li><li><a href=\"https://newsletter.safe.ai/\"><u>AI safety newsletter</u></a> (CAIS/Dan Hendrycks)</li><li><a href=\"https://chinai.substack.com/\"><u>ChinAI</u></a> (Jeffrey Ding)</li><li><a href=\"https://navigatingairisks.substack.com/\"><u>Navigating AI Risks</u></a> (Campos&nbsp;<i>et al</i>)</li></ul><p>The recent progress in AI has made us more reluctant to continue investing time in this project for a separate reason. Much of the work&nbsp;<i>Future Matters</i> demands, as noted earlier, involves collecting and summarizing content. But these are tasks that GPT-4 can already do tolerably well, and which we expect could be mostly delegated within the next few months.</p>", "parentCommentId": null, "user": {"username": "matthew.vandermerwe"}}]