[{"_id": "xzvZ6WegWrFFQyeQT", "postedAt": "2023-02-20T05:50:26.942Z", "postId": "6aYfWyo9DKEheogf8", "htmlBody": "<p>Agreed. It also doesn't seem to me that even a successful alignment of AGI to \"human values\"--the same values which gave us war, slavery, and even today still torture billions of sentient beings per year before killing them--is prima facie a good thing.</p><p>Does AI alignment imply that the AGI must care about blind spots in today's \"human values\", and seek to expand its circle to include moral patients who would otherwise have been ignored? Not necessarily.</p>", "parentCommentId": null, "user": {"username": "Ariel Simnegar"}}, {"_id": "nNrfAdWE3FsvefPwF", "postedAt": "2023-02-20T10:23:23.149Z", "postId": "6aYfWyo9DKEheogf8", "htmlBody": "<p>I think some of these problems apply to the term \"AI safety\" as well. Say we built an AGI that was 100% guaranteed not to conquer humanity as a whole, but still killed thousands of people every year. Would we call that AI \"Safe\"?&nbsp;</p><p>One possible way to resolve this is for:</p><p>\"misalignment\" refers to the AI not doing what it's meant to do</p><p>\"AI safety\" refers to ensuring that the consequences of misalignment are not majorly harmful</p><p>\"AI X-risk safety\" refers to ensuring that AI does not destroy or conquer humanity.&nbsp;</p><p>Each one being an easier subset of the problem above it.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "EZaN9oi9Z7i5PGgGe", "postedAt": "2023-02-20T14:19:58.774Z", "postId": "6aYfWyo9DKEheogf8", "htmlBody": "<p>There's been tons of discussion about this, and I think most of the field disagrees, and there are some complications you're missing.<br><br>In the introduction to a <a href=\"https://arxiv.org/abs/2201.02950\">paper last year</a>, Issa Rice and I tried to clarify some definitions, which explains why \"control\" isn't the same as \"alignment\":</p><blockquote><p>The term AI-safety also encompasses technical approaches that attempt to ensure that humans retain the ability to control such systems or to restrict the capabilities of such systems (AI control), as well as approaches that aim to align systems with human values\u2014that is, the technical problem of figuring out how to design, train, inspect, and test highly capable AI systems such that they produce all and only the outcomes their creators want (AI alignment). &nbsp;</p></blockquote><p>More recently, <a href=\"https://forum.effectivealtruism.org/posts/GEukFgwrrebW7efNz/what-does-it-mean-for-an-agi-to-be-safe-1?commentId=2SYzLwMTJqhP688Gy\">Rob Bensinger said</a>:</p><blockquote><p>\"AGI existential safety\" seems like the most popular relatively-unambiguous term for \"making the AGI transition go well\", so I'm fine with using it until we find a better term.</p><p>I think \"AI alignment\" is a good term for the technical side of differentially producing good outcomes from AI, though it's an imperfect term insofar as it collides with Stuart Russell's \"value alignment\" and Paul Christiano's \"intent alignment\". (The latter, at least, better subsumes a lot of the core challenges in making AI go well.)&nbsp;</p></blockquote>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "iL3nfyvgfemsG6MtC", "postedAt": "2023-02-20T14:22:04.951Z", "postId": "6aYfWyo9DKEheogf8", "htmlBody": "<p>You may enjoy <a href=\"https://acritch.com/arches/\">Critch &amp; Kreuger 2020</a>. They propose a new frame called delegation, which is broken up into instruction, comprehension, and control.&nbsp;</p>", "parentCommentId": null, "user": {"username": "quinn"}}, {"_id": "thgL9jxsPPQMn7Hbp", "postedAt": "2023-02-20T14:41:01.034Z", "postId": "6aYfWyo9DKEheogf8", "htmlBody": "<p>Thanks!</p>\n<p>I hate to be someone who walks into a heated debate and pretends to solve it in one short post, so I hope my post didn\u2019t come off too authoritative (I just genuinely have never seen debate about the term). I\u2019ll look more into these.</p>\n", "parentCommentId": "EZaN9oi9Z7i5PGgGe", "user": {"username": "RedStateBlueState"}}, {"_id": "JcbdhxhHFuAwBrSsL", "postedAt": "2023-02-20T15:57:55.552Z", "postId": "6aYfWyo9DKEheogf8", "htmlBody": "<blockquote><p>\"AI safety\" refers to ensuring that the consequences of misalignment are not majorly harmful</p></blockquote><p>That's saying that AI safety is about protective mechanisms and that alignment is about preventative mechanisms. I haven't heard the distinction drawn that way, and I think that's an unusual way to draw it.</p><p>Context:&nbsp;</p><p>Preventative Barrier: prevent initiating hazardous event (decrease probability(event))</p><p>Protective Barrier: minimize hazardous event consequences (decrease impact(event))</p><p>Broader videos about safety engineering distinctions in AI safety: <a href=\"https://www.youtube.com/watch?v=fNFIdS1rGq8\">[1]</a>, <a href=\"https://www.youtube.com/watch?v=Ic_qDqYEJcA\">[2]</a>.</p>", "parentCommentId": "nNrfAdWE3FsvefPwF", "user": {"username": "Dan Hendrycks"}}, {"_id": "Gi8o28reuzvFi2MhR", "postedAt": "2023-02-20T16:28:15.662Z", "postId": "6aYfWyo9DKEheogf8", "htmlBody": "<p>So partially what I'm saying is that the definitions &nbsp;of \" AI alignment\" and &nbsp;\"AI safety\" are confusing, and people are using them to refer to different things in a way that can mislead. For example, if you declare that your AI is \"safe\" while it's killing people on the daily (because you were referring to extinction), people will rightly feel mislead and angry.&nbsp;</p><p>Similarly, for \"misalignment\", an image generator giving you an image of a hand with the wrong number of fingers is misaligned in the sense than you care about the correct number of fingers and it doesn't know this. But it doesn't cause any real harm the way that a malfunction in a healthcare diagnoser would.&nbsp;</p><p>Your point about safety wanting to prevent as well as protect is a good one. I think \"AI safety\" should refer to both.&nbsp;</p>", "parentCommentId": "JcbdhxhHFuAwBrSsL", "user": {"username": "titotal"}}]