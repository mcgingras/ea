[{"_id": "GnoQ8ei4Ajnk52N97", "postedAt": "2023-11-28T00:06:56.682Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Could you explain what the bars for AMF mean in Figure 2? I looked at page 80 and found the following values for \"Overall cost-effectiveness in WELLBYs per $1,000 (xGD)\"</p><p>AMF 56.16 (6.85x) under deprivationism</p><p>AMF 29.34 (3.58x) under TRIA</p><p>AMF 6.69 (0.82x) under epicureanism</p><p>I considered that the bars could reflect uncertainty about the neutral point from 0-5, but the bars look even wider than the figure on page 83 would suggest.</p>", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "xtu6gSh3j2RBdywzk", "postedAt": "2023-11-28T00:47:42.286Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Hi Jason,&nbsp;</p><p>The bars for AMF in Figure 2 should represent the range of cost-effectiveness estimates that come from inputting different neutral points, and for TRIA the age of connectedness.&nbsp;</p><p>This differs from the values given in Table 25 on page 80 because, as we note below that table, the values there are based on assuming a neutral point of 2 and an TRIA age of connectedness of 15.&nbsp;</p><p>The bar also differs for the range given in Figure 13 on page 83 because the lowest TRIA value has an age of connectivity of 5 years, where in Figure 2 (here) we allow it to go as low as 2 years I believe<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5k3n40f0qtg\"><sup><a href=\"#fn5k3n40f0qtg\">[1]</a></sup></span>.&nbsp;</p><p>I see that the footnote explaining this is broken, I'll fix that.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5k3n40f0qtg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5k3n40f0qtg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>When I plug (Deprivationism, neutral point =0; TRIA, neutral point =0, age of connectedness = 2) into <a href=\"https://samuel-at-happierlivesinstitute.shinyapps.io/test_comparing_life_extending_to_life_improving/\">our calculator</a> it spits out a cost-effectiveness of 91 WELLBYs per $1,000 for deprivationism and 78 for TRIA (age of connectivity = 2) -- this appears to match the upper end of the bar.&nbsp;</p></div></li></ol>", "parentCommentId": "GnoQ8ei4Ajnk52N97", "user": {"username": "JoelMcGuire"}}, {"_id": "FcQ8R3otZy83esoqY", "postedAt": "2023-11-28T01:42:36.547Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Thanks -- that helps. \"The lines for AMF (the Against Malaria Foundation) are different from the others\" was really confusing without the footnote call following it; it makes a lot more sense now after the fix!</p>", "parentCommentId": "xtu6gSh3j2RBdywzk", "user": {"username": "Jason"}}, {"_id": "EfYZzHGT5SH7hMaXy", "postedAt": "2023-11-28T10:59:36.104Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Thanks Joel nice one!</p>\n<p>I'm interested in where this figure Congress from \"cost per person treated for StrongMinds hasadeclined to $63\"  - that's a big change.</p>\n", "parentCommentId": null, "user": {"username": "NickLaing"}}, {"_id": "TgvPEtgPofaG2ZkmH", "postedAt": "2023-11-28T12:52:34.365Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p><strong>Executive summary</strong>: This report updates a previous analysis on the cost-effectiveness of psychotherapy for depression in low- and middle-income countries (LMICs). It finds psychotherapy leads to substantial wellbeing improvements that last over time, with effects varying based on program specifics.</p><p><strong>Key points</strong>:</p><ol><li>Updated meta-analysis of 74 studies finds psychotherapy improves wellbeing by 0.7 standard deviations, a benefit of 2.69 WELLBYs per person.</li><li>Household spillover effects are smaller than previously estimated, at 16% of the direct effect.</li><li>For StrongMinds, updated estimate is 30 WELLBYs per $1000 donated. For Friendship Bench, initial estimate is 58 WELLBYs per $1000.</li><li>StrongMinds now estimated as 3.7 times more cost-effective than cash transfers in terms of WELLBYs. Friendship Bench is 7 times more cost-effective.</li><li>New methodology uses Bayesian updating to combine general psychotherapy evidence with charity-specific factors.</li><li>Overall quality of evidence is judged as moderate, with uncertainty around household spillovers and charity-specific effects.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "peGPR4XcSP6EBrPhc", "postedAt": "2023-11-28T17:23:51.127Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Hi Nick,&nbsp;</p><p>Good question. I haven't dug into this in depth, so consider this primarily my understanding of the story. I haven't gone through an itemized breakdown of StrongMinds costs on a year by year basis to investigate this further.</p><p>It is a big drop from our previous costs. But I originally did the research in Spring 2021, when 2020 was the last full year. That was a year with unusually high costs. I didn't use those costs because I assumed this was mostly a pandemic related aberration, but I wasn't sure how long they'd keep the more expensive practices like teletherapy they started during COVID (programmes can be sticky). But <a href=\"https://strongminds.org/wp-content/uploads/2023/11/2023-Q2-report-8.5x11-FINAL.pdf\">they paused their expensive teletherapy programme this year because of cost concerns (p. 5).&nbsp;</a></p><p>So $63 is a big change from $170, but a smaller change from $109 -- their pre-COVID costs.</p><p>What else accounts for the drop though? I think \"scale\" seems like a plausible explanation. The first part of the story is fixed / overhead costs being spread over a larger number of people treated with variable (per person) costs remaining stable. StrongMinds spends at least $1 million on overhead costs (office, salaries, etc). The more people are treated, the lower the per person costs (all else equal). The second part of the story is that I think it's plausible that variable costs (i.e., training and supporting the person delivering the therapy) are also decreasing. They've also shifted towards moving away from staff-centric delivery model to using more volunteers (e.g., community health workers), which likely depresses costs somewhat further. We discuss their scaling strategy and the complexities it introduces into our analysis <a href=\"https://www.happierlivesinstitute.org/wp-content/uploads/2023/11/Talking-through-depression-The-cost-effectiveness-of-psychotherapy-in-LMICs-revised-and-expanded-November-2023.pdf\">a bit more around page 70 of the report.&nbsp;</a></p><p>Below I've attached <a href=\"https://strongminds.org/wp-content/uploads/2023/11/2023-Q2-report-8.5x11-FINAL.pdf\">StrongMinds most recent reporting</a> about their number treated and cost per person treated, which gives a decent overall picture for how the costs and the number treated have changed over time.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/gcqpvucjkzu7hg0yfs2y\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/gbidmx614bpilvxe9qgd 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/wnbttataod1n3nvv6mcf 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/wo6tpndv1zavub5aavct 570w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/qcnxfcxtk7liut0tyrhs 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/rbjpu5sjltr0jp6pzqdv 950w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/bfqee1wrmqsyni6elarr 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/pimbgv7afprpashwdgz4 1330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/kvaxhxn575u3dcuxylqt 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/vx7hayudvob2pga47umj 1710w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/peGPR4XcSP6EBrPhc/ro28i0lrz69is2fij515 1838w\"></figure>", "parentCommentId": "EfYZzHGT5SH7hMaXy", "user": {"username": "JoelMcGuire"}}, {"_id": "tsNQB4jbFaMMkP3D9", "postedAt": "2023-11-28T19:29:13.381Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Thanks yeah that's a great graphic. Do they include government salaries and other NGO costs as part of their costs too?</p>\n", "parentCommentId": "peGPR4XcSP6EBrPhc", "user": {"username": "NickLaing"}}, {"_id": "gzKimGjvXnE7hppac", "postedAt": "2023-11-28T20:40:05.418Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>They only include costs to the legal entity of StrongMinds. To my understanding, this includes a relatively generous stipend they provide to the community health workers and teachers that are \"volunteering\" to provide StrongMinds or grants StrongMinds makes to NGOs to support their delivery of StrongMinds programs.&nbsp;</p><p>Note that 61% of their partnership treatments are through these volunteer+ arrangements with community health workers and teachers. I'm not too worried about this since I'm pretty sure there aren't meaningful additional costs to consider. these partnership treatments appear to be based on individual CHWs and teachers opting in. I also don't think that the delivery of psychotherapy is meaningfully leading them to do less of their core health or educational work.</p><p>&nbsp;I'd be more concerned if these treatments were happening because a higher authority (say school administrators) was saying \"Instead of teaching, you'll be delivering therapy\". The costs to deliver therapy could then reasonably seen to include the teacher's time and the decrease in teaching they'd do.&nbsp;</p><p>But what about the remaining 39% of partnerships (representing 24% of total treatments)? These are through NGOs. I think that 40% of these are delivered because StrongMinds is giving grants to these NGOs to deliver therapy in areas that StrongMinds can't reach for various reasons. The other 60% of NGO cases appears to be instances where the NGO is paying StrongMinds to train it to deliver psychotherapy. The case for causally attributing these cases of treatment to StrongMinds seems more dubious here, and I haven't gotten all the information I'd like, so to be conservative<strong> I assumed that none of these cases StrongMinds claims as its are attributable to it. </strong>This increases the costs by around 14%<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftcv6o9fdbuf\"><sup><a href=\"#fntcv6o9fdbuf\">[1]</a></sup></span>&nbsp;because it's reducing the total number treated by around 14%.</p><p>Some preemptive hedging: I think my approach so far is reasonable, but my world wouldn't be rocked if I was later convinced this isn't quite the way to think about incorporating costs in a situation with more decentralized delivery and more unclear causal attribution for treatment.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntcv6o9fdbuf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftcv6o9fdbuf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>But 1.14 &nbsp;*<i> 59 is 67</i> not 63! Indeed. The cost we report is lower than $67 because we include an offsetting 7.4% discount to the costs to harmonize the cost figures of StrongMinds (which are more stringent about who counts as treated -- more than half of sessions completed are required) with Friendship Bench (who count anyone as treated as receiving at least 1 session). So 59 *<i> (1 - 0.074)&nbsp;</i> * 1.14 is $63. See page 69 of the report for the section where we discuss this.&nbsp;</p></div></li></ol>", "parentCommentId": "tsNQB4jbFaMMkP3D9", "user": {"username": "JoelMcGuire"}}, {"_id": "eh7F6DF5fPF6DTKwJ", "postedAt": "2023-12-03T20:44:06.249Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>HLI kindly provided me with an earlier draft of this work to review a couple of weeks ago. Although things have gotten better, I noted what I saw as major problems with the draft as-is, and recommended HLI take its time to fix them - even though this would take a while, and likely miss the window of Giving Tuesday.&nbsp;</p><p>Unfortunately, HLI went ahead anyway with the problems I identified basically unaddressed. Also unfortunately (notwithstanding laudable improvements elsewhere) these problems are sufficiently major I think potential donors are ill-advised to follow the recommendations and analysis in this report.</p><p>In essence:</p><ol><li>Issues of study quality loom large over this literature, with a high risk of materially undercutting the results (they did last time). The reports interim attempts to manage these problems are inadequate.</li><li>Pub bias corrections are relatively mild, but only when all effects g &gt; 2 are excluded from the analysis - they are much starker (albeit weird) if all data is included. Due to this, the choice to exclude 'outliers' roughly trebles the bottom line efficacy of PT. This analysis choice is dubious on its own merits, was not pre-specified in the protocol, yet is only found in the appendix rather than the sensitivity analysis in the main report.&nbsp;</li><li>The bayesian analysis completely stacks the deck in favour of psychotherapy interventions (i.e. an 'informed prior' which asserts one should be &gt; 99% confident strongminds is more effective than givedirectly before any data on strongminds is contemplated), such that psychotherapy/strongminds/etc, getting recommended is essentially foreordained.&nbsp;</li></ol><p><strong>Study quality</strong></p><p>It perhaps comes as little surprise that different studies on psychotherapy in LMICs report very different results:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyg9n5eo4ek\"><sup><a href=\"#fnyg9n5eo4ek\">[1]</a></sup></span></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/vp0yvyboxebnqzuqnbqj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/w8aafwidzmjjxqqhewbo 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/fojm5ep5vz5lzh9ejvdh 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/oxqqmzfdfz5xr3oim6aw 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/iy07grideqtwkbq1x1ms 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/ycny8xmmonmkx1pnbqst 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/zyzpgg0hholntdoa3rlz 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/lglxmsvfl6lhyfwb5zbi 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/vuq8dpby8dfnz2fdc1qd 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/oj3z79lmettcj7orku6l 806w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/naq1cq489weatp4ggsde 810w\"></figure><p>The x-axis is a standardized measure of effect size for psychotherapy in terms of wellbeing.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3uxi8eulea1\"><sup><a href=\"#fn3uxi8eulea1\">[2]</a></sup></span>&nbsp;Most - but not all - show a positive effect (g &gt; 0), but the range is vast, HLI excludes effect sizes over 2 as outliers (much more later), but 2 is already a large effect: to benchmark, it is roughly the height difference between male and female populations.</p><p>Something like an '(weighted) average effect size' across this set would look promising (~0.6) - to also benchmark, the effect size of cash transfers on (individual) wellbeing is ~0.1. Yet cash transfers (among many other interventions) have much less heterogeneous results: more like \"0.1 +/- 0.1\", not ~\"0.6 multiply-or-divide by an integer\". It seems important to understand what is going on. &nbsp;&nbsp;</p><p>One hope would be this heterogeneity can be explained in terms of the intervention and length of follow-up. Different studies did (e.g.) different sorts of psychotherapy, did more or less of it, and measured the outcomes at different points afterwards. Once we factor these things in to our analysis, this wide distribution seen when looking at the impact of psychotherapy <i>in general</i> sharpens into a clearer picture for any <i>particular </i>psychotherapeutic intervention. One can then deploy this knowledge to assess - in particular - the likely efficacy of a charity like Strongminds.</p><p>The report attempts this enterprise in section 4 of the report. I think a fair bottom line is despite these efforts, the overall picture is still very cloudy: the best model explains ~12% of the variance in effect sizes. But this best model is still better than no model (but more later), so one can still use it to make a best guess for psychotherapeutic interventions, even if there remains a lot of uncertainty and spread.</p><p>But there could be another explanation for why there's so much heterogeneity: there are a lot of low-quality studies, and low quality studies tend to report inflated effect sizes. In the worst case, the spread of data suggesting psychotherapy's efficacy is instead a mirage, and the effect size melts under proper scrutiny.&nbsp;</p><p>Hence why most systematic reviews do assess the quality of included studies and their risk of bias. Sometimes this is only used to give a mostly qualitative picture alongside the evidence synthesis (e.g. 'X% of our studies have a moderate to high risk of bias') or sometimes incorporated quantitatively (e.g. 'quality score' of studies included as a predictor/moderator, grouping by 'high/moderate/low' risk of bias, etc. - although all are controversial).&nbsp;</p><p>HLI's report does not assess the quality of its included studies, although it plans to do so. I appreciate GRADEing 90 studies or whatever is tedious and time consuming, but skipping this step to crack on with the quantitative synthesis is very unwise:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3qgqksl5qa5\"><sup><a href=\"#fn3qgqksl5qa5\">[3]</a></sup></span>&nbsp;any such synthesis could be hugely distorted by low quality studies. And it's not like this is a mere possibility: I previously <a href=\"https://forum.effectivealtruism.org/posts/g4QWGj3JFLiKRyxZe/the-happier-lives-institute-is-funding-constrained-and-needs?commentId=CQqmoKc8rbkaxpuGY\">demonstrated</a> in the previous meta-analysis that study registration status (<i>one</i> indicator of study quality) explained a lot of heterogeneity, and unregistered studies had on average a three times [!] greater effect size than registered ones.&nbsp;</p><p>The report notes it has done some things to help manage this risk. One is cutting 'outliers' (g &gt; 2, the teal in the earlier histogram), and extensive assessment of publication bias/small study effects. These things do help: all else equal, I'd expect bigger studies to be methodologically better ones, so adjusting for small study effects does partially 'control' for study quality; I'd also expect larger effect sizes to arise from lower-quality work, so cutting them should notch up the average quality of the studies that remain.&nbsp;</p><p>But I do not think they help enough<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6v5892eo72i\"><sup><a href=\"#fn6v5892eo72i\">[4]</a></sup></span>&nbsp;- these are loose proxies for what we seek to understand. Thus the findings would be unreliable in virtue of this alone until after this is properly looked at. Crucially, the risk that these features could <i>confound</i> the earlier moderator analysis has not been addressed:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref481qv5d8an\"><sup><a href=\"#fn481qv5d8an\">[5]</a></sup></span>&nbsp;maybe the relationship of (e.g.) 'more sessions given -&gt; greater effect' is actually due to studies of such interventions tend to be lower quality than the rest. When I looked last time things like 'study size' or 'registration status' explained a lot <i>more</i> of the heterogeneity than (e.g.) all of the intervention moderators combined. I suspect the same will be true this time too.</p><p><strong>Publication bias</strong></p><p>I originally suggested (6m ago?) that correction for publication bias/small study effects could be ~an integer division, so I am surprised the correction was a bit less: ~30%. Here's the funnel plot:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0ecarb4f6ya\"><sup><a href=\"#fn0ecarb4f6ya\">[6]</a></sup></span></p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/bdyrjjdbpoh19ehjzxru\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/siyubhzpuzblxu2tjyqn 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/clh63yiyvijr6srharf6 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/ucvxreszfaowho0jukab 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/fzwrfzw6dey9rermj1f5 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/plg8tb0pfffuz3ouki54 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/swdwrnywspdcl52j33di 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/t3hvjoc5klscmjbhux00 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/kof3xqomqkuaam3lpzyt 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/bfaxrcz2co5nyqdqwsuk 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/hvt49rqqz75qscwtpbvb 844w\"></figure><p>Unsurprisingly, huge amounts of scatter, but the asymmetry, although there, does not leap off the page: the envelope of points is pretty rectangular, but you can persuade yourself it's a bit of a parallelogram, and there's denser part of it which indeed has a trend going down and to the right (so smaller study -&gt; bigger effect).&nbsp;</p><p>But this only plots effect sizes g &lt; 2 (those red, not teal, in the histogram). If we include all the studies again, this picture looks a lot clearer - the 'long tail' of higher effects tends to come from smaller studies, which are clearly asymmetric.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/loxbboxfix4tz1stvguw\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/c4o0wtibvtv3iq8fgani 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/tcjghhfsskkxu5ynstpc 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/okud64mkv5wrxizeozs3 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/gkm2db4zkcg1snqgjobl 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/dnp2yuklt5svgckflxpv 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/quop9p9mylavir9efrhf 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/xlvofhyw4pipkvtqkung 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/c5dhf7ikn8n0djx4kq0b 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/l2vxv6061hzcsn34gmkg 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/jwpox4tdvs8bzt9swjgf 1222w\"></figure><p>This effect, visible to the naked eye, also emerges in the statistics. The report uses a variety of numerical methods to correct for publication bias (some very sophisticated). All of them adjust the results much further downwards on the full data than when outliers are excluded to varying degrees (table B1, <a href=\"https://docs.google.com/document/d/1U-cWku3tV6HAYbdehM6O7ixfdcqMn06709VSvzIAIlI/\">appendix</a>). It would have a stark effect on the results - here's the 'bottom line' result if you take a weighted average of all the different methods, with different approaches to outlier exclusion - red is the full data, green is the outlier exclusion method the report uses.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/thnaf3fshvp9x2slgufl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/aj4189yt3g6ec9bxtsh5 120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/o00e9dhpzridrz3zq4l6 240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/xkewnfppppwqbaehoq4f 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/qwddv9g6krh89lbriivt 480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/xrzkwjg2yzne65a6ebqf 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/xnijqvxa5jfhw6lcqfoe 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/rrxmoexnafflf50gd23a 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/h3hrigqutuholrqlqn4e 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/oghs7opwocjjrlyfnab1 1080w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/hibqs6m5kwi5vnpnff9d 1198w\"></figure><p>Needless to say, this choice is highly material to the bottom line results: without excluding data, SM drops from ~3.6x GD to ~1.1x GD. Yet it doesn't get a look in for the sensitivity analysis, where HLI's 'less favourable' outlier method involves taking an average of the other methods (discounting by ~10%), but not doing no outlier exclusion at all (discounting by ~70%).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5jch5sovqki\"><sup><a href=\"#fn5jch5sovqki\">[7]</a></sup></span>&nbsp;</p><p>Perhaps this is fine if outlier <i>inclusion</i> would be clearly unreasonable. But it's not: cutting data is generally regarded as dubious, and the rationale for doing so here is not compelling. Briefly:</p><ul><li>Received opinion is typically that outlier exclusion should be avoided without a clear rationale why the 'outliers' arise from a clearly discrepant generating process. If it is to be done, the results of the full data should still be presented as the primary analysis (<a href=\"https://stats.stackexchange.com/questions/200534/is-it-ok-to-remove-outliers-from-data\">e.g.</a>).</li><li>The cut data by and large doesn't look visually 'outlying' to me. The histogram shows a pretty smooth albeit skewed distribution. Cutting off the tail of the distribution at various lengths appears ill-motivated.</li><li>Given the interest in assessing small study effects, cutting out the largest effects (which also tend to be the smallest studies) should be expected to attenuate the small study effect (as indeed it does). Yet if our working hypothesis is these effects are large mainly <i>because </i>the studies are small, their datapoints are informative to plot this general trend (e.g. for slightly less small studies which have slightly less inflated results).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref57euz6ojutx\"><sup><a href=\"#fn57euz6ojutx\">[8]</a></sup></span>&nbsp;</li></ul><p>The strongest argument given is that, in fact, some numerical methods to correct <i>publication bias</i> give absurd results if given the full data: i.e. one gives an adjusted effect size of -0.6, another -0.2. I could buy an adjustment that drives the effect down to roughly zero, but not one which suggests, despite almost all the data being fairly or very positive, we should conclude from these studies the real effect is actually (highly!) negative.</p><p>One could have a long argument on what the most appropriate response is: maybe just keep it, as the weighted average across methods is still sensible (albeit disappointing)? Maybe just drop those methods in particular and do an average of those giving sane answers on the full data? Should we keep g &lt; 2 exclusion but drop p-curve analysis, as it (absurdly?) adjusts the effect slightly <i>upwards</i>? Maybe we should reweigh the averaging of different numerical methods by how volatile their results are when you start excluding data? Maybe pick the outlier exclusion threshold which results in the least disagreement between the different methods? Or maybe just abandon numerical correction, and just say \"there's clear evidence of significant small study effects, which the current state of the art cannot reliably quantify and correct\"?</p><p>So a garden of forking paths opens before us. All of these are varying degrees of 'arguable', and they do shift the bottom line substantially. One reason pre-specification is so valuable is it ties you to a particular path before getting to peek at the results, avoiding any risk a subconscious finger on the scale to push one down a path of still-defensible choices nonetheless favour a particular bottom line. Even if you didn't pre-specify, presenting your <i>first</i> cut as the primary analysis helps for nothing up my sleeve reasons.&nbsp;</p><p>It may be the prespecified or initial stab doesn't do a good job of expressing the data, and a different approach does better. Yet making it clear this subsequent analysis is post-hoc cautions a reader about potential risk of bias in analysis.</p><p>Happily, HLI did make a <a href=\"https://docs.google.com/document/d/15zpjvoB5CPa6yMX37CwA1yJyb9BDs6cVgesGOMwO_bA\">protocol</a> for this work, made before they conducted the analysis. Unfortunately, it is silent on whether outlying data would be excluded, or by what criteria. Also unfortunately, because of this (and other things like the extensive discussion in the appendix discussing the value of outlier removal principally in virtue of its impact on publication bias correction), I am fairly sure the analysis with all data included was the first analysis conducted. Only after seeing the initial publication bias corrections did HLI look at the question of whether some data should be excluded. Maybe it should, but if it came second the initial analysis should be presented first (and definitely included in the sensitivity analysis).&nbsp;</p><p>There's also a risk the cloud of quantification buries the qualitative lede. Publication bias is known to be very hard to correct, and despite HLI compiling multiple numerical state of the art methods, they starkly disagree on what the correction factor should be (i.e. from &lt;~0 to &gt; 100%). So perhaps the right answer is we basically do not know how much to discount the apparent effects seen in the PT literature given it also appears to be an extremely compromised one, and if forced to give an overall number, any 'numerical bottom line' should have even wider error bars because of this.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyy2p2l71dkm\"><sup><a href=\"#fnyy2p2l71dkm\">[9]</a></sup></span></p><p><strong>Bayesian methods</strong></p><p>I previously complained that the guestimate/BOTEC-y approach HLI used in integrating information from the meta-analysis and the strongminds trial data couldn't be right, as it didn't pass various sanity tests: e.g. still recommending SM as highly effective even if you set the trial data to zero effect. HLI now has a much cleverer Bayesian approach to combining sources of information. On the bright side, this is mechanistically much clearer as well as much cleverer. On the downside, the deck &nbsp;still looks pretty stacked.</p><p>Starting at the bottom, here's how HLI's Bayesian method compares SM to GD:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/p9mfm5quxnszlsrjegi1\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/q2dkc4uanli888omtjsw 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/rrasc4akqtnhvzoic6h7 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/aj4dwlplupf0yswpkcpe 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/zvvghmek4kzsma5pglf9 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/otu0dfablb61w4dimhc8 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/xxshx8ydcuoq4uqkodrk 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/ifjxaxvcn5z7csacmnld 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/hofcba7c3ig9rpdwhysu 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/hskawgzpykdlxzxw0qwh 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/m8a3mltiazqan0njhdeg 1654w\"></p><p>The informed prior (in essence) uses the meta-analysis findings with some monte carlo to get an expected effect for an intervention with strongminds-like traits (e.g. same number of sessions, same deliverer, etc.). The leftmost point of the solid line gives the expectation for the prior: so the prior is that SM is ~4x GDs cost effectiveness (dashed line).</p><p>The x axis is how much weight one gives to SM-specific data. Of interest, the line slopes <i>down, </i>so the data gives a negative update on SMs cost-effectiveness. This is because HLI - in anticipation of the Baird/Ozler RCT likely showing disappointing results - discounted the effect derived from the original SM-specific evidence by a factor of 20, so the likelihood is indeed much lower than the prior. Standard theory gives the appropriate weighting of this vs. the prior, so you adjust down a bit, but not a lot, from the prior (dotted line).&nbsp;</p><p>Despite impeccable methods, these results are facially crazy. To illustrate:</p><ul><li>The rightmost point on the solid line is the result if you completely discount the prior, and only use the stipulated-to-be-bad SM-specific results. SM is <i>still</i> slightly better than GD on this analysis.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0fwmekhrkz7\"><sup><a href=\"#fn0fwmekhrkz7\">[10]</a></sup></span></li><li>If we 'follow Bayesian updating' as HLI recommends, the recommendation is surprisingly insensitive to the forthcoming Baird/Ozler RCT having disappointing findings. Eyeballing it, you'd need such a result to be replicated <i>half a dozen times</i> for the posterior to update to SM is roughly on a par with GD.</li><li>Setting the forthcoming data to basically showing <i>zero</i> effect will still return SM is 2-3x GD.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9cswsk19hyb\"><sup><a href=\"#fn9cswsk19hyb\">[11]</a></sup></span>&nbsp;I'd guess you'd need the forthcoming RCT to show <i>astonishingly and absurdly negative</i> results (e.g. SM treatment is worse for your wellbeing than bereavement), to get it to approximate equipoise with GD.</li><li>You'd need even stronger adverse findings for the model to update all the way down to SM being ineffectual, rather than merely 'less good than GiveDirectly'.</li></ul><p>I take it most readers would disagree with the model here too - e.g. if indeed the only RCT on strongminds is basically flat, that should be sufficient demote SM from putative 'top charity' status.&nbsp;</p><p>I think I can diagnose the underlying problem: Bayesian methods are very sensitive to the stipulated prior. In this case, the prior is likely too high, and definitely too narrow/overconfident. See this:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/kxsgv7bnn9u4cmka1mgq\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/hfvsprkdliuqmw8rhkpd 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/t7mham4pnujnyqiopqv8 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/qmzijmcwz8myh4usrzmn 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/tl3pdfcbpqvroszojas2 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/kzk1la5zfgw1ebajvaoo 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/bqy7f20wztjuqqagn4cn 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/swktbnprtopiwpuajipi 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/xulhb2tir4m2sxp3kqyv 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/mxn0mliv0md2zrnn0rmc 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/j9gd6q8xbl3kf9bkuxha 1614w\"></figure><p>Per the dashed and dotted lines in the previous figure, the 'GiveDirectly bar' is fractionally below at the blue dashed line (the point estimate of the stipulated-SM data). The prior distribution is given in red. So the expectation (red dashed line) is indeed ~4x further from the origin (see above).</p><p>The solid red curve gives the distribution. Eyeballing the integrals reveals the problem: the integral of this distribution from the blue dashed line to infinity gives the models confidence psychotherapy interventions would be more cost-effective than GD. This is at least 99% of the area, if not 99.9% - 99.99%+. A fortiori, this prior asserts it is essentially certain the intervention is beneficial (total effect &gt;0).</p><p>I don't think anyone should think that <i>any </i>intervention is P &gt; 0.99 more cost-effective than give directly (or P &lt; 0.0001 or whatever it is in fact harmful) as a <i>prior</i>,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8iz4dfaqdna\"><sup><a href=\"#fn8iz4dfaqdna\">[12]</a></sup></span>&nbsp;but if one did, it would indeed take masses of evidence to change one's mind. Hence the very sluggish moves in response to adverse data (the purple line suggests the posterior is also 99%+ confident SM is better than givedirectly).</p><p>I think I can also explain the underlying problem of this underlying problem. HLI constructs its priors exclusively from its primary meta-analytic model (albeit adapted to match the intervention of interest, and recalculated excluding any studies done on this intervention to avoid double counting). Besides the extra uncertainty (so spread) likely implied by variety of factors covered in the sensitivity analysis, in real life our prior would be informed by other things too: the prospect entire literatures can be misguided, a general sense (at least for me) that cash transfers are easy to beat in principle, but much harder in practice, and so on.&nbsp;</p><p>In reality, our prior-to-seeing-the-metaanalysis prior would be very broad and probably reasonably pessimistic, and (even if I'm wrong about the shortcomings I suggest earlier), the 'update' on reading it would be a bit upwards, and a little narrower, but not by that much. In turn, the 'update' on seeing (e.g.) disappointing RCT results for a given PT intervention would be a larger shift downwards, netting out that this was unlikely better than GiveDirectly after all.&nbsp;</p><p>If the Bayesian update was meant only to be a neat illustration, I would have no complaint. But instead the bottom line recommendations and assessments rely upon it - that readers should indeed adopt the supposed prior the report proposes about the efficacy of PT interventions in general. Crisply, I doubt the typical reader seriously believes (e.g.) basically any psychotherapy intervention in LMICs, so long as cost per patient is &lt;$100, is a ~certain bet to beat cash transfers. If not, they should question the report's recommendations too.</p><p><strong>Summing up</strong></p><p>Criticising is easier than doing better. But I think this is a case where a basic qualitative description tells the appropriate story, the sophisticated numerical methods are essentially a 'bridge too far' given the low quality of what they have to work with, and so confuse rather than clarify the matter. In essence:</p><ol><li>The literature on PT in LMICs is a complete mess. Insofar as more sense can be made from it, the most important factors appear to belong to the studies investigating it (e.g. their size) rather than qualities of the PT interventions themselves.</li><li>Trying to correct the results of a compromised literature is known to be a nightmare. Here, the qualitative evidence for publication bias is compelling. But quantifying what particular value of 'a lot?' the correction should be is fraught: numerically, methods here disagree with one another dramatically, and prove highly sensitive to choices on data exclusion.</li><li>Regardless of how PT looks in general, Strongminds, in particular, is looking less and less promising. Although initial studies looked good, they had various methodological weaknesses, and a forthcoming RCT with much higher methodological quality is expected to deliver disappointing results.&nbsp;</li><li>The evidential trajectory here is all to common, and the outlook typically bleak. It is dubious StrongMinds is a good pick even among psychotherapy interventions (picking one at random which <i>doesn't</i> have a likely-bad-news RCT imminent seems a better bet). Although pricing different interventions is hard, it is even more dubious SM is close to the frontier of \"very well evidenced\" vs. \"has very promising results\" plotted out by things like AMF, GD, etc. HLI's choice to nonetheless recommend SM again this giving season is very surprising. I doubt it will weather hindsight well.</li></ol><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyg9n5eo4ek\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyg9n5eo4ek\">^</a></strong></sup></span><div class=\"footnote-content\"><p>All of the figures are taken from the report and appendix. The transparency is praiseworthy, although it is a pity despite largely looking at the right things the report often mistakes the right conclusions to draw.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3uxi8eulea1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3uxi8eulea1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>With all the well-worn caveats about measuring well-being.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3qgqksl5qa5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3qgqksl5qa5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The Cochrane handbook section on meta-analysis is very clear on this (but to make it clearer, I add emphasis)</p><blockquote><p><strong>10.1 Do not start here!</strong></p><p><i>It can be tempting to jump prematurely into a statistical analysis when undertaking a systematic review.</i> The production of a diamond at the bottom of a plot is an exciting moment for many authors, <i>but results of meta-analyses can be very misleading</i> if suitable attention has not been given to formulating the review question; specifying eligibility criteria; identifying and selecting studies; collecting appropriate data; <i>considering risk of bias</i>; planning intervention comparisons; <i>and deciding what data would be meaningful to analyse</i>. Review authors should consult the chapters that precede this one before a meta-analysis is undertaken.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6v5892eo72i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6v5892eo72i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As a WIP, the data and code for this report is not yet out, but in my previous statistical noodling on the last one both study size and registration status significantly moderated the effect downwards when included together, suggesting indeed the former isn't telling you everything re. study quality.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn481qv5d8an\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref481qv5d8an\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The report does mention later (S10.2) controlling a different analysis for study quality, when looking at the effect of sample size itself:</p><blockquote><p>To test for scaling effects, we add sample size as a moderator into our meta-analysis and find that for every extra 1,000 participants in a study the effect size decreases (non-significantly) by -0.09 (95% CI: -0.206, 0.002) SDs. Naively, this suggests that deploying psychotherapy at scale means its effect will substantially decline. However, when we control for study characteristics and quality, the coefficient for sample size decreases by 45% to -0.055 SDs (95% CI: -0.18, 0.07) per 1,000 increase in sample size. This suggests to us that, beyond this finding being non-significant, the effect of scaling can be controlled away with quality variables, more of which that we haven\u2019t considered might be included.</p></blockquote><p>I don't think this analysis is included in the appendix or similar, but later text suggests the 'study quality' correction is a publication bias adjustment. This analysis is least fruitful when applied to study scale, as measures of publication bias are measures of study size: so finding the effects of study scale are attenuated when you control for a proxy of study scale is uninformative.</p><p>What would be informative is the impact measures of 'study scale' or publication bias have on the coefficients for the primary moderators. Maybe they too could end up 'controlled away with quality variables, more of which that we haven't considered might be included'?</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0ecarb4f6ya\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0ecarb4f6ya\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There are likely better explanations of funnel plots etc. online, but my own attempt is <a href=\"https://forum.effectivealtruism.org/posts/g4QWGj3JFLiKRyxZe/the-happier-lives-institute-is-funding-constrained-and-needs?commentId=fSLuCFMhokPB447u7\">here</a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5jch5sovqki\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5jch5sovqki\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The report charts a much wiser course on a different \"Outlier?\" question: whether to include very long follow-up studies, where <i>exclusion</i> would cut the total effect in half. I also think including everything here is fine too, but the report's discussion in S4.2 clearly articulates the reason for concern, displays what impact inclusion vs. exclusion has, and carefully interrogates the outlying studies to see whether they have features (beyond that they report 'outlying' results) which warrants exclusion. They end up going 'half-and-half', but consider both full exclusion and inclusion in sensitivity analysis.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn57euz6ojutx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref57euz6ojutx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If you are using study size as an (improvised) measure of study quality, excluding the smallest studies because on an informal read they are particularly low quality makes little sense: this is the trend you are interested in.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyy2p2l71dkm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyy2p2l71dkm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A similar type of problem crops up when one is looking at the effect of 'dosage' on PT efficacy.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/q5vrno0xcv3uptlbnloe\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/s9momsatqt2bt6kfxhhi 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/vpp67qbbmdqzk2p0dadq 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/mi6zgobaxxfz9zke8ner 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/vmjg7ayrni3sc9apkean 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/w6bgopd1r8tu2rykeybt 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/zoezfcpeakn7g02xnjoq 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/l5okity8f5jpwv2jjrma 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/wyaztmmrezfsmkhvphyv 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/q1jvmz601sm3f0wqp91m 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eh7F6DF5fPF6DTKwJ/mamonow70ydqnmranpin 1670w\"></figure><p>The solid lines are the fit (blue linear, orange log) on the full data, whilst the dashed lines are fits with extreme values of dosage - small or large - excluded (purple). The report freely concedes its choices here are very theory led rather than data driven - and also worth saying getting <i>more</i> of a trend here makes a rod for SM and Friendship Bench's back, as these deliver smaller numbers of sessions than the average, so adjusting with the dashed lines and not the solid ones reduces the expected effect.</p><p>Yet the main message I would take from the scatter plot is the data indeed looks very flat, and there is no demonstrable dose-response relationship of PT. Qualitatively, this isn't great for face validity.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0fwmekhrkz7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0fwmekhrkz7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To its credit, the write-up does highlight this, but does not seem to appreciate the implications are crazy: any PT intervention, so long as it is cheap enough, should be thought better than GD, even if studies upon it show very low effect size (which would usually be reported as a negative result, as almost any study in this field would be underpowered to detect effects as low as are being stipulated):</p><blockquote><p>Therefore, even if the StrongMinds-specific evidence finds a small total recipient effect (as we present here as a placeholder), and we relied solely on this evidence, then it would still result in a cost-effectiveness that is similar or greater than that of GiveDirectly because StrongMinds programme is very cheap to deliver.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9cswsk19hyb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9cswsk19hyb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The report describes this clearly itself, but seems to think this is a feature rather than a bug (my emphasis):</p><blockquote><p>Now, one might argue that the results of the Baird et al. study could be lower than 0.4 WELLBYs. But \u2013 assuming the same weights are given to the prior and the charity-specific data as in our analysis - <i>even if the Baird et al. results were 0.05 WELLBYs (extremely small), then the posterior would still be 1.49 * 0.84 + 0.05 * 0.16 = 1.26 WELLBYs</i>; namely, very close to our current posterior (1.31 WELLBYs).&nbsp;</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8iz4dfaqdna\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8iz4dfaqdna\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm not even sure that \"P &gt; 0.99 better than GD\" would be warranted as posterior even for a Givewell recommended top charity, and I'd guess the GW staff who made the recommendation would often agree.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Gregory_Lewis"}}, {"_id": "nWE3sSwDM6pTpDPsA", "postedAt": "2023-12-04T04:28:27.702Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>I think I agree with the rest of this analysis (or at least the parts I could understand). However, the following paragraph seems off:</p><blockquote><p>To its credit, the write-up does highlight this, but does not seem to appreciate the implications are crazy: any PT intervention, <i>so long as it is cheap enough</i>, should be thought better than GD, even if studies upon it show very low effect size</p></blockquote><p>Apologies if I'm being naive here, but isn't this just a known problem of first-order <i>cost-effectiveness analysis</i>, not with this particular analysis per se? I mean, since cheapness could be arbitrarily low (or at least down to $0.01), \"better than GD\" is a bit of a red herring, the claim is merely that a single (even high-quality) study is not enough for someone to update their prior all the way down to zero, or negative.&nbsp;</p><p>And stated in English, this seems eminently reasonable to me. There might be good second-order etc reasons to not act on a naive first-order analysis (eg risk/ambiguity aversion, wanting to promote better studies, etc). But ultimately I don't think that literal claim is crazy to me, and naively seems like something that naturally falls out of a direct cost-effectiveness framework.</p>", "parentCommentId": "eh7F6DF5fPF6DTKwJ", "user": {"username": "Linch"}}, {"_id": "esgP2rtmPLhb68mb5", "postedAt": "2023-12-04T15:20:41.752Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<blockquote><p>I think I can diagnose the underlying problem: Bayesian methods are very sensitive to the stipulated prior. In this case, the prior is likely too high, and definitely too narrow/overconfident.</p></blockquote><p>Would it have been better to start with a stipulated prior based on evidence of short-course general-purpose<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefulv87oj6nel\"><sup><a href=\"#fnulv87oj6nel\">[1]</a></sup></span>&nbsp;psychotherapy's effect size generally, update that prior based on the LMIC data, and then update <i>that </i>on charity-specific data?</p><p>One of the objections to HLI's earlier analysis was that it was just implausible in light of what we know of psychotherapy's effectiveness more generally. I don't know that literature well at all, so I don't know how well the effect size in the new stipulated prior compares to the effect size for short-course general-purpose psychotherapy generally. However, given the methodological challenges with measuring effect size in LMICs on available data, it seems like a more general understanding of the effect size should factor into the informed prior somehow. Of course, the LMIC context is considerably different than the context in which most psychotherapy studies have been done, but I am guessing it would be easier to manage quality-control issues with the much broader research base available. So both knowledge bases would likely inform my prior before turning to charity-specific evidence.</p><p><i>[Edit 6-Dec-23: Greg's response to the remainder of this comment is <strong>much </strong>better than my musings below. I'd suggest reading that instead!]</i></p><p>To my not-very-well-trained eyes, one hint to me that there's an issue with application of Bayesian analysis here is the failure of the LMIC effect-size model to come anywhere close to predicting the effect size suggested by the SM-specific evidence. If the model were sound, it would seem very unlikely that the first organization evaluated to the medium-to-in-depth level would happen to have charity-specific evidence suggesting an effect size that diverged so strongly from what the model predicted. I think most of us, when faced with such a circumstance, would question whether the model was sound and would put it on the shelf until performing other charity-specific evaluations at the medium-to-in-depth level. That would be particularly true to the extent the model's output depended significantly on the methodology used to clean up some problems with the data.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefygfmf3mbntr\"><sup><a href=\"#fnygfmf3mbntr\">[2]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnulv87oj6nel\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefulv87oj6nel\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By which I mean <i>not</i> psychotherapy for certain narrow problems (e.g., CBT-I for insomnia, exposure therapy for phobias).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnygfmf3mbntr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefygfmf3mbntr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If Greg's analysis is correct, it seems I shouldn't assign the informed prior much more credence than I have credence in HLI's decision to remove outliers (and to a lesser extent, its choice of a method). So, again to my layperson way of thinking, one partial way of thinking about the crux could be that the reader must assess their confidence in HLI's outlier-treatment decision vs. their confidence in the Baird/Ozler RCT on SM.</p></div></li></ol>", "parentCommentId": "eh7F6DF5fPF6DTKwJ", "user": {"username": "Jason"}}, {"_id": "tthdc2ZnjAc7npuLH", "postedAt": "2023-12-05T14:41:14.702Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Thank you for your comments, Gregory. We\u2019re aware you have strong views on the subject and we appreciate your conscientious contributions. We discussed your previous comments internally but largely concluded revisions weren\u2019t necessary as we (a) had already considered them in the report and appendix, (b) will return to them in later versions and didn\u2019t expect they would materially affect the results, or (c) simply don\u2019t agree with these views. To unpack:</p><ol><li><strong>Study quality</strong>. We conclude the data set does contain bias, but we account for it (<a href=\"https://www.happierlivesinstitute.org/report/talking-through-depression-the-cost-effectiveness-of-psychotherapy-in-lmics-revised-and-expanded/\"><u>sections 3.2 and 5</u></a>; it\u2019s an open question among academics how best to do this). We don\u2019t believe that the entire field of LMIC psychotherapy should be considered bunk, compromised, or uninformative. Our results are in line with existing meta-analyses of psychotherapy considered to have low risk of bias (see footnote).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2jfwcd8s09p\"><sup><a href=\"#fn2jfwcd8s09p\">[1]</a></sup></span></li><li><strong>Evidentiary standards.</strong> We drew on a large number of RCTs for our systematic reviews and meta-analyses of cash transfers and psychotherapy (42 and 74, respectively). If one holds that the evidence for something as well-studied as psychotherapy is too weak to justify any recommendations, charity evaluators could recommend very little.</li><li><strong>Outlier exclusion.&nbsp;</strong>The issues regarding outlier exclusion were discussed in some depth (<a href=\"https://www.happierlivesinstitute.org/report/talking-through-depression-the-cost-effectiveness-of-psychotherapy-in-lmics-revised-and-expanded/\"><u>3.2 in the main report</u></a> and in&nbsp;<a href=\"https://docs.google.com/document/d/1U-cWku3tV6HAYbdehM6O7ixfdcqMn06709VSvzIAIlI/edit#heading=h.zb9pc84t777u\"><u>Appendix B</u></a>). Excluding outliers is thought sensible practice here; two related meta-analyses,&nbsp;<a href=\"https://www.tandfonline.com/doi/full/10.1080/10503307.2019.1649732\"><u>Cuijpers et al., 2020c</u></a>;&nbsp;<a href=\"https://www.cambridge.org/core/journals/psychological-medicine/article/psychotherapy-for-adult-depression-in-low-and-middleincome-countries-an-updated-systematic-review-and-metaanalysis/630D2D6E07018C9CA7A63FD27C1B0822\"><u>Tong et al., 2023</u></a>, used a similar approach. It\u2019s consistent with not taking the entire literature at face value but also not taking guilt by association too far. If one excludes outliers, the specific way one does this has a minor effect (e.g., a 10% decline in effectiveness, see appendix). Our analysis necessarily makes analytic choices: some were pre-registered, some made on reflection, many were discussed in our sensitivity analysis. If one insisted only on using charity evaluations that had every choice pre-registered, there would be none to choose from.</li><li><strong>Bayesian analysis</strong>: The method we use (\u2018grid approximation\u2019, see&nbsp;<a href=\"https://www.happierlivesinstitute.org/report/talking-through-depression-the-cost-effectiveness-of-psychotherapy-in-lmics-revised-and-expanded/\"><u>8.3 and 9.3</u></a>) avoids subjective inputs. It is not this Bayesian analysis itself that \u2018stacks the deck\u2019 in favour of psychotherapy, but the evidence. Given that over 70 studies form the prior, it would be surprising if adding one study, as we did for StrongMinds, would radically alter the conclusions. [Edit 5/12/2023: on the point that StrongMinds could be more cost-effective than GiveDirectly, even if StrongMinds only has the small effect we assume it does in our hypothetical placeholder studies, it doesn't seem inconceivable that a small, less effective intervention can still be more cost-effective than a big, expensive one. For context, we estimate it costs StrongMinds $63 per intervention - providing one person with a course of therapy - whereas it costs GiveDirectly $1221 per intervention - an $1000 cash transfer which costs $221 in overheads. As the therapy is about 20x cheaper, it can be far less <i>effective </i>yet still more <i>cost-effective</i>.]</li><li><strong>Making recommendations</strong>: we aim to recommend the most cost-effective ways of increasing WELLBYs we\u2019ve found so far. While we have intuitions about how good different interventions are our perspective as an organisation is that conclusions about what\u2019s cost-effective should be led heavily by the evidence rather than by our pre-evidential beliefs (\u2018priors\u2019). Given the evidence we\u2019ve considered, we don\u2019t see a strong case for recommending cash transfers over psychotherapy.&nbsp;&nbsp;</li></ol><p>This is a working report, and we\u2019ll be reflecting on how to incorporate the above, similarly psychotherapy-sceptical perspectives, and other views in the process of preparing it for academic review.&nbsp; In the interests of transparency, we don\u2019t plan to engage beyond our comments above so as to preserve team resources.<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2jfwcd8s09p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2jfwcd8s09p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We find an initial effect is 0.70 SDs, reduced to 0.46 SDs after publication bias adjustments. <a href=\"https://psycnet.apa.org/doiLanding?doi=10.1037%2Famp0001250\"><u>Cuijpers et al. 2023</u></a> find an effect of psychotherapy of 0.49 SDs for studies with low risk of bias (RoB) in low, middle, and high income countries (comparisons = 218), which reduces to between 0.27 and 0.57 after publication adjustment.&nbsp;<a href=\"https://www.cambridge.org/core/journals/psychological-medicine/article/psychotherapy-for-adult-depression-in-low-and-middleincome-countries-an-updated-systematic-review-and-metaanalysis/630D2D6E07018C9CA7A63FD27C1B0822\"><u>Tong et al. 2023</u></a> find an effect of 0.69 SDs for studies with low RoB in non-western countries (primarily low and middle income; comparisons = 36), which adjust to between 0.42 and 0.60 after publication correction. Hence, our initial and adjusted numbers are similar.</p></div></li></ol>", "parentCommentId": "eh7F6DF5fPF6DTKwJ", "user": {"username": "Happier Lives Institute"}}, {"_id": "pJWjZqBWLSaz8LGme", "postedAt": "2023-12-05T17:15:24.630Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>I wonder with these types of meta-analyses of relatively squishy subjects like \"happiness\" whether all the studies are in fact truly measuring the same outcome variable, as well as the same inputs. Man is not a machine, least of all the \"crooked timber\" of the mind.</p>", "parentCommentId": null, "user": {"username": "Locke"}}, {"_id": "CfDZCJeiEBdtdkFsN", "postedAt": "2023-12-05T19:43:52.750Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>You cannot use the distribution for the expected value of an average therapy treatment as the prior distribution for a SPECIFIC therapy treatment, as there will be a large amount of variation between possible therapy treatments that is missed when doing this. Your prior here is that there is a 99%+ chance that StrongMinds will work better than GiveDirectly before looking at any actual StrongMinds results, this is a wildly implausible claim.</p><p>You also state \"If one holds that the evidence for something as well-studied as psychotherapy is too weak to justify any recommendations, charity evaluators could recommend very little.\" Nothing in Gregory's post suggests that he thinks anything like this, he gives a g of ~0.5 in his meta-analysis that doesn't improperly remove outliers without good cause. A g of ~0.5 suggests that individuals suffering from depression would likely greatly benefit from seeking therapy. There is a massive difference between \"evidence behind psychotherapy is too weak to justify any recommendations\" and claiming that \"this particular form of therapy is not vastly better than GiveDirectly with a probability higher than 99% before even looking at RCT results\". Trying to throw out Gregory's claims here over a seemingly false statement about his beliefs seems pretty offensive to me.</p>", "parentCommentId": "tthdc2ZnjAc7npuLH", "user": {"username": "Burner1989"}}, {"_id": "H82ApbPtm8J9FEXse", "postedAt": "2023-12-06T00:51:29.043Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p><strong><u>Epistemic status</u>: tentative</strong>, it's been a long time since reading social science papers was a significant part of my life. Happy to edit/retract this preliminary view as appropriate if someone is able to identify mistakes.</p><blockquote><p>Excluding outliers is thought sensible practice here; two related meta-analyses, Cuijpers et al., 2020c; Tong et al., 2023, used a similar approach.&nbsp;</p></blockquote><p>I can't access Cuijpers et al., but I don't read Tong et al. as supporting what HLI has done here.&nbsp;</p><p>In their article, Tong et al. provide the effect size with no exclusions, then with outliers excluded, then with \"extreme outliers\" excluded (the latter of which seems to track HLI's removal criterion). They also provide effect size with various publication-bias measures employed. <i>See </i>PDF at 5-6. If I'm not mistaken, the publication bias measures are applied to the no-exclusions version, not a version with outliers removed or limited to those with lower RoB. <i>See id.</i> at 6 tbl.2 (n = 117 for combined and 2 of 3 publication-bias effect sizes; 153 with trim-and-fill adding 36 studies; n = 74 for outliers removed &amp; n = 104 for extreme outliers removed; effect sizes after publication-bias measures range from 0.42 to 0.60 seem to be those mentioned in HLI's footnote above).</p><p>Tong et al. \"conducted sensitive analyses comparing the results with and without the inclusion of extreme outliers,\" PDF at 5, discussing the results without exclusion first and then the results with exclusion. <i>See id. </i>at 5-6. Tables 3-5 are based on data without exclusion of extreme outliers; the versions of Tables 4 and 5 that excludes extreme outliers are relegated to the supplemental tables (not in PDF). <i>See id. </i>at 6. This reads to my eyes as treating both the all-inclusive and extreme-outliers-excluded data seriously, with some pride of place to the all-inclusive data.&nbsp;</p><p>I don't read Tong et al. as having reached a conclusion that either the all-inclusive or extreme-outliers-excluded results were more authoritative, saying things like:</p><p><i>Lastly, we were unable to explain the different findings in the analyses with vs. without extreme outliers. The full analyses that included extreme outliers may reflect the true differences in study characteristics, or they may imply the methodological issues raised by studies with effect sizes that were significantly higher than expected.</i></p><p>and</p><p><i>Therefore, the larger treatment effects observed in non-Western trials may not necessarily imply superior treatment outcomes. On the other hand, it could stem from variations in study design and quality.</i></p><p>and</p><p><i>Further research is required to explain the reasons for the differences in study design</i><br><i>and quality between Western and non-Western trials, as well as the different results in the analyses with and without extreme outliers.</i></p><p>PDF at 10.</p><p>Of course, \"further research needed\" is an almost inevitable conclusion of the majority of academic papers, and Tong et al. have the luxury of not needing to reach any conclusions to inform the recommended distribution of charitable dollars. But I don't read the article by Tong et al. as supporting the proposition that it is appropriate to just run with the outliers-excluded data. Rather, I read the article as suggesting that -- at least in the absence of compelling reasons to the contrary -- one should take both analyses seriously, but neither definitively.&nbsp;</p><p>I lack confidence in what taking both analyses seriously, but neither definitively would mean for purposes of conducting a cost-effectiveness analysis. But I speculate that it would likely involve some sort of weighting of the two views.</p>", "parentCommentId": "tthdc2ZnjAc7npuLH", "user": {"username": "Jason"}}, {"_id": "4BcNTYH6srkiwThQ3", "postedAt": "2023-12-06T01:08:48.480Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Agree that there seem to be some strawmen in HLI's response:</p><blockquote><p>We don\u2019t believe that the entire field of LMIC psychotherapy should be considered bunk, compromised, or uninformative.&nbsp;</p></blockquote><p>Has anyone suggested that the \"entire field of LMIC psychotherapy\" is \"bunk\"?</p><blockquote><p>&nbsp;If one insisted only on using charity evaluations that had every choice pre-registered, there would be none to choose from.</p></blockquote><p>Has anyone suggested that, either? As I understand it, it's typical to look at debatable choices that happen to support the author's position with a somewhat more skeptical lens if they haven't been pre-registered. I don't think anyone has claimed lack of certain choices being pre-registered is somehow fatal, only a factor to consider.</p>", "parentCommentId": "CfDZCJeiEBdtdkFsN", "user": {"username": "Jason"}}, {"_id": "qtoeM57bPcvXSKmtn", "postedAt": "2023-12-06T10:42:02.607Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<blockquote><p><strong>Evidentiary standards.</strong> We drew on a large number of RCTs for our systematic reviews and meta-analyses of cash transfers and psychotherapy (42 and 74, respectively). If one holds that the evidence for something as well-studied as psychotherapy is too weak to justify any recommendations, charity evaluators could recommend very little.<br>&nbsp;</p></blockquote><p>A comparatively minor point, but it doesn't seem to me that the claims in Greg's post [<a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of#:~:text=But%20there%20could,studies%20that%20remain.\">more</a>] are meaningfully weakened by whether or not psychotherapy is well-studied (as measured by how many RCTs HLI has found on it, noting that you already push back on some object level disagreement on study quality in <a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of#:~:text=views.%20To%20unpack%3A-,Study%20quality.,-We%20conclude%20the\">point 1</a>, which feels more directly relevant).<br><br>It also seems pretty unlikely to be true that psychotherapy being well studied necessarily means that StrongMinds is a cost-effective intervention comparable to current OP / GW funding bars (which is one main <a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of#:~:text=it%20is%20even%20more%20dubious%20SM%20is%20close%20to%20the%20frontier%20of%20%22very%20well%20evidenced%22%20vs.%20%22has%20very%20promising%20results%22%20plotted%20out%20by%20things%20like%20AMF%2C%20GD%2C%20etc.%20HLI%27s%20choice%20to%20nonetheless%20recommend%20SM%20again%20this%20giving%20season%20is%20very%20surprising.%20I%20doubt%20it%20will%20weather%20hindsight%20well.\">point of contention</a>), or that charity evaluators need 74+ RCTs in an area before recommending a charity. Is the implicit claim being made here is that the evidence for StrongMinds being a top charity is stronger than that of AMF, which is (AFAIK) based on less than 74 RCTs?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyzm77h7u2q\"><sup><a href=\"#fnyzm77h7u2q\">[1]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyzm77h7u2q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyzm77h7u2q\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.givewell.org/international/technical/programs/insecticide-treated-nets#Evidencefromsmallscalehighqualitystudies\">GiveWell</a>, <a href=\"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD000363.pub3/full\">Cochrane</a></p></div></li></ol>", "parentCommentId": "tthdc2ZnjAc7npuLH", "user": {"username": "bruce"}}, {"_id": "KMx9vpeQiHYSFgkqM", "postedAt": "2023-12-06T12:19:10.856Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>[<strong>Disclaimer:</strong> I worked at HLI until March 2023. I now work at the International Alliance of Mental Health Research Funders]</p><p>Gregory says</p><blockquote><p>these problems are sufficiently major I think potential donors are ill-advised to follow the recommendations and analysis in this report.</p></blockquote><p>That is a strong claim to make and it requires him to present a convincing case that GiveDirectly is more cost-effective than StrongMinds. I've found his previous methodological critiques to be constructive and well-explained. To their credit, HLI has incorporated many of them in the updated analysis. However, in my opinion, the critiques he presents here do not make a convincing case.</p><p>Taking his summary points in turn...</p><blockquote><ol><li>The literature on PT in LMICs is a complete mess. Insofar as more sense can be made from it, the most important factors appear to belong to the studies investigating it (e.g. their size) rather than qualities of the PT interventions themselves.</li></ol></blockquote><p>I think this is much too strong. The three meta-analyses (and Gregory's own calculations) give me confidence that psychotherapy in LMICs is effective, although the effects are likely to be small.</p><blockquote><p>2. Trying to correct the results of a compromised literature is known to be a nightmare. Here, the qualitative evidence for publication bias is compelling. But quantifying what particular value of 'a lot?' the correction should be is fraught: numerically, methods here disagree with one another dramatically, and prove highly sensitive to choices on data exclusion.</p></blockquote><p>There is no consensus on the appropriate methodology for adjusting publication bias. I don't have an informed opinion on this, but HLI's approach seems reasonable to me and I think it's reasonable for Greg to take a different view. From my limited understanding, neither approach makes GiveDirectly more cost-effective.&nbsp;</p><blockquote><p>3. Regardless of how PT looks in general, StrongMinds, in particular, is looking less and less promising. Although initial studies looked good, they had various methodological weaknesses, and a forthcoming RCT with much higher methodological quality is expected to deliver disappointing results.&nbsp;</p></blockquote><p>We don't have any new data on StrongMinds so I'm confused why Greg thinks it's \"less and less promising\". HLI's Bayesian approach is a big improvement on the subjective weightings they used in the first cost-effectiveness analysis. As with publication bias, it's reasonable to hold different views on how to construct the prior, but personally, I do believe that any psychotherapy intervention in LMICs, so long as cost per patient is &lt;$100, is a ~certain bet to beat cash transfers. There are no specific models of psychotherapy that perform better than the others, so I don't find it surprising that training people to talk to other people about their problems is a more cost-effective way to improve wellbeing in LMICs. Cash transfers are much more expensive and the effects on subjective wellbeing are also small.</p><blockquote><p>4. The evidential trajectory here is all to common, and the outlook typically bleak. It is dubious StrongMinds is a good pick even among psychotherapy interventions (picking one at random which doesn't have a likely-bad-news RCT imminent seems a better bet). Although pricing different interventions is hard, it is even more dubious SM is close to the frontier of \"very well evidenced\" vs. \"has very promising results\" plotted out by things like AMF, GD, etc. HLI's choice to nonetheless recommend SM again this giving season is very surprising. I doubt it will weather hindsight well.</p></blockquote><p>HLI had to start somewhere and I think we should give credit to StrongMinds for being brave enough to open themselves up to the scrutiny they've faced. The three meta-analyses and the tentative analysis of Friendship Bench suggest there is 'altruistic gold' to be found here and HLI has only just started to dig. The field is growing quickly and I'm optimistic about the trajectories of CE-incubated charities like <a href=\"https://forum.effectivealtruism.org/posts/MFRqdphhyddjgTwX4/vida-plena-transforming-mental-health-in-ecuador-first-year\">Vida Plena</a> and <a href=\"https://forum.effectivealtruism.org/posts/CdELxtHgQjzCYPhtm/kaya-guides-marginal-funding-for-tech-enabled-mental-health\">Kaya Guides</a>.&nbsp;</p><p>In the meantime, although the gap between GiveDirectly and StrongMinds has clearly narrowed, I remain unconvinced that cash is clearly the better option (but I do remain open-minded and open to pushback).</p>", "parentCommentId": "CfDZCJeiEBdtdkFsN", "user": {"username": "BarryGrimes"}}, {"_id": "PoovhcoBCRre6MAqd", "postedAt": "2023-12-06T13:06:55.443Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>What prior to formally pick is tricky - I agree the factors you note would be informative, but how to weigh them (vs. other sources of informative evidence) could be a matter of taste. However, sources of evidence like this could be handy to use as 'benchmarks' to see whether the prior (/results of the meta-analysis) are consilient with them, and if not, explore why.</p><p>But I think I can now offer a clearer explanation of what is going wrong. The hints you saw point in this direction, although not quite as you describe.</p><p>One thing worth being clear on is HLI is not updating on the actual SM specific evidence. As they model it, the estimated effect on this evidence is an initial effect of g = 1.8, and a total effect of ~3.48 WELLBYs, so this would lie on the <i>right</i> tail, not the left, of the informed prior.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefflx3ff79iq7\"><sup><a href=\"#fnflx3ff79iq7\">[1]</a></sup></span>&nbsp;They <i>discount</i> the effect by a factor of 20 to generate the data they feed into their Bayesian method. Stipulating data which would be (according to their prior) very surprisingly bad would be in itself a strength, not a concern, of the conservative analysis they are attempting.&nbsp;</p><p>Next, we need to distinguish an <i>average effect size</i> from a <i>prediction interval</i>. The HLI does report both (Section 4) for a more basic model of PT in LMICs. The (average, random) effect size is 0.64 (95% CI 0.54 to 0.74), whilst the prediction interval is -0.27 to 1.55. The former is giving you the best guess of the average effect (with a confidence interval), the latter is telling you - if I do another study like those already included, the range I can expect its effect size to be within. By loose analogy: if I sample 100 people and their average height is roughly 5' 7\" (95% CI 5'6\" to 5'8\"), the 95% range of the individual heights will range much more widely (say 5' 0\" to 6' 2\")</p><p>Unsurprisingly (especially given the marked heterogeneity), the prediction interval is much wider than the confidence interval around the average effect size. Crucially, if our 'next study' reports an effect size of (say) 0.1, our interpretation typically should <i>not</i> be: \"This study can't be right, the real effect of the intervention it studies must be much closer to 0.6\". Rather, as findings are heterogeneous, it is much more likely a study which (genuinely) reports a below average effect.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref66hxavekann\"><sup><a href=\"#fn66hxavekann\">[2]</a></sup></span>&nbsp;Back to the loose analogy, we would (typically) assume we got it right if we measured some more people at (e.g.) 6'0\" and 5'4\", even though these are significantly above or below the 95% confidence interval of our average, and only start to doubt measurements much outside our prediction interval (e.g. 3'10\", 7'7\").</p><p>Now the problem with the informed prior becomes clear: it is (essentially) being constructed with <i>confidence intervals of the average</i>, not <i>prediction intervals for its data</i> from its underlying models. As such, it is a prior <i>not </i>of \"What is the expected impact of <i>a given PT intervention</i>\", but rather \"What is the expected <i>average impact of PT interventions as a whole</i>\".<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefztnwu31zt9n\"><sup><a href=\"#fnztnwu31zt9n\">[3]</a></sup></span></p><p>With this understanding, the previously bizarre behaviour is made sensible. For the informed prior should assign very little credence to the <i>average</i> impact of PT <i>overall</i> being ~0.4 per the stipulated Strongminds data, even though it should not be <i>that</i> surprised that a particular intervention (e.g. Strongminds!) has an impact much below average, as many other PT interventions studied also do (cf. Although I shouldn't be surprised if I measure someone as 5'2\", I should be very surprised if the true average height is actually 5'2\", given my large sample averages 5'7\"). Similarly, if we are given a much smaller additional sample reporting a much different effect size, the updated average effect should remain pretty close to the prior (e.g. if a handful of new people have heights &lt; 5'4\", my overall average goes down a little, but not by much).&nbsp;</p><p>Needless to say, the results of such an analysis, if indeed for \"average effect size of psychotherapy as a whole\" are completely inappropriate for \"expected effect size of a given psychotherapy intervention\", which is the use it is put to in the report.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefec97tzbyzb7\"><sup><a href=\"#fnec97tzbyzb7\">[4]</a></sup></span>&nbsp;If the measured effect size of Strongminds was indeed ~0.4, the fact psychotherapy interventions ~average substantially greater effects of ~1.4 gives very little reason to conclude the <i>effect of Strongminds</i> is in fact much higher (e.g. ~1.3). In the same way, if I measure your height is 5'0\", the fact the average heights of other people I've measured is 5'7\" does not mean I should conclude you're probably about 5'6\".<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzwpzxg8uf9s\"><sup><a href=\"#fnzwpzxg8uf9s\">[5]</a></sup></span></p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnflx3ff79iq7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefflx3ff79iq7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Minor: it does lie pretty far along the right tail of the prior (&lt;top 1st percentile?), so maybe one could be a little concerned. Not much, though: given HLI was <i>searching</i> for particularly effective PT interventions in the literature, it doesn't seem that surprising that this effort could indeed find one at the far-ish right tail of apparent efficacy.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn66hxavekann\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref66hxavekann\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Cf. <a href=\"https://www.tandfonline.com/doi/full/10.1080/10503307.2019.1649732\">Cuijpers et al. 2020</a></p><blockquote><p>One problem for many types of the examined psychotherapy is that the level of heterogeneity was high, and many of the prediction intervals were broad and included zero. This means that it is difficult to predict the effect size of the next study that is done with this therapy, and that study may just as well find negative effects. The resulting effect sizes differ so much for one type of therapy, that it cannot be reliably predicted what the true effect size is.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnztnwu31zt9n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefztnwu31zt9n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Cf. your original point about a low result looking weird given the prior. Perhaps the easiest way to see this is to consider a case where the intervention is <i>harmful</i>. The informed prior says P (ES &lt; 0) is very close to zero. Yet &gt;1/72 studies in the sample <i>did</i> have an effect size &lt; 0. So obviously a prior of an intervention should not be <i>that</i> confident in predicting it will not have a -ve effect. But a prior of the average effect of PT interventions should be <i>that</i> confident this average is not in fact negative, given the great majority of sampled studies show substantially +ve effects.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnec97tzbyzb7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefec97tzbyzb7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In a sense, the posterior is not computing the expected effect of StrongMinds, but rather the expected effect of a future intervention <i>like</i> StrongMinds. Somewhat ironically, this (again, simulated) result would be best interpreted as an <i>anti-recommendation</i>: Strongminds performs much <i>below</i> the average we would expect of interventions similar to it.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzwpzxg8uf9s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzwpzxg8uf9s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It is slightly different for measured height as we usually have very little pure measurement error (versus studies with more significant sampling variance). So you'd update a little less towards the reported study effects vs. the expected value than you would for height measurements vs. the average. But the key points still stand.</p></div></li></ol>", "parentCommentId": "esgP2rtmPLhb68mb5", "user": {"username": "Gregory_Lewis"}}, {"_id": "hWxBvTGcBhCB7wLEv", "postedAt": "2023-12-06T20:46:37.755Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<blockquote>\n<p>You cannot use the distribution for the expected value of an average therapy treatment as the prior distribution for a SPECIFIC therapy treatment, as there will be a large amount of variation between possible therapy treatments that is missed when doing this.</p>\n</blockquote>\n<p>A specific therapy treatment is drawn from the distribution of therapy treatments. Our best guess about the distribution of value of a specific therapy treatment, without knowing anything about it, should take into account only that it comes from this distribution of therapy treatments. So I don't see what's unreasonable about this.</p>\n", "parentCommentId": "CfDZCJeiEBdtdkFsN", "user": {"username": "therealslimkt"}}, {"_id": "hWCSrXKS5WG2QidDN", "postedAt": "2023-12-06T22:16:41.858Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>When running a meta-analysis, you can either use a fixed effect assumption (that all variation between studies is just due to sampling error) and a random effect assumption (that studies differ in terms of their \"true effects\".) Therapy treatments differ greatly, so you have to use a random effects model in this case. Then the prior you use for strong minds impact should have a variance that is the sum of the variance in the estimate of average therapy treatments effects AND the variance among different types of treatments effects, both numbers should be available from a random effects meta-analysis. I'm not quite sure what HLI did exactly to get their prior for strong minds here, but for some reason the variance on it seems WAY too low, and I suspect that they neglected the second type of variance that they should have gotten from a random effects meta-analysis.</p>\n", "parentCommentId": "hWxBvTGcBhCB7wLEv", "user": {"username": "Burner1989"}}, {"_id": "tYArqsmdjtEsgpWCo", "postedAt": "2023-12-06T23:21:16.372Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Section 2.2.2 of their report is titled \"Choosing a fixed or random effects model\". They discuss the points you make and clearly say that they use a random effects model. In section 2.2.3 they discuss the standard measures of heterogeneity they use. Section 2.2.4 discusses the specific 4-level random effects model they use and how they did model selection.</p>\n<p>I reviewed a small section of the report prior to publication but none of these sections, and it only took me 5 minutes now to check what they did. I'd like the EA Forum to have a higher bar (as Gregory's parent comment exemplifies) before throwing around easily checkable suspicions about what (very basic) mistakes might have been made.</p>\n", "parentCommentId": "hWCSrXKS5WG2QidDN", "user": {"username": "DavidBernard"}}, {"_id": "j57HTzKgrgYw3JAms", "postedAt": "2023-12-06T23:28:19.676Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Yes, some of Greg's examples point to the variance being underestimated, but the problem does not inherently come from the idea of using the distribution of effects as the prior, since that should include both the sampling uncertainty and true heterogeneity. That would be the appropriate approach even under a random effects model (I think; I'm more used to thinking in terms of Bayesian hierarchical models and the equivalence might not hold)</p>\n", "parentCommentId": "hWCSrXKS5WG2QidDN", "user": {"username": "therealslimkt"}}, {"_id": "spfz9KNFmnuWSiC4o", "postedAt": "2023-12-07T22:31:28.947Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Hey Jason,</p><p>Our (the HLI) comment was in reference to these quotes.&nbsp;</p><blockquote><p>The literature on PT in LMICs is a complete mess.</p></blockquote><blockquote><p>Trying to correct the results of a compromised literature is known to be a nightmare.</p></blockquote><p>I think it is valid to describe these as saying the literature is compromised and (probably) uninformative. I can understand your complaint about the word \u201cbunk\u201d. Apologies to Gregory if this is a mischaracterization.&nbsp;</p><p>Regarding our comment:&nbsp;</p><blockquote><p>If one insisted only on using charity evaluations that had every choice pre-registered, there would be none to choose from.</p></blockquote><p>And your comment:&nbsp;</p><blockquote><p>I don't think anyone has claimed lack of certain choices being pre-registered is somehow fatal, only a factor to consider.</p></blockquote><p>Yeah, I think this is a valid point, and the post should have quoted Gregory directly. The point we were hoping to make here is that we\u2019ve attempted to provide a wide range of sensitivity analyses throughout our report, to an extent that we think goes beyond most charity evaluations. It\u2019s not surprising that we\u2019ve missed some in this draft that others would like to see. Gregory\u2019s comments mentioned \u201cEven if you didn't pre-specify, presenting your first cut as the primary analysis helps for nothing up my sleeve reasons\u201d seemed to imply that we were deliberately hiding something, but in my view our interpretation was overly pessimistic.&nbsp;</p><p>Cheers for keeping the discourse civil.&nbsp;</p>", "parentCommentId": "4BcNTYH6srkiwThQ3", "user": {"username": "Ryan Dwyer"}}, {"_id": "yfb9dM6ENZ2XFZMhX", "postedAt": "2023-12-08T03:23:08.886Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Hi Jason,&nbsp;</p><blockquote><p>\u201cWould it have been better to start with a stipulated prior based on evidence of short-course general-purpose<a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of#fnulv87oj6nel\"><sup><u>[1]</u></sup></a> psychotherapy's effect size generally, update that prior based on the LMIC data, and then update&nbsp;<i>that&nbsp;</i>on charity-specific data?\u201d</p></blockquote><p>1. To your first point, I think adding another layer of priors is a plausible way to do things \u2013 but given the effects of psychotherapy in general appear to be similar to the estimates we come up with<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref73844q6sk0d\"><sup><a href=\"#fn73844q6sk0d\">[1]</a></sup></span>&nbsp;\u2013 it\u2019s not clear how much this would change our estimates.&nbsp;</p><p>There are probably two issues with using HIC RCTs as a prior. First, incentives that could bias results probably differ across countries. I\u2019m not sure how this would pan out. Second, in HICs, the control group (\u201ctreatment as usual\u201d) is probably a lot better off. In a HIC RCT, there\u2019s not much you can do to stop someone in the control group of a psychotherapy trial to go get prescribed antidepressants. However, the standard of care in LMICs is much lower (antidepressants typically aren\u2019t an option), so we shouldn\u2019t be terribly surprised if control groups appear to do worse (and the treatment effect is thus larger).&nbsp;</p><blockquote><p>\u201cTo my not-very-well-trained eyes, one hint to me that there's an issue with application of Bayesian analysis here is the failure of the LMIC effect-size model to come anywhere close to predicting the effect size suggested by the SM-specific evidence.\u201d</p></blockquote><p>2. To your second point, does our model predict charity specific effects?&nbsp;</p><p>In general, I think it\u2019s a fair test of a model to say it should do a reasonable job at predicting new observations. We can\u2019t yet discuss the forthcoming StrongMinds RCT \u2013 we will know how well our model works at predicting that RCT when it\u2019s released, but for the Friendship Bench (FB) situation, it is true that we predict a considerably lower effect for FB than the FB-specific evidence would suggest. But this is in part because we\u2019re using a combination of charity specific evidence to inform our prior and the data. Let me explain.&nbsp;</p><p>We have two sources of charity specific evidence. First, we have the RCTs, which are based on a charity programme but not as it\u2019s deployed at scale. Second, we have monitoring and evaluation data, which can show how well the charity intervention is implemented in the real world. We don\u2019t have a psychotherapy charity at present that has RCT evidence of the programme as it's deployed in the real world<i>.</i> This matters because I think placing a very high weight on the charity-specific evidence would require that it has a high ecological validity. While the ecological validity of these RCTs is obviously higher than the average study, we still think it\u2019s limited. I\u2019ll explain our concern with FB.&nbsp;&nbsp;</p><p>For Friendship Bench, the most recent RCT (<a href=\"https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2807191?utm_campaign=articlePDF&amp;utm_medium=articlePDFlink&amp;utm_source=articlePDF&amp;utm_content=jamanetworkopen.2023.23205\"><u>Haas et al. 2023, n = 516</u></a>) reports an attendance rate of around 90% to psychotherapy sessions, but the Friendship Bench M&amp;E data reports an attendance rate more like 30%. We discuss this in Section 8 of the report.</p><p>So for the Friendship Bench case we have a couple reasonable quality RCTs for Friendship Bench, but it seems like, based on the M&amp;E data, that something is wrong with the implementation. This evidence of lower implementation quality should be adjusted for, which we do. But we include this adjustment&nbsp;<strong>in the prior.&nbsp;</strong>So we\u2019re injecting charity specific evidence into both the prior and the data. Note that this is part of the reason why we don\u2019t think it\u2019s wild to place a decent amount of weight on the prior. This is something we should probably clean up in a future version.&nbsp;</p><p>We can\u2019t discuss the details of the Baird et al. RCT until it\u2019s published, but we think there may be an analogous situation to Friendship Bench where the RCT and M&amp;E data tell conflicting stories about implementation quality.&nbsp;</p><p>This is all to say, judging how well our predictions fair when predicting the charity specific effects isn\u2019t clearly straightforward, since we are trying to predict the effects of the charity as it is actually implemented (something we don\u2019t directly observe), not simply the effects from an RCT.&nbsp;</p><p>If we try and predict the RCT effects for Friendship Bench (which have much higher attendance than the \"real\" programme), then the gap between the predicted RCT effects and actual RCT effects is much smaller, but still suggests that we can\u2019t completely explain why the Friendship Bench RCTs find their large effects.</p><p>So, we think the error in our prediction isn't quite as bad as it seems if we're predicting the RCTs, and stems in large part from the fact that we are actually predicting the charity implementation.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn73844q6sk0d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref73844q6sk0d\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://psycnet.apa.org/doiLanding?doi=10.1037%2Famp0001250\"><u>Cuijpers et al. 2023</u></a> finds an effect of psychotherapy of 0.49 SDs for studies with low RoB in low, middle, and high income countries (comparisons = 218#), and&nbsp;<a href=\"https://www.cambridge.org/core/journals/psychological-medicine/article/psychotherapy-for-adult-depression-in-low-and-middleincome-countries-an-updated-systematic-review-and-metaanalysis/630D2D6E07018C9CA7A63FD27C1B0822\"><u>Tong et al. 2023</u></a> find an effect of 0.69 SDs for studies with low RoB in non-western countries (primarily low and middle income; comparisons = 36). Our estimate of the initial effect is 0.70 SDs (before publication bias adjustments). The results tend to be lower (between 0.27 and 0.57, or 0.42 and 0.60) SDs when the authors of the meta-analyses correct for publication bias. In both meta-analyses (Tong et al. and Cuijpers et al.) the authors present the effects after using three publication bias corrected methods: trim-and-fill (0.6; 0.38 SDs), a limit meta-analysis (0.42; 0.28 SDs), and using a selection model (0.49; 0.57 SDs). If we averaged their publication bias corrected results (which they did without removing outliers beforehand) the estimated effect of psychotherapy would be 0.5 SDs and 0.41 for the two meta-analyses. Our estimate of the initial effect (which is most comparable to these meta-analyses), after removing outliers is 0.70 SDs, and our publication bias correction is 36%, implying that we estimate our initial effect to be 0.46 SDs. You can play around with the data they use on<a href=\"https://www.metapsy.org/\"><u> the metapsy website</u></a>.</p></div></li></ol>", "parentCommentId": "esgP2rtmPLhb68mb5", "user": {"username": "JoelMcGuire"}}, {"_id": "kzduxyCSurEwyXgxG", "postedAt": "2023-12-08T03:29:54.163Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Hi again Jason,&nbsp;</p><p>&nbsp;When we said \"Excluding outliers is thought sensible practice here; two related meta-analyses,&nbsp;<a href=\"https://www.tandfonline.com/doi/full/10.1080/10503307.2019.1649732\"><u>Cuijpers et al., 2020c</u></a>;&nbsp;<a href=\"https://www.cambridge.org/core/journals/psychological-medicine/article/psychotherapy-for-adult-depression-in-low-and-middleincome-countries-an-updated-systematic-review-and-metaanalysis/630D2D6E07018C9CA7A63FD27C1B0822\"><u>Tong et al., 2023</u></a>, used a similar approach\" -- I can see that what we meant by \"similar approach\" was unclear. We meant that, conditional on removing outliers, they identify a similar or greater range of effect sizes as outliers as we do.</p><p>This was primarily meant to address the question raised by Gregory about whether to include outliers: \u201cThe cut data by and large doesn't look visually 'outlying' to me.\u201d&nbsp;</p><p>To rephrase, I think that Cuijpers et al. and Tong et al. would agree that the data we cut looks outlying. Obviously, this is a milder claim than our comment could be interpreted as making.&nbsp;&nbsp;</p><p>Turning to wider implications of these meta-analyses,&nbsp;As you rightly point out, they don\u2019t have a \u201cpreferred specification\u201d and are mostly presenting the options for doing the analysis. They present analyses with&nbsp;<i>and without</i> outlier removal in their main analysis, and they adjust for publication bias without outliers removed (which is not what we do). The first analytic choice doesn\u2019t clearly support including or excluding outliers, and the second \u2013 if it supports any option, favors Greg's proposed approach of correcting for publication bias without outliers removed.</p><p>I think one takeaway is that we should consider surveying the literature and some experts in the field, in a non-leading way, about what choices they\u2019d make if they didn\u2019t have \u201cthe luxury of not having to reach a conclusion\u201d.</p><p>I think it seems plausible to give some weight to analyses with and without excluding outliers \u2013 <i>if</i> we are able find a reasonable way to treat the 2 out of 7 publication bias correction methods that produce the results suggesting that the effect of psychotherapy is in fact sizably negative. We'll look into this more before our next update.</p><p>Cutting the outliers here was part of our first pass attempt at minimising the influence of dubious effects, which we'll follow up with a <a href=\"https://www.riskofbias.info\">Risk of Bias</a> analysis in the next version. Our working assumption was that effects greater than ~ 2 standard deviations are suspect on theoretical grounds (that is, if they behave anything like SDs in an normal distribution), and seemed more likely to be the result of some error-generating process (e.g. data-entry error, bias) than a genuine effect.&nbsp;</p><p>We'll look into this more in our next pass, but for this version we felt outlier removal was the most sensible choice.&nbsp;<br>&nbsp;</p>", "parentCommentId": "H82ApbPtm8J9FEXse", "user": {"username": "JoelMcGuire"}}, {"_id": "fnDvj9tMLdT7rBCek", "postedAt": "2023-12-08T09:59:25.612Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>I recently discovered that GiveWell decided to <a href=\"https://forum.effectivealtruism.org/posts/JifMMsEyhgmRW5RDD/how-we-work-3-our-analyses-involve-judgment-calls\">exclude an outlier</a> in their water chlorination meta-analysis. I'm not qualified to judge their reasoning, but maybe others with sufficient expertise will weigh in?</p><blockquote><p>We excluded one RCT that meets our other criteria because we think the results are implausibly high such that we don't believe they represent the true effect of chlorination interventions (more in footnote).[4] It's unorthodox to exclude studies for this reason when conducting a meta-analysis, but we chose to do so because we think it gives us an overall estimate that is more likely to represent the true effect size.</p></blockquote>", "parentCommentId": "H82ApbPtm8J9FEXse", "user": {"username": "BarryGrimes"}}, {"_id": "btY2nyuFNHiQKGGZ7", "postedAt": "2023-12-08T10:01:23.123Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>I recently discovered that GiveWell decided to <a href=\"https://forum.effectivealtruism.org/posts/JifMMsEyhgmRW5RDD/how-we-work-3-our-analyses-involve-judgment-calls\">exclude an outlier</a> in their water chlorination meta-analysis. I'm not qualified to judge their reasoning, but maybe others with sufficient expertise will weigh in?</p><blockquote><p>We excluded one RCT that meets our other criteria because we think the results are implausibly high such that we don't believe they represent the true effect of chlorination interventions (more in footnote).[4] It's unorthodox to exclude studies for this reason when conducting a meta-analysis, but we chose to do so because we think it gives us an overall estimate that is more likely to represent the true effect size.</p></blockquote>", "parentCommentId": "H82ApbPtm8J9FEXse", "user": {"username": "BarryGrimes"}}, {"_id": "YLNCRwMLTy2atdrY2", "postedAt": "2023-12-08T10:13:52.278Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>It looks like the same comment got posted several times?</p>\n", "parentCommentId": "fnDvj9tMLdT7rBCek", "user": {"username": "bec_hawk"}}, {"_id": "a73D9wZFeeQXLyegH", "postedAt": "2023-12-08T16:36:13.848Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>Thanks Rebecca. I will delete the duplicates.</p>", "parentCommentId": "YLNCRwMLTy2atdrY2", "user": {"username": "BarryGrimes"}}, {"_id": "Q2Sr6niERog5T4RWt", "postedAt": "2023-12-08T16:41:17.510Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>(<a href=\"https://forum.effectivealtruism.org/users/burner1989?mention=user\">@Burner1989</a> <a href=\"https://forum.effectivealtruism.org/users/davidbernard?mention=user\">@David Rhys Bernard</a> <a href=\"https://forum.effectivealtruism.org/users/karthik-tadepalli?mention=user\">@Karthik Tadepalli</a>)</p><p>I think the fundamental point (i.e. \"You cannot use the distribution for the expected value of an average therapy treatment as the prior distribution for a SPECIFIC therapy treatment, as there will be a large amount of variation between possible therapy treatments that is missed when doing this.\") is on the right lines, although subsequent discussion of fixed/random effect models might confuse the issue. (Cf. my reply to Jason).</p><p>The typical output of a meta-analysis is an (~) average effect size estimate (the diamond at the bottom of the forest plot, etc.) The confidence interval given for that is (very roughly)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnaokr3qmg9e\"><sup><a href=\"#fnnaokr3qmg9e\">[1]</a></sup></span>&nbsp;the interval we predict the true average effect likely lies. So for the basic model given in Section 4 of the report, the average effect size is 0.64, 95% CI (0.54 - 0.74). So (again, roughly) our best guess of the 'true' average effect size of psychotherapy in LMICs from our data is 0.64, and we're 95% sure(*) this average is somewhere between (0.54, 0.74).</p><p>Clearly, it is not the case that if we draw another study from the same population, we should be 95% confident(*) the effect size of this new data point will lie between 0.54 to 0.74. This would not be true even in the unicorn case there's no between study heterogeneity (e.g. all the studies are measuring the same effect modulo sampling variance), and even less so when this is marked, as here. To answer that question, what you want is a <i>prediction interval</i>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl1yswaki8fk\"><sup><a href=\"#fnl1yswaki8fk\">[2]</a></sup></span>&nbsp;This interval is always wider, and almost always significantly so, than the confidence interval for the average effect: in the same analysis with the 0.54-0.74 confidence interval, the prediction interval was -0.27 to 1.55.</p><p>Although the full model HLI uses in constructing informed priors is different from that presented in S4 (e.g. it includes a bunch of moderators), they <a href=\"https://www.happierlivesinstitute.org/research/cost-effectiveness-analysis-methodology/#:~:text=Reporting%C2%A0only%20a,effect%20over%20time.\">appear</a> to be constructed with monte carlo on the <i>confidence</i> <i>intervals for the average</i>, not the <i>prediction interval for the data.</i> So I believe the informed prior is actually one of the (adjusted) \"Average effect of psychotherapy interventions as a whole\", not a prior for (e.g.) \"the effect size reported in a given PT study.\" The latter would need to use the prediction intervals, and have a much wider distribution.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftrn981fhwm8\"><sup><a href=\"#fntrn981fhwm8\">[3]</a></sup></span></p><p>I think this ably explains exactly why the Bayesian method for (e.g.) Strongminds gives very bizarre results when deployed as the report does, but they do make much more sense if re-interpreted as (in essence) computing the expected effect size of 'a future strongminds-like intervention', but not the effect size we should believe StrongMinds actually has once in receipt of trial data upon it specifically. E.g.:</p><ul><li>The histogram of effect sizes shows some comparisons had an effect size &lt; 0, but the 'informed prior' suggests P(ES &lt; 0) is extremely low. As a prior for the effect size of the next study, it is much too confident, given the data, a trial will report positive effects (you have &gt;1/72 studies being negative, so surely it cannot be &lt;1%, etc.). As a prior for the average effect size, this confidence is warranted: given the large number of studies in our sample, most of which report positive effects, we would be very surprised to discover the true average effect size is <i>negative</i>.</li><li>The prior doesn't update very much on data provided. E.g. When we stipulate the trials upon strongminds report a near-zero effect of 0.05 WELLBYs, our estimate of 1.49 WELLBYS goes to 1.26: so we should (apparently) believe in such a circumstance the efficacy of SM is ~25 times greater than the trial data upon it indicates. This is, obviously, absurd. However, such a small update is appropriate if it were to ~the average of PT interventions as a whole: that we observe a new PT intervention has much below average results should cause our average to shift a little towards the new findings, but not much.</li></ul><p>In essence, the update we are interested in is not \"How effective should we expect <i>future interventions like Strongminds</i> are given the data on Strongminds efficacy\", but simply \"How effective should we expect <i>Strongminds</i> is given the data on how effective Strongminds is\". Given the massive heterogeneity and wide prediction interval, the (correct) informed prior is pretty <i>uninformative, </i>as it isn't that surprised by anything in a very wide range of values, and so on finding trial data on SM with a given estimate in this range, our estimate should update to match it pretty closely.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefozi8cgeyma\"><sup><a href=\"#fnozi8cgeyma\">[4]</a></sup></span>&nbsp;</p><p>(This also should mean, unlike the report suggests, the SM estimate is not that 'robust' to adverse data. Eyeballing it, I'd guess the posterior should be going down by a factor of 2-3 conditional on the stipulated data versus currently reported results).</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnaokr3qmg9e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnaokr3qmg9e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'm aware confidence intervals are not credible intervals, and that 'the 95% CI tells you where the true value is with 95% likelihood' strictly misinterprets what a confidence interval is, etc. (<a href=\"https://en.wikipedia.org/wiki/Confidence_interval#Common_misunderstandings\">see</a>) But perhaps 'close enough', so I'm going to pretend these are credible intervals, and asterisk each time I assume the strictly incorrect interpretation.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl1yswaki8fk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl1yswaki8fk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Cf. <a href=\"https://training.cochrane.org/handbook/current/chapter-10#section-10-10-4-3\">Cochrane</a>:</p><blockquote><p>The summary estimate and confidence interval from a random-effects meta-analysis refer to the centre of the distribution of intervention effects, but do not describe the width of the distribution. Often the summary estimate and its confidence interval are quoted in isolation and portrayed as a sufficient summary of the meta-analysis. This is inappropriate. The confidence interval from a random-effects meta-analysis describes uncertainty in the location of the mean of systematically different effects in the different studies. It does not describe the degree of heterogeneity among studies, as may be commonly believed. For example, when there are many studies in a meta-analysis, we may obtain a very tight confidence interval around the random-effects estimate of the mean effect even when there is a large amount of heterogeneity. A solution to this problem is to consider a <strong>prediction interval</strong> (see Section <a href=\"https://training.cochrane.org/handbook/current/chapter-10#_Ref522102032\">10.10.4.3</a>).</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntrn981fhwm8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftrn981fhwm8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although I think the same mean, so it will give the right 'best guess' initial estimates.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnozi8cgeyma\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefozi8cgeyma\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Obviously, modulo all the other issues I suggest with both the meta-analysis as a whole, that we in fact would incorporate other sources of information into our actual prior, etc. etc.</p></div></li></ol>", "parentCommentId": "hWCSrXKS5WG2QidDN", "user": {"username": "Gregory_Lewis"}}, {"_id": "7yMEoQ6ayHrpLrwpu", "postedAt": "2023-12-08T20:01:25.911Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>So the problem I had in mind was in the parenthetical in my paragraph:</p><blockquote><p>To its credit, the write-up does highlight this, but does not seem to appreciate the implications are crazy: any PT intervention, so long as it is cheap enough, should be thought better than GD, even if studies upon it show very low effect size <strong>(which would usually be reported as a negative result, as almost any study in this field would be underpowered to detect effects as low as are being stipulated)</strong></p></blockquote><p>To elaborate: the actual data on Strongminds was a n~250 study by Bolton et al. 2003 then followed up by Bass et al. 2006. HLI models this in table 19:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/ocr87bza64fyepadtcby\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/xk42dbe97mkepegxlkxu 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/ehjexcnzxlhoj23tu1rt 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/vnm91m3wczejftois13f 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/ufjdwfwtcj9zpg0ds2xm 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/ggircafns8hcktzmq59l 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/vtdp5gg3pntffopgkypj 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/rlde6l3zcwkdsrobezvs 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/dftp7p4pwm6ffae3faky 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/dikbv552nahxgdasheto 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/7yMEoQ6ayHrpLrwpu/wu9clhj2lkpjtdtvckwt 1452w\"></figure><p>So an initial effect of g = 1.85, and a total impact of 3.48 WELLBYs. To simulate what the SM data will show once the (anticipated to be disappointing) forthcoming Baird et al. RCT is included, they discount this<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsb2qlux73c\"><sup><a href=\"#fnsb2qlux73c\">[1]</a></sup></span>&nbsp;by a factor of 20.&nbsp;</p><p>Thus the simulated effect size of Bolton and Bass is now ~0.1. In this simulated case, the Bolton and Bass studies would be reporting <i>negative</i> results, as they would not be powered to detect an effect size as small as g = 0.1. To benchmark, the forthcoming Baird et al. study is 6x larger than these, and its power calculations have minimal detectable effects g = 0.1 or greater.&nbsp;</p><p>Yet, apparently, in such a simulated case we should conclude that Strongminds is fractionally better than GD <i>purely</i> on the basis of two trials reporting <i>negative</i> findings, because numerically the treatment groups did slightly (but not significantly) better than the control ones.&nbsp;</p><p>Even if in general we are happy with 'hey, the effect is small, but it is cheap, so it's a highly cost-effective intervention', we should not accept this at the point when 'small' becomes 'too small to be statistically significant'. Analysis method + negative findings =! fractionally better in expectation vs. cash transfers, so I take it as diagnostic the analysis is going wrong. &nbsp; &nbsp;&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsb2qlux73c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsb2qlux73c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think 'this' must be the initial effect size/intercept, as 3.48 * 0.05 ~ 1.7 not 3.8. I find this counter-intuitive, as I think the drop in total effect should be super not sub linear with intercept, but ignore that.</p></div></li></ol>", "parentCommentId": "nWE3sSwDM6pTpDPsA", "user": {"username": "Gregory_Lewis"}}, {"_id": "RiE9W85dCfxYYsHTD", "postedAt": "2023-12-10T21:20:16.762Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>I have previously let HLI have the last word, but this is too egregious.&nbsp;</p><p><strong>Study quality: </strong>Publication bias (a property of the literature as a whole) and risk of bias (particular to each individual study which comprise it) are two different things.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa678iv4p0x\"><sup><a href=\"#fna678iv4p0x\">[1]</a></sup></span>&nbsp;Accounting for the former does not account for the latter. This is why the <a href=\"https://training.cochrane.org/handbook/current/chapter-07#:~:text=Review%20authors%20should%20seek,from%20the%20included%20studies.\">Cochrane handbook</a>, the three meta-analyses HLI mentions here, <i>and HLI's own protocol</i> consider distinguish the two.</p><p>Neither Cuijpers et al. 2023 nor Tong et al. 2023 <i>further</i> adjust their low risk of bias subgroup for publication bias.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefx34dtemn42d\"><sup><a href=\"#fnx34dtemn42d\">[2]</a></sup></span>&nbsp;I tabulate the relevant figures from both studies below:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/f3s7qeug9iiyynwyqzsr\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/efkyub59ahseu5y3updo 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/f9xgalmtq5vbkjdztk6l 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/rtehs3ccsfftrdch6lkz 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/e2mdyv1q1hkbtttxjlyy 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/gbvhzhg96bhux1bgo8wu 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/oqbof8farhgebhyddzb4 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/ezetwy4yyrimkawzcbhz 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/abtqo0ua9niobhurbsdi 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/nf9mj6y8tosbdb3f0w3t 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RiE9W85dCfxYYsHTD/lfbu8xz7brfjmuxgrytd 1395w\"></figure><p>So HLI indeed gets similar initial results and publication bias adjustments to the two other meta-analyses they find. Yet - although these are not like-for-like - these other two meta-analyses find similarly substantial effect reductions when accounting for study quality as they do when assessing at publication bias of the literature as a whole.&nbsp;</p><p>There is ample cause for concern here:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsg8cyhx9g5\"><sup><a href=\"#fnsg8cyhx9g5\">[3]</a></sup></span></p><ul><li>Although neither of <i>these </i>studies 'adjust for both', one later mentioned - Cuijpers et al. 2020 - <i>does</i>. It finds an <i>additional</i> discount to effect size when doing so.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0dfnz9k7x0tu\"><sup><a href=\"#fn0dfnz9k7x0tu\">[4]</a></sup></span>&nbsp;So it suggests that indeed 'accounting for' publication bias does not adequately account for risk of bias <i>en passant</i>.</li><li>Tong et al. 2023 - the meta-analysis expressly on PT in LMICs rather than PT generally - finds higher prevalence of indicators of lower study quality in LMICs, and notes this as a competing explanation for the outsized effects.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5vq4z63ilnh\"><sup><a href=\"#fn5vq4z63ilnh\">[5]</a></sup></span>&nbsp;</li><li>As previously mentioned, in the previous meta-analysis, unregistered trials had a 3x greater effect size than registered ones. All trials on Strongminds published so far have not been registered. Baird et al., which is registered, is anticipated to report disappointing results.</li></ul><p><strong>Evidentiary standards: </strong>Indeed, the report drew upon a large number of studies. Yet even a synthesis of 72 million (or whatever) studies can be misleading if issues of publication bias, risk of bias in individual studies (and so on) are not appropriately addressed. That an area has 72 (or whatever) studies upon it does not mean it is <i>well-</i>studied, nor would this number (nor any number) be sufficient, by itself, to satisfy any evidentiary standard.</p><p><strong>Outlier exclusion: </strong>The report's approach to outlier exclusion is <i>dissimilar</i> to both Cuijpers et al. 2020 and Tong et al. 2023, and further is dissimilar with respect to <i>features I highlighted as major causes for concern re. HLI's approach in my original comment.</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr1ecl473ryp\"><sup><a href=\"#fnr1ecl473ryp\">[6]</a></sup></span><i>&nbsp;</i>Specifically:</p><ol><li>Both of these studies present the analysis with the full data first in their results. Contrast HLI's report, where only the results with outliers excluded are presented in the main results, and the analysis without exclusion is found only in the appendix.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwx45kykoec8\"><sup><a href=\"#fnwx45kykoec8\">[7]</a></sup></span></li><li>Both these studies also report the results with the full data as their main findings (e.g. in their respective abstracts). Cuijpers et al. mentions their outlier excluded results primarily in passing (\"outliers\" appears once in the main text); Tong et al. relegates a lot of theirs to the appendix. HLI's report does the opposite. (cf. fn 7 above)</li><li>Only Tong et al. does further sensitivity analysis on the 'outliers excluded' subgroup. As Jason describes, this is done alongside the analysis where all data included, the qualitative and quantitative differences which result from this analysis choice are prominently highlighted to the reader and extensively discussed. In HLI's report, by contrast, the factor of 3 reduction to ultimate effect size when outliers are not excluded is only alluded to qualitatively in a footnote (fn 33)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0wsfx2j968e\"><sup><a href=\"#fn0wsfx2j968e\">[8]</a></sup></span>&nbsp;of the main report's section (3.2) arguing why outliers should be excluded, not included in the reports sensitivity analysis, and only found in the appendix.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7vh2kz994vi\"><sup><a href=\"#fn7vh2kz994vi\">[9]</a></sup></span>&nbsp;</li><li>Both studies adjust for publication bias only on all data, not on data with outliers excluded, and these are the publication bias findings they present. Contrast HLI's report.</li></ol><p>The Cuijpers et al. 2023 meta-analysis previously mentioned also differs in its approach to outlier exclusion from HLI's report in the ways highlighted above. The Cochrane handbook <a href=\"https://training.cochrane.org/handbook/current/chapter-10#:~:text=Exclude%20studies.,in%20the%20protocol.\">also supports</a> my recommendations on what approach should be taken, which is what the meta-analyses HLI cites approvingly as examples of \"sensible practice\" actually do, but what HLI's own work does not.</p><p>The reports (non) presentation of the stark quantitative sensitivity of its analysis - material to its report bottom line recommendations - to whether outliers are excluded is clearly inappropriate. It is indefensible if, as I have suggested may be the case, the analysis with outliers included was indeed the analysis first contemplated and conducted.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd8o0fodfstt\"><sup><a href=\"#fnd8o0fodfstt\">[10]</a></sup></span>&nbsp;It is even worse if it was the <i>publication bias corrections </i>on the full data was what in fact prompted HLI to start making alternative analysis choices which happened to substantially &nbsp;increase the bottom line figures.</p><p><strong>Bayesian analysis: </strong>Bayesian methods notoriously do <i>not </i>avoid subjective inputs - most importantly here, what information we attend to when constructing an 'informed prior' (or, if one prefers, how to weigh the results with a particular prior stipulated).&nbsp;</p><p>In any case, they provide no protection from misunderstanding the calculation being performed, and so misinterpreting the results. The Bayesian method in the report is actually calculating the (adjusted) average effect size of psychotherapy interventions in general, not the expected effect of a given psychotherapy intervention. Although a trial on Strongminds which shows it is relatively ineffectual should not update our view much the efficacy of psychotherapy interventions (/similar to Strongminds) as a whole, it should update us dramatically on <i>the efficacy of Strongminds itself</i>.&nbsp;</p><p>Although as a methodological error this is a subtle one (at least, subtle enough for me not to initially pick up on it), the results it gave are nonsense to the naked eye (e.g. SM would still be held as a GiveDirectly-beating intervention even if there were <i>multiple</i> high quality RCTs on Strongminds giving flat or negative results). HLI should have seen this themselves, should have stopped to think after I highlighted these facially invalid outputs of their method in early review, and definitely should not be doubling down on these conclusions even now. &nbsp;</p><p><strong>Making recommendations: </strong>Although there are other problems, those I have repeated here make the recommendations of the report unsafe. This is why I recommended against publication. Specifically:</p><ol><li>Although I don't think the Bayesian method the report uses would be appropriate, if it was calculated properly on its own terms (e.g. prediction intervals not confidence intervals to generate the prior, etc.), and leaving everything else the same, the SM bottom line would drop (I'm pretty sure) by a factor a bit more than 2.&nbsp;</li><li>The results are already essentially sensitive to whether outliers are excluded in analysis or not: SM goes from 3.7x -&gt; ~1.1x GD on the back of my envelope, again leaving all else equal.&nbsp;</li></ol><p>(1) and (2) combined should net out to SM &lt; GD; (1) or (2) combined with some of the other sensitivity analyses (e.g. spillovers) will also likely net out to SM &lt; GD. Even if one still believes the bulk of (appropriate) analysis paths still support a recommendation, this sensitivity should be made transparent.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna678iv4p0x\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa678iv4p0x\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g. Even if all studies in the field are conducted impeccably, if journals only accept positive results the literature may still show publication bias. Contrariwise, even if all findings get published, failures in allocation/blinding/etc. could lead to systemic inflation of effect sizes across the literature. In reality - and here - you often have <i>both </i>problems, and they only partially overlap.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnx34dtemn42d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefx34dtemn42d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Jason <a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of?commentId=H82ApbPtm8J9FEXse\">correctly interprets</a> Tong et al. 2023: the number of studies included in their publication bias corrections (117 [+36 w/ trim and fill]) equals the number of all studies, not the low risk of bias subgroup (36 - see table 3). I <i>do</i> have access to Cuijpers et al. 2023, which has a very similar results table, with parallel findings (i.e. they do their publication bias corrections on the whole set of studies, not on a low risk of bias subgroup).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsg8cyhx9g5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsg8cyhx9g5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Me, previously:</p><blockquote><p>HLI's report does not assess the quality of its included studies, although it plans to do so. I appreciate GRADEing 90 studies or whatever is tedious and time consuming, but skipping this step to crack on with the quantitative synthesis is very unwise:&nbsp;any such synthesis could be hugely distorted by low quality studies.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0dfnz9k7x0tu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0dfnz9k7x0tu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>From their discussion (my emphasis):</p><blockquote><p>Risk of bias is another important problem in research on psychotherapies for depression. In 70% of the trials (92/309) there was at least some risk of bias. And the studies with low risk of bias, clearly indicated smaller effect sizes than the ones that had (at least some) risk of bias. Only four of the 15 specific types of therapy had 5 or more trials without risk of bias. And the effects found in these studies were more modest than what was found for all studies (including the ones with risk of bias). <strong>When the studies with low risk of bias were adjusted for publication bias, only two types of therapy remained significant (the \u201cCoping with Depression\u201d course, and self-examination therapy).</strong></p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5vq4z63ilnh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5vq4z63ilnh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g. from the abstract (my emphasis):</p><blockquote><p>The larger effect sizes found in non-Western trials were related to the presence of wait-list controls, high risk of bias, cognitive-behavioral therapy, and clinician-diagnosed depression (<i>p</i> &lt; 0.05). <strong>The larger treatment effects observed in non-Western trials may result from the high heterogeneous study design and relatively low validity</strong>. Further research on long-term effects, adolescent groups, and individual-level data are still needed.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr1ecl473ryp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr1ecl473ryp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Apparently, all that HLI really meant with \"<strong>Excluding outliers</strong> is thought sensible practice here; two related meta-analyses, Cuijpers et al., 2020c; Tong et al., 2023, <strong>used a similar approach</strong>\" [my emphasis] was merely \"[C]onditional on removing outliers, they identify a similar or greater range of effect sizes as outliers as we do.\" (<a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of?commentId=kzduxyCSurEwyXgxG\">see</a>).&nbsp;</p><p>Yeah, right.&nbsp;</p><p>I also had the same impression <a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of?commentId=4BcNTYH6srkiwThQ3\">as Jason</a> that HLI's reply repeatedly strawmans me. The passive aggressive sniping sprinkled throughout and subsequent backpedalling (in fairness, I suspect by people who were <i>not</i> at the keyboard of the corporate account) is less than impressive too. But it's nearly Christmas, so beyond this footnote I'll let all this slide.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwx45kykoec8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwx45kykoec8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Me again (my [re-?]emphasis)</p><blockquote><p>Received opinion is typically that outlier exclusion should be avoided without a clear rationale why the 'outliers' arise from a clearly discrepant generating process. <strong>If it is to be done, the results of the full data should still be presented as the primary analysis</strong></p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0wsfx2j968e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0wsfx2j968e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Said footnote:</p><blockquote><p>If we didn\u2019t first remove these outliers, the total effect for the recipient of psychotherapy would be much larger (see Section 4.1) but some publication bias adjustment techniques would over-correct the results and suggest the completely implausible result that psychotherapy has negative effects (leading to a smaller adjusted total effect). Once outliers are removed, these methods perform more appropriately. These methods are not magic detectors of publication bias. Instead, they make inferences based on patterns in the data, and we do not want them to make inferences on patterns that are unduly influenced by outliers (e.g., conclude that there is no effect \u2013 or, more implausibly, negative effects \u2013 of psychotherapy because of the presence of unreasonable effects sizes of up to 10 gs are present and creating large asymmetric patterns). Therefore, we think that removing outliers is appropriate. See Section 5 and Appendix B for more detail.</p></blockquote><p>The sentence in the main text this is a footnote to says:</p><blockquote><p>Removing outliers this way reduced the effect of psychotherapy and improves the sensibility of moderator and publication bias analyses.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7vh2kz994vi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7vh2kz994vi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Me again:</p><blockquote><p>[W]ithout excluding data, SM drops from ~3.6x GD to ~1.1x GD. Yet it doesn't get a look in for the sensitivity analysis, where HLI's 'less favourable' outlier method involves taking an average of the other methods (discounting by ~10%), but not doing no outlier exclusion at all (discounting by ~70%).</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd8o0fodfstt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd8o0fodfstt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>My remark about \"Even if you didn't pre-specify, presenting your first cut as the primary analysis helps for nothing up my sleeve reasons\" which Dwyer <a href=\"https://forum.effectivealtruism.org/posts/GJ4xjAnubWiBJDbE8/talking-through-depression-the-cost-effectiveness-of?commentId=spfz9KNFmnuWSiC4o\">mentions elsewhere</a> was a reference to '<a href=\"https://en.wikipedia.org/wiki/Nothing-up-my-sleeve_number\">nothing up my sleeve numbers</a>' in cryptography. In the same way picking pi or e initial digits for arbitrary constants reassures the author didn't pick numbers with some ulterior purpose they are not revealing, reporting what one's first analysis showed means readers can compare it to where you ended up after making all the post-hoc judgement calls in the garden of forking paths. \"Our first intention analysis would give x, but we ended up convincing ourselves the most appropriate analysis gives a bottom line of 3x\" would rightly arouse a lot of scepticism.</p><p>I've already mentioned I suspect this is indeed what has happened here: HLI's first cut was including all data, but argued itself into making the choice to exclude, which gave a 3x higher 'bottom line'. Beyond \"You didn't say you'd exclude outliers in your protocol\" and \"basically all of your discussion in the appendix re. outlier exclusion concerns the results of publication bias corrections on the bottom line figures\", I kinda feel HLI <i>not</i> denying it is beginning to invite an adverse inference from silence. If I'm right about this, HLI should come clean.&nbsp;</p></div></li></ol>", "parentCommentId": "tthdc2ZnjAc7npuLH", "user": {"username": "Gregory_Lewis"}}, {"_id": "DHPpyKYvvwafhFzCN", "postedAt": "2023-12-11T11:19:18.000Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": "<p>I'm feeling confused by these two statements:</p><blockquote><p>Although there are other problems, those I have repeated here make the recommendations of the report unsafe.</p></blockquote><p>&nbsp;</p><blockquote><p>Even if one still believes the bulk of (appropriate) analysis paths still support a recommendation, this sensitivity should be made transparent.</p></blockquote><p>The first statement says HLI's recommendation is unsafe, but the second implies it is reasonable as long as the sensitivity is clearly explained. I'm grateful to Greg for presenting the analysis paths which lead to SM &lt; GD, but it's unclear to me how much those paths should be weighted compared to all the other paths which lead to SM &gt; GD.<br><br>It's notable that Cuijpers (who has done more than anyone in the field to account for publication bias and risk of bias) is still confident that <a href=\"https://www.nationalelfservice.net/treatment/psychotherapy/towards-better-psychological-treatment-of-depression-depressionsolvingthetoll-part-3/\">psychotherapy is effective</a>.<br><br>I was also surprised by the use of 'unsafe'. Less cost-effective maybe, but 'unsafe' implies harm and I haven't seen any evidence to support that claim.</p>", "parentCommentId": "RiE9W85dCfxYYsHTD", "user": {"username": "BarryGrimes"}}, {"_id": "t42ZWtnx4J4cJWP7B", "postedAt": "2023-12-08T10:00:14.220Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": null, "parentCommentId": "H82ApbPtm8J9FEXse", "user": null}, {"_id": "3hLian9jHdziA5JGZ", "postedAt": "2023-12-08T10:00:43.057Z", "postId": "GJ4xjAnubWiBJDbE8", "htmlBody": null, "parentCommentId": "H82ApbPtm8J9FEXse", "user": null}]