[{"_id": "EXbBXwzwpd5LjZujt", "postedAt": "2023-09-25T03:12:27.969Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>While I\u2019m very uncertain, on balance I think it provides more serial time to do alignment research. As model capabilities improve and we get more legible evidence of AI risk, the will to pause should increase, and so the expected length of a pause should also increase [footnote explaining that the mechanism here is that the dangers of GPT-5 galvanize more support than GPT-4]</p></blockquote><p>I appreciate flagging the uncertainty; this argument doesn't seem right to me.&nbsp;</p><p>One factor affecting the length of a pause would be the (opportunity cost from pause) / (risk of catastrophe from unpause) ratio of marginal pause days, or what is the ratio of the costs to the benefits. I expect both the costs and the benefits of AI pause days to go up in the future \u2014 because risks of misalignment/misuse are greater, and because AIs will be deployed in a way that adds a bunch of value to society (whether the marginal improvements are huge remains unclear, e.g., GPT-6 might add tons of value, but it's unclear how much more GPT-6.5 adds on top of that, seems hard to tell). I don't know how the ratio will change, which is probably what actually matters. But I wouldn't be surprised if that numerator (opportunity cost) shot up <i>a ton</i>.&nbsp;</p><p>I think it's reasonable to expect that marginal improvements to AI systems in the future (e.g., scaling up 5x) could map on to automating an additional 1-7% of a nation's economy. Delaying this by a month would be a huge loss (or a benefit, depending on how the transition is going).&nbsp;</p><p>What relevant decision makers think the costs and benefits are is what actually matters, not the true values. So even if right now I can look ahead and see that an immediate pause pushes back future tremendous economic growth, this feature may not become apparent to others until later.&nbsp;</p><p>To try and say what I'm getting at a different way: you're suggesting that we get a longer pause if we pause later than if we pause now. I think that \"races\" around AI are going to ~monotonically get worse and that the perceived cost of pausing will shoot up a bunch. If we're early on an exponential of AI creating value in the world, it just seems way easier to pause for longer than it will be later on. If this doesn't make sense I can try to explain more.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Aaron_Scher"}}, {"_id": "smhcmCSnGvmbeYkRX", "postedAt": "2023-09-25T03:22:05.187Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>I think these don\u2019t bite nearly as hard for conditional pauses, since they occur in the future when progress will be slower</p></blockquote><p>Your footnote is about compute scaling, so presumably you think that's a major factor for AI progress, and why future progress will be slower. The main consideration pointing the other direction (imo) is automated researchers speeding things up a lot. I guess you think we don't get huge speedups here until after the conditional pause triggers are hit (in terms of when various capabilities emerge)? If we do have the capabilities for automated researchers, and a pause locks these up, that's still pretty massive (capability) overhang territory.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Aaron_Scher"}}, {"_id": "5cnofLpAc4mkqGnDW", "postedAt": "2023-09-26T03:53:15.143Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>It would be bad to create significant public pressure for a pause through advocacy, because this would cause relevant actors (particularly AGI labs) to spend their effort on looking good to the public, rather than doing what is actually good.</p></blockquote><p>I think I can reasonably model the safety teams at AGI labs as genuinely trying to do good. But I don't know that the AGI labs as organizations are best modeled as trying to do good, rather than optimizing for objectives like outperforming competitors, attracting investment, and advancing exciting capabilities \u2013 subject to some safety-related concerns from leadership. That said, public pressure could manifest itself in a variety of ways, some of which might work toward more or less productive goals.</p><p>I agree that conditional pauses better than unconditional pauses, due to pragmatic factors. But I worry about AGI labs specification gaming their way through dangerous-capability evaluations, using brittle band-aid fixes that don't meaningfully contribute to safety.</p>", "parentCommentId": null, "user": {"username": "michaelchen"}}, {"_id": "wfmoxLEPZ46JNvgBK", "postedAt": "2023-09-26T06:41:47.044Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>Yeah, unless we get a lot better at alignment, the conditional pause should hit well before we create automated researchers.</p>", "parentCommentId": "smhcmCSnGvmbeYkRX", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "2orRXMMEfnAzthwwM", "postedAt": "2023-09-26T06:59:07.545Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>I agree it's important to think about the perceived opportunity cost as well, and that's a large part of why I'm uncertain. I probably should have said that in the post.</p><p>I'd still guess that overall the increased clarity on risks will be the bigger factor -- it seems to me that risk aversion is a much larger driver of policy than worries about economic opportunity cost (see e.g. COVID lockdowns). I would be more worried about powerful AI systems being seen as integral to national security; my understanding is that national security concerns drive a lot of policy. (But this could potentially be overcome with international agreements.)</p>", "parentCommentId": "EXbBXwzwpd5LjZujt", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "9rAmCEqBP3WnE2Qpj", "postedAt": "2023-09-26T07:10:18.274Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>I don't know that the AGI labs as organizations are best modeled as trying to do good, rather than optimizing for objectives like outperforming competitors, attracting investment, and advancing exciting capabilities \u2013 subject to some safety-related concerns from leadership.</p></blockquote><p>I will go further -- it's definitely the latter one for at least Google DeepMind and OpenAI; Anthropic is arguable. I still think that's a much better situation than having public pressure when the ask is very nuanced (as it would be for alignment research).</p><p>For example, I'm currently glad that the EA community does not have the power to exert much pressure on the work done by the safety teams at AGI labs, because the EA community's opinions on what alignment research should be done are bad, and the community doesn't have the self-awareness to notice that themselves, and so instead safety teams at labs would have to spend hundreds of hours writing detailed posts explaining their work to defuse the pressure from the EA community.</p><p>At least there I expect the safety teams could defuse the pressure by spending hundreds of hours writing detailed posts, because EAs will read that. With the public there's no hope of that, and safety teams instead just won't do the work that they think is good, and instead do work that they can sell to the public.</p>", "parentCommentId": "5cnofLpAc4mkqGnDW", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "bsefPBkzsPNF4jWS7", "postedAt": "2023-09-26T07:26:37.135Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>Thanks - I definitely don't completely agree, but it's good to hear that people at the labs take this seriously.&nbsp;</p><p>Given that, I'll respond to your response.&nbsp;</p><blockquote><p>I&nbsp;<i>think</i> the idea here is that we should eventually aim for an international regulatory regime that is not a pause, but a significant chunk of x-risk from misaligned AI is in the near future, so we should enact an unconditional pause right now.</p><p>If so, my main disagreement is that I think the pause we enact right now should be conditional: specifically, I think it\u2019s important that you evaluate the safety of a model after you train it, not before<a href=\"https://forum.effectivealtruism.org/posts/BFbsqwCuuqueFRfpW/aim-for-conditional-pauses#fnwucyjfl46gr\"><sup>[18]</sup></a>. I may also disagree (perhaps controversially) that a significant chunk of x-risk from misaligned AI is in the near future, depending on what \u201cnear future\u201d means.</p></blockquote><p>This seems basically on point, and I think we disagree less than it seems. To explain why, two things are critical.&nbsp;</p><p>The first is that x-risks from models come in many flavors, and uncontrolled AI takeoff is only one longer term concern. If there are nearer term risks from misuse, which I think are even more critical for immediate action, we should be worried about short timelines, in the lower single digit year range. (Perhaps you disagree?) &nbsp;</p><p>The second is that \"near future\" means really different things to engineers, domestic political figures, and treaty negotiators. Political deals take years to make - we're optimistically a year or more from getting anything in place. Even if a bill is proposed today and miraculously passes tomorrow, it likely would not go into effect for months or years, and that's excluding court challenges. I don't think we want to wait to get this started - we're already behind schedule!&nbsp;</p><p>(OpenAI says we have 3-4 years until ASI. I'm more skeptical, but also confused by their opposition to immediate attempts to regulate - by the time we get to AGI, it won't matter if regulations are coming online, abuse risks are stratospheric.</p><p>Either way, the suggestion that we shouldn't pause now is already a done deal , and a delayed pause starting in 2 years, which is what I proposed, until there is a treaty, seems behind schedule already. If you'd prefer that be conditional, that seems fine - though I think we're less than 2 years from hitting the remaining fire alarms, so your pause timelines might create a stop faster than mine. But I think we need something to light a fire to act faster.</p><p>Why? Because years are likely needed just to get people on board for negotiating a real international regulatory regime! <a href=\"https://unfccc.int/process-and-meetings/the-paris-agreement\">Climate treaties take a decade or more to convene, then set their milestones to hit a decade or more in the future</a>, <a href=\"https://en.wikipedia.org/wiki/Strategic_Arms_Limitation_Talks\">narrowly bilateral nuclear arms control treaties talks take years to start, span 3-4 years before there is a treaty, and the resulting treaty has a 30-year time span</a>.&nbsp;</p><p>I'm optimistic that in worlds where risk shows up rapidly, we'll see treaties written and signed faster than this - but I think that either way, pushing hard now makes sense. To explain why, I'll note that I predicted that COVID-19 would create incentives to get vaccines faster than previously seen, and along the same logic, I think the growing and increasingly obvious risks from AGI will cause us to do something similar - but this is conditioned on there being significant and rapid advances that scare people much more, which I think you say you doubt.&nbsp;</p><p>If you think that significant realized risks are further away, so that we have time to wait for a conditional pause, I think it's also unreasonable to predict these shorter time frames. So conditional on what seem to be your beliefs about AI risk timelines, I claim I'm right to push now, and conditional on my tentative concerns about shorter timelines, I claim we're badly behind schedule. Either way, we should be pushing for action now.</p>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "gZLudqmvb9FsfKfeu", "postedAt": "2023-09-26T22:44:03.604Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>\u201cif an AI system would be very dangerous, don\u2019t build it\u201d</p></blockquote><p>This sounds like a recipe for disaster. How do you reliably calibrate what \"very dangerous\" is in advance? It doesn't take much error for something considered merely \"potentially dangerous\" (and so allowed to be built) to actually turn out to be very dangerous. Especially if you consider potential lab leaks during training (e.g. <a href=\"https://www.youtube.com/watch?v=DFtnu0Qn1VM&amp;ab_channel=AlignmentWorkshop\">situationally aware</a> mesaoptimisers emerging).</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "ykomeT779W8L3ibiu", "postedAt": "2023-09-27T06:59:35.747Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>If there are nearer term risks from misuse, which I think are even more critical for immediate action, we should be worried about short timelines, in the lower single digit year range. (Perhaps you disagree?)</p></blockquote><p>I agree, but in that case I'm even more baffled by the call for an unconditional pause. It's so much easier to tell whether an AI system can be misused, relative to whether it is misaligned! Why would you not just ask for a ban on AI systems that pose great misuse risks?</p><p>In the rest of your comment you seem to assume that I think we shouldn't push for action now. I'm not sure why you think that -- I'm very interested in pushing for a conditional pause now, because as you say it takes time to actually implement.</p><p>(I agree that we mostly don't disagree.)</p>", "parentCommentId": "bsefPBkzsPNF4jWS7", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "xJbDA34arrjPD2H9R", "postedAt": "2023-09-27T07:06:17.009Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>You're right, you can't do it. Therefore we should not call for a pause on systems more powerful than GPT-4, because we can't reliably calibrate that systems more powerful than GPT-4 would be plausibly very dangerous. &lt;/sarcasm&gt;</p><p>If you actually engage with what I write, I'll engage with what you write. For reference, here's the paragraph around the line you quoted:</p><blockquote><p>We can get much wider support for a conditional pause. Most people can get on board with the principle \u201cif an AI system would be very dangerous, don\u2019t build it\u201d, and then the relevant details are about when a potential AI system should be considered plausibly very dangerous. At one extreme, a typical unconditional pause proposal would say \u201canything more powerful than GPT-4 should be considered plausibly very dangerous\u201d. As you make the condition less restrictive and more obviously tied to harm, it provokes less and less opposition.</p></blockquote><p>There are also a couple of places where I discuss specific conditional pause proposals, you might want to read those too.</p>", "parentCommentId": "gZLudqmvb9FsfKfeu", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "zAqve6gxb2P69mAiM", "postedAt": "2023-09-27T10:05:13.718Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>You're right, you can't do it. Therefore we should not call for a pause on systems more powerful than GPT-4, because we can't reliably calibrate that systems more powerful than GPT-4 would be plausibly very dangerous.</p></blockquote><p>The second sentence really doesn't follow from the first! I would replace should not with <i>should</i>, and would with <i>would not</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0vu9asko4mk9\"><sup><a href=\"#fn0vu9asko4mk9\">[1]</a></sup></span><i>&nbsp;</i>[Added 10 Oct: for those reading now, the \"&lt;/sarcasm&gt;\" was <a href=\"https://www.dropbox.com/scl/fi/493led7s6oetvixgixo5l/Screenshot-2023-09-27-at-11.23.58.png?rlkey=lxt77n052y1cpgeqlpw32rl4b&amp;dl=0\">not there</a> when I responded]<i>:</i><br><br><strong>We should call for a pause on systems more powerful than GPT-4, because we can't reliably calibrate that systems more powerful than GPT-4 would not be very dangerous.</strong><br><br>We need to employ abundant <i>precaution</i> when dealing with extinction level threats!</p><blockquote><p>At one extreme, a typical unconditional pause proposal would say \u201canything more powerful than GPT-4 should be considered plausibly very dangerous\u201d.</p></blockquote><p>I really don't think this is extreme at all. Outside of the major AI labs and their lobbys (and <a href=\"https://forum.effectivealtruism.org/posts/vwK3v3Mekf6Jjpeep/let-s-think-about-slowing-down-ai-1#comments:~:text=Others%3A%20oh,we%20aren%E2%80%99t%20delusional\">this community</a>), I think this is mostly regarded as normal and <a href=\"https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll\">sensible</a>.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0vu9asko4mk9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0vu9asko4mk9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>and remove the \"plausibly\"</p></div></li></ol>", "parentCommentId": "xJbDA34arrjPD2H9R", "user": {"username": "Greg_Colbourn"}}, {"_id": "toz5NazoRkRWckELy", "postedAt": "2023-09-27T13:32:11.237Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>I don't think you can make general AI systems that are powerful enough to be used for misuse that aren't powerful enough to pose risks of essentially full takeoff. I <a href=\"https://forum.effectivealtruism.org/posts/pbrJduve9kLA2yiZq/what-is-autonomy-and-how-does-it-lead-to-greater-risk-from\">talked about this in terms of autonomy</a>, but think the examples used illustrate why these are nearly impossible to separate. Misuse risk isn't an unfortunate correlate of greater general capabilities, it's exactly the same thing.</p><p>That said, if you're not opposed to immediate steps, I think that where we differ most is a strategic detail about how to get to the point where we have a robust international system that conditionally bans only potentially dangerous systems, namely, whether we need a blanket ban on very large models until that system is in place.</p>", "parentCommentId": "ykomeT779W8L3ibiu", "user": {"username": "Davidmanheim"}}, {"_id": "k7Q4zkmfXANhLBg8n", "postedAt": "2023-09-28T07:07:02.865Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>I think that where we differ most is a strategic detail about how to get to the point where we have a robust international system that conditionally bans only potentially dangerous systems, namely, whether we need a blanket ban on very large models until that system is in place.</p></blockquote><p>Yes, but it seems like an important detail, and I still haven't seen you give a single reason why we should do the blanket ban instead of the conditional one. (I named four in my post, but I'm not sure if you care about any of them.)</p><p>The conditional one should get way more support! That seems really important! Why is everyone dying on the hill of a blanket ban?</p><blockquote><p>I don't think you can make general AI systems that are powerful enough to be used for misuse that aren't powerful enough to pose risks of essentially full takeoff.</p></blockquote><p>There are huge differences between misuse threat models and misalignment threat models, even if you set aside whether models will be misaligned in the first place:</p><ol><li>In misuse, an AI doesn't have to hide its bad actions (e.g. it can do chains of thought that explicitly reason about how to cause harm).</li><li>In misuse, an AI gets active help from humans (e.g. maybe the AI is great at creating bioweapon designs, but only the human can get them synthesized and released into the world).</li></ol><p>It's possible that the capabilities to overcome these issues comes at the same time as the capability for misuse, but I doubt that will happen.</p><p>Also, this seems to contradict your previous position:</p><blockquote><p>The first is that x-risks from models come in many flavors, and uncontrolled AI takeoff is only one longer term concern. If there are nearer term risks from misuse, which I think are even more critical for immediate action, we should be worried about short timelines, in the lower single digit year range. &nbsp;</p></blockquote>", "parentCommentId": "toz5NazoRkRWckELy", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "tYAb6AYvvGDN87def", "postedAt": "2023-09-28T07:35:41.263Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>we can't reliably calibrate that systems more powerful than GPT-4 would not be very dangerous.</p></blockquote><p>Seems false (unless you get Pascal's Mugged by the word \"reliably\").</p><p>We're pretty good at evaluating models at GPT-4 levels of capability, and they <a href=\"https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/\">don't seem capable</a> of autonomous replication. (I believe that evaluation is flawed because ARC didn't have finetuning access, but we could do better evaluations.) I don't see any good reason to expect this to change for GPT-4.5.</p><p>Or as a more social argument: I can't think of a single ML expert who'd agree with your claim (including ones who work in alignment, but excluding alignment researchers who aren't ML experts, so e.g. not Connor Leahy or Eliezer Yudkowsky). Probably some do exist, but it seems like it would be a tiny minority.</p><blockquote><p>I really don't think this is extreme at all.</p></blockquote><p>I was using the word extreme to mean \"one end of the spectrum\" rather than \"crazy\". I've changed it to \"end\" instead.</p><p>Though I do think it would both be an overreaction and would increase x-risk, so I am pretty strongly against it.</p>", "parentCommentId": "zAqve6gxb2P69mAiM", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "Ze2zMDfDmqm2uTGpZ", "postedAt": "2023-09-28T08:52:44.173Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>unless you get Pascal's Mugged by the word \"reliably\"</p></blockquote><p>I don't think it's a case of Pascal's Mugging. Given the stakes (extinction), even a 1% risk of a lab leak for a next gen model is more than enough to not build it (I think we're there already).</p><blockquote><p>who aren't ML experts, so e.g. not Connor Leahy</p></blockquote><p>Connor Leahy is an ML expert (he co-founded EleutherAI before realising x-risk was an massive issue).</p><blockquote><p>I don't see any good reason to expect this to change for GPT-4.5.</p></blockquote><p>To me this sounds like you are expecting scaling laws to break? Or not factoring it being given access to other tools such as planners (AutoGPT etc) or plugins.</p><blockquote><p>Though I do think it would both be an overreaction and would increase x-risk, so I am pretty strongly against it.</p></blockquote><p>How would it increase x-risk!? We're not talking about a temporary pause with potential for an overhang. Or a local pause with potential for less safe actors to race ahead. The only sensible (and, I'd say, realistic) pause is global and indefinite, until global consensus on x-safety or global democratic mandate to proceed; and lifted gradually to avoid sudden jumps in capability.&nbsp;<br><br>I think you are also likely to be quite biased if you are, in fact, working for (and being paid good money by) a Major AI Lab (why the anonymity?)</p>", "parentCommentId": "tYAb6AYvvGDN87def", "user": {"username": "Greg_Colbourn"}}, {"_id": "XduwC4pYpPnpKdxGc", "postedAt": "2023-09-28T10:02:03.567Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote>\n<p>I think that the best work on AI alignment happens at the AGI labs</p>\n</blockquote>\n<p>Based on your other discussion e.g. about public pressure on labs, it seems like this might be a (minor?) loadbearing belief?</p>\n<p>I appreciate that you qualify this further in a footnote</p>\n<blockquote>\n<p>This is a controversial view, but I\u2019d guess it\u2019s a majority opinion amongst AI alignment researchers.</p>\n</blockquote>\n<p>I just wanted to call out that I weakly hold the opposite position, and also opposite best guess on majority opinion (based on safety researchers I know). Naturally there are sampling effects!</p>\n<p>This is a marginal sentiment, and I certainly wouldn't trade all lab researchers for non-lab researchers or vice versa. Diversification of research settings seems quite precious, and the dialogue is important to preserve.</p>\n<p>I also question</p>\n<blockquote>\n<p>Reasons include: access to the best alignment talent,</p>\n</blockquote>\n<p>because a lot of us are very reluctant to join AGI labs, for obvious reasons! I know folks inside and outside of AGI labs, and it seems to me that the most talented are among the outsiders (but this also definitely could be an artefact of sample sizes).</p>\n", "parentCommentId": null, "user": {"username": "Oliver Sourbut"}}, {"_id": "iygE6DanBp9N6LwaE", "postedAt": "2023-09-28T12:29:46.220Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>I didn't say \"blanket ban on ML,\" I said \"a blanket ban on very large models.\"<br><br>Why? Because I have not seen, and don't think anyone can make, a clear criterion for \"too high risk of doom,\" both because there are value judgements, and because we don't know how capabilities arise - so there needs to be some cutoff past which we ban everything, until we have an actual regulatory structure in place to evaluate. Is that different than the \"conditional ban\" you're advocating? If so, how?</p>", "parentCommentId": "k7Q4zkmfXANhLBg8n", "user": {"username": "Davidmanheim"}}, {"_id": "EK8gzxEmvBgue4SNT", "postedAt": "2023-09-29T07:23:59.237Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>Every comment of yours so far has misunderstood or misconstrued at least one thing I said, so I'm going to bow out now.</p>", "parentCommentId": "Ze2zMDfDmqm2uTGpZ", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "arqe7vHeGvg2yqcJ5", "postedAt": "2023-09-29T07:33:27.986Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>I didn't say \"blanket ban on ML,\" I said \"a blanket ban on very large models.\"</p></blockquote><p>I know? I never said you asked for a blanket ban on ML?</p><blockquote><p>Because I have not seen, and don't think anyone can make, a clear criterion for \"too high risk of doom,\"</p></blockquote><ol><li>My post discusses \u201cpause once an agent passes 10 or more of the&nbsp;<a href=\"https://evals.alignment.org/blog/2023-08-01-new-report/\"><u>ARC Evals tasks</u></a>\u201d. I think this is too weak a criterion and I'd argue for a harder test, but I think this is already better than a blanket ban on very large models.</li><li>Anthropic just <a href=\"https://www.anthropic.com/index/anthropics-responsible-scaling-policy\">committed</a> to a conditional pause.</li><li>ARC Evals is pushing <a href=\"https://evals.alignment.org/blog/2023-09-26-rsp/\">responsible scaling</a>, which is a conditional pause proposal.</li></ol><p>But also, the blanket ban on very large models is implicitly saying that \"more powerful than GPT-4\" is the criterion of \"too high risk of doom\", so I really don't understand at all where you're coming from.</p>", "parentCommentId": "iygE6DanBp9N6LwaE", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "KNFztf7eumnSPoD3e", "postedAt": "2023-09-29T07:34:26.976Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>Thank you for writing this post, I think I learnt a lot from it (including about things I didn't expect I would, such as waste sites and failure modes in cryonics advocacy - excellent stuff!).</p><p>Question for anyone to chip in on:</p><p>I'm wondering whether if we're to make the \"conditional pause system\" the post is advocating for universal, would it imply that the alignment community needs to drastically scale up (in terms of quantity of researchers) to be able to do similar work to what ARC Evals is doing?&nbsp;</p><p>After all, someone would actually need to check if systems at a given capability are safe, and as the post argues, you would not want AGI labs to do it for themselves. However, if all current top labs were to start throwing their cutting-edge models at ARC Evals, I imagine they would be quite overwhelmed. (And the demand for evals would just increase over time)</p><p>I could see this being less of an issue if the evaluations only need to happen for the models that are really <strong>the most</strong> capable at a given point in time, but my worry would be that as capabilities increase, even if we test the top models rigorously, the second-tier models could still end up doing harm.&nbsp;</p><p>(I guess it would also depend on whether you can \"reuse\" some of the insights you gain from evaluating the top models at a given time on the second-tier models at a given time, but I certainly don't know enough about this topic to know if that would be feasible)</p>", "parentCommentId": null, "user": {"username": "gergogaspar"}}, {"_id": "Mgnp6CCsrGjhsCfX2", "postedAt": "2023-09-29T07:48:37.561Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>Yes, in the longer term you would need to scale up the evaluation work that is currently going on. It doesn't have to be done by the alignment community; there are lots of capable ML researchers and engineers who can do this (and I expect at least some would be interested in it).</p><blockquote><p>I could see this being less of an issue if the evaluations only need to happen for the models that are really <strong>the most</strong> capable at a given point in time</p></blockquote><p>Yes, I think that's what you would do.</p><blockquote><p>my worry would be that as capabilities increase, even if we test the top models rigorously, the second-tier models could still end up doing harm.&nbsp;</p></blockquote><p>The proposal would be that if the top models are risky, then you pause. So, if you haven't paused, then that means your tests concluded that the top models aren't risky. In that case I don't know why you expect the second-tier models to be risky?</p>", "parentCommentId": "KNFztf7eumnSPoD3e", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "gdabE9yxs3sZhbdb2", "postedAt": "2023-09-29T08:28:18.123Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>I think the crux boils down to you basically saying <i>\"we can't be certain that it would be very dangerous, therefore we should build it to find out\".</i> This, to me, it totally reckless when it comes to the stakes being extinction (we really do not want to be <a href=\"https://www.urbandictionary.com/define.php?term=FAFO#:~:text=FAFO%20is%20an%20abbreviation%20for%20the%20term%20%22Fuck%20Around%20and%20Find%20Out%22%2C%20which%20is%20commonly%20used%20by%20software%20developers%20in%20chats%20when%20referring%20to%20certain%20programming%20techniques%20they%20do%20not%20know%20or%20understand%20but%20try%20to%20make%20use%20of%20anyway.\">FAFO</a>-ing this! Where is your <a href=\"https://intelligence.org/2015/12/23/need-scale-miris-methods/#:~:text=Someone%20has%20pointed,in%20the%20field.\">security mindset?</a>). You don't seem to put much (<i>any?</i>) weight on lab leaks during training (as a result of emergent <a href=\"https://www.youtube.com/watch?v=DFtnu0Qn1VM&amp;ab_channel=AlignmentWorkshop\">situational awareness</a>). \"Responsible Scaling\" <a href=\"https://twitter.com/NikSamoylov/status/1707166875019604211\">is</a> <a href=\"https://twitter.com/michael_nielsen/status/1707180385891955039\">anything</a> <a href=\"https://twitter.com/Simeon_Cps/status/1707146252498964645\">but</a>, in the situation we are now in.<br><br>Also, the disadvantages you mention around Goodharting make me think that the only sensible way to proceed is to just <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">shut it all down</a>.<br><br>You say that you disagree with <a href=\"https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6#Fast_takeoff_has_a_really_bad_feedback_loop\">Nora</a> over alignment optimism, but then also that you \"strongly disagree\" with \"the premise that if smarter-than-human AI is developed in the near future, then we almost surely die, regardless of who builds it\" (<a href=\"https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/fSeDA7B7Hve5LeaWq\">Rob's post</a>). I think you are also way too optimistic about alignment work on it's current trajectory actually leading to x-safety by saying this.</p>", "parentCommentId": "EK8gzxEmvBgue4SNT", "user": {"username": "Greg_Colbourn"}}, {"_id": "oY4a4zEEi9KjqSKMf", "postedAt": "2023-09-29T10:51:57.214Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p><strong>Iterative deployment.</strong> We treat AGI like we would treat many other new technologies: something that could pose risks, which we should think about and mitigate, but ultimately something we should learn about through iterative deployment. The default is to deploy new AI systems, see what happens with a particular eye towards noticing harms, and then design appropriate mitigations. In addition, rollback mechanisms ensure that we can AI systems are deployed with a rollback mechanism, so that if a deployment causes significant harms</p><p>[...]<br><br><strong>Conditional pause.</strong> We institute regulations that say that capability improvement must pause once the AI system hits a particular threshold of riskiness, as determined by some relatively standardized evaluations, with some room for error built in. AI development can only continue once the developer has exhibited sufficient evidence that the risk will not arise.</p></blockquote><p>Compared to you, I'm more pessimistic about these two measures. On iterative deployment, I'm skeptical about the safety of rollback mechanisms. On conditional pause, I agree it makes total sense to pause at the latest point possible as long as things are still pretty likely to be safe. However, I don't see why we aren't already at that point.<br><br>I suspect that our main crux might be a disagreement over takeoff speeds, and perhaps AI timelines being another (more minor) crux?</p><p>On takeoff speeds, I place 70% on 0.5-2.5 orders of magnitude for what Tom Davidson calls the <a href=\"https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/\">FLOP gap. </a>(Relatively high robustness because I thought about this a lot.) I also worry less about metrics of economic growth/economic change because I believe it won't take long from \"AI makes a noticeable dent on human macroeconomic productivity\" to \"it's now possible to run millions of AIs that are ~better/faster at all tasks human experts can do on a computer.\" The latter scenario is one from which it is easy to imagine how AIs might disempower humans. I basically don't see how one can safely roll things back at the point where generally smarter-than-human AIs exist that can copy themselves millionfold on the internet.</p><p>On timelines, I have maybe 34% on 3 years and less, and 11% on 1 year.&nbsp;</p><p>Why do I have these views?</p><p>A large part of the story is that if you had described to me back in 2018 all the AI capabilities we have today, without mentioning the specific year by which we'd have those capabilities, I'd have said \"once we're there, we're probably very close to transformative AI.\" And now that we are at this stage, even though it's much sooner than I'd have expected, I feel like the right direction of update is \"AI timelines are sooner than expected\" rather than \"(cognitive) takeoff speeds must be slower than I'd have expected.\"&nbsp;</p><p>Maybe this again comes down to my specific view on takeoff speeds. I felt more confident that takeoff won't be super slow than I felt confident about anything timelines-related.&nbsp;</p><p>So, why the confident view on takeoff speeds? Just looking at humans vs chimpanzees, I'm amazed by the comparatively small difference in brain size. We can also speculate, based on the way evolution operates, that there's probably not much room for secret-sauce machinery in the human brain (that chimpanzees don't already have).&nbsp;</p><p>The main counterargument from the slow(er) takeoff crowd on the chimpanzees vs. humans comparison is that humans faced much stronger selection pressure for intelligence, which must have tweaked a lot of other things besides brain size, and since chimpanzees didn't face that same selection pressure, the evolutionary comparison underestimates how smart an animal with a chimpanzee-sized brain <i>would be</i> if it had also undergone strong selection pressure for the sort of niche that humans inhabit (\"intelligence niche\"). I find that counterargument slightly convincing, but not convincing enough to narrow my FLOP gap estimates too much. Compared to ML progress where we often 10x compute between models, evolution was operating way more slowly.</p><p>As I've written in an (unpublished) review of (a draft of) the FLOP gap report:</p><blockquote><p>I\u2019ll now reply to sections in the report where Tom discusses evidence for the FLOP gap and point out where I\u2019d draw different conclusions. I\u2019ll start with the argument that I consider the biggest crux between Tom and me.</p><p>Tom seems to think chimpanzees \u2013 or even rats, with much smaller probability \u2013 could plausibly automate a significant percent of cognitive tasks \u201cif they had been thoroughly evolutionarily prepared for this\u201d (I\u2019m paraphrasing). A related argument in this spirit is Paul Christiano\u2019s&nbsp;<a href=\"https://sideways-view.com/2018/02/24/takeoff-speeds/\"><u>argument</u></a> that humans far outclass chimpanzees not because of a discontinuity in intelligence, but because chimpanzees hadn\u2019t been naturally selected to be good at a sufficiently general (or&nbsp;<i>generalizable</i>) range of skills.<br><br>I think this argument is in tension with the observation that people at the lower range of human intelligence (as measured by IQ, for instance) tend to struggle to find and hold jobs. If natural selection had straightforwardly turned all humans into specialists for \u201csomething closely correlated with economically useful tasks,\u201d then how come the range for human intelligence is still so wide? I suspect that the reason humans were selected for (general) intelligence more than chimpanzees is&nbsp;<i>because of</i> some \u201cdiscontinuity\u201d in the first place.&nbsp;<a href=\"https://www.lesswrong.com/posts/QqHhr9anrSnZRHCxf/why-so-much-variance-in-human-intelligence\"><u>This comment</u></a> by Carl Shulman explains it as follows (my emphasis in bold):</p><p>\u201cHominid culture took off enabled by human capabilities [<strong>so we are not incredibly far from the minimum need for strongly accumulating culture,</strong> the selection effect you reference in the post], and kept rising over hundreds of thousands and millions of years, at accelerating pace as the population grew with new tech, expediting further technical advance.\u201d</p><p>Admittedly, Carl also writes the following:&nbsp;</p><p>\u201cDifferent regions advanced at different rates (generally larger connected regions grew faster, with more innovators to accumulate innovations), but all but the smallest advanced.&nbsp;<strong>So if humans overall had lower cognitive abilities there would be slack for technological advance to have happened anyway, just at slower rates (perhaps manyfold), accumulating more by trial and error</strong>.\u201d</p><p>So, perhaps chimpanzees (or bonobos), if they had been evolutionarily prepared for social learning or for performing well in the economy, could indeed perform about 20% of today\u2019s tasks. But that\u2019s also my cutoff point: I think we can somewhat confidently rule out that smaller-brained primates, let alone rodents such as rats, could do the same thing in this hypothetical. (Or, in case I\u2019m wrong, it would have to be because it\u2019s possible to turn all of the rodent\u2019s cognition into a highly specialized \u201ctool\u201d that exploits various advantages it can reach over humans \u2013 enabling partial automation of specific workflows, but not full automation of sectors.)</p></blockquote>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "LiyfDrfXwRp28edLX", "postedAt": "2023-09-29T11:10:07.253Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>Interesting post!&nbsp;<br><br>I like your points against the value of public advocacy. I'm not convinced overall, but that's probably mostly because I'm in an overall more pessimistic state where I don't mind trying more desperate measures.&nbsp;</p><p>One small comment on incentives for alignment researchers:</p><blockquote><p>Goodhart\u2019s Law (labs): Safety teams face large pressure to do things that would look good to the public, which is different from doing work that is actually good[9].</p></blockquote><p>I feel like there are already a bunch of misaligned incentives for alignment researchers (and all groups of people in general), so if the people on the safety team aren't selected to be great at caring about the best object-level work, then we're in trouble either way. My model might be too \"black and white,\" but I basically think that people fall into categories where they are either lost causes or they have trained themselves cognitive habits about how to steer clear of distorting influences. Sure, the incentive environment still makes a difference, but I find it unlikely that it's among the most important considerations for evaluating the value of masses-facing AI pause advocacy.</p>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "gSQyvxJy8xEfc2GTt", "postedAt": "2023-09-29T12:17:03.637Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>While I\u2019m uncertain how negative this effect is overall,&nbsp;<a href=\"https://www.openphilanthropy.org/research/some-case-studies-in-early-field-growth/#3-case-studies-i-investigated-less-thoroughly\"><u>this brief analysis of cryonics and molecular nanotechnology</u></a> is quite worrying; it almost reads like a forecast about the field of AI alignment.</p></blockquote><p>I think we are way past this point. Cryonics and molecular nanotechnology have never got to the point where there are Nobel Prize winners in Medicine and Chemistry advocating for them. (Whereas AI x-safety now has <a href=\"https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence#:~:text=Geoffrey%20Hinton%2C%5B7%5D%20Yoshua%20Bengio\">Hinton and Bengio</a>, Turing Award winners<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnw0jzl8wywb\"><sup><a href=\"#fnnw0jzl8wywb\">[1]</a></sup></span>, advocating for it.)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnw0jzl8wywb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnw0jzl8wywb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>the <a href=\"https://en.wikipedia.org/wiki/Turing_Award#:~:text=It%20is%20generally%20recognized%20as%20the%20highest%20distinction%20in%20computer%20science%20and%20is%20colloquially%20known%20as%20or%20often%20referred%20to%20as%20the%20%22Nobel%20Prize%20of%20Computing%22.%5B3%5D%5B4%5D%5B5%5D%5B6%5D\">closest thing to a Nobel Prize in Computer Science</a>.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "urfw5SajnwQJg9XBj", "postedAt": "2023-09-30T08:07:15.337Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>A conditional pause fails to prevent x-risk if either:</p><ul><li>The AI successfully <a href=\"https://aligned.substack.com/p/self-exfiltration\">exfiltrates itself</a> (which is what's needed to defeat rollback mechanisms) during training or evaluation, but before deployment.</li><li>The AI successfully <a href=\"https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations\">sandbags</a> the evaluations. (Note that existing conditional pause proposals depend on capability evaluations, not alignment evaluations.)</li></ul><p>(Obviously there's also the normal institutional failures, e.g. if a company simply ignores the evaluation requirements and forges ahead. I'm setting those aside here.)</p><p>Both of these seem extremely difficult to me (likely beyond human-level, in the sense that if you somehow put a human in the situation the AI would be in, I would expect the human to fail).</p><p>How likely do you think it is that we get an AI capable of one of these failure modes, <i>before</i> we see an AI capable of e.g. passing 10 out of the 12 ARC Evals tasks? My answer would be \"negligible\", and so I'm at least in favor of \"pause once you pass 10 out of 12 ARC Evals tasks\" over \"pause now\". I think we can raise the difficulty of the bar a decent bit more before my answer stops being \"negligible\".</p><p>I don't think this depends on takeoff speeds at all, since I'd expect a conditional pause proposal to lead to a pause well before models are automating 20% of tasks (assuming no good mitigations in place by that point).</p><p>I don't think this depends on timelines, except inasmuch as short timelines correlates with discontinuous jumps in capability. If anything it seems like shorter timelines argue more strongly for a conditional pause proposal, since it seems far easier to build support for and enact a conditional pause.</p>", "parentCommentId": "oY4a4zEEi9KjqSKMf", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "bZwwEvfp6Fnnzf6FZ", "postedAt": "2023-09-30T08:22:49.980Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>I feel like there are already a bunch of misaligned incentives for alignment researchers (and all groups of people in general), so if the people on the safety team aren't selected to be great at caring about the best object-level work, then we're in trouble either way.</p></blockquote><p>I'm not thinking that much about the motivations of the people on the safety team. I'm thinking of things like:</p><ol><li>Resourcing for the safety team (hiring, compute, etc) is conditioned on whether the work produces big splashy announcements that the public will like</li><li>Other teams in the company that are doing things that the public likes will be rebranded as safety teams</li><li>When safety teams make recommendations for safety interventions on the strongest AI systems, their recommendations are rejected if the public wouldn't like them</li><li>When safety teams do outreach to other researchers in the company, they have to self-censor for fear that a well-meaning whistleblower will cause a PR disaster by leaking an opinion of the safety team that the public has deemed to be wrong</li></ol><p>See also <a href=\"https://www.lesswrong.com/posts/PrCmeuBPC4XLDQz8C/unconscious-economics\">Unconscious Economics</a>.</p><p>(I should have said that companies will face pressure, rather than safety teams. I'll edit that now.)</p>", "parentCommentId": "LiyfDrfXwRp28edLX", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "RJv5PHLBPPZpyjuJB", "postedAt": "2023-09-30T08:58:38.706Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>I like your points against the value of public advocacy. I'm not convinced overall, but that's probably mostly because I'm in an overall more pessimistic state where I don't mind trying more desperate measures.&nbsp;</p></blockquote><p>People say this a lot, but I don't get it.</p><ol><li>Your baseline has to be <i>really</i> pessimistic before it looks good to throw in a negative-expectation high-variance intervention. (Perhaps worth making some math models and seeing when it looks good vs not.) Afaict only MIRI-sphere is pessimistic enough for this to make sense.</li><li>It's very uncooperative and unilateralist. I don't know why exactly it has became okay to say \"well <i>I</i> think alignment is doomed, so it's fine if I ruin everyone else's work on alignment with a negative-expectation intervention\", but I dislike it and want it to stop.</li></ol><p>Or to put it a bit more viscerally: It feels crazy to me that when I say \"here are reasons your intervention is increasing x-risk\", the response is \"I'm pessimistic, so actually while I agree that the effect in a typical world is to increase x-risk, it turns out that there's this tiny slice of worlds where it made the difference and that makes the intervention good actually\". It could be true, but it sure throws up a lot of red flags.</p><p>Or in meme form:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RJv5PHLBPPZpyjuJB/gesoicgnpsatlas83sul\"></figure>", "parentCommentId": "LiyfDrfXwRp28edLX", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "4aiajnWnBGzs7K6bg", "postedAt": "2023-09-30T09:05:08.048Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>it seems like this might be a (minor?) loadbearing belief?</p></blockquote><p>Yes, if I changed my mind about this I'd have to rethink my position on public advocacy. I'm still pretty worried about the other disadvantages so I suspect it wouldn't change my mind overall, but I would be more uncertain.</p>", "parentCommentId": "XduwC4pYpPnpKdxGc", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "e6ro38cPAJCfRcW5o", "postedAt": "2023-09-30T09:28:14.510Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>I agree that unilateralism is bad. I'm still in discussion mode rather than confidently advocating for some specific hard-to-reverse intervention. (I should have flagged that explicitly.)</p><p>I think it's not just the MIRI-sphere that's very pessimistic, so there might be a situation where two camps disagree but neither camp is obviously small enough to be labelled unilateralist defectors. Seems important to figure out what to do from a group-rationality perspective in that situation. Maybe the best thing would be to agree on predictions that tell us what world we're more likely in, and then commit to a specific action once one group turned out to be wrong about their worldview's major crux/cruxes. (Assuming we have time for that.)&nbsp;</p><blockquote><p>It feels crazy to me that when I say \"here are reasons your intervention is increasing x-risk\", the response is \"I'm pessimistic, so actually while I agree that the effect in a typical world is to increase x-risk, it turns out that there's this tiny slice of worlds where it made the difference and that makes the intervention good actually\".</p></blockquote><p>That's not how I'd put it. I think we are still in a \"typical\" world, but the world that optimistic EAs assume we are in is the unlikely one where institutions around AI development an deployment suddenly turn out to be saner than our baseline would suggest. (If someone had strong reasons to think something like \"[leader of major AI lab] is a leader with exceptionally high integrity who cares the most about doing the right thing; his understanding of the research and risks is pretty great, and he really knows how to manage teams and so on, so that's why I'm confident,\" then I'd be like \"okay, that makes sense.\")</p>", "parentCommentId": "RJv5PHLBPPZpyjuJB", "user": {"username": "Lukas_Gloor"}}, {"_id": "6DAeL8hqpX2jcJNnH", "postedAt": "2023-09-30T09:49:31.037Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>I don't think this depends on takeoff speeds at all, since I'd expect a conditional pause proposal to lead to a pause well before models are automating 20% of tasks (assuming no good mitigations in place by that point).</p></blockquote><p>I should have emphasized that I'm talking about <i>cognitive</i> AI takeoff, not economic takeoff.&nbsp;</p><p>I don't have a strong view whether there are ~20% of human tasks that are easy and regular/streamlined enough to automate with stochastic-parrot AI tools. Be that as it may, what's more important is what happens once AIs pass the reliability threshold that makes someone a great \"general assistant\" in all sorts of domains. From there, I think it's just a tiny step further to also being a great CEO. Because these capability levels are so close to each other on my model, the world may still look similar to ours at that point.</p><p>All of that said, it's not like I consider it particularly likely that a system would blow past all the evals you're talking about in a single swoop, especially since some of them will be (slightly) <i>before</i> the point of being a great \"general assistant.\" I also have significant trust that the people designing these evals will be thinking about these concerns. I think it's going to be very challenging to make sure evals organizations (or evals teams inside labs in case it's done lab-internally) have enough political power and stay uncorrupted by pressures to be friendly towards influential lab leadership. These problems are surmountable in theory, but I think it'll be hard, so I'm hoping the people working on this are aware of all that could go wrong. I recently wrote up some quick thoughts on safety evals <a href=\"https://docs.google.com/document/d/1bcyeX1C4ujMGMsRqII7CgmNCLA6gitwpeaMTC6Bfgf0/edit\">here</a>. Overall, I'm probably happy enough with a really well-thought out \"conditional pause\" proposal, but I'd need to be reassured that the people who decide in favor of that can pass the Ideological Turing test for positions like fast takeoff or the point that economic milestones like \"20% of tasks are automated\" are probably irrelevant.&nbsp;</p>", "parentCommentId": "urfw5SajnwQJg9XBj", "user": {"username": "Lukas_Gloor"}}, {"_id": "BCNWRrRtsNB8HGD3T", "postedAt": "2023-09-30T10:47:34.159Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>That's not how I'd put it. I think we are still in a \"typical\" world, but the world that optimistic EAs assume we are in is the unlikely one where institutions around AI development an deployment suddenly turn out to be saner than our baseline would suggest.</p></blockquote><p>I don't see at all how this justifies that public advocacy is good? From my perspective you're assuming we're in an unlikely world where the public turns out to be saner than our baseline would suggest. I don't think I have a lot of trust in institutions (though maybe I do have more trust than you do); I think I have a deep distrust of politics and the public.</p><p>I'm also not sure I understood your original argument any more. The argument I thought you were making was something like:</p><blockquote><p>Consider an instrumental variable like \"quality-weighted interventions that humanity puts in place to reduce AI x-risk\". Then public advocacy is:</p><ol><li>Negative expectation: Public advocacy reduces the expected value of quality-weighted interventions, for the reasons given in the post.</li><li>High variance: Public advocacy also increases the variance of quality-weighted interventions (e.g. maybe we get a complete ban on all AI, which seems impossible without public advocacy).</li></ol><p>However, I am pessimistic:</p><ol><li>Pessimism: The required quality-weighted interventions to avoid doom is much higher than the default quality-weighted interventions we're going to get.</li></ol><p>Therefore, even though public advocacy is negative-expectation on quality-weighted interventions, it still reduces p(doom) due to its high variance.</p></blockquote><p>(This is the only way I see to justify rebuttals like \"I'm in an overall more pessimistic state where I don't mind trying more desperate measures\", though perhaps I'm missing something.)</p><p>Is this what you meant with your original argument? If not, can you expand?</p><blockquote><p>I think it's not just the MIRI-sphere that's very pessimistic</p></blockquote><p>What is your p(doom)?</p><p>For reference, I think it seems crazy to advocate for negative-expectation high-variance interventions if you have p(doom) &lt; 50%. As a first pass heuristic, I think it still seems pretty unreasonable all the way up to p(doom) of &lt; 90%, though this could be overruled by details of the intervention (how negative is the expectation, how high is the variance).</p>", "parentCommentId": "e6ro38cPAJCfRcW5o", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "7ZJCGcMynZ6hochwH", "postedAt": "2023-09-30T11:01:49.703Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<p>Sounds like we roughly agree on actions, even if not beliefs (I'm less sold on fast / discontinuous takeoff than you are).</p><p>As a minor note, to keep incentives good, you could pay evaluators / auditors based on how much performance they are able to elicit. You could even require that models be evaluated by at least three auditors, and split up payment between them based on their relative performances. In general it feels like there a huge space of possibilities that has barely been explored.</p>", "parentCommentId": "6DAeL8hqpX2jcJNnH", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "o9raTBxmCtqSymdLX", "postedAt": "2023-09-30T12:12:01.478Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>From my perspective you're assuming we're in an unlikely world where the public turns out to be saner than our baseline would suggest.</p></blockquote><p>Hm, or that we get lucky in terms of the public's response being a good one given the circumstances, even if I don't expect the discourse to be nuanced. It seems like a reasonable stance to think that a crude reaction of \"let's stop this research before it's too late\" is appropriate as a first step, and that it's okay to worry about other things later on. The negatives you point out are certainly significant, so if we could get a conditional pause setup through other channels, that seems clearly better! But my sense is that it's unlikely we'd succeed at getting ambitious measures in place without some amount of public pressure. (For what it's worth, I think the public pressure is already mounting, so I'm not necessarily saying we have to ramp up the advocacy side a lot \u2013 I'm definitely against forming <i>PETA-style</i> anti-AI movements.)</p><blockquote><p>As a first pass heuristic, I think it still seems pretty unreasonable all the way up to p(doom) of &lt; 90%,</p></blockquote><p>It also matters how much weight you give to person-affecting views (I've argued <a href=\"https://forum.effectivealtruism.org/posts/dQvDxDMyueLyydHw4/population-ethics-without-axiology-a-framework\">here</a> for why I think they're not unreasonable). If we can delay AI takeoff for five years, that's worth a lot from the perspective of currently-existing people! (It's probably also weakly positive or at least neutral from a suffering-focused longtermist perspective because everything seems uncertain from that perspective and a first-pass effect is delaying things from getting bigger; though I guess you could argue that particular s-risks are lower if more alignment-research-type reflection goes into AI development.) Of course, buying a delay that somewhat (but not tremendously) worsens your chances later on is a huge cost to upside-focused longtermism. But if we assume that we're already empirically pessimistic on that view to begin with, then it's an open question how a moral parliament between worldviews would bargain things out. Certainly the upside-focused longtermist faction should get important concessions like \"try to ensure that actually-good alignment research doesn't fall under the type of AI research that will be prohibited.\"</p><blockquote><p>What is your p(doom)?</p></blockquote><p>My all-things-considered view (the one I would bet on) is maybe 77%. My private view (what to report to avoid double-counting the opinions of people the community updates towards) is more like 89%. (This doesn't consider scenarios where AI is misaligned but still nice towards humans for weird decision-theoretic reasons where the AI is cooperating with other AIs elsewhere in the multiverse \u2013 not that I consider that particularly likely, but I think it's too confusing to keep track of that in the same assessment.)</p><p>Some context on that estimate: When I look at history, I don't think of humans being \"in control\" of things. I'm skeptical of Stephen Pinker's \"Better Angels\" framing. Sure, a bunch of easily measurable metrics got better (though some may even be reversing for the last couple of years, e.g., life expectancy is lower now than it used to be before Covid). However, at the same time, technological progress introduced new problems of its own that don't seem anywhere close to being addressed (e.g., social media addiction, increases in loneliness, maybe polarization via attention-grabbing news). Even if there's an underlying trend of \"Better Angels,\" there's also a trend of \"new technology increases the strength of Molochian forces.\" We seem to be losing that battle! AI is an opportunity to gain control for the first time via superhuman intelligence/rationality/foresight to bail us out and reign in Molochian forces once and for all, but to get there, we have to accomplish an immense feat of coordination. I don't see why people are by-default optimistic about something like that. If anything, my 11% that humans will gain control over history for the first time ever seems like the outlandish prediction here! The 89% p(doom) is more like what we should expect by default: things get faster and out of control and then that's it for humans.</p>", "parentCommentId": "BCNWRrRtsNB8HGD3T", "user": {"username": "Lukas_Gloor"}}, {"_id": "aevcn9sr2pmRdHarm", "postedAt": "2023-09-30T12:53:04.000Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>A conditional pause fails to prevent x-risk if either:</p></blockquote><blockquote><ul><li>The AI successfully exfiltrates itself (which is what's needed to defeat rollback mechanisms) during training or evaluation, but before deployment.&nbsp;</li><li>The AI successfully sandbags the evaluations. (Note that existing conditional pause proposals depend on capability evaluations, not alignment evaluations.)</li></ul></blockquote><p>Another way evals could fail is if they work locally but it's still <i>too late</i> in the relevant sense because even with the pause mechanism kicking in (e.g., \"from now on, any training runs that use 0.2x the compute of the model that failed the eval will be prohibited\"), algorithmic progress at another lab or driven by people on twitter(/X) tinkering with older, already-released models, will get someone to the same capability threshold again soon enough. There's probably a degree to which algorithmic insights can substitute for having more compute available. So, even if a failed eval triggers a global pause, if cognitive takeoff dynamics are sufficiently fast/\"discontinuous,\" a failed eval may just mean that we're now so close to takeoff that it's too hard to prevent. Monitoring algorithmic progress is particularly difficult (I think compute is the much better lever).&nbsp;</p><p>Of course, you can still incorporate this concern by adding enough safety margin to when your evals trigger the pause button. Maybe that's okay!&nbsp;</p><p>But then we're back to my point of \"Are we so sure that the evals (with the proper safety margin) shouldn't have triggered today already?\"&nbsp;</p><p>(It's not saying that I'm totally confident about any of this. Just flagging that it seems like a tough forecasting question to determine what an appropriate safety margin is on the assumption that algorithmic progress is hard to monitor/regulate and the assumption that fast takeoff into and through the human range is very much a possibility.)</p>", "parentCommentId": "urfw5SajnwQJg9XBj", "user": {"username": "Lukas_Gloor"}}, {"_id": "WGKvvg3gLcjfTgccq", "postedAt": "2023-10-01T08:40:03.144Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>even with the pause mechanism kicking in (e.g., \"from now on, any training runs that use 0.2x the compute of the model that failed the eval will be prohibited\"), algorithmic progress at another lab or driven by people on twitter(/X) tinkering with older, already-released models, will get someone to the same capability threshold again soon enough. [...] you can still incorporate this concern by adding enough safety margin to when your evals trigger the pause button.</p></blockquote><p>Safety margin is one way, but I'd be much more keen on continuing to monitor the strongest models even after the pause has kicked in, so that you notice the effects of algorithmic progress and can tighten controls if needed. This includes rolling back model releases if people on Twitter tinker with them to exceed capability thresholds. (This also implies no open sourcing, since you can't roll back if you've open sourced the model.)</p><p>But I also wish you'd say what exactly your alternative course of action is, and why it's better. E.g. the worry of \"algorithmic progress gets you to the threshold\" also applies to unconditional pauses. Right now your comments feel to me like a search for anything negative about a conditional pause, without checking whether that negative applies to other courses of action.</p>", "parentCommentId": "aevcn9sr2pmRdHarm", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "2AnXba8vY3RJ6aj9k", "postedAt": "2023-10-01T09:06:26.395Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><blockquote><p>From my perspective you're assuming we're in an unlikely world where the public turns out to be saner than our baseline would suggest.</p></blockquote><p>Hm, or that we get lucky in terms of the public's response being a good one given the circumstances, even if I don't expect the discourse to be nuanced.</p></blockquote><p>That sounds like a rephrasing of what I said that puts a positive spin on it. (I don't see any difference in content.)</p><p>To put it another way -- you're critiquing \"optimistic EAs\" about their attitudes towards institutions, but presumably they could say \"we get lucky in terms of the institutional response being a good one given the circumstances\". What's the difference between your position and theirs?</p><blockquote><p>But my sense is that it's unlikely we'd succeed at getting ambitious measures in place without some amount of public pressure.</p></blockquote><p>Why do you believe that?</p><blockquote><p>It also matters how much weight you give to person-affecting views (I've argued <a href=\"https://forum.effectivealtruism.org/posts/dQvDxDMyueLyydHw4/population-ethics-without-axiology-a-framework\">here</a> for why I think they're not unreasonable).</p></blockquote><p>I don't think people would be on board with the principle \"we'll reduce the risk of doom in 2028, at the cost of increasing risk of doom in 2033 by a larger amount\".</p><p>For me, the main argument in favor of person-affecting views is that they agree with people's intuitions. Once a person-affecting view recommends something that disagrees with other ethical theories <i>and</i> with people's intuitions, I feel pretty fine ignoring it.</p><blockquote><p>The 89% p(doom) is more like what we should expect by default: things get faster and out of control and then that's it for humans.</p></blockquote><p>Your threat model seems to be \"Moloch will cause doom by default, but with AI we have one chance to prevent that, but we need to do it very carefully\". But Molochian forces grow much stronger as you increase the number of actors! The first intervention would be to keep the number of actors involved as small as possible, which you do by having the few leaders race forward as fast as possible, with as much secrecy as possible. If this were my main threat model I would be much more strongly against public advocacy and probably also against both conditional and unconditional pausing.</p><p>(I do think 89% is high enough that I'd start to consider negative-expectation high-variance interventions. I would still be thinking about it super carefully though.)</p>", "parentCommentId": "o9raTBxmCtqSymdLX", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "RBjLWzDiinesNRdBu", "postedAt": "2023-10-01T11:37:23.105Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>That sounds like a rephrasing of what I said that puts a positive spin on it. (I don't see any difference in content.)</p></blockquote><p>Yeah, I just wanted to say that my position doesn't require public discourse to turn out to be surprisingly nuanced.</p><blockquote><p>To put it another way -- you're critiquing \"optimistic EAs\" about their attitudes towards institutions, but presumably they could say \"we get lucky in terms of the institutional response being a good one given the circumstances\". What's the difference between your position and theirs?</p></blockquote><p>I'm hoping to get lucky but not expecting it. The more optimistic EAs seem to be expecting it (otherwise they would share more pessimism).</p><blockquote><p>I don't think people would be on board with the principle \"we'll reduce the risk of doom in 2028, at the cost of increasing risk of doom in 2033 by a larger amount\".</p></blockquote><p>If no one builds transformative AI in a five-year period, the risk of doom can be brought down to close to 0%. By contrast, once we do build it (with that five years delay), if we're still pessimistic about the course of civilization, as I am and I was taking as an assumption for my comments about how worldviews would react to these tradeoffs, then the success chances five years later will still be bad. (EDIT:) So, the tradeoff is something like 55% death spread over a five-year period vs no death for five years, for an eventual reward of reducing total chance of death (over a 10y period or whatever) from 89% to 82%. (Or something a bit lower; I probably place &gt;20% on us somehow not getting to TAI even in 10 years.)</p><p>In that scenario, I could imagine that many people would go for the first option. Many care more about life as they know it than for chances of extreme longevity and awesome virtual worlds. (Some people would certainly change their minds if they thought about the matter a lot more under ideal reasoning conditions, but I expect many [including me when I think self-orientedly] wouldn't.)</p><p>(I acknowledge that, given your <i>more optimistic estimates</i> about AI going well and about low imminent takeoff risk, person-affecting concerns seem highly aligned with long-termist concerns.)&nbsp;</p><blockquote><p>Your threat model seems to be \"Moloch will cause doom by default, but with AI we have one chance to prevent that, but we need to do it very carefully\". But Molochian forces grow much stronger as you increase the number of actors! The first intervention would be to keep the number of actors involved as small as possible, which you do by having the few leaders race forward as fast as possible, with as much secrecy as possible. If this were my main threat model I would be much more strongly against public advocacy and probably also against both conditional and unconditional pausing.</p></blockquote><p>That's interesting. I indeed find myself thinking that our best chances of success come from a scenario where most large-model AI research shuts down, but a new org of extremely safety-conscious people is formed where they make progress with a large lead. I was thinking of something like the scenario you describe as \"<i>Variant 2:</i> In addition to this widespread pause, there is a tightly controlled and monitored government project aiming to build safe AGI.\" It doesn't necessarily have to be government-led, but maybe the government has talked to evals experts and demands a tight structure where large expenditures of compute always have to be approved by a specific body of safety evals experts.</p><blockquote><p>&gt;&gt;But my sense is that it's unlikely we'd succeed at getting ambitious measures in place without some amount of public pressure.</p></blockquote><blockquote><p>Why do you believe that?</p></blockquote><p>I don't know. Maybe I'm wrong: If the people who are closest to DC are optimistic that lawmakers would be willing to take ambitious measures soon enough, then I'd update that public advocacy has fewer upsides (while the downsides remain). I was just assuming that the current situation is more like \"some people are receptive, but there's also a lot of resistance from lawmakers.\"</p><p><br><br>&nbsp;</p>", "parentCommentId": "2AnXba8vY3RJ6aj9k", "user": {"username": "Lukas_Gloor"}}, {"_id": "5syoCzm4Zvayq2HNA", "postedAt": "2023-10-01T11:48:57.825Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>But I also wish you'd say what exactly your alternative course of action is, and why it's better. E.g. the worry of \"algorithmic progress gets you to the threshold\" also applies to unconditional pauses. Right now your comments feel to me like a search for anything negative about a conditional pause, without checking whether that negative applies to other courses of action.</p></blockquote><p>The way I see it, the main difference between conditional vs unconditional pause is that the unconditional pause comes with a bigger safety margin (as big as we can muster). So, given that I'm more worried about surprising takeoffs, that position seems prima facie more appealing to me.<br><br>In addition, as I say in my other comment, I'm open to (edit: or, more strongly, I'd ideally prefer this!) some especially safety-conscious research continuing onwards through the pause. I gather that this is one of your primary concerns? I agree that an outcome where that's possible requires nuanced discourse, which we may not get if public reaction to AI goes too far in one direction. So, I agree that there's a tradeoff around public advocacy.&nbsp;</p>", "parentCommentId": "WGKvvg3gLcjfTgccq", "user": {"username": "Lukas_Gloor"}}, {"_id": "rJhi5z3qLxdyJfnTF", "postedAt": "2023-10-02T08:26:19.029Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>So, the tradeoff is something like 55% death spread over a five-year period vs no death for five years, for an eventual reward of reducing total chance of death (over a 10y period or whatever) from 89% to 82%.</p></blockquote><p>Oh we disagree much more straightforwardly. I think the 89% should be going up, not down. That seems by far the most important disagreement.</p><p>(I thought you were saying that person-affecting views means that even if the 89% goes up that could still be a good trade.)</p><p>I still don't know why you expect the 89% to go down instead of up given public advocacy. (And in particular I don't see why optimism vs pessimism has anything to do with it.) My claim is that it should go up.</p><blockquote><p>I was thinking of something like the scenario you describe as \"<i>Variant 2:</i> In addition to this widespread pause, there is a tightly controlled and monitored government project aiming to build safe AGI.\" It doesn't necessarily have to be government-led, but maybe the government has talked to evals experts and demands a tight structure where large expenditures of compute always have to be approved by a specific body of safety evals experts.</p></blockquote><p>But why do evals matter? What's an example story where the evals prevent Molochian forces from leading to us not being in control? I'm just not seeing how this scenario intervenes on your threat model to make it not happen.</p><p>(It does introduce government bureaucracy, which all else equal reduces the number of actors, but there's no reason to focus on safety evals if the theory of change is \"introduce lots of bureaucracy to reduce number of actors\".)</p><blockquote><p>Maybe I'm wrong: If the people who are closest to DC are optimistic that lawmakers would be willing to take ambitious measures soon enough</p></blockquote><p>This seems like the wrong criterion. The question is whether this strategy is more likely to succeed than others. Your timelines are short enough that no ambitious measure is going to come into place fast enough if you aim to save ~all worlds.</p><p>But e.g. ambitious measures in ~5 years seems very doable (which seems like it is around your median, so still in time for half of worlds). We're already seeing signs of life:</p><blockquote><p>note the existence of the UK Frontier AI Taskforce and the people on it, as well as the&nbsp;<a href=\"https://sd11.senate.ca.gov/news/20230913-senator-wiener-introduces-safety-framework-artificial-intelligence-legislation\"><u>intent bill SB 294</u></a> in California about \u201cresponsible scaling\u201d</p></blockquote><p>You could also ask people in DC; my prediction is they'd say something reasonably similar.</p>", "parentCommentId": "RBjLWzDiinesNRdBu", "user": {"username": "AnonResearcherMajorAILab"}}, {"_id": "3ycpnFsbgAtFH4wCW", "postedAt": "2023-10-02T14:26:35.003Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>Oh we disagree much more straightforwardly. I think the 89% should be going up, not down. That seems by far the most important disagreement.</p></blockquote><p>I think we agree (at least as far as my example model with the numbers was concerned). The way I meant it, 82% goes up to 89%.&nbsp;</p><p>(My numbers were confusing because I initially said 89% was my all-things-considered probability, but in this example model, I was giving 89% as the probability for a scenario where we take a (according to your view) suboptimal action. In the example model, 82% is the best chance we can get with optimal course of action, but it comes at the price of way higher risk of death in the first five years.)</p><p>In any case, my assumptions for this example model were:&nbsp;</p><p>(1) Public advocacy is the only way to install an ambitious pause soon enough to reduce risks that happen before 5 years.</p><p>(2) If it succeeds at the above, public advocacy will likely also come with negative side effects that increase the risks later on.&nbsp;</p><p>And I mainly wanted to point out how, from a person-affecting perspective, the difference between 82% and 89% isn't necessarily huge, whereas getting 5 years of zero risk vs 5 years of 55% cumulative risks feels like something that could matter a lot.&nbsp;</p><p>But one can also discuss the validity of (1) and (2). It sounds like you don't buy (1) at all. By contrast, I think (1) is plausible but I'm anot confident in my stance here, and you raise good points.</p><p>Regarding (2), I probably agree that if you achieve an ambitious pause via public advocacy and public pressure playing a large role, this makes some things harder later.</p><blockquote><p>But why do evals matter? What's an example story where the evals prevent Molochian forces from leading to us not being in control? I'm just not seeing how this scenario intervenes on your threat model to make it not happen.</p></blockquote><p>Evals prevent the accidental creation of misaligned transformative AI by the project that's authorized to go beyond the compute cap for safety research (if necessary; obviously they don't have to go above the cap if the returns from alignment research are high enough at lower levels of compute).&nbsp;</p><p>Molochian forces are one part of my threat model, but I also think alignment difficulty is high and hard takeoff is more likely than soft takeoff. (Not all these components to my worldview are entirely independent. You could argue that being unusually concerned about Molochian forces and expecting high alignment difficulty is both produced by the same underlying sentiment. Arguably, most humans aren't really aligned with human values, for Hansonian reasons, which we can think of as a subtype of Moloch problems. Likewise, if it's already hard to align humans to human values, it'll probably also be hard to align AIs to those values [or at least create an AI that is high-integrity friendly towards humans, while perhaps pursuing some of its own aims as well \u2013 I think that would be enough to generate a good outcome, so we don't necessarily have to create AIs that care about <i>nothing else</i> besides human values].)</p><p><br>&nbsp;</p>", "parentCommentId": "rJhi5z3qLxdyJfnTF", "user": {"username": "Lukas_Gloor"}}, {"_id": "ua5KCJ2GuYX4uCPzi", "postedAt": "2023-10-04T07:12:24.024Z", "postId": "BFbsqwCuuqueFRfpW", "htmlBody": "<blockquote><p>The way I meant it, 82% goes up to 89%.&nbsp;</p></blockquote><p>Oops, sorry for the misunderstanding.</p><p>Taking your numbers at face value, and assuming that people have on average 40 years of life ahead of them (Google suggests median age is 30 and typical lifespan is 70-80), the pause gives an expected extra 2.75 years of life during the pause (delaying 55% chance of doom by 5 years) while removing an expected extra 2.1 years of life (7% of 30) later on. This looks like a win on current-people-only views, but it does seem sensitive to the numbers.</p><p>I'm not super sold on the numbers. Removing the full 55% is effectively assuming that the pause definitely happens and is effective -- it neglects the possibility that advocacy succeeds enough to have the negative effects, but still fails to lead to a meaningful pause. I'm not sure how much probability I assign to that scenario but it's not negligible, and it might be more than I assign to \"advocacy succeeds and effective pause happens\".</p><blockquote><p>It sounds like you don't buy (1) at all.</p></blockquote><p>I'd say it's more like \"I don't see why we should believe (1) currently\". It could still be true. Maybe all the other methods really can't work for some reason I'm not seeing, and that reason is overcome by public advocacy.</p>", "parentCommentId": "3ycpnFsbgAtFH4wCW", "user": {"username": "AnonResearcherMajorAILab"}}]