[{"_id": "9jbGvyyi6cuRnG9R4", "title": "GiveWell (Explore site for 5 mins.)", "postedAt": "2021-10-12T23:00:00.000Z", "htmlBody": "", "user": {"username": "GiveWell"}}, {"_id": "da4S6pbkbQ8azcfSA", "title": "Long-term AI policy strategy research and implementation", "postedAt": "2021-11-09T00:00:00.000Z", "htmlBody": "<p><strong>In a nutshell:</strong> Advancing AI technology could have both huge upsides and huge downsides, including potentially catastrophic risks. To manage these risks, we need people making sure the deployment of AI goes well, by thinking about how to:</p><ul><li>Ensure broad sharing of the benefits from developing powerful AI systems.</li><li>Avoid exacerbating military competition or conflict caused by increasingly powerful AI systems.</li><li>Ensure that the groups that develop AI work together to develop and implement safety features.</li></ul><h3><strong>Recommended</strong></h3><p>If you are well suited to this career, it may be the best way for you to have a social impact.</p><p>Review status</p><p>Based on a medium-depth investigation&nbsp;<br>&nbsp;</p><h2><strong>Why might working to improve AI policy be high impact?</strong></h2><p>As <a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\">we\u2019ve argued</a>, in the next few decades, we might see the development of powerful machine learning algorithms with the potential to transform society. This could have major upsides and downsides, including the possibility of <a href=\"https://80000hours.org/articles/existential-risks/\">catastrophic risks</a>.</p><p>To manage these risks, we need technical research into the design of safe AI systems (including the \u2018alignment problem\u2019), which we cover in <a href=\"https://80000hours.org/career-reviews/ai-safety-researcher/\">a separate career review</a>.</p><p>But in addition to solving the technical problems, there are many other important questions to address. These can be roughly categorised into three key challenges of transformative AI strategy:</p><ul><li><strong>Ensuring broad sharing of the benefits</strong> from developing powerful AI systems, rather than letting AI\u2019s development harm humanity or unduly concentrate power.</li><li><strong>Avoiding exacerbating military competition or conflict</strong> caused by increasingly powerful AI systems.</li><li><strong>Ensuring that the groups that develop AI work together</strong> to develop and implement safety features.</li></ul><p>We need a community of experts who <a href=\"https://80000hours.org/articles/us-ai-policy/\">understand the intersection of modern AI systems and policy</a>, and work together to mitigate long-term risks and ensure humanity reaps the benefits of advanced AI.</p><h2><strong>What does this path involve?</strong></h2><p>Experts in AI policy strategy would broadly carry out two overlapping activities:</p><ol><li>Research \u2014 to develop strategy and policy proposals.</li><li>Implementation \u2014 working together to put policy into practice.</li></ol><p>We see these activities as equally important as the technical ones, but currently they are more neglected. Many of the top academic centres and AI companies have started to hire researchers working on technical AI safety, and there\u2019s perhaps a community of 20\u201350 full-time researchers focused on the issue. However, there are only a handful of researchers focused on strategic issues or working in AI policy with a long-term perspective.</p><p>Note that there is already a significant amount of work being done on nearer-term issues in AI policy, such as the regulation of self-driving cars. What\u2019s neglected is work on issues that are likely to arise as AI systems become substantially more powerful than those in existence today \u2014 so-called \u2018<a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence\">transformative AI</a>\u2018 \u2014 such as the three non-technical challenges outlined above.</p><p>Some examples of top AI policy jobs to work towards include the following, which fit a variety of skill types:</p><ul><li>Work at <strong>top AI labs</strong>, such as <a href=\"https://deepmind.com/\">DeepMind</a> or <a href=\"https://openai.com/\">OpenAI</a>, especially in relevant policy team positions or other influential roles.</li><li>Become a researcher at a <strong>think tank</strong> <a href=\"https://80000hours.org/career-reviews/think-tank-research/#positively-shaping-the-development-of-artificial-intelligence\">that works on relevant issues</a>, especially the <a href=\"https://cset.georgetown.edu/\">Center for Security and Emerging Technology</a> at Georgetown \u2014 a new think tank that analyses the security implications of emerging technology, and is currently focused on AI.</li><li>Develop expertise in a relevant issue in the <strong>US government</strong>, such as in the <a href=\"https://www.whitehouse.gov/ostp/\">Office of Science and Technology Policy</a>, <a href=\"https://www.whitehouse.gov/nsc/\">National Security Council</a>, <a href=\"https://dod.defense.gov/About/Office-of-the-Secretary-of-Defense/\">Office of the Secretary of Defense</a>, <a href=\"https://dodcio.defense.gov/Cyber-Workforce/JAIC.aspx\">Joint AI Research Center</a>, or the US government\u2019s defence and intelligence research funding agencies (<a href=\"https://www.darpa.mil/\">DARPA</a> and <a href=\"https://www.iarpa.gov/\">IARPA</a>). The US government is probably the most important nation in shaping AI development right now, but similar roles in other countries can also be useful.</li><li>Research AI governance at <strong>Chinese think tanks</strong> like the <a href=\"http://www.caict.ac.cn/english/\">China Academy of Information and Communications Technology</a>, producing policy recommendations for the Chinese government or more public-facing research. (Learn more about careers in China-specific AI policy in <a href=\"https://80000hours.org/career-reviews/china-related-ai-safety-and-governance-paths/\">our career review on China-related AI safety and governance paths</a>.)</li><li>Become <strong>an academic</strong> or a researcher at a research institute focused on long-term AI policy, especially the <a href=\"https://www.governance.ai/\">Centre for the Governance of AI</a>, which already has several researchers working on these issues.</li><li>Aim to get an influential position in <strong>party politics</strong>, especially as an advisor with a focus on emerging technology policy (e.g. start as a <a href=\"https://80000hours.org/career-reviews/congressional-staffer/\">staffer in Congress</a>).</li></ul><h2><strong>Examples of people pursuing this path</strong></h2><figure class=\"image\"><img src=\"https://80000hours.org/wp-content/uploads/2019/10/helen-toner-300x300.jpeg\"></figure><p>Helen Toner</p><p>Helen worked in consulting before getting a research job at GiveWell and then Open Philanthropy. From there, she explored a couple of different cause areas, and eventually moved to Beijing to learn about the intersection of China and AI. When the <a href=\"https://cset.georgetown.edu/\">Center for Security and Emerging Technology</a> (CSET) was founded, she was recruited to help build the organisation. CSET has since become a leading think tank in Washington on the intersection of emerging technology and national security.<br><a href=\"https://80000hours.org/podcast/episodes/helen-toner-on-security-and-emerging-technology/\"><strong>LEARN MORE</strong></a></p><figure class=\"image\"><img src=\"https://80000hours.org/wp-content/uploads/2022/01/ben_garfinkel-300x300.png\"></figure><p>Ben Garfinkel</p><p>Ben graduated from Yale in 2016, where he majored in physics, math, and philosophy. After graduating, Ben became a researcher at the Centre for Effective Altruism and then moved to the <a href=\"https://www.governance.ai/\">Centre for the Governance of AI</a> (GovAI) at the University of Oxford\u2019s Future of Humanity Institute (now part of the Centre for Effective Altruism). He\u2019s now the acting director there. As of December 2021, <a href=\"https://www.governance.ai/opportunities\">GovAI is hiring</a>.<br><a href=\"https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/\"><strong>LEARN MORE</strong></a></p><h2><strong>How to assess your fit</strong></h2><p>If you can succeed in this area, then you have the opportunity to make a significant contribution to what might well be the most important issue of the next century.</p><p>To be impactful in this path, a key question is whether you have a reasonable chance of getting some of the top jobs listed earlier.</p><p>The government and political positions require people with a well-rounded skillset, the ability to meet lots of people and maintain relationships, and the patience to work with a slow-moving bureaucracy. It\u2019s also ideal if you\u2019re a US citizen (which may be necessary to get security clearance), and don\u2019t have an unconventional past that could create problems if you choose to work in politically sensitive roles.</p><p>The more research-focused positions would typically require the ability to get into a top 10 graduate school in a relevant area, and deep interest in the issues. For instance, when you read about the issues, do you get ideas for new approaches to them? Read more about <a href=\"https://80000hours.org/career-reviews/academic-research/#how-to-assess-your-personal-fit\">predicting fit in research</a>.</p><p>In addition, you should only enter this path if you\u2019re convinced of the importance of long-term AI safety. This path also requires making controversial decisions under huge uncertainty, so it\u2019s important to have excellent judgement, caution, and a willingness to work with others \u2014 or it would be easy to have an <a href=\"https://80000hours.org/articles/accidental-harm/\">unintended negative impact</a>. This is hard to judge, but you can get some information early on by seeing how well you work with others in the field.</p><h2><strong>How to enter this field</strong></h2><p>In the first few years of this path, you\u2019d focus on learning about the issues and how government works, meeting key people in the field, and doing research, rather than pushing for a specific proposal. AI policy and strategy is a deeply complicated area, and it\u2019s easy to make things worse by accident (e.g. see the <a href=\"https://nickbostrom.com/papers/unilateralist.pdf\">Unilateralist\u2019s Curse</a>).</p><p>Some common early career steps include:</p><ul><li><strong>Relevant graduate study</strong>. Some especially useful fields include international relations, strategic studies, machine learning, economics, law, public policy, and political science. Our top recommendation right now is machine learning if you can get into a top 10 school in computer science. Otherwise, our top recommendations are: i) law school if you can get into Yale or Harvard, ii) international relations if you want to focus on research, and iii) strategic studies if you want to focus on implementation.</li><li><strong>Working at a top AI company</strong>, especially DeepMind and OpenAI.</li><li><strong>Working in any general entry-level government and policy positions</strong> (as listed earlier), which let you gain expertise and connections, such as think tank internships, being a researcher or staffer for a politician, joining a campaign, and government leadership schemes.</li></ul><p>This field is at a very early stage of development, which creates a number of challenges. For one, the key questions have not been formalised, which creates a need for \u2018<a href=\"https://forum.effectivealtruism.org/posts/RCvetzfDnBNFX7pLH/personal-thoughts-on-careers-in-ai-policy-and-strategy/\">disentanglement research</a>\u2018 to enable other researchers to get traction. For another, there is a lack of mentors and positions, which can make it hard for people to break into the area.</p><p>Until recently, it\u2019s been very hard to enter this path as a researcher unless you\u2019re able to become one of the top (approximately) 30 people in the field relatively quickly. While mentors and open positions are still scarce, some top organisations have recently recruited junior and mid-career staff to serve as research assistants, analysts, and fellows. Our guess is that obtaining a research position will remain very competitive but positions will continue to gradually open up. On the other hand, the field is still small enough for top researchers to make an especially big contribution by doing field-founding research.</p><p>If you\u2019re not able to land a research position now, then you can either (i) continue to build up expertise and contribute to research when the field is more developed, or (ii) focus more on the policy positions, which could absorb hundreds of people.</p><p>Most of the first steps on this path also offer widely useful career capital. For instance, depending on the sub-area you start in, you could often switch into other areas of policy, the application of AI to other social problems, operations, or earning to give. So, the risks of starting down this path, if you decide to switch later, are not too high.</p><h3><strong>Recommended organisations</strong></h3><ul><li>The American Association for the Advancement of Science offers <a href=\"http://www.stpf-aaas.org/\">Science &amp; Technology Policy Fellowships</a>, which provide hands-on opportunities to apply scientific knowledge and technical skills to important societal challenges. Fellows are assigned for one year to a selected area of the United States federal government, where they participate in policy development and implementation.</li><li>The <a href=\"https://cset.georgetown.edu/\">Center for Security and Emerging Technology</a> at Georgetown University produces data-driven research at the intersection of security and technology (including AI, advanced computing, and biotechnology) and provides nonpartisan analysis to the policy community. See <a href=\"https://cset.georgetown.edu/careers/\">current vacancies</a>.</li><li>The <a href=\"https://www.governance.ai/\">Centre for the Governance of AI</a> is focused on building a global research community that\u2019s dedicated to helping humanity navigate the transition to a world with advanced AI. See <a href=\"https://www.governance.ai/opportunities\">current vacancies</a>.</li><li><a href=\"https://www.longtermresilience.org/\">The Centre for Long-Term Resilience</a> facilitates access to the expertise of leading academics who work on long-term global challenges, such as AI, biosecurity, and risk management policy. It helps convert cutting-edge research into actionable recommendations that are grounded in the UK context.</li><li><a href=\"https://deepmind.com/\">DeepMind</a> is probably the largest research group developing general machine intelligence in the Western world. We\u2019re only confident about recommending DeepMind roles working specifically on safety, ethics, policy, and security issues. See <a href=\"https://deepmind.com/careers/\">current vacancies</a>.</li><li>The <a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> is a multidisciplinary research institute at the University of Oxford. Academics at FHI bring the tools of mathematics, philosophy, and social sciences to bear on big-picture questions about humanity and its prospects.</li><li>The <a href=\"https://www.legalpriorities.org/\">Legal Priorities Project</a> is an independent global research project founded by researchers from Harvard University. It conducts legal research that tackles the world\u2019s most pressing problems, and is influenced by the principles of effective altruism and longtermism. See <a href=\"https://www.legalpriorities.org/opportunities.html\">current vacancies</a>.</li><li><a href=\"https://openai.com/\">OpenAI</a> was founded in 2015 with the goal of conducting research into how to make AI safe. It has received over $1 billion in funding commitments from the technology community. We\u2019re only confident in recommending opportunities in their policy, safety, and security teams. See <a href=\"https://jobs.lever.co/openai\">current vacancies</a>.</li><li>United States Congress (for example, as a <a href=\"https://80000hours.org/career-reviews/congressional-staffer/\">congressional staffer</a>).</li><li>The <a href=\"https://www.whitehouse.gov/ostp/\">United States Office of Science and Technology Policy</a> works to maximise the benefits of science and technology to advance health, prosperity, security, environmental quality, and justice for all Americans.</li><li>The <a href=\"https://www.defense.gov/About/Office-of-the-Secretary-of-Defense/\">United States Office of the Secretary of Defense</a> is where top civilian defence decision-makers work with the secretary to develop policy, make operational and fiscal plans, manage resources, and evaluate programmes. See <a href=\"https://www.defense.gov/News/Contracts/\">current vacancies</a>.</li></ul>", "user": {"username": "Benjamin_Todd"}}, {"_id": "nmQCeuDbxrGq3Mx8f", "title": "The case for taking AI seriously as a threat to humanity", "postedAt": "2020-11-10T00:00:00.000Z", "htmlBody": "", "user": {"username": "EA Fellowship"}}, {"_id": "oYyrEwfRtneCFqmLT", "title": "Diminishing marginal impact", "postedAt": "2021-07-11T23:00:00.000Z", "htmlBody": "<p><strong>Diminishing returns</strong> (or <strong>diminishing returns to scale</strong>) is the decrease in <a href=\"https://forum.effectivealtruism.org/topics/thinking-at-the-margin\">marginal</a> output of a production process as one factor of production is increased while the remaining factors are held constant.</p><p>Giving money to an altruistic project (for example, a charity) increases the amount of useful work that can be done by that project\u2014this is a social return on the money donated. If an area exhibits diminishing returns, then the social return on each additional dollar will continuously decline as more and more money is given to the&nbsp;project.</p><p>The <i>monetary</i> difference between a project receiving nothing and it receiving $500,000 is obviously the same as the monetary difference between it receiving $4.5m and it receiving $5m. However, the theory of diminishing marginal returns suggests that in terms of <i>the amount of good done</i>, the difference between a project receiving nothing and receiving $500,000 is probably bigger than the difference between it receiving $4.5m and it receiving $5m. So if there are diminishing marginal returns, a project will get more good done with their first few dollars than with their last&nbsp;ones.</p><p>This point does not only apply to charities and money: it can also apply to other kinds of funding opportunities (e.g., individuals, universities, scientific research programmes, think tanks, nonprofit organisations)<a href=\"https://forum.effectivealtruism.org/topics/diminishing-returns#fnamvf14ddc0e\"><sup>[1]</sup></a>&nbsp;and other kinds of resources (e.g., time and labour). However, it is unclear to what extent diminishing returns apply in different areas. Owen Cotton-Barratt gives an overview of the variety of cases it seems to apply to.<a href=\"https://forum.effectivealtruism.org/topics/diminishing-returns#fnoohqidgirtf\"><sup>[2]</sup></a>&nbsp;However, Michael Dickens, for instance, argues that there are no detectable diminishing marginal returns in animal advocacy or work on existential&nbsp;risk.<a href=\"https://forum.effectivealtruism.org/topics/diminishing-returns#fn91zr41ihgim\"><sup>[3]</sup></a></p><p>If charities face diminishing marginal returns, it may make sense to only fund them to a certain level: beyond that we say they have no additional <a href=\"https://forum.effectivealtruism.org/topics/room-for-more-funding\">room for more funding</a>. Given that organizations and areas may face different returns to additional labour than they do to additional capital, it may be worth considering whether they are more <a href=\"https://forum.effectivealtruism.org/topics/constraints-on-effective-altruism\">talent- or funding-&nbsp;constrained</a> (though there are also ways that framing could be misleading).<a href=\"https://forum.effectivealtruism.org/topics/diminishing-returns#fnvdh1tdkrmef\"><sup>[4]</sup></a><a href=\"https://forum.effectivealtruism.org/topics/diminishing-returns#fn2d737gtfjw8\"><sup>[5]</sup></a></p><h2>Further reading</h2><p>Dalton, Max &amp; Owen Cotton-Barratt (2017) <a href=\"https://www.centreforeffectivealtruism.org/blog/selecting-the-appropriate-model-for-diminishing-returns/\">Selecting the appropriate model for diminishing returns</a>, <i>Centre for Effective Altruism</i>, May 23.<br><i>Sets out some considerations for choosing between different models of diminishing returns.</i></p><h2>Related entries</h2><p><a href=\"https://forum.effectivealtruism.org/topics/donation-choice\">donation choice</a> | <a href=\"https://forum.effectivealtruism.org/topics/effective-altruism-funding\">effective altruism funding</a> | <a href=\"https://forum.effectivealtruism.org/topics/effective-giving-1\">effective giving</a> | <a href=\"https://forum.effectivealtruism.org/topics/neglectedness\">neglectedness</a> | <a href=\"https://forum.effectivealtruism.org/topics/philanthropic-coordination\">philanthropic coordination</a> | <a href=\"https://forum.effectivealtruism.org/topics/philanthropic-diversification\">philanthropic diversification</a> | <a href=\"https://forum.effectivealtruism.org/topics/room-for-more-funding\">room for more funding</a> | <a href=\"https://forum.effectivealtruism.org/topics/thinking-at-the-margin\">thinking at the margin</a></p>", "user": {"username": "EA Fellowship"}}, {"_id": "PDcgMppffoekqeXeM", "title": "Want to help animals? Focus on corporate decisions, not people\u2019s plates.", "postedAt": "2019-02-13T00:00:00.000Z", "htmlBody": "", "user": {"username": "EA Fellowship"}}, {"_id": "WxpAGzLvBLrZgmTFs", "title": "South Asian Air Quality Cause Investigation", "postedAt": "2021-06-07T19:30:00.000Z", "htmlBody": "<h2><strong>In a nutshell</strong></h2><ul><li><strong>What is the problem?</strong>&nbsp;South Asia experiences some of the world\u2019s highest levels of population-weighted PM2.5 air pollution. Our understanding is that poor air quality contributes significantly to negative health outcomes for the more than 1.8 billion people in the region, and that reducing the levels of particulate matter present in the air could save millions of lives.</li><li><strong>What are possible interventions?</strong>&nbsp;Possible interventions, many of which require coordinated state action, include improving air quality monitoring programs and crafting and implementing sector-specific policies to reduce emissions. A philanthropist interested in reducing South Asia\u2019s air pollution levels could support efforts to inform the design, implementation, and enforcement of more effective air pollution abatement policies, such as funding monitoring programs, research, and technical assistance.</li><li><strong>Who else is working on this?</strong>&nbsp;Philanthropic interest in South Asian air quality appears to be limited but growing rapidly, although many major philanthropic actors seem to address air pollution as a climate concern rather than as a health issue. Outside of the philanthropic sector, we think it\u2019s likely that governments are the biggest spenders on improving air quality. We remain substantially uncertain about the exact levels of funding South Asian governments are directing toward the issue.</li></ul><hr><h2><strong>1. The problem</strong></h2><p>South Asia \u2013 and in particular the Indo-Gangetic Plain that covers parts or all of India, Pakistan, Bangladesh, and Nepal \u2013 experiences some of the world\u2019s highest population-weighted air pollution levels.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote1_oxwwd8p\"><u>1</u></a>&nbsp;Our understanding is that poor air quality contributes significantly to negative health outcomes for the more than 1.8 billion people in this area.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote2_pq2ka7e\"><u>2</u></a>&nbsp;Of the pollutants present in South Asia\u2019s air, we focus on PM<sub>2.5</sub>&nbsp;\u2013 particulate matter no larger than 2.5 micrometers in diameter \u2013 which we understand to be associated with the greatest health costs.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote3_lqa2i1c\"><u>3</u></a>&nbsp;In total, the State of Global Air report \u2013 a collaboration between the Health Effects Institute and the Institute for Health Metrics and Evaluation\u2019s Global Burden of Disease project \u2013 attributes approximately 71.4 million disability-adjusted life years (DALYs) across South Asia annually to air pollution.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote4_u5sg9nx\"><u>4</u></a>&nbsp;According to the Institute for Health Metrics and Evaluation, air pollution in South Asia accounts for nearly 3% of all DALYs worldwide \u2013 i.e. eliminating dangerous levels of air pollution in South Asia alone would reduce the number of prematurely lost years of healthy life by 3% each year.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote5_tdu02gu\"><u>5</u></a></p><p>Exposure to PM<sub>2.5</sub>&nbsp;air pollution can occur outdoors or within households, with the two settings associated with different concentrations, health outcomes, and&nbsp;<a href=\"https://www.openphilanthropy.org/research/south-asian-air-quality/#2-possible-interventions\"><u>interventions</u></a>. Sources of outdoor, or ambient, air pollution in South Asia include brick kilns, vehicles, coal power plants, and crop burning.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote6_5dbku3u\"><u>6</u></a>&nbsp;According to the State of Global Air report, South Asia\u2019s average experienced ambient air pollution in 2019 was 78.2 \u00b5g/m<sup>3</sup>, a concentration higher than both the World Health Organization\u2019s recommended standard of 10 \u00b5g/m<sup>3</sup>&nbsp;and its intermediate standard of 35 \u00b5g/m<sup>3</sup>.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote7_uzz8g1y\"><u>7</u></a></p><p>While we have not investigated the overall evidence base thoroughly, we have encountered widespread agreement that long-term exposure to ambient PM<sub>2.5</sub>&nbsp;pollution can result in significant negative health effects, such as chronic respiratory and cardiovascular diseases, that reduce life expectancy. The State of Global Air report, for example, estimates that, in 2019, almost 40 million DALYs in South Asia were attributable to ambient PM<sub>2.5</sub>&nbsp;air pollution.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote8_pp2akjg\"><u>8</u></a>&nbsp;This number appears to be stable in some South Asian countries and increasing in others.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote9_iy543o3\"><u>9</u></a>&nbsp;While we have not independently vetted this or other mortality and morbidity estimates, it seems reasonably plausible to us based on South Asia\u2019s population-weighted air pollution levels and what the literature we\u2019ve reviewed says about air pollution\u2019s role in chronic illnesses.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote10_sunnzpy\"><u>10</u></a></p><p>Concentrations of household (as opposed to ambient) air pollution in South Asia appear to be far more difficult to measure, with estimates we found ranging from 35 \u00b5g/m<sup>3</sup>&nbsp;to over 2,000 \u00b5g/m<sup>3</sup>.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote11_jii6993\"><u>11</u></a>&nbsp;We found more certainty, however, that household air pollution is widespread: one source estimates that roughly 60% of people in South Asia use solid cooking fuels, the primary source of household air pollution.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote12_eyqtehu\"><u>12</u></a>&nbsp;This percentage is apparently decreasing as people switch to cleaner energy sources.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote13_or3yfwr\"><u>13</u></a></p><p>The lack of reliable household PM<sub>2.5</sub>&nbsp;concentration data makes it difficult to confidently discern health effects. The available evidence indicates that negative health outcomes of household air pollution in South Asia may include low birth weight, preterm birth, and other conditions that are correlated with an increased risk of infant death.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote14_9ajqwy6\"><u>14</u></a>&nbsp;The State of Global Air report, for example, attributed approximately 95,000 infant deaths within the first month of life to household air pollution in South Asia in 2019, estimating an overall impact of approximately 30 million DALYs within the region for that year.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote15_leswn3q\"><u>15</u></a></p><p>Of the nations comprising South Asia, India appears to experience among the highest average annual population-weighted ambient air pollution levels \u2013 83.2 \u00b5g/m<sup>3</sup>&nbsp;\u2013 and to contain the greatest number of DALYs attributable to both ambient and household air pollution \u2013 31.1 million and 20.9 million, respectively.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote16_l2jqo1a\"><u>16</u></a>&nbsp;South Asia\u2019s growing and aging population means that the burden from air quality \u2013 all else equal \u2013 is rising. In the case of household air pollution, this appears to be more than offset by people switching to cleaner cooking fuels, reducing the burden over time.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote17_t9w0pnx\"><u>17</u></a>&nbsp;However, the number of DALYs attributable to ambient air pollution appears to be increasing as outdoor air quality is worsening, accentuating demographic trends.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote18_waetboa\"><u>18</u></a>&nbsp;The outsize impact of air pollution in India relative to other South Asian nations suggests to us that improving India\u2019s air quality could greatly reduce South Asia\u2019s population-weighted annual PM<sub>2.5</sub>&nbsp;concentrations and DALYs resulting from air pollution.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote19_yd70iou\"><u>19</u></a></p><p><strong>1.1 Why believe these estimated harms</strong></p><p>We often have concerns about the quality and reliability of non-experimental social scientific evidence, and prefer to be able to replicate key inputs to our calculations ourselves. That is not possible with the State of Global Air report, which does not have open data and code. So we start with some skepticism that these large DALY estimates should be taken at face value. However, we did a review of the underlying literature and \u2013 while, as with all social science literatures, we think there could be room for improvement \u2013 we came away thinking that we would probably not want to adjust the State of Global Air burden estimates downward by more than a factor of two.</p><p>More specifically, biological mechanisms appear to support the conclusion that exposure to air pollution results in significant negative health effects, including mortality, in humans. Both the American Heart Association and the Lancet Commission on pollution and health, as well as epidemiologists we have spoken with, state that breathing particulate matter generates inflammation and vascular damage. These effects in turn are linked to conditions such as atherosclerosis and high blood pressure, which are known to cause life-threatening diseases such as ischemic heart disease and ischemic stroke.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote20_whc89ln\"><u>20</u></a>&nbsp;In infants, the proposed pathway seems to be that particulate pollution causes lower transmission of nutrients to fetuses, resulting in lower birth weight and nutrient deficiencies associated with higher infant mortality and lifelong health complications.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote21_nco8aa7\"><u>21</u></a></p><p>There are various animal and human RCTs and studies on these biological mechanisms. The studies generally find that particulate pollution causes vascular inflammation, atherosclerosis, and low birth weight.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote22_potbozw\"><u>22</u></a>&nbsp;More recent animal studies, however, do not seem to use mortality as an outcome of interest, and some much older studies found null effects of air pollution exposure on mortality.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote23_qexehsd\"><u>23</u></a>&nbsp;According to Open Philanthropy\u2019s&nbsp;<a href=\"https://www.openphilanthropy.org/focus/scientific-research/\"><u>scientific research</u></a>&nbsp;team, the null mortality results in animal models in the older studies are not necessarily evidence against mortality effects in humans, largely due to innate differences in biology and lifespans, though we do take them to be a mild negative update.</p><p>Outside of studies on the relevant biological mechanisms, we have found various natural experiments conducted by economists that attempt to isolate the causal effect of particulate pollution on mortality.&nbsp;<a href=\"http://www.indiaenvironmentportal.org.in/files/file/airborne%20particulate%20matter%20China.pdf\"><u>Ebenstein et al., 2017</u></a>&nbsp;in particular examines the health effects of air pollution in conditions similar to those in South Asia, although we\u2019re skeptical of this paper\u2019s headline mortality effects.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote24_p9zcf20\"><u>24</u></a>&nbsp;Other quasi-experimental papers, many of which focus on short-term exposure to particulates, generally find meaningful effects on mortality on both infants and adults.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote25_ecnre4p\"><u>25</u></a>&nbsp;These papers have reassured us that the non-experimental social science literature we have found is likely not detecting the mortality effects of a confounding variable.</p><p>We have not found any meta-analyses that look for publication bias in the quasi-experimental evidence mentioned above. There is an epidemiological literature, however, that contains funnel plots that aim to identify publication bias. In a literature with no publication bias, one would expect to see a symmetric, triangle-shaped pattern of dots in the scatter, with the lower-powered analyses equally likely to fall on the right or left of the high-powered analyses. The plots within&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S0013935119307212#fig4\"><u>Pope et al., 2020</u></a>&nbsp;(specifically&nbsp;<a href=\"https://ars.els-cdn.com/content/image/1-s2.0-S0013935119307212-gr4_lrg.jpg\"><u>Figure 4</u></a>), which examines epidemiological papers on the causal effect of air pollution on mortality in cohort studies, appear to have some asymmetry in the middle of the funnels.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote26_e5mui7w\"><u>26</u></a>&nbsp;We very tentatively believe that a publication-bias adjustment based on these charts would reduce the mortality effect size to a number slightly to moderately below the consensus in the epidemiological literature.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote27_ywif16j\"><u>27</u></a></p><hr><h2><strong>2. Possible Interventions</strong></h2><p><strong>2.1 Government action</strong></p><p>Many potential air quality improvements require coordinated state action. The following abatement policies are some of the ones that we thought had a mix of potentially addressing a large portion of the pollution problem and were likely administratively feasible.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote28_8nmnlbp\"><u>28</u></a></p><p><strong>2.1.1 Retrofitting and building efficient brick kilns</strong></p><p>20% of clay bricks are produced in South Asia, although PM<sub>2.5</sub>&nbsp;emissions attributable to the sector seem to vary by country and be concentrated in urban areas.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote29_r3pw65e\"><u>29</u></a>&nbsp;A report by the World Bank estimates that the brick sector is the second-largest PM<sub>2.5</sub>&nbsp;contributor in Bangladesh and Nepal, responsible for 11% and 3% of PM<sub>2.5</sub>&nbsp;emissions, respectively.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote30_axteq8z\"><u>30</u></a>&nbsp;In India, meanwhile, the share of PM<sub>2.5</sub>&nbsp;emissions attributable to brick kilns appears to be comparatively lower, although we have encountered substantial uncertainty around this point. The Health Effects Institute offers one of the lower estimates we found, tracing approximately 2% of India\u2019s PM<sub>2.5</sub>&nbsp;pollution and 2 to 3% of its PM<sub>2.5</sub>-related deaths to brick kilns.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote31_14loott\"><u>31</u></a>&nbsp;The World Bank\u2019s report has the highest estimate of the sources we gathered, attributing 8% of India\u2019s PM<sub>2.5</sub>&nbsp;emissions to the brick sector.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote32_fo4d4gq\"><u>32</u></a>&nbsp;The World Bank estimates that retrofitting existing kilns could reduce PM<sub>2.5</sub>&nbsp;by 30-50%, as well as improving energy efficiency.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote33_ei8wbmi\"><u>33</u></a></p><p>Despite the uncertainties around emission levels, we think it is plausible (but by no means decisive) that a government-championed effort (e.g., regulations and/or subsidies) to retrofit and build efficient brick kilns would be administratively feasible and could meaningfully reduce PM<sub>2.5</sub>&nbsp;pollution from the brick sector.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote34_8hn858b\"><u>34</u></a></p><p><strong>2.1.2 Implementing and enforcing a ban on older vehicles</strong></p><p>At least since 2015, government bodies in India have indicated an interest in limiting the use of older vehicles.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote35_l62cydr\"><u>35</u></a>&nbsp;There are some regional bans, but it\u2019s unclear to us to what extent they have been implemented or enforced.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote36_dybjd6m\"><u>36</u></a>&nbsp;This existing \u2013 if inconsistent \u2013 interest in banning older vehicles, along with what appears to be a low number of vehicles over 10 years old (meaning political/economic costs of a ban are smaller), suggests that this is a potentially promising area for further government action.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote37_7c5al2x\"><u>37</u></a></p><p>We\u2019re uncertain about the percentage of the population-weighted PM<sub>2.5</sub>&nbsp;pollution in South Asia that is vehicular, although it appears fairly significant. A report by India\u2019s Ministry of Environment, Forest and Climate Change estimates that vehicles contribute approximately 28% of population-weighted PM<sub>2.5</sub>&nbsp;emissions in Delhi during the winter and 4% nationally when accounting for all modes of transportation.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote38_me7kljh\"><u>38</u></a>&nbsp;The Energy and Resources Institute attributes 50% of Bangalore\u2019s PM<sub>2.5</sub>&nbsp;load to automobile emissions.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote39_fhjrcat\"><u>39</u></a>&nbsp;A source apportionment study of Mangalore attributed 70% of particulate pollution to vehicles.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote40_2eb63de\"><u>40</u></a>&nbsp;Older vehicles in particular appear to be a significant contributor to vehicle emissions, with one estimate we found claiming that vehicles older than 15 years account for 15% of total vehicular pollution, and tend to pollute 10 to 25 times as much as newer vehicles.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote41_60q3ck4\"><u>41</u></a>&nbsp;Based on these numbers, we think it is likely that a ban on older vehicles could reduce total PM<sub>2.5</sub>&nbsp;pollution, although we\u2019re very uncertain about the total reduction we could reasonably expect and how enforceable (and beneficial) a ban would realistically be.</p><p><strong>2.1.3 Mandating and enforcing coal scrubbers</strong></p><p>Most of the estimates we found attribute approximately 15% of India\u2019s PM<sub>2.5</sub>&nbsp;emissions to coal power generation.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote42_ncq2gmb\"><u>42</u></a>&nbsp;It seems plausible to us that coal is a significant source of PM<sub>2.5</sub>&nbsp;emissions, given the prominence of coal in India\u2019s electricity generation and CO<sub>2</sub>&nbsp;emissions.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote43_rwiy5rg\"><u>43</u></a></p><p>One report we saw claims that installing wet coal scrubbers in power plants could reduce PM<sub>2.5</sub>&nbsp;emissions by as much as 98% and newer fabric filters can reach efficiencies as high as 99.7%.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote44_7s1xy3s\"><u>44</u></a>&nbsp;While we have not independently vetted this estimate, if accurate, it indicates to us that coal scrubbers could significantly improve India\u2019s air quality.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote45_p7xl7ee\"><u>45</u></a></p><p>The Indian government has already mandated that plants install coal scrubbers to limit emissions, although compliance appears to be limited.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote46_9o8lrnc\"><u>46</u></a>&nbsp;Given the apparent magnitude of coal power emissions and the government\u2019s existing interest in pursuing mitigation measures, additional efforts to install coal scrubbers might be a promising intervention.</p><p>Below, we share our rough back-of-the-envelope calculations (BOTECs) on the potential cost-effectiveness of philanthropic support for the installation of coal scrubbers.</p><p><strong>2.1.4 Reducing crop burning with better targeted tractor subsidies</strong></p><p>Our impression is that crop burning is a relatively minor source of emissions in India; one article claims that it constitutes an average of 5% of annual PM<sub>2.5</sub>&nbsp;pollution in Delhi, although it reaches up to 40% at certain points in the year.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote47_y1egmm5\"><u>47</u></a>&nbsp;The vast majority of farmers appear to burn their crops, with only an estimated 20% using tractors to till their fields.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote48_g6ado22\"><u>48</u></a>&nbsp;It seems plausible that better targeting tractor subsidies to increase the percentage of farmers using tractors, while decreasing the percentage of those who burn stubble, could moderately improve Delhi\u2019s air quality.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote49_ctjsq75\"><u>49</u></a>&nbsp;We are unsure of the potential impact tractor subsidies might have on air quality across the broader South Asian region.</p><p><strong>2.1.5 Better targeting liquified petroleum gas subsidies</strong></p><p>From what we have found, solid cooking fuels \u2013 still used by approximately 60% of households \u2013 account for roughly 40% of the health burden from PM<sub>2.5</sub>&nbsp;pollution in South Asia.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote50_mb9kq7w\"><u>50</u></a>&nbsp;We tentatively assume that substantial reductions in solid cooking fuel use could lead to large reductions in health impacts. The main replacement for solid cooking fuel (e.g. wood, agricultural refuse, charcoal, etc.) is liquified petroleum gas (LPG).</p><p>The Indian government already subsidizes LPG use, currently entitling each household to 12 LPG cylinders per year.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote51_9jzwa44\"><u>51</u></a>&nbsp;The subsidies, however, do not provide significant discounts to the market price, suggesting that LPG cylinder prices may remain too high for many poor households to afford.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote52_mgxtbyx\"><u>52</u></a>&nbsp;As a means of increasing the subsidies available to the poor, the government has unsuccessfully attempted to convince wealthier households to voluntarily pay for unsubsidized LPG.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote53_qpnsayb\"><u>53</u></a>&nbsp;Better targeting the subsidies by increasing availability and subsidy size for poorer households could plausibly help reduce the number of households that use solid cooking fuels.</p><p><strong>2.2 What could a philanthropist support?</strong></p><p>We have encountered widespread uncertainty around the share total and population-weighted PM<sub>2.5</sub>&nbsp;that is attributable to different sources in India and across South Asia. Addressing this information deficiency seems to be crucial to appropriately targeting abatement strategies. As such, it seems likely that philanthropic efforts may be able to productively focus on 1) improving the information ecosystem for local decision makers and other stakeholders and 2) increasing the technical capacity of key governmental agencies to address air quality. A philanthropist interested in supporting either of these two outcomes might pursue any of a variety of activities, some of which we\u2019ve listed below.</p><p><strong>2.2.1 Source apportionment studies</strong></p><p>As we mentioned previously, we have found that the deficiency in pollution source apportionment data has made it difficult to gauge the potential impacts of available interventions. Source apportionment studies are scientific studies that attempt to measure what share of the total PM<sub>2.5</sub>&nbsp;concentration in a given city or region can be attributed to different sources, e.g. transportation, power generation, other industrial sources, etc.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote54_qu75w6e\"><u>54</u></a></p><p>Source apportionment studies could be conducted in partnership with interested cities needing technical assistance.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote55_c22aapx\"><u>55</u></a>&nbsp;Such localized studies, in providing governments with rough estimates of their cities\u2019 largest sources of air pollution, could potentially improve governments\u2019 (and philanthropists\u2019) abatement strategies.</p><p>Below, we share our rough back-of-the-envelope calculations on the potential cost-effectiveness of philanthropist support for source apportionment studies.</p><p><strong>2.2.2 Air quality monitoring</strong></p><p>A philanthropist might fund either low-cost sensors or advanced monitoring stations. Low-cost sensors, which&nbsp;<a href=\"https://www.openphilanthropy.org/research/south-asian-air-quality/#4-what-have-we-done-so-far\"><u>we have already supported</u></a>, can be installed locally and could contribute data to real-time air quality maps that report shifts in pollution amounts. We assume that these maps could help improve public awareness of local pollution levels and precipitate minor behavior change, as well as enable governments and other entities to track the impacts of abatement methods. The limited accuracy of low-cost sensors may impede pollution measurements, however, as individual sensors may not be able to detect small changes in concentrations.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote56_ajpnpd8\"><u>56</u></a></p><p>Advanced monitoring stations are much more accurate \u2013 also significantly more expensive \u2013 and could be installed in each of India\u2019s airsheds. Potentially combining the stations with sun photometers to measure the atmospheric column could allow for significantly more accurate and frequent satellite measurements of air pollution sources and concentrations.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote57_r2t9h9q\"><u>57</u></a>&nbsp;These measurements could, in turn, provide governments with more precise pollution targets, enable the effects of abatement policies to be tracked, and contribute to general air quality reporting.</p><p>We think that air quality monitoring could be a fairly large source of philanthropic spending in the short term, with smaller ongoing costs after the initial implementation. From our conversations, we also received the impression that Indian air monitoring is comparatively well-funded, and that supporting monitoring in other South Asian regions might generate more impact on the margin. We have not independently vetted these claims.</p><p><strong>2.2.3 Research on the abatement curve and air pollution health effects in India</strong></p><p>We perceive research on the abatement curve (the graph describing the financial costs and volumes of PM<sub>2.5</sub>&nbsp;reductions by intervention pursued) and air pollution health effects in India as having a variety of benefits. A better defined abatement curve data could serve as a menu of options for interested philanthropists or policymakers. Research on health effects could provide more targeted data on the health effects of PM<sub>2.5</sub>&nbsp;pollution, including potentially distinguishing between the health effects of different types of pollutants.</p><p>In addition, such research could help drive awareness among governments and the public of the extent of the problem and accordingly encourage the adoption of targeted abatement measures (particularly if this research identifies a more narrow set of lower cost policy changes that could solve a large share of the total problem). We have heard that this may be more effective if the research is based at national institutions that also provide expertise to local governments or non-governmental organizations working on this issue.</p><p><strong>2.2.4 Technical assistance</strong></p><p>Providing technical assistance to government entities could improve the outcomes of pollution abatement measures by increasing governmental capacity to implement, enforce, and monitor air pollution abatement measures. A funder interested in this outcome could, for example, work with outside consultants to provide technical assistance to India\u2019s pollution control boards, which, for a number of reasons, have struggled to enforce air quality regulations.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote58_hwszgos\"><u>58</u></a>&nbsp;We have found conflicting estimates of the pollution control boards\u2019 current spending, although it seems to be between $100 million to $300 million a year, split between air pollution, water pollution, noise pollution, and waste management.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote59_3jx5ao6\"><u>59</u></a></p><p><strong>2.2.5 Policy outreach</strong></p><p>The interventions outlined in the&nbsp;<a href=\"https://www.openphilanthropy.org/research/south-asian-air-quality/#21-government-action\"><u>section above</u></a>&nbsp;are largely under the purview of the government. As such, philanthropic efforts might focus on providing decision makers with data and resources to craft effective air pollution abatement policies. Potential funding areas could include source apportionment and concentration research, real-time air quality maps, and reporting on air quality in local news outlets. Other means of increasing the salience of air quality might include funding programs like the Clean Air Fund\u2019s&nbsp;<a href=\"http://doctorsforcleanair.org/\"><u>Doctors for Clean Air</u></a>, which raises awareness of the health impacts of air pollution, or supporting air quality programs at universities.</p><p><strong>2.3 How cost effective could spending in this area be?</strong></p><p>If air pollution costs 71.4 million DALYs annually in South Asia, and we were spending $20 million per year, we would need to be pulling forward solutions to approximately 0.06% of the problem by 10 years for every year of our spending in order to clear our&nbsp;<a href=\"https://www.openphilanthropy.org/research/givewells-top-charities-are-increasingly-hard-to-beat/\"><u>1,000x bar</u></a>.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote60_b6rbe9o\"><u>60</u></a>&nbsp;It is difficult to reason about small numbers like that but given the relatively limited scale of other philanthropy in this space, we do not think that would be an unreasonable bar for us to clear.</p><p>We do not have a specific plan for how to spend money cost-effectively on this problem at that level, but we\u2019ve done a few back-of-the-envelope calculations on promising-seeming potential projects, described in more detail below, that also make us think they could clear the 1,000x bar.</p><p><strong>2.3.1 Air quality monitoring</strong></p><p>We have&nbsp;<a href=\"https://www.openphilanthropy.org/research/south-asian-air-quality/#4-what-have-we-done-so-far\"><u>already recommended funding</u></a>, totalling $3 million, to install a network of low-cost air quality sensors in India. We have removed our current BOTEC since it\u2019s related to our hiring process for a South Asian air quality program officer.</p><p><strong>2.3.2 Source apportionment studies</strong></p><p>By our rough calculations, a source apportionment study would need to accelerate a reduction of 0.8 \u00b5g/m<sup>3</sup>&nbsp;in pollution by 10 years for a city of 5 million to reach our&nbsp;<a href=\"https://www.openphilanthropy.org/research/givewells-top-charities-are-increasingly-hard-to-beat/\"><u>1,000x bar</u></a>.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote61_ubmc48a\"><u>61</u></a>&nbsp;This calculation assumes that:</p><ul><li>The cost of a source apportionment study would scale as a function of population size. We roughly estimate that a study in a city of 5 million would cost $500,000.</li><li>Source apportionment studies would only measure, and thus impact, ambient air pollution levels.</li><li>The PM<sub>2.5</sub>&nbsp;concentration in cities is proportional to the national concentration. The average annual population-weighted concentration of ambient air pollution in India is 83.2 \u00b5g/m<sup>3</sup>. Approximately 31,140,452 DALYs in India are due to ambient air pollution, and every DALY is valued at $50,000.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote62_a6rira9\"><u>62</u></a></li><li>India\u2019s population is 1,366,000,000.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote63_yoqqzel\"><u>63</u></a></li></ul><p><strong>2.3.3 Coal scrubbers</strong></p><p>According to a report by the Disease Control Priorities Network, installing coal scrubbers in all power plants would cost approximately $1.7 billion.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote64_zsg1nz3\"><u>64</u></a>&nbsp;The same report estimates that to retrofit the plants with the lowest cost per life saved would cost $615 million, although other sources we\u2019ve encountered estimate costs that are more than an order of magnitude higher.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote65_sxhdytf\"><u>65</u></a>&nbsp;If the $615 million figure were correct, paying to install coal scrubbers could reach and perhaps surpass our&nbsp;<a href=\"https://www.openphilanthropy.org/research/givewells-top-charities-are-increasingly-hard-to-beat/\"><u>1,000x bar</u></a>, assuming the following conditions are true:</p><ul><li>As we discussed above, coal power generation contributes approximately 15% of India\u2019s PM<sub>2.5</sub>&nbsp;emissions.</li><li>Installing scrubbers reduces PM<sub>2.5</sub>&nbsp;emissions by at least 80%.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote66_rqgrmy2\"><u>66</u></a></li><li>The selected coal power plants are responsible for 75% of the sector\u2019s DALY costs.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote67_5yzrdf0\"><u>67</u></a></li><li>The health effects of air pollution in India cost approximately $2.68 trillion/year.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote68_3kkr4rh\"><u>68</u></a></li><li>Given that the government is already mandating the installation of coal scrubbers, our funding speeds installation up by five years.</li></ul><p>Under these conditions, we would estimate an ROI of $2.68 trillion (total cost of ambient air pollution) \u00d7 .15 (power sector share of total PM<sub>2.5</sub>) \u00d7 .75 (selected plants\u2019 share of power sector DALYs) \u00d7 .8 (reduction in PM<sub>2.5</sub>&nbsp;from scrubbers) \u00d7 5 (years of speed-up) / $615 million (cost of scrubbers) = ~1,960x, though again we do not know these assumptions to be correct and have seen much higher cost estimates in the literature.</p><p><strong>2.4 What scale could a program in this area possibly reach?</strong></p><p>Based on our understanding of the available funding opportunities, we think there is a high likelihood that a program in this area could spend at least $25 million per year on activities such as air quality monitoring, abatement and source apportionment studies, technical assistance, scaling existing organizations working on air quality, and policy outreach, at a cost-effectiveness level comparable to other funding opportunities we pursue. We think there is a lower likelihood of significantly more than $25M/year of capacity in opportunities we would consider quite cost-effective.</p><hr><h2><strong>3. Who else is working on this?</strong></h2><p><strong>3.1 Philanthropic organizations</strong></p><p>Philanthropic interest in South Asian air quality appears to be limited but growing rapidly: an estimate by the Clean Air Fund, which was cited to us in multiple conversations, puts the philanthropic spending in this area at roughly $7 million in 2019, up from $1 million in 2015.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote69_hha15h4\"><u>69</u></a>&nbsp;We have not vetted the report\u2019s estimates and would guess there are structural underestimates because the report is based on self-reported data from foundations, some of whom may not participate in data sharing, but the estimates are broadly consistent with what we heard in conversations.</p><p>The international philanthropic actors working on South Asian air quality that we have heard mentioned most frequently are Bloomberg Philanthropies, the Children\u2019s Investment Fund Foundation, ClimateWorks, the IKEA Foundation, the MacArthur Foundation, the Oak Foundation, Pisces Foundation, and the William and Flora Hewlett Foundation. Some major Indian funders, such as Ashish Dhawan, have also come up in our conversations with experts and funders in the field. We do not believe that this is an exhaustive list: we would guess that we have accounted for the largest philanthropic funders working in this area, but we are certainly missing smaller investments from non-profits and activists.</p><p>Many of these major philanthropic actors appear to address air pollution as a contributor to climate change rather than in terms of direct negative health effects from particulates. Climate-focused philanthropic spending on air quality is part of a broader effort to mitigate emissions in India, with philanthropic annual spending on emissions reductions that we think is on the order of $100M-$350M.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote70_s30bift\"><u>70</u></a></p><p>It is unclear to us to what extent treating air quality as a climate concern versus as a health issue would result in significantly different funding strategies. There is definite potential for overlap between climate and air quality spending, as many interventions that reduce greenhouse gasses tend to reduce PM<sub>2.5</sub>&nbsp;emissions as well (e.g., limiting reliance on coal for electricity generation). But the two goals can also come apart (e.g., flue-gas desulphurization units on coal plants help improve air quality for health, but as far as we know do not mitigate climate impacts). Overall, we do not think that the presence of significant climate funders mitigates the need for more focused work to improve air quality from a health perspective.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote71_u4kpra9\"><u>71</u></a></p><p><strong>3.2 Government</strong></p><p>We found it difficult to find reliable estimates of governmental spending on air quality. According to one source we found, in the 2019-2020 budget cycle, the Indian government created and allocated a fund of Rs 44 billion (approximately $609 million at the time of conversion) to address air pollution in large cities.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote72_wg8oqi7\"><u>72</u></a>&nbsp;Additionally, a 2020 report released by the Council on Energy, Environment and Water and UrbanEmissions notes that the National Clean Air Plan, which directs cities to create action plans to reduce particulate matter concentrations by 20-30% by 2024, receives Rs 4.6 billion (approximately $63 million at the time of conversion). However, the report also noted that there are no penalties for non-achievement or \u201clegal mandate for reviewing and updating plans.\u201d<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote73_k1er8ei\"><u>73</u></a>&nbsp;In fact, only nine cities seem to have noted the costs of execution, which ranged from Rs 890 million to Rs 160 billion (approximately $11.9 million to $2 billion at the time of conversion, respectively).<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote74_3mclebm\"><u>74</u></a>&nbsp;We remain substantially uncertain of the accuracy of these estimates and recognize that it\u2019s plausible that there may be additional state funds we do not know about. Overall, we think it\u2019s likely that the government is the biggest spender on improving air quality, but that the current spending is substantially lower than the amount required to adequately reduce air pollution.</p><hr><h2><strong>4. What have we done so far?</strong></h2><p>Air quality monitoring stood out to us as a particularly tractable abatement strategy that has the capacity to absorb immediate funding. We have accordingly recommended grants totalling $3 million to support a three-year collaboration between&nbsp;<a href=\"https://www.openphilanthropy.org/grants/uc-berkeley-air-quality-sensors-in-south-asia/\"><u>Professor Joshua Apte of UC Berkeley</u></a>, the&nbsp;<a href=\"https://www.openphilanthropy.org/grants/indian-institute-of-technology-delhi-air-quality-sensors-in-south-asia/\"><u>Indian Institute of Technology Delhi (IIT Delhi)</u></a>, and the&nbsp;<a href=\"https://www.openphilanthropy.org/grants/council-on-energy-environment-and-water-air-quality-sensors-in-south-asia/\"><u>Council on Energy, Environment, and Water (CEEW)</u></a>&nbsp;to install a network of low-cost air quality sensors in South Asia.</p><p>The aim of the collaboration is that the data from the sensors will inform the design, implementation, and enforcement of more effective air pollution abatement policies. Additionally, we also see this project as an early learning opportunity for the testing and deployment of low-cost sensors across South Asia; if successful, we predict that the sensors could have spillover effects on the speed at which other low-cost sensors are deployed, although we have not consulted experts on this point. Both of these outcomes could plausibly result in significant reductions to South Asian air pollution levels.</p><p>For our back-of-the-envelope calculations on the potential cost-effectiveness of these grants, see&nbsp;<a href=\"https://www.openphilanthropy.org/research/south-asian-air-quality/#231-air-quality-monitoring\"><u>above</u></a>.</p><hr><h2><strong>5. Potential risks and downsides</strong></h2><p>We have identified a number of potential risks and downsides to funding air quality improvements efforts in South Asia, including:</p><ul><li><strong>Immediate spending capacity appears to be limited.&nbsp;</strong>We have identified a few abatement activities \u2013 such as air quality monitoring and certain forms of technical assistance \u2013 that could benefit from immediate funding. We have otherwise struggled, however, to identify areas that have the capacity for large-scale recurring support, and we expect that a philanthropist hoping to make South Asian air quality a long-term funding area may need to consistently find novel grantmaking opportunities.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote75_mg89ahn\"><u>75</u></a></li><li><strong>Air quality interventions largely depend on government regulation and enforcement.&nbsp;</strong>Accordingly, most philanthropic efforts in South Asian air quality would be limited to activities that inform government policies but that may not directly impact air quality (for example, funding air quality monitoring stations that in turn provide decision makers with data to craft effective abatement measures). It will likely be difficult to predict what the likely impact of these efforts might be.</li><li><strong>There are risks and restrictions specific to funding work in India.&nbsp;</strong>The Indian government has historically regulated foreign grantmaking within India and recently implemented additional restrictions on foreign funding to Indian NGOs.<a href=\"https://www.openphilanthropy.org/research/cause-reports/south-asian-air-quality#footnote76_rileojs\"><u>76</u></a>&nbsp;It is our impression that all of the potential philanthropic efforts listed in this writeup are permissible under current laws, but there is a risk that the Indian government could implement additional restrictions that would change that.</li><li><strong>Philanthropic efforts could lead to overly restrictive policies in some domains,&nbsp;</strong>which in turn could create space for corruption or potentially slow economic growth. While this writeup does not take into account risks from air pollution beyond mortality, we recognize that some abatement strategies may not be economically feasible or desirable after accounting for their costs.</li><li><strong>Philanthropic interest in South Asian air quality appears to be growing,&nbsp;</strong>so additional funding now could risk crowding out other funders who would otherwise enter.</li></ul><hr><h2><strong>6. Our process and next steps</strong></h2><p>We talked to a number of experts and major funders in the field in the process of researching South Asian air quality. The following individuals agreed to being named as sources for this report, though this should not be interpreted to mean that any experts named here endorse our conclusions in part or in total:</p><ul><li>Aaron Van Donkelaar</li><li>Ambuj Sagar</li><li>Amita Ramachandran</li><li>Arden Pope</li><li>Avijit Michael</li><li>Brikesh Singh</li><li>Dan Kass</li><li>Ishwar Gawande</li><li>Jarnail Singh</li><li>Josh Apte</li><li>Kanchi Gupta</li><li>Matt Whitney</li><li>Melanie Hammer</li><li>Michael Greenstone</li><li>Pallavi Pant</li><li>Randall Martin</li><li>Reecha Upadhyay</li><li>Rohini Pande</li><li>Sam Ori</li><li>Sangita Vyas</li><li>Santosh Harish</li><li>Siddarthan Balasubramania</li><li>Vinuta Gopal</li></ul><p>We continue to be open to learning about more opportunities in this space and may make additional grants in the future.</p><p>&nbsp;</p><p>&nbsp;</p><p><i>This work is licensed under a&nbsp;</i><a href=\"https://creativecommons.org/licenses/by/4.0/\"><i>Creative Commons Attribution 4.0 International License</i></a><i>.</i></p><hr><h2><strong>7. Sources</strong></h2><figure class=\"table\" style=\"width:609px\"><table style=\"border:1px solid rgb(68, 82, 119)\"><tbody><tr><td style=\"background-color:rgb(68, 82, 119);border-right:1px solid white;padding:0.75rem;vertical-align:top\">Air Quality Life Index, \u201cIndia Fact Sheet\u201d</td><td style=\"background-color:rgb(68, 82, 119);border-right:1px solid white;padding:0.75rem;vertical-align:top\"><a href=\"https://aqli.epic.uchicago.edu/wp-content/uploads/2019/03/EPIC_IndiaFactSheet_V06-nobleeds.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Anderson et al. (2005)</td><td style=\"padding:0.75rem\"><a href=\"https://www.researchgate.net/profile/Janet-Peacock/publication/8028482_Ambient_particulate_matter_and_health_effects_publication_bias_in_studies_of_short-term_associations/links/5f97086ea6fdccfd7b7fe3d0/Ambient-particulate-matter-and-health-effects-publication-bias-in-studies-of-short-term-associations.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Apte et al. (2018)</td><td style=\"padding:0.75rem\"><a href=\"https://doi.org/10.1021/acs.estlett.8b00360\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Arceo et al. (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://www.nber.org/system/files/working_papers/w18349/w18349.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Belis et al. (2014)</td><td style=\"padding:0.75rem\"><a href=\"https://source-apportionment.jrc.ec.europa.eu/Docu/EU_guide_on_SA.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Berger (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.openphilanthropy.org/research/givewells-top-charities-are-increasingly-hard-to-beat/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">BreatheLife, \u201cCities at the Centre of India\u2019s New National Clean Air Programme\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://breathelife2030.org/news/cities-centre-indias-new-national-clean-air-programme/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Brook et al. (2009)</td><td style=\"padding:0.75rem\"><a href=\"https://www.ahajournals.org/doi/full/10.1161/hypertensionaha.109.130237\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Brook et al. (2010)</td><td style=\"padding:0.75rem\"><a href=\"https://www.ahajournals.org/doi/full/10.1161/CIR.0b013e3181dbece1\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Burnett et al. (2018)</td><td style=\"padding:0.75rem\"><a href=\"https://www.pnas.org/content/pnas/115/38/9592.full.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Business Insider, \u201cIndian Government Will No Longer Pay Out Direct Benefit Transfer for Cooking Gas \u2014 Subsidy Eliminated as Oil Prices Fall\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.businessinsider.in/india/news/indian-government-will-no-longer-pay-out-direct-benefit-transfer-for-cooking-gas-subsidy-eliminated-as-oil-prices-fall/articleshow/77872369.cms\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Centre for Science and Environment, \u201cWhat Will India Do With Its Old Vehicles?\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.cseindia.org/what-will-india-do-with-its-old-vehicles--10380\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Chay and Greenstone (2003)</td><td style=\"padding:0.75rem\"><a href=\"https://www.nber.org/system/files/working_papers/w10053/w10053.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Chen et al. (2013)</td><td style=\"padding:0.75rem\"><a href=\"https://doi.org/10.1073/pnas.1300018110\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Clancy et al. (2002)</td><td style=\"padding:0.75rem\"><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.550.978&amp;rep=rep1&amp;type=pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Clean Air Fund, \u201cThe State of Global Air Quality Funding\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.cleanairfund.org/wp-content/uploads/2020/09/EMBARGOED-7-Sept_State-of-Global-AQ-Funding-2020_Clean-Air-Fund_Final.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Correia et al. (2013)</td><td style=\"padding:0.75rem\"><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3521092/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Council on Foundations, \u201cNew Indian FCRA Amendments Impact Foreign Grants to Indian NGOs\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.cof.org/news/new-indian-fcra-amendments-impact-foreign-grants-indian-ngos\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Cropper (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://cpb-us-e1.wpmucdn.com/blogs.gwu.edu/dist/5/1304/files/2018/05/Cropper-India-Presentation-IIEP-NIPFP-24zw9bh.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Cropper et al. (2017)</td><td style=\"padding:0.75rem\"><a href=\"http://dcp-3.org/sites/default/files/chapters/DCP3%20Injury%20%26%20Environment_Ch13.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Currie (2013)</td><td style=\"padding:0.75rem\"><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4847437/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Deryugina et al. (2019)</td><td style=\"padding:0.75rem\"><a href=\"https://julianreif.com/research/reif.aer.2019.pollution.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Doctors for Clean Air, \u201cHomepage\u201d</td><td style=\"padding:0.75rem\"><a href=\"http://doctorsforcleanair.org/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">DW, \u201cIndia Pollution: How a Farming Revolution Could Solve Stubble Burning\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.dw.com/en/india-pollution-how-a-farming-revolution-could-solve-stubble-burning/a-51166417\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Ebenstein et al. (2017)</td><td style=\"padding:0.75rem\"><a href=\"http://www.indiaenvironmentportal.org.in/files/file/airborne%20particulate%20matter%20China.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Eil et al. (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://documents1.worldbank.org/curated/en/685751588227715919/pdf/Dirty-Stacks-High-Stakes-An-Overview-of-Brick-Sector-in-South-Asia.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">EPA, \u201cParticulate Matter (PM) Basics\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.epa.gov/pm-pollution/particulate-matter-pm-basics\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Ganguly et al. (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.urbanemissions.info/wp-content/uploads/docs/2020-06-CEEW-UEinfo-India-NCAP-Review.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Gao et al. (2018)</td><td style=\"padding:0.75rem\"><a href=\"https://www.sciencedirect.com/science/article/pii/S0160412018313369\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Gardener (1966)</td><td style=\"padding:0.75rem\"><a href=\"https://drive.google.com/file/d/1kL3fnH0uqTfi7mqjHragbgPjKzkb6VVk/view?usp=sharing\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Ghosh (2021)</td><td style=\"padding:0.75rem\"><a href=\"https://www.indiatoday.in/auto/latest-auto-news/story/scrappage-policy-is-here-will-you-be-able-to-drive-your-15-year-old-vehicle-from-1st-april-2021-1762805-2021-01-26\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">GiveWell, \u201cInterpreting the Disability-Adjusted Life-Year (DALY) Metric\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.givewell.org/research/DALY\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Goel et al. (2013)</td><td style=\"padding:0.75rem\"><a href=\"https://wedocs.unep.org/bitstream/handle/20.500.11822/16973/Fuelefficiency_vehicletech.pdf?sequence=1&amp;isAllowed=y\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Greenstone et al. (2015)</td><td style=\"padding:0.75rem\"><a href=\"https://cpb-us-w2.wpmucdn.com/campuspress.yale.edu/dist/7/2986/files/2019/06/EPW-2015_Lower-Pollution-Longer-Lives_Greenstone-Nilekani-Sudarshan-Suganathan-Ryan-Pande.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Haryana State Pollution Control Board, \u201cBudget Estimate\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://hspcb.gov.in/budget.html\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Health Effects Institute, \u201cBurden of Disease Attributable to Major Air Pollution Sources in India\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.healtheffects.org/system/files/GBD-MAPS-SpecRep21-India-revised_0.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Health Effects Institute, \u201cHousehold Air Pollution and Noncommunicable Disease | Summary for Policy Makers\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.healtheffects.org/system/files/Comm18-SummaryForPolicyMakers.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Heft-Neal et al. (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.nature.com/articles/s41893-020-0562-1.epdf?sharing_token=WBKjXZrau1Vqcdbv-O8o-dRgN0jAjWel9jnR3ZoTv0PcDJADffWGtfFrwILO9PaDGwPmZl8shwyuBu1pL8N91aT9NwCez4XAXDWDk7Mp62aXCdGuDYEChxLJoKGUDxOUkiSk2knLdCF7Gm50tv2Afwk666HxGS12v8-_WvUxHbc%3D\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Hindustani Times (2021)</td><td style=\"padding:0.75rem\"><a href=\"https://www.hindustantimes.com/india-news/renewal-of-registration-for-15-year-old-govt-vehicles-to-stop-from-apr-1-2022-101615622822176.html\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Johnson et al. (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://cfpub.epa.gov/si/si_public_record_report.cfm?Lab=CEMM&amp;dirEntryId=348236\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Kalaiarasan et al. (2018)</td><td style=\"padding:0.75rem\"><a href=\"https://www.sciencedirect.com/science/article/pii/S0301479718304225\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Koshy (2019)</td><td style=\"padding:0.75rem\"><a href=\"https://www.downtoearth.org.in/blog/energy/overcoming-india-s-clean-cooking-challenge-68562\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Landrigan et al. (2017)</td><td style=\"padding:0.75rem\"><a href=\"http://ugspace.ug.edu.gh/bitstream/handle/123456789/31420/The%20Lancet%20Commission%20on%20pollution%20and%20health.pdf?sequence=1\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Maharashtra Pollution Control Board, \u201cBudget 2019-2020\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.mpcb.gov.in/sites/default/files/about-us/budget/budget_2019-20_06052019.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Martin et al. (2019)</td><td style=\"padding:0.75rem\"><a href=\"https://www.sciencedirect.com/science/article/pii/S2590162119300437?via%3Dihub\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">McCormick (1985)</td><td style=\"padding:0.75rem\"><a href=\"https://www.nejm.org/doi/pdf/10.1056/NEJM198501103120204\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Menon (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://www.thehindu.com/news/national/Political-meddling-proves-toxic-for-pollution-control-boards/article12016818.ece\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Mohan (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://timesofindia.indiatimes.com/city/mumbai/mumbai-to-get-biggest-chunk-of-grant-from-centre-to-fight-air-pollution/articleshow/74588527.cms\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Myllyvirta et al. (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://www.greenpeace.org/static/planet4-india-stateless/2018/06/Out-of-Sight-How-coal-burning-advances-India%E2%80%99s-Air-Pollution-Crisis.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Narayan (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.livemint.com/industry/energy/cooking-gas-prices-may-see-monthly-revision-to-contain-subsidy-11582037827460.html\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">National Clean Air Programme, \u201cFinal Proposal\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://urbanemissions.info/wp-content/uploads/docs/India-NCAP-Proposal-Final.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Open Philanthropy, \u201cScientific Research\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.openphilanthropy.org/focus/scientific-research\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Peng et al. (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://pubs.acs.org/doi/10.1021/acs.est.0c01622\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Police et al. (2018)</td><td style=\"padding:0.75rem\"><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S167420011730192X\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Pope et al. (2009)</td><td style=\"padding:0.75rem\"><a href=\"https://www.nejm.org/doi/full/10.1056/NEJMsa0805646\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Pope et al. (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5215745/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Pope et al. (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.sciencedirect.com/science/article/pii/S0013935119307212\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Rakshit (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://theswaddle.com/indias-pollution-regulators-are-ineffective-due-to-a-lack-of-expertise-resources-report/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Roeyer et al. (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.climateworks.org/report/funding-trends-climate-change-mitigation-philanthropy/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Sharma and Dikshit (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://cerca.iitd.ac.in/uploads/Reports/1576211826iitk.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Sharma and Kumar (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://www.toi.no/getfile.php/1348396-1530775816/Publikasjoner/Air%20pollution%20book.pdf\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Sharma and Nagpure (2019)</td><td style=\"padding:0.75rem\"><a href=\"https://thewire.in/environment/does-india-have-the-skilled-workforce-needed-to-fight-air-pollution\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Snider et al. (2015)</td><td style=\"padding:0.75rem\"><a href=\"https://amt.copernicus.org/articles/8/505/2015/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">SS Rana &amp; Co (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.lexology.com/library/detail.aspx?g=8aa9fcd0-d087-411b-81a3-1d1cca6ad9b9\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">State of Global Air 2020, \u201cExplore the Data\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.stateofglobalair.org/data/#/health/plot\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">State of Global Air 2020, \u201cHomepage\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.stateofglobalair.org/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Task Force on Hemispheric Transport of Air Polution, \u201cQuestions and Answers\u201d</td><td style=\"padding:0.75rem\"><a href=\"http://htap.org/q/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">The Financial Express, \u201cTo Curb Stubble Burning, Pay Attention to EPCA on Making Straw Management Machines Affordable\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.financialexpress.com/opinion/to-curb-stubble-burning-pay-attention-to-epca-on-making-straw-management-machines-affordable/2096274/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">The New Indian Express, \u201cThree Years on, Not Many Willing to Give Up LPG Subsidy\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://www.newindianexpress.com/nation/2019/jul/29/3-years-on-not-many-willing-to-give-up-lpg-subsidy-2010859.html\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">The World Bank, \u201cPopulation, Total \u2014 India\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://data.worldbank.org/indicator/SP.POP.TOTL?locations=IN\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Times of India, \u201cCentre Cuts Pollution Control Budget, Draws Flak From Experts\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://timesofindia.indiatimes.com/india/centre-cuts-pollution-control-budget-draws-flak-from-experts/articleshow/67796649.cms\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Times of India, \u201cForeign Contribution Regulation Act\u201d</td><td style=\"padding:0.75rem\"><a href=\"https://timesofindia.indiatimes.com/miscellaneous/foreign-contribution-regulation-act/articleshow/56221166.cms\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Tripathi (2020a)</td><td style=\"padding:0.75rem\"><a href=\"https://www.business-standard.com/article/current-affairs/low-on-staff-motivation-regulators-fail-to-enforce-air-quality-standards-120110400334_1.html\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Tripathi (2020b)</td><td style=\"padding:0.75rem\"><a href=\"https://scroll.in/article/977662/reduced-to-mere-advisory-bodies-indias-pollution-boards-are-unable-to-regulate-air-quality\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Tuli (2020)</td><td style=\"padding:0.75rem\"><a href=\"https://www.financialexpress.com/opinion/flue-gas-desulphurisation-a-rs-80000-crore-investment-opportunity/1987779/\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Varadhan (2019)</td><td style=\"padding:0.75rem\"><a href=\"https://www.reuters.com/article/us-india-pollution-coal-exclusive/exclusive-over-half-of-indias-coal-fired-power-plants-set-to-miss-emission-norm-deadline-idUSKBN1XP1NW\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Veras et al. (2008)</td><td style=\"padding:0.75rem\"><a href=\"https://academic.oup.com/biolreprod/article/79/3/578/2557692\"><u>Source</u></a></td></tr><tr><td style=\"padding:0.75rem\">Zhang (2016)</td><td style=\"padding:0.75rem\"><a href=\"https://www.researchgate.net/profile/Xing-Zhang/publication/337446167_Emission_standards_and_control_of_PM25_from_coal-fired_power_plant/links/5ee23b50299bf1faac4b069a/Emission-standards-and-control-of-PM25-from-coal-fired-power-plant.pdf\"><u>Source</u></a></td></tr></tbody></table></figure>", "user": {"username": "Open Philanthropy"}}, {"_id": "4vZECPyvHaDFeEG9j", "title": "GiveWell's \"Giving 101\" guide", "postedAt": "2020-11-10T00:00:00.000Z", "htmlBody": "<p>The key principles we recommend you keep in mind when deciding where to give:</p>\n<h2>Your donation can change someone's life.</h2>\n<p>For a few thousand dollars or so, you can save or improve someone's life in the developing world. This claim isn't a normal \"marketing pitch\" you read in direct mail solicitations\u2014it's the result that outstanding charities achieve. For more, see:</p>\n<ul>\n<li><a href=\"https://www.givewell.org/giving101/Changing-Someones-Life\"><strong>Your Donation Can Change Someone's Life</strong></a></li>\n<li><a href=\"https://www.givewell.org/cost-to-save-a-life\"><strong>How Does $4,500 Save a Life?</strong></a></li>\n<li>Our full list of <a href=\"https://www.givewell.org/charities/top-charities\"><strong>Top Charities</strong></a></li>\n</ul>\n<h2>The wrong donation can accomplish nothing.</h2>\n<p>Charities that demonstrably change lives are the exception, not the rule. Why? Fundraisers often rely on social connections or emotional pleas, and almost never make fact-based demonstrations of programs' effectiveness. This means that lots of charities raise money and run programs without ever demonstrating that their programs actually work. Why should charities have to demonstrate that their programs work?</p>\n<p>Experts, governments and foundations have tried (and often failed) for decades to solve many of the same problems charities are working on today. This means that many charities may not be accomplishing anything at all. For more, see:</p>\n<ul>\n<li><a href=\"https://www.givewell.org/giving101/Accomplishing-Nothing\"><strong>The Wrong Donation Can Accomplish Nothing</strong></a></li>\n<li><a href=\"https://www.givewell.org/i-wouldnt-bet-on-most-charities\"><strong>Most Charities' Evidence</strong></a></li>\n</ul>\n<h2>Your dollar goes further overseas.</h2>\n<p>Ultimately, there's no \"right\" answer to the question of which cause you should support. As you consider that decision, it's worth recognizing that the impact you can have with your donation varies greatly between causes. If you focus on education in New York City, it costs over $100,000 to educate a student throughout 12 years of school. When supporting international aid, you can save a person's life for a few thousand dollars or so.<sup class=\"footnote-ref\"><a href=\"#fn-NCCcNWNPHF8cC2pBt-1\" id=\"fnref-NCCcNWNPHF8cC2pBt-1\">[1]</a></sup> That doesn't mean you should necessarily support international aid, but, just like any time you spend your money, it's important to know what you're getting. For more, see:</p>\n<ul>\n<li><a href=\"https://www.givewell.org/want-to-change-peoples-lives-give-internationally\"><strong>Your Dollar Goes Further Overseas</strong></a></li>\n</ul>\n<p><em>This work is licensed under a <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License.</a></em></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-NCCcNWNPHF8cC2pBt-1\" class=\"footnote-item\"><p>On average, grants from our <a href=\"https://www.givewell.org/top-charities-fund\"><strong>Top Charities Fund</strong></a> in 2021 saved a life for about $5,000. Learn more about our estimated cost to save a life in <a href=\"https://www.givewell.org/impact-estimates\"><strong>\"How We Produce Impact Estimates.\"</strong></a> <a href=\"#fnref-NCCcNWNPHF8cC2pBt-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "GiveWell"}}, {"_id": "rp699NtpE7dEcTQuq", "title": "Double crux: a strategy for mutual understanding", "postedAt": "2021-05-18T23:00:00.000Z", "htmlBody": "", "user": {"username": "Jesse Rothman"}}, {"_id": "ywebnJwyF57mnkNHQ", "title": "Tradeoffs", "postedAt": "2021-03-08T00:00:00.000Z", "htmlBody": "", "user": {"username": "Jesse Rothman"}}, {"_id": "ZhNaizQgYY9dXdQkM", "title": "Introduction to Effective Altruism", "postedAt": "2020-03-20T16:27:00.000Z", "htmlBody": "<h3>What is effective altruism?</h3>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Effective altruism is a project that aims to find the best ways to help others, and put them into practice.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">It\u2019s both a <strong>research field</strong>, which aims to identify the world\u2019s most pressing problems and the best solutions to them, and a <strong>practical community</strong> that aims to use those findings to do good.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">This project matters because, while many attempts to do good fail, some are enormously effective. For instance, some charities help 100 or even 1,000 times as many people as others, when given the same amount of resources.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">This means that by thinking carefully about the best ways to help, we can do far more to tackle the world\u2019s biggest problems.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Effective altruism was formalized by scholars at Oxford University, but has now spread around the world, and is being applied by tens of thousands of people in more than 70 countries.<sup id=\"fnref-1\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-1\">1</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">People inspired by effective altruism have worked on projects that range from funding the distribution of 200 million malaria nets, to academic research on the future of AI, to campaigning for policies to prevent the next pandemic.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">They\u2019re not united by any particular solution to the world\u2019s problems, but by a way of thinking. They try to find <em>unusually</em> good ways of helping, such that a given amount of effort goes an unusually long way. Here are some examples of what they've done so far, followed by the values that unite them:</p>\n<h3>What are some examples of effective altruism in practice?</h3>\n<h4>Preventing the next pandemic</h4>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Why this issue?</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">People in effective altruism <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://forum.effectivealtruism.org/topics/itn-framework\" target=\"_blank\" rel=\"noreferrer\">typically try to</a> identify issues that are big in scale, tractable, and unfairly neglected.<sup id=\"fnref-2\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-2\">2</a></sup> The aim is to find the biggest gaps in current efforts, in order to find where an additional person can have the greatest impact. One issue that seems to match those criteria is preventing pandemics.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Researchers in effective altruism argued as <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.openphilanthropy.org/research/biosecurity/\" target=\"_blank\" rel=\"noreferrer\">early as 2014</a> that, given the history of near-misses, there was a good chance that a large pandemic would happen in our lifetimes.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">But preparing for the next pandemic was, and remains, hugely underfunded compared to other global issues. For instance, the US invests around $8bn per year preventing pandemics, compared to around $280bn per year spent on counterterrorism over the last decade.<sup id=\"fnref-3\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-3\">3</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/2RSKzGMbKhiDFJCCWYAWrl/4d922de1a547a6c84fb9101494628520/preventing-pandemics.png\" alt=\"Pandemic prevention spending\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Preventing terror attacks is certainly important. But the scale of the issue seems smaller. For instance, just to focus on the number of deaths, in the last 50 years, around 500,000 people have been killed by terrorism. But over 21 million people were killed by COVID-19 alone<sup id=\"fnref-4\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-4\">4</a></sup> \u2013 or consider the 40 million killed by HIV/AIDS.<sup id=\"fnref-5\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-5\">5</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/425KeJZGR7KzkYCLd5XGRa/affb2cda3497371f1b1a67bfbdb7387a/people-died-pandemics.png\" alt=\"Pandemic deaths comparison\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Not to mention, a future pandemic could easily be much worse than COVID-19: there\u2019s nothing to rule out a disease that\u2019s more infectious than the Omicron variant, but that\u2019s as deadly as smallpox. (See more on the comparison in footnote 4.)</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In effective altruism, once a big and neglected problem has been identified, the community then looks for solutions that have a chance of making a big contribution to solving the problem, and are neglected by others working on that issue, which brings us to...</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Some examples of what\u2019s been done</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In 2016 Open Philanthropy \u2013 a foundation inspired by effective altruism \u2013 became the largest funder of the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.centerforhealthsecurity.org/\" target=\"_blank\" rel=\"noreferrer\">Johns Hopkins Center for Health Security</a>, which is one of the few groups doing research to identify better policy responses to pandemics, and was an important group in the response to COVID-19.<sup id=\"fnref-6\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-6\">6</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">When COVID-19 broke out, members of the community founded <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.1daysooner.org/\" target=\"_blank\" rel=\"noreferrer\">1DaySooner</a>, a non-profit that advocates for human challenge trials. In this type of vaccine trial, healthy volunteers are deliberately infected with the disease, enabling near-instant testing of the vaccine. As one of the only advocates for this intervention, 1DaySooner has signed up over 30,000 volunteers,<sup id=\"fnref-7\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-7\">7</a></sup> and played an important role in starting the world\u2019s first COVID-19 human challenge trial. This model can be repeated when we face the next pandemic.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Members of the effective altruism community helped to create the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://biodefensecommission.org/reports/the-apollo-program-for-biodefense-winning-the-race-against-biological-threats/\" target=\"_blank\" rel=\"noreferrer\">Apollo Programme for Biodefense</a>, a multibillion dollar policy proposal designed to prevent the next pandemic.</p>\n<h4>Providing basic medical supplies in poor countries</h4>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Why this issue?</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">It\u2019s common to say that charity begins at home, but in effective altruism, charity begins where we can help the most. And this often means focusing on the people who are most neglected by the current system \u2013 which is often those who are more distant from us.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Over 700 million people live on less than $1.90 per day.<sup id=\"fnref-8\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-8\">8</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In contrast, an American living near the poverty line lives on 20 times as much, and the average American college graduate lives on about 107 times as much. This places them in the top 1.3% of income, globally speaking.<sup id=\"fnref-9\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-9\">9</a></sup> (These amounts are already adjusted for the fact that money goes further in poor countries.)</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/2OBdC3oRfp4Naw8wr9CoT3/e4b50e3f0a32612355ad860212aba460/income-inequality.png\" alt=\"Income distribution\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Global inequality is extreme. Because of this, transferring resources to the very poorest people in the world can do a huge amount of good. In richer countries like the US and UK, governments are typically willing to spend over $1 million to save a life.<sup id=\"fnref-10\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-10\">10</a></sup> This is well worth doing, but in the world\u2019s poorest countries, the cost of saving a life is far lower.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.givewell.org/\" target=\"_blank\" rel=\"noreferrer\">GiveWell</a> is an organization that does in-depth research to find the most evidence-backed and cost-effective health and development projects. It discovered that while <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.givewell.org/international/technical/criteria/impact/failure-stories\" target=\"_blank\" rel=\"noreferrer\">many aid interventions don\u2019t work</a>, some, like providing insecticide-treated bednets, can save a child\u2019s life for about $5,500 on average. That's 180 times less.<sup id=\"fnref-11\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-11\">11</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">These basic medical interventions are so cheap and effective that even the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://blog.givewell.org/2015/11/06/the-lack-of-controversy-over-well-targ\" target=\"_blank\" rel=\"noreferrer\">most prominent aid sceptics agree</a> they\u2019re worthwhile.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/4boif3LzHTuApM9zJnLiCO/0240962897ddce69fd87e3b55b505e99/cost-to-save-a-life.png\" alt=\"Cost to save a life\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Some examples of what\u2019s been done</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Over <strong>110,000 individual donors</strong> have used GiveWell\u2019s research to contribute more than $1 billion to its recommended charities, supporting organisations like the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.givewell.org/charities/amf\" target=\"_blank\" rel=\"noreferrer\">Against Malaria Foundation</a>, which has distributed over 200 million insecticide-treated bednets. Collectively these efforts are estimated to have saved <strong>159,000 lives</strong>.<sup id=\"fnref-12\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-12\">12</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In addition to charity, it\u2019s possible to help the world\u2019s poorest people through business. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.wave.com/\" target=\"_blank\" rel=\"noreferrer\">Wave</a> is a technology company founded by members of the effective altruism community, which allows people to transfer money to several African countries faster and several times more cheaply than existing services. It\u2019s especially helpful for migrants sending money home to their families, and has been used by over 800,000 people in countries like Kenya, Uganda and Senegal. In Senegal alone, Wave has saved its users hundreds of millions of dollars in transfer fees \u2013 around 1% of the country\u2019s GDP.<sup id=\"fnref-13\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-13\">13</a></sup></p>\n<h4>Helping to create the field of AI alignment research</h4>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Why this issue?</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">People in effective altruism often end up focusing on issues that seem counterintuitive, obscure or exaggerated. But this is because it\u2019s more impactful to work on the issues that are neglected by others (all else equal), and these issues are (almost by definition) going to be unconventional ones. One example is the AI alignment problem.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Artificial intelligence (AI) is progressing rapidly. The leading AI systems are now able to engage in limited conversation, solve college-level maths problems, explain jokes, generate extremely realistic images from text, and do basic coding.<sup id=\"fnref-14\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-14\">14</a></sup> None of this was possible just ten years ago.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The ultimate goal of the leading AI labs is to develop AI that is as good as, or better than, human beings on all tasks. It\u2019s extremely hard to predict the future of technology, but <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.cold-takes.com/where-ai-forecasting-stands-today/\" target=\"_blank\" rel=\"noreferrer\">various arguments and expert surveys</a> suggest that this achievement is more likely than not this century. And according to <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/\" target=\"_blank\" rel=\"noreferrer\">standard economic models</a>, once general AI can perform at human level, technological progress could dramatically accelerate.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The result would be an enormous transformation, perhaps of a significance similar to or greater than the industrial revolution in the 1800s. If handled well, this transformation  could bring about abundance and prosperity for everyone. If handled poorly, it could result in an extreme concentration of power in the hands of a tiny elite.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In the worst case, we could <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\" target=\"_blank\" rel=\"noreferrer\">lose control</a> of the AI systems themselves. Unable to govern beings with capabilities far greater than our own, we would find ourselves with as little control over our future as chimpanzees have control over theirs.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">This means this issue could not only have a dramatic impact on the present generation, but also on all future generations. This makes it especially pressing from a <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://en.wikipedia.org/wiki/Longtermism\" target=\"_blank\" rel=\"noreferrer\">\u201clongtermist\u201d</a> perspective, a school of thinking which holds that improving the long-term future is a key moral priority of our time.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">How to ensure AI systems continue to further human values, even as they become equal (or superior) to humans in their capabilities, is called the AI alignment problem, and solving it requires advances in computer science.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Despite its potentially historic importance, only a couple of hundred researchers work on this problem, compared to tens of thousands working to make AI systems more powerful.<sup id=\"fnref-15\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-15\">15</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/4Dmdh2qtvIIFset8KpgQ3f/7e8e43eb82f3f6ff42b47828714a701a/ai-alignment.png\" alt=\"AI research distribution\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">It\u2019s hard to sum up the case for the issue in a few paragraphs, so if you\u2019d like to explore more, we\u2019d recommend starting <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment\" target=\"_blank\" rel=\"noreferrer\">here</a>, <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\" target=\"_blank\" rel=\"noreferrer\">here</a> and <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.cold-takes.com/most-important-century/\" target=\"_blank\" rel=\"noreferrer\">here</a>.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Some examples of what\u2019s been done</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">One priority is to simply tell more people about the issue. The book <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" target=\"_blank\" rel=\"noreferrer\">Superintelligence</a> was published in 2014, making the case for the importance of AI alignment, and became a <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies#Reception\" target=\"_blank\" rel=\"noreferrer\">New York Times best-seller</a>.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Another priority is to build a research field focused on this problem. For instance, AI pioneer Stuart Russell, and others inspired by effective altruism, founded <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://humancompatible.ai/\" target=\"_blank\" rel=\"noreferrer\">The Center for Human-Compatible AI</a> at UC Berkeley. This research institute aims to develop a new paradigm of AI development, in which the act of furthering human values is central.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Others have helped to start teams focused on AI alignment at major AI labs such as <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.deepmind.com/safety-and-ethics\" target=\"_blank\" rel=\"noreferrer\">DeepMind</a> and <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://openai.com/alignment/\" target=\"_blank\" rel=\"noreferrer\">OpenAI</a>, and outline research agendas for AI alignment, in works such as <em><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://arxiv.org/abs/1606.06565\" target=\"_blank\" rel=\"noreferrer\">Concrete Problems in AI Safety</a></em>.</p>\n<h4>Ending factory farming</h4>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Why this issue?</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">People in effective altruism try to extend their circle of concern \u2013 not only to those living in distant countries or future generations, but also to non-human animals.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Nearly 10 billion animals live and die in factory farms in the US every year<sup id=\"fnref-16\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-16\">16</a></sup> \u2013 often unable to physically turn around their entire lives, or castrated without anaesthetic.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Lots of people agree we shouldn\u2019t make animals suffer needlessly, but most of this attention goes towards pet shelters. In the US, about 1,400 times more animals pass through factory farms than pet shelters.<sup id=\"fnref-17\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-17\">17</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/5dA70W90MID78GFwwIRqld/b0d116e4fd89ee23038e69867a46e2c1/farm-population.png\" alt=\"Animals welfare populations\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Despite this, pet shelters receive around $5 billion per year in the US, compared to only $97 million on advocacy to end factory farming.<sup id=\"fnref-18\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-18\">18</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/1kZ0PaxZYmCbQojCedld2r/6ad67f274012f394bf0c3fd6962fa6c3/spending-on-farms.png\" alt=\"Animal welfare spending\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Some examples of what\u2019s been done</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">One strategy is advocacy. The <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://openwingalliance.org/impact\" target=\"_blank\" rel=\"noreferrer\">Open Wing Alliance</a>, which received significant funding from funders inspired by effective altruism, developed a campaign to encourage large companies to commit to stop buying eggs from caged chickens. To date, they have won over 2,200 commitments, and as a result over 100 million birds have been spared from cages.<sup id=\"fnref-19\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-19\">19</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Another strategy is to create alternative proteins, which if made cheaper and tastier than factory farmed meat, could make demand disappear, ending factory farming. The <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://gfi.org/\" target=\"_blank\" rel=\"noreferrer\">Good Food Institute</a> is working to kick-start this industry, helping to create companies like Dao Foods in China and Good Catch in the US, encouraging big business to enter the industry (including JBS, the world\u2019s largest meat company) and securing tens of millions of dollars of government support.<sup id=\"fnref-20\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-20\">20</a></sup></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Open Philanthropy was an <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.openphilanthropy.org/grants/impossible-foods-rd-investment/#:~:text=working%20to%20overcome.-,Background,Impossible%20Burger%2C%20in%20July%202016.\" target=\"_blank\" rel=\"noreferrer\">early investor</a> in <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://en.wikipedia.org/wiki/Impossible_Foods\" target=\"_blank\" rel=\"noreferrer\">Impossible Foods</a>, which created the Impossible Burger \u2013 an entirely vegan burger that tastes much more like meat, and is now sold in Burger King.</p>\n<h4>Improving decision-making</h4>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Why this issue?</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">People who want to do good often prefer to <em>directly</em> tackle problems, since it\u2019s more motivating to see the tangible effects of their actions. But what matters is that the world gets better, not that you do it with your own two hands. So people applying effective altruism often try to help indirectly, by empowering others.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">One example of this is by improving decision-making. Namely: if key actors \u2014 such as politicians, private and third sector leaders, or grantmakers at funding bodies \u2014 were generally better at making decisions, society would be in a better position to deal with a whole range of future global problems, whatever they turn out to be.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">So, if we can find new, neglected ways to improve the decision-making of important actors, that could be a route to having a big impact. And it seems like there are some promising solutions that could achieve this.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><em><strong>Some examples of what\u2019s been done</strong></em></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Many global problems are exacerbated by a lack of trustworthy information. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.metaculus.com/questions/\" target=\"_blank\" rel=\"noreferrer\">Metaculus</a> is a forecasting technology platform that identifies important questions (such as the chance of Russia invading Ukraine), aggregates forecasts made by hundreds of forecasters, and weighs them by their past accuracy. Metaculus gave a probability of a Russian invasion of Ukraine of 47% by mid January 2022, and 80% shortly before the invasion on the 24th of February<sup id=\"fnref-21\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-ref MuiTypography-colorPrimary\" href=\"#fn-21\">21</a></sup> \u2013 a time when many pundits, journalists and experts were saying it definitely wouldn\u2019t happen.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><img class=\"jss29\" src=\"//images.ctfassets.net/ohf186sfn6di/22g4yYIsFMo0Kxb5kDsFxW/1358738ab1c09daadf73896c9929e89d/metaculus.png\" alt=\"Metaculus Ukraine Prediction\"></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://globalprioritiesinstitute.org/\" target=\"_blank\" rel=\"noreferrer\">Global Priorities Institute</a> at the University of Oxford does foundational research at the intersection of philosophy and economics into how key decision-makers can identify the world\u2019s most pressing problems. It has helped to create a new academic field of global priorities research, creating a <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://globalprioritiesinstitute.org/research-agenda/\" target=\"_blank\" rel=\"noreferrer\">research agenda</a>, publishing <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://globalprioritiesinstitute.org/papers/\" target=\"_blank\" rel=\"noreferrer\">tens of papers</a>, and helping to inspire relevant research at Harvard, NYU, UT Austin, Yale, Princeton and elsewhere.</p>\n<h3>What values unite effective altruism?</h3>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Effective altruism isn't defined by the projects above, and what it focuses on could easily change. What defines effective altruism are some tentative values and principles that underpin its search for the best ways of helping others:</p>\n<ol>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"0\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><strong>Prioritization:</strong> Our <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://en.wikipedia.org/wiki/Scope_neglect\" target=\"_blank\" rel=\"noreferrer\">intuitions</a> about doing good don't usually take into account the scale of the outcomes \u2014 helping 100 people often makes us feel as satisfied as helping 1000. But since some ways of doing good also achieve dramatically more than others, it\u2019s vital to attempt to use numbers to roughly weigh how much different actions help. The goal is to find <em>the best</em> ways to help, rather than just working to make any difference at all.</p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"1\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><strong>Impartial altruism:</strong> It's easy \u2014 and reasonable \u2014 to have special concern for one's own family, friends or nation. But, when trying to do as much good as possible, it seems that we should give everyone's interests equal weight, no matter where or when they live. This means focusing on the groups who are most neglected, which usually means focusing on those who don\u2019t have as much power to protect their own interests.</p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"2\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><strong>Open truthseeking:</strong> Rather than starting with a commitment to a certain cause, community or approach, it\u2019s important to consider many different ways to help and seek to find the best ones. This means putting serious time into deliberation and reflection on one\u2019s beliefs, being constantly open and curious for new evidence and arguments, and being ready to change one\u2019s views quite radically.</p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"3\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><strong>Collaborative spirit:</strong> It\u2019s often possible to achieve more by working together, and doing this effectively requires high standards of honesty, integrity, and compassion. Effective altruism does not mean supporting \u2018ends justify the means\u2019 reasoning, but rather is about being a good citizen, while working toward a better world.</p>\n</li>\n</ol>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">We\u2019re not totally confident in the above ideas - but we think that they are probably right, and that they are undervalued by much of society. Anyone who is trying to find better ways to help others is participating in effective altruism. This is true no matter how much time or money they want to give, or which issue they choose to focus on.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Effective altruism can be compared to the scientific method. Science is the use of evidence and reason in search of truth \u2013 even if the results are unintuitive or run counter to tradition. Effective altruism is the use of evidence and reason in search of the best ways of doing good.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The scientific method is based on simple ideas (e.g. that you should test your beliefs) but it leads to a radically different picture of the world (e.g. quantum mechanics). Likewise, effective altruism is based on simple ideas \u2013 that we should treat people equally and it\u2019s better to help more people than fewer \u2013 but it leads to an unconventional and ever-evolving picture of doing good.</p>\n<h3>How can you take action?</h3>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">People interested in effective altruism most often attempt to apply the ideas in their lives by:</p>\n<ul>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"0\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Choosing careers that help tackle pressing problems, or by finding ways to use their existing skills to contribute to these problems, such as by using advice from <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://80000hours.org/\" target=\"_blank\" rel=\"noreferrer\">80,000 Hours</a>.</p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"1\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Donating to carefully chosen charities, such as by using research from <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.givewell.org/\" target=\"_blank\" rel=\"noreferrer\">GiveWell</a> or <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.givingwhatwecan.org/best-charities-to-donate-to-2022\" target=\"_blank\" rel=\"noreferrer\">Giving What We Can</a>.</p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"2\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://80000hours.org/career-reviews/founder-impactful-organisations/#lists-of-ideas\" target=\"_blank\" rel=\"noreferrer\">Starting new organizations</a> that help to tackle pressing problems.</p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"3\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Helping to build communities tackling pressing problems.</p>\n</li>\n</ul>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">See a <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://forum.effectivealtruism.org/topics/take-action\" target=\"_blank\" rel=\"noreferrer\">longer list of ways to take action</a>.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The above are not exhaustive. You can apply effective altruism no matter how much you want to focus on doing good, and in any area of your life \u2013 what matters is that, no matter how much you want to contribute, your efforts are driven by the four values above, and you try to make your efforts as effective as possible.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Typically, this involves trying to identify big and neglected global problems, the most effective solutions to those problems, and ways you can contribute to those solutions \u2013 with whatever time or money you\u2019re willing to give.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">By doing this and thinking carefully, you might find it\u2019s possible to have far more impact with those resources. It really is possible to save hundreds of people\u2019s lives over your career. And by teaming up with others in the community, you can play a role in tackling some of the most important issues civilization faces today.</p>\n\n<div class=\"footnotes\">\n<hr class=\"jss33\">\n<ol>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-1\" index=\"0\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">You can see the global distribution of local EA groups on the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://forum.effectivealtruism.org/community\" target=\"_blank\" rel=\"noreferrer\">Effective Altruism Forum</a>, which lists groups in over 70 countries.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-1\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-2\" index=\"1\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The less neglected an issue, the more the best opportunities will have been taken, so the harder it will be for an additional person to make an impact.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In fact, there are <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174703/https://www.fhi.ox.ac.uk/law-of-logarithmic-returns/\" target=\"_blank\" rel=\"noreferrer\">good reasons to expect</a> that returns to investment in an issue are roughly logarithmic.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Logarithmic returns imply that if 10 times more has been invested in one cause compared to another, then additional resources will achieve about 1/10 as much progress.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">If the two issues are equally important, and then an additional person working on the more neglected one will have ten times the impact.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-2\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-3\" index=\"2\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">From 2010 to 2019, US Federal Funding for Health Security is estimated at $141 billion. We judge that 55% of this was spent on what could arguably prevent future pandemics. For example, 4% was spent tackling the ongoing ebola epidemic, which provided infrastructure for potential other pandemics. However, 17% was spent on chemical and nuclear radiation threats in a way unlikely to affect future pandemic spread.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">141 billion * 0.55 = 79 billion</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Annualized over the ten year period is $8 billion per year.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.liebertpub.com/doi/10.1089/hs.2018.0077\" target=\"_blank\" rel=\"noreferrer\">Federal funding for health security in FY2019</a>\nWatson, Crystal, et al., Health security 16.5 (2018): pages 281-303. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20200305190440/https://www.liebertpub.com/doi/10.1089/hs.2018.0077\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 5 March 2020.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220708181859/https://www.openphilanthropy.org/research/biosecurity/\" target=\"_blank\" rel=\"noreferrer\">Open Philanthropy</a> also identifies other foundations and philanthropists working on the topic prior to the COVID pandemic, which we believe total under $100 million in funding.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Crawford, director of the Costs of War project, calculates US spending on Counter-Terrorism from 2001-2022 to be $5.8 trillion.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">5.8 trillion / 20 years = $290 billion per year.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://watson.brown.edu/costsofwar/files/cow/imce/papers/2021/Costs%20of%20War_U.S.%20Budgetary%20Costs%20of%20Post-9%2011%20Wars_9.1.21.pdf\" target=\"_blank\" rel=\"noreferrer\">United States budgetary costs of Post-9/11 wars</a>\nCrawford, Neta C., Watson Institute for International &amp; Public Affairs, Brown University, 2021. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220726160700/https://watson.brown.edu/costsofwar/files/cow/imce/papers/2021/Costs%20of%20War_U.S.%20Budgetary%20Costs%20of%20Post-9%2011%20Wars_9.1.21.pdf\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 26 July 2022.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-3\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-4\" index=\"3\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Deaths from terrorism from 1970-2020 were approximately 456,000. This is from the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220811132237/https://www.start.umd.edu/gtd/search/Results.aspx?chart=fatalities&amp;casualties_type=&amp;casualties_max=\" target=\"_blank\" rel=\"noreferrer\">Global Terrorism Database 2020</a>, accessed 11 August 2022.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Note that Our World in Data says \"The Global Terrorism Database is the most comprehensive dataset on terrorist attacks available and recent data is complete. However, we expect, based on our analysis, that longer-term data is incomplete (with the exception of the US and Europe). We therefore do not recommend this dataset for the inference of long-term trends in the prevalence of terrorism globally.\"</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">This means the source above is likely an undercount of confirmed terrorism deaths; however, even if we assume deaths have been at the same level as the highest recorded decade (2010-2020) since 1970, the total deathtoll would still only be 1.2 million; far less than deaths due to pandemics.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Deaths from COVID-19:</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The Economist estimated cumulative excess deaths due to COVID-19 were 21.47m as of June 2022, and this amount is still rising.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">You can see this data and their model through <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171227/https://ourworldindata.org/grapher/excess-deaths-cumulative-economist-single-entity?country=~OWID_WRL\" target=\"_blank\" rel=\"noreferrer\">Our World in Data</a> (archived page, retrieved 28 July 2022).</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">We see this as the best current estimate of COVID-19\u2019s total death toll. The number of <em>confirmed</em> deaths is lower, at around 6 million, but this excludes deaths that were indirectly caused or weren\u2019t reported. The Economist\u2019s methodology compares excess deaths to the seasonal average, to estimate in total how many additional people died, and adjusts for underreporting.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Deaths due to pandemics and terrorism are both heavy tailed, so past death rates will typically understimate the magnitude of the risk.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">For instance, it\u2019s possible that terrorists could set off a nuclear weapon in a large city, which might kill over 1 million people. This didn\u2019t happen over the last fifty years, but would have been the main driver of the death toll if it had. And likewise, there could have been a pandemic much worse than COVID-19 or HIV/AIDS in the last 50 years.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The key question then becomes whether the historical record is a greater undercount of the risk for terrorism or for pandemics (i.e. whether terrorism deaths are more heavily-tailed than pandemic deaths).</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">It seems plausible that the worst case scenario from pandemics is worse than for terrorism. There\u2019s nothing to rule out the emergence of a pandemic that\u2019s more infectious than COVID-19, but with a fatality rate of 10-50%, or worse. And there seem to be more near-misses in the historical record.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">So, the problem of missing tail events in the sample may well be worse for pandemics than terrorism. Indeed, the most plausible way for terrorism to kill 1+ billion people is probably via causing a pandemic.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Given that terrorism receives ~100x more funding than pandemic prevention, while pandemics seem to have caused 10-100x more deaths historically, the corrections would need to be very heavily in favour of terrorism in order for the current allocation of resources to seem more balanced.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The above analysis was just in terms of the number of deaths, since that\u2019s an important yet relatively measurable metric. Deaths due to both pandemics and terrorism also produce important indirect costs, and a fuller comparison would attempt to consider the relative scale of each.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-4\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-5\" index=\"4\">\n<blockquote>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">\u201c40.1 million [33.6 million\u201348.6million] people have died from AIDS-related illnesses since the start of the epidemic.\u201d</p>\n</blockquote>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.unaids.org/en/resources/fact-sheet\" target=\"_blank\" rel=\"noreferrer\">Global HIV &amp; AIDS statistics \u2014 Fact sheet</a> UNAIDS, 2022. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220811142820/https://www.unaids.org/en/resources/fact-sheet\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 11 August 2022.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-5\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-6\" index=\"5\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Open Philanthropy is a foundation inspired by effective altruism. They first funded the Johns Hopkins Centre for Health Security (CHS) <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171005/https://www.openphilanthropy.org/grants/johns-hopkins-center-for-health-security-emerging-leaders-in-biosecurity-initiative/\" target=\"_blank\" rel=\"noreferrer\">in 2016</a>. This was followed by several other large grants, including one for <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171246/https://www.openphilanthropy.org/grants/johns-hopkins-center-for-health-security-biosecurity-global-health-security-and-global-catastrophic-risks-2017/\" target=\"_blank\" rel=\"noreferrer\">$16m in 2017</a> and another for <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171354/https://www.openphilanthropy.org/grants/johns-hopkins-center-for-health-security-biosecurity-global-health-security-and-global-catastrophic-risks-2019/\" target=\"_blank\" rel=\"noreferrer\">$19.5m in 2019</a>.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-6\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-7\" index=\"6\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">38,659 volunteers, as of 7 July 2022. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220707165739/https://www.1daysooner.org/\" target=\"_blank\" rel=\"noreferrer\">1Day Sooner</a><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-7\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-8\" index=\"7\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Before COVID-19, the number of people living on less that $1.90 per day had decreased to 689 million in 2017. However, estimates now point to the first rise in the extreme poverty rate since 1998, leading to an estimated 731 million people now living on less than $1.90 per day.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://unstats.un.org/sdgs/report/2021/goal-01/\" target=\"_blank\" rel=\"noreferrer\">UN SDG 1 - End poverty in all its forms</a>. UN Statistics, 2022. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"javascript:void(0)\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 26 July 2022.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">These estimates have been adjusted for the fact that money goes further in poor countries (purchasing parity). There are many complications to the estimates, but it\u2019s clear that hundreds of millions of people live at near subsidence levels of income. See \u201c<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171308/https://80000hours.org/2017/04/how-accurately-does-anyone-know-the-global-distribution-of-income/\" target=\"_blank\" rel=\"noreferrer\">how accurately does anyone know the global distribution of income?</a>\u201d if you\u2019d like to learn more.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-8\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-9\" index=\"8\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The US poverty line for 1 person is an annual income of <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171458/https://aspe.hhs.gov/topics/poverty-economic-mobility/poverty-guidelines\" target=\"_blank\" rel=\"noreferrer\">$13,590</a>.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">13,590 / 365 = $37.23 per day.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">This is 20x the international poverty line of $1.90, which is adjusted for purchasing parity.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">According to the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171531/https://www.census.gov/data/tables/2022/demo/acs-2019.html\" target=\"_blank\" rel=\"noreferrer\">2019 census</a>, full-time workers aged 25-65 with a college degree or higher earned a median of $74,000 per year.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">$74,000 / 365 = $202.7 per day</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">$202 / 1.9 = 107x.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">According to <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://smartasset.com/taxes/income-taxes#PgAT7oh2Ae\" target=\"_blank\" rel=\"noreferrer\">SmartAsset</a>, a single person household earning $74,000 pre-tax and living in New York, receives about $53,000 post-tax.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">According to Giving What We Can\u2019s <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728165745/https://howrichami.givingwhatwecan.org/how-rich-am-i?income=53000&amp;countryCode=USA&amp;household%5Badults%5D=1&amp;household%5Bchildren%5D=0\" target=\"_blank\" rel=\"noreferrer\">calculator</a>, a post-tax income of $53,000 puts you in the top 1.3% of income globally.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-9\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-10\" index=\"9\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The UK\u2019s National Institute for Health and Care Excellence recommends spending up to \u00a330,000 per <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://en.wikipedia.org/wiki/Quality-adjusted_life_year\" target=\"_blank\" rel=\"noreferrer\">quality-adjusted life year</a> (QALY) gained, where the intervention is reliable.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">\u201cAbove a most plausible ICER of \u00a330,000 per QALY gained, advisory bodies will need to make an increasingly stronger case for supporting the intervention as an effective use of NHS resources\u201d\n<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.nice.org.uk/process/pmg4/chapter/incorporating-health-economics\" target=\"_blank\" rel=\"noreferrer\">Methods for the development of NICE public health guidance</a>. UK National Institute for Health and Care Excellence, September 2012. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728173611/https://www.nice.org.uk/process/pmg4/chapter/incorporating-health-economics\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed July 28, 2022.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">It\u2019s typical in global health to say that saving one life is equivalent to 30 QALYs. Source: <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728171610/https://openknowledge.worldbank.org/bitstream/handle/10986/7039/364010PAPER0Gl101OFFICIAL0USE0ONLY1.pdf?sequence=1\" target=\"_blank\" rel=\"noreferrer\">World Bank (Box 1.1)</a></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">This amounts to a cost of saving a life of 30 x \u00a330,000 = \u00a3900,000 = $1.1 million.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In the US, different global agencies estimate \u201c<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://en.wikipedia.org/wiki/Value_of_life\" target=\"_blank\" rel=\"noreferrer\">the value of life</a>\u201d, and use this figure in the prioritization of different spending projects. The Federal Emergency Management Agency estimated the value of life at <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220516225829/https://www.fema.gov/sites/default/files/2020-08/fema_bca_toolkit_release-notes-july-2020.pdf\" target=\"_blank\" rel=\"noreferrer\">$7.5 million</a> in 2020. This estimate has fluctuated based on the context. For example, the US Department of Transport estimated the value of life to be between <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728173630/https://www.transportation.gov/sites/dot.gov/files/docs/VSL_Guidance_2014.pdf\" target=\"_blank\" rel=\"noreferrer\">$5.2 million and $13.0 million</a> in 2014.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-10\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-11\" index=\"10\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">GiveWell's estimates of the cost to save a life have varied over time (depending on their research, and which opportunities are available), but have typically fallen within $2,500 to $7,500. In 2021, GiveWell estimated that $5500 spent on distributing insecticide-treated bednets will save one life in expectation.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">You can see their most up-to-date estimates in their full cost-effectiveness analysis:\n<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.givewell.org/impact-estimates\" target=\"_blank\" rel=\"noreferrer\">How We Produce Impact Estimates</a> GiveWell, July 2022. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728173731/https://www.givewell.org/impact-estimates\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 28 July 2022.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-11\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-12\" index=\"11\">\n<blockquote>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">\u201cMore than 110,000 donors have trusted GiveWell to direct their donations. Together, they have given over $1 billion to the organizations we recommend. These donations will save over 150,000 lives and provide cash grants of over $175 million to the global poor.\u201d</p>\n</blockquote>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.givewell.org/about\" target=\"_blank\" rel=\"noreferrer\">About GiveWell</a>, GiveWell, July 2022. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728173759/https://www.givewell.org/about\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 28 July 2022.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-12\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-13\" index=\"12\">\n<blockquote>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">\u201cWhen Wave launched in Senegal, our average transfer would have cost 3-5x more if done via the largest existing mobile money system. Multiplied by our millions of monthly active users, that comes out to a savings of over $200 million every year, \u2026 around 1% of Senegal\u2019s GDP.\u201d</p>\n</blockquote>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.wave.com/en/blog/world/\" target=\"_blank\" rel=\"noreferrer\">Working at Wave is an extremely effective way to improve the world</a>. Ben Kuhn, July 8 2021. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220726160441/https://www.wave.com/en/blog/world/\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 26 July 2022.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-13\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-14\" index=\"13\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Conversation: \"Our best end-to-end trained Meena model achieves a\u2026 SSA [Sensibleness and Specificity Average] score of 72%... our SSA score of 72% is not far from the 86% SSA achieved by the average person.\u201d <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\" target=\"_blank\" rel=\"noreferrer\">Towards a Conversational Agent that Can Chat About\u2026Anything</a>. Adiwardana et. al., Google, 28 January 2020. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174004/https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 28 July 2022.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Math: The graphs in the attached article show Google\u2019s Minerva accurately answers over 50% of \u201chigh school math competition-level problems\u201d. Other state-of-the-art models were achieving less than 10% accuracy.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\" target=\"_blank\" rel=\"noreferrer\">Minerva: Solving Quantitative Reasoning Problems with Language Models</a>. Dyer et. al, Google, 30 June 2022. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174101/https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed 28 July 2022.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Jokes: Google\u2019s AI PaLM can provide explanations for jokes never seen before, including jokes nowhere on the internet. For example:</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">\u201cJoke: Did you see that Google just hired an eloquent whale for their TPU team? It showed them how to communicate between two different pods!</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Explanation: TPUs are a type of computer chip that Google uses for deep learning. A \u2018pod\u2019 is a group of TPUs. a \u2018pod\u2019 is also a group of whales. The joke is that the whale is able to communicate between two groups of whales, but the speaker is pretending that the whale is able to communicate between two groups of TPUs\u201d</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\" target=\"_blank\" rel=\"noreferrer\">Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance</a>. Narang et. al., Google, 4 April 2022. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174127/https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\" target=\"_blank\" rel=\"noreferrer\">Archived link</a></p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Images: Example images from OpenAI\u2019s Dall-E 2 can be seen <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174215/https://openai.com/dall-e-2/\" target=\"_blank\" rel=\"noreferrer\">here</a>.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Coding: Section 3.1 in Salesforce\u2019s <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220404070940/http://arxiv.org/pdf/2203.13474.pdf\" target=\"_blank\" rel=\"noreferrer\">\u2018A Conversational Paradigm for Program Synthesis\u2018</a> research paper about CodeGen, their AI tool turning human instructions into code, outlines that CodeGen achieves a 75% HumanEval score, meaning it can solve 75% of the programming challenges described with normal human language in the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20211011051701/https://paperswithcode.com/dataset/humaneval\" target=\"_blank\" rel=\"noreferrer\">HumanEval set</a>.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-14\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-15\" index=\"14\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">It\u2019s difficult to estimate the number of researchers focused on a certain topic since it\u2019s hard to define the topic in the first place, many researchers work on multiple topics, and it\u2019s hard to know the bar for \u2018being a researcher\u2019. So these numbers should be understood as estimates to within a factor of three or so, and they could be out by an order of magnitude depending on some interpretations of the question.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In 2020, 87,000 authors published AI research on arXiv. The <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20201101062242/https://jfgagne.ai/global-ai-talent-report-2020/#anchor-5\" target=\"_blank\" rel=\"noreferrer\">2020 Global AI Talent Report</a> from Element AI estimates there are even more people than this working on AI development globally, with 155,000 people labelled on social media as working in AI research or engineering. However we expect some working in AI engineering to not be working on engineering new advancements in AI. We have taken the smaller estimate of 87,000 and roughly halved it for an estimate of 40,000.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In 2021, <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174413/https://forum.effectivealtruism.org/posts/8ErtxW7FRPGMtDqJy/the-academic-contribution-to-ai-safety-seems-large\" target=\"_blank\" rel=\"noreferrer\">Gavin Leech</a> estimated that 270 to 830 FTE people worked on AI Safety. However, the upper end of the range of this estimate is based on what we think is an overly broad notion of what constitutes research into AI alignment, and much of the sum was driven by adding up time from lots of researchers spending a small fraction of their time on safety research; while our aim is to quantify the number of researchers <em>focused</em> on AI safety.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728170925/https://aiwatch.issarice.com/\" target=\"_blank\" rel=\"noreferrer\">AI Watch</a> attempted a headcount of AI Safety researchers, which found 160 notable researchers who have worked on AI Safety. This includes many people who have not published on AI safety in over a year, whereas for the estimate of 87,000 above, all had published in the last year.  On the other hand, the bar for being a \u2018notable\u2019 researcher might be higher than publishing in arXiv.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Our final estimate is 300 researchers focused on AI safety.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-15\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-16\" index=\"15\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In 2018, 9.56 billion farm animals were slaughtered for meat in the US. This number has likely risen since then. This includes 9.16 billion chickens; 237 million turkey; 125 million pigs; 34 million cattle and 2 million sheep. Source: visualization at <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://ourworldindata.org/meat-production#number-of-animals-slaughtered\" target=\"_blank\" rel=\"noreferrer\">Our World in Data</a>, using data from the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220811152257/https://www.fao.org/faostat/en/\" target=\"_blank\" rel=\"noreferrer\">UN Food and Agriculture Organization</a>.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-16\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-17\" index=\"16\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In 2021, approximately 6.5 million animals passed through animal shelters in the US. In 2011, this number was 7.2 million. Assuming a steady decline, this means in 2018 there were approximately 6.7 million animals passing through animal shelters.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">9.56 billion / 6.7 million = 1427 times as many animals in factory farms.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\"><a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.aspca.org/helping-people-pets/shelter-intake-and-surrender/pet-statistics\" target=\"_blank\" rel=\"noreferrer\">Pet Statistics</a>. American Society for the Prevention of Cruelty to Animals, 2021. <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20210805170626/https://www.aspca.org/helping-people-pets/shelter-intake-and-surrender/pet-statistics\" target=\"_blank\" rel=\"noreferrer\">Archived link</a>, accessed August 2021.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-17\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-18\" index=\"17\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Animal shelter spending:</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Andrew Rowan calculated $5 billion of funding to the top 3000 animal shelter organizations in the US in 2018, published in his paper <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://www.researchgate.net/publication/335325531_Cat_Demographics_Impact_on_Wildlife_in_the_USA_the_UK_Australia_and_New_Zealand_Facts_and_Values\" target=\"_blank\" rel=\"noreferrer\">\u201cCat Demographics &amp; Impact on Wildlife in the USA, the UK, Australia and New Zealand: Facts and Values\u201d</a>\nRowan et. al. (2020), Journal of Applied Animal Ethics Research, pages 7\u201337.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Andrew Rowan confirmed the data behind these calculations in correspondence with us.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Farm animal advocacy funding:</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">Research from Open Philanthropy published <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://docs.google.com/spreadsheets/d/19vdy-QojtXXcgXVUIYqbCREpX7dnQqSRPTgcly8trXA/edit#gid=1364826426\" target=\"_blank\" rel=\"noreferrer\">here</a> finds the following funding for farm animal advocacy groups in 2018:</p>\n<ul>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"0\">$32.3 million for Established US groups (PETA, PCRM, HSUS, ALDF, ASPCA)</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"1\">$32.6 million for new, major US-based groups (CIWF, WAP, RSPCA, HSI)</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" index=\"2\">$32.2 million for all other US groups</li>\n</ul>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">32.3 + 32.6 + 32.2 = $97.1 million<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-18\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-19\" index=\"18\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">106.5 million hens are now in cage-free housing in the US alone as of May 2022, as reported by the <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174554/https://www.ams.usda.gov/sites/default/files/media/Egg%20Markets%20Overview.pdf\" target=\"_blank\" rel=\"noreferrer\">USDA Egg Markets Overview</a>, compared to <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20160823231743/https://www.ams.usda.gov/sites/default/files/media/Egg%20Markets%20Overview.pdf\" target=\"_blank\" rel=\"noreferrer\">17 million in 2016</a>. We believe another 100 million have become cage-free in Europe as a result of Open Wing Alliance\u2019s work, although this number is harder to attribute to them specifically.</p>\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">In addition, advocates have now secured corporate pledges that, if implemented, should cover more than 500 million hens alive at any time.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-19\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-20\" index=\"19\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">After engagement with GFI, the US government announced a $10 million grant to create a center of excellence in cellular agriculture at Tufts University. The UK\u2019s independent National Food Strategy recommended a \u00a3125 million investment in alternative protein research and innovation. Source: <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220728174648/https://gfi.org/wp-content/uploads/2022/03/GFI_Year-in-Review_2021_v2.pdf\" target=\"_blank\" rel=\"noreferrer\">GFI Year in Review 2021</a> (page 3)<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-20\">\u21a9</a></p>\n</li>\n<li class=\"MuiTypography-root MuiTypography-body1\" id=\"fn-21\" index=\"20\">\n<p class=\"MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom\">The full timeline of forecasts can be found on <a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-colorPrimary\" href=\"https://web.archive.org/web/20220228132434/https://www.metaculus.com/questions/8898/russian-invasion-of-ukraine-before-2023/\" target=\"_blank\" rel=\"noreferrer\">Metaculus</a>.<a class=\"MuiTypography-root MuiLink-root MuiLink-underlineHover footnote-backref MuiTypography-colorPrimary\" href=\"#fnref-21\">\u21a9</a></p></li></ol></div>", "user": {"username": "EA Fellowship"}}, {"_id": "qKHZ6RSmefqYagJbE", "title": "[Creative Writing Contest] I See Dead Kids", "postedAt": "2021-10-13T04:00:00.000Z", "htmlBody": "<p><i>Written by Eneasz Brodski</i></p><hr><p>I see dead kids everywhere. I did it to myself, but it\u2019s OK. You get used to it.</p><p>I\u2019m scrolling through my phone--over a tenth of a dead kid when I bought it--when Tandee walks into the coffee shop.&nbsp; I smile and stand up, but she waves at me to stay at the table. She orders for both of us--less than one two-hundredth of a dead kid total. I don\u2019t have coffee that often, I can make a single dead kid stretch for years.</p><p>Tandee is a divorcee, with one kid. Living, of course. Dead kids don\u2019t ever come up on a first date. Or if they did, there wouldn\u2019t be a second one. Some things you roll out over time, not because you\u2019re hiding, but because breaking that much from social norms signals worrisome traits.</p><p>My daughter\u2019s name would have been Evangaline. I haven\u2019t actually had a relationship go long enough to tell anyone that. Not yet, anyway. I know I\u2019ll get there with the right person, and I\u2019ll know they\u2019re the right person when I\u2019m ready to say it. I\u2019m not in a rush, nowadays. I\u2019d rather be sure.</p><p>Tandee is a teacher, and she\u2019s passionate about education. I like people with a strong passion, they feel more real. More alive. Her opinions on public versus private schooling ignite a fire in her eyes that remind me of Becca, and I have to look away for a moment.</p><p>We go to a movie afterwards, because we\u2019re both relearning this whole process, and cliches are safe. Tickets and snacks combined are over one one-hundredth of a dead kid. A dead kid penny. It seems excessive, but I haven\u2019t gone out in quite a while. I turn off the visual filter for a second, to check the price in US dollars. The small fraction and child\u2019s smiling face in my vision is replaced with a familiar dollar sign and a proportional number. No one else is acting surprised or outraged, so I guess I\u2019m just out of the loop. This is just how much these next couple hours are worth. I turn my filter back on.</p><p>The movie is a comedy, but absolutely not a rom-com. We aren\u2019t stupid. And yet, when a misbehaved pooch destroys a clue, I instinctively think of how much Becca is going to love this part. My smile freezes as I\u2019m pierced by that hot-knife-in-my-lungs pain. What am I doing here? It still hasn\u2019t been long enough, has it?</p><p>Becca and I realized something was wrong early in the third trimester. Rapid-fire batteries of test, ramping quickly in complexity, finally revealed a rare condition, named after a doctor otherwise unknown. It was already too late for Evangaline. She was lost, but the doctors could still save Becca with surgery, if nothing went wrong.</p><p>Fate had other plans.</p><p>After the funeral, during the downward spiral, I grew obsessive. I discovered that this didn\u2019t have to happen. There was a potential cure that could have, theoretically, prevented this entirely. Developing it would have cost hundreds of millions of dollars. Hundreds of millions to cure a couple dozen people per year, most of whom didn\u2019t even die anyway. The research costs would never be recouped.</p><p>I could have bankrolled it myself if I had founded Amazon, or some other revolutionary business, decades ago. As always, I had been too slow. Too little, too late. I was no Bezos, and never would be. Maybe next time I\u2019d think twice before going into geological science.</p><p>Except, that wasn\u2019t entirely true, was it? Maybe Bezos could have saved Becca and Evangaline, and dozens of other young mothers and children, over the course of decades. And so could I. Diseases that cost hundreds of millions of dollars to cure effect relatively few people. On the other hand, there\u2019s huge swaths of the world were malaria is endemic. Malnutrition is endemic. Human parasite infection is endemic. There are already major organizations working in these areas, attacking these issues. The cost of addressing those problems? Saving those lives? Thousands of American dollars, each. For someone in Sierra Leone, that may as well be millions of dollars. But for me?</p><p>Well, I can be the Sierra Leone Bezos. I can save dozens of people over a few decades. Be the Bezos you want to see in the world.</p><p>After the movie we go out for a drink. The night is going well; it\u2019s worth another dead kid penny to keep this going with Tandee. If I can\u2019t spend anything on getting to know someone, I\u2019ll never have a family again. I\u2019ve already done these calculations a hundred times in my head, and I\u2019ve come to my peace with them.</p><p>As we\u2019re winding down the night, Tandee jokes about her school\u2019s fund raiser. I estimate how many dead kids it would take to increase the literacy rate in our suburb by a fraction. I know it would make our state better, make our neighbors stronger. The knock-on effects of increasing literacy would, allegedly, flow through the community for decades. The price per extra grade-level of reading comprehension, per student, is only a few hypothetical dead kids that I\u2019ll never meet.</p><p>Most people would have never met Evangaline. But she would be almost four years old now, and I would have known her all four of those years. Maybe Becca and I would be discussing a potential sibling for her. There are so many Evangalines out there, and every one deserves to have a Bezos rooting for her. Tandee\u2019s fund raiser has plenty of support already. It doesn\u2019t need help from my kids.</p><p>Tandee and I agree on a third date, as we\u2019re waiting for our cars to arrive. I find myself smiling on the trip home. I like her. Her passion for children gives me hope. I can envision telling her about the dead kids I see, eventually. I think she\u2019d understand.</p>", "user": {"username": "EA Forum Archives"}}, {"_id": "W3qwbA3es8vwpaTe5", "title": "The Detection", "postedAt": "2021-10-24T04:00:00.000Z", "htmlBody": "<p><strong>1.&nbsp;</strong></p><p>I remember the first time I read an article accusing me and my colleagues of being puppets of a foreign government. In that same month my sister forwarded me a video in which a preacher confirmed that I was the antichrist. And then about a week later a woman asked me to bless her child. A lot of people had a lot of ideas about what we were, but personally, I think we were just lucky. Honestly I\u2019m surprised that nobody got there before us. But for whatever reason, I have the strange honour of being the first person in history to witness a human soul.&nbsp;</p><p>I'm sure you've heard a version of this story, but let me tell it again, because I don't think you'll have heard the ending. It was autumn, in a lab in Manchester. We were working on a very speculative research project to detect microorganisms by creating these electromagnetic fields that somehow bounced off them. Honestly I still don't fully understand what it was meant to do: my job was to take the raw data and convert it into something intelligible. But I was there in the lab when it happened.&nbsp;</p><p>We had the rig set up, a set of dishes and this radar gun hooked up to a speaker that would beep like a Geiger counter. Or at least that was the theory: we hadn't actually got any results yet. Lots of people tell this story like we were hard at work, all very serious in our lab coats, but the truth is that when it happened we were just playing around with the settings, cycling through different frequencies trying to get a response.&nbsp;</p><p>Emily was holding the radar gun over the dishes each in turn while I was combing through the data on my laptop looking for a signal, when Jack came through the door with a tray of coffees. Emily spun around, still holding the gun, pointed it at Jack, and the speaker just blew.&nbsp;</p><p>Well, we were all a little alarmed to say the least. The first thing we did was make sure the whole machine wasn't bust, but looking at the logs we saw that just before it broke,</p><p>the readings had jumped. It wasn't like they curved up, no, they went straight from dead zero to off the scales.&nbsp;</p><p>We turned down the output as much as we could before we connected another speaker, pointed it at Jack again, and gradually turned it back up until once more we got a clear and recognisable signal. And we got the same signal when we pointed it at Emily, and at me. Nothing else in the room got any response. That was when we started to get curious.&nbsp;</p><p>Whatever we were detecting had no trouble passing through air, but it was blocked by about a centimetre of solid matter. The only thing that triggered it was us, nothing else. And any part of our bodies would trigger it, even a dense clump of hair. After we worked that out I remember Emily said she wasn't sure why it wasn't getting any background readings from all the dust and skin everywhere. Then Jack suggested we try giving someone a haircut.&nbsp;</p><p>We argued for a while about whose hair we should cut off. Back then I was growing out a ponytail, which I at least was quite fond of - there are pictures on the internet, if you're curious - but after enough coercion I decided I could sacrifice a few strands to scientific progress. So we lined up a clump of it, still attached to my head, and pointed the detector at it, with all the normal beeps, then Jack snipped it with a pair of scissors, and in that instant it dropped to nothing. That was when we knew we were on to something.&nbsp;</p><p>The next few days were a blur of discovery. We brought in a select few trusted friends and reached out to some senior academics. We set to work improving the machine, salvaging parts from various other projects and departments, and eventually managed to assemble a 50x50 grid of sensors. I did the thing I am now famous for, which was in fact basically trivial: I wrote a piece of software that took the results of the sensors and converted them into an extremely low-resolution video feed. And when we turned the machine on, what the video showed was the rough shape of whatever people it was pointed at, nothing more and nothing less. There we were, co-conspirators on the frontier of scientific progress, not even slightly sure what it was we were looking at, but increasingly certain that it wasn't like anything we'd ever come across before.&nbsp;</p><p><strong>2.&nbsp;</strong></p><p>I've heard astronauts talk about the way it feels to look down at Earth, every country, every life, and cover it with your thumb. I still don't know if there's a god, or gods, or whatever, and I\u2019ve often wondered if it was a mistake that the word we picked for what we saw was 'soul'. But what a relief! What a relief to know at last that there was something about us, something different.&nbsp;</p><p>I consider it a little unfair how much credit I and my colleagues got, because all the real work came elsewhere. This is the point where my story stops, but I was fortunate enough to get a front row seat to the torrent of discoveries that followed. A team at MIT found a way of scaling our design to produce a detector for about \u00a320. Then there was this genius grad student in Switzerland who refined our process and worked out that you can tell just from looking at a soul how much it's suffering - which, by the way, is not the same thing as experiencing pain. Whole departments got established to try to work out what the process was that caused an embryo to go from being inert to having a recognisable soul over the course of nine months, and within two years, we knew. As for animals, well, it was just a matter of tracking them down and checking.&nbsp;</p><p>Dogs have souls. Not as much as us, but about 1/3, depending on the breed. Cats don't though, not even a trace. Monkeys are anywhere from 50 to 90%. Elephants actually have more soul than we do. Pigs and cows, they suffer. It's about 10%, but it's there. That one was brutal for me. I tried veganism in college and gave up because I just didn't have the energy. And then I saw a live feed of the scale of suffering in a factory farm. I'd previously tried pricking my thumb under a sensor and, if that was like a candle, this was like staring into the sun: blinding. I had to go to the bathroom and throw up. Battery chicken farming, though, seems to be fine. And plants and trees don't really have a soul. On the other hand, really big mushrooms really really do, but oddly they don't seem to suffer.&nbsp;</p><p>Anyway, people were hesitant at first, as they always are with a new discovery. You don't want to upend your whole moral code and turn over millennia of tradition just because some geek in a lab coat found a way to make something beep. But once we knew what we were looking for, suddenly neuroscientists started making discoveries in leaps and bounds. Consciousness, perception, identity, they all began falling into place. Oh, the nihilists tried to stay disaffected and cynical, but it was hard to argue with the data.&nbsp;</p><p>And every time somebody said something like \"what if there's another kind of suffering, one your precious machine can't detect\", well, a year later somebody else would publish a paper finally giving a full theoretical underpinning of what suffering actually is, and everybody who could understand it (in this case it involved quite a lot of topology) would agree that, yes, it was pretty ironclad. I'd never seen anything like it: it's as if we opened the floodgates on truth.&nbsp;</p><p>Obviously reducing suffering isn't the only thing that matters, but at last it stopped being theoretical. We knew what we were doing, we knew how we were causing harm, and we stopped. The public went out on the streets, and one by one we closed the abattoirs, cleared out the cattle pens, half-emptied the zoos: there was no hiding the truth anymore.&nbsp;</p><p>People often ask me if I\u2019m proud of my discovery and honestly I\u2019m not, but I am truly proud of the part I played in what we did next. Those of you who helped, you should be too. As for the rest of it, the chickens, the mice, the insects, and the fish, well, it took a little while to be sure and to really accept it, and of course there were other complications in every case, but we finally stopped worrying about hurting them. Because at last we knew.</p><p>I say that, but there are still a few mysteries: Cephalopods definitely have souls. The detectors can't pierce solids but, thanks to a team in Berlin, they can go through water. And if you find a big ocean trench and point downwards, well, there's something going on down there.&nbsp;</p><p><strong>3.&nbsp;</strong></p><p>I worry sometimes about the things that we don't know though. The truth ruined a lot of lives: farmers who earned their keep selling beef found the demand suddenly dropped. And, less pragmatically, some of them had to live with the knowledge of what they'd been doing. We all did, really.&nbsp;</p><p>I remember in the span of days where we were trying to decide how to share our first discovery with the wider scientific community, and the press, Jack took me aside once and asked me, \"What happens if not everyone has a soul?\" I couldn't sleep that night. I lay in bed looking at my girlfriend and wondering, what would I do if I pointed the device at her and for the first time it stayed silent?&nbsp;</p><p>But we still went ahead with the announcements. I think that even if some people hadn't registered on the machine, it's not as if we would have started harvesting them for organs, but I also think that on the whole, you have to follow the truth wherever it takes you. Even if it scares you, and even if it hurts. I'm glad it didn't come to that though.&nbsp;</p><p>These days a lot of conscientious teens say that, if they'd been born 40 years ago, they'd have had the intellectual humility not to eat beef or go see monkeys in the zoo. It's hard to explain to them that back then we just didn't know how to measure the difference between a cow and a chicken, or at least, not like we can now. Or maybe I'm just making excuses.</p><p>I think they're right to talk about intellectual humility though. There's a question of compassion, whether we have the right to subject animals to extremely painful situations, and then there's a separate question, which is, how confident are we in our estimation of their capacity to suffer? I remember back then, I tried not to think about chickens in cages, but when I did I just assumed it wasn't really affecting them the way it would affect, well, a person in a cage. And in that case I was right. I just wish I'd also been right about all the beef.&nbsp;</p><p>The discoveries we've made have pretty conclusively settled the issue, but they've also revealed that back then we did have more or less all the pieces, even if we didn't know how to put them together. In retrospect, the fact that we hadn't found a near-infallible proof of their suffering seems to be a poor justification for our general willingness to ignore it. There are two things I can say with certainty: I'm glad we know now, and I only wish we'd known sooner.&nbsp;</p><p>But you\u2019ve heard all this before. Why am I up here telling you all over again? The reason is simple. Once more, we as a society are going to be confronted with the answer to one of the fundamental questions of life, and once more, we must decide what to do with the answer. Once more, we will be acting under great uncertainty, and once more, perhaps more so than ever, it is vital that we remain humble in our quest to make sense of our place in the universe. Because there is more.&nbsp;</p><p>A few months ago, I was informed that a research team had developed a way to reduce the interference on our readings in a vacuum to essentially zero. They put a detector on a satellite, and they've spent the past few weeks scanning the observable universe. Three days ago they found something, coming from a planet called Kepler-1652b. I'm honoured to be the one to tell you: somewhere out in space, there is another soul.</p>", "user": {"username": "Raymond D"}}, {"_id": "Me9rj5NvQDhyEFRjf", "title": "Wildness Podcast - Wild Animal Initiative", "postedAt": "2019-04-01T21:33:31.924Z", "htmlBody": "<p><i>Wildness</i> explores humanity\u2019s relationship with nature through the lens of individual animals.</p><p>In each episode, host Maia Laperle explores fundamental concepts in wild animal welfare, and what we do and don\u2019t know about life in the wild.</p><h3><strong>Episode 1: Who cares about wild animals?</strong></h3><p>Maia introduces the values and tenets of wild animal welfare while interviewing leaders in the wild animal welfare space.</p><p>Guests: Persis Eskander, Oscar Horta, Abraham Rowe</p><h3><strong>Episode 2: Addressing uncertainty</strong></h3><p>Maia considers the role uncertainty plays in wild animal welfare research, and how we handle it.</p><p>Guests: Joe Ballenger, Hollis Howe, Marianne van der Werf, Brian Tomasik</p><h3><strong>Episode 3: Who is nature for?</strong></h3><p>Maia asks hard questions about what we value in nature, who we\u2019re preserving it for, and what that means for wild animals.</p><p>Guests: Rachel Bjork, Rob Smith, Mark Davis, Christopher Sebastian</p><h3><strong>Episode 4: A seat at the table</strong></h3><p>In the season finale, Maia explores the moral and political future of wild animal welfare.</p><p>Guests: Natalie Kofler, Jay Shooster</p>", "user": {"username": "evelynciara"}}, {"_id": "5EBmaTSGBJwpi4Giw", "title": "Carl Shulman \u2014 Envisioning a world immune to global catastrophic biological risks", "postedAt": "2020-10-15T13:19:29.806Z", "htmlBody": "<p>The COVID-19 pandemic has highlighted the continuing vulnerability of our civilization to serious harm from novel diseases, but it has also highlighted that our civilization's wealth and technology offer unprecedented ability to contain pandemics.&nbsp; Logical extrapolation of DNA/RNA sequencing technology, physical barriers and sterilization, and robotics point the way to a world in which any new natural pandemic or use of biological weapons can be immediately contained, at increasingly affordable prices.&nbsp; These measures are agnostic as to the nature of pathogens, low in dual use issues, and can be fully established in advance, and so would seem to mark an end to any risk period for global catastrophic biological risks (GCBRs).&nbsp; An attainable defense-dominant 'win condition' for GCBRs means that we should think about GCBRs more in terms of a possible 'time of perils' and not an indefinite risk that sets a short life expectancy for our civilization.</p><h2>Diagnostic technology is advancing towards cheap universal detection of novel pathogens</h2><p>The technology of DNA sequencing has been one of the most rapidly advancing in biology, as <a href=\"https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data\">reported</a> by the National Human Genome Research Institute:<br>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5EBmaTSGBJwpi4Giw/xsvlvrb2rg0hmn72ckfq\"></figure><p>&nbsp;</p><p>The marginal cost of production is likely less, as the market leader, <a href=\"https://en.wikipedia.org/wiki/Illumina,_Inc.\">Illumina</a>, has market power for the moment and reaps high profit margins while restricting the functionality of the systems it sells.&nbsp; A number of&nbsp; competitors are working to contest its dominance with new technologies.&nbsp;<br><br>Sequencing technologies can be used to detect any pathogenic DNA (<a href=\"https://en.wikipedia.org/wiki/RNA-Seq\">or RNA</a>) sequence, including previously unseen organisms: the field of <a href=\"https://en.wikipedia.org/wiki/Metagenomics\">metagenomics</a>&nbsp;studies collections of genetic material from mixed communities of organisms.&nbsp; Researchers have already demonstrated sequencing and metagenomic methods to monitor sewage systems <a href=\"https://www.nature.com/articles/s41467-019-08853-3\">for antimicrobial resistance genes</a> in bacteria and&nbsp;<a href=\"https://academic.oup.com/jid/article/210/suppl_1/S294/2194423\">detect the presence of poliovirus</a> in a community for eradication, and <a href=\"https://www.healio.com/infectious-disease/antimicrobials/news/online/%7Bc56a62ec-7d7d-4f46-b553-b09ecbff8d1c%7D/qa-sewage-represents-a-viable-option-for-antimicrobial-resistance-surveillance\">advocated</a> for the creation of global sewage surveillance systems (of the sort that have <a href=\"https://www.statnews.com/2020/05/28/wastewater-testing-gains-support-as-covid19-early-warning/\">helped track</a>&nbsp;SARS-CoV-2&nbsp;this year), with waste.&nbsp; Such capabilities can be used for surveillance in hospitals, airports, etc.&nbsp;<br><br>The exponential decline in sequencing prices appears likely to eventually bring costs down to the point where sequencing costs are no longer a bottleneck for surveillance not just at the regional or community level, but for individual buildings (including airports and ultimately homes), e.g. by sequencing the condensation pan in HVAC systems.&nbsp; Cheaper sequencing also helps to detect genetic material at lower concentrations, helping with enabling earlier warning and detection in asymptomatic samples.&nbsp; Additional improvements would be needed for efficient sample acquisition and preparation, especially automation to eliminate dependency on skilled workers, but these seem eventually solvable if there is a large market.&nbsp; Combined with routine testing of medical samples in hospitals this c<a href=\"https://blogs.scientificamerican.com/observations/how-to-snuff-out-the-next-pandemic/\">ould give quick warning</a> of the spread of a new organism to flag it for close inspection of pathogenicity (although public health institutions would need to actually respond to investigate any spreading new organisms identified).</p><h2>Blanket coverage of flexible or universal tests can affordably crush human-to-human transmission if adequate capacity is maintained</h2><p>Attempts to contain the SARS-CoV-2 pandemic have been hindered&nbsp;by substantial asymptomatic and presymptomatic transmission.&nbsp; With knowledge of who is infected, strict isolation of infected people and contacts can help to slash the effective reproduction number of a pathogen.&nbsp; For instance, <a href=\"https://www.ft.com/content/0dba7ea8-6713-11ea-800d-da70cff6e4d3\">in the Italian town of&nbsp;&nbsp;V\u00f2</a>, universal testing found 3% infected, cut to 0.3% 10 days later and then zero.&nbsp; With a disease as damaging as COVID-19, frequently performing such blanket tests would easily pass cost-benefit tests at normal prices, as&nbsp;<a href=\"https://paulromer.net/roadmap-to-reopen-america/\">Paul Romer has called for</a>.&nbsp; When isolation measures cost more than 10% of economic output over a year, it's worth spending 10% of output, trillions of dollars, to stop a plague.&nbsp; The current pandemic has shown that implementation and compliance with testing and isolation is imperfect and varies greatly with government policy and other factors, but some countries have managed high performance,&nbsp; and overall the response has been enormous compared to previous pandemics.&nbsp; For a GCBR pathogen more than 100x as lethal, with a higher effective reproduction number and more asymptomatic transmission, the cost-benefit would be even more favorable, justifying spending of most of <a href=\"https://en.wikipedia.org/wiki/Gross_world_product\">world output</a>.<br><br>So we should ask whether continued massive falls in sequencing/diagnostic costs and increasing attention to pandemic risks lead to enough (localized, as opposed to centralized) sequencing capacity being produced during non-pandemic times as to enable continuous and ubiquitous surveillance for new pathogens, and spaces and objects humans come into contact with.&nbsp; Extrapolating falling sequencing costs out a few decades suggests this will be within reach for a minute proportion of global output, and sequencing capacity is valuable for many medical uses, so it seems probable to me that many nations will have the necessary capacity by 2050, and the world will overall have the appropriate levels of capacity, although there may be distributional issues.</p><h2>Could rich countries live at safety standards inspired by BSL-4?</h2><p>The U.S. Center for Disease Control has <a href=\"https://en.wikipedia.org/wiki/Biosafety_level\">4 biosafety levels</a>.&nbsp; At BSL-4, the highest safety level, workers wear <a href=\"https://en.wikipedia.org/wiki/Positive_pressure_personnel_suit\">positive pressure suits</a>&nbsp;with heavy sterilization on entry and exit, while powerful filtration and sterilization systems along with negative air pressure prevent pathogens from being escaping by air.&nbsp; These do <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4271556/\">suffer accidents</a>, but at a rate of one per hundreds of worker-years, which could be driven down further by large-scale studies and investments.&nbsp; The strain could be greater blocking threats from outside rather than inside, but I think it's a reasonable ballpark for overkill defenses inspired by BSL-4.</p><p>At current production scales \"<a href=\"https://www.hpac.com/iaq-ventilation/article/20925827/biosafety-level-4-labs-up-close-and-personal#:~:text=A%20BSL%2D4%20suit%20lab,range%20of%20%24700%20to%20%241%2C200.\">cost</a> per gross square foot for a BSL-4 lab is in the range of $700 to $1,200\" alongside various restrictions on the kinds of design that are compatible.&nbsp; Meanwhile, the median home price in San Francisco, one of the world's most expensive cities, <a href=\"https://sf.curbed.com/2017/9/29/16385146/san-francisco-price-per-square-foot\">is near $1,000 per square foot</a>.&nbsp; The San Francisco housing market drives out many people with low incomes for places with cheaper housing, but American housing consumption is <a href=\"https://notbuyinganything.blogspot.com/2012/03/average-house-size-by-country.html\">exceptionally high</a>, at over 2000 square feet per capita, vs ~800 for the UK, and ~100 for <a href=\"https://en.wikipedia.org/wiki/Housing_in_India\">India</a>.<br><br>Another data point for this is the <a href=\"https://en.wikipedia.org/wiki/David_Vetter#Death\">cost</a>&nbsp;(decades ago) of measures to protect children with <a href=\"https://en.wikipedia.org/wiki/Severe_combined_immunodeficiency\">severe combined immunodeficiency</a> (who usually die within a year from severe infections without special measures) via sterilized environments. [Thanks to Claire Zabel for this point.]<br><br>The cost of producing building to reach BSL-4 is also greatly elevated by limited production runs and amortizing the costs of regulatory approval: produced at scale for housing and workspaces would likely slash costs severalfold.&nbsp; However, costs would greatly expand with attempts to include land-expensive industries such as agriculture (with thousands of square feet for BSL-4 greenhouses to feed people).&nbsp; Those costs would be reduced insofar as telepresence, hazmat suits, and tiered safety measures could be used for lower priority areas, and food or goods produced produced in less safe environments sterilized before contact with humans.</p><p>My guess is that if the world had years of advance warning of a GCBR that required BSL-4 safety conditions, some rich countries could manage this using today's technologies and economies of scale, but this would require WWII mobilization levels, using most economic output for the adjustments.&nbsp; But such expenditures would not be made on the basis of speculation about future risks at a national level, so there would likely only be a limited number of facilities with such protection built in advance of a catastrophe.&nbsp;</p><p>How could the cost-benefit change such that societies would build broad BSL-4 protections in advance of a GCBR?&nbsp; One path would be greatly increased wealth, e.g. <a href=\"https://www.nber.org/papers/w23928\">given advanced artificial intelligence and robotics, total wealth could rise by orders of magnitude</a>, and the relevant technologies improve in price.&nbsp; Under those conditions atmospheric separation of buildings might be justified just to improve air quality, and a serious threat of GCBR could suffice.&nbsp; Robots, controlled by AI or telepresence, could also carry on jobs requiring physical presence without risk of infection, and improve the pace of scaleup of production in the event of a disaster.</p><p>Investment might also increase in response to GCBRs becoming a more serious threat, a negative feedback on potential biorisk levels: if perceived risks become very high then robust countermeasures will be deployed by larger and larger shares of the world.&nbsp;</p><h2>Low dual use risk</h2><p>A general problem with biosecurity is dual use research: if we think that biothreats largely lie with future discoveries in biotechnology, then general biotechnology will increase capabilities for both defense and offense.&nbsp; It is plausible that in the limit advances in biotechnology will overwhelmingly favor the defender, but not guaranteed, and net risk may be increased in the interim.&nbsp; Dual-use risk can substantially attenuate the biosecurity value of R&amp;D: a technology with a 60% chance of providing a benefit of <i>x</i>&nbsp;and a 40% chance of a harm of -<i>x</i>&nbsp;has an expected benefit of 0.2<i>x.</i><br><br>Advances in sequencing do boost biotechnology innovation generally (which may include dangerous innovations), but because of the small size of pathogen genomes, and research vs surveillance contexts, sequencing would probably already be fairly abundant for illegal bioweapons programs.&nbsp; Cheaper and improved BSL-4+ protective equipment and buildings, by reducing the risk of accidental release per capita, might reduce the disincentive to bioweapons research from accidents (which could overwhelm direct reductions of accidental release and resilience to release).</p><p>Nonetheless, both of these approaches seem relatively low in dual-use risk compared to approaches that require detailed understanding of all possible pathogenic attacks to defend against, and thus generate information about those attacks as well as any for which defense fails.</p><h2>Biorisk and the 'time of perils'</h2><p>If there were a fixed 0.05% risk each year of a GCBR, then the likelihood of an eventual event approaches 1 over the millennia.&nbsp; For the extreme case of an extinction event, that risk would set a civilizational life expectancy in the thousands of years.&nbsp; On the other hand, if risk levels eventually decline to ~0, then&nbsp; life expectancy of a civilization thereafter could be trillions of years (depending on other sources of mortality), so per <a href=\"https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\">Toby Ord</a> averting extinction risks is most valuable when there is a limited 'time of perils' when risk is high, followed by a substantial probability of a stable low-risk regime.</p><p>Universal detection technologies and physical barriers/sterilization suggest there is an ~0 biorisk win state attainable.&nbsp; If one expects advanced artificial intelligence this century, then the possible time window for serious biorisk is limited between the possible development of biothreats and their mootness by the deployment of known robust countermeasures.&nbsp; Given the low observed rates of WMD war between states and the barriers to terrorists inventing and using cutting-edge WMD (accidental release from bioweapons programs or publication of dangerous bioweapon recipes may be more likely, but can at least be reduced nonadversarially), passing through some years or decades of vulnerability safely seems probale.&nbsp; And if improvement and deployment of robust countermeasures proceeds quickly enough relative to increases in biothreats there may be no time of perils with respect to biological risks.<br>&nbsp;</p>", "user": {"username": "Pablo_Stafforini"}}, {"_id": "u9Dtz55oH3AshxQyb", "title": "Evidence Action \u2013 We\u2019re Shutting Down No Lean Season, Our Seasonal Migration Program: Here\u2019s Why", "postedAt": "2019-06-16T16:42:18.103Z", "htmlBody": "<h2>Summary</h2>\n<p>Evidence Action is terminating the <a href=\"http://www.noleanseason.org/\">No Lean Season</a> program, which was designed to increase household food consumption and income by providing travel subsidies for seasonal migration by poor rural laborers in Bangladesh, and was based on multiple rounds of rigorous research showing positive effects of the intervention. This is an important decision for Evidence Action, and we want to share the rationale behind it. &nbsp;Two factors led to this, including the disappointing 2017 evidence on program performance coupled with operational challenges given a recent termination of the relationship with our local partner due to allegations of financial improprieties. Ultimately, we determined that the opportunity cost for Evidence Action of rebuilding the program is too high relative to other opportunities we have to meet our vision of measurably improving the lives of hundreds of millions of people. Importantly, we are not saying that seasonal migration subsidies do not work or that they lack impact; rather, No Lean Season is unlikely to be among the best strategic opportunities for Evidence Action to achieve our vision.</p>\n<h2>Background</h2>\n<p>No Lean Season is a program in Evidence Action\u2019s Beta Incubator portfolio, which is designed to rigorously test promising programs along the criteria of evidence, cost-effectiveness, potential for scale, and strategic fit.</p>\n<p>In 2018, the evidence from a 2017 at-scale randomized controlled trial (RCT) showed that the program <a href=\"https://www.evidenceaction.org/blog-full/why-test-at-scale-no-lean-season\">did not have the desired impact on inducing migration</a>, and consequently did not increase household income or consumption. Our hypothesis was that the poor performance in 2017 was due to implementation challenges, and in 2018 we set out to adjust the program design to address these issues and conduct another at-scale RCT. We ceased further fundraising for No Lean Season pending the results of the 2018 RCT, anticipating these results would determine the path forward for the program \u2013 including the possibility that further implementation may not be warranted.</p>\n<p>Scaling programs is inherently difficult, and we expect some programs in the Beta pipeline to \u2018fail\u2019 in initial years due to implementation issues, issues relating to the underlying causal mechanism and theory of change, or some combination thereof that result in not meeting our criteria. Our focus in 2019 was on seeking to untangle this, as our research partners completed the RCT. The research team is still in the midst of collecting that data.</p>\n<h3>Allegations of financial irregularities</h3>\n<p>In 2019, a new challenge emerged that we want to share. In February 2019, we learned of allegations that the local Bangladeshi organization that Evidence Action contracted to implement the No Lean Season program had sought to bribe the Bangladeshi agency responsible for program approval. Specifically, Evidence Action staff were allegedly told by the local organization that the program approval had been forged, and that in trying to rectify the matter, a government agency official may have asked for an additional bribe that the local organization sought to pay. Such actions would have violated the local organization\u2019s agreement with Evidence Action, and we therefore terminated our relationship with that organization.</p>\n<p>We promptly engaged experienced, global legal and forensic audit firms to conduct an independent investigation into the allegations. After an extensive review of thousands of emails, documents, and staff interviews, the independent investigation was unable to corroborate allegations of any bribe. However, we also faced limited partner cooperation and contradictory Evidence Action staff reports during the investigation, which hindered a clear understanding of what transpired.</p>\n<p>During the course of the investigation, we did learn of one instance in which Evidence Action policies and procedures were not followed. Specifically, the investigation discovered a payment of approximately $400 by an Evidence Action staff member in 2018 that was allegedly made to a consultant to assist with obtaining program approval. The investigation was unable to conclude that the payment was actually made to a consultant or that any services were performed by such a consultant. The payment of this amount did not follow established Evidence Action processes and procedures for contracting and paying consultants. These procedures are in place to minimize risks of financial improprieties, and we have taken appropriate internal action in connection with this non-compliance. Additionally, we are taking this opportunity to review and strengthen our compliance controls and provide additional staff training to ensure that our work, including that carried out by our partners, is conducted ethically, legally, and effectively.</p>\n<h3>Decision to end the No Lean Season program</h3>\n<p>Following this recent development, Evidence Action assessed whether to continue the No Lean Season program. In 2018, we decided that, though the program did not have the desired impact in 2017, it merited further efforts to seek to resolve any implementation challenges. Given our existing program and partner infrastructure, these efforts could be achieved in a resource-effective way. However, with the termination of our relationship with our local partner \u2013 who bore primary responsibility for implementing the program \u2013 we have made the difficult decision to discontinue the program. Identifying a new partner and rebuilding the program would require significant investments of time and resources, and substantial leadership and attention, possibly over 1-2 years, just to regain the ground we have lost with the termination of the existing partnership.</p>\n<p>Like many things we struggle with in our work, this was a complex decision. Beta is deliberately experimental, and we expect many programs will not work. It is also quite clearly assessing potential programs against a range of factors \u2013 evidence, cost-effectiveness, potential for scale, and strategic fit, with operational feasibility serving as a critical underpinning. For Beta to work \u2013 and for Evidence Action to continue to ensure we are using scarce development resources as effectively as possible \u2013 we have to be willing to make tough calls and exit programs. Ultimately, the termination of the partnership and resulting operational challenges, coupled with the 2017 RCT results, led to our decision that the opportunity cost for Evidence Action of rebuilding the program is too high relative to other opportunities we have to meet our vision of measurably improving the lives of hundreds of millions of people.</p>\n<h3>Accident in Cumilla, Bangladesh</h3>\n<p>Separately, in January 2019, one of the known risks of the program came to bear in a tragic manner: an overloaded truck fell onto a temporary shelter in which several seasonal migrants who had migrated to work at a brick kiln were sleeping, killing 13 individuals, five of whom were affiliated with households that participated in No Lean Season. Moreover, four of the five were underage males aged 15-17. We were deeply saddened by this incident and the implications for these five No Lean Season households. Although this incident did not drive our decision to end the program, we want to share an update on it because the investigation we launched into the incident has recently concluded.</p>\n<p>From the outset of working on seasonal migration subsidies, Evidence Action and our research partners have been deliberate in carefully examining risks and unintended consequences of the No Lean Season program, such as underage migration. We have sought to minimize these, where possible, in our program design. We recognize that migration is a complex phenomenon that can result in important gains. For example, the promising research that underpinned our enthusiasm to bring seasonal migration subsidies into our Beta Incubator showed <a href=\"http://faculty.som.yale.edu/MushfiqMobarak/featuredresearch/No%20Lean%20Season.pdf\">significant gains in household food consumption and income</a>. However, migration can create both opportunities and risks for migrants, families, and the broader population in both rural and urban areas, as individuals migrate from villages to cities. Recognizing this, Evidence Action and our research partners carefully developed a research agenda to understand the magnitude of potential risks, including studying the impact of seasonal migration subsidies on, for example, gender and social norms, welfare of migrants, long-term migration behavior, wages in origin and destination communities, and underage migration. We also put in place programmatic safeguards to minimize risks where feasible, including those associated with underage migration.</p>\n<p>In response to the tragic incident, we undertook a thorough review of program protocols and procedures focused on preventing underage migration, including commissioning an independent investigation. The investigation found that the safeguards we had in place were robust, though ultimately could not fully eliminate the risk of an adult recipient choosing to pass their cash transport subsidy to a teenager in his place, contrary to program rules and protocols. These protocols were multilayered, and included verbally informing subsidy recipients of the condition that migrants must be at least 18 years of age; requiring subsidy recipients to sign or thumbprint an acknowledgement that both recipients and migrants (where different individuals) must be at least 18 years of age; reviewing national identification cards to verify that the subsidy recipient and any person that the recipient says plans to migrate from the household is at least age 18; and utilizing mobile data collection software that is programmed to prohibit field staff from including individuals reporting to be under the age of 18, in order to prevent accidental enrollment.</p>\n<p>In parallel, we analyzed both program administrative data and data from past and ongoing RCTs of the program to understand the scope of underage migration in the program and, importantly, the causal impact of the program on underage migration. We recognize that underage migration occurs in Bangladesh, where severely limited options may lead a deeply impoverished family to believe that sending a teenage son to the city to find work is their best available option. As such, we analyzed the RCT data to understand whether the program may have inadvertently led to additional underage migration beyond that which would have occurred in the absence of the program. We have concluded that, across RCTs conducted in 2014, 2017, and 2018, there is no evidence to suggest such a causal relationship.</p>\n<h2>Next steps</h2>\n<p>Importantly, we are <em>not</em> saying that seasonal migration subsidies do not work nor that they lack impact. &nbsp;&nbsp;Seasonal migration subsidies may be a useful and cost-effective tool for addressing the critical issue of seasonal poverty. What we are saying, however, is that No Lean Season is unlikely to be among the best strategic opportunities for Evidence Action to achieve <a href=\"https://www.evidenceaction.org/who-we-are#our-ethos\">our mission</a>, and as such we are exiting it from the Beta portfolio.</p>\n<p>We are nonetheless committed to ensuring that the at-scale testing of the program underway with our research partners can inform the evidence base on seasonal migration subsidies, and therefore will complete the RCT that is currently being conducted by an independent organization. Though there is low likelihood of our further work in this area, we will consider the findings from the trial, once available, to inform our organizational view on seasonal migration as an intervention.</p>\n<p>As a result of our decision to terminate No Lean Season, our commitment to ensuring good stewardship of donor funding comes to the fore. We stopped fundraising for No Lean Season in November 2018 due to the 2017 RCT results and continued to use funds already received for the program for our existing commitments; we will continue to do so to complete the ongoing RCT and undertake a responsible close-out of the program. Evidence Action expects that some program funding received from foundations under grant agreements will remain following the completion of these activities. We are engaging with these donors and, where possible, will seek to reallocate the unspent funds to other evidence-based, cost-effective programs within our organization. Though all funds donated directly by individual donors to Evidence Action for No Lean Season have been used for existing program commitments, donors may contact <a href=\"mailto:donate@evidenceaction.org\">donate@evidenceaction.org</a> to request reallocation of a portion of the donation to another Evidence Action program. Any request of this nature should be made by July 31, 2019.</p>\n<p>We also want to ensure that the takeaway is <em>not</em> that if a program faces challenges, an NGO should walk away from doing work that measurably improves the lives of tens or hundreds of thousands of people. International development programs are complex \u2013 operating in high-risk settings where the risk of corruption and a whole host of other challenges is high. Moreover, the reality of working on the ground \u2013 and at scale \u2013 is often complicated. Our job must be to cost-effectively and responsibly mitigate these risks, rather than turn away from opportunities that dramatically improve lives.</p>\n<p>Evidence Action is and will continue to seek to be transparent in sharing these risks and challenges, consistent with our organizational values and those of many of our supporters and partners. To the extent possible, we share the issues as we face them so that we can contribute to an active exchange of learnings among our partners and the broader development community. By doing so, our goal is to ultimately drive better outcomes for the hundreds of millions of people living in poverty that we all seek to serve.</p>\n", "user": {"username": "tessa"}}, {"_id": "n6eWJNLHznTksnR9S", "title": "What stops you doing more forecasting?", "postedAt": "2021-11-16T00:26:37.266Z", "htmlBody": "<p>What stands in the way of you doing more forecasting on EA topics? If you started, what made you stop?</p>", "user": {"username": "nathan"}}, {"_id": "2nySx3hQhvxzbzvFZ", "title": "If you could make a second EA forum with a different vibe to this one, what would it be like?", "postedAt": "2021-11-16T00:17:14.316Z", "htmlBody": "<p>You have all the resources of the current EA forum team, but you have to run a second forum. The current forum still exists, what group do you seek to serve with the second forum and how?</p>", "user": {"username": "nathan"}}, {"_id": "cXy2rGhjumzRwKBWC", "title": "What's the GiveDirectly of longtermism & existential risk?", "postedAt": "2021-11-15T23:55:55.104Z", "htmlBody": "<p>GiveDirectly has a huge space for additional funding and spends the money effectively. It's processes are simple and most poeple can see it's helping.<br><br>What is the longtermist equivalent of this?<br><br>What is the organisation you give to to improve the longterm future, with these critera, which can sit as the organisation of last resort to donate to?<br><br>This question was brought to you by me stealing Ben Todd's tweets and turning them into questions. <a href=\"https://twitter.com/ben_j_todd/status/1459196519924604928\">https://twitter.com/ben_j_todd/status/1459196519924604928</a><br><br>Please write one suggestion per answer.</p>", "user": {"username": "nathan"}}, {"_id": "jWZaWSCkg32XPFH8J", "title": "Seeking Input in Creating an Overview of EA ", "postedAt": "2021-11-15T21:56:53.119Z", "htmlBody": "<p><span>As I was learning about EA, I found myself wanting the core of EA summarized in a single place, as well as a map of sorts to help me orient to&nbsp;where to explore next. This is my (first) attempt to create that. I would love input from the community about how to make the content more representative. After I get more input I will do a final edit, add a bit of&nbsp;design to make it more visually interesting,&nbsp;and share it in a follow-up post.&nbsp;</span></p><p><span>Please have a look &amp; let me know what you think: </span><a href=\"https://docs.google.com/document/d/1R79YdLUET4v4ltq0g7gfWJPsXv3WvHkzJecXk9HxpGU/edit?usp=sharing\"><span>Effective Altruism | A Primer&nbsp;</span></a></p><p>Some Questions:</p><p>1. What is the best way to share about key or common cause areas? Since there is no definitive consensus on these (or even what words to use to speak to them) I wonder what thoughts or input people have?</p><p>2. Stories and examples are powerful. Currently this doesn't include any (primarily to keep it concise but also TBH because I don't love most of the commonly cited examples and think they are already over-referenced, so if people who encounter this continue learning about EA they will likely find them anyways.) Does anyone have ideas or input on what might be a way to still incorporate story into this piece?</p><p>3. Could you see yourself using this? If so, in what ways?</p><p>4. Where do you point people to learn about EA?&nbsp;</p><p>Feel free to leave comments below or directly in the document.</p><p>&amp; Thanks to those who have offered feedback and encouragement thus far!</p>", "user": {"username": "emwalz"}}, {"_id": "iGYTt3qvJFGppxJbk", "title": "Ngo and Yudkowsky on alignment difficulty", "postedAt": "2021-11-15T22:47:46.125Z", "htmlBody": "<p>This post is the first in a series of transcribed Discord conversations between Richard Ngo and Eliezer Yudkowsky, moderated by Nate Soares. We've also added Richard and Nate's running summaries of the conversation (and others' replies) from Google Docs.</p><p>Later conversation participants include Ajeya Cotra, Beth Barnes, Carl Shulman, Holden Karnofsky, Jaan Tallinn, Paul Christiano, Rob Bensinger, and Rohin Shah.</p><p>The transcripts are a complete record of several Discord channels MIRI made for discussion. We tried to edit the transcripts as little as possible, other than to fix typos and a handful of confusingly-worded sentences, to add some paragraph breaks, and to add referenced figures and links. We didn't end up redacting any substantive content, other than the names of people who would prefer not to be cited. We swapped the order of some chat messages for clarity and conversational flow (indicated with extra timestamps), and in some cases combined logs where the conversation switched channels.</p><p>&nbsp;</p><p>Color key:</p><figure class=\"table\"><table><tbody><tr><td>&nbsp;Chat by Richard and Eliezer&nbsp;</td><td style=\"background-color:hsl(0, 0%, 90%)\">&nbsp;Other chat&nbsp;</td><td style=\"background-color:rgb(255, 247, 222)\">&nbsp;Google Doc content&nbsp;</td><td style=\"background-color:#FFEEBB\">&nbsp;Inline comments&nbsp;</td></tr></tbody></table></figure><p>&nbsp;</p><h1>0. Prefatory comments</h1><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#FFEEBB;vertical-align:top\"><p><strong>[Yudkowsky][8:32]</strong> &nbsp;<strong>(Nov. 6 follow-up comment)</strong>&nbsp;</p><p>(At Rob's request I'll try to keep this brief, but this was an experimental format and some issues cropped up that seem large enough to deserve notes.)</p><p>Especially when coming in to the early parts of this dialogue, I had some backed-up hypotheses about \"What might be the main sticking point? and how can I address that?\" which from the standpoint of a pure dialogue might seem to be causing me to go on digressions, relative to if I was just trying to answer Richard's own questions.&nbsp; On reading the dialogue, I notice that this looks evasive or like point-missing, like I'm weirdly not just directly answering Richard's questions.</p><p>Often the questions are answered later, or at least I think they are, though it may not be in the first segment of the dialogue.&nbsp; But the larger phenomenon is that I came in with some things I wanted to say, and Richard came in asking questions, and there was a minor accidental mismatch there.&nbsp; It would have looked better if we'd both stated positions first without question marks, say, or if I'd just confined myself to answering questions from Richard.&nbsp; (This is not a huge catastrophe, but it's something for the reader to keep in mind as a minor hiccup that showed up in the early parts of experimenting with this new format.)</p></td></tr><tr><td style=\"background-color:#FFEEBB;vertical-align:top\"><p><strong>[Yudkowsky][8:32]</strong> &nbsp;<strong>(Nov. 6 follow-up comment)</strong>&nbsp;</p><p>(Prompted by some later stumbles in attempts to summarize this dialogue.&nbsp; Summaries seem plausibly a major mode of propagation for a sprawling dialogue like this, and the following request seems like it needs to be very prominent to work - embedded requests later on didn't work.)</p><p>Please don't summarize this dialogue by saying, \"and so Eliezer's MAIN idea is that\" or \"and then Eliezer thinks THE KEY POINT is that\" or \"the PRIMARY argument is that\" etcetera.&nbsp; From my perspective, everybody comes in with a different set of sticking points versus things they see as obvious, and the conversation I have changes drastically depending on that.&nbsp; In the old days this used to be the Orthogonality Thesis, Instrumental Convergence, and superintelligence being a possible thing at all; today most OpenPhil-adjacent folks have other sticking points instead.</p><p>Please transform:</p><ul><li>\"Eliezer's main reply is...\" -&gt; \"Eliezer replied that...\"</li><li>\"Eliezer thinks the key point is...\" -&gt; \"Eliezer's point in response was...\"</li><li>\"Eliezer thinks a major issue is...\"&nbsp; -&gt; \"Eliezer replied that one issue is...\"</li><li>\"Eliezer's primary argument against this is...\" -&gt; \"Eliezer tried the counterargument that...\"</li><li>\"Eliezer's main scenario for this is...\" -&gt; \"In a conversation in September of 2021, Eliezer sketched a hypothetical where...\"</li></ul><p>Note also that the transformed statements say what you <i>observed,</i> whereas the untransformed statements are (often incorrect) <i>inferences</i> about my latent state of mind.</p><p>(Though \"distinguishing relatively unreliable inference from more reliable observation\" is not necessarily<i> the key idea</i> here or <i>the one big reason</i> I'm asking for this.&nbsp; That's just one point I tried making - one argument that I hope might help drive home the larger thesis.)</p></td></tr></tbody></table></figure><p>&nbsp;</p><h1>1. September 5 conversation</h1><p>&nbsp;</p><h2>1.1. Deep vs. shallow problem-solving patterns</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:00]</strong>&nbsp;</p><p>Hi all! Looking forward to the discussion.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:01]</strong>&nbsp;</p><p>Hi and welcome all.&nbsp; My name is Eliezer and I think alignment is really actually quite extremely difficult.&nbsp; Some people seem to not think this!&nbsp; It's an important issue so ought to be resolved somehow, which we can hopefully fully do today.&nbsp; (I will however want to take a break after the first 90 minutes, if it goes that far and if Ngo is in sleep-cycle shape to continue past that.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:02]</strong>&nbsp;</p><p>A break in 90 minutes or so sounds good.</p><p>Here's one way to kick things off: I agree that humans trying to align arbitrarily capable AIs seems very difficult. One reason that I'm more optimistic (or at least, not confident that we'll have to face the full very difficult version of the problem) is that at a certain point AIs will be doing most of the work.</p><p>When you talk about alignment being difficult, what types of AIs are you thinking about aligning?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:04]</strong>&nbsp;</p><p>On my model of the Other Person, a lot of times when somebody thinks alignment shouldn't be that hard, they think there's some particular thing you can do to align an AGI, which isn't that hard, and their model is missing one of the foundational difficulties for why you can't do (easily or at all) one step of their procedure.&nbsp; So one of my own conversational processes might be to poke around looking for a step that the other person doesn't realize is hard.&nbsp; That said, I'll try to directly answer your own question first.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:07]</strong>&nbsp;</p><p>I don't think I'm confident that there's any particular thing you can do to align an AGI. Instead I feel fairly uncertain over a broad range of possibilities for how hard the problem turns out to be.</p><p>And on some of the most important variables, it seems like evidence from the last decade pushes towards updating that the problem will be easier.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:09]</strong>&nbsp;</p><p>I think that after AGI becomes possible at all and then possible to scale to dangerously superhuman levels, there will be, in the best-case scenario where a lot of other social difficulties got resolved, a 3-month to 2-year period where only a very few actors have AGI, meaning that it was socially possible for those few actors to decide to <i>not</i> just scale it to where it automatically destroys the world.</p><p>During this step, if humanity is to survive, somebody has to perform some feat that causes the world to <i>not</i> be destroyed in 3 months or 2 years when too many actors have access to AGI code that will destroy the world if its intelligence dial is turned up. This requires that the first actor or actors to build AGI, be able to do <i>something</i> with that AGI which prevents the world from being destroyed; if it didn't require superintelligence, we could go do that thing right now, but no such human-doable act apparently exists so far as I can tell.</p><p>So we want the least dangerous, most easily aligned thing-to-do-with-an-AGI, but it does have to be a pretty powerful act to prevent the automatic destruction of Earth after 3 months or 2 years. It has to \"flip the gameboard\" rather than letting the suicidal game play out. We need to align the AGI that performs this pivotal act, to perform that pivotal act without killing everybody.</p><p>Parenthetically, no act powerful enough and gameboard-flipping enough to qualify is inside the Overton Window of politics, or possibly even of effective altruism, which presents a separate social problem. I usually dodge around this problem by picking an exemplar act which is powerful enough to actually flip the gameboard, but not the most alignable act because it would require way too many aligned details: Build self-replicating open-air nanosystems and use them (only) to melt all GPUs.</p><p>Since any such nanosystems would have to operate in the full open world containing lots of complicated details, this would require tons and tons of alignment work, is not the pivotal act easiest to align, and we should do some other thing instead. But the other thing I have in mind is also outside the Overton Window, just like this is. So I use \"melt all GPUs\" to talk about the requisite power level and the Overton Window problem level, both of which seem around the right levels to me, but the actual thing I have in mind is more alignable; and this way, I can reply to anyone who says \"How dare you?!\" by saying \"Don't worry, I don't actually plan on doing that.\"</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:14]</strong>&nbsp;</p><p>One way that we could take this discussion is by discussing the pivotal act \"make progress on the alignment problem faster than humans can\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:15]</strong>&nbsp;</p><p>This sounds to me like it requires extreme levels of alignment and operating in extremely dangerous regimes, such that, if you could do that, it would seem much more sensible to do some other pivotal act first, using a lower level of alignment tech.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:16]</strong>&nbsp;</p><p>Okay, this seems like a crux on my end.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:16]</strong>&nbsp;</p><p>In particular, I would hope that - in unlikely cases where we survive at all - we were able to survive by operating a superintelligence only in the lethally dangerous, but still less dangerous, regime of \"engineering nanosystems\".</p><p>Whereas \"solve alignment for us\" seems to require operating in the even more dangerous regimes of \"write AI code for us\" and \"model human psychology in tremendous detail\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:17]</strong>&nbsp;</p><p>What makes these regimes so dangerous? Is it that it's very hard for humans to exercise oversight?</p><p>One thing that makes these regimes seem less dangerous to me is that they're broadly in the domain of \"solving intellectual problems\" rather than \"achieving outcomes in the world\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:19][11:21]</strong>&nbsp;</p><p>Every AI output <i>effectuates</i> outcomes in the world.&nbsp; If you have a powerful unaligned mind hooked up to outputs that can start causal chains that effectuate dangerous things, it doesn't matter whether the comments on the code say \"intellectual problems\" or not.</p><p>The danger of \"solving an intellectual problem\" is when it requires a powerful mind to think about domains that, when solved, render very cognitively accessible strategies that can do dangerous things.</p></td></tr><tr><td>I expect the first alignment solution you can actually deploy in real life, in the unlikely event we get a solution at all, looks like 98% \"don't think about all these topics that we do not absolutely need and are adjacent to the capability to easily invent very dangerous outputs\" and 2% \"actually think about this dangerous topic but please don't come up with a strategy inside it that kills us\".</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:21][11:22]</strong>&nbsp;</p><p>Let me try and be more precise about the distinction. It seems to me that systems which have been primarily trained to make predictions about the world would by default lack a lot of the cognitive machinery which humans use to take actions which pursue our goals.</p></td></tr><tr><td><p>Perhaps another way of phrasing my point is something like: it doesn't seem implausible to me that we build AIs that are significantly more intelligent (in the sense of being able to understand the world) than humans, but significantly less agentic.</p><p>Is this a crux for you?</p><p>(obviously \"agentic\" is quite underspecified here, so maybe it'd be useful to dig into that first)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:27][11:33]</strong>&nbsp;</p><p>I would certainly have learned very new and very exciting facts about intelligence, facts which indeed contradict my present model of how intelligences liable to be discovered by present research paradigms work, if you showed me... how can I put this in a properly general way... that problems I thought were about searching for states that get fed into a result function and then a result-scoring function, such that the input gets an output with a high score, were in fact not about search problems like that. I have sometimes given more specific names to this problem setup, but I think people have become confused by the terms I usually use, which is why I'm dancing around them.</p><p>In particular, just as I have a model of the Other Person's Beliefs in which they think alignment is easy because they don't know about difficulties I see as very deep and fundamental and hard to avoid, I also have a model in which people think \"why not just build an AI which does X but not Y?\" because they don't realize what X and Y have in common, which is something that draws deeply on having deep models of intelligence. And it is hard to convey this deep theoretical grasp. But you can also see powerful practical hints that these things are much more correlated than, eg, Robin Hanson was imagining during the <a href=\"https://intelligence.org/ai-foom-debate/\">FOOM debate</a>, because Robin did not think something like GPT-3 should exist; Robin thought you should need to train lots of specific domains that didn't generalize. I argued then with Robin that it was something of a hint that humans had visual cortex and cerebellar cortex but not Car Design Cortex, in order to design cars. Then in real life, it proved that reality was far to the Eliezer side of Eliezer on the <a href=\"https://intelligence.org/2017/10/20/alphago/\">Eliezer-Robin axis</a>, and things like GPT-3 were built with <i>less</i> architectural complexity and generalized <i>more</i> than I was arguing to Robin that complex architectures should generalize over domains.</p></td></tr><tr><td>The metaphor I sometimes use is that it is very hard to build a system that drives cars painted red, but is not at all adjacent to a system that could, with a few alterations, prove to be very good at driving a car painted blue.&nbsp; The \"drive a red car\" problem and the \"drive a blue car\" problem have too much in common.&nbsp; You can maybe ask, \"Align a system so that it has the capability to drive red cars, but refuses to drive blue cars.\"&nbsp; You can't make a system that is very good at driving red-painted cars, but lacks the basic capability to drive blue-painted cars because you never trained it on that.&nbsp; The patterns found by gradient descent, by genetic algorithms, or by other plausible methods of optimization, for driving red cars, would be patterns very close to the ones needed to drive blue cars.&nbsp; When you optimize for red cars you get the blue car <i>capability</i> whether you like it or not.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:32]</strong>&nbsp;</p><p>Does your model of intelligence rule out building AIs which make dramatic progress in mathematics without killing us all?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:34][11:39]</strong>&nbsp;</p><p>If it were possible to perform some pivotal act that saved the world with an AI that just made progress on proving mathematical theorems, without, eg, needing to explain those theorems to humans, I'd be <i>extremely</i> interested in that as a potential pivotal act. We wouldn't be out of the woods, and I wouldn't actually know how to build an AI like that without killing everybody, but it would immediately trump everything else as the obvious line of research to pursue.</p><p>Parenthetically, there is very very little which my model of intelligence <i>rules out</i>. I think we all die because we cannot do certain dangerous things correctly, <i>on the very first try in the dangerous regimes where one mistake kills you</i>, and do them <i>before</i> proliferation of much easier technologies kills us. If you have the Textbook From 100 Years In The Future that gives the simple robust solutions for everything, that actually work, you can write a superintelligence that thinks 2 + 2 = 5 because the Textbook gives the methods for doing that which are simple and actually work in practice in real life.</p></td></tr><tr><td>(The Textbook has the equivalent of \"use ReLUs instead of sigmoids\" everywhere, and avoids all the clever-sounding things that will work at subhuman levels and blow up when you run them at superintelligent levels.)</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:36][11:40]</strong>&nbsp;</p><p>Hmm, so suppose we train an AI to prove mathematical theorems when given them, perhaps via some sort of adversarial setter-solver training process.</p><p>By default I have the intuition that this AI could become extremely good at proving theorems - far beyond human level - without having goals about real-world outcomes.</p></td></tr><tr><td><p>It seems to me that in your model of intelligence, being able to do tasks like mathematics is closely coupled with trying to achieve real-world outcomes. But I'd actually take GPT-3 as some evidence against this position (although still evidence in favour of your position over Hanson's), since it seems able to do a bunch of reasoning tasks while still not being very agentic.</p><p>There's some alternative world where we weren't able to train language models to do reasoning tasks without first training them to perform tasks in complex RL environments, and in that world I'd be significantly less optimistic.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:41]</strong>&nbsp;</p><p>I put to you that there is a predictable bias in your estimates, where you don't know about the Deep Stuff that is required to prove theorems, so you imagine that certain cognitive capabilities are more disjoint than they actually are.&nbsp; If you knew about the things that humans are using to reuse their reasoning about chipped handaxes and other humans, to prove math theorems, you would see it as more plausible that proving math theorems would generalize to chipping handaxes and manipulating humans.</p><p>GPT-3 is a... complicated story, on my view of it and intelligence.&nbsp; We're looking at an interaction between tons and tons of memorized shallow patterns.&nbsp; GPT-3 is <i>very</i> unlike the way that natural selection built humans.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:44]</strong>&nbsp;</p><p>I agree with that last point. But this is also one of the reasons that I previously claimed that AIs could be more intelligent than humans while being less agentic, because there are systematic differences between the way in which natural selection built humans, and the way in which we'll train AGIs.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:45]</strong>&nbsp;</p><p>My current suspicion is that Stack More Layers alone is not going to take us to GPT-6 which is a true AGI; and this is because of the way that GPT-3 is, in your own terminology, \"not agentic\", and which is, in my terminology, not having gradient descent on GPT-3 run across sufficiently deep problem-solving patterns.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:46]</strong>&nbsp;</p><p>Okay, that helps me understand your position better.</p><p>So here's one important difference between humans and neural networks: humans face the genomic bottleneck which means that each individual has to rederive all the knowledge about the world that their parents already had. If this genetic bottleneck hadn't been so tight, then individual humans would have been significantly less capable of performing novel tasks.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:50]</strong>&nbsp;</p><p>I agree.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:50]</strong>&nbsp;</p><p>In my terminology, this is a reason that humans are \"more agentic\" than we otherwise would have been.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:50]</strong>&nbsp;</p><p>This seems indisputable.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:51]</strong>&nbsp;</p><p>Another important difference: humans were trained in environments where we had to run around surviving all day, rather than solving maths problems etc.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:51]</strong>&nbsp;</p><p>I continue to nod.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:52]</strong>&nbsp;</p><p>Supposing I agree that reaching a certain level of intelligence will require AIs with the \"deep problem-solving patterns\" you talk about, which lead AIs to try to achieve real-world goals. It still seems to me that there's likely a lot of space between that level of intelligence, and human intelligence.</p><p>And if that's the case, then we could build AIs which help us solve the alignment problem before we build AIs which instantiate sufficiently deep problem-solving patterns that they decide to take over the world.</p><p>Nor does it seem like the reason <i>humans</i> want to take over the world is because of a deep fact about our intelligence. It seems to me that humans want to take over the world mainly because that's very similar to things we evolved to do (like taking over our tribe).</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:57]</strong>&nbsp;</p><p>So here's the part that I agree with: If there were one theorem only mildly far out of human reach, like proving the ABC Conjecture (if you think it hasn't already been proven), and providing a machine-readable proof of this theorem would immediately save the world - say, aliens will give us an aligned superintelligence, as soon as we provide them with this machine-readable proof - then there would exist a plausible though not certain road to saving the world, which would be to try to build a <i>shallow</i> mind that proved the ABC Conjecture by memorizing tons of relatively shallow patterns for mathematical proofs learned through self-play; without that system ever abstracting math as deeply as humans do, but the sheer width of memory and sheer depth of search sufficing to do the job. I am not sure, to be clear, that this would work. But my model of intelligence does not rule it out.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:58]</strong>&nbsp;</p><p>(I'm actually thinking of a mind which understands maths more deeply than humans - but perhaps only understands maths, or perhaps also a range of other sciences better than humans.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:00]</strong>&nbsp;</p><p>Parts I disagree with: That \"help us solve alignment\" bears any significant overlap with \"provide us a machine-readable proof of the ABC Conjecture without thinking too deeply about it\". That humans want to take over the world only because it resembles things we evolved to do.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:01]</strong>&nbsp;</p><p>I definitely agree that humans don't <i>only</i> want to take over the world because it resembles things we evolved to do.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:02]</strong>&nbsp;</p><p>Alas, eliminating 5 reasons why something would go wrong doesn't help much if there's 2 remaining reasons something would go wrong that are much harder to eliminate!</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:02]</strong>&nbsp;</p><p>But if we imagine having a human-level intelligence which <i>hadn't</i> evolved primarily to do things that reasonably closely resembled taking over the world, then I expect that we could ask that intelligence questions in a fairly safe way.</p><p>And that's also true for an intelligence that is noticeably above human level.</p><p>So one question is: how far above human level could we get before a system which has only been trained to do things like answer questions and understand the world will decide to take over the world?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:04]</strong>&nbsp;</p><p>I think this is one of the very rare cases where the intelligence difference between \"village idiot\" and \"Einstein\", which I'd usually see as very narrow, makes a structural difference! I think you can get some outputs from a village-idiot-level AGI, which got there by training on domains exclusively like math, and this will proooobably not destroy the world (<i>if</i> you were right about that, about what was going on inside). I have more concern about the Einstein level.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:05]</strong>&nbsp;</p><p>Let's focus on the Einstein level then.</p><p>Human brains have been optimised very little for doing science.</p><p>This suggests that building an AI which is Einstein-level at doing science is significantly easier than building an AI which is Einstein-level at taking over the world (or other things which humans evolved to do).</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:08]</strong>&nbsp;</p><p>I think there's a certain broad sense in which I agree with the literal truth of what you just said. You will systematically overestimate <i>how much</i> easier, or how far you can push the science part without getting the taking-over-the-world part, for as long as your model is ignorant of what they have in common.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:08]</strong>&nbsp;</p><p>Maybe this is a good time to dig into the details of what they have in common, then.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:09][12:11]][12:13]</strong>&nbsp;</p><p>I feel like I haven't had much luck with trying to explain that on previous occasions. Not to you, to others too.</p><p>There are shallow topics like why p-zombies can't be real and how quantum mechanics works and why science ought to be using likelihood functions instead of p-values, and I can <i>barely</i> explain those to <i>some</i> people, but then there are some things that are apparently much harder to explain than that and which defeat my abilities as an explainer.</p></td></tr><tr><td>That's why I've been trying to point out that, even if you don't know the specifics, there's an estimation bias that you can realize should exist in principle.</td></tr><tr><td>Of course, I also haven't had much luck in saying to people, \"Well, even if you don't know the truth about X that would let you see Y, can you not see by abstract reasoning that knowing <i>any</i> truth about X would predictably cause you to update in the direction of Y\" - people don't seem to actually internalize that much either. Not you, other discussions.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:10][12:11][12:13]</strong>&nbsp;</p><p>Makes sense. Are there ways that I could try to make this easier? E.g. I could do my best to explain what I think your position is.</p><p>Given what you've said I'm not optimistic about this helping much.</p></td></tr><tr><td><p>But insofar as this is the key set of intuitions which has been informing your responses, it seems worth a shot.</p><p>Another approach would be to focus on our predictions for how AI capabilities will play out over the next few years.</p></td></tr><tr><td>I take your point about my estimation bias. To me it feels like there's also a bias going the other way, which is that as long as we don't know the mechanisms by which different human capabilities work, we'll tend to lump them together as one thing.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:14]</strong>&nbsp;</p><p>Yup. If you didn't know about visual cortex and auditory cortex, or about eyes and ears, you would assume much more that any sentience ought to both see and hear.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:16]</strong>&nbsp;</p><p>So then my position is something like: human pursuit of goals is driven by emotions and reward signals which are deeply evolutionarily ingrained, and without those we'd be much safer but not that much worse at pattern recognition.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:17]</strong>&nbsp;</p><p>If there's a pivotal act you can get just by supreme acts of pattern recognition, that's right up there with \"pivotal act composed solely of math\" for things that would obviously instantly become the prime direction of research.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:18]</strong>&nbsp;</p><p>To me it seems like maths is <i>much more</i> about pattern recognition than, say, being a CEO. Being a CEO requires coherence over long periods of time; long-term memory; motivation; metacognition; etc.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:18][12:23]</strong>&nbsp;</p><p>(One occasionally-argued line of research can be summarized from a certain standpoint as \"how about a pivotal act composed entirely of predicting text\" and to this my reply is \"you're trying to get fully general AGI capabilities by predicting text that is <i>about</i> deep / 'agentic' reasoning, and that doesn't actually help\".)</p><p>Human math is very much about goals. People want to prove subtheorems on the way to proving theorems. We might be able to make a <i>different</i> kind of mathematician that works more like GPT-3 in the dangerously inscrutable parts that are all noninspectable vectors of floating-point numbers, but even there you'd need some Alpha-Zero-like outer framework to supply the direction of search.</p></td></tr><tr><td>That outer framework might be able to be powerful enough without being reflective, though. So it would plausibly be <i>much easier</i> to build a mathematician that was capable of superhuman formal theorem-proving but not agentic. The reality of the world might tell us \"lolnope\" but my model of intelligence doesn't mandate that. That's why, if you gave me a pivotal act composed entirely of \"output a machine-readable proof of this theorem and the world is saved\", I would pivot there! It actually does seem like it would be a lot easier!</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:21][12:25]</strong>&nbsp;</p><p>Okay, so if I attempt to rephrase your argument:</p></td></tr><tr><td>Your position: There's a set of fundamental similarities between tasks like doing maths, doing alignment research, and taking over the world. In all of these cases, agents based on techniques similar to modern ML which are very good at them will need to make use of deep problem-solving patterns which include goal-oriented reasoning. So while it's possible to beat humans at some of these tasks without those core competencies, people usually overestimate the extent to which that's possible.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:25]</strong>&nbsp;</p><p>Remember, a lot of my concern is about what happens <i>first</i>, especially if it happens soon enough that future AGI bears any resemblance whatsoever to modern ML; not about what can be done in principle.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][12:26]</strong>&nbsp;</p><p>(Note: it's been 85 min, and we're planning to take a break at 90min, so this seems like a good point for a little bit more clarifying back-and-forth on Richard's summary before a break.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:26]</strong>&nbsp;</p><p>I'll edit to say \"plausible for ML techniques\"?</p><p>(and \"extent to which that's plausible\")</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:28]</strong>&nbsp;</p><p>I think that obvious-to-me future outgrowths of modern ML paradigms are <i>extremely</i> liable to, if they can learn how to do sufficiently superhuman X, generalize to taking over the world. How fast this happens does depend on X. It would plausibly happen relatively slower (at higher levels) with theorem-proving as the X, and with architectures that carefully stuck to gradient-descent-memorization over shallow network architectures to do a pattern-recognition part with search factored out (sort of, this is not generally safe, this is not a general formula for safe things!); rather than imposing anything like the genetic bottleneck you validly pointed out as a reason why humans generalize. Profitable X, and all X I can think of that would actually save the world, seem much more problematic.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:30]</strong>&nbsp;</p><p>Okay, happy to take a break here.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][12:30]</strong>&nbsp;</p><p>Great timing!</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:30]</strong>&nbsp;</p><p>We can do a bit of meta discussion afterwards; my initial instinct is to push on the question of how similar Eliezer thinks alignment research is to theorem-proving.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:30]</strong>&nbsp;</p><p>Yup. This is my lunch break (actually my first-food-of-day break on a 600-calorie diet) so I can be back in 45min if you're still up for that.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:31]</strong>&nbsp;</p><p>Sure.</p><p>Also, if any of the spectators are reading in real time, and have suggestions or comments, I'd be interested in hearing them.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:31]</strong>&nbsp;</p><p>I'm also cheerful about spectators posting suggestions or comments during the break.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][12:32]</strong>&nbsp;</p><p>Sounds good. I declare us on a break for 45min, at which point we'll reconvene (for another 90, by default).</p><p>Floor's open to suggestions &amp; commentary.</p></td></tr></tbody></table></figure><p>&nbsp;</p><h2>1.2. Requirements for science</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:50]</strong>&nbsp;</p><p>I seem to be done early if people (mainly Richard) want to resume in 10min (30m break)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:51]</strong>&nbsp;</p><p>Yepp, happy to do so</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][12:57]</strong>&nbsp;</p><p>Some quick commentary from me:</p><ul><li>It seems to me like we're exploring a crux in the vicinity of \"should we expect that systems capable of executing a pivotal act would, by default in lieu of significant technical alignment effort, be using their outputs to optimize the future\".</li><li>I'm curious whether you two agree that this is a crux (but plz don't get side-tracked answering me).</li><li>The general discussion seems to be going well to me.<ul><li>In particular, huzzah for careful and articulate efforts to zero in on cruxes.</li></ul></li></ul></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:00]</strong>&nbsp;</p><p>I think that's a crux for the specific pivotal act of \"doing better alignment research\", and maybe some other pivotal acts, but not all (or necessarily most) of them.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:01]</strong>&nbsp;</p><p>I should also say out loud that I've been working a bit with Ajeya on making an attempt to convey the intuitions behind there being deep patterns that generalize and are liable to be learned, which covered a bunch of ground, taught me how much ground there was, and made me relatively more reluctant to try to re-cover the same ground in this modality.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:02]</strong>&nbsp;</p><p>Going forward, a couple of things I'd like to ask Eliezer about:</p><ul><li>In what ways are the tasks that are most useful for alignment similar or different to proving mathematical theorems (which we agreed might generalise relatively slowly to taking over the world)?</li><li>What are the deep problem-solving patterns underlying these tasks?</li><li>Can you summarise my position?</li></ul><p>I was going to say that I was most optimistic about #2 in order to get these ideas into a public format</p><p>But if that's going to happen anyway based on Ajeya's work, then that seems less important</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:03]</strong>&nbsp;</p><p>I could still try briefly and see what happens.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:03]</strong>&nbsp;</p><p>That seems valuable to me, if you're up for it.</p><p>At the same time, I'll try to summarise some of my own intuitions about intelligence which I expect to be relevant.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:04]</strong>&nbsp;</p><p>I'm not sure I could summarize your position in a non-straw way. To me there's a huge visible distance between \"solve alignment for us\" and \"output machine-readable proofs of theorems\" where I can't give a good account of why you think talking about the latter would tell us much about the former. I don't know what other pivotal act you think might be easier.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:06]</strong>&nbsp;</p><p>I see. I was considering \"solving scientific problems\" as an alternative to \"proving theorems\", with alignment being one (particularly hard) example of a scientific problem.</p><p>But decided to start by discussing theorem-proving since it seemed like a clearer-cut case.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:07]</strong>&nbsp;</p><p>Can you predict in advance why Eliezer thinks \"solving scientific problems\" is significantly thornier? (Where alignment is like totally not \"a particularly hard example of a scientific problem\" except in the sense that it has science in it at all; which is maybe the real crux; but also a more difficult issue.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:09]</strong>&nbsp;</p><p>Based on some of your earlier comments, I'm currently predicting that you think the step where the solutions need to be legible to and judged by humans makes science much thornier than theorem-proving, where the solutions are machine-checkable.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:10]</strong>&nbsp;</p><p>That's one factor. Should I state the other big one or would you rather try to state it first?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:10]</strong>&nbsp;</p><p>Requiring a lot of real-world knowledge for science?</p><p>If it's not that, go ahead and say it.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:11]</strong>&nbsp;</p><p>That's one way of stating it. The way I'd put it is that it's about making up hypotheses about the real world.</p><p>Like, the real world is then a thing that the AI is modeling, at all.</p><p>Factor 3: On many interpretations of doing science, you would furthermore need to think up experiments. That's planning, value-of-information, search for an experimental setup whose consequences distinguish between hypotheses (meaning you're now searching for initial setups that have particular causal consequences).</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:12]</strong>&nbsp;</p><p>To me \"modelling the real world\" is a very continuous variable. At one end you have physics equations that are barely separable from maths problems, at the other end you have humans running around in physical bodies.</p><p>To me it seems plausible that we could build an agent which solves scientific problems but has very little self-awareness (in the sense of knowing that it's an AI, knowing that it's being trained, etc).</p><p>I expect that your response to this is that modelling oneself is part of the deep problem-solving patterns which AGIs are very likely to have.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:15]</strong>&nbsp;</p><p>There's a problem of <i>inferring the causes of sensory experience</i> in cognition-that-does-science. (Which, in fact, also appears in the way that humans do math, and is possibly inextricable from math in general; but this is an example of the sort of deep model that says \"Whoops I guess you get science from math after all\", not a thing that makes science less dangerous because it's more like just math.)</p><p>You can build an AI that only ever drives red cars, and which, at no point in the process of driving a red car, ever needs to drive a blue car in order to drive a red car. That doesn't mean its red-car-driving capabilities won't be extremely close to blue-car-driving capabilities if at any point the internal cognition happens to get pointed towards driving a blue car.</p><p>The fact that there's a deep car-driving pattern which is the same across red cars and blue cars doesn't mean that the AI has ever driven a blue car, per se, or that it has to drive blue cars to drive red cars. But if blue cars are fire, you sure are playing with that fire.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:18]</strong>&nbsp;</p><p>To me, \"sensory experience\" as in \"the video and audio coming in from this body that I'm piloting\" and \"sensory experience\" as in \"a file containing the most recent results of the large hadron collider\" are very very different.</p><p>(I'm not saying we could train an AI scientist just from the latter - but plausibly from data that's closer to the latter than the former)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:19]</strong>&nbsp;</p><p>So there's separate questions about \"does an AGI <i>inseparably need</i> to model itself inside the world to do science\" and \"did we build something that would be very close to modeling itself, and could easily stumble across that by accident somewhere in the inscrutable floating-point numbers, especially if that was even slightly useful for solving the outer problems\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:19]</strong>&nbsp;</p><p>Hmm, I see</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:20][13:21][13:21]</strong>&nbsp;</p><p>If you're trying to build an AI that literally does science only to observations collected without the AI having had a causal impact on those observations, that's legitimately \"more dangerous than math but maybe less dangerous than active science\".</p></td></tr><tr><td>You might still stumble across an active scientist because it was a simple internal solution to something, but the outer problem would be legitimately stripped of an important structural property the same way that pure math not describing Earthly objects is stripped of important structural properties.</td></tr><tr><td>And of course my reaction again is, \"There is no pivotal act which uses only that cognitive capability.\"</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:20][13:21][13:26]</strong>&nbsp;</p><p>I guess that my (fairly strong) prior here is that something like self-modelling, which is very deeply built into basically every organism, is a very hard thing for an AI to stumble across by accident without significant optimisation pressure in that direction.</p></td></tr><tr><td>But I'm not sure how to argue this except by digging into your views on what the deep problem-solving patterns are. So if you're still willing to briefly try and explain those, that'd be useful to me.</td></tr><tr><td>\"Causal impact\" again seems like a very continuous variable - it seems like the <i>amount</i> of causal impact you need to do good science is much less than the amount which is needed to, say, be a CEO.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:26]</strong>&nbsp;</p><p>The amount doesn't seem like the key thing, nearly so much as what underlying facilities you need to do whatever amount of it you need.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:27]</strong>&nbsp;</p><p>Agreed.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:27]</strong>&nbsp;</p><p>If you go back to the 16th century and ask for just one mRNA vaccine, that's not much of a difference from asking for a <s>million</s> hundred of them.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:28]</strong>&nbsp;</p><p>Right, so the additional premise which I'm using here is that the ability to reason about causally impacting the world in order to achieve goals is something that you can have a little bit of.</p><p>Or a lot of, and that the difference between these might come down to the training data used.</p><p>Which at this point I don't expect you to agree with.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:29]</strong>&nbsp;</p><p>If you have reduced a pivotal act to \"look over the data from this hadron collider you neither built nor ran yourself\", that really is a structural step down from \"do science\" or \"build a nanomachine\". But I can't see any pivotal acts like that, so is that question much of a crux?</p><p>If there's intermediate steps they might be described in my native language like \"reason about causal impacts across only this one preprogrammed domain which you didn't learn in a general way, in only this part of the cognitive architecture that is separable from the rest of the cognitive architecture\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:31]</strong>&nbsp;</p><p>Perhaps another way of phrasing this intermediate step is that the agent has a shallow understanding of how to induce causal impacts.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:31]</strong>&nbsp;</p><p>What is \"shallow\" to you?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:31]</strong>&nbsp;</p><p>In a similar way to how you claim that GPT-3 has a shallow understanding of language.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:32]</strong>&nbsp;</p><p>So it's memorized a ton of shallow causal-impact-inducing patterns from a large dataset, and this can be verified by, for example, presenting it with an example mildly outside the dataset and watching it fail, which we think will confirm our hypothesis that it didn't learn any deep ways of solving that dataset.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:33]</strong>&nbsp;</p><p>Roughly speaking, yes.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:34]</strong>&nbsp;</p><p>Eg, it wouldn't surprise us at all if GPT-4 had learned to predict \"27 * 18\" but not \"what is the area of a rectangle 27 meters by 18 meters\"... is what I'd like to say, but Codex sure did demonstrate those two were kinda awfully proximal.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:34]</strong>&nbsp;</p><p>Here's one way we could flesh this out. Imagine an agent that loses coherence quickly when it's trying to act in the world.</p><p>So for example, we've trained it to do scientific experiments over a period of a few hours or days</p><p>And then it's very good at understanding the experimental data and extracting patterns from it</p><p>But upon running it for a week or a month, it loses coherence in a similar way to how GPT-3 loses coherence - e.g. it forgets what it's doing.</p><p>My story for why this might happen is something like: there is a specific skill of having long-term memory, and we never trained our agent to have this skill, and so it has not acquired that skill (even though it can reason in very general and powerful ways in the short term).</p><p>This feels similar to the argument I was making before about how an agent might lack self-awareness, if we haven't trained it specifically to have that.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:39]</strong>&nbsp;</p><p>There's a set of obvious-to-me tactics for doing a pivotal act with minimal danger, which I do not think collectively make the problem safe, and one of these sets of tactics is indeed \"Put a limit on the 'attention window' or some other internal parameter, ramp it up slowly, don't ramp it any higher than you needed to solve the problem.\"</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:41]</strong>&nbsp;</p><p>You could indeed do this manually, but my expectation is that you could also do this automatically, by training agents in environments where they don't benefit from having long attention spans.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:42]</strong>&nbsp;</p><p>(Any time one imagines a specific tactic of this kind, if one has the <a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\">security mindset</a>, one can also imagine all sorts of ways it might go wrong; for example, an attention window can be defeated if there's any aspect of the attended data or the internal state that ended up depending on past events in a way that leaked info about them. But, depending on how much superintelligence you were throwing around elsewhere, you could maybe get away with that, some of the time.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:43]</strong>&nbsp;</p><p>And that if you put agents in environments where they answer questions but don't interact much with the physical world, then there will be many different traits which are necessary for achieving goals in the real world which they will lack, because there was little advantage to the optimiser of building those traits in.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:43]</strong>&nbsp;</p><p>I'll observe that TransformerXL built an attention window that generalized, trained it on I think 380 tokens or something like that, and then found that it generalized to 4000 tokens or something like that.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:43]</strong>&nbsp;</p><p>Yeah, an order of magnitude of generalisation is not surprising to me.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:44]</strong>&nbsp;</p><p>Having observed one order of magnitude, I would personally not be surprised by two orders of magnitude either, after seeing that.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:45]</strong>&nbsp;</p><p>I'd be a little surprised, but I assume it would happen eventually.</p></td></tr></tbody></table></figure><p>&nbsp;</p><h2>1.3. Capability dials</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:46]</strong>&nbsp;</p><p>I have a sense that this is all circling back to the question, \"But what is it we <i>do</i> with the intelligence thus weakened?\" If you can save the world using a rock, I can build you a very safe rock.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:46]</strong>&nbsp;</p><p>Right.</p><p>So far I've said \"alignment research\", but I haven't been very specific about it.</p><p>I guess some context here is that I expect that the first things we do with intelligence similar to this is create great wealth, produce a bunch of useful scientific advances, etc.</p><p>And that we'll be in a world where people take the prospect of AGI much more seriously</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:48]</strong>&nbsp;</p><p>I mostly expect - albeit with some chance that reality says \"So what?\" to me and surprises me, because it is not as solidly determined as some other things - that we do not hang around very long in the \"weirdly ~human AGI\" phase before we get into the \"if you crank up this AGI it destroys the world\" phase. Less than 5 years, say, to put numbers on things.</p><p>It would not surprise me in the least if the world ends before self-driving cars are sold on the mass market. On some quite plausible scenarios which I think have &gt;50% of my probability mass at the moment, research AGI companies would be able to produce prototype car-driving AIs if they spent time on that, given the near-world-ending tech level; but there will be Many Very Serious Questions about this relatively new unproven advancement in machine learning being turned loose on the roads. And their AGI tech will gain the property \"can be turned up to destroy the world\" before Earth gains the property \"you're allowed to sell self-driving cars on the mass market\" because there just won't be much time.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:52]</strong>&nbsp;</p><p>Then I expect that another thing we do with this is produce a very large amount of data which rewards AIs for following human instructions.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:52]</strong>&nbsp;</p><p>On other scenarios, of course, self-driving becomes possible by limited AI well before things start to break (further) on AGI. And on some scenarios, the way you got to AGI was via some breakthrough that is already scaling pretty fast, so by the time you can use the tech to get self-driving cars, that tech already ends the world if you turn up the dial, or that event follows very swiftly.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:53]</strong>&nbsp;</p><p>When you talk about \"cranking up the AGI\", what do you mean?</p><p>Using more compute on the same data?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:53]</strong>&nbsp;</p><p>Running it with larger bounds on the for loops, over more GPUs, to be concrete about it.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:53]</strong>&nbsp;</p><p>In a RL setting, or a supervised, or unsupervised learning setting?</p><p>Also: can you elaborate on the for loops?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:56]</strong>&nbsp;</p><p>I do not quite think that gradient descent on Stack More Layers alone - as used by OpenAI for GPT-3, say, and as <i>opposed</i> to Deepmind which builds more complex artifacts like Mu Zero or AlphaFold 2 - is liable to be the first path taken to AGI. I am reluctant to speculate more in print about clever ways to AGI, and I think any clever person out there will, if they are really clever and not just a fancier kind of stupid, not talk either about what they think is missing from Stack More Layers or how you would really get AGI. That said, the way that you cannot just run GPT-3 at a greater search depth, the way you can run Mu Zero at a greater search depth, is part of why I think that AGI is not likely to look <i>exactly</i> like GPT-3; the thing that kills us is likely to be a thing that can get more dangerous when you turn up a dial on it, not a thing that intrinsically has no dials that can make it more dangerous.</p></td></tr></tbody></table></figure><p>&nbsp;</p><h2>1.4. Consequentialist goals vs. deontologist goals</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:59]</strong>&nbsp;</p><p>Hmm, okay. Let's take a quick step back and think about what would be useful for the last half hour.</p><p>I want to flag that my intuitions about pivotal acts are not very specific; I'm quite uncertain about how the geopolitics of that situation would work, as well as the timeframe between somewhere-near-human-level AGI and existential risk AGI.</p><p>So we could talk more about this, but I expect there'd be a lot of me saying \"well we can't rule out that X happens\", which is perhaps not the most productive mode of discourse.</p><p>A second option is digging into your intuitions about how cognition works.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:03]</strong>&nbsp;</p><p>Well, obviously, in the limit of alignment not being accessible to our civilization, and my successfully building a model weaker than reality which nonetheless correctly rules out alignment being accessible to our civilization, I could spend the rest of my short remaining lifetime arguing with people whose models are weak enough to induce some area of ignorance where for all they know you could align a thing. But that is predictably how conversations go in possible worlds where the Earth is doomed; so somebody wiser on the meta-level, though also ignorant on the object-level, might prefer to ask: \"Where do you think your knowledge, rather than your ignorance, says that alignment ought to be doable and you will be surprised if it is not?\"</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:07]</strong>&nbsp;</p><p>That's a fair point. Although it seems like a structural property of the \"pivotal act\" framing, which builds in doom by default.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:08]</strong>&nbsp;</p><p>We could talk about that, if you think it's a crux. Though I'm also not thinking that this whole conversation gets done in a day, so maybe for publishability reasons we should try to focus more on one line of discussion?</p><p>But I do think that lots of people get their optimism by supposing that the world can be saved by doing less dangerous things with an AGI. So it's a big ol' crux of mine on priors.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:09]</strong>&nbsp;</p><p>Agreed that one line of discussion is better; I'm happy to work within the pivotal act framing for current purposes.</p><p>A third option is that I make some claims about how cognition works, and we see how much you agree with them.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:12]</strong>&nbsp;</p><p>(Though it's something of a restatement, a reason I'm not going into \"my intuitions about how cognition works\" is that past experience has led me to believe that conveying this info in a form that the Other Mind will actually absorb and operate, is really quite hard and takes a long discussion, relative to my current abilities to Actually Explain things; it is the sort of thing that might take doing homework exercises to grasp how one structure is appearing in many places, as opposed to just being flatly told that to no avail, and I have not figured out the homework exercises.)</p><p>I'm cheerful about hearing your own claims about cognition and disagreeing with them.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:12]</strong>&nbsp;</p><p>Great</p><p>Okay, so one claim is that something like deontology is a fairly natural way for minds to operate.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:14]</strong>&nbsp;</p><p>(\"If that were true,\" he thought at once, \"bureaucracies and books of regulations would be a lot more efficient than they are in real life.\")</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:14]</strong>&nbsp;</p><p>Hmm, although I think this was probably not a very useful phrasing, let me think about how to rephrase it.</p><p>Okay, so in <a href=\"https://docs.google.com/document/d/1XXGbFnWPXtsRiTxleBZ0LAGtU7_7CYKt17nnowfpKvo/edit\">our earlier email discussion</a>, we talked about the concept of \"obedience\".</p><p>To me it seems like it is just as plausible for a mind to have a concept like \"obedience\" as its rough goal, as a concept like maximising paperclips.</p><p>If we imagine training an agent on a large amount of data which pointed in the rough direction of rewarding obedience, for example, then I imagine that by default obedience would be a constraint of comparable strength to, say, the human survival instinct.</p><p>(Which is obviously not strong enough to stop humans doing a bunch of things that contradict it - but it's a pretty good starting point.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:18]</strong>&nbsp;</p><p>Heh. You mean of comparable strength to the human instinct to explicitly maximize inclusive genetic fitness?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:19]</strong>&nbsp;</p><p>Genetic fitness wasn't a concept that our ancestors were able to understand, so it makes sense that they weren't pointed directly towards it.</p><p>(And nor did they understand <i>how</i> to achieve it.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:19]</strong>&nbsp;</p><p>Even in that paradigm, except insofar as you expect gradient descent to work very differently from gene-search optimization - which, admittedly, it does - when you optimize really hard on a thing, you get contextual correlates to it, not the thing you optimized on.</p><p>This is of course one of the Big Fundamental Problems that I expect in alignment.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:20]</strong>&nbsp;</p><p>Right, so the main correlate that I've seen discussed is \"do what would make the human give you a high rating, not what the human actually wants\"</p><p>One thing I'm curious about is the extent to which you're concerned about this specific correlate, versus correlates in general.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:21]</strong>&nbsp;</p><p>That said, I also see basic structural reasons why paperclips would be much easier to train than \"obedience\", even if we could magically instill simple inner desires that perfectly reflected the simple outer algorithm we saw ourselves as running over many particular instances of a loss function.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:22]</strong>&nbsp;</p><p>I'd be interested in hearing what those are.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:22]</strong>&nbsp;</p><p>well, first of all, why <i>is</i> a book of regulations so much more unwieldy than a hunter-gatherer?</p><p>if deontology is just as good as <a href=\"https://arbital.com/p/consequentialist/\">consequentialism</a>, y'know.</p><p>(do you want to try replying or should I just say?)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:23]</strong>&nbsp;</p><p>Go ahead</p><p>I should probably clarify that I agree that you can't just replace consequentialism with deontology</p><p>The claim is more like: when it comes to high-level concepts, it's not clear to me why high-level consequentialist goals are more natural than high-level deontological goals.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:24]</strong>&nbsp;</p><p>I reply that reality is complicated, so when you pump a simple goal through complicated reality you get complicated behaviors required to achieve the goal. If you think of reality as a complicated function Input-&gt;Probability(Output), then even to get a simple Output or a simple partition on Output or a high expected score in a simple function over Output, you may need very complicated Input.</p><p>Humans don't trust each other. They imagine, \"Well, if I just give this bureaucrat a goal, perhaps they won't reason honestly about what it takes to achieve that goal! Oh no! Therefore I will instead, being the trustworthy and accurate person that I am, reason myself about constraints and requirements on the bureaucrat's actions, such that, if the bureaucrat obeys these regulations, I expect the outcome of their action will be what I want.\"</p><p>But (compared to a general intelligence that observes and models complicated reality and does its own search to pick actions) an actually-effective book of regulations (implemented by some nonhuman mind with a large enough and perfect enough memory to memorize it) would tend to involve a (physically unmanageable) vast number of rules saying \"if you observe this, do that\" to follow all the crinkles of complicated reality as it can be inferred from observation.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:28]</strong>&nbsp;</p><blockquote><p>(Though it's something of a restatement, a reason I'm not going into \"my intuitions about how cognition works\" is that past experience has led me to believe that conveying this info in a form that the Other Mind will actually absorb and operate, is really quite hard and takes a long discussion, relative to my current abilities to Actually Explain things; it is the sort of thing that might take doing homework exercises to grasp how one structure is appearing in many places, as opposed to just being flatly told that to no avail, and I have not figured out the homework exercises.)</p></blockquote><p>(As a side note: do you have a rough guess for when your work with Ajeya will be made public? If it's still a while away, I'm wondering whether it's still useful to have a rough outline of these intuitions even if it's in a form that very few people will internalise)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:30]</strong>&nbsp;</p><blockquote><p>(As a side note: do you have a rough guess for when your work with Ajeya will be made public? If it's still a while away, I'm wondering whether it's still useful to have a rough outline of these intuitions even if it's in a form that very few people will internalise)</p></blockquote><p>Plausibly useful, but not to be attempted today, I think?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:30]</strong>&nbsp;</p><p>Agreed.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:30]</strong>&nbsp;</p><p>(We are now theoretically in overtime, which is okay for me, but for you it is 11:30pm (I think?) and so it is on you to call when to halt, now or later.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:32]</strong>&nbsp;</p><p>Yeah, it's 11.30 for me. I think probably best to halt here. I agree with all the things you just said about reality being complicated, and why consequentialism is therefore valuable. My \"deontology\" claim (which was, in its original formulation, far too general - apologies for that) was originally intended as a way of poking into your intuitions about which types of cognition are natural or unnatural, which I think is the topic we've been circling around for a while.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:33]</strong>&nbsp;</p><p>Yup, and a place to resume next time might be why I think \"obedience\" is unnatural compared to \"paperclips\" - though that is a thing that probably requires taking that stab at what underlies surface competencies.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:34]</strong>&nbsp;</p><p>Right. I do think that even a vague gesture at that would be reasonably helpful (assuming that this doesn't already exist online?)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:34]</strong>&nbsp;</p><p>Not yet afaik, and I don't want to point you to Ajeya's stuff even if she were ok with that, because then this in-context conversation won't make sense to others.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:35]</strong>&nbsp;</p><p>For my part I should think more about pivotal acts that I'd be willing to specifically defend.</p><p>In any case, thanks for the discussion \ud83d\ude42</p><p>Let me know if there's a particular time that suits you for a follow-up; otherwise we can sort it out later.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][14:37]</strong>&nbsp;</p><p>(y'all are doing all my jobs for me)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:37]</strong>&nbsp;</p><p>could try Tuesday at this same time - though I may be in worse shape for dietary reasons, still, seems worth trying.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][14:37]</strong>&nbsp;</p><p>(wfm)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:39]</strong>&nbsp;</p><p>Tuesday not ideal, any others work?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:39]</strong>&nbsp;</p><p>Wednesday?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:40]</strong>&nbsp;</p><p>Yes, Wednesday would be good</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 100%);vertical-align:top\"><p><strong>[Yudkowsky][14:40]</strong>&nbsp;</p><p>let's call it tentatively for that</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][14:41]</strong>&nbsp;</p><p>Great! Thanks for the chats.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 100%);vertical-align:top\"><p><strong>[Ngo][14:41]</strong>&nbsp;</p><p>Thanks both!</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 100%);vertical-align:top\"><p><strong>[Yudkowsky][14:41]</strong>&nbsp;</p><p>Thanks, Richard!</p></td></tr></tbody></table></figure><p>&nbsp;</p><h1>2. Follow-ups</h1><p>&nbsp;</p><h2>2.1. Richard Ngo's summary</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%)\"><p><strong>[Tallinn][0:35] &nbsp;(Sep. 6)</strong>&nbsp;</p><p>just caught up here &amp; wanted to thank nate, eliezer and (especially) richard for doing this! it's great to see eliezer's model being probed so intensively. i've learned a few new things (such as the genetic bottleneck being plausibly a big factor in human cognition). FWIW, a minor comment re deontology (as that's fresh on my mind): in my view deontology is more about coordination than optimisation: deontological agents are more trustworthy, as they're much easier to reason about (in the same way how functional/declarative code is easier to reason about than imperative code). hence my steelman of bureaucracies (as well as social norms): humans just (correctly) prefer their fellow optimisers (including non-human optimisers) to be deontological for trust/coordination reasons, and are happy to pay the resulting competence tax.</p></td></tr><tr><td><p><strong>[Ngo][3:10] &nbsp;(Sep. 8)</strong>&nbsp;</p><p>Thanks Jaan! I agree that greater trust is a good reason to want agents which are deontological at some high level.</p><p>I've attempted a summary of the key points so far; comments welcome: [GDocs link]</p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p><i>1st discussion</i></p><p>(Mostly summaries not quotations)</p><p>Eliezer, summarized by Richard: \"To avoid catastrophe, whoever builds AGI first will have to a) align it to some extent, and b) decide not to scale it up beyond the point where their alignment techniques fail, and c) do some pivotal act that prevents others from scaling it up to that level. But <s>our alignment techniques will not be good enough&nbsp;</s> <s>our alignment techniques will be very far from adequate</s> on our current trajectory, our alignment techniques will be very far from adequate to create an AI that safely performs any such pivotal act.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][11:05] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><blockquote><p>will not be good enough</p></blockquote><p>Are not presently on course to be good enough, missing by not a little.&nbsp; \"Will not be good enough\" is literally declaring for lying down and dying.</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][16:03] &nbsp;(Sep. 9 comment)</strong>&nbsp;</p><blockquote><p>will [be very far from adequate]</p></blockquote><p>Same problem as the last time I commented.&nbsp; I am not making an unconditional prediction about future failure as would be implied by the word \"will\".&nbsp; Conditional on current courses of action or their&nbsp;near neighboring courses, we seem to be well over an order of magnitude away from surviving, unless a miracle occurs.&nbsp; It's still in the end a result of people doing what they seem to be doing, not an inevitability.</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Ngo][5:10] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><p>Ah, I see. Does adding \"on our current trajectory\" fix this?</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:46] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><p>Yes.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Consider the pivotal act of 'make a breakthrough in alignment research'. It is likely that, before the point where AGIs are strongly superhuman at seeking power, they will already be strongly superhuman at understanding the world, and at performing narrower pivotal acts like alignment research which don\u2019t require as much agency (by which I roughly mean: large-scale motivations and the ability to pursue them over long timeframes).\"</p><p>Eliezer, summarized by Richard: \"There\u2019s a deep connection between solving intellectual problems and taking over the world - the former requires a powerful mind to think about domains that, when solved, render very cognitively accessible strategies that can do dangerous things. Even mathematical research is a goal-oriented task which involves identifying then pursuing instrumental subgoals - and if brains which evolved to hunt on the savannah can quickly learn to do mathematics, then it\u2019s also plausible that AIs trained to do mathematics could quickly learn a range of other skills. Since almost nobody understands the deep similarities in the cognition required for these different tasks, the distance between AIs that are able to perform fundamental scientific research, and dangerously agentic AGIs, is smaller than almost anybody expects.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][11:05] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><blockquote><p>There\u2019s a deep connection between solving intellectual problems and taking over the world</p></blockquote><p>There's a deep connection by default between chipping flint handaxes and taking over the world, if you happen to learn how to chip handaxes in a very general way. &nbsp;\"Intellectual\" problems aren't special in this way. &nbsp;And maybe you could avert the default, but that would take some work and you'd have to do it before easier default ML techniques destroyed the world.</p></td></tr><tr><td style=\"background-color:rgb(255,247,222)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Our lack of understanding about how intelligence works also makes it easy to assume that traits which co-occur humans will also co-occur in future AIs. But human brains are badly-optimised for tasks like scientific research, and well-optimised for seeking power over the world, for reasons including a) evolving while embodied in a harsh environment; b) the genetic bottleneck; c) social environments which rewarded power-seeking. By contrast, training neural networks on tasks like mathematical or scientific research optimises them much less for seeking power. For example, GPT-3 has knowledge and reasoning capabilities but little agency, and loses coherence when run for longer timeframes.\"</p></td></tr><tr><td style=\"background-color:rgb(255,238,187)\"><p><strong>[Tallinn][4:19] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><blockquote><p>[well-optimised for] seeking power</p></blockquote><p>male-female differences might be a datapoint here (annoying as it is to lean on pinker's point :))</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][11:31] &nbsp;(Sep. 8 comment)</strong>&nbsp;</p><p>I don't think a female Eliezer Yudkowsky doesn't try to save / optimize / takeover the world.&nbsp; Men may do that for nonsmart reasons; smart men and women follow the same reasoning when they are smart enough.&nbsp; Eg Anna Salamon and many others.</p></td></tr><tr><td style=\"background-color:rgb(255,247,222)\"><p><strong>[Ngo] &nbsp;(Sep. 8 Google Doc)</strong>&nbsp;</p><p>Eliezer, summarized by Richard: \"Firstly, there\u2019s a big difference between most scientific research and the sort of pivotal act that we\u2019re talking about - you need to explain how AIs with a given skill can be used to actually prevent dangerous AGIs from being built. Secondly, insofar as GPT-3 has little agency, that\u2019s because it has memorised many shallow patterns in a way which won\u2019t directly scale up to general intelligence. Intelligence instead consists of deep problem-solving patterns which link understanding and agency at a fundamental level.\"</p></td></tr></tbody></table></figure><p>&nbsp;</p><h1>3. September 8 conversation</h1><p>&nbsp;</p><h2>3.1. The Brazilian university anecdote</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:00]</strong>&nbsp;</p><p>(I am here.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:01]</strong>&nbsp;</p><p>Me too.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][11:01]</strong>&nbsp;</p><p>Welcome back!</p><p>(I'll mostly stay out of the way again.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:02]</strong>&nbsp;</p><p>Cool. Eliezer, did you read the summary - and if so, do you roughly endorse it?</p><p>Also, I've been thinking about the best way to approach discussing your intuitions about cognition. My guess is that starting with the obedience vs paperclips thread is likely to be less useful than starting somewhere else - e.g. the description you gave near the beginning of the last discussion, about \"searching for states that get fed into a result function and then a result-scoring function\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:06]</strong>&nbsp;</p><p>made a couple of comments about phrasings in the doc</p><p>So, from my perspective, there's this thing where... it's really quite hard to teach certain <i>general</i> points by talking at people, as opposed to more specific points. Like, they're trying to build a perpetual motion machine, and even if you can manage to argue them into believing their first design is wrong, they go looking for a new design, and the new design is complicated enough that they can no longer be convinced that they're wrong because they managed to make a more complicated error whose refutation they couldn't keep track of anymore.</p><p>Teaching people to see an underlying structure in a lot of places is a very hard thing to teach in this way. Richard Feynman <a href=\"https://v.cx/2010/04/feynman-brazil-education\">gave an example</a> of the mental motion in his story that ends \"Look at the water!\", where people learned in classrooms about how \"a medium with an index\" is supposed to polarize light reflected from it, but they didn't realize that sunlight coming off of water would be polarized. My guess is that doing this properly requires homework exercises; and that, unfortunately from my own standpoint, it happens to be a place where I have extra math talent, the same way that eg Marcello is more talented at formally proving theorems than I happen to be; and that people without the extra math talent, have to do a lot <i>more</i> exercises than I did, and I don't have a good sense of which exercises to give them.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:13]</strong>&nbsp;</p><p>I'm sympathetic to this, and can try to turn off skeptical-discussion-mode and turn on learning-mode, if you think that'll help.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:14]</strong>&nbsp;</p><p>There's a general insight you can have about how arithmetic is commutative, and for some people you can show them 1 + 2 = 2 + 1 and their native insight suffices to generalize over the 1 and the 2 to any other numbers you could put in there, and they realize that strings of numbers can be rearranged and all end up equivalent. For somebody else, when they're a kid, you might have to show them 2 apples and 1 apple being put on the table in a different order but ending up with the same number of apples, and then you might have to show them again with adding up bills in different denominations, in case they didn't generalize from apples to money. I can actually remember being a child young enough that I tried to add 3 to 5 by counting \"5, 6, 7\" and I thought there was some clever enough way to do that to actually get 7, if you tried hard.</p><p>Being able to see \"consequentialism\" is like that, from my perspective.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:15]</strong>&nbsp;</p><p>Another possibility: can you trace the origins of this belief, and how it came out of your previous beliefs?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:15]</strong>&nbsp;</p><p>I don't know what homework exercises to give people to make them able to see \"consequentialism\" all over the place, instead of inventing slightly new forms of consequentialist cognition and going \"Well, now <i>that</i> isn't consequentialism, right?\"</p><p>Trying to say \"searching for states that get fed into an input-result function and then a result-scoring function\" was one attempt of mine to describe the dangerous thing in a way that would maybe sound abstract enough that people would try to generalize it more.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:17]</strong>&nbsp;</p><p>Another possibility: can you describe the closest thing to real consequentialism in humans, and how it came about in us?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:18][11:21]</strong>&nbsp;</p><p>Ok, so, part of the problem is that... before you do enough homework exercises for whatever your level of talent is (and even I, at one point, had done little enough homework that I thought there might be a clever way to add 3 and 5 in order to get to 7), you tend to think that only the very crisp formal thing that's been presented to you, is the \"real\" thing.</p><p>Why would your engine have to obey the laws of thermodynamics? You're not building one of those Carnot engines you saw in the physics textbook!</p><p>Humans contain fragments of consequentialism, or bits and pieces whose interactions add up to partially imperfectly shadow consequentialism, and the critical thing is being able to see that the reason why humans' outputs 'work', in a sense, is because these structures are what is doing the work, and the work gets done because of how they shadow consequentialism and only insofar as they shadow consequentialism.</p></td></tr><tr><td>Put a human in one environment, it gets food. Put a human in a different environment, it gets food again. Wow, different initial conditions, same output! There must be things inside the human that, whatever else they do, are also along the way somehow effectively searching for motor signals such that food is the end result!</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:20]</strong>&nbsp;</p><p>To me it feels like you're trying to nudge me (and by extension whoever reads this transcript) out of a specific failure mode. If I had to guess, something like: \"I understand what Eliezer is talking about so now I'm justified in disagreeing with it\", or perhaps \"Eliezer's explanation didn't make sense to me and so I'm justified in thinking that his concepts don't make sense\". Is that right?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:22]</strong>&nbsp;</p><p>More like... from my perspective, even after I talk people out of one specific perpetual motion machine being possible, they go off and try to invent a different, more complicated perpetual motion machine.</p><p>And I am not sure what to do about that. It has been going on for a very long time from my perspective.</p><p>In the end, a lot of what people got out of all that writing I did, was not the deep object-level principles I was trying to point to - they did not really get <a href=\"https://www.lesswrong.com/s/oFePMp9rKftEeZDDr/p/QkX2bAkwG2EpGvNug\">Bayesianism as thermodynamics</a>, say, they did not become able to see <a href=\"https://www.lesswrong.com/posts/QrhAeKBkm2WsdRYao/searching-for-bayes-structure\">Bayesian structures</a> any time somebody sees a thing and changes their belief. What they got instead was something much more meta and general, a vague spirit of how to reason and argue, because that was what they'd spent a lot of time being exposed to over and over and over again in lots of blog posts.</p><p>Maybe there's no way to make somebody understand why <a href=\"https://arbital.com/p/corrigibility/\">corrigibility</a> is \"unnatural\" except to repeatedly walk them through the task of trying to invent an agent structure that lets you press the shutdown button (without it trying to force you to press the shutdown button), and showing them how each of their attempts fails; and then also walking them through why Stuart Russell's attempt at moral uncertainty produces the <a href=\"https://arbital.com/p/updated_deference/\">problem of fully updated (non-)deference</a>; and hope they can start to see the informal general pattern of why corrigibility is in general contrary to the structure of things that are good at optimization.</p><p>Except that to do the exercises at all, you need them to work within an expected utility framework. And then they just go, \"Oh, well, I'll just build an agent that's good at optimizing things but doesn't use these explicit expected utilities that are the source of the problem!\"</p><p>And then if I want them to believe the same things I do, for the same reasons I do, I would have to teach them why certain structures of cognition are the parts of the agent that are good at stuff and do the work, rather than them being this particular formal thing that they learned for manipulating meaningless numbers as opposed to real-world apples.</p><p>And I have tried to write that page once or twice (eg \"<a href=\"https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities\">coherent decisions imply consistent utilities</a>\") but it has not sufficed to teach them, because they did not even do as many homework problems as I did, let alone the greater number they'd have to do because this is in fact a place where I have a particular talent.</p><p>I don't know how to solve this problem, which is why I'm falling back on talking about it at the meta-level.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:30]</strong>&nbsp;</p><p>I'm reminded of a LW post called \"<a href=\"https://www.lesswrong.com/posts/Q924oPJzK92FifuFg/write-a-thousand-roads-to-rome\">Write a thousand roads to Rome</a>\", which iirc argues in favour of trying to explain the same thing from as many angles as possible in the hope that one of them will stick.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][11:31]</strong>&nbsp;</p><p>(Suggestion, not-necessarily-good: having named this problem on the meta-level, attempt to have the object-level debate, while flagging instances of this as it comes up.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:31]</strong>&nbsp;</p><p>I endorse Nate's suggestion.</p><p>And will try to keep the difficulty of the meta-level problem in mind and respond accordingly.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:33]</strong>&nbsp;</p><p>That (Nate's suggestion) is probably the correct thing to do. I name it out loud because sometimes being told about the meta-problem actually does help on the object problem. It seems to help me a lot and others somewhat less, but it does help others at all, for many others.</p></td></tr></tbody></table></figure><p>&nbsp;</p><h2>3.2. Brain functions and outcome pumps</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td><p><strong>[Yudkowsky][11:34]</strong>&nbsp;</p><p>So, do you have a particular question you would ask about input-seeking cognitions? I did try to say why I mentioned those at all (it's a different road to Rome on \"consequentialism\").</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:36]</strong>&nbsp;</p><p>Let's see. So the visual cortex is an example of quite impressive cognition in humans and many other animals. But I'd call this \"pattern-recognition\" rather than \"searching for high-scoring results\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:37]</strong>&nbsp;</p><p>Yup! And it is no coincidence that there are no whole animals formed entirely out of nothing but a visual cortex!</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:37]</strong>&nbsp;</p><p>Okay, cool. So you'd agree that the visual cortex is doing something that's qualitatively quite different from the thing that animals overall are doing.</p><p>Then another question is: can you characterise searching for high-scoring results in non-human animals? Do they do it? Or are you mainly talking about humans and AGIs?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:39]</strong>&nbsp;</p><p>Also by the time you get to like the temporal lobes or something, there is probably some significant amount of \"what could I be seeing that would produce this visual field?\" that is searching through hypothesis-space for hypotheses with high plausibility scores, and for sure at the human level, humans will start to think, \"Well, could I be seeing this? No, that theory has the following problem. How could I repair that theory?\" But it is plausible that there is no low-level analogue of this in a monkey's temporal cortex; and even more plausible that the parts of the visual cortex, if any, which do anything analogous to this, are doing it in a relatively local and definitely very domain-specific way.</p><p>Oh, that's the cerebellum and motor cortex and so on, if we're talking about a cat or whatever. They have to find motor plans that result in their catching the mouse.</p><p>Just because the visual cortex isn't (obviously) running a search doesn't mean the rest of the animal isn't running any searches.</p><p>(On the meta-level, I notice myself hiccuping \"But how could you not see that when looking at a cat?\" and wondering what exercises would be required to teach that.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:41]</strong>&nbsp;</p><p>Well, I see <i>something</i> when I look at a cat, but I don't know how well it corresponds to the concepts you're using. So just taking it slowly for now.</p><p>I have the intuition, by the way, that the motor cortex is in some sense doing a similar thing to the visual cortex - just in reverse. So instead of taking low-level inputs and producing high-level outputs, it's taking high-level inputs and producing low-level outputs. Would you agree with that?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:43]</strong>&nbsp;</p><p>It doesn't directly parse in my ontology because (a) I don't know what you mean by 'high-level' and (b) whole Cartesian agents can be viewed as functions, that doesn't mean all agents can be viewed as non-searching pattern-recognizers.</p><p>That said, all parts of the cerebral cortex have surprisingly similar morphology, so it wouldn't be at all surprising if the motor cortex is doing something similar to visual cortex. (The cerebellum, on the other hand...)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:44]</strong>&nbsp;</p><p>The signal from the visual cortex saying \"that is a cat\", and the signal to the motor cortex saying \"grab that cup\", are things I'd characterise as high-level.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:45]</strong>&nbsp;</p><p>Still less of a native distinction in my ontology, but there's an informal thing it can sort of wave at, and I can hopefully take that as understood and run with it.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:45]</strong>&nbsp;</p><p>The firing of cells in the retina, and firing of motor neurons, are the low-level parts.</p><p>Cool. So to a first approximation, we can think about the part in between the cat recognising a mouse, and the cat's motor cortex producing the specific neural signals required to catch the mouse, as the part where the consequentialism happens?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:49]</strong>&nbsp;</p><p>The part between the cat's eyes seeing the mouse, and the part where the cat's limbs move to catch the mouse, is the whole cat-agent. The whole cat agent sure is a baby consequentialist / searches for mouse-catching motor patterns / gets similarly high-scoring end results even as you vary the environment.</p><p>The visual cortex is a particular part of this system-viewed-as-a-feedforward-function that is, plausibly, by no means surely, either not very searchy, or does only small local visual-domain-specific searches not aimed per se at catching mice; it has the epistemic nature rather than the planning nature.</p><p>Then from one perspective you could reason that \"well, most of the consequentialism is in the remaining cat after visual cortex has sent signals onward\". And this is in general a dangerous mode of reasoning that is liable to fail in, say, inspecting every particular neuron for consequentialism and not finding it; but in this particular case, there are significantly more consequentialist parts of the cat than the visual cortex, so I am okay running with it.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:50]</strong>&nbsp;</p><p>Ah, the more specific thing I meant to say is: most of the consequentialism is strictly between the visual cortex and the motor cortex. Agree/disagree?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:51]</strong>&nbsp;</p><p>Disagree, I'm rusty on my neuroanatomy but I think the motor cortex may send signals on to the cerebellum rather than the other way around.</p><p>(I may also disagree with the actual underlying notion you're trying to hint at, so possibly not just a \"well include the cerebellum then\" issue, but I think I should let you respond first.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][11:53]</strong>&nbsp;</p><p>I don't know enough neuroanatomy to chase that up, so I was going to try a different tack.</p><p>But actually, maybe it's easier for me to say \"let's include the cerebellum\" and see where you think the disagreement ends up.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][11:56]</strong>&nbsp;</p><p>So since cats are not (obviously) (that I have read about) cross-domain consequentialists with imaginations, their consequentialism is in bits and pieces of consequentialism embedded in them all over by the more purely pseudo-consequentialist genetic optimization loop that built them.</p><p>A cat who fails to catch a mouse may then get little bits and pieces of catbrain adjusted all over.</p><p>And then those adjusted bits and pieces get a pattern lookup later.</p><p>Why do these pattern-lookups with no obvious immediate search element, all happen to point towards the same direction of catching the mouse? Because of the past causal history about how what gets looked up, which was tweaked to catch the mouse.</p><p>So it is legit harder to point out \"the consequentialist parts of the cat\" by looking for which sections of neurology are doing searches right there. That said, to the extent that the visual cortex does not get tweaked on failure to catch a mouse, it's not part of that consequentialist loop either.</p><p>And yes, the same applies to humans, but humans also do more explicitly searchy things and this is part of the story for why humans have spaceships and cats do not.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:00]</strong>&nbsp;</p><p>Okay, this is interesting. So in biological agents we've got these three levels of consequentialism: evolution, reinforcement learning, and planning.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:01]</strong>&nbsp;</p><p>In biological agents we've got evolution + local evolved system-rules that in the past promoted genetic fitness. Two kinds of local rules like this are \"operant-conditioning updates from success or failure\" and \"search through visualized plans\". I wouldn't characterize these two kinds of rules as \"levels\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:02]</strong>&nbsp;</p><p>Okay, I see. And when you talk about searching through visualised plans (the type of thing that humans do) can you say more about what it means for that to be a \"search\"?</p><p>For example, if I imagine writing a poem line-by-line, I may only be planning a few words ahead. But somehow the whole poem, which might be quite long, ends up a highly-optimised product. Is that a central example of planning?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:04][12:07]</strong>&nbsp;</p><p>Planning is one way to succeed at search. I think for purposes of understanding alignment difficulty, you want to be thinking on the level of abstraction where you see that in some sense it is the search itself that is dangerous when it's a strong enough search, rather than the danger seeming to come from details of the planning process.</p><p>One of my early experiences in successfully generalizing my notion of intelligence, what I'd later verbalize as \"computationally efficient finding of actions that produce outcomes high in a preference ordering\", was in writing an (unpublished) story about time-travel in which the universe was globally consistent.</p><p>The requirement of global consistency, the way in which all events between Paradox start and Paradox finish had to map the Paradox's initial conditions onto the endpoint that would go back and produce those exact initial conditions, ended up imposing strong complicated constraints on reality that the Paradox in effect had to navigate using its initial conditions. The time-traveler needed to end up going through certain particular experiences that would produce the state of mind in which he'd take the actions that would end up prodding his future self elsewhere into having those experiences.</p></td></tr><tr><td><p>The Paradox ended up killing the people who built the time machine, for example, because they would not otherwise have allowed that person to go back in time, or kept the temporal loop open that long for any other reason if they were still alive.</p><p>Just having two examples of strongly consequentialist general optimization in front of me - human intelligence, and evolutionary biology - hadn't been enough for me to properly generalize over a notion of optimization. Having three examples of homework problems I'd worked - human intelligence, evolutionary biology, and the fictional Paradox - caused it to finally click for me.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:07]</strong>&nbsp;</p><p>Hmm. So to me, one of the central features of search is that you consider many possibilities. But in this poem example, I may only have explicitly considered a couple of possibilities, because I was only looking ahead a few words at a time. This seems related to the distinction Abram drew a while back between selection and control (<a href=\"https://www.lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control\"><u>https://www.alignmentforum.org/posts/ZDZmopKquzHYPRNxq/selection-vs-control</u></a>). Do you distinguish between them in the same way as he does? Or does \"control\" of a system (e.g. a football player dribbling a ball down the field) count as search too in your ontology?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:10][12:11]</strong>&nbsp;</p><p>I would later try to tell people to \"imagine a paperclip maximizer as <i>not being a mind at all</i>, imagine it as a kind of malfunctioning time machine that spits out outputs which will in fact result in larger numbers of paperclips coming to exist later\". I don't think it clicked because people hadn't done the same homework problems I had, and didn't have the same \"Aha!\" of realizing how part of the notion and danger of intelligence could be seen in such purely material terms.</p></td></tr><tr><td>But the <a href=\"https://arbital.com/p/convergent_strategies/\">convergent instrumental strategies</a>, the anticorrigibility, these things are contained in the <i>true fact about the universe</i> that certain outputs of the time machine <i>will in fact</i> result in there being lots more paperclips later. What produces the danger is not the details of the search process, it's the search being strong and effective <i>at all</i>. The danger is in the territory itself and not just in some weird map of it; that building nanomachines that kill the programmers will produce more paperclips is a fact about reality, not a fact about paperclip maximizers!</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:11]</strong>&nbsp;</p><p>Right, I remember a very similar idea in your writing about Outcome Pumps (<a href=\"https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes\"><u>https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes</u></a>).</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:12]</strong>&nbsp;</p><p>Yup! Alas, the story was written in 2002-2003 when I was a worse writer and the real story that inspired the Outcome Pump never did get published.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:14]</strong>&nbsp;</p><p>Okay, so I guess the natural next question is: what is it that makes you think that a strong, effective search isn't likely to be limited or constrained in some way?</p><p>What is it about search processes (like human brains) that makes it hard to train them with blind spots, or deontological overrides, or things like that?</p><p>Hmmm, although it feels like this is a question I can probably predict your answer to. (Or maybe not, I wasn't expecting the time travel.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:15]</strong>&nbsp;</p><p>In one sense, they are! A paperclip-maximizing superintelligence is nowhere near as powerful as a paperclip-maximizing time machine. The time machine can do the equivalent of buying winning lottery tickets from lottery machines that have been thermodynamically randomized; a superintelligence can't, at least not directly without rigging the lottery or whatever.</p><p>But a paperclip-maximizing strong general superintelligence is epistemically and instrumentally <a href=\"https://arbital.com/p/efficiency/\">efficient</a>, relative to <i>you</i>, or to me. Any time we see it can get at least X paperclips by doing Y, we should expect that it gets X or more paperclips by doing Y or something that leads to even more paperclips than that, because it's not going to miss the strategy we see.</p><p>So in that sense, searching our own brains for how a time machine would get paperclips, asking ourselves how many paperclips are in principle possible and how they could be obtained, is a way of getting our own brains to consider lower bounds on the problem without the implicit stupidity assertions that our brains unwittingly use to constrain story characters. Part of the point of telling people to think about time machines instead of superintelligences was to get past the ways they imagine superintelligences being stupid. Of course that didn't work either, but it was worth a try.</p><p>I don't think that's quite what you were asking about, but I want to give you a chance to see if you want to rephrase anything before I try to answer your me-reformulated questions.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:20]</strong>&nbsp;</p><p>Yeah, I think what I wanted to ask is more like: why should we expect that, out of the space of possible minds produced by optimisation algorithms like gradient descent, strong general superintelligences are more common than other types of agents which score highly on our loss functions?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:20][12:23][12:24]</strong>&nbsp;</p><p>It depends on how hard you optimize! And whether gradient descent on a particular system can even successfully optimize that hard! Many current AIs are trained by gradient descent and yet not superintelligences at all.</p></td></tr><tr><td>But the answer is that some problems are difficult in that they require solving lots of subproblems, and an easy way to solve all those subproblems is to use patterns which collectively have some coherence and overlap, and the coherence within them generalizes across all the subproblems. Lots of search orderings will stumble across something like that before they stumble across separate solutions for lots of different problems.</td></tr><tr><td>I suspect that you cannot get this out of small large amounts of gradient descent on small large layered transformers, and therefore I suspect that GPT-N does not approach superintelligence before the world is ended by systems that look differently, but I could be wrong about that.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:22][12:23]</strong>&nbsp;</p><p>Suppose that we optimise hard enough to produce an epistemic subsystem that can make plans much better than any human's.</p></td></tr><tr><td>My guess is that you'd say that this is <i>possible</i>, but that we're much more likely to first produce a consequentialist agent which does this (rather than a purely epistemic agent which does this).</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:24]</strong>&nbsp;</p><p>I am confused by what you think it means to have an \"epistemic subsystem\" that \"makes plans much better than any human's\". If it searches paths through time and selects high-scoring ones for output, what makes it \"epistemic\"?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:25]</strong>&nbsp;</p><p>Suppose, for instance, that it doesn't actually carry out the plans, it just writes them down for humans to look at.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:25]</strong>&nbsp;</p><p>If it <i>can in fact</i> do the thing that a paperclipping time machine does, what makes it any safer than a paperclipping time machine because we called it \"epistemic\" or by some other such name?</p><p>By what criterion is it selecting the plans that humans look at?</p><p>Why did it make a difference that its output was fed through the causal systems called humans on the way to the causal systems called protein synthesizers or the Internet or whatever? If we build a superintelligence to design nanomachines, it makes no obvious difference to its safety whether it sends DNA strings directly to a protein synthesis lab, or humans read the output and retype it manually into an email. Presumably you also don't think that's where the safety difference comes from. So where does the safety difference come from?</p><p>(note: lunchtime for me in 2 minutes, propose to reconvene in 30m after that)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:28]</strong>&nbsp;</p><p>(break for half an hour sounds good)</p><p>If we consider the visual cortex at a given point in time, how does it decide which objects to recognise?</p><p>Insofar as the visual cortex can be non-consequentialist about which objects it recognises, why couldn't a planning system be non-consequentialist about which plans it outputs?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][12:32]</strong>&nbsp;</p><p>This does feel to me like another \"look at the water\" moment, so what do you predict I'll say about that?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:34]</strong>&nbsp;</p><p>I predict that you say something like: in order to produce an agent that can create very good plans, we need to apply a lot of optimisation power to that agent. And if the channel through which we're applying that optimisation power is \"giving feedback on its plans\", then we don't have a mechanism to ensure that the agent actually learns to optimise for creating really good plans, as opposed to creating plans that receive really good feedback.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][12:35]</strong>&nbsp;</p><p>Seems like a fine cliffhanger?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][12:35]</strong>&nbsp;</p><p>Yepp.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][12:35]</strong>&nbsp;</p><p>Great. Let's plan to reconvene in 30min.</p></td></tr></tbody></table></figure><p>&nbsp;</p><h2>3.3. Hypothetical-planning systems, nanosystems, and evolving generality</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:03][13:11]</strong>&nbsp;</p><p>So the answer you expected from me, translated into my terms, would be, \"If you select for the consequence of the humans hitting 'approve' on the plan, you're still navigating the space of inputs for paths through time to probable outcomes (namely the humans hitting 'approve'), so you're still doing consequentialism.\"</p><p>But suppose you manage to avoid that. Suppose you get exactly what you ask for. Then the system is still outputting <i>plans</i> such that, when humans follow them, they take paths through time and end up with outcomes that score high in some scoring function.</p><p>My answer is, \"What the heck would it mean for a <i>planning system</i> to be <i>non-consequentialist</i>? You're asking for nonwet water! What's consequentialist isn't the system that does the work, it's the work you're trying to do! You could imagine it being done by a cognition-free material system like a time machine and it would still be consequentialist <i>because</i> the output is a <i>plan</i>, a path through time!\"</p><p>And this indeed is a case where I feel a helpless sense of not knowing how I can rephrase things, which exercises you have to get somebody to do, what fictional experience you have to walk somebody through, before they start to look at the water and see a material with an index, before they start to look at the phrase \"why couldn't a planning system be non-consequentialist about which plans it outputs\" and go \"um\".</p></td></tr><tr><td>My imaginary listener now replies, \"Ah, but what if we have plans that <i>don't</i> end up with outcomes that score high in some function?\" and I reply \"Then you lie on the ground randomly twitching because any <i>outcome you end up with</i> which is <i>not that</i> is one that you wanted <i>more than that</i> meaning you <i>preferred it more than the outcome of random motor outputs</i> which is <i>optimization toward higher in the preference function</i> which is <i>taking a path through time that leads to particular destinations more than it leads to random noise</i>.\"</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:09][13:11]</strong>&nbsp;</p><p>Yeah, this does seem like a good example of the thing you were trying to explain at the beginning</p></td></tr><tr><td><p>It still feels like there's some sort of levels distinction going on here though, let me try to tease out that intuition.</p><p>Okay, so suppose I have a planning system that, given a situation and a goal, outputs a plan that leads from that situation to that goal.</p><p>And then suppose that we give it, as input, a situation that we're not actually in, and it outputs a corresponding plan.</p><p>It seems to me that there's a difference between the sense in which that planning system is consequentialist by virtue of making consequentialist plans (as in: if that plan were used in the situation described in its inputs, it would lead to some goal being achieved) versus another hypothetical agent that is just directly trying to achieve goals in the situation it's actually in.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:18]</strong>&nbsp;</p><p>So I'd preface by saying that, <i>if</i> you could build such a system, which is indeed a coherent thing (it seems to me) to describe for the purpose of building it, then there would possibly be a safety difference on the margins, it would be noticeably less dangerous though still dangerous. It would need a special internal structural property that you might not get by gradient descent on a loss function with that structure, just like natural selection on inclusive genetic fitness doesn't get you explicit fitness optimizers; you could optimize for planning in hypothetical situations, and get something that didn't explicitly care only and strictly about hypothetical situations. And even if you did get that, the outputs that would kill or brain-corrupt the operators in hypothetical situations might also be fatal to the operators in actual situations. But that is a coherent thing to describe, and the fact that it was not optimizing our own universe, might make it <i>safer</i>.</p><p>With that said, I would worry that somebody would think there was some bone-deep difference of agentiness, of something they were empathizing with like personhood, of imagining goals and drives being absent or present in one case or the other, when they imagine a planner that just solves \"hypothetical\" problems. If you take that planner and feed it the actual world as its hypothetical, tada, it is now that big old dangerous consequentialist you were imagining before, without it having acquired some difference of <i>psychological</i> agency or 'caring' or whatever.</p><p>So I think there is an important homework exercise to do here, which is something like, \"Imagine that safe-seeming system which only considers hypothetical problems. Now see that if you take that system, don't make any other internal changes, and feed it actual problems, it's very dangerous. Now meditate on this until you can see how the hypothetical-considering planner was extremely close in the design space to the more dangerous version, had all the dangerous latent properties, and would probably have a bunch of actual dangers too.\"</p><p>\"See, you thought the source of the danger was this internal property of caring about actual reality, but it wasn't that, it was the structure of planning!\"</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:22]</strong>&nbsp;</p><p>I think we're getting closer to the same page now.</p><p>Let's consider this hypothetical planner for a bit. Suppose that it was trained in a way that minimised the, let's say, <i>adversarial</i> component of its plans.</p><p>For example, let's say that the plans it outputs for any situation are heavily regularised so only the broad details get through.</p><p>Hmm, I'm having a bit of trouble describing this, but basically I have an intuition that in this scenario there's a component of its plan which is cooperative with whoever executes the plan, and a component that's adversarial.</p><p>And I agree that there's no fundamental difference in type between these two things.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:27]</strong>&nbsp;</p><p>\"What if this potion we're brewing has a Good Part and a Bad Part, and we could just keep the Good Parts...\"</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:27]</strong>&nbsp;</p><p>Nor do I think they're separable. But in some cases, you might expect one to be much larger than the other.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][13:29]</strong>&nbsp;</p><p>(I observe that my model of some other listeners, at this point, protest \"there is yet a difference between the hypothetical-planner applied to actual problems, and the Big Scary Consequentialist, which is that the hypothetical planner is emitting descriptions of plans that <i>would</i> work if executed, whereas the big scary consequentialist is executing those plans directly.\")</p><p>(Not sure that's a useful point to discuss, or if it helps Richard articulate, but it's at least a place I expect some reader's minds to go if/when this is published.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:30]</strong>&nbsp;</p><p>(That is in fact a difference! The insight is in realizing that the hypothetical planner is only one line of outer shell command away from being a Big Scary Thing and is therefore also liable to be Big and Scary in many ways.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:31]</strong>&nbsp;</p><p>To me it seems that Eliezer's position is something like: \"actually, in almost no training regimes do we get agents that decide which plans to output by spending almost all of their time thinking about the object-level problem, and very little of their time thinking about how to manipulate the humans carrying out the plan\".</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:32]</strong>&nbsp;</p><p>My position is that the AI does not neatly separate its internals into a Part You Think Of As Good and a Part You Think Of As Bad, because that distinction is sharp in your map but not sharp in the territory or the AI's map.</p><p>From the perspective of a paperclip-maximizing-action-outputting-time-machine, its actions are not \"object-level making paperclips\" or \"manipulating the humans next to the time machine to deceive them about what the machine does\", they're just physical outputs that go through time and end up with paperclips.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:34]</strong>&nbsp;</p><p>@Nate, yeah, that's a nice way of phrasing one point I was trying to make. And I do agree with Eliezer that these things <i>can be</i> very similar. But I'm claiming that in some cases these things can also be quite different - for instance, when we're training agents that only get to output a short high-level description of the plan.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:35]</strong>&nbsp;</p><p>The danger is in how hard the agent has to work to come up with the plan. I can, for instance, build an agent that very safely outputs a high-level plan for saving the world:</p><p>echo \"Hey Richard, go save the world!\"</p><p>So I do have to ask what kind of \"high-level\" planning output, that saves the world, you are envisioning, and why it was hard to cognitively come up with such that we didn't just make that high-level plan right now, if humans could follow it. Then I'll look at the part where the plan was hard to come up with, and say how the agent had to understand lots of complicated things in reality and accurately navigate paths through time for those complicated things, in order to even invent the high-level plan, and hence it was very dangerous if it wasn't navigating exactly where you hoped. Or, alternatively, I'll say, \"That plan couldn't save the world: you're not postulating enough superintelligence to be dangerous, <i>and you're also</i> not using enough superintelligence to flip the tables on the currently extremely doomed world.\"</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:39]</strong>&nbsp;</p><p>At this point I'm not envisaging a particular planning output that saves the world, I'm just trying to get more clarity on the issue of consequentialism.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:40]</strong>&nbsp;</p><p>Look at the water; it's not the way you're doing the work that's dangerous, it's the work you're trying to do. What work are you trying to do, never mind how it gets done?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:41]</strong>&nbsp;</p><p>I think I agree with you that, in the limit of advanced capabilities, we can't say much about how the work is being done, we have to primarily reason from the work that we're trying to do.</p><p>But here I'm only talking about systems that are intelligent enough to come up with plans and do research that are beyond the capability of humanity.</p><p>And for me the question is: for <i>those</i> systems, can we tilt the way they do the work so they spend 99% of their time trying to solve the object-level problem, and 1% of their time trying to manipulate the humans who are going to carry out the plan? (Where these are not fundamental categories for the AI, they're just a rough categorisation that emerges after we've trained it - the same way that the categories of \"physically moving around\" and \"thinking about things\" aren't fundamentally different categories of action for humans, but the way we've evolved means there's a significant internal split between them.)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][13:43]</strong>&nbsp;</p><p>(I suspect Eliezer is not trying to make a claim of the form \"in the limit of advanced capabilities, we are relegated to reasoning about what work gets done, not about how it was done\". I suspect some miscommunication. It might be a reasonable time for Richard to attempt to paraphrase Eliezer's argument?)</p><p>(Though it also seems to me like Eliezer responding to the 99%/1% point may help shed light.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:46]</strong>&nbsp;</p><p>Well, for one thing, I'd note that a system which is designing nanosystems, and spending 1% of its time thinking about how to kill the operators, is lethal. It has to be such a small fraction of thinking that it, like, never completes the whole thought about \"well, if I did X, that would kill the operators!\"</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:46]</strong>&nbsp;</p><p>Thanks for that, Nate. I'll try to paraphrase Eliezer's argument now.</p><p>Eliezer's position (partly in my own terminology): we're going to build AIs that can perform very difficult tasks using cognition which we can roughly describe as \"searching over many options to find one that meets our criteria\". An AI that can solve these difficult tasks will need to be able to search in a very general and flexible way, and so it will be very difficult to constrain that search into a particular region.</p><p>Hmm, that felt like a very generic summary, let me try and think about the more specific claims he's making.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:54]</strong>&nbsp;</p><blockquote><p>An AI that can solve these difficult tasks will need to be able to</p></blockquote><p>Very very little is universally necessary over the design space. The <i>first</i> AGI that our tech becomes able to build is liable to work in certain easier and simpler ways.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:55]</strong>&nbsp;</p><p>Point taken; thanks for catching this misphrasing (this and previous times).</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:56]</strong>&nbsp;</p><p>Can you, in principle, build a red-car-driver that is totally incapable of driving blue cars? In principle, sure! But the first red-car-driver that gradient descent stumbles over is liable to be a blue-car-driver too.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:57]</strong>&nbsp;</p><p>Eliezer, I'm wondering how much of our disagreement is about how high the human level is here.</p><p>Or, to put it another way: we can build systems that outperform humans at quite a few tasks by now, without having search abilities that are general enough to even try to take over the world.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:58]</strong>&nbsp;</p><p>Indubitably and indeed, this is so.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][13:59]</strong>&nbsp;</p><p>Putting aside for a moment the question of which tasks are pivotal enough to save the world, which parts of your model draw the line between human-level chess players and human-level galaxy-colonisers?</p><p>And say that we'll be able to align ones that they outperform us on <i>these tasks</i> before taking over the world, but not on <i>these other tasks</i>?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][13:59][14:01]</strong>&nbsp;</p><p>That doesn't have a very simple answer, but one aspect there is <i>domain generality</i> which in turn is achieved through <i>novel domain learning</i>.</p></td></tr><tr><td>Humans, you will note, were not aggressively optimized by natural selection to be able to breathe underwater or fly into space. In terms of obvious outer criteria, there is not much outer sign that natural selection produced these creatures much more general than chimpanzees, by training on a much wider range of environments and loss functions.</td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][14:00]</strong>&nbsp;</p><p>(Before we drift too far from it: thanks for the summary! It seemed good to me, and I updated towards the miscommunication I feared not-having-happened.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:03]</strong>&nbsp;</p><blockquote><p>(Before we drift too far from it: thanks for the summary! It seemed good to me, and I updated towards the miscommunication I feared not-having-happened.)</p></blockquote><p>(Good to know, thanks for keeping an eye out. To be clear, I didn't ever interpret Eliezer as making a claim explicitly about the limit of advanced capabilities; instead it just seemed to me that he was thinking about AIs significantly more advanced than the ones I've been thinking of. I think I phrased my point poorly.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:05][14:10]</strong>&nbsp;</p><p>There are complicated aspects of this story where natural selection may metaphorically be said to have \"had no idea of what it was doing\", eg, after early rises in intelligence possibly produced by sexual selection on neatly chipped flint handaxes or whatever, all the cumulative brain-optimization on chimpanzees reached a point where there was suddenly a sharp selection gradient on relative intelligence at Machiavellian planning against other humans (even more so than in the chimp domain) as a subtask of inclusive genetic fitness, and so continuing to optimize on \"inclusive genetic fitness\" in the same old savannah, turned out to happen to be optimizing hard on the subtask and internal capability of \"outwit other humans\", which optimized hard on \"model other humans\", which was a capability that could be reused for modeling the chimp-that-is-this-chimp, which turned the system on itself and made it reflective, which contributed greatly to its intelligence being generalized, even though it was just grinding the same loss function on the same savannah; the system being optimized happened to go there in the course of being optimized even harder for the same thing.</p><p>So one can imagine asking the question: Is there a superintelligent AGI that can quickly build nanotech, which has a kind of passive safety in some if not all respects, in virtue of it solving problems like \"build a nanotech system which does X\" the way that a beaver solves building dams, in virtue of having a bunch of specialized learning abilities without it ever having a cross-domain general learning ability?</p><p>And in this regard one does note that there are many, many, many things that humans do which no other animal does, which you might think would contribute a lot to that animal's fitness if there were animalistic ways to do it. They don't make iron claws for themselves. They never did evolve a tendency to search for iron ore, and burn wood into charcoal that could be used in hardened-clay furnaces.</p><p>No animal plays chess, but AIs do, so we can obviously make AIs to do things that animals don't do. On the other hand, the environment didn't exactly present any particular species with a challenge of chess-playing either.</p></td></tr><tr><td>Even so, though, even if some animal had evolved to play chess, I fully expect that current AI systems would be able to squish it at chess, because the AI systems are on chips that run faster than neurons and doing crisp calculations and there are things you just can't do with noisy slow neurons. So that again is not a generally reliable argument about what AIs can do.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:09][14:11]</strong>&nbsp;</p><p>Yes, although I note that challenges which are trivial from a human-engineering perspective can be very challenging from an evolutionary perspective (e.g. spinning wheels).</p></td></tr><tr><td>And so the evolution of animals-with-a-little-bit-of-help-from-humans might end up in very different places from the evolution of animals-just-by-themselves. And analogously, the ability of humans to fill in the gaps to help less general AIs achieve more might be quite significant.</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:11]</strong>&nbsp;</p><p>So we can again ask: Is there a way to make an AI system that is <i>only</i> good at designing nanosystems, which can achieve some complicated but hopefully-specifiable real-world outcomes, without that AI also being superhuman at understanding and manipulating humans?</p><p>And I roughly answer, \"Perhaps, but not by default, there's a bunch of subproblems, I don't actually know how to do it right now, it's not <i>the easiest</i> way to get an AGI that can build nanotech (and kill you), you've got to make the red-car-driver specifically not be able to drive blue cars.\" Can I explain how I know that? I'm really not sure I can, in real life where I explain X0 and then the listener doesn't generalize X0 to X and respecialize it to X1.</p><p>It's like asking me how I could possibly know in 2008, before anybody had observed AlphaFold 2, that superintelligences would be able to crack the protein folding problem on the way to nanotech, which some people did question back in 2008.</p><p>Though that was admittedly more of a slam-dunk than this was, and I could not have told you that AlphaFold 2 would become possible at a prehuman level of general intelligence in 2021 specifically, or that it would be synced in time to a couple of years after GPT-2's level of generality at text.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:18]</strong>&nbsp;</p><p>What are the most relevant axes of difference between solving protein folding and designing nanotech that, say, self-assembles into a computer?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:20]</strong>&nbsp;</p><p>Definitely, \"turns out it's easier than you thought to use gradient descent's memorization of zillions of shallow patterns that overlap and recombine into larger cognitive structures, to add up to a consequentialist nanoengineer that only does nanosystems and never does sufficiently general learning to apprehend the big picture containing humans, while still understanding the goal for that pivotal act you wanted to do\" is among the more plausible advance-specified miracles we could get.</p><p>But it is not what my model says actually happens, and I am not a believer that when your model says you are going to die, you get to start believing in particular miracles. You need to hold your mind open for any miracle and a miracle you didn't expect or think of in advance, because at this point our last hope is that in fact the future is often quite surprising - though, alas, negative surprises are a tad more frequent than positive ones, when you are trying desperately to navigate using a bad map.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:22]</strong>&nbsp;</p><p>Perhaps one metric we could use here is something like: how much extra reward does the consequentialist nanoengineer get from starting to model humans, versus from becoming better at nanoengineering?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:23]</strong>&nbsp;</p><p>But that's <i>not</i> where humans came from. We didn't get to nuclear power by getting a bunch of fitness from nuclear power plants. We got to nuclear power because if you get a bunch of fitness from chipping flint handaxes and Machiavellian scheming, as found by relatively simple and local hill-climbing, that entrains the same genes that build nuclear power plants.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:24]</strong>&nbsp;</p><p>Only in the specific case where you also have the constraint that you keep having to learn new goals every generation.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:24]</strong>&nbsp;</p><p>Huh???</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][14:24]</strong>&nbsp;</p><p>(I think Richard's saying, \"that's a consequence of the genetic bottleneck\")</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:25]</strong>&nbsp;</p><p>Right.</p><p>Hmm, but I feel like we may have covered this ground before.</p><p>Suggestion: I have a couple of other directions I'd like to poke at, and then we could wrap up in 20 or 30 minutes?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:27]</strong>&nbsp;</p><p>OK</p><blockquote><p>What are the most relevant axes of difference between solving protein folding and designing nanotech that, say, self-assembles into a computer?</p></blockquote><p>Though I want to mark that this question seemed potentially cruxy to me, though perhaps not for others. I.e., if building protein factories that built nanofactories that built nanomachines that met a certain deep and lofty engineering goal, didn't involve cognitive challenges different in kind from protein folding, we could maybe just safely go do that using AlphaFold 3, which would be just as safe as AlphaFold 2.</p><p>I don't think we can do that. And I would note to the generic Other that if, to them, these both just sound like thinky things, so why can't you just do that other thinky thing too using the thinky program, this is a case where having any specific model of why we don't already have this nanoengineer right now would tell you there were specific different thinky things involved.</p></td></tr></tbody></table></figure><p>&nbsp;</p><h2>3.4. Coherence and pivotal acts</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:31]</strong>&nbsp;</p><p>In either order:</p><ul><li>I'm curious how the things we've been talking about relate to your opinions about meta-level optimisation from the AI foom debate. (I.e. talking about how wrapping around so that there's no longer any protected level of optimisation leads to dramatic change.)</li><li>I'm curious how your claims about the \"robustness\" of consequentialism (i.e. the difficulty of channeling an agent's thinking in the directions we want it to go) relate to the reliance of humans on culture, and in particular the way in which humans raised without culture are such bad consequentialists.</li></ul><p>On the first: if I were to simplify to the extreme, it seems like there are these two core intuitions that you've been trying to share for a long time. One is a certain type of recursive improvement, and another is a certain type of consequentialism.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:32]</strong>&nbsp;</p><p>The second question didn't make much sense in my native ontology? Humans raised without culture don't have access to environmental constants whose presence their genes assume, so they end up as broken machines and then they're bad consequentialists.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:35]</strong>&nbsp;</p><p>Hmm, good point. Okay, question modification: the ways in which humans reason, act, etc, vary greatly depending on which cultures they're raised in. (I'm mostly thinking about differences over time - e.g. cavemen vs moderns.) My low-fidelity version of your view about consequentialists says that general consequentialists like humans possess a robust search process which isn't so easily modified.</p><p>(Sorry if this doesn't make much sense in your ontology, I'm getting a bit tired.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:36]</strong>&nbsp;</p><p>What is it that varies that you think I think should predict would stay more constant?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:37]</strong>&nbsp;</p><p>Goals, styles of reasoning, deontological constraints, level of conformity.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:39]</strong>&nbsp;</p><p>With regards to your first point, my first reaction was, \"I just have one view of intelligence, what you see me arguing about reflects which points people have proved weirdly obstinate about. In 2008, Robin Hanson was being weirdly obstinate about how capabilities scaled and whether there was even any point in analyzing AIs differently from ems, so I talked about what I saw as the most slam-dunk case for there being Plenty Of Room Above Biology and for stuff going whoosh once it got above the human level.</p><p>\"It later turned out that capabilities started scaling a whole lot <i>without</i> self-improvement, which is an example of the kind of weird surprise the Future throws at you, and maybe a case where I missed something by arguing with Hanson instead of imagining how I could be wrong in either direction and not just the direction that other people wanted to argue with me about.</p><p>\"Later on, people were unable to understand why alignment is hard, and got stuck on generalizing the concept I refer to as consequentialism. A theory of why I talked about both things for related reasons would just be a theory of why people got stuck on these two points for related reasons, and I think that theory would mainly be overexplaining an accident because if Yann LeCun had been running effective altruism I would have been explaining different things instead, after the people who talked a lot to EAs got stuck on a different point.\"</p><p>Returning to your second point, humans are broken things; if it were possible to build computers while working even worse than humans, we'd be having this conversation at that level of intelligence instead.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:41]</strong>&nbsp;</p><p>(Retracted)<s>I entirely agree about humans, but it doesn't matter that much how broken humans are when the regime of AIs that we're talking about is the regime that's directly above humans, and therefore only a bit less broken than humans.</s></p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:41]</strong>&nbsp;</p><p>Among the things to bear in mind about that, is that we then get tons of weird phenomena that are specific to humans, and you may be very out of luck if you start wishing for the <i>same</i> weird phenomena in AIs. Yes, even if you make some sort of attempt to train it using a loss function.</p><p>However, it does seem to me like as we start getting towards the Einstein level instead of the village-idiot level, even though this is usually not much of a difference, we do start to see the atmosphere start to thin already, and the turbulence start to settle down already. Von Neumann was actually a fairly reflective fellow who knew about, and indeed helped generalize, utility functions. The great achievements of von Neumann were not achieved by some very specialized hypernerd who spent all his fluid intelligence on crystallizing math and science and engineering alone, and so never developed any opinions about politics or started thinking about whether or not he had a utility function.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:44]</strong>&nbsp;</p><p>I don't think I'm asking for the <i>same</i> weird phenomena. But insofar as a bunch of the phenomena I've been talking about have seemed weird according to your account of consequentialism, then the fact that approximately-human-level-consequentialists have lots of weird things about them is a sign that the phenomena I've been talking about are less unlikely than you expect.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:45][14:46]</strong>&nbsp;</p><p>I suspect that some of the difference here is that I think you have to be <i>noticeably</i> better than a human at nanoengineering to pull off pivotal acts large enough to make a difference, which is why I am not instead trying to gather the smartest people left alive and doing that pivotal act directly.</p></td></tr><tr><td>I can't think of anything you can do with somebody just barely smarter than a human, which flips the gameboard, aside of course from \"go build a Friendly AI\" which I <i>did</i> try to set up to just go do and which would be incredibly hard to align if we wanted an AI to do it instead (full-blown chicken-and-egg, that AI is already fully aligned).</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:45]</strong>&nbsp;</p><p>Oh, interesting. Actually one more question then: to what extent do you think that explicitly reasoning about utility functions and laws of rationality is what makes consequentialists have the properties you've been talking about?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:47, moved up in log]</strong>&nbsp;</p><p>Explicit reflection is one possible later stage of the path; an earlier part of the path is from being optimized to do things difficult enough that you need to stop stepping on your own feet and have different parts of your thoughts work well together.</p><p>It's the sort of path that has only one destination at its end, so there will be many ways to get there.</p><p>(Modulo various cases where different decision theories seem reflectively consistent and so on; I want to say \"you know what I mean\" but maybe people don't.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:47, moved down in log]</strong>&nbsp;</p><blockquote><p>I suspect that some of the difference here is that I think you have to be <i>noticeably</i> better than a human at nanoengineering to pull off pivotal acts large enough to make a difference, which is why I am not instead trying to gather the smartest people left alive and doing that pivotal act directly.</p></blockquote><p>Yepp, I think there's probably some disagreements about geopolitics driving this too. E.g. in my earlier summary document I mentioned some possible pivotal acts:</p><ul><li>Monitoring all potential AGI projects to an extent that makes it plausible for the US and China to work on a joint project without worrying that the other is privately racing.</li><li>Provide arguments/demonstrations/proofs related to impending existential risk that are sufficiently compelling to scare the key global decision-makers into bottlenecking progress.</li></ul><p>I predict that you think these would not be pivotal enough; but I don't think digging into the geopolitical side of things is the best use of our time.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:49, moved up in log]</strong>&nbsp;</p><p>Monitoring all AGI projects - either not politically feasible in real life given the actual way that countries behave in history books instead of fantasy; or at politically feasible levels, does not work well enough to prevent the world from ending once the know-how proliferates. The AI isn't doing much work here either; why not go do this now, if it's possible? (Note: please don't try to go do this now, it backfires badly.)</p><p>Provide sufficiently compelling arguments = superhuman manipulation, an incredibly dangerous domain that is just about the worst domain to try to align.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:49, moved down in log]</strong>&nbsp;</p><blockquote><p>With regards to your first point, my first reaction was, \"I just have one view of intelligence, what you see me arguing about reflects which points people have proved weirdly obstinate about. In 2008, Robin Hanson was being weirdly obstinate about how capabilities scaled and whether there was even any point in analyzing AIs differently from ems, so I talked about what I saw as the most slam-dunk case for there being Plenty Of Room Above Biology and for stuff going whoosh once it got above the human level.</p><p>\"It later turned out that capabilities started scaling a whole lot <i>without</i> self-improvement, which is an example of the kind of weird surprise the Future throws at you, and maybe a case where I missed something by arguing with Hanson instead of imagining how I could be wrong in either direction and not just the direction that other people wanted to argue with me about.</p><p>\"Later on, people were unable to understand why alignment is hard, and got stuck on generalizing the concept I refer to as consequentialism. A theory of why I talked about both things for related reasons would just be a theory of why people got stuck on these two points for related reasons, and I think that theory would mainly be overexplaining an accident because if Yann LeCun had been running effective altruism I would have been explaining different things instead, after the people who talked a lot to EAs got stuck on a different point.\"</p></blockquote><p>On my first point, it seems to me that your claims about recursive self-improvement were off in a fairly similar way to how I think your claims about consequentialism are off - which is that they defer too much to one very high-level abstraction.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:52]</strong>&nbsp;</p><blockquote><p>On my first point, it seems to me that your claims about recursive self-improvement were off in a fairly similar way to how I think your claims about consequentialism are off - which is that they defer too much to one very high-level abstraction.</p></blockquote><p>I suppose that is what it could potentially feel like from the inside to not get an abstraction. Robin Hanson kept on asking why I was trusting my abstractions so much, when he was in the process of trusting his worse abstractions instead.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][14:51][14:53]</strong>&nbsp;</p><blockquote><p>Explicit reflection is one possible later stage of the path; an earlier part of the path is from being optimized to do things difficult enough that you need to stop stepping on your own feet and have different parts of your thoughts work well together.</p></blockquote><p>Can you explain a little more what you mean by \"have different parts of your thoughts work well together\"? Is this something like the capacity for metacognition; or the global workspace; or self-control; or...?</p></td></tr><tr><td>And I guess there's no good way to quantify <i>how</i> important you think the explicit reflection part of the path is, compared with other parts of the path - but any rough indication of whether it's a more or less crucial component of your view?</td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][14:55]</strong>&nbsp;</p><blockquote><p>Can you explain a little more what you mean by \"have different parts of your thoughts work well together\"? Is this something like the capacity for metacognition; or the global workspace; or self-control; or...?</p></blockquote><p>No, it's like when you don't, like, pay five apples for something on Monday, sell it for two oranges on Tuesday, and then trade an orange for an apple.</p><p>I have still not figured out the homework exercises to convey to somebody the Word of Power which is \"coherence\" by which they will be able to look at the water, and see \"coherence\" in places like a cat walking across the room without tripping over itself.</p><p>When you do lots of reasoning about arithmetic correctly, without making a misstep, that long chain of thoughts with many different pieces diverging and ultimately converging, ends up making some statement that is... still true and still about numbers! Wow! How do so many different thoughts add up to having this property? Wouldn't they wander off and end up being about tribal politics instead, like on the Internet?</p><p>And one way you could look at this, is that even though all these thoughts are taking place in a bounded mind, they are shadows of a higher unbounded structure which is the model identified by the Peano axioms; all the things being said are <i>true about the numbers</i>. Even though somebody who was missing the point would at once object that the human contained no mechanism to evaluate each of their statements against all of the numbers, so obviously no human could ever contain a mechanism like that, so obviously you can't explain their success by saying that each of their statements was true about the same topic of the numbers, because what could possibly implement that mechanism which (in the person's narrow imagination) is The One Way to implement that structure, which humans don't have?</p><p>But though mathematical reasoning can sometimes go astray, when it works at all, it works because, in fact, even bounded creatures can sometimes manage to obey local relations that in turn add up to a global coherence where all the pieces of reasoning point in the same direction, like photons in a laser lasing, even though there's no internal mechanism that enforces the global coherence at every point.</p><p>To the extent that the outer optimizer trains you out of paying five apples on Monday for something that you trade for two oranges on Tuesday and then trading two oranges for four apples, the outer optimizer is training all the little pieces of yourself to be locally coherent in a way that can be seen as an imperfect bounded shadow of a higher unbounded structure, and then the system is powerful though imperfect <i>because</i> of how the power is present in the coherence and the overlap of the pieces, <i>because</i> of how the higher perfect structure is being imperfectly shadowed. In this case the higher structure I'm talking about is Utility, and doing homework with coherence theorems leads you to appreciate that we only know about one higher structure for this class of problems that has a dozen mathematical spotlights pointing at it saying \"look here\", even though people have occasionally looked for alternatives.</p><p>And when I try to say this, people are like, \"Well, I looked up a theorem, and it talked about being able to identify a unique utility function from an infinite number of choices, but if we don't have an infinite number of choices, we can't identify the utility function, so what relevance does this have\" and this is a kind of mistake I don't remember even coming close to making so I do not know how to make people stop doing that and maybe I can't.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][15:07]</strong>&nbsp;</p><p>We're already pushing our luck on time, so I nominate that we wrap up (after, perhaps, a few more Richard responses if he's got juice left.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:07]</strong>&nbsp;</p><p>Yeah, was thinking the same.</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][15:07]</strong>&nbsp;</p><p>As a proposed cliffhanger to feed into the next discussion, my take is that Richard's comment:</p><blockquote><p>On my first point, it seems to me that your claims about recursive self-improvement were off in a fairly similar way to how I think your claims about consequentialism are off - which is that they defer too much to one very high-level abstraction.</p></blockquote><p>probably contains some juicy part of the disagreement, and I'm interested in Eliezer understanding Richard's claim to the point of being able to paraphrase it to Richard's satisfaction.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:08]</strong>&nbsp;</p><p>Wrapping up here makes sense.</p><p>I endorse the thing Nate just said.</p><p>I also get the sense that I have a much better outline now of Eliezer's views about consequentialism (if not the actual details and texture).</p><p>On a meta level, I personally tend to focus more on things like \"how should we understand cognition\" and not \"how should we understand geopolitics and how it affects the level of pivotal action required\".</p><p>If someone else were trying to prosecute this disagreement they might say much more about the latter. I'm uncertain how useful it is for me to do so, given that my comparative advantage compared with the rest of the world (and probably Eliezer's too) is the cognition part.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:12]</strong>&nbsp;</p><p>Reconvene... tomorrow? Monday of next week?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:12]</strong>&nbsp;</p><p>Monday would work better for me.</p><p>You okay with me summarising the discussion so far to [some people \u2014 redacted for privacy reasons]?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:13]</strong>&nbsp;</p><p>Nate, take a minute to think of your own thoughts there?</p><figure class=\"table\"><table><tbody><tr><td>[Soares: \ud83d\udc4d \ud83d\udc4c]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][15:15]</strong>&nbsp;</p><p>My take: I think it's fine to summarize, though generally virtuous to mark summaries as summaries (rather than asserting that your summaries are Eliezer-endorsed or w/e).</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: \ud83d\udc4d]</td></tr></tbody></table></figure></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:16]</strong>&nbsp;</p><p>I think that broadly matches my take. I'm also a bit worried about biases in the text summarizer, and about whether I managed to say anything that Rob or somebody will object to pre-publication, but we ultimately intended this to be seen and I was keeping that in mind, so, yeah, go ahead and summarize.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:17]</strong>&nbsp;</p><p>Great, thanks</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:17]</strong>&nbsp;</p><p>I admit to being curious as to what you thought was said that was important or new, but that's a question that can be left open to be answered at your leisure, earlier in your day.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:17]</strong>&nbsp;</p><blockquote><p>I admit to being curious as to what you thought was said that was important or new, but that's a question that can be left open to be answered at your leisure, earlier in your day.</p></blockquote><p>You mean, what I thought was worth summarising?</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:17]</strong>&nbsp;</p><p>Yeah.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:18]</strong>&nbsp;</p><p>Hmm, no particular opinion. I wasn't going to go out of my way to do so, but since I'm chatting to [some people \u2014 redacted for privacy reasons] regularly anyway, it seemed low-cost to fill them in.</p><p>At your leisure, I'd be curious to know how well the directions of discussion are meeting your goals for what you want to convey when this is published, and whether there are topics you want to focus on more.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:19]</strong>&nbsp;</p><p>I don't know if it's going to help, but trying it currently seems better than to go on saying nothing.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:20]</strong>&nbsp;</p><p>(personally, in addition to feeling like less of an expert on geopolitics, it also seems more sensitive for me to make claims about in public, which is another reason I haven't been digging into that area as much)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][15:21]</strong>&nbsp;</p><blockquote><p>(personally, in addition to feeling like less of an expert on geopolitics, it also seems more sensitive for me to make claims about in public, which is another reason I haven't been digging into that area as much)</p></blockquote><p>(seems reasonable! note, though, that i'd be quite happy to have sensitive sections stricken from the record, insofar as that lets us get more convergence than we otherwise would, while we're already in the area)</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: \ud83d\udc4d]</td></tr></tbody></table></figure><p>(tho ofc it is less valuable to spend conversational effort in private discussions, etc.)</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: \ud83d\udc4d]</td></tr></tbody></table></figure></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:22]</strong>&nbsp;</p><blockquote><p>At your leisure, I'd be curious to know how well the directions of discussion are meeting your goals for what you want to convey when this is published, and whether there are topics you want to focus on more.</p></blockquote><p>(this question aimed at you too Nate)</p><p>Also, thanks Nate for the moderation! I found your interventions well-timed and useful.</p><figure class=\"table\"><table><tbody><tr><td>[Soares: \u2764\ufe0f]</td></tr></tbody></table></figure></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][15:23]</strong>&nbsp;</p><blockquote><p>(this question aimed at you too Nate)</p></blockquote><p>(noted, thanks, I'll probably write something up after you've had the opportunity to depart for sleep.)</p><p>On that note, I declare us adjourned, with intent to reconvene at the same time on Monday.</p><p>Thanks again, both.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:23]</strong>&nbsp;</p><p>Thanks both \ud83d\ude42</p><p>Oh, actually, one quick point</p><p>Would one hour earlier suit, for Monday?</p><p>I've realised that I'll be moving to a one-hour-later time zone, and starting at 9pm is slightly suboptimal (but still possible if necessary)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][15:24]</strong>&nbsp;</p><p>One hour earlier would work fine for me.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:25]</strong>&nbsp;</p><p>Doesn't work as fine for me because I've been trying to avoid any food until 12:30p my time, but on that particular day I may be more caloried than usual from the previous day, and could possibly get away with it. (That whole day could also potentially fail if a minor medical procedure turns out to take more recovery than it did the last time I had it.)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:26]</strong>&nbsp;</p><p>Hmm, is this something where you'd have more information on the day? (For the calories thing)</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:27]</strong>&nbsp;</p><blockquote><p>(seems reasonable! note, though, that i'd be quite happy to have sensitive sections stricken from the record, insofar as that lets us get more convergence than we otherwise would, while we're already in the area)</p></blockquote><p>I'm a touch reluctant to have discussions that we intend to delete, because then the larger debate will make less sense once those sections are deleted. Let's dance around things if we can.</p><figure class=\"table\"><table><tbody><tr><td>[Ngo: \ud83d\udc4d]</td><td>[Soares: \ud83d\udc4d]</td></tr></tbody></table></figure><p>I mean, I can that day at 10am my time say how I am doing and whether I'm in shape for that day.</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][15:28]</strong>&nbsp;</p><p>great. and if at that point it seems net positive to postpone to 11am your time (at the cost of me being a bit less coherent later on) then feel free to say so at the time</p><p>on that note, I'm off</p></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Yudkowsky][15:29]</strong>&nbsp;</p><p>Good night, heroic debater!</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][16:11]</strong>&nbsp;</p><blockquote><p>At your leisure, I'd be curious to know how well the directions of discussion are meeting your goals for what you want to convey when this is published, and whether there are topics you want to focus on more.</p></blockquote><p>The discussions so far are meeting my goals quite well so far! (Slightly better than my expectations, hooray.) Some quick rough notes:</p><ul><li>I have been enjoying EY explicating his models around consequentialism.<ul><li>The objections Richard has been making are ones I think have been floating around for some time, and I'm quite happy to see explicit discussion on it.</li><li>Also, I've been appreciating the conversational virtue with which the two of you have been exploring it. (Assumption of good intent, charity, curiosity, etc.)</li></ul></li><li>I'm excited to dig into Richard's sense that EY was off about recursive self improvement, and is now off about consequentialism, in a similar way.<ul><li>This also sees to me like a critique that's been floating around for some time, and I'm looking forward to getting more clarity on it.</li></ul></li><li>I'm a bit torn between driving towards clarity on the latter point, and shoring up some of the progress on the former point.<ul><li>One artifact I'd really enjoy having is some sort of \"before and after\" take, from Richard, contrasting his model of EY's views before, to his model now.</li><li>I also have a vague sense that there are some points Eliezer was trying to make, that didn't quite feel like they were driven home; and dually, some pushback by Richard that didn't feel quite frontally answered.<ul><li>One thing I may do over the next few days is make a list of those places, and see if I can do any distilling on my own. (No promises, though.)</li><li>If that goes well, I might enjoy some side-channel back-and-forth with Richard about it, eg during some more convenient-for-Richard hour (or, eg, as a thing to do on Monday if EY's not in commission at 10a pacific.)</li></ul></li></ul></li></ul></td></tr><tr><td style=\"vertical-align:top\"><p><strong>[Ngo][5:40] &nbsp;(next day, Sep. 9)</strong>&nbsp;</p><blockquote><p>The discussions so far are [...]</p></blockquote><p>What do you mean by \"latter point\" and \"former point\"? (In your 6th bullet point)</p></td></tr><tr><td style=\"background-color:hsl(0, 0%, 90%);vertical-align:top\"><p><strong>[Soares][7:09] &nbsp;(next day, Sep. 9)</strong>&nbsp;</p><blockquote><p>What do you mean by \"latter point\" and \"former point\"? (In your 6th bullet point)</p></blockquote><p>former = shoring up the consequentialism stuff, latter = digging into your critique re: recursive self improvement etc. (The nesting of the bullets was supposed to help make that clear, but didn't come out well in this format, oops.)</p></td></tr></tbody></table></figure><p>&nbsp;</p><h1>4. Follow-ups</h1><p>&nbsp;</p><h2>4.1. Richard Ngo's summary</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p><i>2nd discussion</i></p><p>(Mostly summaries not quotations<s>; also hasn\u2019t yet been evaluated by Eliezer</s>)</p><p>Eliezer, summarized by Richard: \"<s>The</s> A core concept which people have trouble grasping is consequentialism. People try to reason about <i>how</i> AIs will solve problems, and ways in which they might or might not be dangerous. But they don\u2019t realise that the ability to solve a wide range of difficult problems implies that an agent must be doing a powerful search over possible solutions, which is <s>the</s> a core skill required to take actions which greatly affect the world. Making this type of AI safe is like trying to build an AI that drives red cars very well, but can\u2019t drive blue cars - there\u2019s no way you get this by default, because the skills involved are so similar. And because the search process <s>is so general</s> is by default so general, <s>it\u2019ll be very hard to</s> I don\u2019t currently see how to constrain it into any particular region.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:48] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>The</p></blockquote><p><i>A</i> concept, which some people have had trouble grasping. &nbsp;There seems to be an endless list. &nbsp;I didn't have to spend much time contemplating consequentialism to derive the consequences. &nbsp;I didn't spend a lot of time talking about it until people started arguing.</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:50] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>the</p></blockquote><p>a</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:52] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>[the search process] is [so general]</p></blockquote><p>\"is by default\". &nbsp;The reason I keep emphasizing that things are only true by default is that the work of surviving may look like doing hard nondefault things. &nbsp;I don't take fatalistic \"will happen\" stances, I assess difficulties of getting nondefault results.</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:52] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>it\u2019ll be very hard to</p></blockquote><p>\"I don't currently see how to\"</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Eliezer, summarized by Richard (continued): \"In biological organisms, evolution is <s>one source</s> the ultimate source of consequentialism. A <s>second</s> secondary outcome of evolution is reinforcement learning. For an animal like a cat, upon catching a mouse (or failing to do so) many parts of its brain get slightly updated, in a loop that makes it more likely to catch the mouse next time. (Note, however, that this process isn\u2019t powerful enough to make the cat a pure consequentialist - rather, it has many individual traits that, when we view them from this lens, point in the same direction.) <s>A third thing that makes humans in particular consequentialist is planning,</s> Another outcome of evolution, which helps make humans in particular more consequentialist, is planning - especially when we\u2019re aware of concepts like utility functions.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:53] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>one</p></blockquote><p>the ultimate</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:53] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>second</p></blockquote><p>secondary outcome of evolution</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:55] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>especially when we\u2019re aware of concepts like utility functions</p></blockquote><p>Very slight effect on human effectiveness in almost all cases because humans have very poor reflectivity.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Consider an AI that, given a hypothetical scenario, tells us what the best plan to achieve a certain goal in that scenario is. Of course it needs to do consequentialist reasoning to figure out how to achieve the goal. But that\u2019s different from an AI which chooses what to say as a means of achieving its goals. I\u2019d argue that the former is doing consequentialist reasoning without itself being a consequentialist, while the latter is actually a consequentialist. Or more succinctly: consequentialism = problem-solving skills + using those skills to choose actions which achieve goals.\"</p><p>Eliezer, summarized by Richard: \"The former AI might be slightly safer than the latter if you could build it, but I think people are likely to dramatically overestimate how big the effect is. The difference could just be one line of code: if we give the former AI our current scenario as its input, then it becomes the latter.&nbsp; For purposes of understanding alignment difficulty, you want to be thinking on the level of abstraction where you see that in some sense it is the search itself that is dangerous when it's a strong enough search, rather than the danger seeming to come from details of the planning process. One particularly helpful thought experiment is to think of advanced AI as an '<a href=\"https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes\"><u>outcome pump</u></a>' which selects from futures in which a certain outcome occurred, and takes whatever action leads to them.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][10:59] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>particularly helpful</p></blockquote><p>\"attempted explanatory\". &nbsp;I don't think most readers got it.</p><p>I'm a little puzzled by how often you write my viewpoint as thinking that whatever I happened to say a sentence about is the Key Thing. &nbsp;It seems to rhyme with a deeper failure of many EAs to pass the MIRI <a href=\"https://www.econlib.org/archives/2011/06/the_ideological.html\">ITT</a>.</p><p>To be a bit blunt and impolite in hopes that long-languishing social processes ever get anywhere, two obvious uncharitable explanations for why some folks may systematically misconstrue MIRI/Eliezer as believing much more than in reality that various concepts an argument wanders over are Big Ideas to us, when some conversation forces us to go to that place:</p><p>(A)&nbsp; It paints a comfortably unflattering picture of MIRI-the-Other as weirdly obsessed with these concepts that seem not so persuasive, or more generally paints the Other as a bunch of weirdos who stumbled across some concept like \"consequentialism\" and got obsessed with it.&nbsp; In general, to depict the Other as thinking a great deal of some idea (or explanatory thought experiment) is to tie and stake their status to the listener's view of how much status that idea deserves.&nbsp; So if you say that the Other thinks a great deal of some idea that isn't obviously high-status, that lowers the Other's status, which can be a comfortable thing to do.</p><p>(cont.)</p><p>(B) It paints a more comfortably self-flattering picture of a continuing or persistent disagreement, as a disagreement with somebody who thinks that some random concept is much higher-status than it really is, in which case there isn't more to done or understood except to duly politely let the other person try to persuade you the concept deserves its high status. As opposed to, \"huh, maybe there is a noncentral point that the other person sees themselves as being stopped on and forced to explain to me\", which is a much less self-flattering viewpoint on why the conversation is staying within a place.&nbsp; And correspondingly more of a viewpoint that somebody else is likely to have of us, because it is a comfortable view to them, than a viewpoint that it is comfortable to us to imagine them having.</p><p>Taking the viewpoint that somebody else is getting hung up on a relatively noncentral point can also be a flattering self-portrait to somebody who believes that, of course.&nbsp; It doesn't mean they're right.&nbsp; But it does mean that you should be aware of how the Other's story, told from the Other's viewpoint, is much more liable to be something that the Other finds sensible and perhaps comfortable, even if it implies an unflattering (and untrue-seeming and perhaps untrue) view of yourself, than something that makes the Other seem weird and silly and which it is easy and congruent for you yourself to imagine the Other thinking.</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Ngo][11:18] &nbsp;(Sep. 12 comment)</strong>&nbsp;</p><blockquote><p>I'm a little puzzled by how often you write my viewpoint as thinking that whatever I happened to say a sentence about is the Key Thing.</p></blockquote><p>In this case, I emphasised the outcome pump thought experiment because you said that the time-travelling scenario was a key moment for your understanding of optimisation, and the outcome pump seemed to be similar&nbsp;enough and easier to convey in the summary, since you'd already written about it.</p><p>I'm also emphasising consequentialism because it seemed like the core idea which kept coming up in our first debate, under the heading of \"deep problem-solving patterns\". Although I take your earlier point that you tend to emphasise things that your interlocutor is more skeptical about, not necessarily the things which are most central to your view. But if consequentialism isn't in fact a very central concept for you, I'd be interested to hear what role it plays.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"There\u2019s a component of 'finding a plan which achieves a certain outcome' which involves actually solving the object-level problem of how someone who is given the plan can achieve the outcome. And there\u2019s another component which is figuring out how to manipulate that person into doing what you want. To me it seems like Eliezer\u2019s argument is that there\u2019s no training regime which leads an AI to spend 99% of its time thinking about the former, and 1% thinking about the latter.\"</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][11:20] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>no training regime</p></blockquote><p>...that the training regimes we come up with first, in the 3 months or 2 years we have before somebody else destroys the world, will not have this property.</p><p>I don't have any particularly complicated or amazingly insightful theories of why I keep getting depicted as a fatalist; but my world is full of counterfactual functions, not constants.&nbsp; And I am always aware that if we had access to a real Textbook from the Future explaining all of the methods that are actually robust in real life - the equivalent of telling us in advance about all the ReLUs that in real life were only invented and understood a few decades after sigmoids - we could go right ahead and build a superintelligence that thinks 2 + 2 = 5.</p><p>All of my assumptions about \"I don't see how to do X\" are always labeled as ignorance on my part and a default because we won't have enough time to actually figure out how to do X.&nbsp; I am constantly maintaining awareness of this because being <strong>wrong</strong> about it being difficult is a major place where <strong>hope</strong> potentially comes from, if there's some idea like ReLUs that robustly vanquishes the difficulty, which I just didn't think of.&nbsp; Which does not, alas, mean that I am wrong about any particular thing, nor that the infinite source of optimistic ideas that is the wider field of \"AI alignment\" is going to produce a good idea from the same process that generates all the previous naive optimism through not seeing where the original difficulty comes from or what other difficulties surround obvious naive attempts to solve it.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard (continued): \"While this may be true in the limit of increasing intelligence, the most relevant systems are the earliest ones that are above human level. But humans deviate from the consequentialist abstraction you\u2019re talking about in all sorts of ways - for example, being raised in different cultures can make people much more or less consequentialist. So it seems plausible that early AGIs can be superhuman while also deviating strongly from this abstraction - not necessarily in the same ways as humans, but in ways that we push them towards during training.\"</p><p>Eliezer, summarized by Richard: \"Even at the Einstein or von Neumann level these types of deviations start to subside. And the sort of pivotal acts which might realistically work require skills <i>significantly</i> above human level. I think even 1% of the cognition of an AI that can assemble advanced nanotech, thinking about how to kill humans, would doom us. Your other suggestions for pivotal acts (surveillance to restrict AGI proliferation; persuading world leaders to restrict AI development) are not politically feasible in real life, to the level required to prevent the world from ending; or else require alignment in the very dangerous domain of superhuman manipulation.\"</p><p>Richard, summarized by Richard: \"I think we probably also have significant disagreements about geopolitics which affect which acts we expect to be pivotal, but it seems like our comparative advantage is in discussing cognition, so let\u2019s focus on that. We can build systems that outperform humans at quite a few tasks by now, without them needing search abilities that are general enough to even try to take over the world. Putting aside for a moment the question of which tasks are pivotal enough to save the world, which parts of your model draw the line between human-level chess players and human-level galaxy-colonisers, and say that we'll be able to align ones that significantly outperform us on <i>these</i> tasks before they take over the world, but not on <i>those</i> tasks?\"</p><p>Eliezer, summarized by Richard: \"One aspect there is domain generality which in turn is achieved through novel domain learning. One can imagine asking the question: is there a superintelligent AGI that can quickly build nanotech the way that a beaver solves building dams, in virtue of having a bunch of specialized learning abilities without it ever having a cross-domain general learning ability? But there are many, many, many things that humans do which no other animal does, which you might think would contribute a lot to that animal's fitness if there were animalistic ways to do it - e.g. mining and smelting iron. (Although comparisons to animals are not generally reliable arguments about what AIs can do - e.g. chess is much easier for chips than neurons.) So my answer is 'Perhaps, but not by default, there's a bunch of subproblems, I don't actually know how to do it right now, it's not the easiest way to get an AGI that can build nanotech.' <s>Can I explain how I know that? I'm really not sure I can.</s>\"</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][11:26] &nbsp;(Sep. 10 comment)</strong>&nbsp;</p><blockquote><p>Can I explain how I know that? I'm really not sure I can.</p></blockquote><p>In original text, this sentence was followed by a long attempt to explain anyways; if deleting that, which is plausibly the correct choice, this lead-in sentence should also be deleted, as otherwise it paints a false picture of how much I would try to explain anyways.</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Ngo][11:15] &nbsp;(Sep. 12 comment)</strong>&nbsp;</p><p>Makes sense; deleted.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Ngo] &nbsp;(Sep. 10 Google Doc)</strong>&nbsp;</p><p>Richard, summarized by Richard: \"Challenges which are trivial from a human-engineering perspective can be very challenging from an evolutionary perspective (e.g. spinning wheels). So the evolution of animals-with-a-little-bit-of-help-from-humans might end up in very different places from the evolution of animals-just-by-themselves. And analogously, the ability of humans to fill in the gaps to help less general AIs achieve more might be quite significant.</p><p>\"On nanotech: what are the most relevant axes of difference between solving protein folding and designing nanotech that, say, self-assembles into a computer?\"</p><p>Eliezer, summarized by Richard: \"This question seemed potentially cruxy to me. I.e., if building protein factories that built nanofactories that built nanomachines that met a certain deep and lofty engineering goal, didn't involve cognitive challenges different in kind from protein folding, we could maybe just safely go do that using AlphaFold 3, which would be just as safe as AlphaFold 2. I don't think we can do that. But it is among the more plausible advance-specified miracles we could get. At this point our last hope is that in fact the future is often quite surprising.\"</p><p>Richard, summarized by Richard: \"It seems to me that you\u2019re making the same mistake here as you did with regards to recursive self-improvement in the AI foom debate - namely, putting too much trust in one big abstraction.\"</p><p>Eliezer, summarized by Richard: \"I suppose that is what it could potentially feel like from the inside to not get an abstraction.&nbsp; Robin Hanson kept on asking why I was trusting my abstractions so much, when he was in the process of trusting his worse abstractions instead.\"</p></td></tr></tbody></table></figure><p>&nbsp;</p><h2>4.2. Nate Soares' summary</h2><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p><i>Consequentialism</i></p><p>Ok, here's a handful of notes. I apologize for not getting them out until midday Sunday. My main intent here is to do some shoring up of the ground we've covered. I'm hoping for skims and maybe some light comment back-and-forth as seems appropriate (perhaps similar to Richard's summary), but don't think we should derail the main thread over it. If time is tight, I would not be offended for these notes to get little-to-no interaction.</p><p>---</p><p>My sense is that there's a few points Eliezer was trying to transmit about consequentialism, that I'm not convinced have been received. I'm going to take a whack at it. I may well be wrong, both about whether Eliezer is in fact attempting to transmit these, and about whether Richard received them; I'm interested in both protests from Eliezer and paraphrases from Richard.</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>1. \"The consequentialism is in the plan, not the cognition\".</p><p>I think Richard and Eliezer are coming at the concept \"consequentialism\" from very different angles, as evidenced eg by Richard saying (Nate's crappy paraphrase:) \"where do you think the consequentialism is in a cat?\" and Eliezer responding (Nate's crappy paraphrase:) \"the cause of the apparent consequentialism of the cat's behavior is distributed between its brain and its evolutionary history\".</p><p>In particular, I think there's an argument here that goes something like:</p><ul><li>Observe that, from our perspective, saving the world seems quite tricky, and seems likely to involve long sequences of clever actions that force the course of history into a narrow band (eg, because if we saw short sequences of dumb actions, we could just get started).</li><li>Suppose we were presented with a plan that allegedly describes a long sequence of clever actions that would, if executed, force the course of history into some narrow band.<ul><li>For concreteness, suppose it is a plan that allegedly funnels history into the band where we have wealth and acclaim.</li></ul></li><li>One plausible happenstance is that the plan is not in fact clever, and would not in fact have a forcing effect on history.<ul><li>For example, perhaps the plan describes founding and managing some silicon valley startup, that would not work in practice.</li></ul></li><li>Conditional on the plan having the history-funnelling property, there's a sense in which it's scary regardless of its source.<ul><li>For instance, perhaps the plan describes founding and managing some silicon valley startup, and will succeed virtually every time it's executed, by dint of having very generic descriptions of things like how to identify and respond to competition, including descriptions of methods for superhumanly-good analyses of how to psychoanalyze the competition and put pressure on their weakpoints.</li><li>In particular, note that one need not believe the plan was generated by some \"agent-like\" cognitive system that, in a self-contained way, made use of reasoning we'd characterize as \"possessing objectives\" and \"pursuing them in the real world\".</li><li>More specifically, the scariness is a property of the plan itself. For instance, the fact that this plan accrues wealth and acclaim to the executor, in a wide variety of situations, regardless of what obstacles arise, implies that the plan contains course-correcting mechanisms that keep the plan on-target.</li><li>In other words, plans that <i>manage to actually funnel history</i> are (the argument goes) liable to have a wide variety of course-correction mechanisms that keep the plan oriented towards <i>some</i> target. And while this course-correcting property tends to be a property of history-funneling plans, the <i>choice of target</i> is of course free, hence the worry.</li></ul></li></ul><p>(Of course, in practice we perhaps shouldn't be visualizing a single Plan handed to us from an AI or a time machine or whatever, but should instead imagine a system that is reacting to contingencies and replanning in realtime. At the least, this task is easier, as one can adjust only for the contingencies that are beginning to arise, rather than needing to predict them all in advance and/or describe general contingency-handling mechanisms. But, and feel free to take a moment to predict my response before reading the next sentence, \"run this AI that replans autonomously on-the-fly\" and \"run this AI+human loop that replans+reevaluates on the fly\", are still in this sense \"plans\", that still likely have the property of Eliezer!consequentialism, insofar as they work.)</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>There's a part of this argument I have not yet driven home. Factoring it out into a separate bullet:</p><p>2. \"If a plan is good enough to work, it's pretty consequentialist in practice\".</p><p>In attempts to collect and distill a handful of scattered arguments of Eliezer's:</p><p>If you ask GPT-3 to generate you a plan for saving the world, it will not manage to generate one that is very detailed. And if you tortured a big language model into giving you a detailed plan for saving the world, the resulting plan would not work. In particular, it would be full of errors like insensitivity to circumstance, suggesting impossible actions, and suggesting actions that run entirely at cross-purposes to one another.</p><p>A plan that is sensitive to circumstance, and that describes actions that synergize rather than conflict -- like, in Eliezer's analogy, photons in a laser -- is much better able to funnel history into a narrow band.</p><p>But, on Eliezer's view as I understand it, this \"the plan is not constantly tripping over its own toes\" property, goes hand-in-hand with what he calls \"consequentialism\". As a particularly stark and formal instance of the connection, observe that one way a plan can trip over its own toes is if it says \"then trade 5 oranges for 2 apples, then trade 2 apples for 4 oranges\". This is clearly an instance of the plan failing to \"lase\" -- of some orange-needing part of the plan working at cross-purposes to some apple-needing part of the plan, or something like that. And this is also a case where it's easy to see how if a plan <i>is</i> \"lasing\" with respect to apples and oranges, then it is behaving as if governed by some coherent preference.</p><p>And the point as I understand it isn't \"all toe-tripping looks superficially like an inconsistent preference\", but rather \"insofar as a plan <i>does</i> manage to chain a bunch of synergistic actions together, it manages to do so precisely insofar as it is Eliezer!consequentialist\".</p><p>cf the analogy to <a href=\"https://www.lesswrong.com/s/oFePMp9rKftEeZDDr/p/QkX2bAkwG2EpGvNug\">information theory</a>, where if you're staring at a maze and you're trying to build an accurate representation of that maze in your own head, you will succeed precisely insofar as your process is Bayesian / information-theoretic. And, like, this is supposed to feel like a fairly tautological claim: you (almost certainly) can't get the image of a maze in your head to match the maze in the world by visualizing a maze at random, you have to add visualized-walls using some process that's correlated with the presence of actual walls. Your maze-visualizing process will work precisely insofar as you have access to &amp; correctly make use of, observations that correlate with the presence of actual walls. You might also visualize extra walls in locations where it's politically expedient to believe that there's a wall, and you might also avoid visualizing walls in a bunch of distant regions of the maze because it's dark and you haven't got all day, but the resulting visualization in your head is accurate precisely <i>insofar</i> as you're managing to act kinda like a Bayesian.</p><p>Similarly (the analogy goes), a plan works-in-concert and avoids-stepping-on-its-own-toes precisely insofar as it is consequentialist. These are two sides of the same coin, two ways of seeing the same thing.</p><p>And, I'm not so much attempting to <i>argue</i> the point here, as to make sure that the <i>shape of the argument</i> (as I understand it) has been understood by Richard. In particular, the <i>shape of the argument</i> I see Eliezer as making is that \"clumsy\" plans don't work, and \"laser-like plans\" work insofar as they are managing to act kinda like a consequentialist.</p><p>Rephrasing again: we have a wide variety of mathematical theorems all spotlighting, from different angles, the fact that a plan lacking in clumsiness, is possessing of coherence.</p><p>(\"And\", my model of Eliezer is quick to note, \"this ofc does not mean that all sufficiently intelligent minds must generate very-coherent plans. If you really knew what you were doing, you could design a mind that emits plans that always \"trip over themselves\" along one particular axis, just as with sufficient mastery you could build a mind that believes 2+2=5 (for some reasonable cashing-out of that claim). But you don't get this for free -- and there's a sort of \"attractor\" here, when building cognitive systems, where just as generic training will tend to cause it to have true beliefs, so will generic training tend to cause its plans to lase.\")</p><p>(And ofc much of the worry is that all the mathematical theorems that suggest \"this plan manages to work precisely insofar as it's lasing in some direction\", say nothing about which direction it must lase. Hence, if you show me a plan clever enough to force history into some narrow band, I can be fairly confident it's doing a bunch of lasing, but not at all confident which direction it's lasing in.)</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>One of my guesses is that Richard does in fact understand this argument (though I personally would benefit from a paraphrase, to test this hypothesis!), and perhaps even buys it, but that Richard gets off the train at a following step, namely that we <i>need</i> plans that \"lase\", because ones that don't aren't strong enough to save us. (Where in particular, I suspect most of the disagreement is in how far one can get with plans that are more like language-model outputs and less like lasers, rather than in the question of which pivotal acts would put an end to the acute risk period)</p><p>But setting that aside for a moment, I want to use the above terminology to restate another point I saw Eliezer as attempting to make: one big trouble with alignment, in the case where we need our plans to be like lasers, is that on the one hand we need our plans to be like lasers, but on the other hand we want them to <i>fail</i> to be like lasers along certain specific dimensions.</p><p>For instance, the plan presumably needs to involve all sorts of mechanisms for refocusing the laser in the case where the environment contains fog, and redirecting the laser in the case where the environment contains mirrors (...the analogy is getting a bit strained here, sorry, bear with me), so that it can in fact hit a narrow and distant target. Refocusing and redirecting to stay on target are part and parcel to plans that can hit narrow distant targets.</p><p>But the humans shutting the AI down is like scattering the laser, and the humans tweaking the AI so that it plans in a different direction is like them tossing up mirrors that redirect the laser; and we want the plan to fail to correct for those interferences.</p><p>As such, on the Eliezer view as I understand it, we can see ourselves as asking for a very unnatural sort of object: a path-through-the-future that is robust enough to funnel history into a narrow band in a very wide array of circumstances, but somehow insensitive to specific breeds of human-initiated attempts to switch which narrow band it's pointed towards.</p><p>Ok. I meandered into trying to re-articulate the point over and over until I had a version distilled enough for my own satisfaction (which is much like arguing the point), apologies for the repetition.</p><p>I don't think debating the claim is the right move at the moment (though I'm happy to hear rejoinders!). Things I would like, though, are: Eliezer saying whether the above is on-track from his perspective (and if not, then poking a few holes); and Richard attempting to paraphrase the above, such that I believe the arguments themselves have been communicated (saying nothing about whether Richard also buys them).</p><p>---</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>My Richard-model's stance on the above points is something like \"This all seems kinda plausible, but where Eliezer reads it as arguing that we had better figure out how to handle lasers, I read it as an argument that we'd better save the world without needing to resort to lasers. Perhaps if I thought the world could not be saved except by lasers, I would share many of your concerns, but I do not believe that, and in particular it looks to me like much of the recent progress in the field of AI -- from AlphaGo to GPT to AlphaFold -- is evidence in favor of the proposition that we'll be able to save the world without lasers.\"</p><p>And I recall actual-Eliezer saying the following (more-or-less in response, iiuc, though readers note that I might be misunderstanding and this might be out-of-context):</p><blockquote><p>Definitely, \"turns out it's easier than you thought to use gradient descent's memorization of zillions of shallow patterns that overlap and recombine into larger cognitive structures, to add up to a consequentialist nanoengineer that only does nanosystems and never does sufficiently general learning to apprehend the big picture containing humans, while still understanding the goal for that pivotal act you wanted to do\" is among the more plausible advance-specified miracles we could get.&nbsp;</p></blockquote><p>On my view, and I think on Eliezer's, the \"zillions of shallow patterns\"-style AI that we see today, is not going to be sufficient to save the world (nor destroy it). There's a bunch of reasons that GPT and AlphaZero aren't destroying the world yet, and one of them is this \"shallowness\" property. And, yes, maybe we'll be wrong! I myself have been surprised by how far the shallow pattern memorization has gone (and, for instance, was surprised by GPT), and acknowledge that perhaps I will continue to be surprised. But I continue to predict that the shallow stuff won't be enough.</p><p>I have the sense that lots of folk in the community are, one way or another, saying \"Why not consider the problems of aligning systems that memorize zillions of shallow patterns?\". And my answer is, \"I still don't expect those sorts of machines to either kill or save us, I'm still expecting that there's a phase shift that won't happen until AI systems start to be able to make plans that are sufficiently deep and laserlike to do scary stuff, and I'm still expecting that the real alignment challenges are in that regime.\"</p><p>And this seems to me close to the heart of the disagreement: some people (like me!) have an intuition that it's quite unlikely that figuring out how to get sufficient work out of shallow-memorizers is enough to save us, and I suspect others (perhaps even Richard!) have the sense that the aforementioned \"phase shift\" is the unlikely scenario, and that I'm focusing on a weird and unlucky corner of the space. (I'm curious whether you endorse this, Richard, or some nearby correction of it.)</p><p>In particular, Richard, I am curious whether you endorse something like the following:</p><ul><li>I'm focusing ~all my efforts on the shallow-memorizers case, because I think shallow-memorizer-alignment will by and large be sufficient, and even if it is not then I expect it's a good way to prepare ourselves for whatever we'll turn out to need in practice. In particular I don't put much stock in the idea that there's a predictable phase-change that forces us to deal with laser-like planners, nor that predictable problems in that domain give large present reason to worry.</li></ul><p>(I suspect not, at least not in precisely this form, and I'm eager for corrections.)</p><p>I suspect something in this vicinity constitutes a crux of the disagreement, and I would be thrilled if we could get it distilled down to something as concise as the above. And, for the record, I personally endorse the following counter to the above:</p><ul><li>I am focusing ~none of my efforts on shallow-memorizer-alignment, as I expect it to be far from sufficient, as I do not expect a singularity until we have more laser-like systems, and I think that the laserlike-planning regime has a host of predictable alignment difficulties that Earth does not seem at all prepared to face (unlike, it seems to me, the shallow-memorizer alignment difficulties), and as such I have large and present worries.</li></ul><p>---</p></td></tr><tr><td style=\"background-color:rgb(255, 247, 222)\"><p><strong>[Soares] &nbsp;(Sep. 12 Google Doc)</strong>&nbsp;</p><p>Ok, and now a few less substantial points:</p><p>There's a point Richard made here:</p><blockquote><p>Oh, interesting. Actually one more question then: to what extent do you think that explicitly reasoning about utility functions and laws of rationality is what makes consequentialists have the properties you've been talking about?</p></blockquote><p>that I suspect constituted a miscommunication, especially given that the following sentence appeared in Richard's summary:</p><blockquote><p>A third thing that makes humans in particular consequentialist is planning, especially when we\u2019re aware of concepts like utility functions.</p></blockquote><p>In particular, I suspect Richard's model of Eliezer's model places (or placed, before Richard read Eliezer's comments on Richard's summary) some particular emphasis on systems reflecting and thinking about their own strategies, as a method by which the consequentialism and/or effectiveness gets in. I suspect this is a misunderstanding, and am happy to say more on my model upon request, but am hopeful that the points I made a few pages above have cleared this up.</p><p>Finally, I observe that there are a few places where Eliezer keeps beeping when Richard attempts to summarize him, and I suspect it would be useful to do the dorky thing of Richard very explicitly naming Eliezer's beeps as he understands them, for purposes of getting common knowledge of understanding. For instance, things I think it might be useful for Richard to say verbatim (assuming he believes them, which I suspect, and subject to Eliezer-corrections, b/c maybe I'm saying things that induce separate beeps):</p><p>1. Eliezer doesn't believe it's impossible to build AIs that have most any given property, including most any given safety property, including most any desired \"non-consequentialist\" or \"deferential\" property you might desire. Rather, Eliezer believes that many desirable safety properties don't happen by default, and require mastery of minds that likely takes a worrying amount of time to acquire.</p><p>2. The points about consequentialism are not particularly central in Eliezer's view; they seem to him more like obvious background facts; the reason conversation has lingered here in the EA-sphere is that this is a point that many folk in the local community disagree on.</p><p>For the record, I think it might also be worth Eliezer acknowledging that Richard probably understands point (1), and that glossing \"you don't get it for free by default and we aren't on course to have the time to get it\" as \"you can't\" is quite reasonable when summarizing. (And it might be worth Richard counter-acknowledging that the distinction is actually quite important once you buy the surrounding arguments, as it constitutes the difference between describing the current playing field and laying down to die.) I don't think any of these are high-priority, but they might be useful if easy :-)</p><p>---</p><p>Finally, stating the obvious-to-me, none of this is intended as criticism of either party, and all discussing parties have exhibited significant virtue-according-to-Nate throughout this process.</p></td></tr></tbody></table></figure><figure class=\"table\"><table><tbody><tr><td><p><strong>[Yudkowsky][21:27] &nbsp;(Sep. 12)</strong>&nbsp;</p><p>From Nate's notes:</p><blockquote><p>For instance, the plan presumably needs to involve all sorts of mechanisms for refocusing the laser in the case where the environment contains fog, and redirecting the laser in the case where the environment contains mirrors (...the analogy is getting a bit strained here, sorry, bear with me), so that it can in fact hit a narrow and distant target. Refocusing and redirecting to stay on target are part and parcel to plans that can hit narrow distant targets.</p><p>But the humans shutting the AI down is like scattering the laser, and the humans tweaking the AI so that it plans in a different direction is like them tossing up mirrors that redirect the laser; and we want the plan to fail to correct for those interferences.</p></blockquote><p>--&gt; GOOD ANALOGY.</p><p>...or at least it sure conveys to <i>me</i> why corrigibility is anticonvergent / anticoherent / actually <i>moderately strongly contrary to</i> and not just <i>an orthogonal property of</i> a powerful-plan generator.</p><p>But then, I already know why that's true and how it generalized up to resisting our various attempts to solve small pieces of more important aspects of it - it's not just true by weak default, it's true by a stronger default where a roomful of people at a workshop spend several days trying to come up with increasingly complicated ways to describe a system that will let you shut it down (but not steer you through time <i>into</i> shutting it down), and all of those suggested ways get shot down. (And yes, people outside MIRI now and then publish papers saying they totally just solved this problem, but all of those \"solutions\" are things we considered and dismissed as trivially failing to scale to powerful agents - they didn't understand what we considered to be the first-order problems in the first place - rather than these being evidence that MIRI just didn't have smart-enough people at the workshop.)</p></td></tr><tr><td style=\"background-color:#FFEEBB\"><p><strong>[Yudkowsky][18:56] &nbsp;(Nov. 5 follow-up comment)</strong>&nbsp;</p><p>Eg, \"Well, we took a system that only learned from reinforcement on situations it had previously been in, and couldn't use imagination to plan for things it had never seen, and then we found that if we didn't update it on shut-down situations it wasn't reinforced to avoid shutdowns!\"</p></td></tr></tbody></table></figure>", "user": {"username": "richard_ngo"}}, {"_id": "4rA3vNbWw64yHvd2W", "title": "What Should be Taught in Workshops for Community Builders?", "postedAt": "2021-11-15T18:41:13.442Z", "htmlBody": "<p><i>TLDR: What are the most important skills to teach student group organizers &amp; community builders? Please let me know in the comments, or email me at </i><a href=\"mailto:wasil@sas.upenn.edu\"><i><u>wasil@sas.upenn.edu</u></i></a></p><p>I (Penn EA organizer and PhD student in psychology) have recently attended several retreats focused on community building &amp; student group organizing.</p><p><strong>I am interested in piloting workshops that equip student group organizers </strong>(and potentially other community builders) <strong>with useful skills</strong>. By \u201cuseful\u201d I mean that they either:</p><ul><li>Improve their ability to be effective organizers</li><li>Improve their own mental health or productivity</li></ul><p><strong>Why might this be important?</strong></p><p><strong>1) Directly supporting community organizers.&nbsp;</strong></p><ul><li>I think the work of community builders is extremely valuable (I won\u2019t list all the reasons in this post, but many are summarized <a href=\"https://80000hours.org/problem-profiles/promoting-effective-altruism/\"><u>here</u></a> and <a href=\"https://www.youtube.com/watch?v=VYBNk0jYJmA\"><u>here</u></a>). With that in mind, efforts that support community organizers seem high-impact, even if they have rather \u201csmall\u201d effect sizes (e.g., improving organizer effectiveness or sustainability by 5-10%).</li></ul><p><strong>2) Creating a culture of information-sharing &amp; support</strong></p><ul><li>Student group organizers at different universities often encounter similar challenges. One challenge is <i>fragmentation</i>; in many cases, student group A simply doesn\u2019t know much about what\u2019s happening at student group B. The organizers of group A might learn useful techniques that could save group B a lot of time, but that information never gets shared.</li><li>I think efforts to create a <strong>culture of information-sharing and support </strong>between student groups would be valuable. One way of doing this is to have retreats. Another way is to have a group organizer slack channel. And yet another way is to have group leaders regularly thinking \u201cwhat are some skills or lessons that I could teach to others?\u201d and share them via short workshops, videos, posts, etc.</li><li>In a world where these workshops go well, I think they would a) provide more settings for group organizers to interact with each other and b) incentivize other group organizers to run short workshops on topics of widespread interest. It\u2019s possible that we could run workshops regularly (e.g., once a month), offering a regular/consistent way for group leaders to meet, share ideas, and learn new skills (of course, I think this would be conditional on receiving feedback from organizers that these workshops are actually useful).</li></ul><p><strong>What would these workshops look like?</strong></p><p>My current plan is to hold them over Zoom. Each workshop would last 60-90 minutes and involve a mix of:&nbsp;</p><ul><li>Didactics (sharing slides and talking at people)</li><li>Role-plays (in which attendees are divided into pairs and have opportunities to practice skills)</li><li>Discussion about the role-plays (in which attendees share best practices, concerns, questions, etc.)</li><li>Reflections (in which attendees brainstorm ways that the skills can be applied to their lives or to their groups)</li></ul><p><strong>Why do these need to be workshops? Could you share these lessons via forum posts?</strong></p><ul><li>Workshops are more conducive to role-plays and discussions</li><li>Part of the benefit of the workshops will involve attendees meeting each other &amp; sharing their own insights/strategies/experiences with each other.</li><li>With that in mind, if one or more of these workshops go well, I could envision creating a forum post to summarize some of the key lessons/takeaways.</li></ul><p><strong>What are some examples of topics that could be covered in workshops?</strong></p><ul><li>Effective communication skills (e.g., active listening skills &amp; productive disagreement skills that help organizers improve their ability to lead discussions or have 1-on-1 conversations with promising members)</li><li>Mental health promotion strategies (e.g., strategies from CBT and positive psychology that can help organizers prevent burnout, manage stress, and support others in their groups)</li><li>How to run good meetings (e.g., best practices for running engaging and informative general meetings)</li><li>How to discuss EA with different audiences (e.g., best practices for talking about EA &amp; how to change the framing based on your audience\u2019s values)</li></ul><p><strong>What can you do to help?</strong></p><ul><li>Let me know if you have any suggestions for workshop ideas!</li><li>Let me know if you, or anyone you know, would be interested in attending a pilot workshop &amp; offering feedback</li><li>Let me know if you think this is an awful idea (or even just a \u201cmeh\u201d idea).</li></ul><p><i>I\u2019m grateful to Olivia Jimenez, Harry Taussig, and Henry Sleight for feedback on this idea. I\u2019m also grateful to everyone who organized and attended community building retreats!</i></p>", "user": {"username": "Akash"}}, {"_id": "SbjotEZsHhRBkk8Pg", "title": "New Effective Thesis services and opportunities to get involved", "postedAt": "2021-11-15T18:15:56.843Z", "htmlBody": "<p>UPDATE (03/22): We have launched two new services - Community platform and PhD funding advice and database - and added them here now for the sake of completeness.</p><p>&nbsp;&nbsp;</p><h1>What is Effective Thesis?</h1><p><a href=\"http://effectivethesis.org/\"><u>Effective Thesis</u></a> is an organisation that supports students to begin research careers that significantly improve the world. We do this primarily by helping students identify important problems where further research could have a big impact. Previously, our main service was individual coaching aiming to help students with their final thesis topic choices (see more details in our 2020 post on <a href=\"https://forum.effectivealtruism.org/posts/4CHcuF2YP3EjZnGD6/effective-thesis-updates-from-2019-and-call-for\"><u>updates from 2019</u></a>). Recently, we have launched a few more services with the same goal that we want to introduce here.</p><h2>Coaching&nbsp;</h2><p>This is the core service that we have been providing for the last few years. We assist students individually with the general process of choosing their thesis/PhD topic and connect them with relevant resources, other students with the same focus and more experienced domain experts. You can <a href=\"https://effectivethesis.org/thesis-coaching/\"><u>apply for our coaching here</u></a>.&nbsp;&nbsp;&nbsp;</p><p><br>Our current team consists of one Head of Coaching Management, one Head of Coaching Development and three volunteer coaches. As our organisation grows, we are continuously looking for promising new volunteer coaches to complement our team. See the Get involved section below for more information.</p><h2>Help with finding a supervisor</h2><p>Finding a good supervisor is often a struggle for students planning to pursue research careers. At the same time, we believe that choice of supervisor can make a significant difference to a student\u2019s development as a researcher. We have decided to create resources to support students searching for supervisors and help them make better decisions.<br><br>We are compiling, on an ongoing basis, a list of academics who we believe could be a good fit to act as undergraduate/masters/PhD thesis supervisors for the students interested in doing research in one of <a href=\"https://effectivethesis.org/research-directions-list/\"><u>the research directions we recommend</u></a>. We don't match and introduce students to potential supervisors ourselves, but we give students instructions on how to communicate with potential supervisors. You can apply for <a href=\"https://effectivethesis.org/potential-supervisors/\"><u>access to this database here</u></a>.<br><br>We are also building a series of posts covering how to find potential supervisors (with an emphasis on choosing PhD supervisors), how to choose between multiple potential supervisors, and other key questions students should ask themselves before embarking on a PhD.&nbsp;<br><br>If you can supervise students interested in some of <a href=\"https://effectivethesis.org/research-directions-list/\"><u>the research directions we recommend</u></a> and would like to be added to our database, please <a href=\"https://airtable.com/shr42PBunhLvvoJOW\"><u>fill in this form</u></a>. Similarly, please fill in the form if you know anyone who you think would be a good supervisor in any of the research directions mentioned.&nbsp;&nbsp;</p><h2>Sharing early career research-related opportunities</h2><p>We are running an opportunity search service for students looking for research opportunities in some of our <a href=\"https://effectivethesis.org/research-directions-list/\"><u>prioritised research directions</u></a>. The opportunities we cover include jobs, conferences, scholarships, internships, awards, collaboration offers, calls for papers, seminars, reading groups and funding. We only share research opportunities and focus on opportunities suited to students and early career researchers with up to 2 years of professional experience. You can sign up for this <a href=\"https://effectivethesis.org/opportunities-newsletter/\"><u>service here</u></a>.<br><br>We try to cover as large a space of opportunities as possible and thus rely on multiple people contributing to our database of opportunities whenever they find out about something relevant. If you\u2019d like to help us search for these opportunities in a given research direction you're interested in/have networks in, we would be very happy to have you join our team. See the Get involved section below for more information.<br><br>If you know about a one-off opportunity that could fit our focus and you would like us to share it with other students (e.g. you run research internships, are hiring for early-career research jobs, are seeking collaborators, etc..), please message <a href=\"mailto:isobel.phillips@effectivethesis.org\"><u>isobel.phillips@effectivethesis.org</u></a>.</p><h2>Exceptional Research Award</h2><p>We have launched the Effective Thesis Exceptional Research Award to encourage and recognize promising research by students that has the potential to significantly improve the world. To be eligible for the award, students need to focus their final thesis on some of our <a href=\"https://effectivethesis.org/research-directions-list/\"><u>prioritised research directions</u></a> and to succeed, their research must be of high quality, ideally making a novel contribution, and make as much progress as possible towards solving some problem described within our prioritised research directions. We place emphasis on how well the research questions were chosen and how well the work was done rather than the direct measurable impact of the written work. We aim to reward good work rather than good results, therefore we also welcome \u201cfailed attempts\u201d (e.g. negative results, etc..), as long as they have been carefully conducted. This award has undergraduate and graduate tiers and the submission deadline is September 1st, 2022. You can <a href=\"https://effectivethesis.org/effective-thesis-award/\"><u>read more and apply here</u></a>.</p><h2>PhD Funding</h2><p>While some PhD programmes offer a stipend that covers students' tuition and living expenses, other programmes require them to attain funding independently. If you are doing an independent research project, you might also need funding to support yourself. We have collated the best advice on PhD funding we could find and created a <a href=\"https://effectivethesis.org/funding/\">database of PhD funding opportunities</a> available for people focusing their PhD thesis on one of our <a href=\"https://effectivethesis.org/research-directions-list/\"><u>prioritised research directions.</u></a></p><h2>Community</h2><p>We have also noticed that students often benefit from connecting with other students, sharing their experiences, and getting a sense of not being alone on their path. Indeed, about 20 % of students referred in our feedback forms that expanding their network of people with similar research interests was the the thing they most benefited from Effective Thesis. We wanted to generate this kind of impact more explicitly. Therefore, we decided to create an online space - a community platform - where students can meet, get to know others working on similar research problems, share their experiences, etc\u2026&nbsp;All students who apply for our coaching are automatically invited to the community and other can <a href=\"https://effectivethesis.org/community/\">apply for access here</a>.&nbsp;</p><p>&nbsp;</p><p>We are very happy to hear any feedback about our services and are also open to various types of collaborations. Please comment below or message <a href=\"mailto:david.janku@effectivethesis.org\"><u>david.janku@effectivethesis.org</u></a> with feedback/collaboration suggestions.&nbsp;</p><h1>Get involved</h1><p>There are various ways you can get involved with us besides <a href=\"https://effectivethesis.org/thesis-coaching/\">applying for our coaching </a>or <a href=\"https://effectivethesis.org/other-services/\"><u>other services</u></a> if you are a student. Please reach out to us if you\u2019re interested in any of the roles below!</p><p>Right now, we are most interested in people who could join our coaching team and opportunity search team as volunteers. Please see a more in-depth description of these roles below.</p><p>We will also be hosting an <a href=\"https://www.facebook.com/events/574577543628956\"><u>online live Q&amp;A</u></a> on Friday 26th of November at 9 CET and at 19 CET (<a href=\"https://us02web.zoom.us/j/7355400515\">Zoom link</a>), so feel free to join and ask any questions you have on your mind.&nbsp;</p><h2>Join our coaching team</h2><p>Our coaches are guides to students in their process of choosing a research topic. They help students reflect on and clarify which research direction is the best fit for them, offer useful resources for doing impactful research, and facilitate connections with other students and experts in their preferred research direction. For you as a coach, this would be a chance to create a long-term positive impact on students as well as the broader world, meet lots of motivated and talented students from all around the world, be in touch with experienced researchers internationally via connecting them to students, grow as a coach and develop your skillset through your coaching sessions, discussions with others and workshops/training and you will also become a part of a very friendly, international and supportive team. For more information see our role description <a href=\"https://docs.google.com/document/d/1PAOwhIIwgljfo8GRvDzG2Xvjanrr4iQrLgZK5FHBuQE/edit?usp=sharing\"><u>here</u></a>.<br><br>If you're interested in volunteering to coach our students we\u2019d love to hear from you. Email vorathep.sachdev@effectivethesis.org for more information.</p><h2>Join our opportunity search team</h2><p>Our opportunity search volunteers scan the EA and academic ecosystems for opportunities relevant to students who use our services and add them to our database. It\u2019s a growing service where you can deepen your knowledge of the EA and academic ecosystems, improve your knowledge of various impactful research directions, learn new skills (e.g. working with airtable, automations, advanced google search) and network within our international team, but also you can take ownership and shape the direction of the service. Current students are encouraged to apply as there are lots of cross-over benefits of this work with your personal search for your next opportunity. For more information see the role description <a href=\"https://docs.google.com/document/d/1xfjeTXaHUkIB9v2fyOfcqNaqCUVNBO8rteNqoqFV8So/edit?usp=sharing\">here</a>.<br><br>If you're interested in joining our opportunities search team, we\u2019d love to hear from you. Email <a href=\"mailto:isobel.phillips@effectivethesis.org\"><u>isobel.phillips@effectivethesis.org</u></a> for more information on how to get involved.</p><h2>Join our network of experts</h2><p>Our experts provide valuable guidance to students interested in their field of research. An expert might offer feedback on a research proposal, have a one-off call with a student to discuss their ideas, or provide ongoing mentorship throughout the thesis-writing process \u2014 the role is very flexible and you can choose the level of commitment. Please <a href=\"https://airtable.com/shrE3xrUVI5J90HSy\">get in touch</a> if you\u2019d like to join our network.</p><h2>Supervise students or recommend supervisors</h2><p>If you are potentially interested in supervising students working on one or more of our research directions, please <a href=\"https://airtable.com/shr42PBunhLvvoJOW\">let us know</a> \u2014 we are always glad to hear of new supervisors. Likewise, if you\u2019ve had a positive experience with a supervisor who you think would supervise students working on at least one of our research directions, you can let us know via the same form.</p><h2>Contribute to our research profiles</h2><p>If you have expertise related to any of the research profiles featured on our site and you're interested in helping to make them more detailed and up-to-date, please get in touch with <a href=\"mailto:sophie.kirkham@effectivethesis.org\"><u>sophie.kirkham@effectivethesis.org</u></a>. We would be particularly interested in contributions to improving or creating the profiles listed <a href=\"https://docs.google.com/document/d/1bQZZ0xXAHGyQe6YAW93TEANoaliQA7uqA0DwcaIN7Xo/edit?usp=sharing\"><u>here</u></a>.</p><h2>Let others know about us</h2><p>If you know anyone who is deciding about their final thesis topics or PhD applications soon, please tell them about us and encourage them to <a href=\"https://effectivethesis.org\"><u>visit our website!</u></a></p><p>&nbsp;</p><p><br><br><br>&nbsp;</p>", "user": {"username": "DavidJanku"}}, {"_id": "D499oMCiFiqHT92TT", "title": "We\u2019re Rethink Priorities. Ask us anything!", "postedAt": "2021-11-15T16:25:05.734Z", "htmlBody": "<p>Hi all,</p>\n<p>We're the staff at Rethink Priorities and we would like you to Ask Us Anything! We'll be answering all questions starting Friday, November 19.</p>\n<h2>About the Org</h2>\n<p>Rethink Priorities is an EA research organization focused on helping improve decisions among funders and key decision-makers within EA and EA-aligned organizations. You might know of our work on quantifying the <a href=\"https://forum.effectivealtruism.org/posts/pT7AYJdaRp6ZdYfny/estimates-of-global-captive-vertebrate-numbers\">number of farmed vertebrates</a> and <a href=\"https://forum.effectivealtruism.org/posts/9drbh8sKzzykaX38P/the-scale-of-direct-human-impact-on-invertebrates\">invertebrates</a>, <a href=\"https://forum.effectivealtruism.org/posts/4Xg3dC6rrW4WFnSne/comparisons-of-capacity-for-welfare-and-moral-status-across\">interspecies comparisons of moral weight</a>, <a href=\"https://forum.effectivealtruism.org/posts/2LdswNsEZAgDfJDzo/intervention-profile-ballot-initiatives\">ballot initiatives as a tool for EAs</a>, <a href=\"https://forum.effectivealtruism.org/posts/MsJvzmYLMpsdJBb6C/which-nuclear-wars-should-worry-us-most-1\">the risk of nuclear winter</a>, or <a href=\"https://forum.effectivealtruism.org/tag/effective-altruism-survey\">running the EA Survey</a>, among other projects. You can see <a href=\"https://www.rethinkpriorities.org/research\">all of our work to date here</a>.</p>\n<p>Over the next few years, we\u2019re expanding our farmed animal welfare and moral weight research programs, launching an AI governance and strategy research program, and continuing to grow our new global health and development wing (including evaluating climate change interventions).</p>\n<h2>Team</h2>\n<p><a href=\"https://rethinkpriorities.org/team\">You can find bios of our team members here</a>. Links on names below go to RP publications by the author (if any are publicly available at this point).</p>\n<h3>Leadership</h3>\n<ul>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Marcus%20A.%20Davis\">Marcus Davis</a></strong> \u2014 Co-CEO \u2014 Focus on animal welfare and operations</li>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Peter%20Wildeford\">Peter Wildeford</a></strong> \u2014 Co-CEO \u2014 Focus on longtermism, global health and development, surveys, and EA movement research</li>\n</ul>\n<h3>Animal Welfare</h3>\n<ul>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Kim%20Cuddington\">Dr. Kim Cuddington</a></strong> \u2014 Senior Ecologist \u2014 Wild animal welfare</li>\n<li><strong>Dr. William McAuliffe</strong> \u2014 Senior Research Manager \u2014 Wild animal welfare, farmed animal welfare</li>\n<li><strong>Jacob Peacock</strong> \u2014 Senior Research Manager \u2014 Farmed animal welfare</li>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Jason%20Schukraft\">Dr. Jason Schukraft</a></strong> \u2014 Senior Research Manager \u2014 Moral weight, global health and development</li>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Daniela%20R.%20Waldhorn\">Daniela Waldhorn</a></strong> \u2014 Senior Research Manager \u2014 Invertebrate welfare, farmed animal welfare</li>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Neil%20Dullaghan\">Dr. Neil Dullaghan</a></strong> \u2014 Senior Researcher \u2014 Farmed animal welfare</li>\n<li><strong>Dr. Samara Mendez</strong> \u2014 Senior Researcher \u2014 Farmed animal welfare</li>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Saulius%20Simcikas\">Saulius \u0160im\u010dikas</a></strong> \u2014 Senior Researcher \u2014 Farmed animal welfare</li>\n<li><strong>Meghan Barrett</strong> \u2014 Entomology Specialist \u2014 Invertebrate welfare</li>\n<li><strong>Dr. Holly Elmore</strong> \u2014 Researcher \u2014 Wild animal welfare</li>\n<li><strong>Michael St. Jules</strong> \u2014 Associate Researcher \u2014 Farmed animal welfare</li>\n</ul>\n<h3>Longtermism</h3>\n<ul>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Michael%20Aird\">Michael Aird</a></strong> \u2014 Researcher \u2014 Nuclear war, AI governance and strategy</li>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Linchuan%20Zhang\">Linch Zhang</a></strong> \u2014 Researcher \u2014 Forecasting, AI governance and strategy</li>\n</ul>\n<h3>Surveys and EA movement research</h3>\n<ul>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=David%20Moss\">David Moss</a></strong> \u2014 Principal Research Director \u2014 Surveys and EA movement research</li>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=David%20Reinstein\">Dr. David Reinstein</a></strong> \u2014 Senior Economist \u2014 EA Survey, effective giving research</li>\n<li><strong>Dr. Jamie Elsey</strong> \u2014 Senior Behavioral Scientist \u2014 Surveys</li>\n<li><strong>Dr. Willem Sleegers</strong> \u2014 Senior Behavioral Scientist \u2014 Surveys</li>\n</ul>\n<h3>Global Health and Development</h3>\n<ul>\n<li><strong>Dr. Greer Gosnell</strong> \u2014 Senior Environmental Economist \u2014 Climate change, global health interventions</li>\n<li><strong>Ruby Dickson</strong> \u2014 Researcher \u2014 Global health interventions</li>\n<li><strong>Jenny Kudymowa</strong> \u2014 Researcher \u2014 Global health interventions</li>\n<li><strong>Bruce Tsai</strong> \u2014 Researcher \u2014 Climate change, global health interventions</li>\n</ul>\n<h3>Operations</h3>\n<ul>\n<li><strong><a href=\"https://rethinkpriorities.org/publications?tag=Abraham%20Rowe\">Abraham Rowe</a></strong> \u2014 COO \u2014 Operations, finance, HR, development, communications</li>\n<li><strong>Janique Behman</strong> \u2014 Director of Development \u2014 Development, communications</li>\n<li><strong>Dr. Dominika Krupocin</strong> \u2014 Senior People and Culture Coordinator \u2014 HR</li>\n<li><strong>Carolina Salazar</strong> \u2014 Project and Hiring Manager \u2014 HR, project management</li>\n<li><strong>Romina Giel</strong> \u2014 Operations Associate \u2014 Operations, finance</li>\n</ul>\n<h2>Ask Us Anything</h2>\n<p>Please ask us anything \u2014 about the org and how we operate, about the staff, about our research\u2026 anything!</p>\n<p>You can read more about us in <a href=\"https://forum.effectivealtruism.org/posts/K7tjvcDurrCj72D7H/rethink-priorities-2021-impact-and-2022-strategy\">our 2021 Impact and 2022 Strategy update</a> or visit our website: <a href=\"https://www.rethinkpriorities.org/\">rethinkpriorities.org</a>.</p>\n<p>If you're interested in hearing more, please <a href=\"https://www.rethinkpriorities.org/newsletter\">subscribe to our newsletter</a>.</p>\n<p>Also, we\u2019re currently raising funds to continue growing in 2022. We consider ourselves funding constrained \u2014 we continue to get far more qualified applicants to our roles than we are able to hire, and have scalable infrastructure to support far more research. We accept and track restricted funds by cause area if that is of interest.</p>\n<p>If you'd like to support our work, visit <a href=\"https://www.rethinkpriorities.org/donate\">https://www.rethinkpriorities.org/donate</a>, give on <a href=\"https://www.facebook.com/donate/290097786185640/\">Giving Tuesday via Facebook</a> to potentially secure matching funds, or email Janique Behman at <a href=\"mailto:janique@rethinkpriorities.org\">janique@rethinkpriorities.org</a>.</p>\n<p>We'll be answering all questions starting Friday, November 19.</p>\n", "user": {"username": "Peter_Hurford"}}, {"_id": "K7tjvcDurrCj72D7H", "title": "Rethink Priorities - 2021 Impact and 2022 Strategy", "postedAt": "2021-11-15T16:17:34.212Z", "htmlBody": "<h1>Summary</h1>\n<h2>Our purpose and people</h2>\n<ul>\n<li>Rethink Priorities is a research organization that conducts research to inform policymakers and major foundations about how to best help people and nonhuman animals in both the present and the long-term future. Our work informs key stakeholders of the effective altruism community regarding decisions around funding, interventions, and research time allocation worth millions of dollars every year.</li>\n<li>This year we hired 14 new people. We now have a staff of 28 people, corresponding to 24.75 full-time equivalents (with 19.75 FTE focused on research and 5 FTE on operations). We\u2019ll have spent about $2.1M USD in 2021.</li>\n</ul>\n<h2>Our 2021 impact</h2>\n<p>In 2021, we achieved some tangible impact from our work, such as:</p>\n<ul>\n<li>Improving animal welfare strategies in the European Union (moving several million dollars to what we believe are more effective approaches).</li>\n<li>Setting up an ambitious project to study the capacity for welfare of different species.</li>\n<li>Starting to staff up a team to address AI Governance and Strategy.</li>\n<li>Investigating lead reduction as a possible intervention competitive with GiveWell\u2019s top charities.</li>\n<li>Running an intern program that successfully got new researchers involved in effective altruism and led to some interns getting permanent jobs in organizations like the Centre for Effective Altruism, Founders Pledge, and here at Rethink Priorities.</li>\n</ul>\n<h2>Our plans for 2022</h2>\n<p>Among other projects in 2022, we\u2019re especially excited to:</p>\n<ul>\n<li>Begin work with our newly expanded AI Governance and Strategy team and Global Health and Development team, both of which are directly focused on identifying high-impact giving opportunities.</li>\n<li>Build a larger longtermist research team to explore longtermist work and interventions more broadly.</li>\n<li>Tentatively conclude our intensive work on interspecies comparisons of moral weight, which could help us better prioritize across many cause areas.</li>\n<li>Help solve the <a href=\"https://forum.effectivealtruism.org/posts/zA6AnNnYBwuokF8kB/is-effective-altruism-growing-an-update-on-the-stock-of\">funding overhang in EA</a> and unlock tons of impact by identifying interventions across cause areas that can take lots of money while still meeting a high bar for cost-effectiveness.</li>\n</ul>\n<h2>Our funding goals</h2>\n<p>If better funded, we would be able to do more high-quality work and employ more talented researchers than we otherwise would.</p>\n<p>Currently, our goal is to raise $5,435,000 by the end of 2022. This consists of gaps of:</p>\n<ul>\n<li>$2,230,000 for animal-focused research</li>\n<li>$1,410,000 for longtermism research</li>\n<li>$1,275,000 for EA movement research and surveying</li>\n<li>$520,000 for global health and development research</li>\n</ul>\n<p>However, we believe that if we were maximally ambitious and expanded as much as is feasible, we could effectively spend the funds if we raised up to $12,900,000 in 2022.</p>\n<p>If you\u2019d like to support our work, you can donate to us as part of <a href=\"https://www.facebook.com/donate/290097786185640/\">Facebook\u2019s donation matching on Giving Tuesday on November 30</a>, or <a href=\"https://www.rethinkpriorities.org/donate\">donate directly to us here</a>. We do accept and track restricted funds by cause area if that is of interest. If you have questions about tax-deductibility in your country or are interested in making a major gift, please contact our Director of Development <a href=\"mailto:janique@rethinkpriorities.org\">Janique Behman</a>.</p>\n<h2>Ask us more</h2>\n<p><a href=\"https://forum.effectivealtruism.org/posts/D499oMCiFiqHT92TT/we-re-rethink-priorities-ask-us-anything\">We\u2019re running an AMA on the EA Forum this Friday, November 19</a>. Ask us any questions you may have!</p>\n<h1>Our path to impact</h1>\n<p>Rethink Priorities achieves impact by improving the decisions made by grantmakers and on-the-ground organizations, multiplying the impact of their work. The effective altruism movement currently allocates hundreds of millions of dollars and millions of hours of work every year \u2014 we aim to improve that allocation.</p>\n<p>We work independently to uncover new insights while also collaborating with existing groups and funders to ensure the most effective actions are taken based on rigorous research.</p>\n<p>Our organization can be understood as all three of the following:</p>\n<ul>\n<li>A research institute driven by research agendas we set according to our own priorities.</li>\n<li>A consultancy doing commissioned work in response to demands from EA-aligned organizations.</li>\n<li>A think tank aiming to inform public policy to improve the world.</li>\n</ul>\n<p>Our theory of change works as follows:</p>\n<p><img src=\"https://i.ibb.co/z6xvV7x/theory-of-change.png\" alt=\"\"></p>\n<h2>Organizational structure and staff</h2>\n<p>Rethink Priorities currently has a staff of 28 people, corresponding to 24.75 full-time equivalents (including 5 FTE operations staff). This year we spent 61% of our time working on research relevant to farmed and wild animal welfare, 23% on longtermism, 8% on EA movement research, 9% on global health and development, and 8% on other research projects.<sup class=\"footnote-ref\"><a href=\"#fn-MHixtpTXNhMy53zdf-1\" id=\"fnref-MHixtpTXNhMy53zdf-1\">[1]</a></sup></p>\n<h2>New hires in 2021</h2>\n<p>In 2021, we made a number of hires to improve our team. Significantly, we added a global health and development team, currently supervised by Jason Schukraft, which will explore global health and climate change interventions that might be competitive with GiveWell\u2019s top charities. We\u2019ve already begun this work by looking into interventions like <a href=\"https://rethinkpriorities.org/publications/global-lead-exposure-report\">lead reduction campaigns</a> and <a href=\"https://rethinkpriorities.org/publications/intervention-report-charter-cities\">charter city development</a>.</p>\n<p>We also expanded our animal welfare team, integrating much of The Humane League Labs (the research wing of The Humane League) and adding new staff \u2014 a staff entomologist, a new research manager for wild animal welfare, and an additional researcher. In addition, we launched a major expansion of our moral weight research, working with dozens of academic collaborators to measure capacity for welfare of different animal species, and try to establish reasonable interspecies moral weights.</p>\n<p>We added two additional survey team staff members to help us tackle the growing number of requests we\u2019ve received to conduct public opinion surveys, and to expand our ability to analyze annual projects like the EA Survey.</p>\n<p>And finally, we are currently in the process of hiring researchers in longtermism and AI governance and strategy, who will help expand our research in both fields. \\</p>\n<p>Here\u2019s an outline of all of the hires we added this year.</p>\n<h3>Animal welfare</h3>\n<ul>\n<li><strong>Meghan Barrett</strong> - Entomology Specialist. Meghan is a PhD candidate in Biology at Drexel University. She is also earning an MS in Teaching, Learning, and Curriculum, with a concentration in Higher Education. Her research interests lie at the intersection of social insect biology, neuroecology, and thermal physiology. She has worked on a wide range of study systems, including termites, bees, ants, wasps, spiders, fruit flies, and wood roaches. She works on projects related to invertebrate welfare and sentience at Rethink Priorities.</li>\n<li><strong>William McAuliffe</strong> - Senior Research Manager for the wild animal welfare team. William earned his PhD in Psychology at the University of Miami, where he researched the evolution of cooperation in humans. Before joining RP, he was a postdoctoral scholar at Harvard Medical School and Cambridge Health Alliance.</li>\n<li><strong>Samara Mendez</strong> - Senior Researcher. Samara holds a PhD in Economics from the University of Colorado Boulder. Her research lies at the intersection of agricultural economics, industrial organization, and public policy. She studies the effects of economic policies on participants in food systems, with a special focus on the impact of these policies in industries with market power and/or incomplete information issues. Before joining RP, Samara worked at The Humane League.</li>\n<li><strong>Jacob Peacock</strong> - Senior Research Manager. Jacob previously served as the Director of The Humane League Labs, where he studied plant-based milk alternatives, education to reduce meat consumption, and global cage-free campaigns. He earned a degree in Computational Biology from Rutgers University and is an advocate for open science.</li>\n<li><strong>Michael St. Jules</strong> - Associate Researcher. Michael has recently held internships in animal welfare at Charity Entrepreneurship, Animal Charity Evaluators, and Rethink Priorities. Before these, he worked as a deep learning research engineer and received master's degrees in Mathematics and Computational Mathematics.</li>\n</ul>\n<h3>Global health and development</h3>\n<ul>\n<li><strong>Ruby Dickson</strong> - Researcher. Ruby has a background in economics, policy, and development, and recently completed an MPhil in Economics at the University of Oxford. Before coming to RP, she worked in strategy consulting and policy research, with a focus on impact measurement and program evaluation.</li>\n<li><strong>Greer Gosnell</strong> - Senior Environmental Economist. Before joining RP, Greer earned her doctorate and completed a postdoctoral fellowship in Environmental Economics from the London School of Economics and Political Science. Greer is an open science advocate and field experimentalist specializing in rigorously testing the impact of policies and practices on environmentally relevant behavior and decision-making.</li>\n<li><strong>Jenny Kudymowa</strong> - Researcher. Jenny has a background in economics with a specialization in global health, development, and impact evaluation. Before coming to RP, she was a PhD candidate at Erasmus University Rotterdam and Tinbergen Institute and worked as a policy evaluation consultant for the Dutch Ministry of Foreign Affairs. In her work and research, she investigated how to improve healthy behaviors in low- and middle-income countries.</li>\n<li><strong>Bruce Tsai</strong> - Researcher. Before joining us at Rethink Priorities, Bruce\u2019s interest in global health led him to intern with the World Health Organization on universal health coverage and do work engaging with the United Nations. Bruce has previous experience ranging from entrepreneurship and innovation processes to grassroots activism. His recent research interests revolve around end-of-life care issues and uncertainties in accelerated COVID-19 vaccine pathways. Bruce is currently completing his medical degree and serves as an executive board member of the New Zealand Climate &amp; Health Council.</li>\n</ul>\n<h3>Surveys and EA movement research</h3>\n<ul>\n<li><strong>Jamie Elsey</strong> - Senior Behavioral Scientist. Jamie has a background in psychology and neuroscience. He earned a PhD in Clinical Psychology from the University of Amsterdam, where he investigated novel approaches to the treatment of anxiety disorders. His previous research work has also included the development of tools for statistical inference, online interventions, discussion of drug and food policies, and neuroethics.</li>\n<li><strong>Willem Sleegers</strong>- Senior Behavioral Scientist. Willem conducts research on attitude assessments and attitude change, using surveys and experimental designs. He is a strong supporter of and contributor to open science. Before joining RP, he worked as an Assistant Professor at Tilburg University, where he also obtained his PhD.</li>\n</ul>\n<h3>Operations and development</h3>\n<ul>\n<li><strong>Romina Giel</strong> - Operations Associate. Romina is driven by her commitment to social justice, and has held several roles at nonprofits dedicated to creating more equitable cities and to ending the abuse of animals raised on factory farms. Most recently, she worked for The Humane League\u2019s Open Wing Alliance, where she organized some of the animal welfare movement\u2019s largest international conferences.</li>\n<li><strong>Dominika Krupocin</strong> - Senior People and Culture Coordinator. Dominika holds a PhD in Security Studies with specialization in cultural and national security. Dominika is interested in EA community building: she is an organizer for EA Philadelphia and also volunteers with Giving What We Can.</li>\n<li><strong>Carolina Salazar</strong> - Project and Hiring Manager. Carolina studied literature and environmental ethics at Universidad de Monterrey. She has served various roles at NGOs dedicated to biodiversity conservation and farmed animal rights, and she is an active supporter of social justice movements such as representation, equity, and inclusion, and EA. She works on supporting and advancing talent on the RP team.</li>\n</ul>\n<h2>Internships and fellowships</h2>\n<p>Our internship/fellowship program welcomed 11 aspiring researchers in 2021, who were given the chance to build their skills, test their fit, and strengthen their career networks. Several of these interns went on to immediately take high-impact jobs at EA organizations, including Founders Pledge, Centre for Effective Altruism, and here at Rethink Priorities.</p>\n<p>This program also helped our own staff build research management capacity, and gave our operations team an opportunity to stress test processes to ensure we could handle planned growth.</p>\n<p>We plan to continue running the program in future years (or otherwise provide a valuable entry-level EA position), though we may reduce the total number of interns/fellows.</p>\n<h1>Our work and impact in 2021</h1>\n<h2>Summary of our 2021 achievements</h2>\n<p>We launched a new <a href=\"https://www.rethinkpriorities.org/research\">research database</a>, where you can browse all our published reports.</p>\n<p>We\u2019ve also achieved several milestones and breakthroughs across all four of our cause areas:</p>\n<h3>Our <a href=\"https://rethinkpriorities.org/animal-welfare\">Animal Welfare</a> department</h3>\n<ul>\n<li>Mapped opportunities to improve <a href=\"https://forum.effectivealtruism.org/posts/JsH7jWaDYRiX8xwZz/strategic-considerations-for-upcoming-eu-farmed-animal\">European Union (EU) farmed animal protection policies</a> with negotiations starting in late 2023, moving several million dollars.</li>\n<li>Highlighted the challenges for cost-effective <a href=\"https://forum.effectivealtruism.org/posts/y8jHKDkhPXApHp2gb/cultured-meat-a-comparison-of-techno-economic-analyses\">cultured meat</a> technologies.</li>\n<li>Set up an ambitious project to study the capacity for welfare of different species, which will inform prioritization of philanthropic spending between nonhuman animals, and potentially between humans and nonhuman animals as well. The work of this team on moral weight, which follows <a href=\"https://rethinkpriorities.org/animal-welfare\">our work from 2020</a>,  comprises three researchers and another seven contractors with academic backgrounds in philosophy, economics, comparative psychology, entomology, and neuroethology.</li>\n<li>Continued work on welfare of invertebrates such as insects and shrimps, drawing attention to these neglected species farmed by the trillions. Based on forthcoming work, we believe wild-caught shrimp may significantly outnumber all other animals slaughtered for use by humans combined.</li>\n<li>Continued to examine tractable interventions to improve wild animal welfare that could be robustly positive in impact, and could potentially demonstrate techniques on a small scale that could be important at a larger scale in the future.</li>\n</ul>\n<h3>Our <a href=\"https://rethinkpriorities.org/global-health-and-development\">Global Health and Development</a> department</h3>\n<ul>\n<li>Published a report showing that interventions to <a href=\"https://forum.effectivealtruism.org/posts/naTwu3xD3WFWu5fbp/global-lead-exposure-report\">reduce exposure to lead</a> could be as or more cost-effective than interventions ranked most highly by GiveWell.</li>\n<li>Discussed the promise and limitations of <a href=\"https://forum.effectivealtruism.org/posts/EpaSZWQkAy9apupoD/intervention-report-charter-cities\">charter cities</a> to foster economic development.</li>\n<li>Looked into <a href=\"https://forum.effectivealtruism.org/posts/LempBhdJe6HzwtDxd/intervention-report-agricultural-land-redistribution\">agricultural land redistribution</a> and its challenges.</li>\n</ul>\n<h3>Our <a href=\"https://rethinkpriorities.org/longtermism\">Longtermism</a> department</h3>\n<ul>\n<li>Continued work analyzing <a href=\"https://forum.effectivealtruism.org/posts/HGWvBXZpHwk5TSvg8/overview-of-rethink-priorities-work-on-risks-from-nuclear\">risks from nuclear weapons</a>.</li>\n<li>Tried to understand the accuracy of <a href=\"https://forum.effectivealtruism.org/users/charles_dillon\">forecasting</a> for effective altruist goals and find ways to make more accurate forecasts.</li>\n<li>Is setting up a team to study <a href=\"https://forum.effectivealtruism.org/posts/3vXXthjBKhNo8sgFv/job-ad-research-important-longtermist-topics-at-rethink\">AI governance and strategy</a>.</li>\n</ul>\n<h3>Our <a href=\"https://rethinkpriorities.org/ea-movement-research\">EA Movement Research</a> and Surveying Department</h3>\n<ul>\n<li>Analyzed data and developments in the effective altruist community through the <a href=\"https://forum.effectivealtruism.org/tag/effective-altruism-survey\">EA Survey</a>.</li>\n<li>Conducted several polls and surveys to inform policy groups as well as the effective animal advocacy movement.</li>\n</ul>\n<p>The results from several commissioned research projects were shared with the organizations who requested them. We do surveys across multiple cause areas.</p>\n<h2>Our impact in 2021</h2>\n<p>We are internally driven by a motto that \u201cgood research is not enough\u201d and <strong>created a strategy to ensure our research gets put into action:</strong></p>\n<ul>\n<li>We\u2019ve begun hiring for a team to explore questions related to the governance of transformative artificial intelligence to inform the strategy of Open Philanthropy.</li>\n<li>The data and analyses of the EA Survey and the EA Groups Survey we conduct <a href=\"https://rethinkpriorities.org/ea-movement-research\">inform movement building strategy</a> at the Centre for Effective Altruism and Open Philanthropy as well as career advice at 80,000 Hours.</li>\n<li>We produced three research reports for Open Philanthropy assessing the potential of <a href=\"https://rethinkpriorities.org/global-health-and-development\">global health and development interventions</a>, looking for interventions that could be as or more cost-effective as the ones currently ranked top by GiveWell.</li>\n<li>We informed the animal welfare strategy at Open Philanthropy and other large US-based foundations.</li>\n<li>Surveys and polls we conducted for several animal advocacy organizations were able to test public support for corporate campaigns and policy proposals.</li>\n<li>We informed dozens of European animal advocacy organizations about opportunities to improve <a href=\"https://forum.effectivealtruism.org/posts/JsH7jWaDYRiX8xwZz/strategic-considerations-for-upcoming-eu-farmed-animal\">EU farmed animal protection policies</a>, moving several million dollars.</li>\n<li>Our analysis of studies about <a href=\"https://forum.effectivealtruism.org/posts/y8jHKDkhPXApHp2gb/cultured-meat-a-comparison-of-techno-economic-analyses\">cost-effective cultured meat technologies</a> informed funders and advocates in the alternative protein space.</li>\n<li>For <a href=\"https://forum.effectivealtruism.org/posts/HGWvBXZpHwk5TSvg8/overview-of-rethink-priorities-work-on-risks-from-nuclear\">our work on nuclear security</a>, we engaged with relevant organizations and funders.</li>\n<li>We provided analyses about the accuracy of <a href=\"https://forum.effectivealtruism.org/users/charles_dillon\">forecasting</a> to the effective altruist and rationalist communities.</li>\n<li>We offered visiting fellowships to 11 aspiring researchers to test their fit for a career in EA-aligned research. We helped build the talent pipeline into work on global priorities.</li>\n</ul>\n<p>As a research organization, we\u2019re usually a step or two removed from direct work, and thus it can be challenging to determine what impact on the world we\u2019re having. We\u2019re very interested in ascertaining how those in a position to implement are acting on our work, if at all, and we\u2019re committed to tracking our impact in multiple ways.</p>\n<h3>Impact survey qualitative interviews</h3>\n<p>Over the past month we\u2019ve conducted structured interviews with key decision-makers and leaders at EA organizations that either use our work or that we want to use our work. We sought interviewees\u2019 feedback on the general importance of our work for them and for the community, what they have and have not found helpful in what we\u2019ve done, what we can do in the future that would be useful for them, and ways we can <a href=\"http://improve.To\">improve.To</a> encourage frankness, interviewees were promised that the details of these conversations would not be made public. Some interviews are still forthcoming, so these are tentative conclusions.</p>\n<p>Overall we found that all of our stakeholders do read at least some of our research and consider at least some of it to be generally useful. For ~50% of our stakeholders, we did successfully help them improve the quality of at least one important decision. For ~25% of our stakeholders, we had not yet improved any of their decisions with our work because it is too early in our relationship and we had not yet produced any relevant work. For the remaining 25% of our stakeholders, we have so far failed to be useful to them in the way we had hoped.</p>\n<h3>Areas for improvement</h3>\n<p>According to the stakeholders we interviewed, there is still an unmet desire for us to directly present our work to organizations and to funders. Despite our efforts to better summarize our work and key takeaways over the last year (a focus from our interviews last year), some think there\u2019s still more to be gained from directly presenting our work to them, such as through webinars or other presentations. We\u2019re currently hiring a communications coordinator, who will work in part on more direct presentations of our work to organizations and funders.</p>\n<p>Another area of feedback given to us was that we sometimes spend a lot of research time working on analysis that does not end up being relevant to any particular important decisions. While we think research is inherently exploratory and there are going to be inevitable dead-ends, we are going to work more on considering more clearly what parts of our work we ought to invest a lot of time in to get right and what parts of our work can be cut or done more quickly. We will also work more closely and more frequently with our stakeholders and target audiences to ensure that we are working on useful things.</p>\n<h1>Our goals and funding needs for 2022</h1>\n<h2>Project plans</h2>\n<h3>Longtermism</h3>\n<ul>\n<li>Our main focus will be establishing a strong team of permanent researchers and rotating fellows tackling AI governance projects on behalf of Open Philanthropy.</li>\n<li>Simultaneously, we will figure out possible additional research directions for the non-AI portion of our longtermism team. This may include continuing existing work on nuclear risks and meta-forecasting and/or exploring a new area.</li>\n</ul>\n<h3>Surveys and EA movement research</h3>\n<ul>\n<li>We will use surveys, message testing, and focus groups to explore how to best talk about longtermism and effective altruism with the general public and interested policymakers.</li>\n<li>We will run another iteration of the EA Survey and continue to invest in understanding our movement.</li>\n<li>We will continue to have capacity - especially after Q1 2022 - to do bespoke survey work on behalf of the EA community and EA-adjacent organizations. We do surveys across all cause areas.</li>\n</ul>\n<h3>Global health and development</h3>\n<ul>\n<li>We will continue to do contracted research for Open Philanthropy and explore opportunities to conduct actionable research for other organizations, as well as explore working with other large funders in the space.</li>\n<li>Our goal remains to find competitive giving opportunities that beat GiveWell\u2019s top charities from an expected value / \u201chits-based\u201d perspective (though perhaps with less rigorously established evidence bases and lower certainty of impact).</li>\n</ul>\n<h3>Animal welfare</h3>\n<ul>\n<li>We will continue to directly support grantmakers and organizations in the farmed animals space by providing decision-relevant analyses of current interventions, working to identify new promising interventions, and providing missing theoretical inputs for their work.</li>\n<li>We expect our initial results on interspecies comparisons of moral weight to be complete by the end of 2022.</li>\n<li>We will develop and validate measures of individuals\u2019 attitudes toward the welfare of wild animals. We\u2019ll apply these findings to our exploratory analyses of possible interventions in this space to work toward our goal of identifying interventions that could be tractable, non-controversial, and reasonably cost-effective (in the short to medium term).</li>\n<li>We will publish our scoping report on farmed shrimp welfare by the end of March 2022, which explores the scale of the industry, conditions, and possible interventions.</li>\n<li>We will continue to assess whether there are any viable interventions in the invertebrate welfare space.</li>\n</ul>\n<h2>Funding status</h2>\n<p>We have set a 2022 goal to raise at least $5,435,000, and as much as $12,990,000. This will enable us to:</p>\n<ul>\n<li>Continue to fund our existing team and projects in all cause areas, maintaining 12 months\u2019 reserves.</li>\n<li>Continue our work in animal welfare, including exploring wild animal welfare interventions, evaluating farmed animal welfare improvements, and doing fundamental work to understand how we should approach comparing and weighing the interests of different animal species.</li>\n<li>Ramp up our global health and development research, exploring several promising global health interventions, and studying how EAs might approach addressing climate change.</li>\n<li>Build up our longtermist research team and explore more longtermist research directions.</li>\n<li>Build a communications team to better reach people who will use it, getting more impact out of our research.</li>\n</ul>\n<p>Our budget will be allocated as shown below in low, high, and maximally ambitious scenarios.</p>\n<p><strong>Budget - Low</strong><sup class=\"footnote-ref\"><a href=\"#fn-MHixtpTXNhMy53zdf-2\" id=\"fnref-MHixtpTXNhMy53zdf-2\">[2]</a></sup></p>\n<p><img src=\"https://i.ibb.co/tZ1qLQ8/Screen-Shot-2021-11-15-at-10-03-41-AM.png\" alt=\"\"></p>\n<p><strong>Budget - High</strong></p>\n<p><img src=\"https://i.ibb.co/C8nhsgg/Screen-Shot-2021-11-15-at-10-04-27-AM.png\" alt=\"\"></p>\n<p><strong>Budget - Maximally ambitious</strong></p>\n<p><img src=\"https://i.ibb.co/HF4QrK2/Screen-Shot-2021-11-15-at-10-05-39-AM.png\" alt=\"\"></p>\n<p><strong>Room for more funding in 2022</strong></p>\n<p><img src=\"https://i.ibb.co/B2b2JrN/Screen-Shot-2021-11-15-at-10-06-13-AM.png\" alt=\"\"></p>\n<p>Meeting our low budget scenario will allow us to continue our projects with our current staff and some small already-planned expansion.</p>\n<p>The high budget scenario will enable us to deploy greater research capacity by hiring about a dozen additional staff to address some of the most important questions we\u2019ve identified. We are highly confident that we could effectively deploy funding at this level to build up our organization for sustainable impact moving forward.</p>\n<p>Our maximally ambitious scenario would enable RP to scale at the maximum rate we think is possible. This plan involves:</p>\n<ul>\n<li>Hiring additional operations staff in 2022 to prepare for a much larger research team, then adding several dozen researchers over the remainder of 2022 and early 2023.</li>\n<li>Adding a new department (listed in the budgets temporarily under \u201cEA movement research\u201d) focused on investigating the consequences on EA approaches of adapting various worldviews (such as how various species of animals are valued, or how we weigh the interests of far-future people against uncertainty in our ability to help them).</li>\n<li>Significantly building our longtermism team.</li>\n</ul>\n<p>We are less confident of our ability to execute the maximally ambitious plan, but it does feel at least moderately plausible, and we think this number represents a good view for the maximum amount of money we could possibly put to productive use, such that any money raised beyond this amount would very likely be deferred to cover spending in 2024 or later.</p>\n<p>For context, a new researcher costs us on average around $130,000 per year (including all benefits, fees, taxes, and necessary operations support).</p>\n<p>We\u2019d be happy to discuss the details of how each of these budget levels would unfold with funders upon request.</p>\n<h2>Reasons to fund Rethink Priorities</h2>\n<ul>\n<li>Each year we have many well-qualified applicants that we would like to hire but that we do not, mostly due to lack of funds. With every new $130,000, we can hire one new researcher (including all benefits, fees, taxes, and necessary operations support). Additional funding will make a key difference in how much Rethink Priorities grows over the next two years.</li>\n<li>We have a track record of producing actionable research that has informed decisions worth millions of dollars. Our work amplifies the impact of several key effective altruist and longtermist organizations.</li>\n<li>Rethink Priorities has been <a href=\"https://funds.effectivealtruism.org/funds/payouts/may-2021-animal-welfare-fund-grants#rethink-priorities-225000\">trusted by EA Funds</a> and <a href=\"https://forum.effectivealtruism.org/posts/CwFyTacABbWuzdYwB/ea-needs-consultancies#Current_state_of_EA_consultancies\">Open Philanthropy</a> to start new projects (e.g., on capacity for welfare of different animal species) and open entire new departments (such as AI governance).</li>\n<li>These and other large organizations often only fund 25\u201350% of our needs in any particular area because they trust our ability to find other sources of funding. Therefore we rely on a broad range of individual donors to continue our work.</li>\n<li>Support outside of our main funders will provide more resilience against the risk of a major funder changing direction, as well as more independence to pursue our research agenda without the fear of a big funder pulling out. Unrestricted funding in particular is immensely valuable for us building a robust, stable, and effective organization. However, we do accept and track restricted funds by cause area if requested by donors.</li>\n<li>We provide value to the entire EA community through public analyses, the EA Survey, helping people with ad hoc analysis requests, and training new EA researchers. However, such benefits are hard to fundraise for: they don\u2019t help any one particular funder enough to make them want to fund this work, and the primary beneficiaries often have limited ability to support us. A diverse donor base will help us show that there is strong support for these community-wide benefits we provide, and will also keep us accountable to continue delivering value to the EA community as a whole in addition to our more specific stakeholders.</li>\n</ul>\n<h2>How to give</h2>\n<p>We believe we are entering 2022 prepared to do more important research than ever before, and with the ability to continue growing. We are excited about where we could go with your support.</p>\n<p>If you\u2019d like to help fund our work, you can donate to us as part of <a href=\"https://www.eagivingtuesday.org/\">Facebook\u2019s donation matching on Giving Tuesday on November 30</a>, or <a href=\"https://www.rethinkpriorities.org/donate\">donate directly to us here</a>. If you have questions about tax-deductibility in your country or are interested in making a major gift, please contact our Director of Development <a href=\"mailto:janique@rethinkpriorities.org\">Janique Behman</a>.</p>\n<h1>Credits</h1>\n<p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995157/mirroredImages/K7tjvcDurrCj72D7H/vkgmmdsq5orpyrtwtjlp.png\" alt=\"\"></p>\n<p>This post is a project of <a href=\"%5Bhttp://www.rethinkpriorities.org/%5D(http://www.rethinkpriorities.org/)\">Rethink Priorities</a>.</p>\n<p>It was written by Marcus A. Davis, Peter Wildeford, Abraham Rowe, and Janique Behman. Thanks to Katy Moore for copyediting.</p>\n<p>If you like our work, please consider <a href=\"https://www.rethinkpriorities.org/newsletter\">subscribing to our newsletter</a>. You can <a href=\"https://www.rethinkpriorities.org/research\">see more of our work here</a>.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-MHixtpTXNhMy53zdf-1\" class=\"footnote-item\"><p>We also work extensively with external collaborators. By the end of 2021, we expect to have around 3.1 FTE-years of research completed by collaborators, primarily as part of our two-year moral weight research project. We worked with 17 collaborators in 2021. <a href=\"#fnref-MHixtpTXNhMy53zdf-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-MHixtpTXNhMy53zdf-2\" class=\"footnote-item\"><p>All budgets include overhead costs, including administrative expenses, communications, and fundraising costs. <a href=\"#fnref-MHixtpTXNhMy53zdf-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Peter_Hurford"}}, {"_id": "KNMRojuJgSjGth7m2", "title": "GiveWell\u2019s money moved in 2020", "postedAt": "2021-11-15T15:28:45.652Z", "htmlBody": "<p>2020 was another year of tremendous growth. GiveWell donors contributed over $240 million to our recommended charities (our \u201c2020 money moved\u201d), a 60% increase from the approximately $150 million we directed in 2019. This is part of an exciting, long-term trend. Just a decade ago, in 2010, GiveWell\u2019s total money moved was $1.5 million.<sup class=\"footnote-ref\"><a href=\"#fn-DGGhMLenxPA7Qkq8x-1\" id=\"fnref-DGGhMLenxPA7Qkq8x-1\">[1]</a></sup></p>\n<p>We believe these donations will save tens of thousands of lives and benefit many others. This incredible impact would not be possible without the continued support and generosity of our donors. While our research enables us to identify and recommend highly cost-effective giving opportunities, our donors are responsible for turning those recommendations into real change for some of the poorest individuals in the world.</p>\n<p>This post lays out highlights from <a href=\"https://blog.givewell.org/files/metrics/GiveWell_Metrics_Report_2020.pdf\">our final 2020 money moved report</a> and shares more details about how donors gave to GiveWell\u2019s recommended charities in 2020.<sup class=\"footnote-ref\"><a href=\"#fn-DGGhMLenxPA7Qkq8x-2\" id=\"fnref-DGGhMLenxPA7Qkq8x-2\">[2]</a></sup></p>\n<p><strong>Summary of influence:</strong> In 2020, GiveWell influenced charitable giving in several ways. The following table summarizes our understanding of this influence.</p>\n<p><img src=\"https://blog.givewell.org/files/metrics/Money_moved_summary_table_2020.png\" alt=\"\"></p>\n<p><strong>Headline money moved:</strong> In 2020, we confidently tracked $244 million in money moved to our <a href=\"https://www.givewell.org/charities/top-charities\">recommended charities</a>, and via our <a href=\"https://www.givewell.org/research/incubation-grants\">GiveWell Incubation Grants</a> program. This amount, which we call \u201cheadline money moved,\u201d only counts donations that we are confident were influenced by our recommendations. This includes the grants we make through the <a href=\"https://www.givewell.org/maximum-impact-fund\">Maximum Impact Fund</a>. See Appendix 1 of our <a href=\"https://blog.givewell.org/files/metrics/GiveWell_Metrics_Report_2020.pdf\">2020 metrics report</a> for additional details on how we calculate our money moved.</p>\n<p>We also estimate that we are responsible for an additional $3 million in donations, but we are unable to attribute these donations directly to GiveWell. Because we are more uncertain about this influence, we do not include this amount in our \u201cheadline money moved\u201d figure but include it in our \u201cbest guess of total money directed to charities\u201d figure.<sup class=\"footnote-ref\"><a href=\"#fn-DGGhMLenxPA7Qkq8x-3\" id=\"fnref-DGGhMLenxPA7Qkq8x-3\">[3]</a></sup></p>\n<p>The chart below shows the breakdown of our headline money moved into the following categories: grants that Open Philanthropy made to our recommended charities, donations from other donors to our recommended charities, and Incubation Grants. Please note that Open Philanthropy support (marked in gray) does not include funding it provided for GiveWell Incubation Grants, which are shown separately in purple.<sup class=\"footnote-ref\"><a href=\"#fn-DGGhMLenxPA7Qkq8x-4\" id=\"fnref-DGGhMLenxPA7Qkq8x-4\">[4]</a></sup></p>\n<p><img src=\"https://blog.givewell.org/files/metrics/Money_moved_by_category_2020.png\" alt=\"\"></p>\n<p><strong>Money moved by charity (excluding Incubation Grants):</strong> Our nine top charities received the majority of our money moved. Our nine standout charities received a total of $2.2 million. Note that as of October 2021, we have discontinued the standout charity designation.<sup class=\"footnote-ref\"><a href=\"#fn-DGGhMLenxPA7Qkq8x-5\" id=\"fnref-DGGhMLenxPA7Qkq8x-5\">[5]</a></sup></p>\n<p><img src=\"https://blog.givewell.org/files/metrics/Money_moved_by_charity_chart_2020.png\" alt=\"\"></p>\n<p><strong>Money moved by program (excluding Incubation Grants):</strong> Our recommended charities implement a variety of health and poverty alleviation programs. But some charities work on the same type of program. For example, we recommend four charities for their programs that support treatments for parasitic worm infections (deworming programs), and two charities for their programs to prevent malaria (Malaria Consortium\u2019s seasonal malaria chemoprevention program and the Against Malaria Foundation). Here, we look at the breakdown of money moved by program type.</p>\n<p>The majority of our money moved, including donations to our Maximum Impact Fund, was directed to malaria prevention programs\u2014followed by unconditional cash transfers, conditional cash transfers to promote vaccination, deworming, and vitamin A supplementation. Other programs each received less than 1% of our total money moved.</p>\n<p><img src=\"https://blog.givewell.org/files/metrics/Money_moved_by_program_2020.png\" alt=\"\"></p>\n<p><strong>Money moved by size of donor:</strong> We also analyze our money moved by the amount that different donors give, which we categorize into six different \u201csize buckets\u201d (see the chart below, which excludes funding from Open Philanthropy).</p>\n<p>A caveat: Our analysis of money moved by donor size is incomplete because for approximately 39% of donations (excluding Open Philanthropy), we do not have data disaggregated by individual donor. Among the donations we <em>can</em> attribute to individual donors, the amount of money given increased across all donor size categories compared to 2019. Details are available in <a href=\"https://blog.givewell.org/files/metrics/GiveWell_Metrics_Report_2020.pdf\">the full report</a>.</p>\n<p><img src=\"https://blog.givewell.org/files/metrics/Money_moved_by_donor_size_(amount_donated)_2020.png\" alt=\"\"></p>\n<p><img src=\"https://blog.givewell.org/files/metrics/Money_moved_by_donor_size_(percentage_of_donors)_2020.png\" alt=\"\"></p>\n<p><strong>Donations supporting GiveWell\u2019s operations:</strong> GiveWell raised $43.6 million in unrestricted funding in 2020, compared to $19 million in 2019. Donors who gave over $100,000, including Open Philanthropy, contributed around 84% of GiveWell\u2019s unrestricted funding in 2020. GiveWell\u2019s total operating expenses in 2020 were $8.5 million.</p>\n<p>We have only retained a portion of our unrestricted 2020 revenue for operating costs, and will be reallocating the remainder to discretionary grantmaking.<sup class=\"footnote-ref\"><a href=\"#fn-DGGhMLenxPA7Qkq8x-6\" id=\"fnref-DGGhMLenxPA7Qkq8x-6\">[6]</a></sup></p>\n<p>For more detail, see our &gt;<a href=\"https://blog.givewell.org/files/metrics/GiveWell_Metrics_Report_2020.pdf\">full metrics report (PDF)</a>.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-DGGhMLenxPA7Qkq8x-1\" class=\"footnote-item\"><p>See the blog post on our 2010 money moved <a href=\"https://blog.givewell.org/2011/02/08/stats-on-givewells-money-moved-and-web-traffic/\">here</a>. <a href=\"#fnref-DGGhMLenxPA7Qkq8x-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-DGGhMLenxPA7Qkq8x-2\" class=\"footnote-item\"><p>Please note:</p>\n<ul>\n<li>We report on \"metrics years\" that run from February through January; for example, our 2020 data cover February 1, 2020 through January 31, 2021.</li>\n<li>In an effort to present a more comprehensive measure of our influence on charitable giving, we included <a href=\"https://www.givewell.org/research/incubation-grants\">GiveWell Incubation Grants</a> in our headline \"money moved\" figure for our 2018 and 2019 metrics reports and continue to do so for our 2020 report. In previous reports, we excluded Incubation Grants from this figure.</li>\n</ul>\n <a href=\"#fnref-DGGhMLenxPA7Qkq8x-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-DGGhMLenxPA7Qkq8x-3\" class=\"footnote-item\"><p>This $3 million comes from the approximately $6 million of donations for which we believe our research played an important role. We only count 50% of these donations in our \"best guess of total money directed to charities\" because we believe our research was only partially responsible for these donations. <a href=\"#fnref-DGGhMLenxPA7Qkq8x-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-DGGhMLenxPA7Qkq8x-4\" class=\"footnote-item\"><p>In previous years, Open Philanthropy has been the primary funder of GiveWell Incubation Grants. However, in 2020, other donors constituted 47% of total Incubation Grant funding. <a href=\"#fnref-DGGhMLenxPA7Qkq8x-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-DGGhMLenxPA7Qkq8x-5\" class=\"footnote-item\"><p>See more details about this decision <a href=\"https://blog.givewell.org/2021/10/05/discontinuing-standout-charity-designation/\">here</a>. <a href=\"#fnref-DGGhMLenxPA7Qkq8x-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-DGGhMLenxPA7Qkq8x-6\" class=\"footnote-item\"><p>We often reallocate unrestricted funds due to our <a href=\"https://www.givewell.org/about/official-records/Excess-Assets-Policy\">excess assets policy</a> and <a href=\"https://www.givewell.org/donate/more-information#How_is_GiveWell_supported\">cap on operating revenue from a single donor</a>. For additional details on unrestricted funds that were reallocated in 2020, see the <a href=\"https://www.givewell.org/files/metrics/GiveWell_Metrics_Report_2020.pdf\">full metrics report</a>. <a href=\"#fnref-DGGhMLenxPA7Qkq8x-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "GiveWell"}}, {"_id": "Ei2uYbn2zrzmBjEsp", "title": "Sleep: effective ways to improve it", "postedAt": "2021-11-15T14:42:30.967Z", "htmlBody": "<h1>Introduction</h1><p>Our research found that melatonin supplementation, cognitive behavioural therapy for insomnia (CBT-I), light regulation, mindfulness-based stress reduction (MBSR), and improved night-time air circulation could all be effective ways for you to increase the quality and quantity of your sleep.</p><p>This is a summary of research conducted into the most worthwhile practices for sleeping better. The post is the first in a <a href=\"https://forum.effectivealtruism.org/posts/hdfMrY4QGEKaoFubH/shortform\">series</a> looking into the most effective ways people can improve their wellbeing, aiming to present this information as simply and practically as possible.</p><p>Thanks to the EA Infrastructure Fund for financing this project. If you find this information useful and/ or take up any of the suggestions, please let me know in the comments or a personal message!</p><p><i><strong>Important note:</strong> None of the following constitutes professional medical advice. Some of the interventions suggested have risks of negative side effects that are discussed below. We encourage you to experiment with these practices but please be cautious in doing so and take any risks seriously.</i></p><h1>Top takeaway</h1><p>Our principal recommendations for improving sleep quality are (in order):</p><ol><li><strong>Melatonin supplements</strong>: 0.3mg (300mcg) daily taken two hours before bed.</li><li><strong>CBT-I</strong>: a six-to-seven-week, self-guided course in cognitive behavioural therapy for insomnia, accessed through an app.</li><li><strong>Light therapy</strong>: greatly increasing exposure to bright light during the day, either through building a lumenator or purchasing a SAD lamp.</li><li><strong>Improved night-time air circulation</strong>: opening a window to reduce overnight CO<sub>2</sub> accumulation.</li><li><strong>Mindfulness-based stress reduction</strong>: mindfulness training through an app with a focus on sleep.</li></ol><h1>Key Findings</h1><p>Following a broad search of possible interventions, this review evaluated the effectiveness of 11 practices for improving sleep quality and quantity using an adapted weighted factor model.<a href=\"#fn-m9Fjo6beDKbjvbwKz-1\"><sup>[1]</sup></a></p><p>The five most promising interventions are listed below along with a brief explanation. More detailed assessments of all 11 practices are provided further down.</p><p>This research compared interventions across six criteria: strength of evidence, quality of evidence, ease of implementation, risk, externalities, and novelty. Full results from the model, along with reasoning for the metrics used and their respective weightings, can be found <a href=\"https://docs.google.com/spreadsheets/d/1dnLsFoGf_6aT-fDnbpNQiRur1SE698E2WI4hWdRM59E/edit?usp=sharing\">here</a>.</p><h2><strong>Melatonin</strong></h2><p><strong>Ranking</strong>: 1<sup>st</sup></p><p><strong>Intervention</strong>: 0.3mg daily supplementation (e.g. Options <a href=\"https://www.sundownnutrition.com/products/melatonin-300-mcg-tablets/\">A</a>, <a href=\"https://www.carethy.co.uk/nutrition-and-supplements/life-extension/p-461770\">B</a> and <a href=\"https://www.lifeextensioneurope.com/melatonin-300-mcg\">C</a>)</p><p><strong>Summary</strong>: Melatonin is a natural sleep hormone that can <a href=\"https://pubmed.ncbi.nlm.nih.gov/8843534/\">improve sleep quality and quantity</a> when taken in small, daily doses. Melatonin supplementation appears highly practical, with a low risk of notable side effects as well as possible spillover benefits for other conditions.</p><h2><strong>CBT-I</strong></h2><p><strong>Ranking:</strong> 2<sup>nd</sup></p><p><strong>Intervention</strong>: Completion of an app-based CBT-I course (e.g. <a href=\"https://www.dozy.health/index.html\">Dozy</a>; <a href=\"https://play.google.com/store/apps/details?id=gov.va.mobilehealth.ncptsd.cbti&amp;hl=en&amp;gl=US\">CBT-I Coach</a>)</p><p><strong>Summary</strong>: Cognitive Behavioural Therapy for Insomnia (CBT-I) involves a combination of reframing negative thoughts around sleep, improving sleep hygiene, and <a href=\"https://johnhalstead.org/index.php/2020/10/11/how-to-cure-your-insomnia/\">implementing sleep restriction</a>. We found <a href=\"https://pubmed.ncbi.nlm.nih.gov/28400355/\">good evidence</a> that completing a self-guided CBT-I course via an app can be an effective way to sleep better.</p><h2><strong>Light therapy</strong></h2><p><strong>Ranking:</strong> 3<sup>rd</sup></p><p><strong>Intervention</strong>: Increasing indoor lighting brightness, preferably up to 10,000 lux <a href=\"https://www.medrxiv.org/content/10.1101/2021.10.29.21265530v1\">or more</a> (e.g. a <a href=\"https://arbital.com/p/lumenators/\">\u2018lumenator\u2019</a> or a <a href=\"https://www.amazon.com/s?k=sad+lamp&amp;crid=24IR27SPJK11A&amp;sprefix=sad%2Caps%2C271&amp;ref=nb_sb_ss_ts-doa-p_1_3\">Seasonal Affective Disorder [SAD] lamp</a>).</p><p><strong>Summary</strong>: Multiple studies have found that significantly increased exposure to <a href=\"https://pubmed.ncbi.nlm.nih.gov/26606319/\">bright morning light</a> improves sleep quality and quantity. We found negligible risks to this practice as well as possible benefits to mood and alertness, though this does require some time and money to set up.</p><h2><strong>Improved night-time air circulation</strong></h2><p><strong>Ranking</strong>: 4<sup>th</sup></p><p><strong>Intervention</strong>: Leaving a window or internal door open while sleeping.</p><p><strong>Summary</strong>: There is <a href=\"https://pubmed.ncbi.nlm.nih.gov/26452168/\">reasonable evidence</a> to suggest that high levels of carbon dioxide can accumulate overnight in a closed room and that this reduces sleep quality. Leaving a window open overnight is a simple and effective solution with minimal downsides.</p><h2><strong>Mindfulness-Based Stress Reduction (MBSR)</strong></h2><p><strong>Ranking</strong>: 5<sup>th</sup></p><p><strong>Intervention</strong>: Completing an app-based mindfulness-based stress reduction course (MBSR) (e.g. <a href=\"https://play.google.com/store/apps/details?id=com.calm.android&amp;hl=en_GB&amp;gl=US\">Calm</a>; <a href=\"https://play.google.com/store/apps/details?id=se.lichtenstein.mind.en&amp;hl=en_GB&amp;gl=US\">The Mindfulness App</a>)</p><p><strong>Summary</strong>: Mindfulness-based stress reduction courses are associated with improved total sleep time and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6614998/\">reduced sleep disturbance</a>. MBSR is not targeted at sleep as an intervention and likely offers <a href=\"https://pubmed.ncbi.nlm.nih.gov/22805898/\">significant additional benefits</a>.</p><h1>Why sleep matters</h1><p>The selection effects of choosing to read this article mean that you are likely already convinced that improving your sleep is a valuable use of your time. In case that is a poor assumption, this is a very brief argument in favour of the importance of sleep.</p><h3><strong>Sleep is fundamental to wellbeing</strong></h3><p><a href=\"https://www.cdc.gov/sleep/data_statistics.html\">35% of American adults</a> are estimated to sleep less than seven hours on average. This is considered sleep deprivation, with more than seven hours of sleep a night <a href=\"https://doi.org/10.5665/sleep.4886\">the consensus recommendation</a>.</p><p>Sleep deprivation is linked to a higher risk of many (perhaps most<a href=\"#fn-m9Fjo6beDKbjvbwKz-2\"><sup>[2]</sup></a>) negative health conditions, including <a href=\"https://doi.org/10.2174/157340310790231635\">hypertension, coronary heart disease, diabetes</a>, and <a href=\"https://doi.org/10.1111/j.1365-2869.2007.00569.x\">obesity</a>. It has also been directly linked to lower reported <a href=\"https://doi.org/10.1007/s11136-013-0475-9\">quality of life</a> and <a href=\"https://doi.org/10.1016/S0022-3999(97)00004-4\">wellbeing</a>.</p><p>This makes sleep one of the most effective interventions for improving other aspects of wellbeing. Though health issues like those above can in themselves reduce the quality of sleep, there seems to be a range of tractable ways in which to improve sleep regardless.</p><h3><strong>Modern living is not set up for ideal sleep</strong></h3><p>Perhaps some of the practices suggested here feel to you like an excessive intervention into the natural process of sleep, something that should not require supplements or training programmes. But in the excellent words of the writer <a href=\"https://www.gwern.net/Melatonin\">Gwern Branwen</a>, \u2018I would point out to such readers that they are <i>already</i> profoundly tampering with their natural sleep cycle, and indeed, all of Western civilization is tampering with it\u2019.</p><p>People tend to go to bed more than <a href=\"https://doi.org/10.1016/j.cub.2013.06.039\">an hour earlier each night</a> when taken away from any artificial light through the evening. Mobile phone use in the evening is <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7320888/\">a direct act</a> <a href=\"https://www.sciencedirect.com/science/article/pii/S0277953615302458?casa_token=3GmErXwz2bYAAAAA:OndEmwjj1Mz2vdBjtHZUE0ViYuYGaVPaVHc-0FmauCjVPiNji4xGdC1bTwSHajANAIcTN_wa61zM\">of sabotage</a> on the quality of your sleep.</p><h3><strong>EAs are smart people and have shown significant previous interest in sleep</strong></h3><p>There are eight previous articles on improving sleep on the Forum<a href=\"#fn-m9Fjo6beDKbjvbwKz-3\"><sup>[3]</sup></a>, as well as multiple high-quality posts on EA-related websites<a href=\"#fn-m9Fjo6beDKbjvbwKz-4\"><sup>[4]</sup></a>. Taking the premise that effective altruists are generally smart people, the prevalent interest in sleep among the EA community suggests it is a smart decision to take sleep seriously.</p><h1>Common (sense) interventions</h1><p>The focus of this research is to explore the effectiveness of practices that are likely to be new to the majority of readers. Given this, we have omitted the exploration of a few common suggestions for improving sleep that are nevertheless worthy of some mention:</p><ol><li>Use earplugs or a white noise machine if you frequently notice ambient noise when trying to fall asleep.</li><li>Install blackout blinds or wear an eye mask if you are often woken up early by ambient light.</li><li>Choose a realistic time to get up each day and stick to it as much as possible. Trying to catch up on sleep over the weekends <a href=\"https://pubmed.ncbi.nlm.nih.gov/30827911/\">is ineffective</a> and can be counter-productive in disrupting your circadian rhythm.</li><li>Minimise the use of your bed for anything that isn\u2019t sleeping.</li></ol><h1>Discussion of Primary Recommendations</h1><h2><strong>Melatonin</strong></h2><p><strong>Intervention</strong>: 0.3mg daily supplementation</p><p><strong>Score:</strong> 5.95<sup>/10</sup> (1<sup>st</sup>)</p><p>Melatonin is a hormone that triggers sleepiness and works to regulate sleep cycles. A small dose of melatonin taken daily (0.3mg, or 300mcg) produced a 4.8% increase in total sleep time (time spent in bed) and a 12.8% increase in sleep efficiency (the % of time spent actually asleep out of the time spent in bed) in <a href=\"https://academic.oup.com/jcem/article/86/10/4727/2849013#53493863\">this study</a>.</p><p>Most commercial melatonin supplements come in significantly larger doses - often 3, 5 or even 10mg. While these doses also appear reasonably effective, there is a greater risk of <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1600-079X.2011.00958.x\">negative side effects</a> from regularly taking melatonin in these larger quantities.</p><p>These side effects are mostly mild (headaches, dizziness, or drowsiness the next day) but can be more significant (nausea, diarrhoea, or worsened depression). It is also worth being cautious about taking melatonin if you already take regular medication.</p><p>There is some evidence to suggest that melatonin supplementation can <a href=\"https://www.webmd.com/vitamins/ai/ingredientmono-940/melatonin\">reduce blood pressure</a>. It has also been linked with an <a href=\"https://www.webmd.com/vitamins/ai/ingredientmono-940/melatonin\">improvement in symptoms</a> for people with certain chronic health conditions, including cancer and endometriosis. However, it seems plausible that these improvements are a result of improved sleep rather than any additional effects of melatonin.</p><p>Once you\u2019ve found a supplier of melatonin at the right dosage, this is a highly practical intervention. A year\u2019s supply of melatonin should not cost more than $30 or so. Taking a capsule each night in the hour before bed requires little time or motivation.</p><h2><strong>CBT-I (Cognitive Behavioural Therapy for Insomnia)</strong></h2><p><strong>Intervention</strong>: Completing a self-guided CBT-I course</p><p><strong>Score:</strong> 5.78<sup>/10</sup> (2<sup>nd</sup>)</p><p>CBT-I is a version of cognitive behavioural therapy specifically tailored for insomnia. Principally, it involves strict sleep hygiene coupled with sleep restriction, concepts that are nicely explained <a href=\"https://johnhalstead.org/index.php/2020/10/11/how-to-cure-your-insomnia/\">here</a>. Previous research has found that CBT-I provided via individual or group therapy sessions can be <a href=\"https://doi.org/10.1186/1471-2296-13-40\">more effective than prescription medication</a> at treating insomnia, while also having far fewer notable side effects.</p><p>Though CBT-I is targeted at insomnia, the key principles of sleep restriction and strict sleep hygiene seem an improvement on common sleep behaviours. This makes CBT-I valuable to most people as a set of tools for improving sleep.</p><p>Our recommendation is based on the use of a CBT-I app, enabling access to the benefits of CBT-I without the cost or difficulty of finding a specialist practitioner. We found good evidence to suggest that six to seven weeks spent following an app-based CBT-I course produces <a href=\"https://pubmed.ncbi.nlm.nih.gov/28400355/\">significant improvements</a> to sleep quality and quantity.</p><p>The best CBT-I apps currently available appear to be either <a href=\"https://www.dozy.health/index.html\">Dozy</a>, an EA-aligned CBT-I app available via beta testing, or <a href=\"https://play.google.com/store/apps/details?id=gov.va.mobilehealth.ncptsd.cbti&amp;hl=en&amp;gl=US\">CBT-I Coach</a>, a partnership between the US Department of Veteran Affairs and the Stanford School of Medicine. As an alternative to using an app, <a href=\"https://www.amazon.co.uk/Overcoming-Insomnia-Sleep-Problems-Behavioral/dp/1845290704\">Overcoming Insomnia</a> is a CBT-I workbook produced by leading sleep medicine researcher, Colin Espie.</p><h2><strong>Light therapy</strong></h2><p><strong>Intervention</strong>: Completing a self-guided CBT-I course</p><p><strong>Score:</strong> 5.63<sup>/10</sup> (3<sup>rd</sup>)</p><p>There is a significant range of research suggesting the benefits of increasing daytime and minimising night-time exposure to bright light. Most people working indoor jobs are exposed to only a small amount of bright light throughout the day. <a href=\"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD003403/full\">Bright light treatment</a> generally aims for exposure to 10,000 lux, the equivalent of the light exposure outside on a day with light cloud, for two hours or more.</p><p><a href=\"https://doi.org/10.1016/j.cub.2013.06.039\">One study</a> found that the average time spent in light over 1,000 lux (10 times less than the recommended level of \u2018bright\u2019 light) was 23% in \u2018modern\u2019 conditions vs. 71% in \u2018natural\u2019 outdoor living conditions. It seems reasonable to conclude that most people are now experiencing far less bright light than was normal for most of human history.</p><p>Exposure to bright morning light can reset the circadian rhythm. This in turn can reduce the <a href=\"https://doi.org/10.1111/j.1479-8425.2007.00272.x\">time taken to fall asleep</a> and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8368439/\">minimise sleep disturbances</a> through the night. The effects on total sleep time and sleep efficiency were generally moderate but consistently positive in the studies reviewed, with some studies showing improvements of <a href=\"https://doi.org/10.1016/j.sleep.2021.10.011\">more than 15%</a> for patients with significant sleep issues.</p><p>Achieving exposure to 10,000 lux requires a moderate investment of time and money, but once a solution is found it can provide permanent benefit.</p><p>The simplest method is to <a href=\"https://www.amazon.com/s?k=sad+lamp&amp;crid=24IR27SPJK11A&amp;sprefix=sad%2Caps%2C271&amp;ref=nb_sb_ss_ts-doa-p_1_3\">purchase a SAD lamp</a> that emits 10,000 lux and place this on your desk, maximising exposure while working. However, the light levels received from a SAD lamp can decrease significantly if placed <a href=\"https://www.google.com/search?q=sad+lamp+distance&amp;rlz=1C1CHBF_en-GBGB864GB864&amp;oq=sad+lamp+distance&amp;aqs=chrome..69i57j69i59j0i271j69i60l5.1977j0j4&amp;sourceid=chrome&amp;ie=UTF-8\">too far</a> from the face, while the lamp\u2019s light offers minimal benefit when doing non-desk based activities.</p><p>The alternative is to install sufficient lighting in a room that the whole space is lit to 10,000 lux or more. This tends to involve buying 10-25 very bright lightbulbs which in combination produce the required brightness. <a href=\"https://www.benkuhn.net/lux/\">Ben Kuhn</a> and <a href=\"https://arbital.com/p/lumenators/\">Eliezer Yudkowsky</a> have each written good, practical guides for doing this.</p><h2><strong>Improved night-time air circulation</strong></h2><p><strong>Intervention</strong>: Leaving a window or internal door open while sleeping</p><p><strong>Score:</strong> 5.31<sup>/10</sup> (4<sup>th</sup>)</p><p>Opening a window overnight can <a href=\"https://pubmed.ncbi.nlm.nih.gov/26452168/\">reduce CO<sub>2</sub> levels</a> in a bedroom by 60%. There is reasonable evidence to suggest that abnormally high levels of carbon dioxide cause significant decreases in <a href=\"https://onlinelibrary.wiley.com/doi/10.1111/ina.12435\">sleep quality and cognitive performance</a> the next day. Direct studies on the impact of improved air circulation by opening a window or internal door overnight are limited but suggestive of improvements in sleep quality.</p><p>Given the simplicity of leaving a window or door open, this intervention seems highly worthwhile. While an open window could make for a cold room, this too could be <a href=\"https://www.sleepfoundation.org/bedroom-environment/best-temperature-for-sleep\">beneficial for sleep quality</a>. An overly cold room also seems simple enough to fix with warmer bedding.</p><p>On top of this, improved overnight air circulation could plausibly reduce the accumulation of household air pollution, providing additional benefits to health and wellbeing.</p><h2><strong>Mindfulness (MBSR)</strong></h2><p><strong>Intervention</strong>: Completing a self-guided MBSR programme</p><p><strong>Score:</strong> 5.10<sup>/10</sup> (5<sup>th</sup>)</p><p>Mindfulness-based stress reduction <a href=\"https://en.wikipedia.org/wiki/Mindfulness-based_stress_reduction#Program\">combines</a> practising mindfulness meditation, non-judgmental awareness, and exploration of the feelings and sensations associated with stressful events. A six-week MBSR course produced a <a href=\"https://pubmed.ncbi.nlm.nih.gov/25686304/\">37% improvement</a> in Pittsburgh Sleep Quality Index scores, an aggregated measure of sleep quality and quantity. While app-based therapy appears less effective than this, it is still likely of significant benefit. Specific research into the effects of app-based MBSR on sleep is limited, but this <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6614998/\">randomised control trial</a> found a significant decrease in sleep disturbance for users of the meditation app Calm.</p><p>Given that this and other research has found <a href=\"https://pubmed.ncbi.nlm.nih.gov/16636636/\">significant benefits</a> from mindfulness training for numerous aspects of wellbeing, MBSR appears a valuable intervention. In some cases, mindfulness training can make individuals <a href=\"https://www.brown.edu/research/labs/britton/sites/britton-lab/files/images/Britton_2019_Can%20mindfulness%20be%20too%20much%20of%20a%20good%20thing.pdf\">more aware</a> of negative behaviours and thought processes without providing relief, though this appears to be an uncommon outcome.</p><p>Apps tailored to improving sleep do not appear common. Two suggestions based on a limited review of available solutions are <a href=\"https://play.google.com/store/apps/details?id=com.calm.android&amp;hl=en_GB&amp;gl=US\">Calm</a> and <a href=\"https://play.google.com/store/apps/details?id=se.lichtenstein.mind.en&amp;hl=en_GB&amp;gl=US\">The Mindfulness App</a>.</p><h1>Secondary Recommendations</h1><p>The following is a summary of the other interventions explored in-depth that did not score as high in our assessment.</p><h2><strong>Mattresses</strong></h2><p><strong>Intervention</strong>: Changing to a <a href=\"https://www.silentnight.co.uk/all-mattresses/medium_firm.html\">medium-firm mattress</a></p><p><strong>Score:</strong> 5.05<sup>/10</sup> (6<sup>th</sup>)</p><p>Replacing current bedding with a new, medium-firm mattress appears to produce a <a href=\"https://doi.org/10.1016/S0899-3467(07)60145-1\">5-6% improvement</a> in sleep quality and efficiency. While individual preferences may differ, the majority of studies that we analysed concluded that a medium-firm mattress was the optimal type for maximising sleep quality.</p><p>Mattresses, however, are a significant investment that may not be practical or worthwhile for many given the only moderate improvement in sleep quality. Additionally, studies tended to test subjects who were previously sleeping on mattresses several years old. Given this, we can reasonably expect a diminished effect for people with more recently purchased bedding.</p><p>We make no attempt to recommend specific mattresses given the large differences in price and larger amounts of marketing involved in their sale.</p><h2><strong>Caffeine</strong></h2><p><strong>Intervention</strong>: Eliminating caffeine consumption from the late afternoon onwards</p><p><strong>Score:</strong> 4.95<sup>/10</sup> (7<sup>th</sup>)</p><p>Caffeine is a commonly known and consumed stimulant. That caffeine can make it more difficult to fall asleep is fairly obvious. However, regularly high consumption of caffeine can also have a significant impact on sleep quality.</p><p>The impact of caffeine on sleep can be avoided by limiting consumption to the first half of the day. Individuals\u2019 speed of processing caffeine <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK223808/\">varies significantly</a> but it is common for it to still have notable effects up to 10 hours after consumption.</p><p>At a minimum, avoiding caffeine consumption within the six hours prior to sleeping seems an easy and valuable intervention. Adjusting the results of <a href=\"https://doi.org/10.5664/jcsm.3170\">this study</a> to the caffeine content of a <a href=\"https://www.healthline.com/nutrition/how-much-caffeine-in-coffee#TOC_TITLE_HDR_4\">standard double-shot coffee</a>, we estimate a 3% reduction in sleep time and sleep quality from drinking a coffee six hours in advance of sleep.</p><p>Though caffeine consumption has a significant effect and strong evidence base, we expect that avoiding caffeine later in the day is largely common knowledge. Given this, people who currently drink coffee in the evening are likely doing so for benefits that they weigh higher than a small increase in sleep quality (such as increased alertness for evening work), making this a less valuable recommendation.</p><h2><strong>Magnesium</strong></h2><p><strong>Intervention</strong>: Taking a 300mg magnesium supplement daily</p><p><strong>Score:</strong> 4.76<sup>/10</sup> (8<sup>th</sup>)</p><p>Around half of Americans likely consume <a href=\"https://pubmed.ncbi.nlm.nih.gov/28140318/\">less than the recommended levels</a> of magnesium, while some studies have suggested that recommended levels are <a href=\"https://doi.org/10.1136/openhrt-2017-000668\">significantly lower</a> than the optimum daily magnesium intake.</p><p>While the <a href=\"https://pubmed.ncbi.nlm.nih.gov/21199787/\">quality of evidence is low</a>, with no known method of causation, magnesium supplementation is associated with <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3703169/\">improved sleep efficiency</a> and total sleep time.</p><p>Most multivitamins only contain a modest amount of magnesium<a href=\"#fn-m9Fjo6beDKbjvbwKz-5\"><sup>[5]</sup></a> so we recommend taking a specific, 300-375mg <a href=\"https://www.boots.com/boots-magnesium-375-mg-60-2-a-day-tablets-10145805\">magnesium supplement</a> to test for any noticeable benefit to your sleep. However, magnesium supplementation can have <a href=\"https://www.webmd.com/diet/supplement-guide-magnesium#2-6\">significant side effects</a>, particularly for people taking other medication, so it is important to note possible issues and stop taking supplementary magnesium should these occur.</p><h2><strong>Cold lighting</strong></h2><p><strong>Intervention</strong>: Installing blue-enriched lighting to your workspace</p><p><strong>Score:</strong> 4.53<sup>/10</sup> (9<sup>th</sup>)</p><p>While lighting can come in a wide range of brightness, it also ranges widely in colour temperature. Blue-enriched (cold) lighting can <a href=\"https://www.tandfonline.com/doi/full/10.3109/07420528.2015.1056304\">improve daytime alertness</a> and bring a modest benefit to sleep quality but is difficult to find commercially.</p><p>In our brief search, we found these <a href=\"https://www.amazon.co.uk/Bayonet-LOHAS-Light-Equivalent-Dimmable/dp/B07XZ82BZS/ref=sr_1_6?keywords=Daylight+Cool+Light+Bulbs&amp;qid=1636121292&amp;sr=8-6\">household lights at 6,000K</a> as the coolest option available. However, the blue-enriched lights used in the study referenced above were <a href=\"https://www.tandfonline.com/doi/full/10.3109/07420528.2015.1056304\">17,000K</a>, meaning that the benefits of commercially available cold lighting are likely to be significantly lower than those produced in research.</p><p>Given the greater evidence base and strength of effect for increasing the brightness of household lighting, we feel that this is a more worthwhile intervention than installing colder lighting.</p><h2><strong>Morning exercise</strong></h2><p><strong>Intervention</strong>: Shifting exercise from evening to morning</p><p><strong>Score:</strong> 4.43<sup>/10</sup> (10<sup>th</sup>)</p><p>The <a href=\"https://journals.scholarsportal.info/details/10870792/v28icomplete/32_trbpaaromaam.xml\">positive effect of regular exercise</a> on sleep quality is well-known. However, the timing of exercise during the day could play a significant mediating role in increasing or decreasing the beneficial effects of exercise on sleep.</p><p>For people accustomed to exercising in the evening, shifting their practice to the morning could <a href=\"https://pubmed.ncbi.nlm.nih.gov/26333783/\">increase sleep quality</a> while reducing sleep disturbance and the time taken to fall asleep. However, these effects were relatively modest in the research we found and evidence on the topic is thin.</p><p>With the constraints of coordinating work, family, and social schedules, we expect that shifting exercise time will be relatively difficult for many people. Feasibly, it could even result in reduced sleep hours if you start waking up earlier to fit in exercise in the morning.</p><h2><strong>Houseplants</strong></h2><p>(<i>Note: see </i><a href=\"https://forum.effectivealtruism.org/posts/Ei2uYbn2zrzmBjEsp/sleep-effective-ways-to-improve-it?commentId=teAwXCPvuEweqbvss\"><i>this comment</i></a><i> for why the estimate given below is likely a significant overestimate of the potential effect of houseplants on bedroom CO2)</i></p><p><strong>Intervention</strong>: Buying multiple succulent houseplants for your bedroom</p><p><strong>Score:</strong> 3.94<sup>/10</sup> (11<sup>th</sup>)</p><p>As an alternative to opening a window, there is some evidence that some houseplants can produce a notable reduction in CO<sub>2</sub> levels in a room. In particular, certain plants that store CO<sub>2</sub> at night to then use for photosynthesis during the day (a process known as <a href=\"https://en.wikipedia.org/wiki/Crassulacean_acid_metabolism\">crassulacean acid metabolism</a> [CAM]) appear particularly effective at reducing night-time CO<sub>2</sub> levels.</p><p>While there has been little research into this area, one study found several common houseplants <a href=\"https://www.researchgate.net/publication/315968651_Effectiveness_of_Indoor_Plant_to_Reduce_CO_2_in_Indoor_Environment\">reduced CO<sub>2</sub> concentration in a room by 15-20%</a>. It seems reasonable to assume that placing several houseplants in a room would significantly increase this effect, though likely with diminishing returns.</p><p>A study <a href=\"https://ntrs.nasa.gov/citations/19930073077\">conducted by NASA</a> into the effectiveness of houseplants at removing a range of air pollutants (to then use them on the ISS) found that Variegated Snake Plants and English Ivy were particularly promising.</p><p>We leave the decision of whether to create a small indoor forest in your bedroom in your hands but note that this is unlikely to be more effective at reducing CO<sub>2</sub> concentration than simply opening a window.</p><h1>Summary</h1><p>This research forms an attempt to find the most effective ways an individual can improve their sleep, with a focus on suggestions that readers are less likely to have previous knowledge of. The methods used are rough and a work in progress, forming the basis of a <a href=\"https://forum.effectivealtruism.org/posts/hdfMrY4QGEKaoFubH/shortform\">larger project</a> researching the most effective ways people can improve their wellbeing.</p><p>Our principal recommendations for improving sleep quality are (in order):</p><ol><li><strong>Melatonin supplements</strong>: 0.3mg (300mcg) daily taken two hours before bed.</li><li><strong>CBT-I</strong>: a six-to-seven-week, self-guided course in cognitive behavioural therapy for insomnia, accessed through an app.</li><li><strong>Light therapy</strong>: greatly increasing exposure to bright light during the day, either through building a lumenator or purchasing a SAD lamp.</li><li><strong>Improved night-time air circulation</strong>: opening a window to reduce overnight CO<sub>2</sub> accumulation.</li><li><strong>Mindfulness-based stress reduction</strong>: mindfulness training through an app with a focus on sleep.</li></ol><p>Thank you to everyone who has previously written about improving sleep here on the Forum<a href=\"#fn-m9Fjo6beDKbjvbwKz-3\"><sup>[3:1]</sup></a> and on other EA-aligned sites.<a href=\"#fn-m9Fjo6beDKbjvbwKz-4\"><sup>[4:1]</sup></a> We hope that this research has been useful and that you\u2019ll consider experimenting with our recommendations!</p><p><i>Note</i>: We are currently running a <a href=\"https://forms.gle/x8ZGKo4EGQmz4suk8\">5-minute survey</a> evaluating our work which we'd greatly appreciate you filling out!</p><hr><hr><p>The model used for this research is a rough one, loosely based on the weighted factor models used by the <a href=\"https://www.charityentrepreneurship.com/weighted-factor-model.html\">Charity Entrepreneurship</a> team. A model like this allows for the inclusion of a wider range of subjective and objective measures, combining practical aspects of an intervention with evidence of its effectiveness. Results should not be taken as definitive, acting instead as a guide for updating knowledge in a positive or negative direction. As the wider project grows, this model will likely be further developed. The results posted here will be updated accordingly. <a href=\"#fnref-m9Fjo6beDKbjvbwKz-1\">\u21a9\ufe0e</a></p><p>We were going to establish the accuracy of this claim but decided it would be more worthwhile to get some extra sleep instead [it was]. <a href=\"#fnref-m9Fjo6beDKbjvbwKz-2\">\u21a9\ufe0e</a></p><p>Thank you to everyone who has previously written about sleep on the Forum. The following is a (hopefully!) comprehensive directory of previous articles on the topic published on the Forum (in order of karma): <a href=\"https://forum.effectivealtruism.org/posts/NDszJWMsdLCB4MNoy/burnout-what-is-it-and-how-to-treat-it#Sleep\">Burnout \u2013 what it is and how to treat it: Sleep</a> [Elizabeth]; <a href=\"https://forum.effectivealtruism.org/posts/ztrYNRJ66NQeMpS64/insomnia-with-an-ea-lens-bigger-than-malaria\">Insomnia with an EA lens: Bigger than malaria?</a> [samstowers]; <a href=\"https://forum.effectivealtruism.org/posts/q8g2MXQCmKoYhEjsT/insomnia-a-promising-cure\">Insomnia: A promising cure</a> [Halstead]; <a href=\"https://forum.effectivealtruism.org/posts/Ca2LJHEBvYkdcLj57/on-sleep-procrastination-going-to-bed-at-a-reasonable-hour-2\">On Sleep Procrastination: Going to Bed at a Reasonable Hour</a> [emily.fan]; <a href=\"https://forum.effectivealtruism.org/posts/BnnDRy2yFzJqJ4Mvz/things-i-recommend-you-buy-and-use#Health___Sleep\">Things I recommend you buy and use: Health/ Sleep</a> [BenSchifman] <a href=\"https://forum.effectivealtruism.org/posts/G6aZMjXBke326WJLp/should-we-consider-the-sleep-loss-epidemic-an-urgent-global\">Should we consider the sleep loss epidemic an urgent global issue?</a> [orenmn]; <a href=\"https://forum.effectivealtruism.org/posts/Zc9D2zkgLywFtotkJ/instructions-on-potential-insomnia-cure\">Instructions on potential insomnia cure</a> [Halstead]. <a href=\"#fnref-m9Fjo6beDKbjvbwKz-3\">\u21a9\ufe0e</a> <a href=\"#fnref-m9Fjo6beDKbjvbwKz-3:1\">\u21a9\ufe0e</a></p><p>Other great EA-adjacent articles: <a href=\"https://lynettebye.com/blog/2019/10/24/lu1xjfsg8i9rzkatmnqgh2r9ykb0r1\">How to improve your sleep</a> [Lynette Bye]; <a href=\"https://www.openphilanthropy.org/behavioral-treatments-insomnia\">Behavioural treatments for insomnia</a> [Luke Muehlhauser]; <a href=\"https://slatestarcodex.com/2018/07/10/melatonin-much-more-than-you-wanted-to-know/\">Melatonin: Much more than you wanted to know</a> [Scott Alexander] <a href=\"#fnref-wPG3sJMqpLvqJ4caK-4\">\u21a9\ufe0e</a> <a href=\"#fnref-m9Fjo6beDKbjvbwKz-4\">\u21a9\ufe0e</a> <a href=\"#fnref-m9Fjo6beDKbjvbwKz-4:1\">\u21a9\ufe0e</a></p><p>Magnesium content in the first 5 multivitamins I found through a quick Google search: <a href=\"https://www.boots.com/vitabiotics-wellman-max-triple-pack-84-tablets-capsules-10188746\">Vitabiotics Wellman Max</a> [60mg]; <a href=\"https://www.hollandandbarrett.com/shop/product/holland-barrett-ultra-man-multivitamin-caplets-60031489?skuid=034512&amp;utm_campaign=shopping&amp;utm_medium=cpc&amp;utm_source=google&amp;gclid=Cj0KCQjw5oiMBhDtARIsAJi0qk0S7HHzFIcwyMVVDQBVOyNDlpf8lFrGcdgtPUZvuLm3H4xdhU2wnhgaAiKOEALw_wcB&amp;gclsrc=aw.ds\">Holland &amp; Barrett Ultra Man Multivitamin 30 Caplets</a> [125mg]; <a href=\"https://www.boots.com/boots-a-z-180-tablets-10274970\">Boots A-Z 180 Tablets</a> [60mg]; <a href=\"https://www.boots.com/centrum-advance-50-100-tablets-10092970\">Centrum Advance 50+</a> [100mg]; <a href=\"https://www.boots.com/vitabiotics-wellwoman-max-84-tablets-capsules-10188747\">Vitabiotics Wellwoman Max</a> [100mg]. <a href=\"#fnref-m9Fjo6beDKbjvbwKz-5\">\u21a9\ufe0e</a></p>", "user": {"username": "ben3536"}}, {"_id": "EktaZtz6DSPbzymjg", "title": "Free 1:1 Learning Coaching!", "postedAt": "2021-11-15T11:29:35.537Z", "htmlBody": "<p>Hi there!&nbsp;</p><p>As part of my <a href=\"https://www.teachingyouhowtolearn.com/\">https://www.teachingyouhowtolearn.com/</a> project, I'm interested in trying out 1:1 coaching as a way to help people improve their learning abilities in a high-leverage with a short time commitment.&nbsp;</p><p>I have 3 key ideas posts that outline a lot of the good stuff, but naturally the high-friction task of setting all this stuff up is on the reader, so I'm interested in trying a 30-60 minute session with people where I share the key insights, help set up the software, do a trial run on a resource you'd like to learn, etc. Potentially a follow up session at a later date too to troubleshoot anything, go a bit deeper, etc.&nbsp;</p><p>A note that this is a speculative trial so will be fairly unpolished, but if you're interested, please email me at alexanderklarge@gmail.com!&nbsp;</p>", "user": {"username": "alexanderklarge"}}, {"_id": "ujRGGBxJN9AHXfzJe", "title": "\"Slower tech development\" can be about ordering, gradualness, or distance from now", "postedAt": "2021-11-14T20:58:04.899Z", "htmlBody": "<p><em>Epistemic status: Shower thought quickly written (once dry). Partly just a reframing of existing ideas. See also other work on <a href=\"https://forum.effectivealtruism.org/tag/differential-progress\">differential progress</a>. My examples are oversimplified, and I don\u2019t necessarily actually endorse the intervention ideas mentioned.</em></p>\n<h2>Summary</h2>\n<p>Let\u2019s say, for example, that we want certain major advances in AI or synthetic biology to occur 50 rather than 5 years from now. I think there are three major reasons why we might want that - three different \u201cactive ingredients\u201d that could account for the potential benefits of slower technology development:</p>\n<ul>\n<li><strong>Ordering</strong>: it might be good if a given technology were developed after some other specific development(s)</li>\n<li><strong>Gradualness</strong>: it might be good if a given technology were developed gradually rather than suddenly/\u201cdiscontinuously\u201d</li>\n<li><strong>Distance from now</strong>: it might be good if a given technology were developed later rather than sooner, for reasons other than ordering and gradualness</li>\n</ul>\n<p>I think it would often be useful to explicitly distinguish between these reasons and consider how much we care about each in a given case, because they suggest different interventions and different factors to consider. I give some examples below.</p>\n<h2>Ordering</h2>\n<p>We might think it would be good if a given technology (let\u2019s give it the imaginative name \u201ctechnology X\u201d) were developed <strong>after some other specific development(s)</strong>, because those other developments reduce the risks from technology X. For example, we might want various developments in AI capability to occur after rather than before various developments in AI safety, alignment, policy, or governance.</p>\n<p>For this, absolute distance from now doesn\u2019t matter in itself, but rather serves as a <em>useful</em> <em>proxy</em> - since, generally speaking, the longer it\u2019ll be till technology X is developed, the more likely it is that the risk-reducing development(s) occur first. For example, we might not mind whether technology X will be developed 5 or 50 years from now <em>if</em> we believed that, either way, the risk-reducing developments are equally likely to occur first.</p>\n<p>The more we care about ordering, the more we might be interested in:</p>\n<ul>\n<li>Improving our forecasts of when the risk-reducing developments will occur, or our forecasts on binary questions about which of various pairs of developments will occur first</li>\n<li>Accelerating progress towards risk-reducing developments</li>\n<li>Increasing the chance that progress toward the risk-reducing developments speeds up <em>if</em> development of technology X speeds up, or that the development of technology X slows down <em>if</em> progress toward the risk-reducing developments seems to be lagging behind\n<ul>\n<li>E.g., increasing the chance that AI development slows down if it becomes apparent that AI safety/alignment progress is proceeding much more slowly than anticipated</li>\n</ul>\n</li>\n</ul>\n<h2>Gradualness</h2>\n<p>We might think it would be good if technology X were developed <strong>gradually rather than suddenly/\u201cdiscontinuously\u201d</strong>. That is, roughly speaking, we might want it to be the case that the development proceeds in many small steps rather than a smaller number of big jumps (separately from how long from now those steps/jumps occur). See also <a href=\"https://www.lesswrong.com/tag/ai-takeoff\">AI Takeoff</a> and <a href=\"https://www.fhi.ox.ac.uk/strategic-considerations-about-different-speeds-of-ai-takeoff/\">Strategic considerations about different speeds of AI takeoff</a>.</p>\n<p>I think there are basically four key reasons why more gradual development could be good:</p>\n<ul>\n<li>\n<p><strong>\u201c<a href=\"https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence\">Fire alarms</a>\u201d and \u201c<a href=\"https://forum.effectivealtruism.org/tag/warning-shot\">warning shots</a>\u201d</strong>: More gradual development provides more chances for various actors to notice that the risky technology is coming, is coming soon, and could be very risky. That would probably increase how many resources end up getting allocated to various ways of reducing the risks (e.g., developing risk-reducing technologies or improving governance mechanisms).</p>\n</li>\n<li>\n<p><strong>More time with more clarity</strong>: Roughly speaking, if the technology jumps suddenly to a very risky point, almost all risk-reducing work has to be done with quite little clarity on what the technology will ultimately look like, how it will be used, who\u2019ll develop it, who\u2019ll govern it, etc. The more gradual the development is, the more time we\u2019ll have <em>before</em> the especially risky period but with a decent picture of how things will ultimately end up.<sup class=\"footnote-ref\"><a href=\"#fn-7nNwKEuEKmKK3vKev-1\" id=\"fnref-7nNwKEuEKmKK3vKev-1\">[1]</a></sup></p>\n</li>\n<li>\n<p><strong>Ordering</strong>: More gradual development also just makes it more likely that the technology reaches a dangerously mature stage after specific other risk-reducing developments occur, which could be good for reasons discussed in the previous section.</p>\n</li>\n<li>\n<p><strong>Distance from now</strong>: More gradual development also just makes it more likely that the technology reaches a dangerously mature stage longer from now, which could be good for reasons discussed in the previous section.</p>\n</li>\n</ul>\n<p>For gradualness, as with ordering, absolute distance from now doesn\u2019t matter in itself, but rather could serve as a somewhat <em>useful</em> <em>proxy</em> - that is, a technology being developed further in the future could serve as some evidence that it will be developed more gradually. (Though the opposite can also be true - e.g. if AI software improvements happen further in the future, that could increase the chance that there\u2019s a large <a href=\"https://aiimpacts.org/hardware-overhang/\">hardware overhang</a> at that point, which could increase takeoff speeds.)</p>\n<p>The more we care about gradualness, the more we might be interested in:<sup class=\"footnote-ref\"><a href=\"#fn-7nNwKEuEKmKK3vKev-2\" id=\"fnref-7nNwKEuEKmKK3vKev-2\">[2]</a></sup></p>\n<ul>\n<li>Improving our forecasts of how gradually the development will occur</li>\n<li>Reducing our need for fire alarms, warning shots, or more time with more clarity. For example, we could:\n<ul>\n<li>Convince more people that development may not be gradual and that they should therefore ramp up risk-reducing work <em>now</em></li>\n<li>Improve our insight <em>now</em> into how the development and deployment will play out</li>\n</ul>\n</li>\n<li>Perhaps trying to increase that chance that, <em>when</em> some jumps in progress occur or appear to be on the horizon, development <em>then</em> slows to a more gradual pace. For example, we could:\n<ul>\n<li>Highlight the importance of gradualness to people involved in doing, funding, regulating, etc. the development, and suggest they slow down after certain key breakthroughs occur</li>\n<li>Make it more likely that the jumps will actually be noticed by relevant actors (e.g., increasing the transparency of the relevant development processes)</li>\n</ul>\n</li>\n</ul>\n<h2>Distance from now</h2>\n<p>Finally, we might think it would be better if the technology were developed a longer <strong>time from now</strong>, for reasons unrelated to how gradually it\u2019s developed or whether it\u2019s developed before/after <em>specific</em> risk-reducing developments occur. I think the main reason for this is if various risk-reducing efforts are already occurring or are expected to occur in future by default (not just in response to initial steps of a gradual development progress), such that extending timelines would \u201cbuy more time\u201d and mean that more such work occurs by the \u201cdeadline\u201d.<sup class=\"footnote-ref\"><a href=\"#fn-7nNwKEuEKmKK3vKev-3\" id=\"fnref-7nNwKEuEKmKK3vKev-3\">[3]</a></sup></p>\n<p>There are two ways this is distinct from \u201cordering\u201d:</p>\n<ul>\n<li>As I defined \u201cordering\u201d, it was just about the order of <em>technological developments</em>, which most people would interpret as not including things like the emergence of norms and governance mechanisms that can mitigate risks from the technology. In contrast, <strong>\u201cdistance from now\u201d also \u201cbuys time\u201d for things like norms and governance mechanisms</strong>.</li>\n<li>\u201cOrdering\u201d is about the order in which <em>a specific set of developments we already have in mind</em> occur, whereas <strong>\u201cdistance from now\u201d buys time for <em>a broad range of possible risk-reducing efforts</em></strong>. So more \u201cdistance from now\u201d offers a less targeted but also less brittle protection than a better ordering of specifically identified events does.</li>\n</ul>\n<p>The more we care about distance from now, the more we might be interested in:</p>\n<ul>\n<li>Improving our forecasts of when the technological development(s) will occur</li>\n<li>Improving our estimates of how much work is currently occurring and how much will occur in various future years (perhaps given that there are no major advancements in the risky technology), to gain a clearer sense of how much we\u2019d really gain by \u201cbuying more time\u201d</li>\n<li>Slowing overall progress towards the risky technological development(s)</li>\n<li>Ramping up risk-reducing efforts now or in future, without relying on things like fire alarms or warning shots to cause this</li>\n</ul>\n<p><em>My thanks to Neil Dullaghan, Ben Snodin, and James Wagstaff for helpful comments on an earlier draft.</em></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-7nNwKEuEKmKK3vKev-1\" class=\"footnote-item\"><p>See also the idea of \u201cnearsightedness\u201d in <a href=\"http://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/\">The timing of labour aimed at reducing existential risk</a>. <a href=\"#fnref-7nNwKEuEKmKK3vKev-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-7nNwKEuEKmKK3vKev-2\" class=\"footnote-item\"><p>We could also in theory try to accelerate some initial steps, if we were somehow justifiably convinced we could do this without thereby also accelerating later steps to a similar extent. One reason that condition could hold is if we think a certain risky technology (a) would be rapidly developed by or with the assistance of sufficiently advanced AI systems but (b) is very unlikely to be developed before then. If so, then nearer-term steps towards that technology have little effect on when the technology will reach a particularly risky stage of maturity, but could still inspire and inform risk-reduction efforts. But I expect that sort of scenario to be relatively rare, and I think anyone considering accelerating initial steps based on that sort of logic should seriously consider <a href=\"https://forum.effectivealtruism.org/tag/accidental-harm\">downside risks</a> related to possibly worsening the order of developments and reducing the distance between now and the technology\u2019s development. <a href=\"#fnref-7nNwKEuEKmKK3vKev-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-7nNwKEuEKmKK3vKev-3\" class=\"footnote-item\"><p>Another possible reason it could be better if a technology were developed a longer time from now is if we think the world is simply becoming safer, more cooperative, more stable, or similar over time for reasons other than \u201crisk-reducing efforts\u201d. For example, we might think international relations, cooperation, and governance will gradually improve for reasons related to things like a desire to facilitate profitable trade and improve near-term health outcomes, and this will happen to also mean that technological developments will be less risky if they happen later, once such processes have had longer to play out. <a href=\"#fnref-7nNwKEuEKmKK3vKev-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "MichaelA"}}, {"_id": "QhPyQTXuGt58Nzxnu", "title": "You are probably underestimating how good self-love can be", "postedAt": "2021-11-14T14:47:41.370Z", "htmlBody": "<p><i>I am very grateful to the following people, in general, and for their helpful feedback on this post: Nick Cammarata, Kaj Sotala, Miranda Dixon-Luinenburg, Sam Clarke, Mrinank Sharma, Matej Vrzala, Vlad Firoiu, Ollie Bray, Alan Taylor, Max Heitmann, Rose Hadshar, and Michelle Hutchinson.</i></p><hr><p>This is a cross-post from <a href=\"https://www.lesswrong.com/posts/BfTW9jmDzujYkhjAb/you-are-probably-underestimating-how-good-self-love-can-be\">LessWrong</a>. I almost didn't post here, since this type of content is a little unfamiliar to the forum. But it saddens me to see my friends pour their hearts into the flourishing of humanity, and yet hurt so badly. I write later in the post:</p><blockquote><p>A lot of people go their whole lives making their self-worth conditional in order to act better: they take damage--dislike or judge themselves--whenever they act imperfectly or realise they are imperfect or don\u2019t achieve the things they want to. In a world as unfair and uncontrollable as this one, I think taking so much damage is often not that functional. Moreover, I claim that<i> you can care deeply while feeling worthwhile and suffused with compassion and affection and joy</i>.&nbsp;</p></blockquote><p>It is hard to do the most good when depressed, burned out, or feeling worthless. Even if this is not you, I think self-love might be worth aiming for--especially if you want to do something as difficult as saving the world. &nbsp;</p><hr><p>I was on a plane to Malta when I realised I had lost something precious. I was struggling to meditate. I knew there was some disposition that made meditation easier for me in the past, something to do with internal harmony and compassion and affection. Alas, these handles failed to impact me. On a whim, I decided to read and meditate on some of my notes. 3h later, I had recovered the precious thing. It was one of the most special experiences of my life. I felt massive relief, but I was also a little scared--I knew that this state would likely pass. I made a promise to myself to not forget what I felt like, then, and to live from that place more. This post is, in part, an attempt to honour that promise. I spent most of my holiday in Malta reading about and meditating on the precious thing, and I now feel like I'm in a place where I can share something useful.</p><p>This post is about self-love. Until recently, I didn\u2019t know that self-love was something I could aim for; that it was something <i>worth</i> aiming for. My guess is that I thought of self-love as something vaguely Good, a bit boring, a bit of a chore, a bit projection-loaded (I\u2019m lovable; I love me so you can love me too), and lumped together with self-care (e.g. taking a bath). Then I found <a href=\"https://twitter.com/nickcammarata\"><u>Nick Cammarata</u></a> on Twitter and was blown away by the experiences he was describing. Nick tweeted about self-love from Sep 2020 to May 2021, and then moved on to other things. His is the main body of work related to self-love that I\u2019m aware of, and I don't want it to be lost to time. My main intention with this post is to summarise Nick\u2019s work and build on it with my experiences; I want to get the word out on self-love, so that you can figure out whether it\u2019s something <i>you</i> want to aim for. But I'm also going to talk a little about how to cultivate it and the potential risks to doing that. One caveat to get out of the way is that I\u2019m a beginner--I\u2019ve been doing this stuff for under a year, for way less than 1h/day. Another is that I expect that my positive experiences with self-love are strongly linked to me being moderately depressed before I started.<br>&nbsp;</p><h1>What is self-love?</h1><p>Self-love is related to a lot of things and I'm not sure which are central. But I can point to some experiences that I have when I'm in high self-love states. While my baseline for well-being and self-love is significantly higher than it used to be, and I can mostly access self-love states when I want to, most of the time I am not in very high self-love states, because my attention is elsewhere. Some of the following experiences point to the core of what self-love feels like, some are actions or tendencies that self-love spins up out of, and some are consequences of self-love. It is hard to untangle these categories so I don\u2019t try to.&nbsp;</p><ul><li>Take a second to imagine the love you might feel towards a newborn child or a cute animal. They probably haven't done anything to \u2018earn\u2019 your love; they might even be acting unskillfully (admittedly, I don't know what a skilful baby looks like). But you might love them anyway. Self-love feels quite like that for me: unconditional, newborn love.</li><li>I feel bad <i>about myself</i> a lot less: I'll notice a character defect or a way I acted unskillfully, and won't feel bad <i>about myself</i>--similarly to how I would feel about a close friend messing up. It doesn't follow from this that I don't feel bad (I think traditionally \u2018negative\u2019 emotions can be functional), don't want to change, or act differently in the future. More on this later. A consequence of this is that it is easier to see my imperfections, and to see the world, as opposed to flinching away from them. <i>When states of the world directly impact your perceived self-worth, it can be really scary to see the world as it is</i>. Some examples: a <a href=\"https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the\"><u>kid</u></a> who wants to be a writer and cannot admit that she made a spelling mistake; my aversion to studying AI safety because doing that puts me in contact with the fact that I don\u2019t know that much and hence that I\u2019m worthless.</li><li>Affection: I'll drop and smash a plate, and where previously there might have been some frustration or self-judgement, the mental motion might be \"Oh, silly Charlie, I still love you\". Or when I toned down a claim in this post just now I was like \"Oh thank you for protecting me\". Importantly, I'm not saying empty words--I'm translating my feelings into words. The examples of affection above closely resemble how I\u2019d feel towards a small child, but the affection can also feel more friend or partner-like. For example, I got drunk for the first time in a while last weekend, and found drunk Charlie really adorable.</li><li>Compassionate awareness: I\u2019ll define \u201ccompassion\u201d as seeing suffering and being moved by it, where \u201cbeing moved\u201d might connote warmth and caring and non-judgement and desire to help. I\u2019m often including my experience (emotions, thoughts, sensations) in my moment-to-moment awareness, greeting and feeling what\u2019s happening to me. Sometimes I\u2019ll notice that I\u2019m conflicted or struggling or suffering, and will dive deeper. I find it useful to view myself as having many <a href=\"https://www.alignmentforum.org/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model\"><u>parts</u></a>, who have different feelings and goals and functions. So I'll often be talking to my parts and figuring out what they want and why, how they\u2019re feeling, what they think of each other--and holding compassionate space for all of that to happen in. I find the parts model pretty useful for compassion and affection, in part because it\u2019s easier for me to feel/send love when there\u2019s some distance between the lover and the loved.</li><li>Nick Cammarata says that a heuristic for self-love is that you feel like you're walking around with someone you have a crush on (<a href=\"https://twitter.com/nickcammarata/status/1312920733027192832\"><u>here</u></a> is a thread from him about this, and <a href=\"https://twitter.com/nickcammarata/status/1383595296500817929\"><u>here</u></a> is one where he discusses the controversy that thread caused). It feels like that for me: romantic. But read \u201cromantic\u201d more as beautiful and exciting than including desire or projection, if those are part of your dating experience. There's curiosity--wanting to know more about my experience--and awe and affection, and joy, at being able to share these moments with myself.</li><li>Relatedly, I feel like I\u2019m \u201cwith myself\u201d, as opposed to \u201cby myself\u201d. This is how Nick describes feeling too: \u201cMy body feels different. Being in my body used to feel a bit like being in a neutrally-charged hollow shell interacting with the world, now it feels a bit like a stable and warm castle with a cozy quality. I feel like I am \u201cwith myself\u201d inside of it. Others are outside, and I can open the castle and feel close to them, but staying inside with myself is the default.\"</li><li>Loving action: For example, attending to my experiences; paying attention to what I want and acting to make that happen; prioritising resolving internal conflict; not ignoring or shutting parts of me down; noticing a flicker of not-ok-ness while watching TV and pausing the TV to figure out what\u2019s wrong and whether it\u2019s ok to continue watching.</li><li>I feel substantially safer, like I have a blanket wrapped around me. I don't fully understand this, but I think it's because I'm clinging less to external conditions being satisfied in order to feel worthwhile. I feel like I\u2019m more capable of taking whatever the world throws at me. I still care about the external things, like whether a partner loves me, but I don\u2019t <a href=\"https://handsandcities.com/2021/01/24/on-clinging/\"><u>cling</u></a> to them in the same way.</li><li>Worthiness/self-esteem:&nbsp;I used to have a strongly bad filter on my self-perception. Now I can more easily remember (to some extent) my inherent goodness and preciousness and beauty. I can also more clearly see all of the amazing things about me.</li><li>Spending time with myself used to be unbearable--I would sink into awareness-collapsing distractions. In these states, spending time with myself is really fun, often more fun than spending time with friends. Time alone is nourishing and special.</li><li>Happiness: In my experience, it is a lot easier to do anything when I have surplus happiness, and it is extremely difficult to do anything when depressed. Self-love makes me very happy so I\u2019m able to do the things that matter to me. <a href=\"https://twitter.com/nickcammarata/status/1392551239938871299\"><u>This</u></a> is a tweet thread where Nick writes that raising happiness baselines is possible and incredibly important.</li><li>Energy: Part of this is fighting myself less, which includes less suffering-based motivation and internal conflict. Freeing up those resources has been astonishingly powerful for me. Part is not needing to invest emotional resources into trying and needing to feel loved, because I have a wellspring within.</li><li>More love for others and the world.&nbsp;<br>&nbsp;</li></ul><h1>But won\u2019t I turn into jello?</h1><p>It\u2019s easy to imagine that, if you feel unconditionally worthwhile, if you have access to a deep source of self-compassion and affection and joy, then you will care less about changing or pursuing your goals. This was my worry, so I want to tackle it head-on.&nbsp;</p><p>I think turning into jello is a very understandable worry. A lot of people go their whole lives making their self-worth conditional in order to act better: they take damage--dislike or judge themselves--whenever they act imperfectly or realise they are imperfect or don\u2019t achieve the things they want to. In a world as unfair and uncontrollable as this one, I think taking so much damage is often not that functional. Moreover, I claim that<i> you can care deeply while feeling worthwhile and suffused with compassion and affection and joy</i>. All that said, messing with the strategy that helps you act better is a big deal (see Risks).&nbsp;</p><p>I don\u2019t have any good arguments about how often we\u2019d expect people to turn to jello, besides looking at the people who have walked the path. However, I\u2019m confident that more self-love does not <i>necessitate</i> less caring, because I and many others have experienced that more self-love leads to <i>more</i> caring. Nick Cammarata says that he has never seen people turn into jello, and that, \u201cIn fact, it usually pushes [people] far in the other direction\u201d. This accords with the behaviourism literature (at least as summarized in \"<a href=\"https://www.lesswrong.com/posts/Cf2xxC3Yx9g6w7yXN/notes-from-don-t-shoot-the-dog\"><u>Don't Shoot the Dog</u></a>\"), which claims that both animals and humans are best trained by only giving them rewards and no punishments. This probably generalizes to internal rewards and punishments, which are largely learned and internalized based on how people have treated us in the past.</p><p>I\u2019m reminded of Nate Soares\u2019 writings on <a href=\"https://replacingguilt.com/\"><u>Replacing Guilt</u></a>. He writes that it's <i>you</i> that cares about your goals, that wants to become stronger or save the world. Those things that you actually care about won\u2019t go away with more self-love; what changes is your strategy for pursuing them. <strong>You no longer pursue things in order to feel worthwhile, but simply because you want to</strong>. Indeed, it is not self-loving to shut down those parts of you that care about things. An essential component of self-love, as I see it, is being there with and feeling fully whatever is happening for me, <i>especially</i> when I want things to be different.<br>&nbsp;</p><h1>Risks</h1><ul><li>Your goals and strategies might change, even if your values remain the same. For example, I was aiming to pursue a PhD in machine learning, partly because I thought it would make me worthwhile. When I felt worthwhile I stopped that; I was able to think more freely about which strategy looked best according to my values.</li><li>Changing your brain might have negative effects in the short term, even if things are good in the end. For example, as I walked down the self-love path I felt my external obligations start to drop away. While things are clearly better now, I\u2019m still figuring out how to be internally motivated and also get shit done, and for a while I got less shit done than when I was able to coerce myself.</li><li>It\u2019s easy to misunderstand what you\u2019re aiming for.</li><li>It\u2019s also easy to miss your target. For example, for a while, I used to throw what I thought was compassion at negative emotions and they would go away. And on the surface that seems kinda reasonable. But, for me, industry-grade compassion requires seeing the emotions fully--holding them and understanding them and letting them be felt as strongly as they want to be felt.</li><li>The techniques you use to develop self-love might have side effects of their own. For example, if you\u2019re doing a lot of meditation I would be surprised if you didn\u2019t have some negative experiences at some point. (That said, lovingkindness meditation seems like one of the safer types, and I\u2019m broadly pro-meditation.)&nbsp;<br>&nbsp;</li></ul><h1>How to self-love</h1><p>I\u2019m really confused about this, sorry. The path is muddy, at least to me. That\u2019s why I focused on describing self-love. I realise that this might be frustrating, especially if I managed to get you excited about self-love. That said, I decided to write <i>something</i> here rather than nothing. Please take this section with a bunch of salt.</p><p><i><strong>Nick thinks that the two most promising avenues are solo MDMA trips and metta (lovingkindness) meditation.</strong></i><strong>&nbsp;</strong></p><p><a href=\"https://twitter.com/search?q=%40nickcammarata%20mdma&amp;src=typed_query&amp;f=top\"><strong><u>MDMA</u></strong></a><strong>:</strong> I am not recommending that people take MDMA, because that would be illegal, and because I have no idea what your situation is. If you intend to take MDMA, please do some research on safety (e.g. read at least <a href=\"https://rollsafe.org/mdma/\"><u>this</u></a> and <a href=\"https://rollsafe.org/mdma-supplements/\"><u>this</u></a>) to get a sense of the costs, and because you can substantially reduce risks and side effects if you do decide to take it. Here is my impression of the benefits: MDMA makes you feel a lot of love--very likely a <i>lot</i> more than you\u2019ve ever experienced, possibly orders of magnitude more--including self-love. I\u2019ve seen and heard of many people experiencing <i>extremely large and lasting</i> improvements to self-love when they take MDMA alone, close their eyes, and focus on investigating their experiences--including how they relate to themselves. This accords with preliminary <a href=\"https://www.nature.com/articles/s41591-021-01336-3\"><u>research</u></a> on the efficacy of MDMA-assisted therapy for PTSD. My guess for why this happens is that MDMA is extremely good at memory reconsolidation <i>a la </i><a href=\"https://www.lesswrong.com/posts/i9xyZBS3qzA8nFXNQ/book-summary-unlocking-the-emotional-brain\"><u>Unlocking the Emotional Brain</u></a>, presumably because it makes painful experiences/memories safe to look at and the love makes them easy to rewrite. Another benefit is that it gives you some information about what it\u2019s like to have self-love--for example, you might for the first time experience complete self-acceptance while also caring deeply about doing things and changing, and that\u2019s cool if that was a crux for you wanting more self-love. It also substantially clarifies what to aim for when sober, which is important:&nbsp;</p><blockquote><p>Nick: \u201cI can't overstate how impossible it would have been for me to get to a state of self-love without MDMA, even after hundreds of hours of metta (which I did before the MDMA). Not sure it'll be the case for everyone, but I suspect it makes things way easier\u201d.</p></blockquote><p><strong>Lovingkindness meditation (metta):</strong> Metta is a slower way to increase your capacity for love, albeit substantially. You could think of metta as doing reps to strengthen the love muscle (but more beautiful than that). Alongside love, metta also builds awareness, <a href=\"https://www.shinzen.org/wp-content/uploads/2016/08/WhatIsMindfulness_SY_Public_ver1.5.pdf\">indistractability, sensory clarity, and equanimity</a>--all of which are pretty useful for self-love. Below, I discuss some introspection techniques that might be useful. One reason to expect metta to be better than those techniques is that, when you\u2019re good at it, metta has feedback loops that can get you into very high self-love states. <strong>Resources:</strong> The canonical metta book is <a href=\"https://www.amazon.co.uk/Lovingkindness-Revolutionary-Happiness-Shambhala-Library/dp/1611806240/ref=sr_1_1?dchild=1&amp;keywords=lovingkindness&amp;qid=1635348283&amp;s=books&amp;sr=1-1\"><u>this</u></a> one, but I think it\u2019s mostly good for giving you models and not for practice. Kaj Sotala says that lots of people find <a href=\"http://library.dhammasukha.org/uploads/1/2/8/6/12865490/a_guide_to_twim.pdf\"><u>TWIM</u></a> really effective. <a href=\"https://www.youtube.com/watch?v=7Jb72-QgXOc\"><u>Here\u2019s</u></a> a guided meditation and <a href=\"http://www.leighb.com/DoYouLikeToBeHappy.mp3\"><u>here\u2019s</u></a> one with a different style. You can do <a href=\"https://www.amazon.co.uk/Mind-Illuminated-Meditation-Integrating-Mindfulness/dp/1781808201\"><u>concentration meditation</u></a> with love as the object of concentration instead of the breath, and can get coaching for that <a href=\"https://rationaldharma.com/meditation-teaching/\"><u>here</u></a>.&nbsp;</p><p>&nbsp;</p><h2>Other things that might help</h2><p>I wrote this section for someone like me a year ago--someone who strongly wants self-love and is desperate to read anything they can about how to get it. Consequently, this section is long and lower-quality; feel free to skip it!</p><p><strong>Deepen your understanding of self-love</strong>: If you have an hour or two, <a href=\"https://twitter.com/search?q=%40nickcammarata%20self-love&amp;src=typed_query&amp;f=top\"><u>searching</u></a> for \u2018@nickcammarata self-love\u2019 might be the best use of your self-love time. You could also try to spend lots of time with/read/take workshops with/take retreats with people who are really good at self-love. I don\u2019t know of specific people but you might find some within Nick\u2019s Twitter circles and (lovingkindness/metta) meditation communities (Tara Brach, Sharon Salzburg).&nbsp;</p><p><strong>Be with yourself:</strong> Being with yourself (your experiences) is the training ground for self-love. It is hard to become your best friend if you do not know yourself. Being with yourself requires some baseline self-love, though--it might not be good or advisable at first. One idea is to walk around without external input when possible. You can also be with yourself whenever you notice suffering, or even moment-to-moment (e.g. while working)--though this requires some skill to be able to do with little cost (see <a href=\"https://expandingawareness.org/courses/\"><u>this</u></a> course). I refresh my awareness about my experiences very frequently, and sometimes the awareness is roughly continuous.</p><p><strong>Figure out what you believe</strong>: The ability to self-love seems strongly mediated by ones (implicit) beliefs about whether it\u2019s safe and good to do so. So I would focus on figuring out what you believe. Indeed, many of the introspection techniques I list below work to facilitate this process, and could be done with this process in mind. You can ask yourself, with gentle curiosity, why you don\u2019t want self-love or why you think it\u2019s good to make your self-worth conditional. This is important because there are probably reasons, and your current set-up might be doing something very useful (such as <a href=\"https://replacingguilt.com/\"><u>guilt-based motivation</u></a>). And until you understand those functions it will be hard and maybe bad to shift things up.</p><p><strong>Introspection/therapy techniques</strong>: Explaining each technique well is beyond the scope of this post, but I have linked to a short blog post and a more comprehensive resource where possible.</p><ul><li><a href=\"https://www.lesswrong.com/posts/i9xyZBS3qzA8nFXNQ/book-summary-unlocking-the-emotional-brain\"><u>Coherence therapy</u></a> (<a href=\"https://www.amazon.co.uk/Unlocking-Emotional-Brain-Bruce-Ecker/dp/0415897173\"><u>book</u></a>) and <a href=\"https://www.lesswrong.com/posts/bNHzQyMrSx8ZKKxYk/memory-reconsolidation-for-self-affection\"><u>memory reconsolidation for self-affection</u></a>. I think these posts are worth reading even if you don\u2019t intend to practice the techniques, because they have useful models of how therapy progress works.</li><li><a href=\"https://www.lesswrong.com/tag/focusing\"><u>Focusing</u></a> (<a href=\"https://www.amazon.co.uk/Power-Focusing-Finding-Inner-Voice/dp/157224044X/ref=sr_1_1?dchild=1&amp;keywords=the+power+of+focusing&amp;qid=1635348097&amp;s=books&amp;sr=1-1\"><u>book</u></a>) is a bread-and-butter self-inquiry technique. Getting good at focusing will probably aid many self-love strategies. You can also use focusing to enquire directly about self-love. <a href=\"https://www.rationality.org/resources/coaching/profiles/jack\"><u>Jack</u></a> from CFAR facilitates high-quality and very reasonably-priced focusing sessions.</li><li><a href=\"https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model\"><u>Internal family systems</u></a> (<a href=\"https://www.amazon.co.uk/Self-Therapy-2nd-Edition/dp/B010FX6DKC/ref=sr_1_1?keywords=self-therapy&amp;qid=1636202391&amp;s=books&amp;sr=1-1\"><u>book</u></a>) is a type of therapy that works with your parts. You could try to find an IFS therapist but I expect the average therapist to bad.</li><li>I haven\u2019t tried this but my therapist (who I trust) recommends <a href=\"https://www.cambridge.org/core/journals/advances-in-psychiatric-treatment/article/introducing-compassionfocused-therapy/ECBC8B7B87E90ABB58C4530CDEE04088\"><u>compassion-focused therapy</u></a>. \u201cThe primary focus of CFT is identifying sources of resistance to (self-)compassion and then building and strengthening the compassionate self (the Healthy Adult mode in Schema Therapy; the Wise Mind in Dialectical Behavior Therapy).\u201d</li><li>I just heard about <a href=\"https://neuroticgradientdescent.blogspot.com/2019/07/core-transformation.html\"><u>Core Transformation</u></a> (<a href=\"https://www.amazon.co.uk/gp/product/0911226338/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1\"><u>book</u></a>) and it seems really cool but I don\u2019t know how cool. Maybe read the <a href=\"https://neuroticgradientdescent.blogspot.com/2019/07/core-transformation.html\"><u>blog post</u></a>?</li><li>Kaj Sotala says he got significant value from guided Ideal Parent Figure practice (<a href=\"https://www.youtube.com/watch?v=EAcUlVEbAtg\"><u>guided meditation</u></a>, <a href=\"http://attachmentrepair.com/\"><u>course</u></a>, <a href=\"https://smile.amazon.com/Attachment-Disturbances-Adults-Treatment-Comprehensive-ebook/dp/B016APOD0G/\"><u>book</u></a>--see chapter 8). The idea is that a lot of our emotional conditioning around self-worth comes from childhood, where we learn what kinds of behaviours get us love and acceptance from our caregivers. IPF exploits the fact that the emotional brain doesn't fully distinguish between the real and the imagined, so you can reprogram your mind by imagining yourself as a young child with ideal parents who always express unconditional love and delight towards you.</li><li>The <a href=\"https://self-compassion.org/category/exercises\">exercises</a> from <a href=\"https://self-compassion.org/\">Self-Compassion</a> (<a href=\"https://www.amazon.co.uk/Self-Compassion/dp/B07Z8BDVG2/ref=tmm_aud_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=\">book</a>) seem pretty good. I read the book a while ago but can't remember how good it was.</li></ul><p><strong>Miscellaneous:</strong></p><ul><li>Other meditation: either <a href=\"https://www.amazon.co.uk/Mind-Illuminated-Meditation-Integrating-Mindfulness/dp/B07NPY1GFR/ref=sr_1_1?crid=1NLX64G3LZCEV&amp;dchild=1&amp;keywords=the+mind+illuminated&amp;qid=1635337753&amp;sprefix=the+mind+i%2Caps%2C211&amp;sr=8-1\"><u>concentration</u></a> (this is mostly what I did) or <a href=\"https://www.shinzen.org/wp-content/uploads/2016/08/SeeHearFeelIntroduction_ver1.8.pdf\"><u>noting</u></a> (what I weakly recommend now). These techniques build skills <a href=\"https://www.shinzen.org/wp-content/uploads/2016/08/WhatIsMindfulness_SY_Public_ver1.5.pdf\"><u>(concentration, sensory clarity, equanimity</u></a>, and awareness) that I expect to indirectly affect all of your other self-love endeavors (including MDMA). You can get coaching for concentration (and metta) meditation <a href=\"https://rationaldharma.com/meditation-teaching/\"><u>here</u></a>.</li><li><a href=\"https://expandingawareness.org/courses/\"><u>Expanded awareness</u></a>: A big part of self-love is holding your experience in awareness. This course will help you do that.</li><li>Therapy.</li></ul><p><strong>Other books:</strong></p><ul><li><a href=\"https://www.amazon.co.uk/Radical-Acceptance-Awakening-Heals-Shame/dp/B088KT5Q6V/ref=sr_1_1?keywords=radical+acceptance&amp;qid=1636127553&amp;s=books&amp;sr=1-1\"><u>Radical Acceptance</u></a> (I remember really enjoying this). It worked strongly on the belief level for me. Some people might be a bit allergic to the Buddhist stuff.</li><li><a href=\"https://replacingguilt.com/\"><u>Replacing Guilt</u></a> (is truly awesome). Also works really strongly on the belief level. Written for people like you.</li><li><a href=\"https://www.amazon.com/Mindful-Compassion-Science-Understand-Emotions/dp/1626250618\"><u>Mindful Compassion</u></a> (my therapist recommends this and I trust her but I haven\u2019t read it).</li></ul>", "user": {"username": "CharlieRS"}}, {"_id": "WhYpFfLRGyvibiPCt", "title": "[Linkpost] New post on COP26-related grants from the FP Climate Fund", "postedAt": "2021-11-14T13:55:21.416Z", "htmlBody": "<p>This is a linkpost for <a href=\"https://founderspledge.com/stories/the-ideas-of-economists\">\"The ideas of economists\"</a>, a new blog post that describes two grants we made from the FP Climate Fund around COP 26, focused on changing conversations (and, subsequently, action) around (a) the fact that expensive-looking early-stage deployment policies are much better than their reputation due to strong trajectory-changing innovation effects and (b) the challenge of committed emissions, lots of new coal plants with long lifetimes left, and the importance of a broader solution set to deal with this problem (including advanced nuclear for repowering).<br><br>As always, this is an FP blog post so the style is a bit different than EA Forum post, but I thought it would be good to share here given how many EAs are contributing to the Climate Fund.</p>", "user": {"username": "jackva"}}, {"_id": "oB6CEDaqFfzkJw875", "title": "What the future will look like", "postedAt": "2021-11-14T00:54:47.866Z", "htmlBody": "<p>The alarm rings \u2014 gentle, cascading sounds, increasing gradually in volume, to softly rouse you from your REM cycle. The sound radiates from micro-speakers distributed around your room; the 360\u00b0 soundscape makes it sound like the walls are singing, beckoning you to take on a new day.</p><p>You wake up rejuvenated, and peel off the neuro-pellets from the side of your head, that quietly stimulated delta-waves in your brain through the stages of deepest sleep, accelerating cognitive regeneration while you explored a different world \u2014 one that hasn\u2019t been touched by humans yet.</p><p>\u201cAliza, play today\u2019s <i>Robinhood Snacks </i>podcast<i>,\u201d </i>you<i> </i>tell your voice-enabled smart-home device, one that you built yourself in minutes from a DIY-kit you found online, because you know better than to install big-tech listening devices in your own home.</p><p>The podcast continues to play as you enter the bathroom \u2014 the audio syncs across the house, switching on and off as you enter each room to conserve energy. Speaking of energy, you wonder how much you saved yesterday. You click into an app, and a giant dashboard tells you you hit this month\u2019s goals \u2014 <i>\u201cHuzzah! Check your texts for a special reward.\u201d </i>Amazing, they\u2019ve sent you $10 in cryptocurrency for being a good energy citizen.</p><p>Your phone knows what to do with rewards \u2014 you\u2019ve trained it well to manage personal finances. 10% goes to your favourite social cause \u2014 AI Safety Research. You volunteer with a local nonprofit that is working to keep humans and machines safe from each other by aligning their goals, and you\u2019re their top contributor. 50% is distributed across your investment portfolio with predetermined weighting, and you use the remaining 40% to invest in metaverse assets \u2014 last time you bought a tiny bit of land in Decentraland-Iceland, and it\u2019s looking beautiful, so you buy a few more square-feet. In September, the developers ship property-owners special VR-headsets that click into your neuro-pellets, creating the most dazzling multisensory <i>Northern Light</i>s experience, transporting you to the 2020s when the lights were scintillant and unclouded by pollution and space-dust.</p><p>The podcast ends, and you add a small tip for the creators before going to make breakfast \u2014 people who don\u2019t tip creators are so rude, you think. The fridge displays the recommended breakfast items \u2014 based on your caloric history, the weather, and your previous meals \u2014 but you pick frozen waffles because it\u2019s Friday and it\u2019s been a long week. Quickly you project different outfits on <i>ARfit</i>, a closet assistant that shows you what you would look like in a range of outfit combinations \u2014 you only go into the office once a week, so you want to look good. <i>Ugh. </i>You forgot to order this week\u2019s fit, so you call the nearest warehouse listed on lastminute.com and ask for expedited shipping on item-number <i>JS823GJ. </i>Ten minutes and a two toasted waffles later, it is at your doorstep. The vacuum-packing inside the delivery drone left the shirt creased\u2026 you shrug. Nothing a bit of steam-ironing can\u2019t fix.</p><p>Your phone beeps, the calendar app reminding you to call a driverless car for pickup. But a beautiful summer day awaits, and you can\u2019t think of a reason not to enjoy it. It is decided \u2014 you will walk to work.</p><p>You put on your AR-enabled glasses \u2014 they\u2019re fancier than what you would usually wear, but that they were solar-powered and DuckDuckGo-enabled justified the purchase. You see a homeless man across the street, outside the Safeway, an AR QR-code hovering above his head. You surreptitiously scan it with your phone, and it takes you directly to checkout, with a meal in the cart, ready for delivery to his precise geolocation. <i>Why not</i>, you reason, completing the anonymous one-click checkout.</p><p>The next street is lined with shops of all kinds, selling everything imaginable \u2014 groceries, clothes, shoes, antiques, toys\u2026 toys! It\u2019s your niece\u2019s birthday tomorrow, and the little AR Cryptopuppy bobbing up and down in the window is adorable. \u201cHow much for the doggy in the window,\u201d you chuckle, and click it to find out. \u201cEnjoy the little blockchain-based collectible,\u201d you write in the message at checkout, and schedule-send it to her for tomorrow.</p><p>As you continue your walk, the morning sun dances on the sidewalk in crescent-shadows, an omnipresent reminder of the massive coordinated effort between world superpowers to harness the energy of the sun to power the millions of servers and central-processing units that make the world go round. For a second it hits you \u2014 the scale at which humanity is operating, the audacity of the species to interface directly with the Sun, the technological prowess your generation has inherited from millennia of humans preparing for such conquests, and you are distracted by the wafting aroma of fresh bread.</p><p>\u201cEnough capitalism for today\u201d, you decide, as you click off your AR-glasses, and get ready for a new day of work.</p><figure class=\"image image_resized\" style=\"width:728px\"><img src=\"https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F82169bc9-0781-4448-84ed-0882b7876ac8_500x350.jpeg\"></figure><hr><p>The purpose of this vignette is to provide a techno-optimistic vision of the future. Too often, futuristic fiction focuses on machines as the centerpiece of the narrative. Or human characters' lives are no longer relatable, because advanced technologies have radically transformed them. The disparity in experience and relatability between readers and characters can cast a dystopian shadow on the futuristic universe, making frontier-tech seem unattractive - almost creepy.</p><p>The vignette shows that advanced technology can substantially improve the quality of human life without radically disrupting it, and serves as a symbol for what can be achieved if we continue to prioritise the safe development of technological systems.</p><p>We need human-centered, techno-optimistic literature to frame the scope of possibility for future creators.</p>", "user": {"username": "avantika.mehra"}}, {"_id": "kvNbjmrLoTB3QfSa5", "title": "$15,000 (est) finder fee for finder and EA cause", "postedAt": "2021-11-13T22:01:51.936Z", "htmlBody": "<p>Hi. We have a property, over 11 acres undeveloped residential, available in Germantown, TN. Germantown is the most expensive suburb of Memphis, and it is believed the property can be developed into 12 to 20 houses. Recent nearby sales for houses is over $600k per house. (Keep in mind the median house price in Memphis is about $120k). We currently have the property listed for $1.5 million, but the price is negotiable upwards or downwards.</p><p>If you can connect us to a buyer for part or all of this property, we would pay a 1% of the sale bounty, 2/3 to you and 1/3 to an EA aligned charity of your choosing (so long as it is EA aligned). For example, if you connected us to a buyer who bought the whole property for $1.5 million, then of the 1% ($15,000), you'd get $10k and your EA charity would get $5k. Or if you connected us to someone who bought part of it, say for $1,000,000, then you'd get $6666.67 and your EA charity would get $3333.33.&nbsp;<br>&nbsp;</p><p>If you find a potential buyer, please let me know so I can ensure you and your EA charity get paid. My email address ends with ATberkeleyDOTedu and starts with mrprasad<br>&nbsp;</p><p>For more information on the property, visit: https://www.zillow.com/homes/2167-Sunset-Rd-Germantown,-TN-38138_rb/42358398_zpid/&nbsp;</p>", "user": {"username": "Mahendra Prasad"}}, {"_id": "bwhDhZQvbEcG4FEb8", "title": "Preprint is out! 100,000 lumens to treat seasonal affective disorder", "postedAt": "2021-11-13T18:37:34.802Z", "htmlBody": "<p>Let\u2019s give people with winter depression (seasonal affective disorder, SAD) LOTS OF LIGHT and see what happens!</p><p>Our preprint is out now for our paper \u201c100,000 lumens to treat seasonal affective disorder: A proof of concept RCT of bright, whole-room, all-day (BROAD) light therapy\u201d! We have sent it to a number of professors working in that area and have received very encouraging and helpful feedback, which has made me even more excited about continuing this research.</p><p>\u2192 Paper: <a href=\"https://medrxiv.org/cgi/content/short/2021.10.29.21265530v1\">https://medrxiv.org/cgi/content/short/2021.10.29.21265530v1</a>, short summary Twitter thread: <a href=\"https://twitter.com/FabienneSand/status/1457745472773296128\">https://twitter.com/FabienneSand/status/1457745472773296128</a></p><p>Jan Brauner and I are very thankful to the LessWrong/EA communities, which have inspired this first study (there will be more) and through which we have found funding. In particular, thank you Eliezer Yudkowsky for helping us find funding and for inspiring the study with Inadequate Equilibria, David Chapman for inspiring us with <a href=\"https://meaningness.com/sad-light-lumens\">these</a> <a href=\"https://meaningness.com/sad-light-led-lux\">two</a> posts in the Meaningness blog, Raemon for inspiring us with <a href=\"https://www.lesswrong.com/posts/hC2NFsuf5anuGadFm/how-to-build-a-lumenator\">this</a> LessWrong post and everyone who discussed with us setups they have tried. &lt;3</p>", "user": {"username": "Fabienne"}}, {"_id": "K2cmHxscYFtrK5np8", "title": "[Linkpost] GiveWell money moved in 2020 - up by 60%!", "postedAt": "2021-11-13T17:25:20.209Z", "htmlBody": "<p>Linking a post from the <a href=\"https://blog.givewell.org/2021/11/12/givewells-money-moved-in-2020/\">GiveWell blog</a> on the amount of money they moved in 2020.</p><p>Some notable stats:</p><ul><li>GiveWell donors contributed over $240 million to our recommended charities in 2020, a <strong>60% increase </strong>from the approximately $150 million directed in 2019.</li><li>GiveWell raised $43.6 million in unrestricted funding in 2020, compared to $19 million in 2019. GiveWell\u2019s total operating expenses in 2020 were $8.5 million.<ul><li>Seems like GiveWell is having no trouble fundraising! In the <a href=\"https://files.givewell.org/files/metrics/GiveWell_Metrics_Report_2020.pdf\">full metrics report</a> they write: In July 2021, GiveWell's Board of Directors voted to reallocate $25.8 million in unrestricted funds to making grants at GiveWell's discretion.</li><li>I wonder how much they're considering expanding or scaling up their research/team in light of this funding. They have <a href=\"https://www.givewell.org/about/jobs\">two open posts</a> for senior researchers but I wonder how many of each they'll be planning to hire.</li></ul></li></ul><p>&nbsp;</p><p>Some related thoughts:</p><ul><li><a href=\"https://twitter.com/ben_j_todd/status/1454093966299877380\">Ben Todd recently saying </a>that GiveWell top charities (besides GiveDirectly) don't have too much room for more finding. If this is true when GiveWell were moving $160m/year, seems like it'll only be exacerbated if GiveWell is moving $240m/year.</li><li>The funding gap for GiveWell top charities excluding GiveDirectly was <a href=\"https://docs.google.com/spreadsheets/d/11HsJLpq0Suf3SK_PmzzWpK1tr_BTd364j0l3xVvSCQw/edit?usp=sharing\">$73.2m as of July 2021</a> and $455.5m for GiveDirectly. Seems like GiveWell might have substantially filled the funding gap for their top charities excluding GiveDirectly so I wonder what the implications of this are going forward.</li><li>Does this mean we should spend a lot more trying to find near-termist interventions of similar effectiveness to AMF e.g. scale up GiveWell, Rethink Priorities (Global Health &amp; Development team), etc. as fast as we can? Although it seems like Rethink Priorities is expanding fairly fast and I can imagine the same for GiveWell so there could be a management/recruitment bottleneck.</li><li>If the above organisation's expansion is limited by their ability to recruit or manage new staff, could this be incentive for new organisations to start who do near-termist charity/intervention/cause research? There could be an argument that new organisations would bring a different approach or set of assumptions to the work that encourages healthy competition. &nbsp;Plus it does seem like <a href=\"https://forum.effectivealtruism.org/posts/MSYhEatxkEfg46j3D/the-case-of-the-missing-cause-prioritisation-research\">people</a> think more prioritisation research could be useful. The downside is possible duplication of work but I think this is outweighed by the potential benefits of finding new AMF-like opportunities.</li><li>To what degree do we lower the funding bar and try to fill the funding gap for GiveDirectly or wait until research discovers opportunities on par/close to AMF's cost-effectiveness? &nbsp;I get the impression that people most concerned about global health and development don't want to invest or put aside money to help people later on so lowering the bar seems possible. In practice, I assume it'll be a bit of both but I'm curious how the allocation of funds would look like e.g. 50% of surplus funding after GiveWell top charities are fully funded goes towards GiveDirectly and the other 50% is invested.</li><li>Is there a case for funding the <a href=\"https://blog.givewell.org/2021/10/05/discontinuing-standout-charity-designation/\">standout charities</a> that GiveWell no longer recommends?</li></ul>", "user": {"username": "JamesOz"}}, {"_id": "5gPubzt79QsmRJZnL", "title": "Minimalist axiologies and positive lives", "postedAt": "2021-11-13T10:57:04.391Z", "htmlBody": "<p><i>This is part two of a series on minimalist axiologies (i.e.&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#1__What_is_axiology_\"><i><u>axiologies</u></i></a><i> that essentially say \u201cthe less this, the better\u201d).</i></p><p><i>Every part of this series builds on the&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused\"><i><u>previous</u></i></a><i>&nbsp;parts, but can also be read independently.</i></p><h2><strong>Summary</strong></h2><p>Minimalist views of value (axiologies) are evaluative views that define betterness solely in terms of the absence or reduction of independent bads, such as suffering. This chapter looks at minimalist axiologies that are impartial and welfarist (i.e. concerned with the welfare of all sentient beings), with a focus on their theoretical and practical implications. For example, these views reject the \u2018Very Repugnant Conclusion\u2019 implied by <a href=\"https://forum.effectivealtruism.org/posts/oJJLgJTsQKX3oQ9xw/minimalist-views-of-wellbeing#Reasons_to_doubt_the_offsetting_premise__A_brief_overview\">offsetting (\u2018good minus bad\u2019)</a> views in population ethics.</p><p>Minimalist views are arguably neglected in population ethics due to their apparent implication that no life could be axiologically positive. After all, they reject the concept of independent goods. Yet minimalist views are perfectly compatible with the notion of relational goods, and can thereby endorse relationally positive lives that make a positive difference for other beings.</p><p>Unfortunately, this notion of relationally positive value is entirely excluded by the standard, restrictive assumption of treating lives as isolated value-containers. However, this&nbsp;<i>ceteris paribus</i> assumption is practically always false, enabling the possibility of highly positive lives according to minimalist views.</p><p>Minimalist views become more intuitive when we adopt a relational view of the overall value of individual lives, that is, when we don\u2019t track only the causally isolated \u201ccontents\u201d of these lives, but also their (often far more significant) causal roles.</p><h2><strong>1. What is axiology?</strong></h2><p><a href=\"https://en.wikipedia.org/wiki/Axiology\"><u>Axiology</u></a> is the philosophical study of value. The field of axiology is specifically concerned with the question of what things, if any, have <i>independent</i> value, also known as <i>intrinsic</i> value. \u2018Axiologies\u2019 in the plural refer to specific views on this axiological question. Once we assume a specific axiology that ascribes independent value to certain entities or states, we may see the value of all other things as <i>extrinsic</i>, <i>instrumental, </i>or <i>relational</i> in terms of their effects on these entities or states.</p><p>This distinction applies at the level of our assumed axiology, and not necessarily at the level of our everyday perception: we may both formally deny that something has independent value, and also be right to practically <i>feel</i> that it does have value \u2014 without explicitly \u201cunpacking\u201d what this value depends on \u2014 such as when we treat some widely-held values as <a href=\"https://forum.effectivealtruism.org/s/MBadsrYLmzLNmYjaj/p/t3St6Fz4DmHtKfgqm#2__Possible_misconceptions_about_instrumental_value\"><u>valid heuristics</u></a> until they run into conflicts with each other. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#5__What_do_these_views_imply_in_practice_\"><u>section on practical implications</u></a>, and in a future post on multi-level minimalism.)</p><p>Commonly, we seek clarity about the nature of independent value by listening to what our supposedly value-tracking intuitions say about certain thought experiments. For example, we construct thought experiments where only a single thing is intended to be changing, \u201call else equal\u201d, and ask whether it feels true that this change is accompanied by a change in value. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>section on the \u201call else equal\u201d assumption</u></a>.)</p><p>Based on such thought experiments of isolated value, we might feel that more blissful mind-moments mean more value, and that more agonized mind-moments mean more disvalue, and thereby come to follow two independent standards of value: <i>\u201cThe more bliss, the better, all else equal\u201d</i> (BliMax), and <i>\u201cThe less agony, the better, all else equal\u201d </i>(AgoMin).</p><p>Dilemmas famously arise when we want to follow <i>both</i> BliMax and AgoMin, as they are not always perfectly anticorrelated. That is, we often cannot <i>both</i> \u201cmaximize bliss\u201d <i>and</i> \u201cminimize agony\u201d, because even as these two guiding principles may<i> seem</i> to be polar opposites of each other, they do not always constitute a coherent twin-principle similar to <i>\u201cHead North, Avoid South\u201d</i>. The field of population axiology has highlighted many ways in which BliMax and AgoMin pull us into mutually incompatible directions, as well as the lack of consensus on how to compare the supposed independent value of bliss with the supposed independent disvalue of agony. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#3__How_do_these_views_help_us_make_sense_of_population_ethics_\"><u>section on population ethics</u></a>.)</p><h2><strong>2. What are minimalist axiologies?</strong></h2><h3><strong>The less this, the better</strong></h3><p>Minimalist axiologies may be a suitable name for the class of axiologies whose central conception of independent value is of the kind that says <i>\u201cThe less this, the better.\u201d </i>In other words, their fundamental standard of value is about the <i>avoidance</i> of something, and not about the maximization of something else. To list a few examples, minimalist axiologies may be formulated in terms of avoiding cravings (<a href=\"https://longtermrisk.org/tranquilism/\"><u>tranquilism</u></a> seen as a welfarist monism; certain <a href=\"https://blogs.dickinson.edu/buddhistethics/files/2015/12/Breyer-Axiology-final.pdf\"><u>Buddhist</u></a> <a href=\"https://www.utilitarianism.com/nu/nufaq.html#2.2\"><u>axiologies</u></a>); disturbances (<a href=\"https://www.simonknutsson.com/epicurean-ideas-about-pleasure-pain-good-and-bad/\"><u>Epicureanism</u></a>); pain or suffering (<a href=\"https://en.wikisource.org/wiki/Studies_in_Pessimism/On_the_Sufferings_of_the_World\"><u>Schopenhauer</u></a>; <a href=\"https://en.wikiquote.org/wiki/Richard_D._Ryder#Painism:_A_Modern_Morality_(2001)\">Richard</a> <a href=\"https://www.amazon.com/Painism-Morality-Richard-D-Ryder/dp/0900001461/\">Ryder</a>); frustrated preferences (<a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>antifrustrationism</u></a>); or unmet needs (some interpretations of <a href=\"https://iep.utm.edu/care-eth/\"><u>care ethics</u></a>).</p><p>This essay looks at minimalist axiologies that are interpreted as <a href=\"https://plato.stanford.edu/entries/value-theory/#Mon\"><i><u>monist</u></i></a><i>,</i> <a href=\"https://plato.stanford.edu/entries/impartiality/\"><i><u>impartial</u></i></a><i>,</i> and <a href=\"https://en.wikipedia.org/wiki/Welfarism\"><i><u>welfarist</u></i></a> \u2014 i.e. concerned with the welfare of all sentient beings \u2014 and is not meant to apply to axiologies that are pluralist, partial, or concerned with non-welfarist avoidance goals, such as minimizing human intervention in nature, or avoiding the loss of unique information. (Pluralism can still be introduced at the level of practical <a href=\"https://www.utilitarianism.net/types-of-utilitarianism#multi-level-utilitarianism-versus-single-level-utilitarianism\"><u>decision procedures</u></a>. More in a future post on multi-level minimalism.)</p><p>In \u201csacred value tradeoffs\u201d between multiple (<i>seemingly</i>) independent standards of value, minimalist axiologies avoid the problem of having to determine independent<i> </i>\u201cpriority weights\u201d for different <i>intrinsic</i> values in order to resolve their mutual conflict \u2014 such as the conflict between creating <i>bliss</i> for many at the cost of <i>agony</i> for others. In other words, instead of using multiple standards of value, such as both BliMax and AgoMin, minimalist axiologies construe \u201cpositive value\u201d in a purely relational way, i.e. with regard to their overall avoidance goal for all beings. This enables value comparisons to be made under a shared standard of value.</p><p>When we look at only one kind of change, <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>all else equal</u></a>, it may seem intuitive that bliss is independently good and agony is independently bad. Yet many people may also feel internally conflicted about tradeoffs where value and disvalue would need to be compared with each other (so that we could say whether some tradeoff between them is \u201cnet positive\u201d or not). To solve these dilemmas, minimalist axiologies would respect promotion intuitions to the degree that they are conducive to the overall avoidance goal, but draw a line before agreeing to create more (isolated) value for some at the cost of disvalue for others.</p><p>For example, suffering-focused minimalism would promote <i>happiness in the place of suffering</i>, but <i>not at the cost of suffering</i>, all else equal (cf. Vinding, <a href=\"https://magnusvinding.files.wordpress.com/2020/05/suffering-focused-ethics.pdf\"><u>2020b</u></a>, Chapter 3). To illustrate, let us compare two different ways to weigh the pros and cons of the <a href=\"https://en.wikipedia.org/wiki/Intensive_animal_farming\"><u>factory farming</u></a> of sentient beings:</p><ol><li>To the extent that this process entails a lot of suffering, some might argue that this process is still \u201cworthwhile\u201d due to the happiness that it also entails or promotes in others \u2014 i.e.<i> happiness at the cost of suffering.</i></li><li>Conversely, suffering-focused minimalism would reject the implicit pluralism of justifying suffering with happiness. Instead, it would \u201ccompare suffering with suffering\u201d by looking at relations like, \u201cWhat is this happiness needed for?\u201d or \u201cDoes this happiness help prevent worse suffering than what it costs?\u201d \u2014 i.e. <i>happiness in the place of suffering, </i>or<i> happiness as a </i><a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused#4_1_Wellbeing_as_a_resource\"><i><u>way</u></i></a><i> to prevent worse suffering.</i></li></ol><p>In other words, minimalist axiologies sidestep the problem of having to find acceptable \u201ctradeoff ratios\u201d between different independent values. These views reject the \u201c<a href=\"https://en.wikipedia.org/wiki/Additive_utility\"><u>board game</u></a>-like\u201d logic of placing different amounts of positive value on different kinds of things, which can be replaced with a focus on the relational question of how the objects of our promotion intuitions could help with the overall avoidance goal.</p><p>A common misunderstanding of this \u201cdenial of positive value\u201d relates to the mismatch between abstract <i>population theory</i>, where we are dealing with causally isolated lives subject to an assumption of \u201call else equal\u201d, and the real world, where lives can be <i>relationally</i> positive, even on minimalist axiologies, precisely because they can make a positive difference for the lives of others. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>section on the \u201call else equal\u201d assumption</u></a>.)</p><h3><strong>Contents versus roles</strong></h3><p>Our practical intuitions about the value and worthwhileness of lives are arguably correct in saying that <i>something crucial is lacking</i> if we naively translate our abstract population theory directly into practice and think that \u201cno life could be positive\u201d. After all, as soon as we break the artificial boundaries of individual lives as \u201cisolated value-containers\u201d \u2014 which they are usually treated as in population ethics \u2014 we return to the practical world where lives virtually always have significant effects on other lives. <a href=\"https://www2.psych.ubc.ca/~henrich/pdfs/WeirdPeople.pdf\"><u>Western</u></a> culture may condition us to think of <a href=\"https://forum.effectivealtruism.org/posts/LJwGdex4nn76iA8xy/some-blindspots-in-rationality-and-effective-altruism#5__We_view_individuals_as_independent\"><u>individuals</u></a> as \u201cindependent, self-contained, autonomous entities\u201d, and to pay insufficient attention to a more relational perspective. Yet for an analysis of the overall value of lives to reach any kind of practical relevance, we need to recognize that the concept of an \u201cindependent individual\u201d is a blind spot to be filled by an account of the interactions between lives. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#5__What_do_these_views_imply_in_practice_\"><u>section on practical implications</u></a>.)</p><p>Specifically, our intuitions about value arguably evolved in a fundamentally interpersonal world where we intuitively account not only for the \u201ccontents\u201d of individual lives, but also for their roles in other lives. Therefore, when we object to the idea that \u201cno life could be positive, all else equal\u201d, this need not stem from the sentiment that <i>\u201cSurely lives have at least some <strong>isolated</strong> positive value\u201d</i>. We might just as plausibly be objecting to the highly unrealistic assumption of \u201c<a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>all else equal</u></a>\u201d, which effectively strips these lives of all their positive effects on anyone, and thereby leaves us only with what these lives subjectively \u201ccontain\u201d in the absence of all their (often highly significant) <a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused\"><u>positive roles</u></a>.</p><p>As social animals, some of us may think that positive value is fundamentally not something that we \u201chave\u201d, \u201ccontain\u201d, or \u201caccumulate\u201d in isolation. Instead, our intuitions may be animated by a <i>relational</i> view that sees positive value as something that we \u201cdo\u201d for each other, and something that we cannot simply produce in causally isolated <a href=\"https://en.wikipedia.org/wiki/Experience_machine\"><u>experience machines</u></a> to make the world a better place. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#6__Without_the_concept_of_intrinsic_positive_value__how_can_life_be_worth_living_\"><u>section on lives worth living</u></a>.)</p><p>To the extent that our value intuitions track not only the \u201ccontents\u201d but also the social roles of individual lives, it may be difficult to determine the degree to which we think of positive value as an <i>independent</i> or <i>relational</i> phenomenon. Yet many counterintuitive conclusions in population ethics may be attributed to the assumption of positive value as an independent, and independently aggregable, phenomenon. That is, we may have good reasons to question the conception of positive value as a \u201cplus-point\u201d that could be summed up or stacked in isolation from the social roles of the lives that contain it, while still endorsing positive value in a strong, relational sense. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#3__How_do_these_views_help_us_make_sense_of_population_ethics_\"><u>section on population ethics</u></a>.)</p><p><i>(The clarification of how minimalist axiologies can support a notion of \u201ca life worth living\u201d will be a recurring theme throughout the remaining sections.)</i></p><h2><strong>3. How do these views help us make sense of population ethics?</strong></h2><p>The field of <a href=\"https://forum.effectivealtruism.org/tag/population-ethics\"><u>population</u></a> <a href=\"https://en.wikipedia.org/wiki/Population_ethics\"><u>ethics</u></a> is \u201cthe philosophical study of the ethical problems arising when our actions affect <i>who</i> is born and <i>how many</i> people are born in the future\u201d. A subfield of population ethics is <i>population axiology</i>, which is about figuring out what makes one state of affairs better than another. This is a famously tricky question to answer without running into counterintuitive conclusions, provided that we make the assumption of <i>independently</i> positive lives (cf. Arrhenius, <a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.227.8341&amp;rep=rep1&amp;type=pdf\"><u>2000a</u></a>). Minimalist axiologies do not make this assumption, and they neatly avoid the conclusions that are pictured in the three diagrams in the next three subsections.</p><p><i>(Note: The conclusions are named \u201cparadoxical\u201d or \u201crepugnant\u201d after the intuitions of people who find them troubling. Generally, people differ a lot in which intuitions they are willing to \u201cgive up\u201d in population ethics. Arguably, many would accept the </i><a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#The_Mere_Addition_Paradox\"><i><u>Mere-Addition Paradox</u></i></a><i>, some would accept the </i><a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#The_Repugnant_Conclusion\"><i><u>Repugnant Conclusion</u></i></a><i>, and few would accept the </i><a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#The_Very_Repugnant_Conclusion\"><i><u>Very Repugnant Conclusion</u></i></a><i>.)</i></p><p>Before looking at the diagrams, let us already note two ways in which people might implicitly disagree about how to interpret them.</p><p>First, the diagrams contain populations that should arguably be imagined to consist only of lives that never interact with each other. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>section on the \u201call else equal\u201d assumption</u></a>.)</p><p>Second, some of the diagrams contain a horizontal line that indicates a \u201czero level\u201d of \u201cneutral welfare\u201d, which may be interpreted in different ways. For example, when diagrams illustrating the (Very) Repugnant Conclusion contain lives that are \u201cbarely worth living\u201d, some may think that these lives involve \u201cslightly more\u201d happiness than suffering, while others may think that they \u201cnever suffer\u201d. (More in <a href=\"https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2#fn-rL6KptdovxyEp69MJ-16\"><u>footnote 16</u></a> in DiGiovanni, 2021.)</p><p>A different interpretation of the horizontal line is used in <a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>antifrustrationism</u></a> by Christoph Fehige (1998), which equates welfare with the avoidance of preference dissatisfaction (or \u201cfrustration\u201d). When Fehige\u2019s own diagrams contain the horizontal line, it just means the point above which the person \u201chas a weak preference for leading her life rather than no life\u201d (Fehige, 1998, p. 534). On Fehige\u2019s view, the lives with \u201cvery high welfare\u201d are still much better off than the lives \u201cbarely worth living\u201d. Yet if we assume that the lives above the horizontal line have <i>all</i> their preferences satisfied and \u201cnever suffer\u201d, then minimalist axiologies would find no (subjective) problems in the Mere-Addition Paradox or the Repugnant Conclusion. Even so, they would still not <i>strictly</i> prefer larger populations, finding all populations of such problem-free lives rather equally perfect (<a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>in causal isolation</u></a>).</p><p>However, it is arguably highly unrealistic to assume that the lives people usually refer to as \u201cbarely worth living\u201d would be subjectively <i>perfect</i> or \u201cnever suffer\u201d. Therefore, we will use Fehige\u2019s view as an extended example of how minimalist axiologies would actively reject the following conclusions. (For a similar axiology centered on experiences rather than preferences, see <a href=\"https://longtermrisk.org/tranquilism/\"><u>tranquilism</u></a> by Gloor, 2017.)</p><h3><strong>The Mere-Addition Paradox</strong></h3><p>Derek Parfit\u2019s <a href=\"https://en.wikipedia.org/wiki/Mere_addition_paradox\"><u>Mere-Addition Paradox</u></a> is based on a comparison of four populations. Each bar is a distinct group of beings. The bar\u2019s width indicates their numbers and the bar\u2019s height indicates their level of welfare. We assume that every being in this diagram has \u201ca life worth living\u201d. (The populations in <i>A</i>+ and <i>B\u2212</i> consist of two distinct groups that are \u201cdivided by water\u201d; the population in <i>B</i> is simply the two groups of <i>B\u2212 </i>combined into one.)</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/fesodla4a9e9re8zlal3\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/h37fcz3x0gcof5qp3loj 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/z68vzvwhyaovsjbx4dre 190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/h6raqmivf2gtbdm3fed1 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/aj1y0rbxskvjrgbhzteb 350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/mxipc3vd5rlsmpi76wxe 430w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/zniava6b55n0drkxagcr 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/jjifrqluwbkpu1e18aws 590w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/ouztxcdvrk3bbbr4lvil 670w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/tj2ueorxcg7zbxkgd26t 750w\"><figcaption><i>(A diagram of the Mere-Addition Paradox; </i><a href=\"https://en.wikipedia.org/wiki/Mere_addition_paradox\"><i><u>source</u></i></a><i>.)</i></figcaption></figure><p>The paradox results from the following comparisons that contradict some people\u2019s intuitive preference for the high-average population of <i>A </i>over the lower-average population of <i>B</i>:</p><ol><li>Intuitively, \u201c<i>A</i>+<i> </i>is no worse than <i>A,</i>\u201d since <i>A</i>+ simply contains more lives, all worth living.</li><li>Next, \u201c<i>B\u2212</i> is better than <i>A</i>+,\u201d since <i>B\u2212 </i>has <i>both</i> greater total welfare <i>and</i> greater average welfare.</li><li>Finally, \u201c<i>B\u2212 </i>is equal to <i>B,</i>\u201d since <i>B </i>is simply the same groups, only combined.</li><li>Now, \u201c<i>B </i>is better than <i>A,</i>\u201d based on steps 1\u20133.</li></ol><p>This paradox is a problem for people who strongly feel that \u201c<i>A </i>is better than <i>B</i>\u201d<i> </i>but who are also sympathetic to total utilitarianism \u2014 perhaps due to wanting to avoid average utilitarianism for its implying the <a href=\"https://en.wikipedia.org/wiki/Population_ethics#Sadistic_conclusion\"><u>sadistic conclusion</u></a>. Yet if we assume that subjective problems are experienced more by the lives in <i>B</i> than by the lives in <i>A</i>, then minimalist axiologies would prefer <i>A</i> over <i>B</i> without implying the sadistic conclusion.</p><p>Essentially, the solution of Fehige (<a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>1998</u></a>) is to assume that the welfare of a life depends entirely on its level of preference dissatisfaction (or \u201cfrustration\u201d). On this view, a population of perfectly (or almost perfectly) satisfied beings cannot, <i>other things being equal</i>, be improved by the \u201cmere addition\u201d of new, less satisfied beings. This is because the frustration of those new beings is an additional subjective problem in comparison to the non-problematic non-existence of their imaginary counterparts in the smaller population.</p><p>(We should remember that Fehige\u2019s use of the term \u201cpreference frustration\u201d is much broader than the everyday feeling that we call frustration; after all, basically all lives in the real world have at least some of their preferences frustrated, even if some may be free of the feelings of frustration.)</p><p>While this may be a theoretically tidy solution to the Mere-Addition Paradox, many critics have objected that it depends on a theory of welfare that they find to be counterintuitive, incomplete, or unconvincing. (These objections will be responded to in a future post.)</p><p>Overall, we would be wise to abstain from hastily dismissing minimalist axiologies as being absurd, because there are plenty of ways to interpret them in less absurd ways without losing their theoretical benefits. A lot of their intuitive absurdity might already result from the unrealistic thought experiments themselves, which imply that we are not actually comparing the value of lives of the kind that are familiar to us, but only of lives of a very peculiar and asocial kind, namely lives in complete causal isolation from each other. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>section on the \u201call else equal\u201d assumption</u></a>.)</p><h3><strong>The Repugnant Conclusion</strong></h3><p>Next, by continuing the logic of \u201cmere addition\u201d, we arrive at the <a href=\"https://plato.stanford.edu/archives/spr2014/entries/repugnant-conclusion/\"><u>\u2018Repugnant Conclusion\u2019</u></a>:</p><blockquote><p>In Derek Parfit's original formulation[,] the Repugnant Conclusion is characterized as follows: \u201cFor any possible population of at least ten billion people, all with a very high quality of life, there must be some much larger imaginable population whose existence, if other things are equal, would be better even though its members have lives that are barely worth living\u201d (Parfit 1984). ... The Repugnant Conclusion is a problem for all moral theories which hold that welfare at least matters when all other things are equal. (Arrhenius, Ryberg, &amp; T\u00e4nnsj\u00f6, <a href=\"https://plato.stanford.edu/archives/spr2014/entries/repugnant-conclusion/\"><u>2014</u></a>.)</p></blockquote><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/wetmi7yg3fjyjs31qce0\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/luwc0utaeuu1gxamchqx 152w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/j8ql2j1hjl3koxkgoenv 232w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/panqdznjswokvivcb8eu 312w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/pm2ghotpnezz0vzxna5a 392w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/raknqb9fsp6yn4pywg1g 472w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/usvrpdvr9fei3qsodxtb 552w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/biesnf52ak8ybb0jqwry 632w\"><figcaption><i>(A diagram of the Repugnant Conclusion; </i><a href=\"https://plato.stanford.edu/archives/spr2014/entries/repugnant-conclusion/\"><i><u>source</u></i></a><i>.)</i></figcaption></figure><p>Minimalist axiologies avoid the Repugnant Conclusion, as they deny that lives \u201cbarely worth living\u201d would constitute a vast heap of independent \u201cplus-points\u201d in the first place. For example, Fehige (<a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>1998</u></a>) would assume that the lives with a \u201cvery high quality of life\u201d would be quite free from subjective problems, which is better, <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>all else equal</u></a>, than a much larger set of lives that still have a lot of their preferences dissatisfied.</p><p>As noted by another commenter on Fehige (1998):</p><blockquote><p>Among its virtues, [antifrustrationism] rescues total utilitarianism from the repugnant conclusion. If utility is measured by the principle of harm avoidance instead of aggregated preference satisfaction, utilitarianism does not, as the accusation often goes, entail that it is better the more (acceptably) happy lives there are[, other things being equal]. (Karlsen, <a href=\"http://www.philosophyoflife.org/jpl201309.pdf\"><u>2013</u></a>, p. 160.)</p></blockquote><p>Fehige\u2019s theory of welfare is seemingly dismissed by Arrhenius, Ryberg, and T\u00e4nnsj\u00f6 (<a href=\"https://plato.stanford.edu/archives/spr2014/entries/repugnant-conclusion/\"><u>2014</u></a>) on the grounds that it would, counterintuitively, deny the possibility of lives worth living:</p><blockquote><p>However, a theory about welfare that denies the possibility of lives worth living is quite counter-intuitive [Ryberg, 1996]. It implies, for example, that a life of one year with complete preference satisfaction has the same welfare as a completely fulfilled life of a hundred years, and has higher welfare than a life of a hundred years with all preferences but one satisfied. Moreover, the last life is not worth living (Arrhenius <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A170236&amp;dswid=7142\"><u>2000b</u></a>).</p></blockquote><p>Yet this objection seems to imply that a life could be worth living <i>only</i> for its own sake, i.e. for some kind of satisfaction that it independently \u201ccontains\u201d, and to deny that a life could also be worth living for its <a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused\"><u>positive roles</u></a>. Again, we need to properly account for the fact that Fehige\u2019s model is comparing lives only in causal isolation. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>section on the \u201call else equal\u201d assumption</u></a>.)</p><p>As soon as we step outside of the thought experiment (of \u201call else equal\u201d) and start comparing these lives in our actual, interpersonal world, we may well think \u2014 even on Fehige\u2019s terms \u2014&nbsp;that a subjectively perfect life of one year would be much <i>less</i> valuable (overall, for all beings) than would be the subjectively <i>almost</i> perfect century. After all, many of our preferences and preferred actions have significant implications for the welfare of others. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#5__What_do_these_views_imply_in_practice_\"><u>section on practical implications</u></a>.)</p><p>However, it is <i>not</i> necessarily counterintuitive to prefer the perfect year \u2014 or even nonexistence \u2014&nbsp;over the less-than-perfect century in complete <i>causal isolation</i>, where we can be no-one\u2019s friend or partner, do no good work for anyone, and generally make no positive difference in any way. Regardless of how we felt during the year, or during the century, others would live as if we never had. In other words, we may question the overall worth of spending our life in the <a href=\"https://en.wikipedia.org/wiki/Experience_machine\"><u>experience machine</u></a>, provided that it does not help solve any problem. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#6__Without_the_concept_of_intrinsic_positive_value__how_can_life_be_worth_living_\"><u>section on lives worth living</u></a>.)</p><h3><strong>The Very Repugnant Conclusion</strong></h3><p>The Repugnant Conclusion was termed repugnant due to the intuition that a legion of lives \u201cbarely worth living\u201d cannot be better than a smaller population of lives with very high welfare. Some say that in this case, the intuition is wrong and that we should simply bite the bullet and follow the utilitarian math (of <a href=\"https://www.utilitarianism.net/types-of-utilitarianism#additive-aggregationism\"><u>additive aggregationism</u></a>). However, presumably fewer people would accept the \u2018Very Repugnant Conclusion\u2019 (VRC), in which the \u201cbetter\u201d world contains a lot of subjectively hellish lives, supposedly \u201c<a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/\"><u>compensated for</u></a>\u201d by a vast number of lives \u201cbarely worth living\u201d:</p><blockquote><p>There seems to be more trouble ahead for total [<a href=\"https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2#Appendix__Moral_Uncertainty_and_Totalism\"><u>symmetric</u></a>] utilitarians. Once they assign some positive value, however small, to the creation of each person who has a weak preference for leading her life rather than no life, then how can they stop short of saying that some large number of such lives can compensate for the creation of lots of dreadful lives, lives in pain and torture that nobody would want to live? (Fehige, <a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>1998</u></a>, pp. 534\u2013535.)</p></blockquote><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5gPubzt79QsmRJZnL/yz9n4r3dzmotcotwoxbj\"><figcaption><i>(A diagram of the VRC; </i><a href=\"https://www.cambridge.org/core/journals/utilitas/article/why-derek-parfit-had-reasons-to-accept-the-repugnant-conclusion/AB0C0590403FE9519FF76EF5A6A2426B\"><i><u>source</u></i></a><i>.)</i></figcaption></figure><p>In more formal terms:</p><blockquote><p>Let W1 be a world filled with very happy people leading meaningful lives [<strong>A</strong>]. Then, according to total [symmetric] utilitarianism, there is a world W2 which is better than W1, where there is a population of suffering people [<strong>N</strong>] much larger than the total population of W1, and everyone else has lives barely worth living [<strong>Z</strong>] - but the population is very huge. (<a href=\"https://www.lesswrong.com/posts/6vYDsoxwGQraeCJs6/the-very-repugnant-conclusion\"><u>Source</u></a>.)</p></blockquote><p>One way to avoid the VRC is to follow Fehige\u2019s suggestion and interpret utility as \u201ca measure of avoided preference frustration\u201d. On this view, utilitarianism \u201casks us to minimize the amount of preference frustration\u201d, which leads us to prefer <i>W1</i> over <i>W2 </i>(Fehige, 1998, pp. 535\u2013536). As noted by Fehige (1998, p. 518), \u201c<i>Maximizers of preference satisfaction</i> should instead call themselves <i>minimizers of preference frustration.\u201d</i></p><p>Every minimalist axiology would prefer <i>W1</i> over <i>W2</i> due to being structurally similar to Fehige\u2019s view \u2014&nbsp;that is, none of them would say that the supposed \u201cplus-points\u201d of <i>W2</i> could somehow <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>independently</u></a> \u201ccounterbalance\u201d the agony of the others, regardless of the number of the lives \u201cbarely worth living\u201d.</p><p>In contrast, the VRC is a problem for a lot of \u201csymmetric\u201d axiologies besides classical hedonism:</p><blockquote><p>Consider an axiology that maintains that any magnitude of suffering can be morally outweighed by a sufficiently great magnitude of preference satisfaction, virtue, novelty, beauty, knowledge, honor, justice, purity, etc., or some combination thereof. It is not apparent that substituting any of these values for happiness in the VRC makes it any more palatable[.] (DiGiovanni, <a href=\"https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2#1_1_1__Counterarguments_and_some_responses\"><u>2021</u></a>.)</p></blockquote><p>At the moment, the VRC is not even mentioned on <a href=\"https://www.utilitarianism.net/population-ethics#objecting-to-the-total-view\"><u>Utilitarianism.net</u></a>, which only states that <i>\u201c[the] most prominent objection to the total view is the repugnant conclusion\u201d</i>, nor in <a href=\"https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\"><i><u>The Precipice</u></i></a>, which similarly only claims that <i>\u201c[the] main critique of the Total [</i><a href=\"https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2#Appendix__Moral_Uncertainty_and_Totalism\"><i><u>symmetric</u></i></a><i>] View is that it leads to something called the repugnant conclusion\u201d </i>(Ord, 2020, Appendix B: Population Ethics and Existential Risk). Yet for anyone who is bothered by the repugnant conclusion, a much stronger reason to reject symmetric total views would be that they support the VRC.</p><h3><strong>Solving problems: A way to make sense of population ethics?</strong></h3><p>In general, one can avoid the VRC (and the two other conclusions above) by maintaining that ethics is about <a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/#Ethics_as_being_about_problems\"><u>solving problems</u></a>. On this view, any choice between two populations (all else equal) will depend on preventing the overall greater amount of subjectively problematic states, such as extreme suffering. Regardless of the precise definition of what constitutes a subjectively problematic state, all minimalist views (as explored here) are also \u201cproblem-focused views\u201d. In other words, they reject the metaphor that ethical problems could be \u201ccounterbalanced\u201d instead of prevented:</p><blockquote><p>[Only] the existence of such problematic states imply genuine victims, while failures to create supposed positive goods (whose absence leaves nobody troubled) do not imply any real victims \u2014 such \u201cfailures\u201d are mere victimless \u201ccrimes\u201d. ... According to this view, we cannot meaningfully \u201ccancel out\u201d or \u201cundo\u201d a problematic state found somewhere by creating some other state elsewhere. (Vinding, <a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/\"><u>2020a</u></a>.)</p></blockquote><p>Generally, the metaphor of \u201cethical counterbalancing\u201d may rest on a terminological confusion. As argued in Vinding (<a href=\"https://magnusvinding.files.wordpress.com/2020/05/suffering-focused-ethics.pdf\"><u>2020b</u></a>, pp. 155\u2013156) \u2014&nbsp;based on Knutsson (<a href=\"https://www.tandfonline.com/doi/full/10.1080/0020174X.2019.1658631\"><u>2021</u></a>, section 3) \u2014 when we speak of a <i>\u201cnegative\u201d </i>experience, we may automatically assume that it could be counterbalanced by its symmetrically <i>\u201cpositive\u201d </i>counterpart. Yet we would not say that a <i>problematic</i> experience could be counterbalanced by its \u201ccorresponding\u201d <i>unproblematic</i> counterpart:</p><blockquote><p>Consider, by analogy, the states of being below and above water respectively. One can certainly say that being below water is the opposite of being above water. In particular, one can say that, in one sense, being 50 meters below water is the opposite of being 50 meters above water. But this does not mean, quite obviously, that a symmetry exists between these respective states in terms of their value and moral significance. Indeed, there is a sense in which it matters much more to have one\u2019s head just above the water surface than it does to get it higher up still. (Vinding, <a href=\"https://magnusvinding.files.wordpress.com/2020/05/suffering-focused-ethics.pdf\"><u>2020b</u></a>, p. 156.)</p></blockquote><p>Similarly, <a href=\"https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2%231_1__Non_person_affecting_objections_to_total_symmetric_axiology_11_#1_1__Non_person_affecting_objections_to_total_symmetric_axiology_11_\"><u>intuitions</u></a> that reject the VRC may be framed in terms of subjectively <i>unproblematic </i>versus subjectively <i>problematic </i>experiences. For example, we could reject the VRC based on the following principle:</p><blockquote><p><a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>All else equal</u></a>, any world that contains <i>only unproblematic experiences </i>is no worse than a second world that contains <i>problematic experiences</i>, regardless of what else the second world contains.</p></blockquote><p>A sufficient criterion for \u201cproblematic experiences\u201d (in the VRC) could be to consider whether the worlds contain <a href=\"https://centerforreducingsuffering.org/research/clarifying-lexical-thresholds/#Lexical_views_based_on_consent\"><u>unconsentable</u></a> suffering. Presumably, all beings in the happy world of <i>W1 </i>would consent to living their lives (qualifying their lives as subjectively <i>unproblematic</i> on this criterion), whereas many of the beings in <i>W2</i> would <i>not</i> consent to living their lives (qualifying their lives as subjectively <i>problematic</i>)<i>. </i>Thus, the previous principle \u2014 coupled with this criterion \u2014 would reject the VRC on the grounds of consent.</p><p>On the water analogy, such a \u201cproblem-focus\u201d would imply that we prioritize helping sentient beings avoid the depths of extreme suffering, but not that we attempt to \u201c<a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/\"><u>outweigh</u></a>\u201d the depths of some with the heights of others, <a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused#4_1_Wellbeing_as_a_resource\"><u>unless</u></a> this actually seems to be helpful for counteracting the overall amount of <a href=\"https://crucialconsiderations.org/rationality/expected-utility/\"><u>expected</u></a> <a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/#Ethics_as_being_about_problems\"><u>problems</u></a> in the world. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#5__What_do_these_views_imply_in_practice_\"><u>section on practical implications</u></a>, and in a future post on multi-level minimalism.)</p><h2><strong>4. What are we comparing when we make the assumption of \u201call else being equal\u201d?</strong></h2><h3><strong>Isolated value-containers</strong></h3><p>The <a href=\"https://en.wikipedia.org/wiki/Ceteris_paribus\"><i><u>ceteris paribus</u></i></a> assumption is often translated into English as something like \u201call else equal\u201d, \u201call else unchanged\u201d, or \u201cother things held constant\u201d. Generally, this<i> </i>assumption means that we <i>exclude any changes other than those explicitly mentioned</i>. And so when we make this assumption in population theory, the idea is to compare any two hypothetical populations <i>only</i> with respect to their explicit differences \u2014&nbsp;such as the level and distribution of welfare among these populations \u2014 and to rule out the influence of any other factors.</p><p>Yet we should be careful to appreciate the full implications of comparing populations in this way. After all, we may all too automatically simply agree to this as <i>standard practice</i>, and forget to give a second thought to what we are thereby implicitly agreeing to give up. Quite often, we only see it in the form of a parenthetical remark (<i>ceteris paribus</i>), and sometimes we are simply silently expected to play by its rules where it is the unvoiced and unquestioned background assumption. But in the high-stakes game of population theory, this seemingly innocuous assumption may decisively influence our view on what kinds of lives, if any, are worth living, and what they are worth living for. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#6__Without_the_concept_of_intrinsic_positive_value__how_can_life_be_worth_living_\"><u>section on lives worth living</u></a>.)</p><p>To illustrate, let us consider a scenario where the <i>ceteris paribus</i> assumption would <i>actually be true:</i> namely, we are comparing only \u201cisolated value-containers\u201d or \u201cisolated <a href=\"https://en.wikipedia.org/wiki/Experience_machine\"><u>Matrix</u></a>-lives\u201d that <i>never interact with each other</i> (not even by acausal \u201cinfluence\u201d). If this sounds radical, then we may not always realize how radical the <i>ceteris paribus </i>assumption in fact is. After all, it represents only an \u201cisolated view\u201d of lives worth living, as it focuses only on their own \u201ccontents\u201d (in terms of subjective welfare), and completely excludes their overall effects on the welfare of others.</p><h3><strong>Counterintuitive boundaries</strong></h3><p>Now, our practical intuitions about the <i>overall value</i> of lives \u2014&nbsp;such as of all the lives \u201cbarely worth living\u201d in the (<a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#The_Very_Repugnant_Conclusion\"><u>Very</u></a>) <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#The_Repugnant_Conclusion\"><u>Repugnant Conclusion</u></a> \u2014&nbsp;may implicitly be tracking not only the \u201ccontents\u201d of these lives (i.e. their own level of welfare), but also their overall effects on the welfare of others<i>.</i> And in practice, it may indeed seem like a repugnantly bad idea to trade away a high-welfare population for a legion of lives \u201cbarely worth living\u201d, as the latter would seem to not have enough <a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused#4_1_Wellbeing_as_a_resource\"><u>well-being as a resource</u></a> to adequately take care of each other in the long term. (More in the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#5__What_do_these_views_imply_in_practice_\"><u>section on practical implications</u></a>.) A practical intuition in the opposite direction is also possible, namely that a larger population could create more goods, insights, and resources that everyone could benefit from, and thus have a brighter future in the long run.</p><p>Yet to give <i>any</i> weight to such instrumental effects, even implicitly, would already violate the <i>ceteris paribus </i>assumption, which was meant to rule out all interactions from affecting the comparison. And so we should be very careful to properly respect the boundaries of this<i> </i>assumption, such as by explicitly imagining that we are comparing only \u201cisolated Matrix-lives\u201d. After all, our intuitions are arguably adapted for an <i>interpersonal</i> world with a <i>time</i> dimension: two features of life that are difficult for us to put aside when entering thought experiments about the overall value of individual lives. To the extent that our practical intuition may, by default, be evaluating the hypothetical lives roughly like it would in the real world, we need to make an extra effort to really constrain it from introducing any other factors whose influence was supposed to be ruled out.</p><p>Perhaps a lot of axiological disagreement could be resolved by simply being more clear about the mismatch between our <i>practical intuitions</i> and <i>abstract population theory</i>. In any case, we need to recognize how the (properly respected) <i>ceteris paribus </i>assumption \u2014 i.e. the isolated view \u2014 is radically exclusive of many of the things that our practical intuitions are implicitly tracking.</p><h2><strong>5. What do these views imply in practice?</strong></h2><h3><strong>Naive versus sophisticated minimalism</strong></h3><p>The <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#3__How_do_these_views_help_us_make_sense_of_population_ethics_\"><u>section on population ethics</u></a> showed how minimalist axiologies avoid what (for other views) are often called tricky problems in population <i>theory</i>. Yet one may still worry that these views would have counterintuitive implications in <i>practice.</i> However, we should be aware that many of<i> </i>these supposedly counterintuitive or radical implications could result <i>not only</i> from an <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#4__What_are_we_comparing_when_we_make_the_assumption_of__all_else_being_equal__\"><u>isolated view</u></a> \u2014 which excludes the <a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused\"><u>positive roles</u></a> of individual lives \u2014 but also from a <a href=\"https://forum.effectivealtruism.org/tag/naive-vs-sophisticated-consequentialism\"><u>naive consequentialism</u></a>, which ignores the positive roles of various <a href=\"https://forum.effectivealtruism.org/s/MBadsrYLmzLNmYjaj/p/t3St6Fz4DmHtKfgqm#3__Life_and_diversity\"><i><u>norms</u></i></a> of everyday morality, such as those of autonomy, cooperation, and non-violence. (More in a future post on multi-level minimalism.)</p><p>Moreover, a naive consequentialism is often not based on a nuanced understanding of <a href=\"https://crucialconsiderations.org/rationality/expected-utility/\"><u>expected value thinking</u></a>, and instead falls victim to a kind of \u201cnarrative misconception\u201d of consequentialism, in which a view would support any means necessary to bring about its axiologically ideal \u201cend state\u201d. One could argue that the idea of a <a href=\"https://www.hedweb.com/social-media/pre2014.html\"><u>utilitronium shockwave</u></a> amounts to such a misconception relative to the practical implications of classical utilitarianism. In the case of minimalist axiologies, this misconception looks like the claim that we must, at any cost, \u201cseek a future where problems are eventually reduced to zero\u201d, which is very different from minimizing problems over <i>all </i>time in expectation. After all, only the latter kind of thinking (i.e. expected value thinking) is properly sensitive to risks of making things worse, whereas the first, misconceived view is closer to a cost-insensitive \u201call in\u201d gamble for manifesting an ideal outcome at some particular point in time.</p><p>For example, <a href=\"https://www.youtube.com/watch?v=-2oRgxxafXk\"><u>naive</u></a> minimalism might disregard the norms of everyday morality as soon as there would (apparently) be even the slightest chance of bringing about its hypothetically ideal \u201cend state\u201d, even if this would violate the preferences of others. By contrast, <a href=\"https://forum.effectivealtruism.org/tag/naive-vs-sophisticated-consequentialism\"><u>sophisticated</u></a> minimalism would be concerned with the \u201ctotal outcome\u201d \u2014 which spans all of time \u2014&nbsp;and be highly sensitive to the risk of making things worse overall. For instance, any violent strategy for \u201cpreventing problems\u201d would very likely backfire in various ways, such as by undermining one\u2019s credibility as a potential ally for large-scale <a href=\"https://centerforreducingsuffering.org/research/why-altruists-should-be-cooperative/\"><u>cooperation</u></a>, ruining the reputation of one\u2019s (supposedly altruistic) cause, and eroding the (<a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused#3_1_Autonomy\"><u>positive</u></a>) norm of respecting individual autonomy. Because the backfire risks depend on complex interactions that happen over considerable spans of time, they are likely to be underemphasized by simple thought experiments that collapse hypothetical populations into two-dimensional images. (More on the \u201cnarrative misconception\u201d of consequentialism in the next post.)</p><h3><strong>Compatibility with everyday intuitions</strong></h3><p>What, then, do these views imply in practice, assuming a sophisticated minimalism over <i>all</i> time? The second half of <a href=\"https://magnusvinding.files.wordpress.com/2020/05/suffering-focused-ethics.pdf\"><i><u>Suffering-Focused Ethics</u></i></a><i> </i>(Vinding, 2020b, pp. 141\u2013277) is an accessible and extensive treatment of basically the same question, particularly for views that prioritize minimizing extreme suffering. In large part, the practical implications for minimalist views are probably the same as those for suffering-focused views. Yet minimalist views differ from at least some suffering-focused views in one respect, which is that minimalist axiologies explicitly deny the concept of \u201cindependently aggregable positive value\u201d, i.e. anything that could be \u201cstacked\u201d in causal isolation to make the world a better place in the axiological sense.</p><p>For that reason, minimalist views may <i>feel</i> like being somehow opposed to the things that we cherish as being intrinsically valuable. Yet minimalist views need not imply anything radical about the \u201c<a href=\"https://forum.effectivealtruism.org/s/MBadsrYLmzLNmYjaj/p/t3St6Fz4DmHtKfgqm#2__Possible_misconceptions_about_instrumental_value\"><u>quantity</u></a>\u201d of positive value that we intuitively attribute to many things at the level of our everyday psychology. After all, the kinds of things that we may deem \u201cintrinsically valuable\u201d at an intuitive level are often precisely the kinds of things that rarely need any <i>extrinsic justification</i> in everyday life, such as sound physical and mental health, close relationships, and intellectual curiosity. If required, we <i>could </i>often \u201cunpack\u201d the value of these things in terms of their long-term and indirect effects, namely their usefulness for preventing more problems than they cause. But when our (intuitively) positive pursuits have many beneficial effects across a variety of contexts, we are often practically <i>wise</i> to avoid spending the unnecessary effort to separately \u201cunpack\u201d their value in relational terms.</p><p>By only focusing on our positive feelings in the immediate moment, we may actually even <i>underestimate</i> the overall <i>usefulness</i> of things like maintaining a rich social life, learning new skills, and coming up with new insights. After all, if the overall goal is to minimize <a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/#Ethics_as_being_about_problems\"><u>problems</u></a>, then we are faced with the dauntingly complex meta-problem of identifying interventions that can reasonably be expected to prevent more problems than they cause \u2014 and by any measure, this meta-problem will require us to combine a vast amount of knowledge and <i>supportive</i> values. Minimalist views do not imply that we hyper-specialize in this meta-problem in a way that would dismiss all apparent \u201cintrinsic values\u201d as superfluous, but rather that we adhere to a <a href=\"https://forum.effectivealtruism.org/s/MBadsrYLmzLNmYjaj/p/t3St6Fz4DmHtKfgqm#4__Valuable_experiences\"><u>diverse range</u></a> of these values so as to advance a mature and comprehensive approach to alleviating problems.</p><h3><strong>Preventing instead of outweighing hell</strong></h3><p>Of course, it would be a <a href=\"https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence\"><u>suspicious convergence</u></a> if <i>everything</i> that people may think of as being intrinsically valuable would also be relationally aligned with the minimization of overall problems. Yet the <i>everyday</i> implications of minimalist views are not necessarily very different from those of other consequentialist views, because all of them share the personal ideal of living an effective and strategic life aligned with some overall optimization goal, which implies some common constraints and recommendations for everyday conduct. Where the views differ (the most) may be in their <a href=\"https://forum.effectivealtruism.org/tag/longtermism\"><i><u>long-term</u></i></a> implications. Rather than primarily ensuring that we spread out into space, minimalist views would arguably imply that we prioritize avoiding worst-case scenarios (cf. Gloor, <a href=\"https://forum.effectivealtruism.org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems\"><u>2018</u></a>; DiGiovanni, <a href=\"https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2\"><u>2021</u></a>), because prioritizing large-scale space colonization may well increase the amount of subjective problems over the long term (<a href=\"https://crucialconsiderations.org/rationality/expected-utility/\"><u>in expectation</u></a>).</p><p>On non-minimalist utilitarian grounds, Bostrom (<a href=\"https://www.nickbostrom.com/astronomical/waste.html\"><u>2003</u></a>) argues that the main priority of human civilization should be to enable astronomical amounts of independently positive lives to exist in the far future. Yet minimalist axiologies imply that the non-existence of those lives is not a problem for their own sake; after all, in terms of subjective problems, non-existent beings do not need to be saved from non-existence. In particular, non-existent lives do not subjectively need to exist in the way that some other beings need to avoid subjective hell. (A different question is whether the existence of future generations may help to overall prevent subjectively problematic experiences across all beings. More in a future post.)</p><p>Tradeoffs like the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#The_Very_Repugnant_Conclusion\"><u>Very Repugnant Conclusion</u></a> (VRC) are <a href=\"https://reducing-suffering.org/omelas-and-space-colonization/\"><u>not</u></a> only theoretical, because arguments like that of Bostrom (2003) imply that the stakes may be astronomically high in practice. When non-minimalist axiologies find the VRC a worthwhile tradeoff, they would presumably also have similar implications on an arbitrarily large scale. Therefore, we need to have an inclusive discussion about the extent to which the subjective problems (e.g. extreme suffering) of some can be \u201c<a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/\"><u>counterbalanced</u></a>\u201d by the \u201cgreater (intrinsic) good\u201d for others, because this has direct implications for what kind of large-scale space colonization could be called \u201cnet positive\u201d.</p><h3><strong>Self-contained versus relational flourishing</strong></h3><p>When psychologists speak of <a href=\"https://en.wikipedia.org/wiki/Flourishing\"><u>flourishing</u></a>, it can have many different meanings. As a value-laden concept, it is often bundled together with things like optimal growth and functioning, social contribution, and having a purpose in life. Yet when we load the concept of flourishing with axiological value, as is done by the authors of <a href=\"https://www.utilitarianism.net/population-ethics\"><u>Utilitarianism.net</u></a> and <a href=\"https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\"><i><u>The Precipice</u></i></a> (Ord, 2020), we should ask whether this value is <i>intrinsic</i> or <i>relational </i>in the axiological sense.</p><p>On minimalist views, flourishing would not be a \u201cself-contained\u201d state of isolated value. Yet minimalist views <i>do</i> support a notion of flourishing as personal alignment with something beyond ourselves. Minimalist flourishing would mean that we are skillfully serving the unmet needs of all sentient beings, aligning <a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused#4_1_Wellbeing_as_a_resource\"><u>our well-being</u></a> with the overall prevention of ill-being.</p><p>In practice, a minimalist sense of \u201coptimal growth and functioning\u201d would probably look more like strategic self-investment and healthful living rather than self-sacrifice (similar to other impartial consequentialist views). After all, we first need to patiently grow our strengths, skills, and relationships before we can sustainably and effectively apply ourselves to help others. And because life is long, it makes sense to keep growing these capacities, meeting our needs in harmony with the needs of others, and actively seek the best ways in which we can play positive roles for all sentient beings.</p><h2><strong>6. Without the concept of intrinsic positive value, how can life be worth living?</strong></h2><h3><strong>A more complete view</strong></h3><p>What we see in standard population theory are only the isolated \u201cwelfare bars\u201d of what each individual life independently \u201ccontains\u201d. In practice, we also have hidden \u201crelational roles bars\u201d.</p><p>On <i>any</i> impartial and welfarist view, our own \u201caggregate welfare\u201d is often a much <i>smaller</i> part of our life\u2019s overall value compared to our effects on the welfare of others. And if we think of our own ideal life, or perhaps the life of our favorite historical or public figure, we are often practically right to focus on the <i>roles</i> of this life for others, and <i>not</i> only, or even mostly, on how it feels from the inside. After all, the value of the roles is (ultimately) measured in the <a href=\"https://forum.effectivealtruism.org/s/MBadsrYLmzLNmYjaj/p/t3St6Fz4DmHtKfgqm#2__Possible_misconceptions_about_instrumental_value\"><u>same</u></a> unit of value as the welfare, and in that sense the roles can be much bigger than what any single life independently \u201ccontains\u201d.</p><p>In other words, we may, in the bigger picture, \u201ccompare effort with effort\u201d, and find that a sufficient reason to <i>spend</i> effort is to <i>save</i> effort, reduce inner conflict, and lighten the load for all sentient beings in the long term.</p><p>Indeed, if we assume that basically all of our daily struggles are much easier to bear compared to instances of the most intense pains (G\u00f3mez-Emilsson, <a href=\"https://forum.effectivealtruism.org/posts/gtGe8WkeFvqucYLAF/logarithmic-scales-of-pleasure-and-pain-rating-ranking-and\"><u>2019</u></a>), then we may already find some lightness and relief in being relatively problem-free at the personal level. And we may further realize that we can play very worthwhile roles by focusing our spare efforts on helping to relieve such extreme burdens on the whole. Conversely, if we assume that our burdens are worthwhile for some \u201cpositive essence\u201d, then we again face interpersonal tradeoffs like the <a href=\"https://forum.effectivealtruism.org/posts/5gPubzt79QsmRJZnL/minimalist-axiologies-and-positive-lives#The_Very_Repugnant_Conclusion\"><u>VRC</u></a>, as well as the question of whether we would allow arbitrarily large harms for the \u201cgreater good\u201d of creating astronomical amounts of this essence.</p><p>Finally, we might question the practical relevance of thinking that a life could be worth living only for some kind of \u201cself-contained\u201d satisfaction. After all, our practical intuitions and dilemmas are always related to tradeoffs in the interpersonal world. Without the concept of intrinsic positive value, a life can be worth living for its positive roles.</p><h2><strong>What additional questions do you have about these views?</strong></h2><p>The next posts will address more specific questions, such as whether minimalist views would imply that we should seek to \u201cminimize populations\u201d (meanwhile: see Vinding, <a href=\"https://magnusvinding.files.wordpress.com/2020/05/suffering-focused-ethics.pdf\"><u>2020b</u></a>, pp. 141\u2013148), and whether a <a href=\"https://forum.effectivealtruism.org/posts/mLqKcYahP5nbjK9ir/tristan-cook-s-shortform?commentId=aBvMzniWEDmTwEdYZ\"><u>vast</u></a> amount of <a href=\"https://www.facebook.com/groups/effective.altruists/posts/507565719299790/?comment_id=507661125956916\"><u>small</u></a> pains could imply \u201ca preference for hell over heaven\u201d (meanwhile: see Vinding, <a href=\"https://centerforreducingsuffering.org/comparing-repugnant-conclusions/\">2021</a>).</p><p>A future post will also address many of the published objections to antifrustrationism (Fehige, <a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>1998</u></a>), and take a closer look at tranquilism (Gloor, <a href=\"https://longtermrisk.org/tranquilism/\"><u>2017</u></a>).</p><p>What additional questions or feedback arise in relation to minimalist axiologies? Please let me know in the comments or via this <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdZyj5r49Hbcag5TSooxbXyRmkZM7VFkEOof7pD398r73YSMg/viewform\">anonymous form</a>.</p><h2><strong>Acknowledgments</strong></h2><p>Special thanks to Magnus Vinding for help with editing, and to <a href=\"https://forum.effectivealtruism.org/posts/ZeXqBEvABvrdyvMzf/editing-available-for-ea-forum-drafts\"><u>Aaron Gertler</u></a> for commenting on an early draft.</p><p>I am also grateful for helpful comments by Tobias Baumann, Simon Knutsson, and Winston Oswald-Drummond.</p><p>Commenting does not imply endorsement of any of my claims.</p><h2><strong>References</strong></h2><p>Ajantaival, T. (2021). Positive roles of life and experience in suffering-focused ethics. <a href=\"https://centerforreducingsuffering.org/positive-roles-of-life-and-experience-in-suffering-focused-ethics/\"><u>Ungated</u></a>; <a href=\"https://forum.effectivealtruism.org/posts/t3St6Fz4DmHtKfgqm/positive-roles-of-life-and-experience-in-suffering-focused\"><u>EA Forum</u></a>.</p><p>Arrhenius, G. (2000a). An impossibility theorem for welfarist axiologies. <i>Economics &amp; Philosophy</i>, <i>16</i>(2), 247\u2013266. <a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.227.8341&amp;rep=rep1&amp;type=pdf\"><u>Ungated</u></a>.</p><p>Arrhenius, G. (2000b). <i>Future Generations: A Challenge for Moral Theory</i> (Doctoral dissertation, Acta Universitatis Upsaliensis). <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A170236&amp;dswid=7142\"><u>Ungated</u></a>.</p><p>Arrhenius, G., Ryberg, J., &amp; T\u00e4nnsj\u00f6, T. (2014). The Repugnant Conclusion. In Zalta E. N. (ed.), <i>The Stanford Encyclopedia of Philosophy </i>(Spring 2014 Edition). <a href=\"http://plato.stanford.edu/archives/spr2014/entries/repugnant-conclusion/\"><u>Ungated</u></a>.</p><p>Bostrom, N. (2003). Astronomical waste: The opportunity cost of delayed technological development. <i>Utilitas</i>, <i>15</i>(3), 308\u2013314. <a href=\"https://www.nickbostrom.com/astronomical/waste.html\"><u>Ungated</u></a>.</p><p>DiGiovanni, A. (2021). A longtermist critique of \u201cThe expected value of extinction risk reduction is positive\u201d. <a href=\"https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2\"><u>EA Forum</u></a>.</p><p>Fehige, C. (1998). A Pareto Principle for Possible People. In Fehige, C. &amp; Wessels, U. (eds.) <i>Preferences</i> (pp. 508\u2013543). Berlin: Walter de Gruyter. <a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>Ungated</u></a>.</p><p>Gloor, L. (2017). Tranquilism. <a href=\"https://longtermrisk.org/tranquilism/\"><u>Ungated</u></a>.</p><p>Gloor, L. (2018). Cause prioritization for downside-focused value systems.<i> </i><a href=\"https://longtermrisk.org/cause-prioritization-downside-focused-value-systems/\"><u>Ungated</u></a>; <a href=\"https://forum.effectivealtruism.org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems\"><u>EA Forum</u></a>.</p><p>G\u00f3mez-Emilsson, A. (2019). Logarithmic Scales of Pleasure and Pain. <a href=\"https://www.qualiaresearchinstitute.org/blog/log-scales\"><u>Ungated</u></a>; <a href=\"https://forum.effectivealtruism.org/posts/gtGe8WkeFvqucYLAF/logarithmic-scales-of-pleasure-and-pain-rating-ranking-and\"><u>EA Forum</u></a>.</p><p>Karlsen, D. S. (2013). Is God Our Benefactor? An Argument from Suffering. <i>Journal of Philosophy of Life, 3</i>, 145\u2013167. <a href=\"http://www.philosophyoflife.org/jpl201309.pdf\"><u>Ungated</u></a>.</p><p>Knutsson, S. (2021). The world destruction argument. <i>Inquiry</i>, <i>64</i>(10), 1004\u20131023. <a href=\"https://www.tandfonline.com/doi/full/10.1080/0020174X.2019.1658631\">Ungated</a>; <a href=\"https://www.tandfonline.com/doi/epub/10.1080/0020174X.2019.1658631?needAccess=true\">EPUB</a>.</p><p>MacAskill, W., Chappell, R. Y., &amp; Meissner, D. (2021). Population Ethics: The Total View. <a href=\"https://www.utilitarianism.net/population-ethics#the-total-view\"><u>Ungated</u></a>.</p><p>Ord, T. (2020). <i>The Precipice: Existential Risk and the Future of Humanity</i>. Hachette Books.</p><p>Ryberg, J. (1996). Is the repugnant conclusion repugnant?. <i>Philosophical Papers</i>, <i>25</i>(3), 161\u2013177.</p><p>Vinding, M. (2020a). On purported positive goods \u201coutweighing\u201d suffering. <a href=\"https://centerforreducingsuffering.org/research/on-purported-positive-goods-outweighing-suffering/\"><u>Ungated</u></a>.</p><p>Vinding, M. (2020b). <i>Suffering-Focused Ethics: Defense and Implications</i>. Ratio Ethica. <a href=\"https://magnusvinding.files.wordpress.com/2020/05/suffering-focused-ethics.pdf\"><u>Ungated</u></a>.</p><p>Vinding, M. (2021). Comparing repugnant conclusions: Response to the \u201cnear-perfect paradise vs. small hell\u201d objection. <a href=\"https://centerforreducingsuffering.org/comparing-repugnant-conclusions/\">Ungated</a>.</p>", "user": {"username": "Teo Ajantaival"}}, {"_id": "QPfdxLs4YqD3ASDZ9", "title": "What would you ask on MTurk? (I could possibly run a study for you)", "postedAt": "2021-11-13T00:57:15.870Z", "htmlBody": "<p><a href=\"https://www.mturk.com/\">Amazon Mechanical Turk</a> (\"MTurk\") is a platform that researchers can use to recruit participants for studies. I did a fair number of MTurk studies during my PhD, and know a bunch of details about how to use it that a newcomer wouldn't. I currently have some time to run interesting EA-driven surveys on MTurk! So my question is:&nbsp;</p><p><strong>What questions would you ask on MTurk?&nbsp;</strong></p><p>Notes:</p><ul><li>This is a survey of the (usually U.S.) general population. You can access more specific subsets of the population, but that isn't the primary use case.</li><li>Surveys should be under 10 mins</li><li>Pay is ~$7-11 / hour / person + 20% platform fees</li><li>If you want this to be publishable in a conference / journal via an academic affiliation: You'll need (funding*1.08) to pass it through a university as \"gift funding\". You'll also need IRB (Institutional Review Board) approval, which will take ~2 months for a new protocol if all goes well. You'll also need a set of questions that can be framed as useful research, such that the work can be accepted at a conference / journal (though your academic affiliate should be able to help you with that). This is way more work for your academic affiliate, for the price of peer-reviewed mainstream approval!</li></ul><p>I think it could be useful to have a community-constructed list of questions, such that anyone who happens to have time / funding / expertise could run any studies they think are interesting. If you'd like me specifically to run something (I'm mostly interested in questions pertaining to existential risk), especially if you have funding, please comment here or email me at <a href=\"mailto: vlgates@stanford.edu\">vlgates@stanford.edu</a>. Excited to hear from you!</p>", "user": {"username": "Vael Gates"}}, {"_id": "LKdtHdETxSYAXwoW6", "title": "Public reports are now optional for EA Funds grantees", "postedAt": "2021-11-13T00:37:23.281Z", "htmlBody": "<p>Public reports are now explicitly optional for applicants to EA Funds. We have updated our application form and other outwardly-facing materials to reflect this change.</p><ul><li><strong>If you are an individual applicant or a new organization</strong><i>,</i> choosing not to have a public report will very rarely affect the chance that we fund you (and we will reach out to anyone for whom it would make a substantial difference).</li><li><strong>If you are an established organization</strong><i><strong>,</strong></i> choosing not to have a public report may slightly decrease the chance that we fund you. We are generally happy to omit mentions of individuals from public grant reports of organizations at their request.</li><li>If we are uncomfortable making a grant privately with EA Funds money, we may ask to forward your application to private donors we are connected to, or to other large funders in the space.</li></ul><p>Broadly, we think there are many valid reasons not to want a public report, and we don't want anyone to be discouraged from applying to us for funding.<strong> If you or someone you know could use funding productively but was previously discouraged by our payout reports, please apply or encourage them to apply.</strong></p><p><a href=\"https://funds.effectivealtruism.org/apply-for-funding\"><strong><u>Apply here.</u></strong></a></p>", "user": {"username": "abergal"}}, {"_id": "kGbHxYhfqttQZx2QD", "title": "Should Earners-to-Give Work at Startups Instead of Big Companies?", "postedAt": "2021-11-12T22:55:20.332Z", "htmlBody": "<h2>Summary</h2>\n<p><em><a href=\"https://mdickens.me/confidence_tags\">Confidence</a>: Somewhat likely</em></p>\n<p>Effective altruist earners-to-give might be able to donate more money if, instead of working at big companies for high salaries, they work at startups and get paid in equity. Startups are riskier than big companies, but EAs care less about risk than most people.</p>\n<p>Working at a startup is easier than starting one. It doesn't pay as well, but based on my research, it looks like EA startup employees can earn more than big company employees in expectation.</p>\n<p>Does the optimal EA investment portfolio include a significant allocation to startups? To answer that question, I estimated the expected return and risk of startups by adding up the <a href=\"#What_factors_determine_the_expected_return_of_startups_\">following considerations</a>:</p>\n<ol>\n<li>Find a baseline of startup performance by looking at historical data on VC firm returns.</li>\n<li>VC performance is somewhat persistent. EAs can beat the average by working at startups that the top VC firms invest in.</li>\n<li>Startup employees get worse equity terms than VCs, but they also don't have to pay management fees, and they get <a href=\"https://www.benkuhn.net/optopt/\">meta-options</a>. Overall, employees come out looking better than VCs.</li>\n<li>Current market conditions suggest that future performance will be worse than past performance.</li>\n<li>Startups are much riskier than publicly-traded stocks, and the startup market is moderately correlated with stocks (r=0.7).</li>\n</ol>\n<p>All things considered, my best guess is that more earners-to-give should consider working at startups.</p>\n<h1>Framing the problem</h1>\n<p>Suppose you're an effective altruist and you want to donate as much money as possible. Perhaps you've heard the arguments that <a href=\"https://80000hours.org/2012/01/salary-or-startup-how-do-gooders-can-gain-more-from-risky-careers/\">EAs should start startups</a>. Starting a startup is a lot of work and requires special skills, so you'd rather not. Maybe you'd like to invest in venture capital, but the really good VC firms won't accept your money. However, you wouldn't mind <em>working</em> at a startup. You could also work at a big company that pays a high salary. Which should you choose?</p>\n<p>This is how I would think about the problem:</p>\n<p>Say a startup offers you an equity package that's worth $X per year at the current valuation. At the same time, a big company offers you a salary that's $X higher than your salary would be at the startup. Both compensation packages have the same face value.</p>\n<p>If you work at the startup, you get <code>$X</code> per year of equity. Some number of years later, the startup might go public or get acquired, at which point you can sell your equity for <code>$Y</code>. Over that time, your equity earned a return of <code>Y/X</code>.</p>\n<p>If you work at the big company, you could invest your extra <code>$X</code> salary in the stock market. Will that investment earn a higher or lower return than the startup equity? Whichever you expect to earn a higher return is the one you should pick. (Well, that's not really true. Read the next section to find out why not.)</p>\n<h1>The right question</h1>\n<p>Which do we expect to earn a higher return, startups or the public stock market?</p>\n<p>We don't have good data on the historical performance of startup employees. We do have data on VC returns, so we can use that.</p>\n<p>\"Have VCs historically outperformed the public market?\" is the wrong question, because some VCs consistently outperform the average.</p>\n<p>\"Have top VCs historically outperformed the public market?\" is the wrong question, because future expected performance probably isn't the same as past performance.</p>\n<p>\"Can we expect top VCs to outperform the public market?\" is the wrong question, because startup employees don't earn the same returns as VCs. Employee equity has a worse liquidation preference than VC equity, but investors in VC firms have to pay fund fees, which employees don't. And employees have the <a href=\"https://www.benkuhn.net/optopt/\">meta-option</a> to keep vesting when their company does well, or to quit when it does poorly. We can use these considerations to estimate the value of employee equity compared to VC equity.</p>\n<p>\"Can we expect startup employees to outperform the public market?\" is the wrong question, because we need to consider <a href=\"https://mdickens.me/2020/01/06/how_much_leverage_should_altruists_use/\">leverage</a>. Investors in public markets can use leverage to increase their risk and expected return, but startup employees can't.</p>\n<p>\"Can we expect startup employees to outperform the leveraged public market?\" is the wrong question, because an effective altruist's goal isn't to maximize their own portfolio return, it's to maximize the expected utility of the overall EA portfolio. If no other EAs work at the startup where you choose to work, then you're adding better diversification than if you invest in the public market.</p>\n<p>\"Do startup employees contribute more expected utility to the EA portfolio than if they invested in the leveraged public market?\" is the wrong question, because if they did work at a big company and invest their salary, they might be able to invest in something better than the broad market. For example, I have <a href=\"https://mdickens.me/2020/12/14/asset_allocation_for_altruists_with_constraints/\">previously discussed</a> investing in concentrated value/momentum/trend portfolios, and I made a rough attempt to calculate the expected utility of doing so. For us to prefer to become startup employees, startups would have to look better than the best possible public investment (whether that's value/momentum/trend or something else).</p>\n<p>\"Do startup employees contribute more expected utility to the EA portfolio than if they invested in the optimal set of public investments?\" is more or less the right question.</p>\n<h1>Modeling the solution</h1>\n<p>Do startup employees contribute more expected utility to the EA portfolio than if they invested in the optimal set of public investments?</p>\n<p>The answer to this depends on two things:</p>\n<ol>\n<li>How do we model the answer?</li>\n<li>What values should we use for the model inputs?</li>\n</ol>\n<p>#2 is hard. #1 is sort of hard, but luckily, it's already a solved problem. I described an applicable model in <a href=\"https://mdickens.me/2020/12/14/asset_allocation_for_altruists_with_constraints/\">Asset Allocation and Leverage for Altruists with Constraints</a>. In short, we set up a mean-variance optimization problem where we assume 99% of the capital is controlled by other people, and we can decide how to allocate the remaining 1%. Suppose we can allocate between three investment choices:</p>\n<ol>\n<li>A typical investment portfolio, such as a stock market index fund</li>\n<li>The optimal (ex ante) public investment portfolio</li>\n<li>Startups</li>\n</ol>\n<p>The simplest method is to assume we should put all our money into just one choice. What is the overall expected utility if we put our money into choice 1, choice 2, or choice 3?</p>\n<p>If we tell our model the expected returns, standard deviations, correlations between these three portfolios, and a utility function, then the model will spit out the expected utility of each choice.</p>\n<p>Let's say the average EA investment portfolio equals the global equity market, which is at least sort of correct. What's the expected return and standard deviation of global equities?</p>\n<p>We have no idea how equities will perform in the short run. But in the long run, the market's return is <a href=\"https://awealthofcommonsense.com/2016/09/the-john-bogle-expected-return-formula/\">somewhat predictable</a>. And over long time horizons, volatility stays pretty consistent, so we can simply assume the future standard deviation of global equities equals the historical standard deviation.</p>\n<p>Similarly, we can approximate the future standard deviation of startups, and their correlation with global equities, by taking the historical standard deviation and correlation and assuming they will stay the same.</p>\n<p>The most difficult input variable is the expected return of startups.</p>\n<h1>What factors determine the expected return of startups?</h1>\n<p>How to estimate the expected return of startups for employees:</p>\n<ol>\n<li>Start with some index of VC returns, such as the <a href=\"https://www.cambridgeassociates.com/cmb_benchmark_labels/us-venture-capital/\">Cambridge Associates Venture Capital Index</a>.</li>\n<li>These indexes usually provide returns net of fees. VC investors have to pay fees, but startup employees don't. Add <a href=\"https://www.investopedia.com/terms/t/two_and_twenty.asp\">2-and-20 fees</a> back in to get the gross return.</li>\n<li>Unlike with public market investors, VC firms that beat the market in the past tend to continue to beat the market. Employees can choose to work at startups with funding from top VCs. Add a premium to the expected return to account for this.</li>\n<li>Maybe EAs can pick startups better than top VC firms. Possibly add a premium.</li>\n<li>Employees get worse equity terms than VCs, so subtract some discount to account for tihs.</li>\n<li>Startup employees get <a href=\"https://www.benkuhn.net/optopt/\">meta-options</a>, which VCs don't get. Add an appropriate premium.</li>\n<li>Use the current market environment to forecast how future returns will look compared to past returns. (This is the sketchiest step, so we might skip this and just assume future returns equal historical returns. But I think we don't want to skip this because we don't even really know what historical returns were\u2014more on that later.)</li>\n<li>Giving now might be better than giving later. If so, that means we shouldn't compare startups to public investments because public investments aren't the best thing to do with money. Instead, we should compare startup equity to money donated now.</li>\n</ol>\n<p>In the next four subsections, let's break down each of these steps.</p>\n<ul>\n<li><a href=\"#Returns_for_vc_firms\">Returns for VC firms</a> covers steps 1\u20133;</li>\n<li><a href=\"#Returns_for_eas\">Returns for EAs</a> covers steps 4\u20136;</li>\n<li><a href=\"#Forecasting_future_returns\">Forecasting future returns</a> covers step 7; and</li>\n<li><a href=\"#Giving_now_vs_later\">Giving now vs. later</a> covers step 8.</li>\n</ul>\n<h3>Returns for VC firms</h3>\n<p>I used <a href=\"https://www.cambridgeassociates.com/wp-content/uploads/2020/07/WEB-2020-Q1-USVC-Benchmark-Book.pdf\">Cambridge Associates' Venture Capital Index</a> 2020 report to find the aggregate historical return of VC firms from 1995 to 2018<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-1\" id=\"fnref-Kf9Dcpq5kutDdaxAj-1\">[1]</a></sup>. According to this data set, VCs had a geometric mean return of 13.1% with a standard deviation of 18.9%. That's a good starting point.</p>\n<p>As the saying goes, a person with one clock always knows what time it is. Someone with two clocks is never quite sure. Cambridge Associates' <a href=\"https://www.cambridgeassociates.com/wp-content/uploads/2018/05/WEB-2017-Q4-USVC-Benchmark-Book.pdf\">2017 report</a> has data from 1988 to 2016, which gives a geometric mean return of 14.9% and a standard deviation of 17.2%.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-2\" id=\"fnref-Kf9Dcpq5kutDdaxAj-2\">[2]</a></sup></p>\n<p>According to these data sets, VC experienced several regimes:</p>\n<ul>\n<li>Strong performance 1988\u20132000, and especially 1998\u20132000 at the peak of the tech bubble.</li>\n<li>Very bad returns 2001\u20132003.</li>\n<li>Mediocre returns 2004\u20132013, with a mix of good years and bad years.</li>\n<li>Strong returns 2014\u20132020.</li>\n</ul>\n<p>The \"historical performance of VC\" substantially changes depending on which time period you look at. And we don't know what sort of regime will come next.</p>\n<p>Another problem: Other VC return databases give entirely different numbers. For example, I could have used the VentureXpert database, which some (e.g., <a href=\"http://www.vernimmen.com/ftp/KOCH_S_Research_paper.pdf\">Koch (2014)</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-3\" id=\"fnref-Kf9Dcpq5kutDdaxAj-3\">[3]</a></sup>) claim is more accurate. (I used Cambridge Associates purely out of convenience.) Cambridge Associates tends to give higher VC returns than other databases (by a couple percentage points).<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-4\" id=\"fnref-Kf9Dcpq5kutDdaxAj-4\">[4]</a></sup></p>\n<p>I will use the 2020 Cambridge Associates report as a starting point. Just be aware that the startup market will likely behave very differently in the future.</p>\n<p>Now let's convert net returns to gross returns. If we assume VCs usually charge 2-and-20 fees, this step is pretty easy. Using the Cambridge Associates 1995\u20132018 data, we find a gross historical return of 18.1% with a 23.5% standard deviation.</p>\n<p>Some people (especially VCs) like to talk about how the \"top quartile\" of VC firms persistently beat the market. This is true, but potentially misleading. VC firms who beat the market in year N are more likely than chance to beat the market in year N+1. But top-quartile firms are by no means guaranteed to stay in the top quartile.</p>\n<p>How strongly do top-VC returns persist? Data from <a href=\"https://bfi.uchicago.edu/wp-content/uploads/2020/11/BFI_WP_2020167.pdf\">Harris et al. (2020)</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-5\" id=\"fnref-Kf9Dcpq5kutDdaxAj-5\">[5]</a></sup> (1984\u20132014) is presented in the table below<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-6\" id=\"fnref-Kf9Dcpq5kutDdaxAj-6\">[6]</a></sup>.</p>\n<p>The table uses these terms:</p>\n<ul>\n<li>IRR = internal rate of return, or the annual return achieved by investors.</li>\n<li>PME = public market equivalent, or the total return of VCs relative to the S&amp;P 500 (see <a href=\"http://web.mit.edu/aschoar/www/KaplanSchoar2005.pdf\">Kaplan &amp; Schoar (2005)</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-7\" id=\"fnref-Kf9Dcpq5kutDdaxAj-7\">[7]</a></sup>). For example, if a VC fund earns a total return of 60% and the S&amp;P earns 50% in the same period, then the PME is 60% / 50% = 1.2.</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>IRR</th>\n<th>PME</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Average VC</td>\n<td>14.8%</td>\n<td>1.22</td>\n</tr>\n<tr>\n<td>Top-quartile VC, backward-looking</td>\n<td>45.3%</td>\n<td>2.60</td>\n</tr>\n<tr>\n<td>Top-quartile VC, forward-looking</td>\n<td>26.3%</td>\n<td>1.70</td>\n</tr>\n</tbody>\n</table>\n<p>Results for VC firms 2001\u20132014:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>IRR</th>\n<th>PME</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Average VC</td>\n<td>10.4%</td>\n<td>(+)</td>\n</tr>\n<tr>\n<td>Top-quartile VC, backward-looking</td>\n<td>30.0%</td>\n<td>2.11</td>\n</tr>\n<tr>\n<td>Top-quartile VC, forward-looking</td>\n<td>14.7%</td>\n<td>1.20</td>\n</tr>\n</tbody>\n</table>\n<p>(+) This figure is not provided by Harris et al.</p>\n<p>In the full sample (1984\u20132014), top-quartile VCs retained about half of their outperformance out of sample, although they lost most of their relative outperformance (compared to the S&amp;P 500). In the post-2001 sample, they lost most of their outperformance both in relative and absolute terms, but still showed nonzero persistence.</p>\n<p>Harris et al. also did a regression analysis, and found that across all VC firms, one third of PME outperformance persisted.</p>\n<p>(Note: Harris et al. used data from <a href=\"https://www.burgiss.com/\">Burgiss</a>, yet another source for VC returns.)</p>\n<h3>Returns for employees</h3>\n<p>Almost all startups give preferred shares to VCs and common shares to employees. Normally, preferred shares get a 1x <a href=\"https://www.investopedia.com/terms/l/liquidation-preference.asp\">liquidation preference</a>. That means if the company exits, VCs are guaranteed to get back at least the money they put in before employees get anything. This makes employee equity worth less than it appears.</p>\n<p>Example:</p>\n<ul>\n<li>A startup raises funding at a $100 million valuation. VCs have $20 million of preferred shares; founders and employees have $80 million of common shares.</li>\n<li>Later, the startup is acquired for $50 million.</li>\n<li>VCs get back their $20 million. That leaves just $30 million to split among the common shareholders. The valuation went down by 50%, but employees lost 62.5% of their equity value.</li>\n</ul>\n<p>This is basically standard practice. Some startups also give special advantages to VCs. There are lots of ways they can do this, such as:</p>\n<ol>\n<li>a liquidation preference that's higher than 1x (e.g., a 2x preference guarantees that VCs get to double their money before employees get anything)</li>\n<li>a <a href=\"https://www.investopedia.com/terms/f/fullratchet.asp\">ratchet</a>, which gives VCs protection against dilution at the expense of employees</li>\n</ol>\n<p>These sorts of conditions are really bad for startup employees. You might just want to avoid any startup that offers terms like these. (If you work at a startup with no sketchy terms, and they raise a new round of funding that introduces sketchy terms, that alone might be enough reason to start looking for a new job.) For more on what to watch out for, read Ben Kuhn's <a href=\"https://www.benkuhn.net/terms/\">How bad are fundraising terms?</a></p>\n<p>Even without any bad terms, employee stock options introduce some other problems:</p>\n<ul>\n<li>If you don't exercise your options, they could expire before the company exits.</li>\n<li>If you don't exercise your options and they go up in value, you might have to pay income tax or <a href=\"https://www.investopedia.com/terms/a/alternativeminimumtax.asp\">alternative minimum tax</a> instead of capital gains tax.</li>\n</ul>\n<p>You can avoid these problems by exercising your options as soon as they vest, or even <a href=\"https://www.investopedia.com/terms/e/earlyexercise.asp\">early exercising</a> if you can. But even if you do exercise, you might end up paying higher taxes when the startup exits because you'll get pushed into a higher tax bracket. You can (at least partially<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-8\" id=\"fnref-Kf9Dcpq5kutDdaxAj-8\">[8]</a></sup>) mitigate this by donating the stock instead of selling it.</p>\n<p>(A lot of people can't afford to exercise their employee stock options. Perhaps an EA org could make grants or loans to help EAs exercise their options. That would be difficult to set up and they'd have to carefully vet grant recipients, but maybe it could work.)</p>\n<p>According to my rough estimate, a 1x liquidation preference reduces the expected value of common shares by about 10%<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-9\" id=\"fnref-Kf9Dcpq5kutDdaxAj-9\">[9]</a></sup>. That equates to around 1\u20132% per year, depending on how long the company takes to exit. Let's assume a 3% annual discount due to liquidation preference plus tax disadvantage.</p>\n<p>Many startups pay below-market compensation by claiming that their equity is underpriced. Don't buy it. The whole point of working at a startup is that your equity will earn (in expectation) above-market returns. If your employer adjusts for this by giving you less equity, that ruins the (monetary) advantage of working at a startup.</p>\n<p>This essay is not about employee equity terms, but it's an important topic for anyone considering working at a startup. For an in-depth guide, see <a href=\"https://github.com/jlevy/og-equity-compensation\">The Open Guide to Equity Compensation</a> by Joshua Levy et al. For something shorter, I recommend Ben Kuhn's <a href=\"https://www.benkuhn.net/offer/\">checklist for stock option offers</a>.</p>\n<p><a href=\"https://www.benkuhn.net/optopt/\">Startup options are better than they look</a> because employees get \"meta-options\": your compensation package gives you the option to vest stock options at the current price for the next four years. If the company does well, you can \"exercise\" your meta-options by continuing to work there. If it doesn't do well, you can quit. Neither startup founders nor startup investors can do this.</p>\n<p>I piggybacked on <a href=\"https://github.com/benkuhn/option-val\">Ben Kuhn's meta-option model</a> (my code <a href=\"https://github.com/michaeldickens/option-val/tree/apr\">here</a>) and found that meta-options are worth an extra 16 percentage points of return (!!). I just did a quick calculation and didn't perform a sensitivity analysis or anything, so this number could be way off, but let's go with it for now. If correct, this number is so large that working at a startup looks more profitable than starting a startup, unless you believe you'd make for an unusually good entrepreneur.</p>\n<p>It's worth mentioning that you could work at a big company that offers equity compensation, which also behaves like a meta-option\u2014albeit a much less valuable one, because big company stock is not as volatile. Using similar methodology, I found that meta-options at a big company are worth 5 percentage points. That means startup meta-options provide an extra 11 percentage points of value (16% \u2013 5%).</p>\n<p>As far as I know, Ben Kuhn invented the concept of meta-options, and no one has ever rigorously analyzed them. My own modification of his program could contain bugs or logical flaws. The value of meta-options <em>could</em> be large enough to dominate every other factor, or they could be worth nothing. This subject strongly warrants a deeper investigation.</p>\n<p>\u2013\u2013\u2013\u2013\u2013</p>\n<p>If we can get around the practical concerns, EAs can easily match the returns of top VC firms by getting jobs at their portfolio companies. Can EAs do even better? Can we outperform VCs at picking winning startups?</p>\n<p>Let me say up front that I don't believe EAs in general can outperform top-quartile VC firms. But when I say I don't believe it, what I really mean is I assign it less than a 50% probability. So it might still be worth trying.</p>\n<p>(To be more precise, I would give an 80% probability that at least a handful of EAs could pick winning startups better than top VCs, but I don't know how to identify those people in advance.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-10\" id=\"fnref-Kf9Dcpq5kutDdaxAj-10\">[10]</a></sup> I'd estimate a 30% chance that, if a group of EAs decide to go work at startups and make a conscious effort to pick winning startups, then they will do better than top-quartile VCs.)</p>\n<p>EAs as a group are really smart. But professional investors are also really smart, and the overwhelming majority of them still fail to beat the market. Maybe EAs are smarter? Maybe EAs are more rational or clear-thinking in some way that most professional investors aren't? I don't know.</p>\n<p>It's possible that EAs could do a better job than VCs of identifying the best startups. On the other hand, EAs might also do a better job of identifying the best <em>public</em> investments. On the <em>other</em> other hand, startups are hard to invest in, so it might be easier to find underappreciated opportunities. If EAs have an edge in public markets, they probably have an even bigger edge in startups.</p>\n<p>I don't have strong evidence on this either way, so I'm leaning on my prior that almost nobody can beat the market. We do have at least <em>some</em> evidence, but I'm not sure how to interpret it.</p>\n<p>The evidence we do have:</p>\n<ol>\n<li>Over the past few years, EA investors <a href=\"https://forum.effectivealtruism.org/posts/zA6AnNnYBwuokF8kB/is-effective-altruism-growing-an-update-on-the-stock-of\">have beaten the market</a>. This is mostly driven by a single company (FTX), so I don't know how much we can infer from this.</li>\n<li>One person reviewed a few months' worth of EAs' proposed investing ideas and found that they had beaten the market over those few months. (I don't want to go into specifics because this review was not shared publicly, but that's the gist of it.)</li>\n</ol>\n<p>In the rest of this essay, I will assume EAs can't beat top-quartile VCs\u2014not because I am confident that this is true, but because I don't know how to evaluate the evidence. It could be a good idea to look into this in more depth.</p>\n<h3>Forecasting future returns</h3>\n<p>As discussed <a href=\"#Historical_returns\">above</a>, startup returns tend to vary a lot over time, so past performance does a poor job of predicting future performance. But we can't choose between two investments (in this case, public investments vs. startups) unless we believe something about how they will perform. So what should we believe?</p>\n<ul>\n<li>Public US equity and bond markets have unusually low return expectations right now, thanks to high valuations/low yields. When stocks and bonds look bad, money flows into alternatives, including VC. Therefore, it's reasonable to expect VC to have worse future returns as well.</li>\n<li>For the past few years, VCs have been investing much more money than the historical average (<a href=\"https://www.statista.com/statistics/277501/venture-capital-amount-invested-in-the-united-states-since-1995/\">Statista</a>, 2021). Crowdedness suggests low future returns. The only other time we saw similar crowdedness was in the year 2000, which was the beginning of a major losing streak for VC.</li>\n<li>Private equity (that is, leveraged buyouts, not VC) has gotten more expensive over the last decade or so (<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2639647\">Chingono &amp; Rasmussen, 2015</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-11\" id=\"fnref-Kf9Dcpq5kutDdaxAj-11\">[11]</a></sup>), which predicts muted future performance. And Harris et al.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-5\" id=\"fnref-Kf9Dcpq5kutDdaxAj-5:1\">[5:1]</a></sup> found that, while top VC firms' persistence persisted in their whole sample, top private equity firms' returns stopped persisting after 2001. It's not clear why private equity's persistence didn't persist, but whatever the reason, the same thing might happen to VCs.</li>\n</ul>\n<p>The outlook for VCs looks worse than usual. The question is, how much worse? Should we expect future returns to be 1 percentage point lower per year? Or 20 percentage points?</p>\n<p>Well, how much worse to US equities and bonds look? We can reliably predict bonds' long-term returns using the yield. 5-year bonds currently yield around 1%, compared to a 1984\u20132014 average nominal return of 8% (<a href=\"http://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/histretSP.html\">Damodaran, 2021</a>).</p>\n<p>Stock returns are harder to predict. In the short term, they're almost impossible to predict, but we can estimate their return over 10+ year periods with reasonable accuracy. Under a more <a href=\"https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp\">EMH</a>-y model that assumes no change in market valuation (e.g., <a href=\"https://www.aqr.com/Insights/Research/Alternative-Thinking/2021-Capital-Markets-Assumptions-for-Major-Asset-Classes\">AQR, 2021</a>), forward-looking US equity return expectations look around 5 percentage points worse than they did from 1984 to 2014 (4% vs. 9% after inflation). According to a model that assumes valuations will revert to their historical average (such as <a href=\"https://interactive.researchaffiliates.com/asset-allocation#!/?currency=USD&amp;model=ER&amp;scale=LINEAR&amp;terms=REAL\">Research Affiliates, 2021</a>), returns look 10 percentage points worse (-1% vs. 9%). The truth is probably somewhere in the middle.</p>\n<p>In theory, all asset classes should have the same risk-adjusted return. Startups are riskier than stocks or bonds. So if return expectations for stocks/bonds go down by some amount, expectations for startups should go down by more than that. But we don't know if this holds in practice, and we don't even know exactly how risky startups are. If bonds look 7% worse, and stocks look between 5% and 10% worse, then maybe we could assume VC will perform 7% to 10% worse in expectation.</p>\n<p>As for top-quartile VCs: according to Harris et al., over the full historical sample, they outperformed average VCs by a full 11 percentage points. In the post-2000 era, they only outperformed by 4 percentage points, and had a public-market equivalent performance of 1.2 (which means they only weakly outperformed the S&amp;P 500). It seems fair to assume that the future for VCs will look more like 2001\u20132014 than like 1984\u20132000, as the pre-2000 VC market was probably less efficient. We could simply assume top VCs will perform 4 percentage points better than average VCs going forward. But it's also possible that the gap between average and top VCs will continue to narrow.</p>\n<h3>Giving now vs. later</h3>\n<p>Working at a startup is comparable to working for a salary and investing it. But if giving now beats giving later, then you wouldn't want to invest your salary. Instead, you'd want to donate it right away. This makes working at a startup look worse because you can't donate your equity until it becomes liquid.</p>\n<p>This possibility makes the comparison more difficult, so I will mostly ignore it. It's not as simple as applying a fixed discount rate to the value of your startup equity. Just be aware that my methodology for comparing startups vs. big companies only works if giving later is at least as good as giving now, at least for the next few years.</p>\n<h1>Putting together the expected return</h1>\n<p>If we combine all the numbers I came up with in the previous section, we get:</p>\n<ol>\n<li>13% after-fees historical return to VC firms, or 10% after inflation.</li>\n<li>15% real historical return before fees.</li>\n<li>Add 4% for the persistence of top-quartile VCs, giving 19%.</li>\n<li>Add 0% for EAs' extra outperformance. Still 19%.</li>\n<li>Subtract 3% for employee equity terms, giving 16%.</li>\n<li>Add 11% for meta-options, giving 27%.</li>\n<li>Subtract 9% for the relatively poor market outlook, giving 18%.</li>\n<li>Ignore giving now vs. later. Still 18%.</li>\n</ol>\n<p>Thus, I predict a 18% real return for startup employees who try to maximize their earnings (by working at startups with funding from top VCs, getting good equity terms, and exercising their meta-options when necessary).</p>\n<p>How big are the error bars on each of these numbers? In order:</p>\n<ol>\n<li>Historical return depends a lot on what time period you look at. <strong>Wide error bars.</strong></li>\n<li>Calculating before-fees return just requires knowing fees, which are usually 2-and-20. Narrow error bars.</li>\n<li>It wouldn't be too surprising it top-quartile VCs had as much as 11% extra return or as little as 0%. <strong>Wide error bars.</strong></li>\n<li>Even if we have good reason to expect EAs to do better at picking startups than top-quartile VCs, it seems unlikely that they could perform <em>much</em> better. Narrow error bars.</li>\n<li>Liquidation preference matters relatively little. Narrow error bars.</li>\n<li>The concept of meta-options is complicated and has received hardly any attention. My best-guess estimate for their value is very large, but I could be way off. <strong>Extremely wide error bars.</strong></li>\n<li>Future performance is really hard to predict, even over long time horizons. <strong>Wide error bars.</strong></li>\n</ol>\n<p>By my estimate, startup employees' expected returns could optimistically be as high as 52% (!); or they could be as low as -2%.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-12\" id=\"fnref-Kf9Dcpq5kutDdaxAj-12\">[12]</a></sup> (Remember, these are <em>expected</em> returns. <em>Realized</em> returns could fluctuate by much more than this. Startups in aggregate could easily realize a 100% return next year, and I wouldn't find that surprising, but I would be crazy to <em>expect</em> it to happen.)</p>\n<h1>Risk and correlation of startups</h1>\n<p>Expected return alone isn't what we care about. We really want to know <em>risk-adjusted return</em>.</p>\n<p>And we don't just care about the risk-adjusted return of startups in isolation. We want to know how they fit into a broader EA investment portfolio.</p>\n<p>There are two equivalent ways of looking at this:</p>\n<ol>\n<li>Find the risk of startups, and their correlation to the aggregate EA investment portfolio. Then we can calculate whether EAs on the margin should work at startups instead of big companies.</li>\n<li>Find the <a href=\"https://www.investopedia.com/terms/a/alpha.asp\">alpha</a> of startups relative to the EA portfolio. If alpha &gt; 0, that means at least some EAs should work at startups.</li>\n</ol>\n<p>I will focus on the first because I find it more intuitive. I also calculated the second and got similar results (not presented in this essay).</p>\n<p>I'm looking at the risk and correlation of the startup industry, rather than the average risk/correlation of a single startup. We can think of it as a collective decision by many EAs to work at a diversified group of startups, rather than the decision of a single person.</p>\n<p>As with expected return, we have no way to know the future risk of startups, or their correlation to the EA portfolio. But with risk and correlation, we get to make some simplifying assumptions.</p>\n<p>We can't learn much about the future return of an asset class by looking at its past return. Markets are reasonably efficient, so if an asset class performs well, more money floods in and performance reverts to the mean. But the efficient market hypothesis doesn't say <em>risk</em> mean-reverts. Studies show that, at least for public equities, historical volatility is a pretty good predictor of future volatility (e.g., <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3074529\">Dreyer &amp; Hubrich, 2017</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-13\" id=\"fnref-Kf9Dcpq5kutDdaxAj-13\">[13]</a></sup>).</p>\n<p>Three different data sets all give similar(ish) numbers for startups' standard deviations:</p>\n<table>\n<thead>\n<tr>\n<th>Data Set (Gross)</th>\n<th>Standard Deviation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cambridge Associates, 1988\u20132016</td>\n<td>21.4%</td>\n</tr>\n<tr>\n<td>Cambridge Associates, 1995\u20132018</td>\n<td>23.5%</td>\n</tr>\n<tr>\n<td>Harris et al., 1984\u20132014</td>\n<td>28.5%</td>\n</tr>\n</tbody>\n</table>\n<p>Note that, according to Cambridge Associates, top-quartile VCs have higher standard deviations than average VCs (25% or 28%, depending on which time horizon you use). So if we only work at top startups, we should bump these numbers up by a few percentage points. Also, startups don't have public prices the same way stocks do, and VCs have some leeway to value their portfolios however you want. I expect that they tilt their portfolios toward low volatility to make themselves look better, so the \"true\" volatility is probably higher.</p>\n<p><a href=\"http://sandhillecon.com/pdf/MeasuringRiskForVentureAndBuyouts.pdf\">Woodward (2009)</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-14\" id=\"fnref-Kf9Dcpq5kutDdaxAj-14\">[14]</a></sup> argues that, because startup valuations tend to lag the market, a naive regression doesn't show the true relationship between startups and public equities. The paper finds that startups have a stock market <a href=\"https://www.investopedia.com/terms/b/beta.asp\">beta</a> of a little over 2, which corresponds to a standard deviation of about 35%.</p>\n<p>For correlation, as with standard deviation, we can assume the future looks the same as the past. Historical correlation between startups and public equities was around r=0.7. (My own analysis found a correlation of 0.6, and <a href=\"http://sandhillecon.com/pdf/MeasuringRiskForVentureAndBuyouts.pdf\">Woodward (2009)</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-14\" id=\"fnref-Kf9Dcpq5kutDdaxAj-14:1\">[14:1]</a></sup> found a correlation of around 0.7\u20130.8 using more limited data but better methodology. Woodward's analysis suggests that the naive approach underestimates the true correlation. So let's use 0.7.)</p>\n<h1>Leverage</h1>\n<p>Many EA investors probably want to use <a href=\"https://mdickens.me/2020/01/06/how_much_leverage_should_altruists_use/\">leverage</a>. But startup employees can't leverage their equity: they get however much they get based on their employment contract, and there's no way to borrow money to get more equity.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-15\" id=\"fnref-Kf9Dcpq5kutDdaxAj-15\">[15]</a></sup> Instead of comparing startup equity to a public investment portfolio, we should compare startup equity to an <em>optimally leveraged</em> public investment portfolio (taking into account that leverage typically costs more than theoretical models assume).</p>\n<h1>Startups vs. public equities</h1>\n<p>Now that we've discussed the main considerations, we can return to the original question: is it better for earners-to-give to work at high-paying companies and invest their salaries in the market, or to work at startups and \"invest\" in startup equity?</p>\n<p>Some additional assumptions:</p>\n<ol>\n<li>Our goal is to maximize the <a href=\"https://www.effisols.com/basics/rebal.pdf\">geometric return</a> of the overall EA investment portfolio. (This is consistent with logarithmic utility of money.)</li>\n<li>We can only invest in two things: public equities or startups.</li>\n<li>We control 1% of the EA portfolio. We can't affect the other 99%.</li>\n<li>EAs currently invest all their money in public equities, and none in startups. (The latter is obviously false, but it's also sort of true: on the margin, earners-to-give can consider working for startups that don't already have any EAs working for them. That set of startups has 0% EA investment.)</li>\n<li>If we buy public investments, we can use up to 2:1 leverage.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-16\" id=\"fnref-Kf9Dcpq5kutDdaxAj-16\">[16]</a></sup></li>\n<li>Public equities earn an expected real return of 3% with a standard deviation of 16%.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-17\" id=\"fnref-Kf9Dcpq5kutDdaxAj-17\">[17]</a></sup></li>\n</ol>\n<p>Recall from above that we're giving startup equity an 18% expected real return, a 35% standard deviation, and a 0.7 correlation to public equities.</p>\n<p>Our two choices:</p>\n<ol>\n<li>Work at a big company. Invest our salary in public equities with 2:1 leverage.</li>\n<li>Work at a startup.</li>\n</ol>\n<p>Given all the stated assumptions, working at a startup is more than four times better than working at a big company (37 expected utility vs. 200 expected utility, according to a scaled logarithmic utility function<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-18\" id=\"fnref-Kf9Dcpq5kutDdaxAj-18\">[18]</a></sup>).</p>\n<p>Suppose we hold everything else constant but reduce the expected real return of startups. The return needs to be as low as 1% before the startup looks like the worse choice. (Notice that that's lower than the 3% expected return of the public stock market, even before accounting for leverage. Startups with a 2% expected return are still (barely) preferable to public equities with a 3% return because we're assuming startups have a lower correlation to the EA portfolio.) So even under a much more pessimistic projection for startup returns, they still look preferable to big companies.</p>\n<h1>Startups vs. an optimized public investment portfolio</h1>\n<p>Buying an index of public equities might not be the best way to invest one's big-company salary. I personally prefer to invest in <a href=\"https://mdickens.me/2020/11/23/uncorrelated_investing/#Factor_investing\">concentrated value, momentum, and trend strategies</a>. Some EAs believe cryptocurrency or AI stocks will beat the market. The specifics don't matter too much. What matters is that if you believe some other investment has substantially better expected performance than a broad index fund, then you should use that other investment as your benchmark instead. And startups need to look better than that benchmark.</p>\n<p>My best guess is that a concentrated value/momentum/trend portfolio will earn an expected real return of 6% with a standard deviation of 11%. (Of course, as with my estimates for startup returns, these numbers are not remotely robust.) If we also use 2:1 leverage, then value/momentum trend still looks somewhat worse than startups, although not by a as big of a margin (126 expected utility points vs. 200). If startups returned 10% instead of 18%, then value/momentum/trend would be the better choice.</p>\n<h1>Alternative: Predictionless approach</h1>\n<p>Alternatively, take the same basic model as above, but don't try to predict the future. Instead, assume asset classes will perform exactly as well in the future as they performed in the past. As I discussed above, this approach has issues\u2014performance fluctuates a lot over time, so past performance doesn't tell us what will happen in the future. But there's also something appealing about this method. Trying to predict future performance leaves lots of room for you to bias the outcome toward what you (perhaps subconsciously) want. It's harder to introduce bias if you just use past performance.<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-19\" id=\"fnref-Kf9Dcpq5kutDdaxAj-19\">[19]</a></sup></p>\n<p>For the predictionless approach, I estimated the expected return to employee equity as:</p>\n<ol>\n<li>13% after-fees historical return to VC firms, or 10% after inflation</li>\n<li>15% real historical return before fees</li>\n<li>Subtract 2% for employee liquidation preference, giving 13%</li>\n<li>Add 11% for meta-options, giving 24%</li>\n<li>Add 4% for the persistence of top-quartile VCs, giving 28%</li>\n</ol>\n<p>For public equities and for my value/momentum/trend portfolio, instead of making projections, I used the (estimated) historical return after inflation from 1995 to 2018:<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-20\" id=\"fnref-Kf9Dcpq5kutDdaxAj-20\">[20]</a></sup></p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Return</th>\n<th>Std Dev</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Public Equities</td>\n<td>9%</td>\n<td>19%</td>\n</tr>\n<tr>\n<td>Val/Mom/Trend</td>\n<td>15%</td>\n<td>12%</td>\n</tr>\n</tbody>\n</table>\n<p>The three choices have the following expected utilities:</p>\n<ol>\n<li>Public equities: 142 utility</li>\n<li>Val/Mom/Trend: 288 utility</li>\n<li>Startups: 279 utility</li>\n</ol>\n<p>Startups look preferable to public equities, but slightly worse than value/momentum/trend.</p>\n<h1>Alternative: Models are bad. What if we don't use a model?</h1>\n<p>I love using quantitative models like the one in this essay. I think more people should use them. But most models are bad, including mine. They depend on lots of assumptions, and you can change the model output by making small changes to the assumptions.</p>\n<p>How could we reason through this decision <em>without</em> using an explicit model? Let's review some arguments, both pro- and anti-startup.</p>\n<p><strong>Argument from risk preferences:</strong> Most startup employees don't want to donate all their equity. That makes them much more risk-averse than EAs who work at startups. If they're acting rationally, we should expect them to demand higher equity to compensate for the risk. Therefore, startup equity should look particularly compelling to EAs.</p>\n<p><strong>Argument from inefficiency:</strong> The market for startups is illiquid and has high barriers to entry. We might reasonably expect it to be less efficient than public markets, which means we have a better chance of identifying startups that will outperform.</p>\n<p><strong>Argument from investability:</strong> The most reputable VC firms usually don't accept new investors. Even if they can beat the market, you can't invest with them, so it doesn't matter. But there's nothing stopping you from getting jobs at top VCs' portfolio companies.</p>\n<p><strong>Argument from overpopularity of startups:</strong></p>\n<ul>\n<li>A lot of people want to work at startups because startups are cool, and they're willing to accept below-market compensation.</li>\n<li>Total VC investment dollars have <a href=\"https://www.toptal.com/finance/venture-capital-consultants/state-of-venture-capital-industry-2019\">increased a lot</a> over the past few years, even though the number of startups hasn't changed much. So the startup market might be overinflated.</li>\n</ul>\n<p><strong>Argument from underappreciated risk:</strong> In my experience, almost nobody understands how risky individual startups are. Even medium-sized companies are about <a href=\"https://mdickens.me/2020/10/18/risk_of_concentrating/\">3x as risky</a> as the S&amp;P 500. I don't have sufficiently granular data on startups, but startup-sized public companies are about 5\u20136x as volatile as the S&amp;P 500, and my guess is startups are even worse. When I see people discussing the value of startup equity, they almost never properly account for this.</p>\n<p><strong>Argument from diversification:</strong> If you get a job at a startup where no other EAs work, you're adding an entirely new investment to the EA portfolio. That could be a good thing even if that particular investment has a worse expected value than the market. On the other hand, there are <a href=\"https://mdickens.me/2020/11/23/uncorrelated_investing/\">other ways of diversifying</a> that might be better.</p>\n<p>These qualitative arguments don't obviously lean one way or the other. My intuition from my time working at startups and knowing lots of startup employees is that most people overvalue startups and underestimate risk, which means they probably push down the market rate for equity compensation. But even if most startup employees don't behave consistently with their personal risk appetite, they still might behave more risk-aversely than EAs ought to.</p>\n<h1>Practical details</h1>\n<p>If more EAs want to work at startups, there are some ways that people or organizations could support this effort, such as:</p>\n<ul>\n<li>Maintain a list of startups with funding from top VCs, or startups that look particularly promising for whatever reason.</li>\n<li>Coordinate to identify companies that don't already have EAs working at them, or that might provide the most diversification benefits to the EA portfolio.</li>\n<li>Help EAs review employment contracts from prospective employers.</li>\n<li>Make loans or grants to EAs to help them exercise stock options as soon as they vest.</li>\n<li>Career support/recruiting services for EAs who want to work at startups.</li>\n<li>Support for EAs whose startups fail. Maybe even offer some kind of insurance to reduce risk, e.g., if you go work for a startup and it fails, we will pay you to compensate for the earnings you could have had.</li>\n<li>Help people negotiate for better equity terms.</li>\n</ul>\n<p>Some of these ideas are logistically difficult, maybe even impossible. I'm not sure the best way to provide support for earners-to-give who choose to work at startups, but it's something to consider. I believe it would be valuable if an organization existed that helped EAs with these practical details.</p>\n<h1>Conclusion</h1>\n<p>Assuming my model is approximately correct, what type of person might want to work at a startup?</p>\n<ul>\n<li>Someone who wants to earn to give.</li>\n<li>Someone who doesn't have the right skills or temperament to start a startup, but still might want to work at one.</li>\n<li>Someone with special insight into a field who thinks they can identify the most promising companies.</li>\n<li>Perhaps someone who can't invest with leverage, or who doesn't want to use leverage, but who is comfortable with the risk of startup equity.</li>\n</ul>\n<p>Who might not want to work at a startup?</p>\n<ul>\n<li>Someone who's not comfortable with the risk (equity risk or career risk or both).</li>\n<li>Someone who believes they can see particularly good investment opportunities /outside/ of startups, and wants to earn a high salary so they can invest in those other opportunities.</li>\n<li>Someone who thinks donating now is significantly better than donating a few years from now, and therefore doesn't want to wait for startup equity to vest.</li>\n</ul>\n<p>My analysis suggests that working at a startup has good expected value <strong>under ideal conditions</strong>. If you get a job offer from a startup, remember to pay attention to the <a href=\"https://www.benkuhn.net/offer/\">specifics of the offer</a>:</p>\n<ol>\n<li>Is your total compensation competitive with what you'd get at a big company? (Taking startup equity at face value)</li>\n<li>Does your equity contract include any sketchy terms?</li>\n<li>etc. (Too many specifics to list all of them)</li>\n</ol>\n<h1>Areas for further research</h1>\n<p>Many subjects warrant a deeper investigation:</p>\n<ul>\n<li>Historical VC returns</li>\n<li>Historical returns earned by startup employees</li>\n<li>How employee equity terms affect the value of equity</li>\n<li>Value of meta-options</li>\n<li>How most startup employees decide where to work\u2014most importantly, how sensitive are they to the value of equity?</li>\n<li>Why EAs might or might not expect to beat the market</li>\n<li>How Woodward (2009)<sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-14\" id=\"fnref-Kf9Dcpq5kutDdaxAj-14:2\">[14:2]</a></sup>'s analysis looks if you update it to include more recent data</li>\n<li>Relevance of giving now vs. later</li>\n<li>Career capital from working at startups. Does working at a startup train you to start a startup or a nonprofit?</li>\n<li>Other considerations worth including</li>\n</ul>\n<h1>Acknowledgements</h1>\n<p>Thanks to Linchuan Zhang for commissioning this research project and providing support. Thanks to Charles Dillon for feedback.</p>\n<h1>Appendix A: Startups for founders and investors</h1>\n<p>This essay has looked at startups from the perspective of employees. How do startups look for other types of people?</p>\n<p><strong>Founders:</strong> Similar to employees in many ways. The upside is you get a lot more equity. (<a href=\"https://web.stanford.edu/~rehall/Hall-Woodward%20on%20entrepreneurship.pdf\">Hall &amp; Woodward (2012)</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-21\" id=\"fnref-Kf9Dcpq5kutDdaxAj-21\">[21]</a></sup> found that VC-backed startup founders on average made much more money than salaried employees.) The downside is you have to actually start a startup, which is much harder and may require an entirely different skillset.</p>\n<p>Other people have written about whether EAs should start startups:</p>\n<ul>\n<li>Applied Divinity Studies, <a href=\"https://applieddivinitystudies.com/billionaire/\">Life Advice: Become a Billionaire</a></li>\n<li>Mathieu Putz, <a href=\"https://forum.effectivealtruism.org/posts/m35ZkrW8QFrKfAueT/an-update-in-favor-of-trying-to-make-tens-of-billions-of\">An update in favor of trying to make tens of billions of dollars</a></li>\n<li>Carl Shulman, <a href=\"https://80000hours.org/2012/01/salary-or-startup-how-do-gooders-can-gain-more-from-risky-careers/\">Salary or startup? How do-gooders can gain more from risky careers</a></li>\n<li>Brian Tomasik, <a href=\"https://reducing-suffering.org/calculator-expected-utility-founding-startup/\">Calculator for Expected Utility of Founding a Startup</a></li>\n</ul>\n<p><strong>VC limited partners:</strong> If you give your money to a VC firm to invest, this is probably worse than being a startup employee (although it does have the advantage that you don't need to get a new job). You have to pay VC fees and you don't get meta-options. You do get a better liquidation preference, but that's usually not worth as much. For a more detailed discussion on investing in VC, see <a href=\"https://gigaom2.files.wordpress.com/2012/05/vc-enemy-is-us-report.pdf\">Mulcahy et al. (2012)</a><sup class=\"footnote-ref\"><a href=\"#fn-Kf9Dcpq5kutDdaxAj-22\" id=\"fnref-Kf9Dcpq5kutDdaxAj-22\">[22]</a></sup>.</p>\n<p><strong>Angel investors:</strong> If you become an angel investor, you don't have to pay VC fund fees, but you do have to evaluate startups on your own.</p>\n<h1>Appendix B: Some important tangents</h1>\n<p>The points below are all important, but they distract from the thesis of this essay, so I'm not commenting on them in detail.</p>\n<ol>\n<li>Within the context of my model, some big companies' compensation packages behave more like startups'. Companies such as Facebook and Google offer equity to employees. Unlike with startups, you can sell the equity as soon as you get it. But you usually have to wait a year, and a big company's equity grant still behaves like a meta-option.</li>\n<li>There are many non-monetary pros and cons to working at a startup. For instance, see <a href=\"https://danluu.com/startup-tradeoffs/\">Big companies vs. startups</a> and <a href=\"https://forum.effectivealtruism.org/posts/ejaC35E5qyKEkAWn2/early-career-ea-s-should-consider-joining-fast-growing\">Early career EA's should consider joining fast-growing startups in emerging technologies</a>.</li>\n<li>An unimportant point that I nonetheless want to address: Startup employees' equity will get diluted by future fundraising rounds. This doesn't matter because VCs will get diluted by the same amount, so it doesn't make employee equity look worse relative to VC equity (unless the employment contract contains sketchy terms around who gets diluted, in which case maybe you shouldn't work there). Normally, VCs have the option to invest more money in future rounds to negate their dilution, but this also doesn't matter because it doesn't change the return on their initial equity purchase.</li>\n</ol>\n<h1>Notes</h1>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-1\" class=\"footnote-item\"><p>The report includes VC returns up to 2020, but it only includes detailed data up to 2018. So when I did my analysis, I used the 1995\u20132018 data. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-2\" class=\"footnote-item\"><p>Somewhat concerningly, these two data sets show different numbers even for years where they overlap. For example, the 1988\u20132016 data set quotes a 60.09% return for the year 1996, whereas the 1995\u20132018 data set claims a 63.46% return for the same year. This discrepancy is at least partially because the 1995\u20132018 series includes more VC firms, but I haven't read the Cambridge Associates reports in enough detail to say if that's the only reason. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-3\" class=\"footnote-item\"><p>Koch (2014). <a href=\"http://www.vernimmen.com/ftp/KOCH_S_Research_paper.pdf\">The risk and return of venture capital.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-4\" class=\"footnote-item\"><p>Woodward (2009). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1458050\">Measuring risk for venture capital and private equity portfolios.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-5\" class=\"footnote-item\"><p>Harris, Jenkinson, Kaplan &amp; Stucke (2020). <a href=\"https://bfi.uchicago.edu/wp-content/uploads/2020/11/BFI_WP_2020167.pdf\">Has Persistence Persisted in Private Equity? Evidence from Buyout and Venture Capital Funds</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-5\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-5:1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-6\" class=\"footnote-item\"><p>Figures are copied or inferred from Harris et al. (2020), Table 1, Table 2, and Table 4. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-7\" class=\"footnote-item\"><p>Kaplan &amp; Schoar (2005). <a href=\"http://web.mit.edu/aschoar/www/KaplanSchoar2005.pdf\">Private Equity and Performance: Returns, Persistence, and Capital Flows.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-8\" class=\"footnote-item\"><p>At least in the United States, if you donate stock to charity, you can only deduct up to 30% of your income. If you get lucky and make a bunch of money when your startup exits, your startup equity could account for something like 90% of your income. You can only deduct 30%, so you're stuck paying taxes on the other 60%. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-9\" class=\"footnote-item\"><p>Very roughly, a startup has a 70% chance to be worth 0x, 20% chance of 0\u20131x, and 10% chance of &gt;1x. Liquidation preference only matters in the 0\u20131x case, where common shares are worth about half as much as their face value.</p>\n<p>If you don't filter out sketchy terms, the appropriate discount is <a href=\"https://www.benkuhn.net/terms/\">more like 36%</a>. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-10\" class=\"footnote-item\"><p>Actually, I know a few people who I believe could do a good job of identifying top startups if they took the time to conduct lots of interviews and due diligence. But they're not going to do that because they're busy doing other important EA-related activities. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-11\" class=\"footnote-item\"><p>Chingono &amp; Rasmussen (2015). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2639647\">Leveraged Small Value Equities.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-12\" class=\"footnote-item\"><p>For the optimistic estimate, I assumed: top-quartile VCs' average out-of-sample return of 26% (or 23% real) fully persists; EAs perform 5% better than top VCs; and meta-options are worth 20% (which follows from optimistic assumptions about how meta-options behave).</p>\n<p>For the pessimistic estimate, I assumed: public equity valuations fully mean revert, and startups perform even worse due to higher risk; top quartile VCs' returns do not persist at all; and meta-options are worthless (probably because there's some flaw with them that I haven't thought of). <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-13\" class=\"footnote-item\"><p>Dreyer &amp; Hubrich (2017). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3074529\">Tail Risk Mitigation with Managed Volatility Strategies.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-14\" class=\"footnote-item\"><p>Woodward (2009). <a href=\"http://sandhillecon.com/pdf/MeasuringRiskForVentureAndBuyouts.pdf\">Measuring Risk for Venture Capital and Private Equity Portfolios.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-14\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-14:1\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-14:2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-15\" class=\"footnote-item\"><p>Technically, if you get employee stock options, your equity is leveraged. But the amount of leverage approaches zero as the stock price increases. And most stock options are only a little bit leveraged to begin with. For example, if your company stock was last valued at $4 and you get options with a $1 strike price, that's only 1.33:1 leverage. If the stock price doubles to $8, now you only have 1.14:1 leverage. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-16\" class=\"footnote-item\"><p>In keeping with my <a href=\"https://mdickens.me/2020/01/06/how_much_leverage_should_altruists_use/#Cost_of_leverage\">previous work</a> on leverage, I assume that borrowers must pay 1% plus the risk-free rate. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-17\" class=\"footnote-item\"><p>This is the average of the projections by AQR and Research Affiliates as of October 2021. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-17\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-18\" class=\"footnote-item\"><p>The utility function takes the geometric mean return as the utility and multiplies by 100,000 to make the numbers more readable. As a baseline, it calculates the expected utility of the EA portfolio without your investment, and then subtracts that from the total expected utility of the EA portfolio including your investment. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-18\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-19\" class=\"footnote-item\"><p>Still not impossible, because you could pick the data set or the time series that most closely matches the outcome you want. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-19\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-20\" class=\"footnote-item\"><p>For public equities, I used the historical return of the total US stock market, assuming zero fees or trading costs. To find the historical return of Val/Mom/Trend, I created a hypothetical portfolio that invested 80% in Alpha Architect's Value Momentum Trend Index and 20% in AQR's Managed Futures Index, which roughly reflects how I actually invest my money. Both indexes subtract estimated fees and trading costs. The historical returns are hypothetical, not actual. I didn't have data for 2018, so I calculated summary statistics over 1995\u20132017 instead. <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-20\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-21\" class=\"footnote-item\"><p>Hall &amp; Woodward (2012). <a href=\"https://web.stanford.edu/~rehall/Hall-Woodward%20on%20entrepreneurship.pdf\">The Burden of the Nondiversifiable Risk of Entrepreneurship.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-21\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Kf9Dcpq5kutDdaxAj-22\" class=\"footnote-item\"><p>Mulcahy, Weeks &amp; Bradley (2012). <a href=\"https://gigaom2.files.wordpress.com/2012/05/vc-enemy-is-us-report.pdf\">We Have Met the Enemy...and He Is Us: Lessons from Twenty Years of the Kauffman Foundation's Investments in Venture Capital Funds and the Triumph of Hope over Experience.</a> <a href=\"#fnref-Kf9Dcpq5kutDdaxAj-22\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "MichaelDickens"}}, {"_id": "wrdWS2K8hWfoAzRst", "title": "What would you do if you had a lot of money/power/influence and you thought that AI timelines were very short?", "postedAt": "2021-11-12T21:59:07.383Z", "htmlBody": "<p>To make things more specific:<br>Lot of money = $1B+; lot of power = CEO of $10B+ org; lot of influence = 1M+ followers, or an advisor to someone with a lot of money or power.<br>AI timelines = time until an AI-mediated existential catastrophe<br>Very short = \u2265 10% chance of it happening in \u2264 2 years.</p><p>Please don\u2019t use this space to argue that AI x-risk isn\u2019t possible/likely, or that timelines aren\u2019t that short. There are <a href=\"https://forum.effectivealtruism.org/tag/ai-forecasting\">plenty of</a> <a href=\"https://www.lesswrong.com/tag/ai-timelines\">other</a> <a href=\"https://www.alignmentforum.org/tag/ai-timelines\">places</a> to do that. I want to know what you would do <i>conditional on being in this scenario</i>, not whether you think the scenario is likely.</p>", "user": {"username": "Greg_Colbourn"}}, {"_id": "Xosrru2dTmGmqb3Db", "title": "What has helped you write better?", "postedAt": "2021-11-12T18:54:10.081Z", "htmlBody": "<p>What content or processes have helped you improve your writing?&nbsp;<br><br>I read Kat Wood's article (below) and it made me wonder what resources people would recommend.<br><br>https://forum.effectivealtruism.org/posts/dAbs7w4J4iNm89DjP/why-boring-writing-is-unethical-the-case-for-it-being-high</p>", "user": {"username": "nathan"}}, {"_id": "DGyQAJuBZc4ZHJn6n", "title": "What does the growth of EA mean for our priorities and level of ambition?", "postedAt": "2021-11-15T09:07:18.535Z", "htmlBody": "<p>Lots of people have claimed that effective altruism hasn\u2019t been growing in recent years. In a talk at the most recent EA Global, I argued that it has.</p><p>I then explored how this growth has changed the priorities for the movement, and argued that we should be more ambitious.</p><p>You can see the video by clicking <a href=\"https://80000hours.org/2021/11/growth-of-effective-altruism/#watch-the-talk\">here</a>, or read a transcript  below. The talk was 30 minutes, followed by a Q&amp;A with  audience-submitted questions.</p><p>It summarises several recent pieces of research, including <a href=\"https://80000hours.org/2021/07/effective-altruism-growing/\">Is effective altruism growing?</a>, <a href=\"https://80000hours.org/2021/08/effective-altruism-allocation-resources-cause-areas/\">How are resources allocated across causes?</a>, <a href=\"https://80000hours.org/articles/be-more-ambitious/\">Why to be more ambitious</a>, and <a href=\"https://twitter.com/ben_j_todd/status/1423318852801290248\">Why we need more megaprojects</a>.</p><p>I added an explanation of why the large amount of additional funding available doesn\u2019t mean that it\u2019s <em>easy</em>  to fundraise (and why me talking about a \u2018funding overhang\u2019 was  probably a mistake). A better framing is that there is a lot of funding  available for any projects that can clear the current funding bar, but  this bar is still pretty high.</p><p>Finally, I suggest in the talk that the recent success of <a href=\"https://80000hours.org/stories/sam-bankman-fried/\">Sam Bankman-Fried</a> is an additional reason to aim high.</p><p>First, he shows that it\u2019s possible to succeed. Back in 2015, perhaps only about 1,000  people were seriously directing their careers on the basis of effective  altruism. And now one of them has made billions of dollars to donate,  and become the world\u2019s richest person under 30 \u2014 that\u2019s not bad odds!</p><p>Second,  the increase in the funding available to the community means that new  projects that couldn\u2019t have happened before should now be possible.</p><p><strong>Watch the talk with the Q&amp;A <a href=\"https://80000hours.org/2021/11/growth-of-effective-altruism/#watch-the-talk\">here</a>.</strong></p><p><strong>Or you can read the transcript (without the Q&amp;A):</strong></p><p>Hey everyone. Today, I\u2019m going to talk about why I think the  resources available to the effective altruism community have been  growing, how that might change our priorities, and why I think it means  we should try to be more ambitious.</p><p>The first part of the talk is  about whether effective altruism has been growing. Lots of people have  been saying that effective altruism isn\u2019t growing, and they cite things  like this chart, which is the number of people searching for the term  \u2018effective altruism\u2019 on Google. And it does indeed look like this isn\u2019t  growing, which is not ideal.</p><p>But to get a better picture of the  growth of the movement, it\u2019s useful to make a couple of distinctions.  The first distinction is between the number of people finding out about  effective altruism each year, and the total number of people involved \u2014  so that\u2019s the difference between the flow and the stock. And then the  second is, it\u2019s useful to distinguish between different levels of  engagement. So there\u2019s the number of people who\u2019ve just heard about  effective altruism, the number of people kind of reading a lot, and then  there\u2019s highly engaged people. And people often call this the top,  middle, and bottom of the funnel.</p><p>The search volume chart  basically shows that the flow into the top of the funnel is not growing,  and that is obviously bad \u2014 it would be better if all aspects of the  funnel were growing. But if I was going to pick just one thing to focus  on in order to evaluate whether effective altruism is growing or not, it  would be the growth rate of the stock at the bottom of the funnel. And  then that could then further be divided into a couple of different  things we could look at.<br> You can think of the impact of effective  altruism as a function of how many people there are and how much funding  there is, where the efficiency with which that\u2019s turned into impact  depends on how good our ideas are.</p><p>So this is an illustration of  that. As you get more funding and people, impact hopefully goes up,  though at a diminishing rate. And then if we have better ideas, it  shifts how much impact we\u2019re able to have with each level of people and  funding. There\u2019s a lot more complications you could add to this \u2014 like  there\u2019s probably some complementarities between funding and people and  so on \u2014 but just as a very rough illustration.</p><p>I\u2019m now going to look at the growth in funding and then the growth in people. Earlier this year, <a href=\"https://80000hours.org/2021/07/effective-altruism-growing/\">I made these estimates</a>  of how much funding is committed to effective altruism in total. These  are all very rough figures \u2014 they could easily vary by a factor of five,  either side \u2014 but in total, I added up to about $46 billion. And maybe  more strikingly, I estimated that that seems to have grown almost  fivefold since 2015, which is a 37% annual rate of growth.</p><p>What  drove the growth? One thing was that Dustin Moskovitz\u2019s net worth \u2014 he\u2019s  the main source of the funds behind Open Philanthropy \u2014 has grown a  lot, as Facebook stock has gone up and his new company Asana also IPO\u2019d.</p><p>And then perhaps more strikingly is <a href=\"https://80000hours.org/stories/sam-bankman-fried/\">Sam Bankman-Fried</a>,  the founder of FTX and Alameda. We met Sam in 2012 when Will was giving  a talk at MIT at a student group event. And he made the arguments for  earning to give, and then Sam turned around and actually went and did  it. And the <em>Financial Times</em> now estimates that he\u2019s the world\u2019s richest person under 30, and <em>Forbes</em>  recently estimated his net worth is over $20 billion \u2014 actually gone up  another five or six billion since I made the estimates a few months  ago. I definitely wouldn\u2019t have predicted this would\u2019ve happened back in  2012 when we started 80,000 Hours.</p><p>It\u2019s not just Sam though; all  these other sources I think have grown a lot as well since 2015. So for  instance, Founders Pledge in 2015 only had under $100 million dollars  pledged when I made the estimate. I was using a figure of $3 billion  pledged, and I\u2019ve heard that\u2019s now actually more like over $5 billion.  So there\u2019s a lot of growth in other terms as well.</p><p>That\u2019s the  stock of available funds. Just to talk quickly about the amounts of  funding deployed each year, I estimated that\u2019s around $400 million. It\u2019s  just important to remember with this one, it\u2019s very lumpy from year to  year. Open Philanthropy accounts for a lot of those funds, and you can  see that that grew very rapidly from 2015 to 2017. It\u2019s very lumpy each  year, but then they\u2019ve held it roughly constant from 2017 \u2014 but that was  a deliberate decision to consolidate their existing cause areas, build  up staff capacity. The plan with Open Philanthropy is that Good Ventures  \u2014 so Dustin Moskovitz and Cari Tuna \u2014 want to spend down all the money  before they die. And in order to do that, they need to hit something  more like a billion per year fairly soon, so that\u2019s definitely expected  to go up. Sorry, ignore the 2021 figure, that\u2019s incomplete.</p><p>So  that was funding. Estimating the number of people involved is a lot more  difficult. This is my favourite way to do it. This is probably not  obeying best practice for slides, but the basic idea is you look at the <a href=\"https://forum.effectivealtruism.org/tag/effective-altruism-survey\">Effective Altruism Survey</a>,  you see how many people say they\u2019re engaged, and then you adjust it by  the response rate. And the way you estimate the response rate is you  take a group, like everyone who works at CEA, and then find out exactly  how many filled out the survey, and then that gives you a proxy for  that.</p><p>Applying that method in 2019, there were 900 or so people  who filled out the survey saying they were very engaged, and then we  estimated a 40% response rate, so there were 2,300 highly engaged  members of the community. If you used a wider definition, then that  number would be a lot larger, but just focusing on that very core at the  centre. Applying the same method in 2020, around the same number of  people said that they were engaged, but we estimated the response rate  was a bit lower, so this method ended up with a 13% growth rate.</p><p>I\u2019ve  tried a number of other methods, and I generally end up with estimates  between zero and 30%. Many ways of estimating it don\u2019t really work,  because they don\u2019t take account of the fact that it takes people several  years between hearing about effective altruism and getting involved.  And so if you look at like 2020 data, it\u2019s missing almost everyone who  actually came in in 2020.</p><p>Just one interesting thing I was  figuring out today is, I also estimated last year that around maybe 3%  of people leave each year in that group of engagement. That\u2019s also very  uncertain, but if that holds up, then the stock of the community would  eventually tend to around 10,000 people at equilibrium size, if the flow  stays constant. So even if we don\u2019t grow the number of people getting  involved each year, this will naturally drift up to around 10,000 \u2014  though hopefully we can.</p><p>That was number of people and funding. I  just want to do a final bit of this section, just to talk about how  that\u2019s allocated across issues. I also made these rough estimates of the  breakdown of funding by cause area. This is mainly Open Philanthropy\u2019s  funding \u2014 averaged over a couple of years to take account of the  lumpiness \u2014 and then GiveWell added in. Open Philanthropy is about 60%,  then GiveWell 20%, and then I made rough estimates of the remaining 20%.  And you can see the big thing here is there\u2019s a big allocation to  global health, which is driven by GiveWell.</p><p>The situation for  people is actually really different. This is people who said five out of  five for engagement on the Effective Altruism Survey \u2014 so this is the  most engaged people. But you can see, firstly, they\u2019re much more spread  out over issues, and then these more meta things and AI are actually at  the top when it comes to the most engaged people. This is actually  exactly what we should hope for in some ways, because in a cause where  you can hire people outside the community to do really useful work \u2014  like in global health \u2014 then you can deploy lots of funding quickly. And  then people who are engaged in effective altruism should focus on the  areas where it\u2019s hardest to hire people outside of the community. For  instance, in movement building, you can\u2019t really hire people who don\u2019t  believe in effective altruism to do movement building for it. And same  with AI alignment, because there\u2019s not such an existing field there. You  need people who are really into the ideas themselves.</p><p>So in some  ways, we should expect quite different allocation for both funding and  people. Some final speculations around what might be the ideal  allocation of people across issues. This was a survey in the 2019  Coordination Forum shown in blue there, where people were asked what  would be the ideal fraction of resources put into each area. And then  also, shown in red, is users of the Effective Altruism Forum in the EA  Survey \u2014 which is about 1,000 people \u2014 saying which their top cause  preference was. And you can see those two actually line up quite well,  which is interesting.</p><p>Now, I made this very made-up actual figure,  which is a combination of people and funding, where I used a $100,000  per year conversion rate to get the total in each one. And then I\u2019ve  compared that to one estimate of the ideal portfolio. And what jumps out  here potentially \u2014 though it\u2019s very sensitive to exactly how you  aggregate them \u2014 is that it seems like global poverty is a bit  overallocated. I wouldn\u2019t suggest we cut back funding to global poverty,  but rather my hope is that the other causes will be able to grow faster  over the next 10 years.</p><p>The other thing that really leapt out to  me is the biggest gap is actually the bottom one, which is broad  longtermism. That\u2019s things like reducing great power conflict and  improving institutional decision-making \u2014 efforts to make society  generally better at dealing with big risks or other important  challenges. And people in the survey thought that that area should have  5% to 15% of resources, depending on the person. But as far as I can  tell, it\u2019s under 1% currently, so on a proportional basis, that\u2019s the  biggest gap in the portfolio currently.</p><p>That\u2019s the first section.  The second section is about what might be some of the implications of  this, and in particular, what are the implications of how much funding  there is now? $46 billion \u2014 what should we be doing in response to this?  And you could see there being two main options.</p><p>Option 1 is just  to find the most scalable thing we can find, and spend the money as  quickly as possible on that. Within more neartermist issues, that might  be something like GiveDirectly or other cash transfers, which could  absorb billions of dollars of funding and achieve a lot. Within  longtermism, it might be something more like green energy research and  development, which currently receives $35 billion per year of funding.  You could add billions of dollars to that with probably similar returns  to the current amounts. Maybe even better might be scaling up some  existing biosecurity pandemic prevention methods on a billion-dollar  basis. So that\u2019s Option 1.</p><p>Option 2 is to try to find something  even more effective than these. This seems to be what funders are doing  currently \u2014 basically, funders are holding their bar significantly  higher than GiveDirectly. And that\u2019s because they believe that by  waiting, we\u2019ll be able to find and also create new opportunities that  are significantly more cost effective than GiveDirectly, and therefore  over the long term have a much bigger impact. I\u2019d say the current bar of  funding within Global Development is more around the level of Against Malaria  Foundation, which GiveWell estimates is 15 times more cost effective  than GiveDirectly. Generally, charities that are around that level of  cost effectiveness, that level of evidence base, and good in the same  other ways, have a good shot of getting funding.</p><p>Within  longtermism, it\u2019s a bit harder to say where the bar is. Maybe projects  that have reasonable shots of doing something useful in AI safety or  global catastrophic biorisks have a reasonable shot of getting funded,  which you could think of as some of the recent things that have been  funded by the <a href=\"https://funds.effectivealtruism.org/funds/far-future\">Effective Altruism Long-Term Fund</a>.  The hope is that these things might be 10 or even more times more  effective than something like green energy R&amp;D. Meta-charities would  then be expected to provide a multiplier on either of the other two.</p><p>To get a more concrete thing, there was a good post on the Effective Altruism Forum recently, which is called \u201c<a href=\"https://forum.effectivealtruism.org/posts/DqwxrdyQxcMQ8P2rD/list-of-ea-funding-opportunities\">List of EA funding opportunities</a>.\u201d  It lists specific places you can get funding from, which also helps to  give an idea of what funders are currently focusing on. One point I want  to make about this is that these are hard bars to clear \u2014 green energy  R&amp;D might well be one of the best things within climate change, and  we\u2019re saying people are trying to find things that are 10 or 100 times  more cost effective than that. And the same within global health versus  GiveDirectly. It\u2019s even harder to clear those bars if you take account  of the fact that funders are going to be worried about things like the  estimates being too optimistic, discount rates, opportunity costs, lots  of other complications.</p><p>So a startup charity might well need to be  targeting a level that\u2019s even 10 times higher than those bars again,  with the expectation that it will come down over time. The challenge  that the community faces is, can we prove these funders right and  actually generate all these opportunities that they\u2019re hoping to find,  and perhaps even expecting to find? And therefore have much greater  impacts with this money than if we just donated it all now?</p><p>And so  this means in terms of careers, there are these opportunities to have a  lot more impact than something like earning to give and then donating  to GiveDirectly. Which is already an extremely impactful level \u2014 I  wouldn\u2019t be surprised if something like earning to give and giving to  GiveDirectly was more impactful than 98% of jobs that people take, and  it\u2019s really making a big difference in the lives of hundreds of people  over the course of your career.</p><p>But because of this huge amount of  funding, there\u2019s an opportunity to go even beyond that. Here are some  of the things that seem like priorities based on it. The first would be  active grantmakers: people able to help find and also cultivate  opportunities and projects that are above those funding bars that I  mentioned before. If you\u2019re earning to give currently, this could be the  direction you could take your giving \u2014 can you get into the position  where you could be like an angel donor in effective altruism and  spotting things that other donors aren\u2019t yet spotting? That\u2019s a really  valuable way of kind of adding value as a donor currently.</p><p>Secondly,  we need researchers to figure out what these ideas for spending a lot  more funding above the bar would be, or other ways to make the portfolio  more effective. We need people founding projects that are more at this  level of cost effectiveness. We need managers who are able to take the  projects that are already there and help scale them up and reach their  full potential. Then we also need people doing movement building to find  all the people, those types of people above. There\u2019s also a big need  for feeder roles and supporting roles for all of these kinds of things \u2014  things like being a research assistant and all types of  organisation-building roles, like operations management, office  management, recruiting, things like that.</p><p>I would roughly say that  if I had to put an estimate on it \u2014 so you could imagine we could  magically create someone who\u2019s earning to give donating certain amounts  of money each year, or we could magically create someone who\u2019s a good  fit for one of these roles (especially the more senior versions) \u2014 I  would often prefer the person to around millions of dollars of funding  per year, in some cases over $10 million. That\u2019s just a rough way that I  would try to quantify the value of some of these positions in terms of  earning to give; you could scale those figures down for the more junior  equivalents.</p><p>Then based on this, I would say some rough priorities  early career\u2026 Oh yeah, actually just before that\u2026 These things here are  just the roles that impact has been changed by the funding situation.  But there\u2019s loads of roles that don\u2019t require funding from the effective  altruism community that are still really impactful. Some quick examples  of those would be, people could have a lot of impact by improving  things in government and policy, but they don\u2019t require funding to do  that \u2014 they can just take those positions. And I just want to flag, I\u2019m  not talking about all positions here, just the ones that have been  changed by the funding situation. Just as another example, lots of  people could work in journalism or the media, and promote really  neglected and pressing issues and have a big impact doing that. And  again, they don\u2019t particularly need funding from effective altruism to  do that necessarily.</p><p>Here\u2019s very, very rough early-career  priorities based on those roles. I\u2019d say a big priority early would be  trying to test out your fit for those roles, and then if you find a good  opportunity, aiming at them. Secondly, there\u2019s building career capital  for those roles or building career capital for 20 years in the future  when effective altruism will hopefully be a lot bigger and it\u2019ll need a  much wider range of skills \u2014 skills need to become broader over time.  And then a third option is to just find a job that\u2019s a good fit, and  then on the side, don\u2019t just donate, but you could also do community  building and being a sensible advocate for some of these effective  altruism issues. I think that can be a route to having just as much  impact, and perhaps more than donating.</p><p>If you are currently  earning to give, then I would say it\u2019s worth seriously doing a round of  applying to these types of roles and seeing whether there\u2019s any  opportunities that are a good fit \u2014 so seriously considering switching.  It will be the best role for many people in the end.</p><p>In the past, I  think I\u2019ve helped to be fairly confusing a couple of times. One was by  talking about talent constraints, the community being talent  constrained. It makes it sound like it\u2019s easy to get a job. And then  more recently, talking about there being a funding overhang. It makes it  sound like it\u2019s easy to get funding.</p><p>Unfortunately, none of those  two things are true. It\u2019s more accurate to not use those terms. If  you\u2019re able to find opportunities that are much more effective than  these funding bars \u2014 which are very, very high \u2014 then there\u2019s a lot of  funding available. But it\u2019s not easy to think of something that\u2019s 10  times more effective than something that\u2019s already among the most  effective things, which is earning to give to GiveDirectly, or the EA  Long-Term Fund.</p><p>I should probably also be clear that allocation in  the community is definitely not perfect. There\u2019s going to be some good  projects that are indeed above the bar, but getting missed. That\u2019s  always a risk. The way I would suggest orienting towards this is try to  set up your life so that you can afford to fail \u2014 so make sure you have a  backup plan, things like that. And then given that, try and aim as high  as you can. The slogan would be, \u201cFind ways to limit your downsides and  then aim high.\u201d</p><p>We can apply that on an individual level, and  then I think also from the community, we can stand to be more ambitious.  There\u2019s many reasons why to be more ambitious. Partly there\u2019s the  general case that people who want to have an impact can afford to take  more risks than people who are just in it for themselves personally \u2014  those kind of old-school effective altruism arguments.</p><p>Two new  points \u2014 which I want to emphasise today \u2014 is that, firstly, I think  recent events have shown that the community can achieve really, really  big things by anyone\u2019s standards. Coming back just briefly to Sam\u2019s  story: back in 2015, there were probably only around 1,000 highly  engaged people in the community. So if one of those has become the  world\u2019s richest person under 30, that\u2019s a pretty good base rate out of  1,000 people \u2014 those are pretty good odds. And I\u2019ve seen people  succeeding ahead of what I would\u2019ve expected in other career paths as  well \u2014 many people in effective altruism are just really capable of  achieving a lot. And another interesting thing about Sam is, Sam had a  lot going for him, and has a really privileged background \u2014 his parents  are both Stanford law professors, I think. But one of his other really  notable traits is that he aimed really high. I mean, he probably took  the idea of becoming a billionaire more seriously than almost anyone  else in the community. And I think that was maybe also necessary for his  success. That\u2019s showing that maybe we could raise our sights. It is  possible.</p><p>The second point is that now there\u2019s just a lot more  funding available, so that should mean that we\u2019re able to do things that  just weren\u2019t possible five or 10 years ago. In contrast to that though,  a lot of effective altruism projects look quite similar to how they did  back in 2015. Many of them would struggle to deploy more than $10  million per year effectively. And 80,000 Hours is definitely in that  category. Our programmes are the same as before there was all this  funding. But it seems like, looking forwards, we should be thinking  about projects that are potentially a lot more scalable than our current  ones.</p><p>If the challenge is we\u2019re currently deploying $400 million a  year, but we need to get that to above $1 billion a year, either we\u2019re  going to have to massively grow the community, or we\u2019re going to have to  set up projects that can deploy a lot more funding effectively than the  current projects \u2014 like those higher bars of funding that I mentioned  earlier. People have been talking about this idea of \u2018megaprojects\u2019 \u2014  this is just defined as a project that could cost effectively deploy  $100 million a year rather than $10 million a year, which is where many  of the current projects are.</p><p>I don\u2019t know what these projects  should be. There are some very rough ideas that have been floating  around. One is something like a global screening programme to detect new  pathogens to help with biosecurity. Another is an effective  altruism\u2013inspired school or university. A third would be to step into  and try to become the biggest funders of nuclear security, because the  biggest funder, MacArthur Foundation, has recently quit that area. So  that could be another opportunity, and there\u2019s a couple of talks about  that at the conference.</p><p>That\u2019s another way of seeing the  challenge: what might these megaprojects be, and how could we get them  started? It would be really exciting if something like this could emerge  from this conference. I hope that is something to think about for the  rest of the weekend. Thank you for listening.</p>", "user": {"username": "Benjamin_Todd"}}, {"_id": "q2idqdy5Md7ktg5BF", "title": "Writing about my job: NGO Advocacy (UK context)", "postedAt": "2021-11-12T13:45:52.222Z", "htmlBody": "<p><i>I work for a London-based NGO which aims to influence the UK Government on trade policy; particularly how trade affects global development and climate change. <strong>This post is not about trade policy, but about working in advocacy more generally.</strong>&nbsp;</i></p><p><i>I used to work in the UK Parliament, which I have written about </i><a href=\"https://forum.effectivealtruism.org/posts/J4LkAy9vJmzGPJqBH/working-in-parliament-how-to-get-a-job-and-have-an-impact\"><i>here</i></a><i>. I will be moving on again soon to work for a foreign policy think tank, but I believe advocacy is a potentially high impact route which very few EAs (in my experience) go for.</i></p><h3>Contents</h3><ol><li>What is NGO advocacy?</li><li>What sort of roles are available?</li><li>What does the role involve?</li><li>Salary</li><li>How to apply</li><li>How can working in NGO advocacy have an impact?</li><li>What are the risks?</li><li>What sort of skills are involved?</li><li>What sort of experience is helpful?</li><li>What are future career prospects?</li><li>&nbsp;</li></ol><h2>1. What is NGO advocacy?</h2><p>For the purposes of this post, I mean working for a nonprofit organisation to influence government policy. 'Advocacy' roles are often distinguished from 'policy' roles: while the latter focuses on research and policy development, advocacy is about communicating with decision-makers to influence their policy.&nbsp;</p><p>This includes things like building and managing relationships with politicians and civil servants; writing briefings and other materials for these audiences; speaking at or organising events to influence policy-makers, and working with the media, among other things.</p><p><strong>Advocacy is the same as 'public affairs', 'lobbying' or 'political communications' </strong>- it's just a slightly nicer word, and more common in the nonprofit sector!&nbsp;</p><p>This post does not cover agency public affairs, where you work for a company which takes on clients (including nonprofits) to help them with their lobbying goals. I have no experience of working for an agency but my instinct is that you have greater impact by working for an NGO that is focused on an issue you care about, though I'm sure agency public affairs involves similar skills and may be better for professional development.&nbsp;</p><h2>2. What sort of roles are available?</h2><p>As mentioned above, roles are typically distinguished from policy or research teams, though in smaller NGOs the same person may end up doing both. Roles tend to be divided by seniority rather than responsibility. Examples of the titles associated with different levels are listed below, in order of highest to lowest seniority. Note that \u2018public affairs\u2019 may be replaced with \u2018political communications\u2019, \u2018parliamentary\u2019, \u2018advocacy\u2019, \u2018government relations\u2019 or similar.</p><ul><li>Head of communications (or \u2018director\u2019): a role which may include managing non-public affairs communications, such as press communications and external (nongovernmental) relations.</li><li>Head of public affairs (or \u2018director\u2019, \u2018manager\u2019): responsible for all public affairs communications, likely to manage junior roles</li><li>Senior public affairs adviser (or \u2018manager\u2019)</li><li>Public affairs officer (or \u2018executive\u2019)&nbsp;</li><li>Public affairs assistant (or \u2018executive assistant\u2019)</li><li>Public affairs intern</li></ul><p>Smaller organisations may only have one or two of these roles, and may not always use the more senior labels. For instance, the \u2018senior public affairs adviser\u2019 for a small NGO may be effectively the head of public affairs, and may not manage any junior officers.</p><h2>3. What does the role involve?</h2><p>Compared to corporate or agency lobbyists, NGOs tend to focus on political influencing and seeking policy change, rather than generally managing government relationships, though some NGOs will have more corporate-style interests in policy, compliance and government relations.&nbsp;</p><p>At more junior levels, working in advocacy involves monitoring political and legislative opportunities for impact, sending politicians suggested questions, drafting briefings and organising events in Parliament/Congress to influence legislators. For instance, a political advocacy officer at ChristianAid might read Hansard debates on international development and send briefings to MPs ahead of development questions, or organise an event at Labour Party conference on development and climate change (insert equivalent institutions for US, EU or other contexts).</p><p>At more senior levels, advocacy involves building relationships with politicians and senior civil servants to effect policy change. This might include meeting with politicians and (shadow) ministers, drafting amendments to legislation, attending civil service stakeholder meetings and speaking on panels, including giving evidence to parliamentary committees.&nbsp;</p><p>It is likely that, particularly for smaller NGOs, advocacy roles will include some press communications, cross-sector campaigning and policy work. Smaller NGOs may not have separate policy and public affairs staff, so advocacy officers may be expected to understand the policy issues in depth and produce policy research.</p><h2>4. Salary</h2><p>In the UK:&nbsp;</p><p><strong>Overall range: \u00a320,000 - \u00a380,000+</strong> (highly dependent on the size of the NGO and seniority)</p><p><strong>Typical salary: \u00a345,000</strong> (advocacy manager in a medium-sized NGO, perhaps managing one staff members)</p><p>A head of advocacy for a large NGO in the UK is unlikely to earn more than \u00a380,000. An advocacy officer at an NGO with 5 years of work experience will typically earn around \u00a335,000.</p><p>I believe the median and range are both much higher in the US, though in Europe rates are probably similar to the UK, or even less outside of the larger capitals. Corporate lobbying is unsurprisingly better paid in all countries.</p><h2>5. How to apply</h2><p>In the UK, the vast majority of entry-level and mid-level advocacy roles are advertised on the <a href=\"http://www.w4mpjobs.org/\"><u>Work4MP Jobs website</u></a>.&nbsp;</p><p>Other roles may be advertised on <a href=\"https://www.charityjob.co.uk/jobs?gclid=Cj0KCQjwpdqDBhCSARIsAEUJ0hPFM_5m_STlA3u1QhWoHP5NAgbb7s5hdnsFV7aL-qKTBdYIt1nl-KgaArhTEALw_wcB\"><u>CharityJob</u></a>, <a href=\"https://www.publicaffairsnetworking.com/public-affairs-jobs.php\"><u>Public Affairs Networking</u></a> or <a href=\"https://jobs.theguardian.com/\"><u>Guardian Jobs</u></a>. Using search terms (public affairs, advocacy, parliament, political) and filters can help identify advocacy roles from more general charity, campaigning or PR roles.</p><p>Every organisation has its own recruitment process. Smaller NGOs typically ask for a CV and cover letter, followed by an interview. Larger companies may have a more formalised recruitment process, with aptitude tests and assessment centres, though this is somewhat rare for advocacy roles.</p><h2>6. How can working in NGO advocacy have an impact?</h2><p>In short, advocacy for an NGO can have an impact by driving government policy in a better direction. An individual\u2019s impact will depend on:</p><ul><li><strong>The cause area:</strong> perhaps the most important consideration; an NGO lobbying on a low-impact area may even inadvertently cause harm by directing attention or money away from other issues. Likewise, an NGO working on a high priority, neglected and tractable issue can have a huge impact. Especially if this is the kind of issue which requires policy solutions and cannot be solely addressed by philanthropy/donations (such as AI policy or great power conflict).&nbsp;</li><li><strong>The NGO\u2019s effectiveness:</strong> some of the most influential NGOs have good relationships with key decision-makers, most often the Government (such as Oxfam or WWF). Others may be more adversarial in their approach, but draw on strong public and media support (such as Greenpeace or 38 Degrees). It is difficult to assess NGO effectiveness when it comes to lobbying, as policy change can have a wide range of causes, and having a close relationship with the Government is no guarantee of impact.</li><li><strong>Influence within the NGO:</strong> some NGOs prioritise advocacy more than others, often due to caution about political lobbying. This can affect an individual advocacy officer's influence. Note that having influence over direction might be more important if the NGO is <i>not </i>very effective, to help them prioritise better. For example, an advocacy manager may be able to get their animal welfare charity to focus on farm animals rather than pets. This is potentially more impactful if their organisation was previously ineffectively working in a high-impact area, or has the option to move to a higher impact area.</li><li><strong>Dial-shifting:</strong> even if NGO advocacy does not lead to direct policy change, there is good reason to believe that \u2018shifting the dial\u2019 or encouraging political conversation in a particular direction can have a long run impact. Many successful campaigns take decades, or even centuries, and rely on early adopters going against the status quo (for example, feminism, which now dates back centuries and has seen a progression of different wins).&nbsp;</li></ul><p>As with other roles, working in NGO advocacy can also be impactful through skill-development and exposure to important networks. This can be used to work at a more senior level, in a more impactful area, or leveraged to pursue other careers in the wider policy sector (discussed later on).</p><h2>7. What are the risks?</h2><p>Compared to other jobs in politics, the political or reputational risks of working for an NGO are generally low. Most NGOs have favourable public opinion, though an important minority may be associated with particular political ideologies. The career risks are also low, in that it is possible to go from NGO advocacy into other political and policy roles.</p><p>The biggest risk is that the role is not impactful, or even has a negative impact. This will depend very much on the factors described in answer to the previous question.</p><p>As with other roles, picking the right cause will be important. In some areas, EAs might caution against public lobbying. For example, many have identified US-China conflict as a priority area in terms of importance and neglectedness, but it is not clear how to lobby on this issue, and engagement could risk inflaming a volatile situation further.&nbsp;</p><p>Political lobbying can also have the inadvertent effect of politicising an issue, which can hinder progress if significant parties or individuals decide they are set against it. EA engagement with advocacy therefore needs to be carefully thought through, and is not simply a matter of applying lobbying pressure to existing EA cause areas.</p><h2>8. What sort of skills are involved?</h2><p>Compared to other roles in policy, strategy and communication skills are more useful than research and academic skills. The following skills are particularly useful:</p><ul><li><strong>Interpersonal skills: </strong>a large part of the job is likely to involve speaking to politicians, civil servants, the media and/or individuals in the NGO sector.&nbsp;</li><li><strong>Writing skills:</strong> particularly blogs, short briefings and emails to decision-makers. Simple English and persuasive communication is more important than in-depth policy writing.</li><li><strong>Strategic skills:</strong> particularly in terms of predicting political changes, having a sense of how policy change comes about, and what influence an individual organisation has.</li><li><strong>Presentation and public speaking skills</strong> are likely to be useful at more senior levels, as public affairs directors may end up speaking on panels, speaking to the media or giving evidence to parliamentary committees.</li><li><strong>Political knowledge and monitoring:</strong> a keen eye for political developments, potential hooks and wider political context is useful.</li></ul><h2>9. What sort of experience is helpful?</h2><p>Some NGOs, particularly larger ones, may offer internships and entry level jobs similar to private sector graduate schemes. For these roles, a university degree and some demonstration of interest (such as research area or volunteering) may be sufficient experience.&nbsp;</p><p>For mid-level and more senior roles, the following experience is helpful:</p><ul><li>Working in Parliament/Congress or for a political party, especially if this work involved focusing on the same policy area&nbsp;</li><li>Working in a think tank on a relevant policy area</li><li>Media, press or digital communications</li><li>Public Relations or advertising</li><li>Relevant academic study</li><li>Involvement with the same or similar NGOs - e.g. as a volunteer, trustee or adviser</li></ul><p>Demonstrating commitment to the cause and emotional engagement is important, especially for smaller organisations, or those working on more niche issues.</p><h2>10. What are future career prospects?</h2><p>Working in NGO advocacy can be a stepping stone towards more or less any other kind of work in policy. It is not unusual for advocacy staff to move between different cause areas, since the skills are seen as relevant and applicable. Advocacy professionals may go on to work in:</p><ul><li>Government / civil service - either in communications, or in the same policy area that they advocated on</li><li>Parliament/Congress and political parties - mostly as researchers, though there is also evidence of advocacy professionals become MPs/legislators</li><li>The media&nbsp;</li><li>Think tanks and policy research</li><li>Non advocacy charity work - including policy, management and operations</li><li>Law, especially if relevant to the policy area</li><li>Private sector public affairs or public relations</li></ul><p><i>If you are interested in working in NGO advocacy, particularly in a UK context, I'd love to chat to you - just drop me a message.</i></p>", "user": {"username": "DavidZhang"}}, {"_id": "vMEfQR3DTQCmWwbYR", "title": "[Linkpost] Apply For An ACX Grant", "postedAt": "2021-11-12T09:44:36.017Z", "htmlBody": "<p><em>This is a linkpost for: <a href=\"https://astralcodexten.substack.com/p/apply-for-an-acx-grant\">https://astralcodexten.substack.com/p/apply-for-an-acx-grant</a></em></p>\n<p>Some context: Scott Alexander writes a popular blog and wants to give away $250,000 in small grants (~$100,000 max) for EA-related projects. There's also an option to be connected with wealthier donors if your project is a particularly good fit.</p>\n<p>The link addresses overlap with EA Funds and other similar grant programs.</p>\n", "user": {"username": "AppliedDivinityStudies"}}, {"_id": "yt9SzyuEKwFmJzhrv", "title": "Penn EA Residency Takeaways", "postedAt": "2021-11-12T09:34:09.904Z", "htmlBody": "<p><i>Thanks to Zack Dugue, Oliver Habryka, Adam Krivka, and the Penn EA team for useful comments.</i></p><p>In September 2021,<a href=\"https://forum.effectivealtruism.org/users/sydv\"> <u>sydv</u></a> and I (Thomas Kwa) helped revive the University of Pennsylvania EA group through a<a href=\"https://forum.effectivealtruism.org/posts/yrSiWNypE6AMNApDi/ea-residencies-as-an-outreach-activity\"> <u>residency</u></a>. <strong>We think this went very well, and the ~300 hours invested during the 3 weeks we were there probably sped up the group by 4 months, and possibly made the group significantly better overall [3].</strong> As of November 2021, Penn EA is currently a thriving group with <a href=\"https://www.eapenn.org/about\"><u>six organizers</u></a>, ~30 Intro Program participants and ~20 weekly dinner attendees, and potential to grow further into its huge 10,000 undergrad population, whereas I'd guess the counterfactual looks something like slowly building up to this size over the course of ~8 months with significant risk of the group dying again.</p><h2>What is a residency?</h2><p>In (our model of) a residency, one or two EA community builders travel to a large university at the <a href=\"https://forum.effectivealtruism.org/posts/tXSXvqTPwgWtYWFqR/the-importance-of-optimizing-the-first-few-weeks-of-uni-for\"><u>start of the school year</u></a>, and spend at least 1 FTE building a new or existing EA student group. At the end, the group is handed off to students, and there might be a retreat for new organizers. The primary goal is to build organizer capacity to rapidly make the group large and self-sustaining (at least 2 students with 10h/week each); a secondary goal is general community-building.</p><p><strong>Penn is strong evidence that this model can work well.</strong> It's only one data point, but we think a lot of the success is generalizable. Note there were other residencies this fall that didn't go as well.</p><h2>Misc comments</h2><ul><li>Sydney (a Stanford student) and I (a Caltech student) were able to invest this much time despite being full-time students with our own groups to run because Penn starts 3 weeks earlier than Caltech or Stanford. [1]</li><li>Our time was mostly spent building the<a href=\"https://www.eapenn.org/\"> <u>website</u></a>,<a href=\"https://docs.google.com/document/d/150OsF9VgzE6NKQV-R1jEewhY8iccpfbsQFHwUQ8ctwY/edit#\"> <u>tabling</u></a> (sitting outside in a high-traffic area and advertising the club), having one-on-ones with prospective organizers and others who signed up through the website, organizing and speaking at events like intro sessions and dinners, other advertising (like flyers) and organizing a retreat (see below). Of these actions, we spent the most time <strong>tabling</strong>. But <strong>one-on-ones</strong> and the <strong>retreat</strong> were the most efficient use of time, as they build organizer capacity. The other actions are still roughly as good as standard university organizer time, given the <a href=\"https://forum.effectivealtruism.org/posts/tXSXvqTPwgWtYWFqR/the-importance-of-optimizing-the-first-few-weeks-of-uni-for\"><u>importance of the first few weeks of uni</u></a>.<ul><li><strong>Tabling</strong>: At Penn, it's really hard to send emails to the whole school. Emails are necessary to get attendance at events. We used the labor-intensive but very effective strategy of tabling to collect ~900 emails, and averaged ~20 emails and ~1 Intro Program application per hour (more on club fair days). Just as importantly, we met a couple more potential organizers. We highly recommend that group organizers read our <a href=\"https://docs.google.com/document/d/150OsF9VgzE6NKQV-R1jEewhY8iccpfbsQFHwUQ8ctwY/edit#\"><u>Guide to Tabling</u></a>, because following these practices more than doubled our tabling effectiveness.</li><li><strong>One-on-ones</strong>: Sydney did most of these; the goal was to identify potential organizers / active members and what they might contribute to the club, and also just get to know people. I think there were about 20 one-hour 1-1s.</li><li><strong>Retreat</strong>: We ran a weekend retreat for Mid-Atlantic region EA group organizers after the residency, with a total of ~30 people including ~8 from Penn. Retreats are out of scope of this post, but we think they're great and pair well with residencies; all the newly excited people you've had 1-1s with can talk to each other and learn more about EA and EA community building.</li></ul></li><li>We did expensive messaging experiments which were inconclusive (we technically started 3 different clubs, and tabled once as Penn Rationality and a couple times as <a href=\"https://www.eapenn.org/careers\"><u>Penn Impactful Careers</u></a>). In the end we stuck with neutral EA branding, roughly \"we want to help students solve the world's most pressing problems\". I think we needed to do messaging experiments, because there are probably a lot of campuses where (say) career-first messaging would do 50% better, and it's hard to read the culture of a campus without trying messaging. But maybe there's a less time-consuming way.</li><li>While at Penn we met with other uni EA groups in the area, went to EA Philadelphia events with potential organizers, and once drove to Overcoming Bias NYC. We think this was really good for networking.</li><li>We think there is room for efficiency improvement: I think I could do this at a similar university in ~25% less time given more advance planning, better prioritization, and lessons learned.</li></ul><h2>Requirements for a residency</h2><ul><li>Connections: We already knew Ashley Lin, now co-president of the club, and some other people at Penn. We met Brandon Sayler, the other co-president, in the first week. We knew that both were fairly excited about community building, and Ashley was on a gap year and had ~20 h/week to spare. This helped with administrative things (establishing the club as an official Penn student group, which requires Penn students) and handing off responsibility at the end of the residency. To get connections, you can search the <a href=\"https://eahub.org/\">EA Hub</a>, but not everyone is there, so you can also contact someone who knows hundreds of EAs worldwide, like Sydney or <a href=\"https://forum.effectivealtruism.org/users/kuhanj\">Kuhan</a>.</li><li>Funding: We spent a few thousand dollars on various supplies, including a lot on food for events and smaller amounts on advertising materials. We obtained funding by verbally confirming with CEA about the expected scope and cost of the project beforehand, and submitting a reimbursement request afterwards. It would also have been possible to get funding from the EA Infrastructure fund. Once organizers are found, <a href=\"https://www.centreforeffectivealtruism.org/community-building-grants\">CBGs</a> are a great way to increase available organizer time.</li><li>Lots of time: I think there were increasing returns to the first 100-150 hours of the residency. This is because the first week was mostly spent getting situated, and the basic infrastructure of the club (website, finding organizers) also took a lot of time, and Penn is a large school. I don't know if a residency could succeed with less than 1 FTE (full-time equivalent, 40h/week). That might be more of an advisory / campus specialist role.</li><li>The right school and situation: Penn students are on average pretty receptive to (our framing of) EA. I'm not sure exactly why this is, maybe they have an entrepreneurial mindset, haven't committed to career plans, or aren't too excited by the popular options of consulting and tech jobs. Penn EA used to be pretty big in ~2017 before going dormant due to lack of organizer time, so we knew a group could succeed. We also think a residency at a school with an extant group might be more challenging than restarting a group or building one from scratch.</li><li>Prioritization and taking initiative: There's never enough time to do all the advertising / organizing you want to. But sometimes the best thing to do is &gt;2x as good as the others and requires a lot of agency.</li><li>Infrastructure: We knew how to use Wix, Canva, Mailchimp, and Airtable. Also, we picked up best practices for starting EA groups from talking to other organizers from Stanford, many other uni groups, and uni group support people like<a href=\"https://forum.effectivealtruism.org/users/emmaabele\"> <u>Emma Abele</u></a> during a previous retreat. We continued to share residency-specific best practices during the residency with other big groups like MIT, and we expect that had there been even more support and dialogue, the residency could have gone even better.</li></ul><p><br>&nbsp;</p><h2>FAQ</h2><p><i>Why do you think Penn EA was sped up by ~4 months?</i></p><p>There are a bunch of caveats; see [3]. Sydney thinks it might be more than this, especially if we permanently added value to the club rather than just providing a speedup. Ashley should probably answer this more.</p><p><i>Could you have done this alone? Could Sydney have done this alone? Should I do a residency?</i></p><p>Thomas thinks he would have produced ~20-40% the value had he tried this alone, due to lacking various skills that Sydney, Ashley, Brandon and others had, and generally not having much community-building experience. Thomas thinks Sydney would have produced maybe 40-65% the value; but not more due to dumb problems like \"tabling is great, but the gear for tabling takes 2 people to carry and Ashley and Brandon are both busy in the mornings when it's optimal to start\".&nbsp;</p><p>&nbsp;</p><p>If you want to do a residency, at least one of the residents should meet the requirements in the \"Requirements\" section and have university community building experience. Also strongly consider meeting with <a href=\"http://calendly.com/sydv\"><u>Sydney</u></a> or <a href=\"http://calendly.com/tkwa\"><u>me</u></a>.</p><p><i>If you were to re-do the residency, what would you do differently?</i></p><ul><li>Generally optimize more for the club fair. Start tabling a couple of days before club fair to test pitches. Plan a big intro event to advertise at club fair.</li><li>Spend a bit less effort on the website, or copy someone else's website. The main purpose of the website is a button for people to join the email list. A secondary purpose is information about programs.</li><li>Learn how to do 1-1s myself, and take some of them so Sydney would be less overworked.</li></ul><p><i>If you only had, say, 100 hours, what would you do differently?</i></p><p>With 100 hours (say 1 resident for 2 weeks), the startup costs (getting oriented) and handoff costs are a larger proportion of the residency. To reduce these, I'd video call my connections at the school and make sure there are 2 organizers to ensure a smooth handoff. I'd also ask them about the culture of the school, and schedule an in-person meeting for the first day so everyone is on the same page regarding logistics, messaging, and other important things.</p><p>During the residency itself, I'd spend as much time as possible on one-on-ones and some on tabling, and skip lower-value activities like hanging flyers and expensive messaging experiments. If emails are easy to get, I put a lot of effort into intro events, which could be well-attended despite limited tabling. If time is really short, I'd invite organizers to a regional retreat planned by someone else rather than planning it myself, or find some other solution to fill the role of a retreat.</p><p><i>What do you see as the most impactful parts of a residency?</i></p><p>I think the first goal of a residency should be to <strong>build organizer capacity</strong> and enthusiasm to maximize the self-sustaining size of the future group. For us, this meant one-on-ones and a structured retreat. Maybe in other situations the structured retreat should be replaced with board meetings, on-campus organizer training, an unstructured camping trip, or something else.</p><p><i>What are the biggest challenges with residencies &amp; how might they fail?</i></p><p>Pretty uncertain about this, but the <strong>handoff</strong> seems like the biggest class of problems. Succession is already a huge problem for EA groups, and succession in 2-3 weeks adds an additional challenge. Possible failure modes:</p><ul><li><strong>No qualified club leadership</strong>: Sometimes, despite networking, residents can't find two people who are aligned, enthusiastic, have the right skills, and willing to put in 10h/week each. Needless to say, this is really bad.</li><li><strong>Club leadership resistant to residents taking over</strong>: There is an existing club, and the leadership doesn't like the residents taking over the club for several weeks. This could be because the residents and club leadership aren't aligned, or just have conflicting visions. We didn't have to deal with this because we were restarting a dormant club, but we heard this was a significant problem at other residencies.<ul><li><strong>conflicting visions</strong>: If the resident is a student from a successful uni group like Stanford/Cambridge/wherever, the local club leadership can probably learn something about the group model. But maybe the resident is importing problems with their own group, or the Stanford/Cambridge model is inappropriate for the school. This is tricky and I see no easy solution.</li></ul></li><li>Club leadership doesn't know club logistics: Someone in the club leadership should know how to use Airtable, plan meetings, interface with faculty, etc.</li><li>Club leadership has no social cohesion / hard to form an organizing team: I think there's a world where we forgot to have the organizers meet each other or decide on responsibilities, so the leadership struggles to act as a team in the absence of outside guidance.</li></ul><p>It's also important to <strong>think carefully about messaging</strong>. If we had been lazy and copied old marketing material that overemphasizes earning-to-give, students would get an inaccurate idea of EA. Also, every school has a different culture, and framings of EA ideas that are common at some schools can be controversial at other schools. I think it's good to <a href=\"https://forum.effectivealtruism.org/posts/dqFjPFHmgFEZpg8ua/what-makes-outreach-to-progressives-hard\"><u>understand progressive viewpoints</u></a> when at progressive schools. Residents should probably also talk about messaging with the club leadership, and with organizers at schools similar to the target school. That said, one can go too far and waste time dithering rather than tabling, and tabling is a great way to gather data on messaging.</p><p><i>Next target school?</i></p><p>Stanford EA organizers went to the Ivy League schools and MIT because they started late. But with enough capacity, especially among community builders who are not bound to the schedule of university students (recent alums, or students on a leave of absence), I think the residency model could also reach many large, highly-ranked public schools.</p><h2>Notes</h2><p>[1]: I almost didn't do this residency. Sydney and I were at an EA organizers retreat in Boston in August, and I heard Stanford EA organizers talking about planned residencies to various schools immediately afterwards. Sydney had decided to go to Penn, and I made the last-minute decision to join, canceling my vacation plans. We flew out to Philadelphia the next day. Sydney and I get along well (we managed to not go insane living and working together for 3 weeks), and we think this was important to the success.</p><p>[3]:&nbsp; I thought about the estimate for a couple of hours and am pretty uncertain about the exact number but am pretty sure the residency was good. The valuable things we did include&nbsp;</p><ol><li>lowering activation energy for the Penn EA team (they have to run a club, not start one)</li><li>getting Intro Program applications and emails by tabling</li><li>transmitting best practices to organizers</li><li>being motivational/inspiring, which transmits enthusiasm to organizers</li><li>connecting organizers to &nbsp;each other and other EAs in the region</li></ol><p>These are hard to quantify, but they all seem pretty necessary for a uni group to generate value. So the question is, how fast could Penn organizers have gotten there without us? Imagine instead of doing the residency, we flew back to California and just kept remotely pestering Ashley/Brandon to find more organizers, then flew back three weeks later for the retreat and tried to transmit as much best practices and enthusiasm as we could, with only ~25 hours of investment. I think in these worlds the Intro Program is smaller due to disorganization and lack of tabling, events have ~half the attendance, and many of the organizers don't come to the retreat and so don't get involved due to lack of one-on-ones. Eventually capacity gets built up, which takes maybe a year and a few retreats (possibly run by us). But this is already better than doing nothing.</p><p>The next marginal 25 hours are one-on-ones, which are just as good per hour because they increase the value of the retreat. The next best ~100 hours are tabling, which club leadership would do anyway if they had large blocks of time, and the next 150 hours include startup costs but also doing things like the website that the club needs eventually anyway and probably can't do in less than 150 hours. At 30 hours/week of club organizer time, it takes 10 weeks to do 300 hours of stuff, which is 3 months; we think we were more efficient too. Combining my vague vision of the counterfactual with this tracking of hours makes me think that 4 months is reasonable. But I should note that \"sped up by 4 months\" has a kind of optimistic expectation that the group will keep growing in the future. So maybe a more accurate wording is \"caused a semester's worth of Intro Program applications, lots of momentum, and various positive intangibles\".</p><p>Also, it's only been 1.5 months since we left. So the group could still die this year, which would make this post look very silly.&nbsp;</p>", "user": {"username": "tkwa"}}, {"_id": "ZR7C86MihL3Lw7xJW", "title": "How to decide which productivity coach to try?", "postedAt": "2021-11-12T08:04:26.974Z", "htmlBody": "<p>A list of coaches (and therapists) in the EA/rationality communities can be found <a href=\"https://docs.google.com/document/d/1q0NUPXpTOz6xygf4UMT-CsNMC187AHdwAWv55HyBodQ/edit#\">here</a>.&nbsp;</p><p>I think I should try productivity coaching, but I'm not sure which coach to try or how to decide on that. Presumably it's a matter of (1) the coaches' average quality/impact across clients and (2) the coaches' fit for me. But I don't know how to assess (1) or (2).&nbsp;</p><p>So I'm wondering:</p><ol><li>Does anyone have thoughts on who <i>I specifically</i> should get coaching from?</li><li>Are there factors I should take into account other than average quality/impact and fit for me?</li><li>How much weight should I put on average quality/impact vs fit for me?</li><li>How can I best assess average quality/impact and fit for me?<ol><li>Are there people who've had many sessions with multiple different coaches who could comment on the differences?</li><li>Many coaches (but not all) gather testimonials, feedback from clients from anonymous surveys, and other data on their apparent impact. Has anyone compared that info across coaches to see what it suggests about average quality/impact and fit for particular types of people? If not, can someone do that?</li><li>Should I just try multiple coaches and see how it goes? But I imagine it'd take several sessions with each before I got a relatively clear conclusions (which would impose a substantial time cost and - less importantly to me - perhaps financial cost). And I also expect there might be order effects.</li><li>Currently my shortlist (Lynette Bye and Daniel Kestenholz) is basically just based on who I've heard recommended most often.</li></ol></li></ol><p>I'm hoping answers could also be useful to other people considering trying coaching.</p><p>(I'm currently intending to restrict my search to just productivity coaches in the EA/rationality communities. But let me know if you think that's a mistake.)</p>", "user": {"username": "MichaelA"}}, {"_id": "Ch5NZ7LhsY3mumMzn", "title": "EA-Aligned Impact Investing: Mind Ease Case Study", "postedAt": "2021-11-15T15:57:20.191Z", "htmlBody": "<h1>Summary</h1>\n<ul>\n<li>We use the term \u201cimpact investing\u201d to refer to any investment made for the dual purpose of having a positive impact and generating a financial return</li>\n<li>Impact investing has received relatively little attention from an EA perspective</li>\n<li>We review an investment case study produced by the Total Portfolio Project (TPP) leveraging research from Lionheart Ventures, Let\u2019s Fund, and Rethink Priorities</li>\n<li>The case study uses TPP\u2019s Total Portfolio Return (TPR) <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3934090\">framework</a>, a rigorous framework which can be used to assess the value of an impact investment relative to other ways of generating impact</li>\n<li>The case study estimates that an impact investment into <a href=\"http://mindease.io/\">Mind Ease</a> averts <a href=\"https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158\">DALYs</a> at $150 per dollar invested in expectation, comparing favorably to GiveWell top charities <em>before</em> financial returns are taken into account</li>\n<li>We show how the TPR framework can be used to determine that, for a donor who is <strong>already</strong> making the best possible use of donations and investing to give, investing in Mind Ease appears better than either additional donations or more investing to give</li>\n<li>Given the lack of prior research in this area and the potential for impact investing to significantly benefit many cause areas, we recommend that the EA community direct more research and attention to this area.\n<ul>\n<li>We highlight several ways to contribute including: dialogue and movement building, evaluations, research, entrepreneurship, and shifting money towards impact investing.</li>\n</ul>\n</li>\n</ul>\n<h1>Background and Implications</h1>\n<p>Impact investing has largely been neglected by the EA community. This is perhaps due to the perception that most impact investing is relatively low impact per dollar invested. In turn, EA principles are largely neglected within impact investing. We believe there could be benefits on both sides to exploring the potential of impact investing to produce effective results.</p>\n<p>Complicating things is the fact that the term \u201cimpact investing\u201d has many possible interpretations and connotations. It generally refers to one or more strategies within a disparate <a href=\"https://hbr.org/2016/01/making-sense-of-the-many-kinds-of-impact-investing\">range of financial strategies</a> that are associated (correctly or not) with \u201cimpact,\u201d from buying (or not buying) certain stocks, to providing loans to nonprofits and disadvantaged communities, to funding early-stage startups with a specific social mission.</p>\n<p>We only want to distinguish an investment as impactful if results are generated that would not occur in expectation without the investment. Among the various interpretations of the term impact investing, we believe that only some of these approaches offer potential for impact. Thus, we will use the term \u201cimpact investing\u201d in this article to refer to any investment that is expected to have a positive impact on the world and generate financial returns. Such investments could take the form of anything from early stage startup investments to <a href=\"https://en.wikipedia.org/wiki/Environmental,_social_and_corporate_governance\">ESG-related strategies</a> in liquid stock markets (but note that ESG does not imply impact).</p>\n<p>There are many parallels between the impact investing space at present and the donation space before EA emerged. Most EAs believe the impact of charitable donations <a href=\"https://www.effectivealtruism.org/articles/prospecting-for-gold-owen-cotton-barratt/\">has</a> a heavy-tailed distribution, meaning a small number of donations/charities have a much higher impact (sometimes orders of magnitude more) than donations/charities on average. We think impact investing likely has a distribution of impact similar to or even more extreme than donations (see <a href=\"https://papers.ssrn.com/abstract=3939991\">here</a> for a technical discussion). Analogous to the early days of EA, there is no evaluator like GiveWell using rigorous evaluation techniques to find the most impactful investing opportunities.</p>\n<p>In this article, we share what we think is the first work in the EA community attempting to rigorously evaluate a specific impact investment and compare it to high impact donation options\u2014in this case, comparing an investment into the EA-aligned anti-anxiety app Mind Ease with a donation to GiveWell top charities. We find that after careful counterfactual adjustments, an investment into Mind Ease has an expected investment amount per <a href=\"https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158\">DALY</a> averted of $150 compared to a cost per DALY averted of <a href=\"https://forum.effectivealtruism.org/posts/uHskkqWkwpjnvdLmQ/an-evaluation-of-mind-ease-an-anti-anxiety-app\">$860 for GiveDirectly and $50 for AMF</a>. However, unlike with a donation, in expectation the investment will generate financial returns. As long as the investment generates a financial return that is different from that of a donation then this needs to be accounted for. To handle this, we review how the Total Portfolio Return (TPR) framework enables impact and financial returns to be considered together.</p>\n<p>While these findings are promising, we would like to emphasize the potential risks with impact investing. Firstly, if done without care, impact investing could make the world counterfactually worse (e.g. if the impact is incorrectly assessed as positive when it is negative). Secondly, we suspect investing is psychologically easier than donating, as investments hold the promise of generating financial returns. This may draw some people away from donating and towards impact investing, even when the impact case for a donation is stronger. Finally, it may ultimately be determined that there are so few highly impactful investment opportunities that further investigation into impact investing is not a good use of resources.</p>\n<p>Despite these risks, we think the potential for impact investing is sufficiently great as to warrant more attention from an EA perspective. There has been extensive EA-aligned work to assess the impact of donations one can make today, and recent work has explored the idea of <a href=\"https://forum.effectivealtruism.org/posts/CfLoq8nJBzRARohtQ/the-case-for-investing-to-give-later\">investing to give later</a>. Impact investing opens avenues for effecting change in the world that are complementary and categorically different. This can include \"differential technology\" strategies where you invest to scale up and accelerate robustly good technologies that compete against other technologies with negative impacts (or risks of negative impacts). Or engagement strategies such as using shareholder voting and securing board seats to influence corporate behavior. There also seems to be a lot of unexplored territory that may contain valuable ideas we have not thought of yet or have not properly explored.</p>\n<p>Impact investing could be useful for a wide range of value systems and across cause areas ranging from AI and biosecurity to animal welfare and global development. Not only can the direct impacts of the investment be impactful, as in this case study, but the indirect and long term effects can be important, for example, through field building. The impact could come from targeted shifts in investment behavior for a small number of people (such as the EA community) towards highly impactful ventures or from a small shift in the <a href=\"https://thegiin.org/research/publication/impinv-market-size\">$500</a> billion to <a href=\"https://www.bloomberg.com/professional/blog/esg-assets-may-hit-53-trillion-by-2025-a-third-of-global-aum/\">$31</a> trillion already allocated to responsible investing.</p>\n<p>Given (1) the potential size of the impact investing space, (2) the lack of attention it has received, (3) this case study that demonstrates the existence of a highly impactful investment, and (4) the unique and appealing characteristics of impact investing (such as corporate influence and return of capital), we think the field of impact investing is a promising area for further inquiry. If further research supports the existence of high-impact opportunities that can be identified in an efficient way, we think there is an exciting opportunity to influence a large amount of external capital and to deploy EA capital more effectively. At the end of this post, we highlight ways you can contribute to these efforts.</p>\n<h1>Mind Ease Case Study</h1>\n<h2>Acknowledgements</h2>\n<p>Brendon Wong and Will Roderick\u2019s investigation of impact investing would not have started without the <a href=\"https://forum.effectivealtruism.org/posts/z56YFpphrQDTSPLqi/what-we-learned-from-a-year-incubating-longtermist\">Longtermist Entrepreneurship Fellowship</a>, spearheaded by Jade Leung, Ben Clifford, and Rebecca Kagan. This case study would not have been possible without the efforts of the <a href=\"https://total-portfolio.org/\">Total Portfolio Project</a> (TPP), led by Jonathan Harris and Marek Duda (now at Lantern Ventures), which has been pioneering ways to apply an EA perspective to investing including impact investing. TPP\u2019s work on the case study, in turn, would not have been possible without input from <a href=\"https://lionheart.vc/\">Lionheart Ventures</a> (David Langer) who assessed Mind Ease from a business perspective and from <a href=\"https://lets-fund.org/\">Let\u2019s Fund</a> (Hauke Hillebrandt) and <a href=\"https://rethinkpriorities.org/\">Rethink Priorities</a> (Derek Foster) in assessing Mind Ease from an impact perspective. <a href=\"http://mindease.io/\">Mind Ease</a> was created by the startup studio <a href=\"http://www.sparkwave.tech/\">Spark Wave</a>. The entire analysis was reliant on engagement from the Mind Ease and Spark Wave teams, including close collaboration with Spencer Greenberg and Peter Brietbart. In addition to these stakeholders, we\u2019d like to acknowledge Aaron Gertler, Connor Dickson, and Rhys Lindmark for reviewing this article. As a disclaimer, reviewers may or may not agree with the points made in this publication. Thank you to all the partners and stakeholders involved!</p>\n<h2>Framework</h2>\n<p>TPP has developed the Total Portfolio Return <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3934090\">framework</a> (the \"Framework\") to help altruistic investors combine the financial and impact considerations related to an investment together in their decision making. For the analysis of Mind Ease, they assessed the total portfolio returns of three options: to fund the entire seed round (\"<strong>Solo\"</strong>), be the first investor in the round and encourage others to follow (\"<strong>Lead\"</strong>), or to pass on the investment (\"<strong>Pass\"</strong>).</p>\n<p>The Framework groups the value of an investment into several \"return\" categories. The main categories necessary to understand the case study are:</p>\n<ul>\n<li><strong>Financial returns</strong>: the conventional risk-adjusted, excess, expected financial returns of the investment (excess means over and above the market interest rate). These returns have altruistic value because they can be used for future giving (and impact investing). To distinguish this definition from basic financial returns, we refer to these returns as \"<strong>adjusted financial returns</strong>.\" Because of market efficiency, most investments will have zero adjusted financial returns.</li>\n<li><strong>Impact returns</strong>: the counterfactual impacts on Mind Ease's users of accelerating the company's progress with the investment. These impacts are valued in terms of the donation to a benchmark highly effective charity required to generate an equivalent amount of impact.</li>\n<li><strong>Strategic returns</strong>: the more indirect effects of investing in Mind Ease, such as forging connections in the mental health space.</li>\n</ul>\n<p>An investment's Total Portfolio Return (TPR) is then the sum of its returns in each category. In general any investment with a TPR greater than zero is worth doing. However, often an investor may only be able to make one investment among a set of alternatives. In this case, they should choose the investment with the greatest TPR.</p>\n<p>To allow for the inclusion of all potentially important considerations in the decision making process, all returns are aggregated into a decision matrix with each decision option given a column and each consideration a row. This way of transparently presenting the assessments of each consideration borrows from the field of <a href=\"https://en.m.wikipedia.org/wiki/Multiple-criteria_decision_analysis\">Multiple-Criteria Decision Analysis</a> while retaining a \"returns\" framing that is natural for investors.</p>\n<h2>TPR Framework Applied to Mind Ease</h2>\n<p>The <a href=\"https://papers.ssrn.com/abstract=3934090\">Framework</a> is careful to distinguish Enterprise Impact and Investor Impact. Enterprise Impact is defined as the counterfactual good done by the enterprise (i.e. relative to a world where Mind Ease doesn't exist). Investor Impact is the counterfactual increase in Enterprise Impact caused by the investor making the investment in Mind Ease (rather than choosing to not invest).</p>\n<p>To assess Investor Impact, an investor first needs to form an assessment of the Enterprise Impact if they invest and then compare this to an assessment of Enterprise Impact if they choose not to invest (\"Pass\"). Only after having assessed the likely Enterprise Impact in both scenarios can they assess their own impact. This is the challenge of assessing your impact with any funding opportunity. With donations it is perhaps a little easier as you could argue that what happens under \"Pass\" is simply that the charity doesn\u2019t get the money and doesn\u2019t have the impact. But with (return-generating) investments it\u2019s even more crucial to assess the counterfactual scenario.</p>\n<p>For illustrative purposes we consider two options under Pass: <strong>Pass &amp; Invest to Give</strong> and <strong>Pass &amp; Give Now</strong>.</p>\n<p>The following steps were taken to assess the TPRs for each option.</p>\n<h3>Step 1: Business Analysis and Financial Returns</h3>\n<p>TPP first considered Lionheart's assessment of the business prospects of Mind Ease. The assessment included Mind Ease\u2019s background and plans (including offering a free or discounted version of the app to low/middle income countries), historical downloads and active users, competitors, and financial details. Some strengths are that Mind Ease is operating with a revenue model that has already been validated by competitors, the team\u2019s product development is above average, the unit economics are promising, and the potential user base is large. Some identified risks are that the unit economics could be threatened by factors such as competition and increasing costs of user acquisition with scale, and the team doesn\u2019t have a lot of past experience scaling consumer applications.</p>\n<p>Based on Lionheart Ventures\u2019 analysis, the assessment considered a range of possible user growth estimates over the next eight years. These estimates go up to 1,500,000 users in an optimistic success scenario (with considerable expected variance). These estimates are used in the calculation of impact returns below.</p>\n<p><strong>Financial returns</strong></p>\n<p>In setting the adjusted financial return it is important to consider the <a href=\"https://en.m.wikipedia.org/wiki/Efficient-market_hypothesis\">efficient market hypothesis</a> (EMH). In the words of Eugene Fama, the original author of the <a href=\"https://www.jstor.org/stable/2328565\">EMH</a>: \"<em>I take the market efficiency hypothesis to be the simple statement that security prices fully reflect all available information\u2026 A weaker and economically more sensible version of the efficiency hypothesis says that prices reflect information to the point where the marginal benefits of acting on information (the profits to be made) do not exceed the marginal costs</em>.\" This hypothesis is supported by compelling intuition\u2014if prices did not accurately reflect all information, then traders would have an opportunity to exploit the discrepancy to make a profit, which would lead to prices becoming more accurate.</p>\n<p>Lionheart's view was that Mind Ease is as strong as many other mental health startups that raise venture financing. Based on this we follow the original analysis and take the view that the adjusted financial returns of the investment in Mind Ease are around zero (like most investments in the market as these are adjusted returns).<sup class=\"footnote-ref\"><a href=\"#fn-LxqLDSvs2scDrTJJr-1\" id=\"fnref-LxqLDSvs2scDrTJJr-1\">[1]</a></sup></p>\n<p>Note that an altruistic investor that is generating Investor Impact with their investment should be willing to make it even if the adjusted financial return is negative. In fact, we expect this to be the case for most impact investments, especially if pursued at scale. This doesn't necessarily mean such investments will have \"low\" returns in an absolute sense. It means they will have <em>lower</em> adjusted financial returns than a socially neutral investor would accept, because impact investors will be willing to take large, concentrated positions in the best opportunities (resulting in large risk adjustments). The absolute level of the non-risk adjusted returns could still be quite high and potentially attractive to altruistic investors with high risk appetites.</p>\n<p>We assume that the investor has optimized the rest of their portfolio before considering investing in Mind Ease. For Pass &amp; Invest to Give, assuming their main investments generate no other types of returns (e.g. strategic), the adjusted financial return will be 0%. This is because if this were not the case then the investor would want to increase the amount they invest to give, suggesting they were actually not yet optimized. For Pass &amp; Give Now the adjusted financial return is simply -100%.</p>\n<h3>Step 2: Investor Impact and Impact Returns</h3>\n<p><strong>User Impact Assessment</strong></p>\n<p>TPP first analyzed the expected impact of Mind Ease per user. Hauke's part of this analysis is <a href=\"https://forum.effectivealtruism.org/posts/uHskkqWkwpjnvdLmQ/an-evaluation-of-mind-ease-an-anti-anxiety-app\">available online</a>, with the full 76-page report <a href=\"https://docs.google.com/document/d/1Y0Mc0pI-pDMQMPg8M4F0zA1KYiXuvW5q7MPXRH9sX7k/edit?usp=sharing\">here</a>.</p>\n<p>At a high level, the assessment was informed by priors based on the standard Scale, Neglectedness, and Solvability (SNS) framework. For scale, Hauke cited a finding from the Global Burden of Disease Study indicating that over 1% of all poor health and death worldwide is caused by anxiety. For neglectedness, Hauke cited The Happier Lives Institute\u2019s findings that most people suffering from mental health issues don\u2019t get proper treatment, particularly in low/middle income countries (76%-85% don\u2019t get any treatment). For solvability, Hauke determined there is \u201crelatively good evidence that Mind Ease\u2019s interventions work [in general, not including apps], that they can work in the context of an app, and the average <a href=\"https://en.wikipedia.org/wiki/Effect_size\">effect size</a> is 0.3.\u201d</p>\n<p>The main units of the analysis were disability adjusted life years (<a href=\"https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158\">DALYs</a>).<sup class=\"footnote-ref\"><a href=\"#fn-LxqLDSvs2scDrTJJr-2\" id=\"fnref-LxqLDSvs2scDrTJJr-2\">[2]</a></sup> Hauke utilized estimates of the counterfactual GAD-7 anxiety score reduction of Mind Ease use and the number of years the effect is likely to persist to estimate that users served by Mind Ease will experience a counterfactual 0.25 DALY reduction. Derek Foster's estimate was much lower at 0.01 DALYs averted per user. This was principally because they modeled users as having less severe anxiety on average and they were much more pessimistic about the duration of the effect.</p>\n<p>Based on principles of robust decision making under uncertainty, it is important to explore how the results of the analysis vary across a range of possible weights on Foster\u2019s versus Hillebrandt\u2019s analysis. In the analysis below, we focus on the case where we weight Foster\u2019s analysis (which was more pessimistic) more highly than Hillebrandt\u2019s. This is because if Mind Ease still looks attractive in this \"worst case,\" then it would definitely look attractive with weights that are less tilted towards Foster\u2019s estimate. So, to determine a single value to use in the rest of the model, TPP tilted towards Foster's estimate as much as they thought was reasonable, resulting in a value of 0.02 DALYs per user.</p>\n<p><strong>Investor Impact</strong></p>\n<p>Investor Impact is the total counterfactual DALYs averted by making the investment. It comes from the investment's counterfactual effect on Enterprise Impact. For simplicity we analyze this in terms of the probability of Mind Ease getting its required funding (and then in expectation generating its Enterprise Impact). We use this as a proxy for a range of ways the investment could affect the enterprise. In reality, for example, the investment could increase the chance of finding success, or decrease the time required to obtain the funding, or increase the final amount raised.</p>\n<p>How can an investment affect the enterprise given the EMH and our assumption of a \"market rate\" zero adjusted financial return? First, the EMH only holds on average - it doesn't rule out exceptions. We are not claiming Mind Ease is an average investment - it could be exceptional but evaluating that is out of scope for this case study. Second, the strength of market efficiency is open to debate and early stage venture markets are definitely less efficient than the liquid stock markets that are the setting for most studies of the EMH. Third, and perhaps most importantly, the EMH really isn't the correct economic concept to be pointing to in this context. The principles behind the EMH may be relevant here, but not the EMH itself. As discussed in the above subsection on financial returns, the EMH is only a statement about pricing.</p>\n<p>What is relevant for impact is to know the number of \"good\" venture ideas that arise and would offer zero adjusted financial returns (because of the EMH), but that never get funded. These ventures could exist (and fail) because of various informational and structural frictions. One general reason to expect such attempted ventures to exist is because \"ideas are cheap\" while economic resources are scarce. There are usually more venture ideas to fund than there is capital and talent to invest in them. Thus, it is possible for Mind Ease to have an adjusted financial return of zero (from the perspective of an investor who has already found it) and still have a probability of not being funded. Based on empirical base rates <sup class=\"footnote-ref\"><a href=\"#fn-LxqLDSvs2scDrTJJr-3\" id=\"fnref-LxqLDSvs2scDrTJJr-3\">[3]</a></sup> and input from experts, TPP set the increase in funding probability (relative to Pass) at 15% and 20% for Lead and Solo, respectively, and the chance of success at 25%.</p>\n<p>TPP merged the business analysis and the per user impact assessment into a quantitative model for assessing Investor Impact. A simplified back-of-the-envelope calculation that summarizes TPP\u2019s full model proceeds as follows for the case of Lead:</p>\n<ul>\n<li>On average a successful Mind Ease has 1,000,000 users in the first 8 years.</li>\n<li>A successful Mind Ease will serve at least 4x this many users in future years.</li>\n<li>The average counterfactual benefit is 0.02 DALYs averted per user.</li>\n<li>A chance of success of 25% implies an Enterprise Impact of 22,000 DALYs.</li>\n<li>The funding probability increases then imply Investor Impact of 3,300 DALYs for Lead and 4,400 DALYs for Solo.</li>\n</ul>\n<p>The key steps above are represented in the table below.</p>\n<p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4d48a4ba12baa16c70dc3894320d6a2e7f713d3d7555ac9c.png\" alt=\"\"></p>\n<p><strong>Impact Returns</strong></p>\n<p>One way to compare Mind Ease to different investments and donations is to divide the investment amount by the DALYs averted to get an \"investment-effectiveness\" of approximately $150 invested per DALY averted. This can be compared with a $860 cost per DALY averted for GiveDirectly and $50 cost per DALY averted for AMF, which are <a href=\"https://docs.google.com/spreadsheets/d/1nEv5w9qyU6mpwdK5du3w8Eq-0_zXC4DP79X5_aPqplI/edit#gid=1070366421\">cited in Hauke\u2019s report</a>. Such a comparison indicates that an investment in Mind Ease has an \"investment-effectiveness\" of the same order of magnitude as the cost-effectiveness of a GiveWell top charity. However, this ignores the expected financial returns of the investment.</p>\n<p>To compare impact and financial returns, it is useful to define a Best Available Charitable Option (BACO). This represents the most effective way that the same impacts could be produced with a donation (e.g. AMF and other GiveWell top charities for global health). We use the most effective opportunities identified by GiveWell as a single BACO that can avert a DALY for roughly $50.</p>\n<p>Investors are comfortable working with and estimating financial returns. To utilize this comfort, Investor Impact can be compared to the additional financial return that would be necessary to produce enough additional funding for the future BACO to generate the same impact. This hypothetical \"return\" is the \"<strong>Impact Return\"</strong> = (\"Investor Impact\" * \"Future BACO cost-effectiveness\") / \"Investment Amount\".<sup class=\"footnote-ref\"><a href=\"#fn-LxqLDSvs2scDrTJJr-4\" id=\"fnref-LxqLDSvs2scDrTJJr-4\">[4]</a></sup> This values Investor Impact in terms of the most cost-effective charitable way to produce the impact in the future.</p>\n<p>At least in the case of global health, we believe future cost-effectiveness ratios are generally expected to be higher (that is, less impact per $). To be conservative in calculating impact returns, we simply use current cost-effectiveness as a lower bound. If a more detailed forecast was used, since we expect the forecast to be for lower effectiveness on average, this would imply higher impact returns for the Mind Ease investment. This reflects that if it is going to be more costly to generate impact in the future then it is more urgent to generate impact today. Given this potential urgency, we expect that our assessments of the impact returns of investing in Mind Ease are an underestimate in this regard.</p>\n<p>For Lead, the investment amount is $500,000 USD and the Investor Impact is 3,000 DALYs. It would cost $150,000 to avert this many DALYs with grants to the BACO. This implies an impact return of 30% ($150,000 / $500,000) for Lead. For Solo, the Investor Impact of 4,000 DALYs similarly implies an impact return of 13% on the associated $1.5 million investment.</p>\n<p>Note that there are compelling reasons to compare the impact of different interventions in terms of <a href=\"https://forum.effectivealtruism.org/posts/mY4pZSwvFCDsjorJX/donating-money-buying-happiness-new-meta-analyses-comparing\">subjective well-being</a> instead of DALYs. This was not done in the original Mind Ease analysis and we don\u2019t attempt it here. If we did, we suspect it would increase our assessment of Mind Ease\u2019s impact while leaving the BACO roughly the same, increasing the impact returns.</p>\n<p>Impact returns can be assessed for any investment including grants. The impact return of \"Pass &amp; Invest to Give\" is 0% as we assume the associated investments generate zero Investor Impact (the impact of the eventual giving is accounted for as financial returns). The impact return of \"Pass &amp; Give Now\" is 100% as we assume any additional donations are given to the BACO.<sup class=\"footnote-ref\"><a href=\"#fn-LxqLDSvs2scDrTJJr-5\" id=\"fnref-LxqLDSvs2scDrTJJr-5\">[5]</a></sup></p>\n<h3>Step 3: Strategic Returns and Time Costs</h3>\n<p>Other criteria that were considered were the time cost of ongoing engagement with the investment and the value of building strategic connections with other investors. Just as for the financial and impact returns, the strategic return and time cost are assessed based on the value placed on these considerations divided by the investment amount. The values of these returns were assessed intuitively based on how much the investor might value their time and be willing to pay for similar strategic connections. The values we present below for these returns are only illustrative and in this case they don't impact the ultimate decision.</p>\n<h3>Step 4: Decision Matrix</h3>\n<p>Having reviewed the assessment of each consideration, we can now combine them into a decision matrix.</p>\n<p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0cb936a0589f7ef6a95d89261627ab6b7085f0fc751c45f6.png\" alt=\"\"></p>\n<p>Based on the decision matrix, Lead is the best option. The decision is dominated by the assessed impact returns. Lead would still be the best option even if the adjusted financial return was as low as -30%. Note that while these returns may seem large, they are for the entire length of the investment (i.e. potentially 8+ years, so 30% corresponds to ~3% per year).</p>\n<p>Ultimately, the investor for whom this analysis was prepared chose Lead. Mind Ease was able to raise the remaining funds and is now fully funded and growing.</p>\n<h1>How to Contribute to This Area</h1>\n<p>Here, we list some of the ways to contribute to this space, but these are just a high-level starting point. If you have an idea for a contribution that you would like to see or do, please get in touch by <a href=\"mailto:jonathan@total-portfolio.org\">email</a>.</p>\n<ul>\n<li><em>Dialogue and movement building</em>. This could be both within EA and in the broader impact investing community, creating a movement to give more attention and perspectives to new ideas. This could look like more EA forum posts about how impact investments can play a role in the impact portfolio of EA. Another example of this is <a href=\"https://forum.effectivealtruism.org/posts/4vRdt9Z9LsmaP7dHY/the-usd100trn-opportunity-esg-investing-should-be-a-top\">Sanjay\u2019s work on shaping the field of ESG investing.</a> It could also be helpful to bring EA-aligned ideas into the public consciousness, like counterfactual impact. This could also look like giving critique and feedback on the TPR framework discussed in this post.</li>\n<li><em>Evaluations</em>. Impact evaluations could look like assessing counterfactual impact per dollar invested across a range of cause areas, investment types, or individual opportunities. The Mind Ease case study covered above, conducted by Total Portfolio Project, is an example of this type of work. In 2017, Will MacAskill recommended that a person or group should explore creating a GiveWell for impact investing, <a href=\"https://forum.effectivealtruism.org/posts/ucEgGZDYpenXLDCWR/projects-i-d-like-to-see\">\u201cwhere you could search for the best impact investing opportunities from an EA perspective.\u201d</a></li>\n<li><em>Research</em>. Separate from the applied work of evaluations, there are important research questions that require a more theoretical approach. This includes questions such as what impact we should expect from investments with different fundamental characteristics and how impact investments should fit into the EA portfolio. Total Portfolio Project's recent technical papers (<a href=\"https://papers.ssrn.com/abstract=3939991\">model</a>, <a href=\"https://papers.ssrn.com/abstract=3934090\">framework</a>) are a start in this direction that offers a foundation on which future work can build. As far as we are aware, academic quality research on impact investing from an EA perspective is very neglected.\n<ul>\n<li>Even if debates about the value of impact investing are never resolved, research in this area can still be highly valuable because it demands answers to fundamental questions in altruistic economics. As Eugene Fama wrote about the EMH: \"It is a disappointing fact that\u2026 precise inferences about the degree of market efficiency are likely to remain impossible. Nevertheless, judged on how it has improved our understanding of the behavior of security returns, research on market efficiency is among the most successful in empirical economics.\"</li>\n</ul>\n</li>\n<li><em>Entrepreneurship</em>. EA-aligned entrepreneurship through new impactful startup ventures like Mind Ease could leverage the funding in this space. These ventures could be in a range of cause areas from mental health and clean meat to biosecurity and AI.</li>\n<li><em>Shifting money towards impact investing</em>. Money could be directed towards specific investments (for informational or impact purposes), research, or evaluations within EA or in the broader impact investing community.</li>\n</ul>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-LxqLDSvs2scDrTJJr-1\" class=\"footnote-item\"><p>Financial Returns</p>\n<p>If Mind Ease was to actually receive VC funding then the EMH would imply it is a zero adjusted financial return investment for these VCs. Because VCs must earn a fee for their work, this suggests our investor would be looking at a positive adjusted financial return if they were able to invest on the same terms as the fund manager but without paying for the VC\u2019s operating costs (e.g. without paying a fee, essentially being fortunate enough to free load off the hypothetical VC). If this happened it could get viewed as monetizing the investor's reputation (or whatever other reason they were allowed to participate).</p>\n<p>The adjusted financial returns would be more likely to be zero if only angels invest. However, the returns could also be positive if the founders wanted to offer aligned money an incentive to invest. The returns could be negative if the business were to warrant a lower valuation than an average venture-backed startup but if it were not possible to negotiate the valuation down sufficiently (because to do this too much would damage the incentives of the management team).</p>\n<p>Note that the adjusted financial return ignores the costs of finding and evaluating the opportunity prior to making the investment decision. This is appropriate for this case study as it is meant as a demonstration of a tactical impact investment analysis. We believe this tactical analysis supports the case for more strategic work on impact investing, including assessing how efficiently good opportunities can be identified. For experienced philanthropic impact evaluators, we expect the marginal cost to assess the impact of investments in their area of expertise will be relatively small. Even if search costs are high now, it may be possible to bring them down over time. Detailed discussion of such strategic issues is out of scope for this case study. <a href=\"#fnref-LxqLDSvs2scDrTJJr-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-LxqLDSvs2scDrTJJr-2\" class=\"footnote-item\"><p>DALYs</p>\n<p><a href=\"https://docs.google.com/document/d/1Y0Mc0pI-pDMQMPg8M4F0zA1KYiXuvW5q7MPXRH9sX7k/edit?usp=sharing\">Hauke\u2019s report</a> provides a background on the units used to quantify the impact of Mind Ease. DALYs represent the harm inflicted on people either by a shortened lifespan (in which living 1 year shorter = 1 DALY) or by living a year in less than full health (in which living 1 year with blindness could = 0.2 DALYs depending on how the conversion is done). \u201cAverting\u201d 1 DALY is roughly equivalent to giving someone a full year of life. The efficacy of anxiety reduction apps is measured by rating scales like the GAD-7 (Generalised Anxiety Disorder Assessment) score. <a href=\"#fnref-LxqLDSvs2scDrTJJr-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-LxqLDSvs2scDrTJJr-3\" class=\"footnote-item\"><p>Startup Base Rates</p>\n<p>Our base rates for the increase in the probability of funding given angel investment (17%) and success given funding (9-27%) are guided by <a href=\"https://www.jstor.org/stable/24464820\">Kerr, Lerner and Schoar (2013)</a> who studied data from two angel investment groups from 2001-2006. They found that:</p>\n<ul>\n<li>Funded ventures had a 76% survival rate (to 2010) and 27% \"success\" rate (exit or &gt;75 employees). Unfunded ventures found other funding 26% of the time, had a 56% survival rate given funding, and had a 9% \"success\" rate.</li>\n<li>The angel groups had a positive impact on the business outcomes of their investees. Adjusted for other effects, the survival probability impact of angel investment was 22% and statistically significant, while the funding probability impact was 16% but not significant.</li>\n<li>Among the groups of ventures with medium levels of interest, the probability of securing any funding was between 55%-71%.</li>\n<li>Among all ventures that were selected to pitch, only 30% ultimately secured any funding (from the angels or other investors).</li>\n<li>Even among the ventures they liked the most (top 2% of all ventures by pitch rating), the chance of a group funding a venture was only 41%.</li>\n<li>It is extremely rare for a lot of angels to indicate interest in a venture. Most pitches generate 0 interest. This means that there is little information value in what a given investor rejects (whereas there is information in what they like).</li>\n</ul>\n <a href=\"#fnref-LxqLDSvs2scDrTJJr-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-LxqLDSvs2scDrTJJr-4\" class=\"footnote-item\"><p>Impact Returns</p>\n<p>Mathematically we are looking for the impact return such that \"Investor Impact\" = \"Investment Amount\" * \"Impact Return\" / \"Future BACO cost-effectiveness.\" Rearranged to be a formula for the Impact Return, this is: \"Impact Return\" = (\"Investor Impact\" * \"Future BACO cost-effectiveness\") / \"Investment Amount.\" Note that \"Future BACO cost-effectiveness\" has units of $/impact. <a href=\"#fnref-LxqLDSvs2scDrTJJr-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-LxqLDSvs2scDrTJJr-5\" class=\"footnote-item\"><p>Taxes</p>\n<p>We have ignored taxes in this analysis. We do this for simplicity and also because our view is that they aren\u2019t relevant for donations &amp; investments made with tax-exempt funds (e.g. inside a foundation or DAF). For donors that are subject to taxes, we expect accounting for this would decrease the impact returns by the amount it increases the adjusted financial return, resulting in no net change to the TPR. <a href=\"#fnref-LxqLDSvs2scDrTJJr-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Brendon_Wong"}}, {"_id": "L7pqaDudTipsuFEdQ", "title": "TikTok EA Book Giveaway Intermediate Results", "postedAt": "2021-11-11T23:53:39.050Z", "htmlBody": "<p>Myself and Lacey (<a href=\"https://www.tiktok.com/@benthamite\"><u>@benthamite</u></a>) and Emma Williamson (<a href=\"https://www.tiktok.com/@dailydosenews?lang=en\"><u>@dailydosenews</u></a>) piloted giving away copies of Doing Good Better on TikTok. This is a short write up of intermediate results.</p><ul><li>Executive summary<ul><li>We gave away 359 books across 5 videos</li><li>320 of those came from the most successful video</li><li>Each video took a couple of hours of work</li><li>I want to do a follow-up survey to see how many of these people actually read the book and got involved in EA. My guess is that a few of them did, which would make this a fairly cost-effective outreach strategy. I plan to publish a longer write up once those survey results are in.</li></ul></li><li>Process<ul><li>We created videos advertising various EA things, with a call to action for them to get a free book by accessing a link in our bio</li><li>People filled out a form with their contact information to receive the book. We did not require a credit card etc.</li><li>The form let them opt out of receiving a follow-up survey</li></ul></li><li>Advertising<ul><li>I tried purchasing promotion for one video. In TikTok, this means that the video was shown to more people than usual with small text indicating that it's being shown to the user via paid promotion.</li><li>As expected, this dramatically increased the number of people viewing the video, but very few of them signed up to get a book.</li><li>Probably someone should try this with a larger sample size though, and also with video which is inherently more compelling.</li><li>(I am unable to promote the most popular video as the sound is copyrighted.)</li></ul></li><li>Thoughts on the most popular video<ul><li>I filmed Sam doing a dance (as opposed to e.g. talking into the camera) because this let me decide later what text to put on top of it instead of having to decide that before filming.</li><li>I think this was the right call \u2013 my first attempt was not very successful, but the second one was.</li><li>Sam obviously has starpower but the fact that the first video was unsuccessful indicates that people aren't just watching because of his celebrity. This makes me hopeful that at least some of the people who got books did so because they were intrigued by the message.</li><li>The success of the video seems to have mostly been driven by having a higher watch percentage (as opposed to engagement through likes/comments etc.)</li></ul></li><li>Thanks to <a href=\"https://forum.effectivealtruism.org/posts/YgJXLMvvvygXaeK79/get-100s-of-ea-books-for-your-student-group\"><u>the EABD team</u></a> and the Open Philanthropy Project for making this project possible</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom;width:140px\"><strong>Video</strong></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom;width:80px\"><strong>Views</strong></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom;width:60px\"><strong>Likes</strong></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom;width:95px\"><strong>Comments</strong></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom;width:60px\"><strong>Shares</strong></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><strong>Ad Clicks</strong></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom;width:62px\"><strong>Watch %</strong></td><td style=\"border-style:solid;padding:2pt;vertical-align:bottom\"><strong>Estimated books given away</strong></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Daily dose news (hidden)</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>16</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://www.tiktok.com/@benthamite/video/6996418088946126086?is_copy_url=1&amp;is_from_webapp=v1\"><u>Replaceability</u></a></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1,543</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>136</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>5</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>5</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>21.0%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>3-10</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://www.tiktok.com/@benthamite/video/6999022959381859590?is_copy_url=1&amp;is_from_webapp=v1\"><u>Suffering per calorie</u></a></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1,090</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>69</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>9</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>17.0%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>4-7</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://www.tiktok.com/@benthamite/video/6999022959381859590?is_copy_url=1&amp;is_from_webapp=v1\"><u>suffering per calorie ($49</u></a><u> of advertising)</u></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>14,200</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>247</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>0</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>23</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">0-1</td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">Sam v1 (hidden)</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>2,580</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>84</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>18</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>7</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>31.7%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>9</p></td></tr><tr><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><a href=\"https://www.tiktok.com/@benthamite/video/7017568248652696837\"><u>Sam v2</u></a></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>125,800</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>1,652</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>182</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>390</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\">&nbsp;</td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>43.9%</p></td><td style=\"border:0.75pt solid #cccccc;padding:2pt;vertical-align:bottom\"><p>320</p></td></tr></tbody></table></figure><p><br>&nbsp;</p>", "user": {"username": "Ben_West"}}, {"_id": "MLaBz6isBTuTe2shY", "title": "Weak point in \"most important century\": lock-in", "postedAt": "2021-11-11T22:02:44.392Z", "htmlBody": "<p>This is the second of (for now) two posts covering what I see as the weakest points in the <a href=\"https://www.cold-takes.com/most-important-century/\">\u201cmost important century\u201d series</a>. (The first one is <a href=\"https://www.cold-takes.com/weak-point-in-most-important-century-full-automation/\">here</a>.)</p>\n<p>The weak point I\u2019ll cover here is the discussion of \u201clock-in\u201d: the idea that transformative AI could lead to societies that are <strong>stable for billions of years</strong>. If true, this means that how things go this century could affect what life is like in predictable, systematic ways for unfathomable amounts of time.</p>\n<p>My main coverage of this topic is in a <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/#lock-in\">section of my piece on digital people</a>. It\u2019s pretty hand-wavy, not super thorough, and isn\u2019t backed by an in-depth technical report (though I do link to <a href=\"https://jessriedel.com/index_files/Value%20Lock-in%20Notes%202021%20(Public%20version).pdf\">some informal notes</a> from physicist <a href=\"https://jessriedel.com/\">Jess Riedel</a> that he made while working at Open Philanthropy). <a href=\"https://www.overcomingbias.com/2021/07/will-tech-help-totalitarians.html\">Overcoming Bias</a> critiqued me on this point, leading to a <a href=\"https://www.overcomingbias.com/2021/07/will-tech-help-totalitarians.html\">brief exchange in the comments</a>.</p>\n<p>I\u2019m not going to be dramatically more thorough or convincing here, but I will say a bit more about how the overall \u201cmost important century\u201d argument is affected if we ignore this part of it, and a bit more about why I find \u201clock-in\u201d plausible.</p>\n<p>(Also note that \"lock-in\" will be discussed at some length in an upcoming book by Will MacAskill, <em>What We Owe the Future</em>.)</p>\n<p>Throughout this piece, I\u2019ll be using \u201clock-in\u201d to mean \u201ckey things about society, such as who is in power or which religions/ideologies are dominant, are locked into place indefinitely, plausibly for billions of years,\u201d and \u201cdynamism\u201d or \u201cinstability\u201d to mean the opposite: \u201csuch things change on much shorter time horizons, as in decades/centuries/millennia.\u201d As noted <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/#lock-in\">previously</a>, I consider \"lock-in\" to be a scary possibility by default, though it's imaginable that certain kinds of lock-in (e.g., of human rights protections) could be good.</p>\n<h2>\u201cMost important century\u201d minus \u201clock-in\u201d</h2>\n<p>First, let\u2019s just see what happens if we throw out this entire part of the argument and assume that \u201clock-in\u201d isn\u2019t a possibility at all, but accept the <a href=\"https://www.cold-takes.com/forecasting-transformative-ai-whats-the-burden-of-proof/#some-rough-probabilities\">rest of the claims</a>. In other words, we assume that:</p>\n<ul>\n<li>Something like <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\">PASTA</a> (advanced AI that automates scientific and technological advancement) is likely to be developed this century.</li>\n<li>That, in turn, would lead to <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#impacts-of-pasta\">explosive scientific and technological advancement</a>, resulting in a world run by <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">digital people</a> or <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#misaligned-ai-mysterious-potentially-dangerous-objectives\">misaligned AI</a> or something else that would make it fair to say we have \"transitioned to a state in which humans as we know them are no longer the main force in world events.\"</li>\n<li>But it would <em>not</em> lead to any particular aspect of the world being permanently set in stone. There would remain billions of years full of unpredictable developments.</li>\n</ul>\n<p>In this case, I think there is still an important sense in which this would be the \u201cmost important century for humanity\u201d: it would be our last chance to shape the transition from a world run by humans to a world run by something very much unlike humans. This is one of the two definitions of \u201cmost important century\u201d given <a href=\"https://www.cold-takes.com/some-additional-detail-on-what-i-mean-by-most-important-century/\">here</a>.</p>\n<p>More broadly, in this case, I think there\u2019s an important sense in which the <a href=\"https://www.cold-takes.com/most-important-century/\">\u201cmost important century\u201d series</a> should be thought of as \u201cPointing to a drastically underrated issue; correct in its most consequential, controversial implications, if not in every detail.\u201d When people talk about the most significant issues of our time (in fact, even when they are specifically talking about <a href=\"https://www.cold-takes.com/technological-unemployment-ai-vs-most-important-century-ai-how-far-apart/\">likely consequences of advanced AI</a>), they rarely include much discussion of the sorts of issues emphasized in this series; and they should, whether or not this series is correct about the possibility of \u201clock-in.\u201d</p>\n<p>As noted <a href=\"https://www.cold-takes.com/some-additional-detail-on-what-i-mean-by-most-important-century/\">here</a>, I ultimately care more about whether the \u201cmost important century\u201d series is correct in this sense - pointing at drastically underappreciated issues - than about how likely its title is to end up describing reality. (Though I care about both.) It\u2019s for this reason that I think the relatively thin discussion of lock-in is a less important \u201cweak point\u201d than the <a href=\"https://www.cold-takes.com/weak-point-in-most-important-century-full-automation/\">weak point I wrote about previously</a>, which raises questions about whether advanced AI would change the world very quickly or very much at all.</p>\n<p>But I\u2019ve included the mention of lock-in because I think it\u2019s a real possibility, and it would make the stakes of this century even higher.</p>\n<h2>Dissecting \u201clock-in\u201d</h2>\n<p>There have probably been many people in history (emperors, dictators) with enormous power over their society, and who would\u2019ve liked to keep things going just as they were forever. There may also have been points in time when democratically elected governments would have \u201clocked in\u201d at least some things about their society for good, if they could have.</p>\n<p>But they couldn\u2019t. Why not?</p>\n<p>I think the reasons broadly fall into a few categories, and <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">digital people</a> (or <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#misaligned-ai-mysterious-potentially-dangerous-objectives\">misaligned AI</a>, but I\u2019ll focus on digital people to keep things simple for now) could change the picture quite a bit.</p>\n<p>First I'll list factors that seem particularly susceptible to being changed by technology, then one factor that seems less so.</p>\n<h3>Factors that seem particularly susceptible to being changed by technology</h3>\n<p><strong>Aging and death.</strong> Any given powerful person has to die at some point. They can try to transfer power to children or allies, but a lot changes in the handoff (and over very long periods of time, there are a lot of handoffs).</p>\n<p>Digital people need not age or die. (More broadly, sufficient advances in science and technology seem pretty likely to be able to eliminate aging and death, even if not via digital people.) So if some particular set of them had power over some particular part of the galaxy, death and aging need not interfere here at all.</p>\n<p><strong>Other population changes.</strong> Over time, the composition of any given population changes, and in particular, one generation replaces the previous one. This tends to lead to changes in values and power dynamics.</p>\n<p>Without aging or death, and with extreme productivity, we could end up quickly exhausting the carrying capacity of any particular area - so that area might not see changes in population composition at all (or might see much smaller, more controlled changes than we are used to today - no cases where a whole generation is replaced by a new one). Generational turnover seems like quite a big driver of dynamism to date.</p>\n<p><strong>Chaos.</strong> To date, even when some government is officially \u201cin charge\u201d of a society, it has very limited ability to monitor and intervene in everything that\u2019s going on. But I think technological advancement to date has already greatly increased the ability of a government to exercise control over a large number of people and large geography. An explosion in scientific and technological advancement could radically further increase governments\u2019 in-practice control of what\u2019s going on.</p>\n<p>(Digital people provide an extreme example: controlling the server running a virtual environment would mean being able to monitor and control everything about the people in that environment. And powerful figures could create many copies of themselves for monitoring and enforcement.)</p>\n<p><strong>Natural events.</strong> All kinds of things might disrupt a human society: changes in the weather/climate, running lower on resources, etc. Sufficient advances in science and technology could drive this sort of disruption to extremely low levels (and in particular, <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">digital people</a> have pretty limited resource needs, such that they need not run low on resources for billions of years).</p>\n<p><strong>Seeking improvement.</strong> While some dictators and emperors might prefer to keep things as they are forever, most of today\u2019s governments don\u2019t tend to have this as an aspiration: elected officials see themselves as accountable to large populations whose lives they are trying to improve.</p>\n<p>But dramatic advances in science and technology would mean dramatically more control over the world, as well as potentially less scope for <em>further</em> improvement (I generally expect that the rate of improvement has to <a href=\"https://www.cold-takes.com/this-cant-go-on/\">trail off at some point</a>). This could make it increasingly likely that some government or polity decides they\u2019d prefer to lock things in as they are.</p>\n<p><strong>But could these factors be eliminated so thoroughly as to cause stability for billions of years?</strong> I think so, if enough of society were digital (e.g., <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">digital people</a>, such that those seeking stability could use digital error correction (essentially, making multiple copies of any key thing, which can be used to roll back anything that changes for any reason - for more, see <a href=\"https://jessriedel.com/index_files/Value%20Lock-in%20Notes%202021%20(Public%20version).pdf\">Jess Riedel\u2019s informal notes</a>, which argue that digital error correction could be used to reach quite extreme levels of stability).</p>\n<p>A tangible example here would be <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/#lock-in\">tightly controlled virtual environments</a>, containing <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">digital people</a>, programmed to reset entirely (or reset key properties) if any key thing changed. These represent one hypothetical way of essentially eliminating all of the above factors as sources of change.</p>\n<p>But even if we prefer to avoid thinking about such specific scenarios, I think there are broader cases for explosive scientific and technological advancement radically reducing the role of each of the above factors, as outlined above.</p>\n<p>Of course, just because some government <em>could</em> achieve \"lock-in\" doesn't mean it <em>would</em>. But over the course of a long enough time, it seems that \"anti-lock-in\" societies would simply gain ever more chances to become \"pro-lock-in\" societies, whereas even a few years of a \"pro-lock-in\" society could result in indefinite lock-in. (And in a world of <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">digital people</a> operating a lot faster than humans, a lot of \"time\" could go by by the end of this century.)</p>\n<h3>A factor that seems less susceptible to being changed by technology: competition between societies</h3>\n<p>Even if a government had complete control over its society, this wouldn\u2019t ensure stability, because it could always be attacked from outside. And <strong>unlike the above factors, this is not something that radical advances in science and technology seem particularly likely to change</strong>: in a world of digital people, different governments would still be able to attack each other, and would be able to negotiate with each other with the threat of attack in the background.</p>\n<p>This could cause sustained instability such that the world is constantly changing. This is the point emphasized by the <a href=\"https://www.overcomingbias.com/2021/07/will-tech-help-totalitarians.html\">Overcoming Bias critique</a>.</p>\n<p>I think this dynamic might - or might not - be an enduring source of dynamism. Some reasons it might not:</p>\n<ul>\n<li>If AI caused an explosion in scientific and technological advancement, then whoever develops it first could quickly become very powerful - being \u201cfirst to develop <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\">PASTA</a> by a few months\u201d could effectively mean developing the equivalent of a several-centuries lead in science and technology after that. This could lead to consolidation of power on Earth, and there are <a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\">no signs of intelligent life outside Earth</a> - so that could be the end of \u201cattack\u201d dynamics as a force for instability.</li>\n<li>Awareness of the above risk might cause the major powers to explicitly <a href=\"https://www.cold-takes.com/making-the-best-of-the-most-important-century/#second-best-negotiation-and-governance\">negotiate</a> and divide up the galaxy, committing (perhaps enforceably, depending on how the technological picture shakes out) never to encroach each others\u2019 territory. In this case, any particular part of the galaxy would not be subject to attacks.</li>\n<li>It might turn out that space settlements are generally easier to defend than attack, such that once someone establishes one, it is essentially not subject to attack.</li>\n</ul>\n<p>Any of the above, or a combination (e.g., attacks are possible but risky and costly; world powers choose not to attack each other in order not to set off a war), could lead to the permanent disappearance of military competition as a factor, and open up the possibility for some governments to \u201clock in\u201d key characteristics of their societies.</p>\n<h2>Three categories of long-run future</h2>\n<p>Above, I\u2019ve listed some factors that may - or may not - continue to be sources of dynamism even after explosive scientific and technological advancement. I think I have started to give a sense for why, at a minimum, sources of dynamism could be greatly <em>reduced</em> in the case of digital people or other radically advanced technology, compared to today.</p>\n<p>Now I want to divide the different possible futures into three broad categories:</p>\n<p><strong>Full discretionary lock-in.</strong> This is where a given government (or coalition or negotiated setup) is able to essentially lock in whatever properties it chooses for its society, indefinitely.</p>\n<p>This could happen if essentially every source of dynamism outlined above goes away, and governments choose to pursue lock-in.</p>\n<p><strong>Predictable competitive dynamics.</strong> I think the source of dynamism that is most likely to persist (in a world of <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\">digital people</a> or comparably advanced science and technology) is the last one discussed in the above section: military competition between advanced societies.</p>\n<p>However, I think it could persist in a way that makes the <strong>long-run outcomes importantly predictable.</strong> In fact, I think \u201cimportantly predictable long-run outcomes\u201d is part of the vision implied by the <a href=\"https://www.overcomingbias.com/2021/07/will-tech-help-totalitarians.html\">Overcoming Bias critique</a>, which argues that the world will need to be near-exclusively populated by beings that spend nearly their entire existence working (since the population will expand to the point that it\u2019s necessary to work constantly just to survive).</p>\n<p>If we end up with a world full of digital beings that have full control over their environment <em>except for</em> having to deal with military competition from others, we might expect that there will be strong pressures for the digital beings that are most ambitious, most productive, hardest-working, most aggressive, etc. to end up populating most of the galaxy. These may be beings that do little else but strive for resources.</p>\n<p><strong>True dynamism.</strong> Rather than a world where governments lock in whatever properties they (and/or majorities of their constituents) want, or a world where digital beings compete with largely predictable consequences, we could end up with a world in which there is true freedom and dynamism - perhaps deliberately preserved via putting specific measures in place to stop the above two possibilities, and enforce some level of diversity and even randomness.</p>\n<p>Having listed these possibilities, I want to raise the hypothesis that <strong>if we could end up with any of these three, and this century determines which (or which mix) we end up with, that makes a pretty good case for this century having especially noteworthy impacts, and thereby being the most important century of all time for intelligent life.</strong></p>\n<p>For example, say that from today\u2019s vantage point, we\u2019re equally likely to get (a) a world where powerful governments employ \u201clock-in,\u201d (b) a world where unfettered competition leads the galaxy to be dominated by the strong/productive/aggressive, or (c) a truly dynamic world where future events are unpredictable and important. In that case, if we end up with (c), and future events end up being enormously interesting and consequential, I would think that there would still be an important sense in which the most important development of all time was the <em>establishment of that very dynamic</em>. (Given that one of the other two could have instead ended up determining the shape of civilization across the galaxy over the long run.)</p>\n<p>Another way of putting this: if lock-in (and/or predictably competitive dynamics) is a serious possibility starting this century, the opportunity to <em>prevent</em> it could make this century the most important one.</p>\n<h2>Boiling it down</h2>\n<p>This has been a lot of detail about radically unfamiliar futures, and readers may have the sense at this point that things have gotten too specific and complex to put much stock in. But I think the broad intuitions here are fairly simple and solid, so I\u2019m going to give a more high-level summary:</p>\n<ul>\n<li>Scientific and technological advancement can reduce or eliminate many of today\u2019s sources of instability, from aging and death to chaos and natural events. An explosion in scientific and technological advancement could therefore lead to a big drop in dynamism. (And as one vivid example, digital people could set up tightly controlled virtual environments with very robust error correction - something I consider a scary possibility by default, as noted in the intro.)</li>\n<li>Dynamism may or may not remain, depending on a number of factors about how consolidated power ends up being and how different governments/societies deal with each other. The \u201cmay or may not\u201d could be determined this century.</li>\n<li>I think this is a serious enough possibility that it heightens the stakes of the \u201cmost important century,\u201d but I\u2019m far from confident in the thinking here, and I think most of the spirit of the \u201cmost important century\u201d hypothesis survives even if we forget about all of it.</li>\n</ul>\n<p>Hopefully these additional thoughts have been helpful context on where I\u2019m coming from, but I continue to acknowledge that this is one of the more under-developed parts of the series, and I\u2019m interested in further exploration of the topic.</p>\n", "user": {"username": "HoldenKarnofsky"}}, {"_id": "rqo9AswmH7KRF77ko", "title": "Improve delegation abilities today, delegate heavily tomorrow", "postedAt": "2021-11-11T21:52:18.782Z", "htmlBody": "<p><i>TLDR: Eventually, we want higher and higher levels of worthwhile delegation. This will take work.</i></p><p><i>Rigor: Very quickly written rant. I want to experiment with getting more pieces like these out there, as opposed to focusing more on polish. Feedback is appreciated.</i></p><p>&nbsp;</p><p>Example 1: Bricks</p><ol><li>Low delegation: You assemble bricks for your house</li><li>Medium: You hire a team to put together your house</li><li>High: You tell an assistant that you want a house. They go about choosing a plan and selecting a team.</li><li>Very high: You tell an assistant to \u201cmake your life better\u201d, they decide that a house is the best approach, and then they go about figuring it out.</li></ol><p>Example 2: Charity</p><ol><li>Low delegation: You give food to a homeless person</li><li>Medium: You donate to a nonprofit feeding the homeless</li><li>High: You donate to a nonprofit fund that re-grants to nonprofits that help the homeless</li><li>Very high: You donate to a fund to \u201cdo useful stuff\u201d, and they sometimes decide that helping the homeless is ideal.</li></ol><p>If delegation can be well managed, it\u2019s incredibly useful. There\u2019s always more and more stuff to do and be concerned about. We clearly want to hand off as much work as possible to others and then for those tasks to achieve economies of scale.</p><p>Those who can delegate well, often win. This is exactly the job of CEOs/entrepreneurs/managers. Companies with great teams that can be given bold projects with little oversight, or nations with very low political corruption or infighting.</p><p>Right now in the US, we don't trust the government much, so it's relegated to a low delegation level. This means it just simply can't do that much.</p><p>The higher level you delegate, the more agency you (typically) sacrifice. When you choose a house construction crew, you\u2019ll have a bit less control over your brick types. When you choose some highly-meta-nonprofit-fund, you give them incredibly broad authority to handle your money.</p><p>Typically delegation breaks down somewhere between levels 1-4. Sometimes this is due to undertrust; you expect the authority will be corrupt or poor, even if they won\u2019t be. Sometimes it\u2019s overtrust; you mistakenly expect things to go well, then they abuse or misuse the agency.</p><p>To work well:</p><ol><li>There needs to be significant amounts of <i>justified trust </i>between client and provider</li><li>The costs of delegation must be lower than the benefits</li></ol><p>Most altruists seem to be in either the \u201cmedium\u201d levels, or the unjustified \u201cvery high levels\u201d (for example, donating to a single politician or cult to fix all of one\u2019s problems). Some are in the low levels and just go out and try to help people personally (with widely mixed results).&nbsp;</p><p>On one hand, you have literal cult leaders, and on the other, you have millionaire altruists literally laying bricks to build houses.&nbsp;</p><p>Right now in EA, we have some \u201chigh\u201d levels of funding delegation (EA Funds, GiveWell), but it's limited.</p><p>There\u2019s a ton of value at hand on moving towards more delegation and increasing the amount of justified trust. \u201cEveryone trying to do all of the necessary analysis on all things\u201d themselves clearly doesn\u2019t scale, and it should be clear that very few people are strong at this.</p><p>I don\u2019t see that much literature at this level of abstraction, but I often see arguments like,</p><blockquote><p>\u201cWe can\u2019t trust the authorities\u201d</p></blockquote><blockquote><p>\u201cIt\u2019s important to think for yourself\u201d</p></blockquote><p>I get that, but it seems like it\u2019s arguing on the wrong axis. We really want to push forward the Pareto frontier of delegation potential. That\u2019s much more important, in the long run, than small decisions of where on that frontier we want to be right now. The frontier right now is bad and I think we could do much better.</p>", "user": {"username": "oagr"}}, {"_id": "7xPhfta5CrSKrKGne", "title": "Isaac Asimov: The Last Question", "postedAt": "2021-11-11T17:00:41.018Z", "htmlBody": "<p>In the spirit of <a href=\"https://forum.effectivealtruism.org/posts/XTLFZiqCS5Jai4mQN/cgp-grey-the-fable-of-the-dragon-tyrant\">this post</a><br><br>This is the only story I know of that is both transhumanist and spiritual. We read it at the rationalist solstice in the Netherlands. One of my all-time favorites. Hope you enjoy it</p>", "user": {"username": "toonalfrink"}}, {"_id": "cf7oECpkmzaLFRM5z", "title": "Why EAs should become Accredited / Experienced Investors (anecdotal evidence)", "postedAt": "2021-11-11T15:28:31.603Z", "htmlBody": "<h1><strong>Key Insight</strong></h1><p>Becoming an Accredited / Experienced Investor unlocks access to high value investing, networking and startup formation opportunities.</p><h1><strong>Anecdotal Evidence</strong></h1><p>I went to the <a href=\"https://www.angelinvestorsontario.ca/\">Angel Investors Ontario</a> meeting yesterday and invited 5 founders to attend as guests. Only one was from EA; 2 were from <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:6864416625576206336/\">Complexity Weekend</a>. I introduced them to angels, VCs, potential co-founders, customers, someone who works for the Ontario Premier and someone who has met Obama multiple times.<br><br>I gained access to the event as a Member of <a href=\"https://mapleleafangels.com/\">Maple Leaf Angels</a> (MLA). I joined MLA earlier this year when I <a href=\"https://forum.effectivealtruism.org/posts/FmvSqXXinQjExt69J/how-eas-can-become-accredited-experienced-investors\">qualified as an Accredited Investor</a>.</p><h2>Founders</h2><p>The event was very valuable for me personally since I made lots of really great connections AND valuable for the founders I invited. One of them (non-EA) wrote:</p><blockquote><p>Thanks so much for the opportunity to attend the Angel Investment Forum today. What an amazing event it was! I learned a lot and met so many wonderful people.&nbsp;</p></blockquote><h2>President of Angel Investors Ontario (AIO)</h2><p>At the event, the President of Angel Investors Ontario (AIO) interviewed the CEO of Sanofi (they are building a new facility in Ontario!). He also interviewed the Conservative Finance Minister for Ontario. At the evening reception, the President of AIO found me and introduced me to a Toronto-based founder building an incredible <a href=\"https://en.wikipedia.org/wiki/Property_technology\">PropTech </a>startup (I think the President overheard me talking about PropTech to one of my Angel friends from MLA). The prop-tech founder was really impressive! I pitched him on taking me on in a sales capacity for $0 salary and a pure equity stake. He and I are still in discussions about it...</p><h2>Chair of the Board for&nbsp;Angel Investors Ontario (AIO)</h2><p>I also met the Chair of the Board for Angel Investors Ontario. My contact from <a href=\"https://www.complexityweekend.com/\">Complexity Weekend</a> and I pitched him on running hybrid in-person/online events in Gather Town. We'll see what comes of that...</p><h2>Rockstar</h2><p>Notable at the event -- the Liberal Federal Finance Minister / Deputy Prime Minister presented -- she is an absolute Rockstar! (I say that as someone who is about to renew my Conservative Party membership...)</p><h1><strong>Summary</strong></h1><p>Thanks to the opportunities unlocked by becoming an Accredited Investor, I was able to gain access to high value networking opportunities for myself and close, trusted associates. When they say <strong>the rich get richer</strong>, it's worth <a href=\"https://forum.effectivealtruism.org/posts/FmvSqXXinQjExt69J/how-eas-can-become-accredited-experienced-investors\">paying attention to how</a>!</p><h2><strong>Caveat</strong></h2><p>Most of the value from the event was introducing my Toronto <a href=\"https://en.wikipedia.org/wiki/Posse\">Posse </a>of Founders to each other and other event-goers. Possibly due to Covid, the tickets were free and fewer people attended than a normal year. Next year, I might not be able to invite 5 founders for free.</p>", "user": {"username": "--alex--"}}, {"_id": "FmvSqXXinQjExt69J", "title": "How EAs can become Accredited / Experienced Investors (practical tips)", "postedAt": "2021-11-11T14:00:35.982Z", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/cf7oECpkmzaLFRM5z/why-eas-should-become-accredited-experienced-investors\"><strong>Motivation</strong></a><strong>:</strong> Accredited / Experienced Investors gain access to high value investing, networking and startup formation opportunities.</p><h2>Criteria</h2><p>Exact details vary by country, but in general there are three criteria to qualify as an Accredited / Experienced Investor:</p><ol><li>Income</li><li>Assets</li><li>Financial Knowledge</li></ol><p>If you meet ANY of the three criteria, you qualify as an Accredited / Experienced Investor.</p><h2>Canadian Example</h2><p>In Ontario, Canada the criteria are roughly:</p><ol><li>Income (CAD) - over $200,000/year (individual) or over $300,000/year (with spouse)</li><li>Assets (CAD) - over $5,000,000 in total assets net liabilities including primary residence or over $1,000,000 in financial assets net liabilities (excludes primary residence)</li><li>Financial Knowledge - CFA Level 3 or equivalent (may have changed recently)</li></ol><p>Please take these as a rough guideline only and check with a lawyer/financial advisor for exact details as they apply to your specific circumstances. If you find more details for your specific country/region or some interesting cypto-currency exceptions, please provide in comments.</p><h1><strong>$1M Question: HOW do I get there?</strong></h1><ol><li>Income - <a href=\"https://80000hours.org/\">80,000 hours</a> has great content. Also, one awesome EA is offering <a href=\"https://forum.effectivealtruism.org/posts/FkWHn6WaFGzrzqb9P/i-m-offering-free-coaching-for-software-developers-in-the-ea\">Free Coaching for Software Developers in EA!</a> Maybe other EAs will soon offer free mentoring/coaching for valuable skills too!!</li><li>Assets - save, invest and join <a href=\"https://join.slack.com/t/eaentrepreneursgroup/shared_invite/zt-ybbzsfja-nE5_MWw0~ewh_k07Hjzn3Q\">EA Entrepreneurs</a></li><li>Financial Knowledge - join <a href=\"https://www.eafinance.org/\">EA Finance</a> and offer to create a CFA Exam Study Group</li></ol><p><strong>Note: </strong>please have a low-bar for commenting, especially if you have any of your own tips, offers to help, openness to <a href=\"https://www.linkedin.com/in/alex-barnes-toronto/\">connecting by LinkedIn</a>, etc&nbsp;</p>", "user": {"username": "--alex--"}}, {"_id": "XTLFZiqCS5Jai4mQN", "title": "CGP Grey: The Fable of the Dragon-Tyrant", "postedAt": "2021-11-11T09:01:49.213Z", "htmlBody": "<p><a href=\"https://www.youtube.com/watch?v=cZYNADOHhVY\">CGP Grey's animation</a> of <a href=\"https://www.nickbostrom.com/fable/dragon.pdf\">The Fable of the Dragon-Tyrant</a> is probably the most moving piece of EA content I know of, by some margin.<sup class=\"footnote-ref\"><a href=\"#fn-ppb6uwrFd2uqsqrsL-1\" id=\"fnref-ppb6uwrFd2uqsqrsL-1\">[1]</a></sup></p>\n<p>I'm linkposting it because (1) I've been surprised by the number of people who haven't seen it, and (2) I think it could be really cool if we had more content like this.</p>\n<p>I'd also appreciate any comments linking to moving EA content I might have missed!</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-ppb6uwrFd2uqsqrsL-1\" class=\"footnote-item\"><p>At least, if you account for length. I found <a href=\"https://archiveofourown.org/series/936480\">this fiction</a> more moving, but it's over 1.5 million words, so not really a fair comparison :) <a href=\"#fnref-ppb6uwrFd2uqsqrsL-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "SamClarke"}}, {"_id": "tixykMyAuHFMCMp9G", "title": "How Do We Make Nuclear Energy Tractable?", "postedAt": "2021-11-11T04:48:38.536Z", "htmlBody": "<p>Countless people have for decades claimed generating more nuclear energy is one of the most important and effective interventions to mitigate climate change. Nobody asks the question of how to make developing nuclear energy more tractable. An associated question would be: what are the reasons why generating more nuclear energy has been so intractable?</p>\n", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "bGBm2yTiLEwwCbL6w", "title": "Discussion with Eliezer Yudkowsky on AGI interventions", "postedAt": "2021-11-11T03:21:50.685Z", "htmlBody": "<p>The<strong> </strong>following is a partially redacted and lightly edited transcript of a chat conversation about AGI between Eliezer Yudkowsky and a set of invitees in early September 2021. By default, all other participants are anonymized as \"Anonymous\".</p><p>I think this Nate Soares quote (excerpted from Nate's <a href=\"https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential\">response to a report by Joe Carlsmith</a>) is a useful context-setting preface regarding timelines, which weren't discussed as much in the chat transcript:</p><blockquote><p>[...] My odds [of AGI by the year 2070] are around 85%[...]</p><p>I can list a handful of things that drive my probability of AGI-in-the-next-49-years above 80%:</p><p>1. 50 years ago was 1970. The gap between AI systems then and AI systems now seems pretty plausibly greater than the remaining gap, even before accounting the recent dramatic increase in the rate of progress, and potential future increases in rate-of-progress as it starts to feel within-grasp.</p><p>2. I observe that, 15 years ago, everyone was saying AGI is far off because of what it couldn't do -- basic image recognition, go, starcraft, winograd schemas, programmer assistance. But basically all that has fallen. The gap between us and AGI is made mostly of intangibles. (Computer Programming That Is Actually Good? Theorem proving? Sure, but on my model, \"good\" versions of those are a hair's breadth away from full AGI already. And the fact that I need to clarify that \"bad\" versions don't count, speaks to my point that the only barriers people can name right now are intangibles.) That's a very uncomfortable place to be!</p><p>3. When I look at the history of invention, and the various anecdotes about the Wright brothers and Enrico Fermi, I get an impression that, when a technology is pretty close, the world looks a lot like how our world looks.</p><ul><li>Of course, the trick is that when a technology is a little far, the world might also look pretty similar!</li><li>Though when a technology is <strong>very</strong> far, the world <strong>does</strong> look different -- it looks like experts pointing to specific technical hurdles. We exited that regime a few years ago.</li></ul><p>4. Summarizing the above two points, I suspect that I'm in more-or-less the \"penultimate epistemic state\" on AGI timelines: I don't know of a project that seems like they're right on the brink; that would put me in the \"final epistemic state\" of thinking AGI is imminent. But I'm in the second-to-last epistemic state, where I wouldn't feel all that shocked to learn that some group has reached the brink. Maybe I won't get that call for 10 years! Or 20! But it could also be 2, and I wouldn't get to be indignant with reality. I wouldn't get to say \"but all the following things should have happened first, before I made that observation\". I have made those observations.</p><p>5. It seems to me that the Cotra-style compute-based model provides pretty conservative estimates. For one thing, I don't expect to need human-level compute to get human-level intelligence, and for another I think there's a decent chance that insight and innovation have a big role to play, especially on 50 year timescales.</p><p>6. There has been a lot of AI progress recently. When I tried to adjust my beliefs so that I was <strong>positively</strong> surprised by AI progress just about as often as I was <strong>negatively</strong> surprised by AI progress, I ended up expecting a bunch of rapid progress. [...]</p></blockquote><p>&nbsp;</p><p><strong>Further preface by Eliezer:</strong>&nbsp;</p><p>In some sections here, I sound gloomy about the probability that coordination between AGI groups succeeds in saving the world.&nbsp; Andrew Critch reminds me to point out that gloominess like this can be a self-fulfilling prophecy - if people think successful coordination is impossible, they won\u2019t try to coordinate.&nbsp; I therefore remark in retrospective advance that it seems to me like at least some of the top AGI people, say at Deepmind and Anthropic, are the sorts who I think would rather coordinate than destroy the world; my gloominess is about what happens when the the technology has propagated further than that.&nbsp; But even then, anybody who would <i>rather</i> coordinate and <i>not</i> destroy the world shouldn\u2019t rule out hooking up with Demis, or whoever else is in front if that person also seems to prefer not to completely destroy the world.&nbsp; (Don\u2019t be too picky here.)&nbsp; Even if the technology proliferates and the world ends a year later when other non-coordinating parties jump in, it\u2019s still better to take the route where the world ends one year later instead of immediately.&nbsp; Maybe the horse will sing.</p><hr><p>&nbsp;<strong>Eliezer Yudkowsky</strong>&nbsp;</p><p>Hi and welcome. Points to keep in mind:&nbsp;</p><p>- I'm doing this because I would like to learn whichever <i>actual</i> thoughts this target group may have, and perhaps respond to those; that's part of the point of anonymity. If you speak an anonymous thought, please have that be your actual thought that you are thinking yourself, not something where you're thinking \"well, somebody else might think that...\" or \"I wonder what Eliezer's response would be to...\"</p><p>- Eliezer's responses are uncloaked by default. Everyone else's responses are anonymous (not pseudonymous) and neither I nor MIRI will know which potential invitee sent them.</p><p>- Please do not reshare or pass on the link you used to get here.</p><p>- I do intend that parts of this conversation may be saved and published at MIRI's discretion, though not with any mention of who the anonymous speakers could possibly have been.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky</strong>&nbsp;</p><p>(Thank you to Ben Weinstein-Raun for building <a href=\"https://www.chathamroom.com/\">chathamroom.com</a>, and for quickly adding some features to it at my request.)</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky</strong>&nbsp;</p><p>It is now 2PM; this room is now open for questions.</p><p>&nbsp;</p><p><strong>Anonymous</strong>&nbsp;</p><p>How long will it be open for?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky</strong>&nbsp;</p><p>In principle, I could always stop by a couple of days later and answer any unanswered questions, but my basic theory had been \"until I got tired\".</p><hr><p><strong>Anonymous</strong>&nbsp;</p><p>At a high level one thing I want to ask about is research directions and prioritization. For example, if you were dictator for what researchers here (or within our influence) were working on, how would you reallocate them?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky</strong>&nbsp;</p><p>The first reply that came to mind is \"I don't know.\" I consider the present gameboard to look incredibly grim, and I don't actually see a way out through hard work alone. We can hope there's a miracle that violates some aspect of my background model, and we can try to prepare for that unknown miracle; preparing for an unknown miracle probably looks like \"Trying to die with more dignity on the mainline\" (because if you can die with more dignity on the mainline, you are better positioned to take advantage of a miracle if it occurs).</p><p>&nbsp;</p><p><strong>Anonymous</strong>&nbsp;</p><p>I'm curious if the grim outlook is currently mainly due to technical difficulties or social/coordination difficulties. (Both avenues might have solutions, but maybe one seems more recalcitrant than the other?)</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky</strong>&nbsp;</p><p>Technical difficulties. Even if the social situation were vastly improved, on my read of things, everybody still dies because there is nothing that a handful of socially coordinated projects can do, or even a handful of major governments who aren't willing to start nuclear wars over things, to prevent somebody else from building AGI and killing everyone 3 months or 2 years later. There's no obvious winnable position into which to play the board.</p><p>&nbsp;</p><p><strong>Anonymous</strong>&nbsp;</p><p>just to clarify, that sounds like a large scale coordination difficulty to me (i.e., we - as all of humanity - can't coordinate to not build that AGI).</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky</strong>&nbsp;</p><p>I wasn't really considering the counterfactual where humanity had a collective telepathic hivemind? I mean, I've written fiction about a world coordinated enough that they managed to shut down all progress in their computing industry and only manufacture powerful computers in a single worldwide hidden base, but Earth was never going to go down that route. Relative to remotely plausible levels of future coordination, we have a technical problem.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong></p><p>Curious about why building an AGI aligned to its users' interests isn't a thing a handful of coordinated projects could do that would effectively prevent the catastrophe. The two obvious options are: it's too hard to build it vs it wouldn't stop the other group anyway. For \"it wouldn't stop them\", two lines of reply are nobody actually wants an unaligned AGI (they just don't foresee the consequences and are pursuing the benefits from automated intelligence, so can be defused by providing the latter) (maybe not entirely true: omnicidal maniacs), and an aligned AGI could help in stopping them. Is your take more on the \"too hard to build\" side?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Because it's too technically hard to align some cognitive process that is powerful enough, and operating in a sufficiently dangerous domain, to stop the next group from building an unaligned AGI in 3 months or 2 years. Like, they can't coordinate to build an AGI that builds a nanosystem because it is too technically hard to align their AGI technology in the 2 years before the world ends.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Summarizing the threat model here (correct if wrong): The nearest competitor for building an AGI is at most N (&lt;2) years behind, and building an aligned AGI, even when starting with the ability to build an unaligned AGI, takes longer than N years. So at some point some competitor who doesn't care about safety builds the unaligned AGI. How does \"nobody actually wants an unaligned AGI\" fail here? It takes &gt;N years to get everyone to realise that they have that preference and that it's incompatible with their actions?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Many of the current actors seem like they'd be really gung-ho to build an \"unaligned\" AGI because they think it'd be super neat, or they think it'd be super profitable, and they don't expect it to destroy the world. So if this happens in anything like the current world - and I neither expect vast improvements, nor have very long timelines - then we'd see Deepmind get it first; and, if the code was not <i>immediately</i> stolen and rerun with higher bounds on the for loops, by China or France or whoever, somebody else would get it in another year; if that somebody else was Anthropic, I could maybe see them also not amping up their AGI; but then in 2 years it starts to go to Facebook AI Research and home hobbyists and intelligence agencies stealing copies of the code from other intelligence agencies and I don't see how the world fails to end past that point.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>What does trying to die with more dignity on the mainline look like? There's a real question of prioritisation here between solving the alignment problem (and various approaches within that), and preventing or slowing down the next competitor. I'd personally love more direction on where to focus my efforts (obviously you can only say things generic to the group).</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I don't know how to effectively prevent or slow down the \"next competitor\" for more than a couple of years even in plausible-best-case scenarios. Maybe some of the natsec people can be grownups in the room and explain why \"stealing AGI code and running it\" is as bad as \"full nuclear launch\" to their foreign counterparts in a realistic way. Maybe more current AGI groups can be persuaded to go closed; or, if more than one has an AGI, to coordinate with each other and not rush into an arms race. I'm not sure I believe these things can be done in real life, but it seems understandable to me how I'd go about trying - though, please do talk with me a lot more before trying anything like this, because it's easy for me to see how attempts could backfire, it's not clear to me that we should be inviting more attention from natsec folks at all. None of that saves us without technical alignment progress. But what are other people supposed to do about researching alignment when I'm not sure what to try there myself?</p><p><br>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>thanks! on researching alignment, you might have better meta ideas (how to do research generally) even if you're also stuck on object level. and you might know/foresee dead ends that others don't.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I definitely foresee a whole lot of dead ends that others don't, yes.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Does pushing for a lot of public fear about this kind of research, that makes all projects hard, seem hopeless?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>What does it buy us? 3 months of delay at the cost of a tremendous amount of goodwill? 2 years of delay? What's that delay for, if we all die at the end? Even if we then got a technical miracle, would it end up impossible to run a project that could make use of an alignment miracle, because everybody was afraid of that project? Wouldn't that fear tend to be channeled into \"ah, yes, it must be a government project, they're the good guys\" and then the government is much more hopeless and much harder to improve upon than Deepmind?</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>I imagine lack of public support for genetic manipulation of humans has slowed that research by more than three months</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>'would it end up impossible to run a project that could make use of an alignment miracle, because everybody was afraid of that project?'</p><p>...like, maybe, but not with near 100% chance?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I don't want to sound like I'm dismissing the whole strategy, but it sounds a <i>lot</i> like the kind of thing that backfires because you did not get <i>exactly</i> the public reaction you wanted, and the public reaction you actually got was bad; and it doesn't sound like that whole strategy actually has a visualized victorious endgame, which makes it hard to work out what the exact strategy should be; it seems more like the kind of thing that falls under the syllogism \"something must be done, this is something, therefore this must be done\" than like a plan that ends with humane life victorious.</p><p>Regarding genetic manipulation of humans, I think the public started out very unfavorable to that, had a reaction that was not at all exact or channeled, does not allow for any 'good' forms of human genetic manipulation regardless of circumstances, driving the science into other countries - it is not a case in point of the intelligentsia being able to successfully cunningly manipulate the fear of the masses to some supposed good end, to put it mildly, so I'd be worried about deriving that generalization from it. The reaction may more be that the fear of the public is a big powerful uncontrollable thing that doesn't move in the smart direction - maybe the public fear of AI gets channeled by opportunistic government officials into \"and that's why We must have Our AGI first so it will be Good and we can Win\". That seems to me much more like a thing that would happen in real life than \"and then we managed to manipulate public panic down exactly the direction we wanted to fit into our clever master scheme\", especially when we don't actually <i>have</i> the clever master scheme it fits into.</p><hr><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I have a few stupid ideas I could try to investigate in ML, but that would require the ability to run significant-sized closed ML projects full of trustworthy people, which is a capability that doesn't seem to presently exist. Plausibly, this capability would be required in any world that got some positive model violation (\"miracle\") to take advantage of, so I would want to build that capability today. I am not sure how to go about doing that either.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>if there's a chance this group can do something to gain this capability I'd be interested in checking it out. I'd want to know more about what \"closed\"and \"trustworthy\" mean for this (and \"significant-size\" I guess too). E.g., which ones does Anthropic fail?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>What I'd like to exist is a setup where I can work with people that I or somebody else has vetted as seeming okay-trustworthy, on ML projects that aren't going to be published. Anthropic looks like it's a package deal. If Anthropic were set up to let me work with 5 particular people at Anthropic on a project boxed away from the rest of the organization, that would potentially be a step towards trying such things. It's also not clear to me that Anthropic has either the time to work with me, or the interest in doing things in AI that aren't \"stack more layers\" or close kin to that.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>That setup doesn't sound impossible to me -- at DeepMind or OpenAI or a new org specifically set up for it (or could be MIRI) -- the bottlenecks are access to trustworthy ML-knowledgeable people (but finding 5 in our social network doesn't seem impossible?) and access to compute (can be solved with more money - not too hard?). I don't think DM and OpenAI are publishing everything - the \"not going to be published\" part doesn't seem like a big barrier to me. Is infosec a major bottleneck (i.e., who's potentially stealing the code/data)?</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Do you think Redwood Research could be a place for this?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Maybe! I haven't ruled RR out yet. But they also haven't yet done (to my own knowledge) anything demonstrating the same kind of AI-development capabilities as even GPT-3, let alone AlphaFold 2.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I would potentially be super interested in working with Deepminders if Deepmind set up some internal partition for \"Okay, accomplished Deepmind researchers who'd rather not destroy the world are allowed to form subpartitions of this partition and have their work not be published outside the subpartition let alone Deepmind in general, though maybe you have to report on it to Demis only or something.\" I'd be more skeptical/worried about working with OpenAI-minus-Anthropic because the notion of \"open AI\" continues to sound to me like \"what is the worst possible strategy for making the game board as unplayable as possible while demonizing everybody who tries a strategy that could possibly lead to the survival of humane intelligence\", and now a lot of the people who knew about that part have left OpenAI for elsewhere. But, sure, if they changed their name to \"ClosedAI\" and fired everyone who believed in the original OpenAI mission, I would update about that.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Context that is potentially missing here and should be included: I wish that Deepmind had more internal closed research, and internally siloed research, as part of a larger wish I have about the AI field, independently of what projects I'd want to work on myself.</p><p>The present situation can be seen as one in which a common resource, the remaining timeline until AGI shows up, is incentivized to be burned by AI researchers because they have to come up with neat publications and publish them (which burns the remaining timeline) in order to earn status and higher salaries. The more they publish along the spectrum that goes {quiet internal result -&gt; announced and demonstrated result -&gt; paper describing how to get the announced result -&gt; code for the result -&gt; model for the result}, the more timeline gets burned, and the greater the internal and external prestige accruing to the researcher.</p><p>It's futile to wish for everybody to act uniformly against their incentives.&nbsp; But I think it would be a step forward if the relative incentive to burn the commons could be <i>reduced</i>; or to put it another way, the more researchers have the <i>option</i> to not burn the timeline commons, without them getting fired or passed up for promotion, the more that unusually intelligent researchers might perhaps decide not to do that. So I wish in general that AI research groups in general, but also Deepmind in particular, would have affordances for researchers who go looking for interesting things to not publish any resulting discoveries, at all, and still be able to earn internal points for them. I wish they had the <i>option</i> to do that. I wish people were <i>allowed</i> to not destroy the world - and still get high salaries and promotion opportunities and the ability to get corporate and ops support for playing with interesting toys; if destroying the world is prerequisite for having nice things, nearly everyone is going to contribute to destroying the world, because, like, they're not going to just <i>not</i> have nice things, that is not human nature for almost all humans.</p><p>When I visualize how the end of the world plays out, I think it involves an AGI system which has the ability to be cranked up by adding more computing resources to it; and I think there is an extended period where the system is not aligned enough that you can crank it up that far, without everyone dying. And it seems <i>extremely</i> likely that if factions on the level of, say, Facebook AI Research, start being able to deploy systems like that, then death is very automatic. If the Chinese, Russian, and French intelligence services all manage to steal a copy of the code, and China and Russia sensibly decide not to run it, and France gives it to three French corporations which I hear the French intelligence service sometimes does, then again, everybody dies. If the builders are sufficiently worried about that scenario that they push too fast too early, in fear of an arms race developing very soon if they wait, again, everybody dies.</p><p>At present we're very much waiting on a miracle for alignment to be possible at all, even if the AGI-builder successfully prevents proliferation and has 2 years in which to work. But if we get that miracle at all, it's not going to be an instant miracle.&nbsp; There\u2019ll be some minimum time-expense to do whatever work is required. So any time I visualize anybody trying to even start a successful trajectory of this kind, they need to be able to get a lot of work done, without the intermediate steps of AGI work being published, or demoed at all, let alone having models released.&nbsp; Because if you wait until the last months when it is really really obvious that the system is going to scale to AGI, in order to start closing things, almost all the prerequisites will already be out there. Then it will only take 3 more months of work for somebody else to build AGI, and then somebody else, and then somebody else; and even if the first 3 factions manage not to crank up the dial to lethal levels, the 4th party will go for it; and the world ends by default on full automatic.</p><p>If ideas are theoretically internal to \"just the company\", but the company has 150 people who all know, plus everybody with the \"sysadmin\" title having access to the code and models, then I imagine - perhaps I am mistaken - that those ideas would (a) inevitably leak outside due to some of those 150 people having cheerful conversations over a beer with outsiders present, and (b) be copied outright by people of questionable allegiances once all hell started to visibly break loose. As with anywhere that handles really sensitive data, the concept of \"need to know\" has to be a thing, or else everyone (and not just in that company) ends up knowing.</p><p>So, even if I got run over by a truck tomorrow, I would still very much wish that in the world that survived me, Deepmind would have lots of penalty-free affordance internally for people to not publish things, and to work in internal partitions that didn't spread their ideas to all the rest of Deepmind.&nbsp; Like, <i>actual</i> social and corporate support for that, not just a theoretical option you'd have to burn lots of social capital and weirdness points to opt into, and then get passed up for promotion forever after.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>What's RR?</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>It's a new alignment org, run by Nate Thomas and ~co-run by Buck Shlegeris and Bill Zito, with maybe 4-6 other technical folks so far. My take: the premise is to create an org with ML expertise and general just-do-it competence that's trying to do all the alignment experiments that something like Paul+Ajeya+Eliezer all think are obviously valuable and wish someone would do. They expect to have a website etc in a few days; the org is a couple months old in its current form.</p><hr><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>How likely really is hard takeoff? Clearly, we are touching the edges of AGI with GPT and the like. But I'm not feeling this will that easily be leveraged into very quick recursive self improvement.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Compared to the position I was arguing in the Foom Debate with Robin, reality has proved way to the further Eliezer side of Eliezer along the Eliezer-Robin spectrum. It's been very unpleasantly surprising to me how little architectural complexity is required to start producing generalizing systems, and how fast those systems scale using More Compute. The flip side of this is that I can imagine a system being scaled up to interesting human+ levels, without \"recursive self-improvement\" or other of the old tricks that I thought would be necessary, and argued to Robin would make fast capability gain possible. You could have fast capability gain well before anything like a FOOM started. Which in turn makes it more plausible to me that we could hang out at interesting not-superintelligent levels of AGI capability for a while before a FOOM started. It's not clear that this helps anything, but it does seem more plausible.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>I agree reality has not been hugging the Robin kind of scenario this far.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Going past human level doesn't necessarily mean going \"foom\".</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I do think that if you get an AGI significantly past human intelligence in all respects, it would obviously tend to FOOM. I mean, I suspect that Eliezer fooms if you give an Eliezer the ability to backup, branch, and edit himself.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>It doesn't seem to me that an AGI significantly past human intelligence necessarily tends to FOOM.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I think in principle we could have, for example, an AGI that was just a superintelligent engineer of proteins, and of nanosystems built by nanosystems that were built by proteins, and which was corrigible enough not to want to improve itself further; and this AGI would also be dumber than a human when it came to eg psychological manipulation, because we would have asked it not to think much about that subject. I'm doubtful that you can have an AGI that's significantly above human intelligence in <i>all</i> respects, without it having the capability-if-it-wanted-to of looking over its own code and seeing lots of potential improvements.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Alright, this makes sense to me, but I don't expect an AGI to <i>want</i> to manipulate humans that easily (unless designed to). Maybe a bit.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Manipulating humans is a convergent instrumental strategy if you've accurately modeled (even at quite low resolution) what humans are and what they do in the larger scheme of things.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Yes, but human manipulation is also the kind of thing you need to guard against with even mildly powerful systems. Strong impulses to manipulate humans, should be vetted out.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I think that, by default, if you trained a young AGI to expect that 2+2=5 in some special contexts, and then scaled it up without further retraining, a generally superhuman version of that AGI would be very likely to 'realize' in some sense that SS0+SS0=SSSS0 was a consequence of the Peano axioms. There's a natural/convergent/coherent output of deep underlying algorithms that generate competence in some of the original domains; when those algorithms are implicitly scaled up, they seem likely to generalize better than whatever patch on those algorithms said '2 + 2 = 5'.</p><p>In the same way, suppose that you take weak domains where the AGI can't fool you, and apply some gradient descent to get the AGI to stop outputting actions of a type that humans can detect and label as 'manipulative'.&nbsp; And then you scale up that AGI to a superhuman domain.&nbsp; I predict that deep algorithms within the AGI will go through consequentialist dances, and model humans, and output human-manipulating actions that can't be detected as manipulative by the humans, in a way that seems likely to bypass whatever earlier patch was imbued by gradient descent, because I doubt that earlier patch will generalize as well as the deep algorithms. Then you don't get to retrain in the superintelligent domain after labeling as bad an output that killed you and doing a gradient descent update on that, because the bad output killed you. (This is an attempted very fast gloss on what makes alignment difficult <i>in the first place</i>.)</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>[i appreciate this gloss - thanks]</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>\"deep algorithms within it will go through consequentialist dances, and model humans, and output human-manipulating actions that can't be detected as manipulative by the humans\"</p><p>This is true if it is rewarding to manipulate humans. If the humans are on the outlook for this kind of thing, it doesn't seem that easy to me.</p><p>Going through these \"consequentialist dances\" to me appears to presume that mistakes that should be apparent haven't been solved at simpler levels. It seems highly unlikely to me that you would have a system that appears to follow human requests and human values, and it would suddenly switch at some powerful level. I think there will be signs beforehand. Of course, if the humans are not paying attention, they might miss it. But, say, in the current milieu, I find it plausible that they will pay enough attention.</p><p>\"because I doubt that earlier patch will generalize as well as the deep algorithms\"</p><p>That would depend on how \"deep\" your earlier patch was. Yes, if you're just doing surface patches to apparent problems, this might happen. But it seems to me that useful and intelligent systems will require deep patches (or deep designs from the start) in order to be apparently useful to humans at solving complex problems enough. This is not to say that they would be perfect. But it seems quite plausible to me that they would in most cases prevent the worst outcomes.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>\"If you've got a general consequence-modeling-and-searching algorithm, it seeks out ways to manipulate humans, even if there are no past instances of a random-action-generator producing manipulative behaviors that succeeded and got reinforced by gradient descent over the random-action-generator. It invents the strategy de novo by imagining the results, even if there's no instances in memory of a strategy like that having been tried before.\" Agree or disagree?</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Creating strategies de novo would of course be expected of an AGI.</p><blockquote><p>\"If you've got a general consequence-modeling-and-searching algorithm, it seeks out ways to manipulate humans, even if there are no past instances of a random-action-generator producing manipulative behaviors that succeeded and got reinforced by gradient descent over the random-action-generator. It invents the strategy de novo by imagining the results, even if there's no instances in memory of a strategy like that having been tried before.\" Agree or disagree?</p></blockquote><p>I think, if the AI will \"seek out ways to manipulate humans\", will depend on what kind of goals the AI has been designed to pursue.</p><p>Manipulating humans is definitely an instrumentally useful kind of method for an AI, for a lot of goals. But it's also counter to a lot of the things humans would direct the AI to do -- at least at a \"high level\". \"Manipulation\", such as marketing, for lower level goals, can be very congruent with higher level goals. An AI could clearly be good at manipulating humans, while not manipulating its creators or the directives of its creators.</p><p>If you are asking me to agree that the AI will generally seek out ways to manipulate the high-level goals, then I will say \"no\". Because it seems to me that faults of this kind in the AI design is likely to be caught by the designers earlier. (This isn't to say that this kind of fault couldn't happen.) It seems to me that manipulation of high-level goals will be one of the most apparent kind of faults of this kind of system.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>RE: \"I'm doubtful that you can have an AGI that's significantly above human intelligence in <i>all</i> respects, without it having the capability-if-it-wanted-to of looking over its own code and seeing lots of potential improvements.\"</p><p>It seems plausible (though unlikely) to me that this would be true in practice for the AGI we build -- but also that the potential improvements it sees would be pretty marginal. This is coming from the same intuition that current learning algorithms might already be approximately optimal.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><blockquote><p>If you are asking me to agree that the AI will generally seek out ways to manipulate the high-level goals, then I will say \"no\". Because it seems to me that faults of this kind in the AI design is likely to be caught by the designers earlier.</p></blockquote><p>I expect that when people are trying to stomp out convergent instrumental strategies by training at a safe dumb level of intelligence, this will not be effective at preventing convergent instrumental strategies at smart levels of intelligence; also note that at very smart levels of intelligence, \"hide what you are doing\" is also a convergent instrumental strategy of that substrategy.</p><p>I don't know however if I should be explaining at this point why \"manipulate humans\" is convergent, why \"conceal that you are manipulating humans\" is convergent, why you have to train in safe regimes in order to get safety in dangerous regimes (because if you try to \"train\" at a sufficiently unsafe level, the output of the unaligned system deceives you into labeling it incorrectly and/or kills you before you can label the outputs), or why attempts to teach corrigibility in safe regimes are unlikely to generalize well to higher levels of intelligence and unsafe regimes (qualitatively new thought processes, things being way out of training distribution, and, the hardest part to explain, corrigibility being \"anti-natural\" in a certain sense that makes it incredibly hard to, eg, exhibit any coherent planning behavior (\"consistent utility function\") which corresponds to being willing to let somebody else shut you off, without incentivizing you to actively manipulate them to shut you off).</p><hr><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>My (unfinished) idea for buying time is to focus on applying AI to well-specified problems, where constraints can come primarily from the action space and additionally from process-level feedback (i.e., human feedback providers understand why actions are good before endorsing them, and reject anything weird even if it seems to work on some outcomes-based metric). This is basically a form of boxing, with application-specific boxes. I know it doesn't scale to superintelligence but I think it can potentially give us time to study and understand proto AGIs before they kill us. I'd be interested to hear devastating critiques of this that imply it isn't even worth fleshing out more and trying to pursue, if they exist.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>(I think it's also similar to CAIS in case that's helpful.)</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>There's lots of things we can do which don't solve the problem and involve us poking around with AIs having fun, while we wait for a miracle to pop out of nowhere. There's lots of things we can do with AIs which are weak enough to not be able to fool us and to not have cognitive access to any dangerous outputs, like automatically generating pictures of cats.&nbsp; The trouble is that nothing we can do with an AI like that (where \"human feedback providers understand why actions are good before endorsing them\") is powerful enough to save the world.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>In other words, if you have an aligned AGI that builds complete mature nanosystems for you, that <i>is</i> enough force to save the world; but that AGI needs to have been aligned by some method other than \"humans inspect those outputs and vet them and their consequences as safe/aligned\", because humans cannot accurately and unfoolably vet the consequences of DNA sequences for proteins, or of long bitstreams sent to protein-built nanofactories.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>When you mention nanosystems, how much is this just a hypothetical superpower vs. something you actually expect to be achievable with AGI/superintelligence? If expected to be achievable, why?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>The case for nanosystems being possible, if anything, seems even more slam-dunk than the already extremely slam-dunk case for superintelligence, because we can set lower bounds on the power of nanosystems using far more specific and concrete calculations. See eg the first chapters of Drexler's Nanosystems, which are the first step mandatory reading for anyone who would otherwise doubt that there's plenty of room above biology and that it is possible to have artifacts the size of bacteria with much higher power densities. I have this marked down as \"known lower bound\" not \"speculative high value\", and since Nanosystems has been out since 1992 and subjected to attemptedly-skeptical scrutiny, without anything I found remotely persuasive turning up, I do not have a strong expectation that any new counterarguments will materialize.</p><p>If, after reading Nanosystems, you still don't think that a superintelligence can get to and past the Nanosystems level, I'm not quite sure what to say to you, since the models of superintelligences are much less concrete than the models of molecular nanotechnology.</p><p>I'm on record as early as 2008 as saying that I expected superintelligences to crack protein folding, some people disputed that and were all like \"But how do you know that's solvable?\" and then AlphaFold 2 came along and cracked the protein folding problem they'd been skeptical about, far below the level of superintelligence.</p><p>I can try to explain how I was mysteriously able to forecast this truth at a high level of confidence - not the exact level where it became possible, to be sure, but that superintelligence would be sufficient - despite this skepticism; I suppose I could point to prior hints, like even human brains being able to contribute suggestions to searches for good protein configurations; I could talk about how if evolutionary biology made proteins evolvable then there must be a lot of regularity in the folding space, and that this kind of regularity tends to be exploitable.</p><p>But of course, it's also, in a certain sense, very <i>obvious</i> that a superintelligence could crack protein folding, just like it was obvious years before <i>Nanosystems</i> that molecular nanomachines would in fact be possible and have much higher power densities than biology. I could say, \"Because proteins are held together by van der Waals forces that are much weaker than covalent bonds,\" to point to a reason how you could realize that after just reading <i>Engines of Creation</i> and before <i>Nanosystems</i> existed, by way of explaining how one could possibly guess the result of the calculation in advance of building up the whole detailed model. But in reality, precisely because the possibility of molecular nanotechnology was already obvious to any sensible person just from reading <i>Engines of Creation</i>, the sort of person who wasn't convinced by <i>Engines of Creation</i> wasn't convinced by <i>Nanosystems</i> either, because they'd already demonstrated immunity to sensible arguments; an example of the general phenomenon I've elsewhere termed the Law of Continued Failure.</p><p>Similarly, the sort of person who was like \"But how do you know superintelligences will be able to build nanotech?\" in 2008, will probably not be persuaded by the demonstration of AlphaFold 2, because it was already clear to anyone sensible in 2008, and so anyone who can't see sensible points in 2008 probably also can't see them after they become even clearer. There are some people on the margins of sensibility who fall through and change state, but mostly people are not on the exact margins of sanity like that.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>\"If, after reading Nanosystems, you still don't think that a superintelligence can get to and past the Nanosystems level, I'm not quite sure what to say to you, since the models of superintelligences are much less concrete than the models of molecular nanotechnology.\"</p><p>I'm not sure if this is directed at <i>me</i> or the <a href=\"https://en.wikipedia.org/wiki/Generic_you\"><u>https://en.wikipedia.org/wiki/Generic_you</u></a>, but I'm only expressing curiosity on this point, not skepticism :)</p><hr><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>some form of \"scalable oversight\" is the naive extension of the initial boxing thing proposed above that claims to be the required alignment method -- basically, make the humans vetting the outputs smarter by providing them AI support for all well-specified (level-below)-vettable tasks.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I haven't seen any plausible story, in any particular system design being proposed by the people who use terms about \"scalable oversight\", about how human-overseeable thoughts or human-inspected underlying systems, compound into very powerful human-non-overseeable outputs that are trustworthy. Fundamentally, the whole problem here is, \"You're allowed to look at floating-point numbers and Python code, but how do you get from there to trustworthy nanosystem designs?\" So saying \"Well, we'll look at some thoughts we can understand, and then from out of a much bigger system will come a trustworthy output\" doesn't answer the hard core at the center of the question. Saying that the humans will have AI support doesn't answer it either.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>the kind of useful thing humans (assisted-humans) might be able to vet is reasoning/arguments/proofs/explanations. without having to generate neither the trustworthy nanosystem design nor the reasons it is trustworthy, we could still check them.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>If you have an untrustworthy general superintelligence generating English strings meant to be \"reasoning/arguments/proofs/explanations\" about eg a nanosystem design, then I would not only expect the superintelligence to be able to fool humans in the sense of arguing for things that were not true in a way that fooled the humans, I'd expect the superintelligence to be able to covertly directly hack the humans in ways that I wouldn't understand even after having been told what happened. So you must have some prior belief about the superintelligence being aligned before you dared to look at the arguments. How did you get that prior belief?</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>I think I'm not starting with a general superintelligence here to get the trustworthy nanodesigns. I'm trying to build the trustworthy nanosystems \"the hard way\", i.e., if we did it without ever building AIs, and then speed that up using AI for automation of things we know how to vet (including recursively). Is a crux here that you think nanosystem design requires superintelligence?</p><p>(tangent: I think this approach works even if you accidentally built a more-general or more-intelligent than necessary foundation model as long as you're only using it in boxes it can't outsmart. The better-specified the tasks you automate are, the easier it is to secure the boxes.)</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>I think that China ends the world using code they stole from Deepmind that did things the easy way, and that happens 50 years of natural R&amp;D time before you can do the equivalent of \"strapping mechanical aids to a horse instead of building a car from scratch\".</p><p>I also think that the speedup step in \"iterated amplification and distillation\" will introduce places where the fast distilled outputs of slow sequences are not true to the original slow sequences, because gradient descent is not perfect and won't be perfect and it's not clear we'll get any paradigm besides gradient descent for doing a step like that.</p><hr><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>How do you feel about the safety community as a whole and the growth we've seen over the past few years?</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Very grim. I think that almost everybody is bouncing off the real hard problems at the center and doing work that is predictably not going to be useful at the superintelligent level, nor does it teach me anything I could not have said in advance of the paper being written. People like to do projects that they know will succeed and will result in a publishable paper, and that rules out all real research at step 1 of the social process.</p><p>Paul Christiano is trying to have real foundational ideas, and they're all wrong, but he's one of the few people trying to have foundational ideas at all; if we had another 10 of him, something might go right.</p><p>Chris Olah is going to get far too little done far too late. We're going to be facing down an unalignable AGI and the current state of transparency is going to be \"well look at this interesting visualized pattern in the attention of the key-value matrices in layer 47\" when what we need to know is \"okay but was the AGI plotting to kill us or not\u201d. But Chris Olah is still trying to do work that is on a pathway to anything important at all, which makes him exceptional in the field.</p><p>Stuart Armstrong did some good work on further formalizing the shutdown problem, an example case in point of why corrigibility is hard, which so far as I know is still resisting all attempts at solution.</p><p>Various people who work or worked for MIRI came up with some actually-useful notions here and there, like Jessica Taylor's expected utility quantilization.</p><p>And then there is, so far as I can tell, a vast desert full of work that seems to me to be mostly fake or pointless or predictable.</p><p>It is very, very clear that at present rates of progress, adding that level of alignment capability as grown over the next N years, to the AGI capability that arrives after N years, results in everybody dying very quickly. Throwing more money at this problem does not obviously help because it just produces more low-quality work.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>\"doing work that is predictably not going to be really useful at the superintelligent level, nor does it teach me anything I could not have said in advance of the paper being written\"</p><p>I think you're underestimating the value of solving small problems. Big problems are solved by solving many small problems. (I do agree that many academic papers do not represent much progress, however.)</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>By default, I suspect you have longer timelines and a smaller estimate of total alignment difficulty, not that I put less value than you on the incremental power of solving small problems over decades. I think we're going to be staring down the gun of a completely inscrutable model that would kill us all if turned up further, with no idea how to read what goes on inside its head, and no way to train it on humanly scrutable and safe and humanly-labelable domains in a way that seems like it would align the superintelligent version, while standing on top of a whole bunch of papers about \"small problems\" that never got past \u201csmall problems\u201d.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>\"I think we're going to be staring down the gun of a completely inscrutable model that would kill us all if turned up further, with no idea how to read what goes on inside its head, and no way to train it on humanly scrutable and safe and humanly-labelable domains in a way that seems like it would align the superintelligent version\"</p><p>This scenario seems possible to me, but not very plausible. GPT is not going to \"kill us all\" if turned up further. No amount of computing power (at least before AGI) would cause it to. I think this is apparent, without knowing exactly what's going on inside GPT. This isn't to say that there aren't AI systems that wouldn't. But <i>what kind of system would</i>? (A GPT combined with sensory capabilities at the level of Tesla's self-driving AI? That still seems too limited.)</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Alpha Zero scales with more computing power, I think AlphaFold 2 scales with more computing power, Mu Zero scales with more computing power. Precisely because GPT-3 doesn't scale, I'd expect an AGI to look more like Mu Zero and particularly with respect to the fact that it has some way of scaling.</p><hr><p><strong>Steve Omohundro&nbsp;</strong>&nbsp;</p><p>Eliezer, thanks for doing this! I just now read through the discussion and found it valuable. I agree with most of your specific points but I seem to be much more optimistic than you about a positive outcome. I'd like to try to understand why that is. I see mathematical proof as the most powerful tool for constraining intelligent systems and I see a pretty clear safe progression using that for the technical side (the social side probably will require additional strategies). Here are some of my intuitions underlying that approach, I wonder if you could identify any that you disagree with. I'm fine with your using my name (Steve Omohundro) in any discussion of these.</p><p>1) Nobody powerful wants to create unsafe AI but they do want to take advantage of AI capabilities.</p><p>2) None of the concrete well-specified valuable AI capabilities require unsafe behavior</p><p>3) Current simple logical systems are capable of formalizing every relevant system involved (eg. MetaMath <a href=\"http://us.metamath.org/index.html\"><u>http://us.metamath.org/index.html</u></a> currently formalizes roughly an undergraduate math degree and includes everything needed for modeling the laws of physics, computer hardware, computer languages, formal systems, machine learning algorithms, etc.)</p><p>4) Mathematical proof is cheap to mechanically check (eg. MetaMath has a 500 line Python verifier which can rapidly check all of its 38K theorems)</p><p>5) GPT-F is a fairly early-stage transformer-based theorem prover and can already prove 56% of the MetaMath theorems. Similar systems are likely to soon be able to rapidly prove all simple true theorems (eg. that human mathematicians can prove in a day).</p><p>6) We can define provable limits on the behavior of AI systems that we are confident prevent dangerous behavior and yet still enable a wide range of useful behavior.&nbsp;</p><p>7) We can build automated checkers for these provable safe-AI limits.&nbsp;</p><p>8) We can build (and eventually mandate) powerful AI hardware that first verifies proven safety constraints before executing AI software&nbsp;</p><p>9) For example, AI smart compilation of programs can be formalized and doesn't require unsafe operations&nbsp;</p><p>10) For example, AI design of proteins to implement desired functions can be formalized and doesn't require unsafe operations&nbsp;</p><p>11) For example, AI design of nanosystems to achieve desired functions can be formalized and doesn't require unsafe operations.</p><p>12) For example, the behavior of designed nanosystems can be similarly constrained to only proven safe behaviors</p><p>13) And so on through the litany of early stage valuable uses for advanced AI.</p><p>14) I don't see any fundamental obstructions to any of these. Getting social acceptance and deployment is another issue!&nbsp;</p><p>Best, Steve</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Steve, are you visualizing AGI that gets developed 70 years from now under absolutely different paradigms than modern ML? I don't see being able to take anything remotely like, say, Mu Zero, and being able to prove any theorem about it which implies anything like corrigibility or the system not internally trying to harm humans. Anything in which enormous inscrutable floating-point vectors is a key component, seems like something where it would be very hard to prove any theorems about the treatment of those enormous inscrutable vectors that would correspond in the outside world to the AI not killing everybody.</p><p>Even if we somehow managed to get structures far more legible than giant vectors of floats, using some AI paradigm very different from the current one, it still seems like huge key pillars of the system would rely on non-fully-formal reasoning; even if the AI has something that you can point to as a utility function and even if that utility function's representation is made out of programmer-meaningful elements instead of giant vectors of floats, we'd still be relying on much shakier reasoning at the point where we claimed that this utility function meant something in an intuitive human-desired sense, say. And if that utility function is learned from a dataset and decoded only afterwards by the operators, that sounds even scarier. And if instead you're learning a giant inscrutable vector of floats from a dataset, gulp.&nbsp;</p><p>You seem to be visualizing that we prove a theorem and then get a theorem-like level of assurance that the system is safe. What kind of theorem? What the heck would it say?&nbsp;</p><p>I agree that it seems plausible that the good cognitive operations we want do not <i>in principle</i> require performing bad cognitive operations; the trouble, from my perspective, is that generalizing structures that do lots of good cognitive operations will automatically produce bad cognitive operations, especially when we dump more compute into them; \"you can't bring the coffee if you're dead\".</p><p>So it takes a more complicated system and some feat of insight I don't presently possess, to \"just\" do the good cognitions, instead of doing all the cognitions that result from decompressing the thing that compressed the cognitions in the dataset - even if that original dataset only contained cognitions that looked good to us, even if that dataset actually <i>was</i> just correctly labeled data about safe actions inside a slightly dangerous domain. Humans do a lot of stuff besides maximizing inclusive genetic fitness, optimizing purely on outcomes labeled by a simple loss function doesn\u2019t get you an internal optimizer that pursues only that loss function, etc.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>Steve's intuitions sound to me like they're pointing at the \"well-specified problems\" idea from an earlier thread. Essentially, only use AI in domains where unsafe actions are impossible by construction. Is this too strong a restatement of your intuitions Steve?</p><p>&nbsp;</p><p><strong>Steve Omohundro&nbsp;</strong>&nbsp;</p><p>Thanks for your perspective! Those sound more like social concerns than technical ones, though. I totally agree that today's AI culture is very \"sloppy\" and that the currently popular representations, learning algorithms, data sources, etc. aren't oriented around precise formal specification or provably guaranteed constraints. I'd love any thoughts about ways to help shift that culture toward precise and safe approaches! Technically there is no problem getting provable constraints on floating point computations, etc. The work often goes under the label \"Interval Computation\". It's not even very expensive, typically just a factor of 2 worse than \"sloppy\" computations. For some reason those approaches have tended to be more popular in Europe than in the US. Here are a couple lists of references: <a href=\"http://www.cs.utep.edu/interval-comp/\"><u>http://www.cs.utep.edu/interval-comp/</u></a> <a href=\"https://www.mat.univie.ac.at/~neum/interval.html\"><u>https://www.mat.univie.ac.at/~neum/interval.html</u></a></p><p>I see today's dominant AI approach of mapping everything to large networks ReLU units running on hardware designed for dense matrix multiplication, trained with gradient descent on big noisy data sets as a very temporary state of affairs. I fully agree that it would be uncontrolled and dangerous scaled up in its current form! But it's really terrible in every aspect except that it makes it easy for machine learning practitioners to quickly slap something together which will actually sort of work sometimes. With all the work on AutoML, NAS, and the formal methods advances I'm hoping we leave this \"sloppy\" paradigm pretty quickly. Today's neural networks are terribly inefficient for inference: most weights are irrelevant for most inputs and yet current methods do computational work on each. I developed many algorithms and data structures to avoid that waste years ago (eg. \"bumptrees\" <a href=\"https://steveomohundro.com/scientific-contributions/)\"><u>https://steveomohundro.com/scientific-contributions/</u>)</a></p><p>They're also pretty terrible for learning since most weights don't need to be updated for most training examples and yet they are. Google and others are using Mixture-of-Experts to avoid some of that cost: <a href=\"https://arxiv.org/abs/1701.06538\"><u>https://arxiv.org/abs/1701.06538</u></a></p><p>Matrix multiply is a pretty inefficient primitive and alternatives are being explored: <a href=\"https://arxiv.org/abs/2106.10860\"><u>https://arxiv.org/abs/2106.10860</u></a></p><p>Today's reinforcement learning is slow and uncontrolled, etc. All this ridiculous computational and learning waste could be eliminated with precise formal approaches which measure and optimize it precisely. I'm hopeful that that improvement in computational and learning performance may drive the shift to better controlled representations.</p><p>I see theorem proving as hugely valuable for safety in that we can easily precisely specify many important tasks and get guarantees about the behavior of the system. I'm hopeful that we will also be able to apply them to the full AGI story and encode human values, etc., but I don't think we want to bank on that at this stage. Hence, I proposed the \"Safe-AI Scaffolding Strategy\" where we never deploy a system without proven constraints on its behavior that give us high confidence of safety. We start extra conservative and disallow behavior that might eventually be determined to be safe. At every stage we maintain very high confidence of safety. Fast, automated theorem checking enables us to build computational and robotic infrastructure which only executes software with such proofs.</p><p>And, yes, I'm totally with you on needing to avoid the \"basic AI drives\"! I think we have to start in a phase where AI systems are not allowed to run rampant as uncontrolled optimizing agents! It's easy to see how to constrain limited programs (eg. theorem provers, program compilers or protein designers) to stay on particular hardware and only communicate externally in precisely constrained ways. It's similarly easy to define constrained robot behaviors (eg. for self-driving cars, etc.) The dicey area is that unconstrained agentic edge. I think we want to stay well away from that until we're very sure we know what we're doing! My optimism stems from the belief that many of the socially important things we need AI for won't require anything near that unconstrained edge. But it's tempered by the need to get the safe infrastructure into place before dangerous AIs are created.</p><p>&nbsp;</p><p><strong>Anonymous&nbsp;</strong>&nbsp;</p><p>As far as I know, all the work on \"verifying floating-point computations\" currently is way too low-level -- the specifications that are proved about the computations don't say anything about what the computations mean or are about, beyond the very local execution of some algorithm. Execution of algorithms in the real world can have very far-reaching effects that aren't modelled by their specifications.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>Yeah, what they said. How do you get from proving things about error bounds on matrix multiplications of inscrutable floating-point numbers, to saying anything about what a mind is trying to do, or not trying to do, in the external world?</p><p>&nbsp;</p><p><strong>Steve Omohundro&nbsp;</strong>&nbsp;</p><p>Ultimately we need to constrain behavior. You might want to ensure your robot butler won't leave the premises. To do that using formal methods, you need to have a semantic representation of the location of the robot, your premise's spatial extent, etc. It's pretty easy to formally represent that kind of physical information (it's just a more careful version of what engineers do anyway). You also have a formal model of the computational hardware and software and the program running the system.</p><p>For finite systems, any true property has a proof which can be mechanically checked but the size of that proof might be large and it might be hard to find. So we need to use encodings and properties which mesh well with the safety semantics we care about.</p><p>Formal proofs of properties of programs has progressed to where a bunch of cryptographic, compilation, and other systems can be specified and formalized. Why it's taken this long, I have no idea. The creator of any system has an argument as to why its behavior does what they think it will and why it won't do bad or dangerous things. The formalization of those arguments should be one direct short step.</p><p>Experience with formalizing mathematician's informal arguments suggest that the formal proofs are maybe 5 times longer than the informal argument. Systems with learning and statistical inference add more challenges but nothing that seems in-principal all that difficult. I'm still not completely sure how to constrain the use of language, however. I see inside of Facebook all sorts of problems due to inability to constrain language systems (eg. they just had a huge issue where a system labeled a video with a racist term). The interface between natural language semantics and formal semantics and how we deal with that for safety is something I've been thinking a lot about recently.</p><p>&nbsp;</p><p><strong>Steve Omohundro&nbsp;</strong>&nbsp;</p><p>Here's a nice 3 hour long tutorial about \"probabilistic circuits\" which is a representation of probability distributions, learning, Bayesian inference, etc. which has much better properties than most of the standard representations used in statistics, machine learning, neural nets, etc.: <a href=\"https://www.youtube.com/watch?v=2RAG5-L9R70\"><u>https://www.youtube.com/watch?v=2RAG5-L9R70</u></a> It looks especially amenable to interpretability, formal specification, and proofs of properties.</p><p>&nbsp;</p><p><strong>Eliezer Yudkowsky&nbsp;</strong>&nbsp;</p><p>You're preaching to the choir there, but even if we were working with more strongly typed epistemic representations that had been inferred by some unexpected innovation of machine learning, automatic inference of those representations would lead them to be uncommented and not well-matched with human compressions of reality, nor would they match exactly against reality, which would make it very hard for any theorem about \"we are optimizing against this huge uncommented machine-learned epistemic representation, to steer outcomes inside this huge machine-learned goal specification\" to guarantee safety in outside reality; especially in the face of how corrigibility is unnatural and runs counter to convergence and indeed coherence; especially if we're trying to train on domains where unaligned cognition is safe, and generalize to regimes in which unaligned cognition is not safe. Even in this case, we are not nearly out of the woods, because what we can prove has a great type-gap with that which we want to ensure is true. You can't handwave the problem of crossing that gap even if it's a solvable problem.</p><p>And that whole scenario would require some major total shift in ML paradigms.</p><p>Right now the epistemic representations are giant inscrutable vectors of floating-point numbers, and so are all the other subsystems and representations, more or less.</p><p>Prove whatever you like about that Tensorflow problem; it will make no difference to whether the AI kills you. The properties that can be proven just aren't related to safety, no matter how many times you prove an error bound on the floating-point multiplications. It wasn't floating-point error that was going to kill you in the first place.</p>", "user": {"username": "RobBensinger"}}, {"_id": "dAbs7w4J4iNm89DjP", "title": "Why fun writing can save lives: the case for it being high impact to make EA writing entertaining", "postedAt": "2021-11-11T22:17:31.993Z", "htmlBody": "<h3>EA content is often dry, which leads to fewer people reading it, which leads to the content having less impact.</h3><blockquote><em>Dry writing \u2192 \u2193 People reading \u2192 \u2193 Impact</em></blockquote><p><strong>EA writing can\u2019t have an impact if the right people don\u2019t read it, and the right people are more likely to read it if it\u2019s interesting.</strong> Therefore, if you want to maximize impact (which I imagine you do since you\u2019re reading this), you shouldn\u2019t make your writing dry.   </p><p>In the rest of this essay I'll explain why it's a problem, potential objections, and practical tips for how to make writing more engaging. </p><span><figure><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995605/mirroredImages/dAbs7w4J4iNm89DjP/gz26skgyt9cp9ykhbd3m.png\" class=\"draft-image center\" style=\"width:96%\"></figure></span><br><h1>Is this really a problem? </h1><p>Of course, there\u2019s a spectrum of how entertaining EA writing is and it\u2019s a wide distribution, with some very engaging writing being produced. Joe Carlsmith\u2019s \u201c<u><a href=\"https://forum.effectivealtruism.org/posts/HGLK3igGprWQPHfAp/against-neutrality-about-creating-happy-lives\">Against neutrality about creating happy lives</a></u>\u201d and Nate Soares\u2019 <u><a href=\"https://mindingourway.com/guilt/\">Replacing Guilt</a></u> series come to mind.</p><p>However, I think we can all agree that a large amount of the writing is a bit. . . well, mostly sticking to a dry, unobjectionable list of claims and arguments. The percentage of posts that have a <em>single</em> joke in them is probably less than half, perhaps as low as 10%. </p><p>The numbers are similarly bad for most other ways that an article can be spiced up, such as with clever turns of phrase, images, or anything that might cause you to feel any sort of emotion. </p><p>The focus is almost entirely on accuracy with little consideration to other possible metrics, such as being engaging or beautiful. </p><p>The writing is, in perhaps unsurprising news, rather utilitarian. </p><br><h1>Why this matters: dry writing leads to less utility for the world on average</h1><p>This is a problem because if nobody reads an article, it has no impact. Research and writing have an impact <em>through</em> other people, so other people need to have a way of being affected by the research, which is typically by reading or <u><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\">listening</a></u> to the content. Here\u2019s a <u><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library#:~:text=preferred%20podcasting%20app.-,goal%3A%20increase%20the%20number%20of%20people%20who%20read%20ea%20research,-An%20EA%20koan\">quick excerpt from a previous post I wrote about why increasing (impact-adjusted) readership is important</a>:</u></p><blockquote><em>Here are some purely hypothetical numbers just to illustrate this way of thinking:</em></blockquote><blockquote><em>Imagine that you, a researcher, have spent 100 hours producing outstanding research that is relevant to 1,000 out of a total of 10,000 EAs.</em></blockquote><blockquote><em>Each relevant EA who reads your research will generate $1,000 of positive impact. So, <strong>if all 1,000 relevant EAs read your research, you will generate $1 million of impact</strong>.</em></blockquote><blockquote><em>You post it to the EA Forum, where posts receive 500 views on average. Let\u2019s say, because your report is long, only 20% read the whole thing - that\u2019s 100 readers. So you\u2019ve created 100*1,000 = $100,000 of impact. Since <strong>you spent 100 hours and created $100,000 of impact, that\u2019s $1,000 per hour </strong>- pretty good!</em></blockquote><blockquote><em>But if you were to spend, say 1 hour, promoting your report -  for example, by posting links on EA-related Facebook groups - to generate another 100 readers, that would produce another $100,000 of impact. That\u2019s <strong>$100,000 per marginal hour</strong> or ~$2,000 per hour taking into account the fixed cost of doing the original research.</em></blockquote><p><strong>Likewise, if you spend a bit of time while writing your essay to make it interesting and fun, you could potentially 2-100x the readership and thus impact of your research.</strong> In this example, that could lead to another tens of thousands to millions of dollars worth of value per marginal hour. This is an extremely good investment of your effort. </p><p>Perhaps the most compelling example of this effect is Eliezer Yudkowsky. I remember one day deciding that I would read decision theory books instead of relying on LessWrong. To my surprise, I realized that pretty much everything in the sequences is in the intro to decision theory textbooks. However, they present it in the most boring, theoretical, non-actionable, and uninspiring way possible. If this had been my only introduction to rational thinking, I would most likely have either: ignored it; thought I ought to read it but never managed to motivate myself to; or diligently read it and never applied a thing because there was no community around it. </p><p>There\u2019s a reason why the decision theory didn\u2019t inspire a movement of rationalists (or at least nearly to the same degree) whereas Eliezer\u2019s writings inspired thousands to actually incorporate better decision making into their lives. <strong>Making important ideas interesting can be the difference between an obscure field and a thriving community.</strong></p><br><span><figure><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995605/mirroredImages/dAbs7w4J4iNm89DjP/kzixot2ghmiymhzzjkwz.png\" class=\"draft-image \" style=\"width:100%\"></figure></span><p><u><a href=\"https://www.facebook.com/groups/OMfCT/posts/2979865732328325\">Source</a></u></p><br><h1>Objections and counterarguments</h1><h2>Won\u2019t interesting writing lower the quality of the epistemics?</h2><p><strong>What do Eliezer Yudkowsky, Astral Codex Ten, Robert Miles, Brian Tomasik, and Wait But Why have in common? They all are epistemically rigorous <em>and</em> entertaining. They also, not at all incidentally, have large, intelligent audiences.</strong> </p><p>I bring these people up as examples illustrating an important fact: entertainment and truth are orthogonal. They are unrelated to each other. (1) You can have deeply entertaining truth and really boring falsehood. More commonly, the best epistemics are put forth in drab garb and the incorrect facts are dressed up in shiny outfits, but that\u2019s not because one causes the other or vice versa. </p><p>I don\u2019t know why this is the case, but I predict that school and academia play a large role in this:</p><ol type=\"1\"><li>The way you\u2019re taught to write at school bears only the most passing resemblance to how one ought to write in the real world. <strong>In school, you have a captive audience who\u2019s paid to read your material (i.e. the teacher). In the real world, you face ruthless competition for people\u2019s attention</strong> and if you bore them for even a sentence or two, they move on. </li><li><strong>Academia seems stuck in a local optimum of trying to convince others of their \u201cseriousness\u201d by writing in the driest, most emotionless, jargon-filled way.  The result is that few outside their tiny academic niche read their work - even if their work would be valuable to a wider audience. </strong>I think this may have seeped over into EA writing since so many EAs are academics or are in school.</li></ol><p>Regardless of the causes, hardly anybody would accuse the list of thinkers above of being epistemic lightweights. These people put a ton of work into figuring out the best conclusions <em>and</em> portraying it in interesting ways that people want to engage with. Clearly, we can have posts that are both epistemically rigorous and entertaining. </p><br><h2>Not everybody can write interesting content and this will make people feel bad</h2><p>I have definitely been self-conscious of this while writing this post. Unsurprisingly, writing a piece on why writing should be interesting makes you feel a lot of pressure to make it more interesting! </p><p>There are two main arguments against this line of reasoning though. Firstly, I\u2019m not making the case that everybody should hold themselves to the standard of Eliezer. This would lead to practically nobody publishing anything and would be a clear downgrade. </p><p>Rather, <strong>it\u2019s more of an aspirational value, focused on improvement and effort, not on consequences. Starting from wherever you\u2019re at, try to make your writing a little more interesting.</strong> Perhaps a better phrasing of the title would be \u201cit\u2019s your ethical duty to <em>try</em> to make your writing more interesting\u201d. Or maybe even, \u201cdon\u2019t try to be taken more seriously by writing in a dry technical manner. Instead, try to have high epistemic standards while simultaneously writing in a way that your intended audience will find enjoyable.\u201d </p><p>Secondly, if you follow this reasoning, we also shouldn\u2019t encourage people to write posts with rigorous epistemics or with creative insights because not everybody can do so. Not everybody is intelligent and hardworking enough to investigate claims as thoroughly as Scott Alexander, but that doesn\u2019t mean that only the most intellectually rigorous people should post. It just means that we all should continue the lifelong project of slowly but surely improving our rationality. </p><br><h2>Reputation hazards - what if people don\u2019t respect interesting writing?</h2><p>Elon Musk reads Wait But Why and it\u2019s mostly stick figure comics. Astral Codex Ten is one of the most respected people in the field and he averages at least three jokes per essay. Toby Ord wrote the most lyrical EA book, The Precipice, which is filled with beautiful turns of phrases and emotional appeals. And yet, it\u2019s also still full of facts, figures, graphs, and compelling logic. </p><p>If anything, all of these writers<em> improve</em> the reputation of EA. <strong>The entertainment value of an article doesn\u2019t necessarily worsen the respectability of the work. </strong></p><p>To add some nuance, I do think that a non-negligible percentage of people in certain contexts will not respect something unless it\u2019s a peer-reviewed PDF behind a paywall of a top journal. And sometimes it\u2019s worth catering to this audience, such as when we\u2019re trying to get the machine learning community to incorporate a particular technical solution. If you are explicitly trying to convince the academic world of something, it\u2019s usually best to play by their rules. However, most of the time the EA community is not interacting with that audience. Those who only respect peer-reviewed PDFs aren\u2019t reading this because this isn\u2019t a peer-reviewed PDF. <br><br>Generally speaking, if you\u2019re in the EA / rationalist community, you\u2019ve already self-filtered to be the sort of person who can see that rigorous reasoning can be combined in entertaining ways. You most likely enjoy the writing of at least one of Eliezer Yudkowsky, Scott Alexander, Tim Urban, or Brian Tomasik. You judge essays by the strength of their arguments and supporting evidence, not based on whether it pattern matches to \u201cserious\u201d writing. If somebody engagingly makes good points, all the better. </p><br><h2>Saying \u201cboring writing is unethical\u201d is a bit strong</h2><p>Fair enough. But \u201cIf you buy the drowning child argument and are a utilitarian, then you ought to make your writing more interesting\u201d wasn\u2019t nearly as pithy. And the whole point of this essay is to convince people to be pithier! </p><p>Really, I think the sub-title of the post is closer to my true claim. It's not that it's bad to write dry essays. It's that it's higher impact to make them more engaging. </p><br><h2>What about information hazards?</h2><p>If you\u2019ve written something that might be an info hazard, a way to make it less risky is to make it dry so fewer people will read it. This is a decent strategy for a lot of such situations. If you\u2019re writing something that seems potentially hazardous, by all means, make it as lifeless as you like. For the rest of the time though, please, liven it up a little for us.</p><br><h2>The highest impact people don\u2019t care about it being interesting</h2><p>As altruists, we\u2019re not just trying to maximize general readership. If we had one million reads of a post but nobody acted on it, that would be less high value than having only one person reading it, but they\u2019re a grantmaker and it improved their grantmaking decisions for millions of dollars. We want an \u201cimpact-adjusted audience\u201d if you will.</p><p>One might make the case that the highest impact audience to target usually are those high achievers we all know and want to hate but can\u2019t. Those terrible humans who seem to be in perfect control of their lives, who work 80 hours on a treadmill desk eating only the healthiest foods, whose idea of a vacation is a 10-day Vipassana retreat. Those sorts of people won\u2019t care if it\u2019s written in a dry style, and since it\u2019s a power law of impact, then it mostly matters how these people respond. </p><p>The first argument against this is that even amongst the most conscientious people, they can only spend so much time in the day reading dry, dense articles. At the end of the day they\u2019re tired and have limited energy to do hard things. If you\u2019re a scientific paper, you don\u2019t stand a chance of being read. If you\u2019re an exciting new Wait But Why article though, that\u2019s a different story. Even the highest performers are humans too and are more likely to do a thing if it\u2019s easier. </p><p>The second argument is that a lot of the highest impact people are, in fact, human. EA leadership is disproportionately filled with incredibly disciplined people, sure, but there are also tons of high impact people who struggle with willpower and procrastination just like the rest of us. <u><a href=\"https://mindingourway.com/not-yet-gods/\">Being high impact does not make you a god.</a></u></p><p>The third argument is that a lot of the time people read articles because it was recommended to them by somebody else. So if you write something interesting, it\u2019s more likely that it then gets shared with one of the highest impact people.</p><br><h1>How to make writing more engaging</h1><p>It\u2019s all well and good to say that there should be less boring EA writing in the world, but how do you make it happen? Here\u2019s a hodgepodge list of potential things to do:</p><ul><li><strong><u><a href=\"https://copyblogger.com/magnetic-headlines/\">Read Copyblogger</a></u>. </strong>It\u2019s the writing class written by people who actually write for a living in the real world (unlike your English teachers). Essential if you want to write for a cause. And, since they\u2019re good at writing, it\u2019s a joy to read of course. <u><a href=\"https://copyblogger.com/magnetic-headlines/\">Their book on headlines</a></u> is probably their highest value content. You have to give them your email address to get the free e-book, but it\u2019s worth it. They also have a ton of free content there that I highly recommend. If you don\u2019t want to read a whole book, here are the top three articles that I think cover the highest value ideas:</li><ul><li><u><a href=\"https://copyblogger.com/writing-headlines-that-get-results/\">The most important thing in writing - the headline</a></u></li><li><u><a href=\"https://copyblogger.com/first-sentence/\">The second most important thing in writing - the first sentence</a></u></li><li><u><a href=\"https://copyblogger.com/benefits-and-features/\">Lead with benefits, not features</a></u></li></ul><li><strong>Add pictures. </strong>Tip: look up your idea, then add the word \u201cfunny\u201d, and look through the images Google finds. You\u2019ll often find some really good material that way. </li><li><strong>Add jokes.</strong> Or just don\u2019t remove them. Robert Miles describes his process here which I really like: \u201cA single datapoint, but I don't think I really <em>try</em> to be engaging in my writing. Or like, it's one of the things I'm aiming for but it's not <strong>effort</strong>, it's not <strong>work</strong>. It's the default. I don't think I \u201cadd jokes\u201d, or \u201cadd flourishes\u201d or \u201cadd emotions\u201d, I just leave in the ones that come up naturally while explaining the idea. And I don't think this is anything special about me; I think most people are pretty engaging when they talk about their ideas, and pretty boring when they write about them. So for most people I wouldn't say 'add jokes', I would say 'stop taking out the jokes'. My advice is more like \u2018get out of your own way\u2019, or \u2018stop trying to be serious and respectable'.\"</li><li><strong>Add flourishes.</strong> Add little flourishes or witty turns of phrases. Get creative!</li><li><strong>Add emotion.</strong> Being rational doesn\u2019t mean we have to be Spock. It\u2019s OK and important to have writing that informs <em>and</em> inspires. <u><a href=\"https://www.nickbostrom.com/utopia.html\">Letter from Utopia</a></u> is one of the reasons I\u2019m interested in x-risks and <u><a href=\"https://i.imgur.com/Z8Mucdo.png\">this comic</a></u> (content warning: extreme suffering) is one of the reasons why I\u2019m motivated by s-risks. Using only emotional appeals is bad, but solely using rational ones is also suboptimal. </li><li><strong>Praise good writing publicly.</strong> People do more of what\u2019s socially approved of. Make it part of the culture to leave comments saying that the content was well written.</li><li><strong>Win a prize. </strong>We at <u><a href=\"http://nonlinear.org/\">Nonlinear</a></u> are considering launching a prize for the best-written essays each month. If you want to be notified if / when it\u2019s launched, <u><a href=\"https://www.nonlinear.org/subscribe.html\">subscribe to our newsletter</a></u> or to <u><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library#:~:text=Listen%20here%3A%20Spotify%2C%20Google%20Podcasts%2C%20Pocket%20Casts%2C%20Apple\">The Nonlinear Library</a></u>.</li><li><strong>Imagine someone is paying you $1000 for every <u>word</u> you remove</strong>. Brevity matters.</li><li><strong>Imagine someone is paying you $1000 for every <u>giant paragraph</u> you break into two smaller paragraphs</strong>. Few things cause people to stop reading faster than seeing an intimidating wall of text.</li><li><strong><u><a href=\"https://hemingwayapp.com/\">Use the Hemmingway App</a></u></strong> to make your sentences shorter and easier to read.</li></ul><p>These are just a few ideas I had on how to make writing more engaging. I\u2019m sure there are more things that could be done. Please share ideas in the comments! </p><p>Let's make the EA Forum even better. Let's make it so that at the end of a really long day, instead of mindlessly scrolling through social media, you read the EA Forum. Because it's not only meaningful and true, but it's also fun, and a hell of a good time. </p><h1>Footnotes</h1><p>1 - They are not completely unrelated to each other in the sense that people have limited energy and time, and so time spent on one goal will usually come at the expense of another. </p><p>2 - David Moss made a point that didn\u2019t make it into the body of the piece but that I think is worth bearing in mind: \u2018For many...people 'the appearance of rigour' is more important than the article actually being engaging for them to read. For example, even if they don't read/understand all of it, if you give them a 300 page report full of technical details they will say, \"This seems very rigorous\" and act on its conclusions, which they wouldn't do with a shorter more informal doc, even if they read it all and found it very engaging.\u2019</p>", "user": {"username": "katherinesavoie"}}, {"_id": "Nn2eudXZsRHx2xvti", "title": "BERI is hiring an ML Software Engineer", "postedAt": "2021-11-10T19:36:34.213Z", "htmlBody": "<p>All info and application here: <a href=\"https://existence.org/jobs/ml-engineer-seldonian\">https://existence.org/jobs/ml-engineer-seldonian</a></p><p>BERI is seeking a full-time machine learning software engineer as part of our collaboration with the <a href=\"https://all.cs.umass.edu/\">Autonomous Learning Laboratory</a> at UMass Amherst.</p><p>The engineer will create a software library that makes it easier for researchers and practitioners to apply and create <a href=\"http://aisafety.cs.umass.edu/overview.html\">Seldonian algorithms</a>. This library would facilitate and advance academic research on safe machine learning and would provide a practical tool for corporations to responsibly apply machine learning to high-risk high-reward applications. The end goal is to get more safety constraints built into ML systems.</p><p>We\u2019ve secured funding for this position for at least one year, which we believe will be enough time to implement the core functionality. Extensions will depend on our ability to secure more funding during the first year of work.</p>", "user": {"username": "sawyer"}}, {"_id": "8w7BrsvZ4CkABmstF", "title": "Recruitment Season", "postedAt": "2021-11-10T12:32:32.623Z", "htmlBody": "<p>At EA Geneva/EA Switzerland we have a recruitment season twice a year, once from mid-August to mid-October and once from mid-January to the end of February.&nbsp;</p><p>During the recruitment season, we focus the majority of our efforts on doing active outreach, for example by:</p><ul><li>Sending emails through (university) newsletters;&nbsp;</li><li>Posting on social media;&nbsp;</li><li>Announcements within the community, and;</li><li>Actively asking members to refer others to the upcoming Impact Seminar (our introduction program).&nbsp;</li></ul><p>Outside of the recruitment season, we focus on supporting our existing members and only do passive outreach by a) calling with interested people who reach out to us and b) occasionally, providing a short introduction talk on EA, if we are invited to speak for a potentially promising audience.&nbsp;</p><h2>Reasons why your group might consider working with a recruitment season:</h2><ul><li><strong>Focus:</strong> by working with a \u2018recruitment season\u2019 and a \u2018retainment season\u2019 you can focus your efforts more, and don\u2019t need to worry about the other activity too much.&nbsp;</li><li><strong>Keeping the message fresh:</strong> if you do active outreach only twice a year, the message may be perceived as more novel, therefore attracting more attention. We also try to create \u2018buzz\u2019 in our community during the recruitment season, actively asking members to refer their friends, fellow students, and colleagues to the upcoming introduction program. I think this can be more successful if done sparsely since if you do this too often, people might become non-responsive to your message.&nbsp;</li><li><strong>Trends:</strong> during my previous job, I analyzed the data from a volunteer job board, and observed a clear trend in interest in volunteer vacancies:&nbsp;<br>&nbsp;</li></ul><figure class=\"image image_resized\" style=\"width:83.65%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8w7BrsvZ4CkABmstF/tvdz7kv0ra2pqojdd4vc.png\"></figure><p><i>Unfortunately, I lost access to the data, but the trend looked roughly like this. The higher the line, the more expressions of interest in volunteer vacancies there were. I analyzed ~ 7 years of data, with +1000 responses a year.&nbsp;</i></p><p>I assume that this trend maps reasonably well to interest in EA and, moreover, I have slightly adapted the recruitment season to align with the start of the university semesters (which seems to be a period when students are more likely to explore).</p><ul><li><strong>Aligning activities:</strong> working with a recruitment season aligns your outreach activities into a \u2018<a href=\"https://en.wikipedia.org/wiki/User_journey\"><u>journey</u></a>\u2019, especially if you work with a clear next step (\u2018<a href=\"https://en.wikipedia.org/wiki/Call_to_action_(marketing)\"><u>call to action</u></a>\u2019). Effective Altruism is a rich set of ideas and for somebody new to the movement it can easily become overwhelming. I think that having a clear next step helps to get people into the movement, and since we run our Impact Seminar only twice a year, it makes sense to do outreach when people can quickly join an introduction program.&nbsp;</li></ul><p>Our Spring 2021 recruitment season looked like this:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994840/mirroredImages/8w7BrsvZ4CkABmstF/wkvvouaonabcwnmzyve5.png\"></figure><p>During the information evening, we gave a quick introduction to EA and longtermism, followed by a Q&amp;A about effective altruism and the upcoming Impact Seminar. The goal of the information evening was to reduce the drop-out rate of the seminar as participants would understand better what they were signing up for. In our experience, having a longer introduction program (several weeks) works better than only one introduction evening, as more participants continue to engage with our group afterward.&nbsp;</p><p>We deliberately call our program \u2018Impact Seminar\u2019 instead of \u2018Introduction to EA\u2019 as we have the impression that this is a more attractive wording for people who have never heard about effective altruism. I haven\u2019t noticed confusion with the participants about this wording, as the program is organized by Effective Altruism Switzerland.&nbsp;</p><p>&nbsp;</p><p><i>I would like to thank the other community builders during the CBG retreat for pointing out to me that I should write about our recruitment season, and would like to thank Antony in particular for this. I\u2019m also grateful to Tobias, Nadia, Jonathan, and Daniel for their feedback on this write-up.&nbsp;</i>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Naomi N"}}, {"_id": "wWGi4jTNNMhz2pHhJ", "title": "Persistence - A critical review [ABRIDGED]", "postedAt": "2021-11-10T11:30:51.522Z", "htmlBody": "<p><i>[this is a link post to the preprint </i><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#\"><i><u>Persistence - a Critical Review</u></i></a><i>, by Jaime Sevilla]</i></p><p><i>EDIT: For a more informal and less dry piece read </i><a href=\"https://forum.effectivealtruism.org/posts/uH4kGL4LgQdCgMpDP/can-we-influence-the-values-of-our-descendants\"><i>my takeaways here</i></a><i>.&nbsp;</i></p><p>IN SHORT: I review, replicate and extend the analysis from seven econometric papers studying how events that happened to and values held by our ancestors affect their descendants several generations afterwards (intergenerational persistence). I argue that together the papers provide moderate evidence of the existence of long term causal effects mediated by parentage.</p><p>KEYWORDS: persistence, cultural persistence, economic history, multiple hypothesis testing, post design power analysis, spatial autocorrelation bias, causality, natural experiments, instrumental variables.</p><p>Intergenerational persistence is an important topic for Effective Altruism, because it can help us understand how our actions today can affect many generations after. I undertook this research to help us shed light on whether cultural interventions (like increasing the degree at which present people value truth and cooperation) can be an effective way of affecting the long-term future.</p><p>The papers I review are:</p><ul><li><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.79js9smkm53d\"><i><u>The long term effects of Africa\u2019s slave trades</u></i></a><i> </i>(Nunn, 2008)</li><li><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.r65blp5p5d7m\"><i><u>The slave trade and the origins of mistrust in Africa</u></i></a><i> </i>(Nunn &amp; Wantchekon, 2011)</li><li><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.yszf6p1wdh77\"><i><u>On the Origins of Gender Roles: Women and the Plough</u></i></a> (Alesina et al., 2013)</li><li><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.xcdnm0xhpt50\"><i><u>The Church, intensive kinship, and global psychological variation</u></i></a> (Schulz et al., 2019)</li><li><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.6g5gv13a50p5\"><i><u>Persecution perpetuated: The Medieval Origins of Anti-Semitic Nazi Violence</u></i></a><i> </i>(Voigtl\u00e4nder &amp; Voth, 2012)</li><li><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.8u5kuh4qu3eg\"><i><u>Trade, Institutions, and Ethnic Tolerance: Evidence from South Asia</u></i></a> (Jha, 2013)</li><li><a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.mpekqsot0sm9\"><i><u>Long-term persistence</u></i></a> (Guiso et al., 2016)</li></ul><p>HIGHLIGHTS:</p><ul><li>I discuss a gold standard for cultural persistence studies, covering how to (1) identify robust long term correlations via regression studies under different sets of controls, (2) identify causal effects via natural experiments and (3) identify whether culture is a significant mediator via children-of-immigrant studies. <a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.zejhte7r1jeu\"><u>More</u></a></li><li>I find that many of the papers manage to find statistically significant results. A naive aggregation of the estimated correlation effect sizes suggests that future correlational studies might find effects of around \u03b2 \u2248 0.28 (0.13) standard deviations per standard deviation of exposure variation. That is, future studies in similar topics should expect to find that one standard deviation of variation on an event would predict ~28% of variation in long term outcomes. However it is hard to rule out spurious correlations due to issues such as spatial autocorrelation or outliers. <a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.u5d12v739xq2\"><u>More</u></a></li><li>Some of the papers attempt to study causation via natural experiments. While a couple of such papers arguably succeed in identifying a causal effect, we cannot discard that subsequent robustness checks will cast doubt on the results. A naive aggregation of the estimated correlation effect sizes suggests that future causal studies might find effects of around \u03b2 \u2248 0.11 (0.02) standard deviations per standard deviation of exposure variation. That is, future studies in similar topics should expect to find that one standard deviation of difference on an event would cause ~11% of variation in long term outcomes. <a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.58lhfcq7tnn9\"><u>More</u></a></li><li>I find that children-of-immigrant analysis suggests the possibility of long term persistence of variation mediated by parentage. The authors of the papers tend to explain this persistence in terms of cultural variation, relying mostly on historical accounts as evidence. <a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.vjwnawyxy5gx\"><u>More</u></a></li><li>Whether long term persistence of variation usually stays constant, wanes or increases with time is an open question. Studying better these dynamics of persistence would be critical to understand the very long-term impact of cultural interventions today. <a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.sadunm6c75xa\"><u>More</u></a></li></ul><p>&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f75b679183bbe815785a54208e6cfd5077b213953d5b921e.png/w_950 950w\"><figcaption>Table summary of the papers I reviewed. <a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#heading=h.uwdse6103p4l\"><u>More</u></a></figcaption></figure><h2>Acknowledgements</h2><p>This work was funded by the Forethought Foundation, and helped inform the decision of whether to rely on results about cultural persistence in William MacAskill\u2019s forthcoming book.</p><p>Thanks to David Rhys Bernard, Nathan Nunn, Aron Valinder, Luisa Rodriguez, William MacAskill, Florian Penner, Ehud Reiter, Faatima Osman, Pablo Villalobos, Ronja Lutz and Neel Nanda for feedback and constructive criticism.</p><p>Leticia Garc\u00eda provided invaluable research assistance, including carefully reviewing all the analyses conducted in the paper. All remaining mistakes are solely my fault.</p><p>Pablo Villalobos, Ronja Lutz and Neel Nanda assisted me in acquiring and cleaning the data I needed for the analyses, for which I am incredibly grateful.</p><p>David Roodman and Julian Jamison provided a blind peer review of this paper.</p><p>Nu\u00f1o Sempere provided comments on this post.</p><p>I welcome comments and feedback on the piece. <a href=\"https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit#\"><u>Read the rest of the piece.</u></a></p>", "user": {"username": "Jsevillamol"}}, {"_id": "bP6cLB6Dz3cN3YMDA", "title": "How many people should get self-study grants and how can we find them?", "postedAt": "2021-11-10T09:28:54.589Z", "htmlBody": "<p>Most individuals are heavily constrained by income. They're spending the majority of their time and energy on generating an income, which is usually low-impact. Meanwhile, as EA grows, the inferential distance one needs to cover to become able to contribute is growing.&nbsp;<br><br>By \"bridging the inferential gap\", I don't just mean learning the basics of EA, but also putting in the time to find highly impactful ideas for projects and learning the skills required to execute them. I would wager that it is many times easier to bridge this gap if someone is working on it full-time. For the sake of simplicity, I'd like to focus on individuals who are self-starting, i.e. all they need is an income and they can take care of the rest.<br><br>So let's divide self-starting EA's into three buckets.<br>A) the ones who are already working on effective projects full-time through employment or a grant<br>B) the ones who are not working on effective projects, but would put themselves in that position through self-study if they had a year's runway&nbsp;<br>C) the ones who would not be able to work on effective projects even after a year of self-study<br><br>I have three related questions:<br>- How large is bucket B? How large is it relative to bucket A?<br>- How can we identify individuals that belong in bucket B?<br>- If we do identify such an individual, under what conditions is it cost-effective to throw a year's salary at them to let them figure things out?<br>&nbsp;</p>", "user": {"username": "toonalfrink"}}, {"_id": "DCSgPHsDzkF2BNzoB", "title": "EA Slack Groups - Mental Health, SW Eng, Entrepreneurs (and more!)", "postedAt": "2021-11-10T03:35:25.229Z", "htmlBody": "<p>If you haven't quite yet found your niche in EA, maybe one of the following is for you...&nbsp;</p><h1>New EA Slack Groups</h1><ul><li><a href=\"https://join.slack.com/t/eamentalhealth/shared_invite/zt-y9nx933l-jwDgoJoN6jYBIsStDHCuTA\">EA Mental Health</a></li><li><a href=\"https://join.slack.com/t/ea-sw/shared_invite/zt-xz8p3xfa-bT5uRd8ztiMfiz6NQAK6vQ\">Software Engineers in Effective Altruism</a></li><li><a href=\"https://join.slack.com/t/eaentrepreneursgroup/shared_invite/zt-ybbzsfja-nE5_MWw0~ewh_k07Hjzn3Q\">EA Entrepreneurs</a></li></ul><h1>Other resources</h1><h2>Related <a href=\"https://forum.effectivealtruism.org/tags/all\">EA Forum Wiki</a> Tags</h2><ul><li><a href=\"https://forum.effectivealtruism.org/tag/mental-health\">Mental Health</a></li><li><a href=\"https://forum.effectivealtruism.org/tag/software-engineering\">Software Engineering</a></li><li><a href=\"https://forum.effectivealtruism.org/tag/entrepreneurship\">Entrepreneur</a></li></ul><h2>Groups/Directory</h2><ul><li>this list of groups on the EA Hub: <a href=\"https://resources.eahub.org/learn/connect/\">https://resources.eahub.org/learn/connect/</a> and</li><li>this post about the Creatives &amp; Communicators group: <a href=\"https://forum.effectivealtruism.org/posts/3sRZkuQDbss3vwddk/ea-creatives-and-communicators-slack\">https://forum.effectivealtruism.org/posts/3sRZkuQDbss3vwddk/ea-creatives-and-communicators-slack</a></li><li>David Nash's awesome <a href=\"https://www.ealondon.com/group-directory\">directory </a>(more than just London, but mostly FB groups)</li></ul><h1>Offers of Support</h1><p>If you are looking to connect to other EAs (or even outside of EA), but don't know where to start: please <a href=\"https://www.linkedin.com/in/alex-barnes-toronto/\">connect with me on LinkedIn</a> and I'll help you out!<br>&nbsp;</p><h1>Gratitude</h1><p>William, Lizka, Hauke: THANK YOU for the suggestions!<br>&nbsp;</p>", "user": {"username": "--alex--"}}, {"_id": "HmohraBsqhxZkrpeD", "title": "Will Givewell recommend breastfeeding promotion before 2027 [forecast] [crosspost]", "postedAt": "2021-11-09T22:58:40.064Z", "htmlBody": "<p>This is crossposted from Metaculus (I wrote the original question). If you'd like to forecast it, go here:</p><figure class=\"media\"><div data-oembed-url=\"https://www.metaculus.com/questions/8408/givewell-recommend-breastfeeding-promotion/\">\n\t\t\t\t<div data-metaculus-id=\"8408\" class=\"metaculus-preview\">\n\t\t\t\t\t<iframe src=\"https://d3s0w6fek99l5b.cloudfront.net/s/1/questions/embed/8408/?plot=pdf\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><ul><li>Feel free to ask questions (here, there, on my twitter @nathanpmyoung)</li><li>There are forecasting group chats and the<a href=\"https://discord.gg/hNvbGjpnUz\"> metaculus discord</a> if you'd like to forecast more, finding a group to do it with would be my top recommendation</li></ul><hr><h1>Full text of question</h1><p>GiveWell has recommended grants to over 10 charities over the years. They are currently investigating <a href=\"https://docs.google.com/spreadsheets/d/1TG7WRU85p1SEjir-5qvIEg4kVG9a4Lnzdgwcub8aKSs/edit#gid=0\">14 charity areas</a> including breastfeeding promotion.</p><p>The following sections are quoted from GiveWell\u2019s <a href=\"https://www.givewell.org/international/technical/programs/breastfeeding-promotion\">explanation of the topic</a>:</p><p>\u201cThe World Health Organization (WHO) and UNICEF recommend early initiation of breastfeeding, exclusive breastfeeding to 6 months, and partial breastfeeding to age 24 months to improve infant and maternal health, but the majority of infants are not fed according to these guidelines. Mothers may not breastfeed as long or as intensively as they would like due to a lack of skills and support, so various maternal counselling and support interventions promote breastfeeding behaviour change.\u201d</p><p>Effectiveness for the following reasons: \u201cWe believe there is reasonably strong evidence that breastfeeding support programs can lead to increases in rates of exclusive breastfeeding up to 6 months (compared to some or predominant breastfeeding) and breastfeeding duration, as recommended by WHO/UNICEF. Their impacts on exclusive breastfeeding may be larger in low-income countries. There is additional evidence that increasing breastfeeding reduces diarrhoea morbidity, which likely leads to reductions in childhood mortality from diarrhoea. It may also cause additional benefits that we have not yet vetted.\u201d</p><h1><i><strong>Will GiveWell recommend 1 or more grants to support breastfeeding promotion or denote it a \"top charity\" before 2027?</strong></i></h1><p>This question resolves positively if GiveWell publishes a recommendation that grants be given to support breastfeeding promotion or on or before December 31, 2026. Or if a breastfeeding promotion charity is designated a top charity before December 31, 2026.</p><p>This may happen in the following ways:</p><ul><li>GiveWell writes \u201cyes\u201d in the column \u201cHave we recommended one or more grants to support this program?\u201d in the \u201cbreastfeeding promotion\u201d row of the GiveWell <a href=\"https://docs.google.com/spreadsheets/d/1TG7WRU85p1SEjir-5qvIEg4kVG9a4Lnzdgwcub8aKSs/edit#gid=0\">program reviews spreadsheet</a> or the spreadsheet which supersedes this one (see below).</li><li>Givewell publishes a blog in which which they announce they are recommending a grant in the area of breastfeeding promotion</li><li>Givewell adds an incubation grant to a breastfeeding promotion charity on this page <a href=\"https://www.givewell.org/research/incubation-grants\">https://www.givewell.org/research/incubation-grants</a></li><li>A GiveWell blog post announcing that a charity in the area of breastfeeding promotion is a \u201cTop Charity\u201d</li><li>GiveWell publishes a section on breastfeeding promotion on its <a href=\"https://www.givewell.org/charities/top-charities\">Top Charities</a> page</li></ul><h1>Fine print</h1><p>If GiveWell publishes a recommendation that is no longer in force in 2027, this question resolves positively. Eg GiveWell no longer recommends grants to GiveDirectly, but it once did.</p><p>If GiveWell revokes a recommendation due to making errors in its calculations, this question will resolve ambiguously.</p><p>The category \u201cstandout charities\u201d used to exist. It no longer does, so can be ignored.</p><p>If the GiveWell <a href=\"https://docs.google.com/spreadsheets/d/1TG7WRU85p1SEjir-5qvIEg4kVG9a4Lnzdgwcub8aKSs/edit#gid=0\">program reviews spreadsheet</a> is no longer linked from <a href=\"https://www.givewell.org/research\">this page</a>, a superseding spreadsheet may be used. A superseding spreadsheet should have very similar column headings or the same title (\u201cGiveWell programme reviews) or be linked under \u201cprioritized list of programmes\u201d from <a href=\"https://www.givewell.org/research\">this page</a> or failing that, be the consensus choice of admins commenting on the question.</p><p>For clarity, if the above criteria aren\u2019t met, but a reasonable spreadsheet exists which contains a grant for breastfeeding promotion exists, admins can post their suggestions of the spreadsheet in the comments of this question. Admins can change their votes, but at the time of resolution, all admins should have commented in favour of the same spreadsheet. If they have not, and the question does not resolve positively by any of the other methods, the question resolves ambiguously.</p><hr><h1>Some thoughts</h1><ul><li>Could I have written this question better? If so how?</li><li>I am interested in how forecasts can support researchers. If I do this a lot, it will take me about an hour per question. If that provides useful information to researchers it could be very cheap<ul><li>eg If 3 of the cause areas have much higher % forecast, then GiveWell might choose to look at them first. If this is the right call, it could be a useful prioritisation</li></ul></li><li>I think this question is harder to forecast than \"will breastfeeding support be a top charity?\". Someone at Givewell I spoke to said \"any grants\" would be more useful to them, so I went with it. But a more complex question means that forecasters might try and forecast the GiveWell's actions than the impact of the cause area which would be less useful.</li><li>If this gets a lot of takeup I intend to write a question for all of Givewell's potential cause areas (there are about 15)</li><li>Are there EA questions you think could be usefully forecasted?</li></ul>", "user": {"username": "nathan"}}, {"_id": "BH6uKrkdrHDKexmqn", "title": "Give Me Career Advice", "postedAt": "2021-11-09T21:52:25.616Z", "htmlBody": "<p>I find myself asking this often, so I thought I\u2019d write a public post instead. This will be informal and drafty, but it\u2019s split into sections you can skip to.</p><h1>General</h1><p>I\u2019m a software developer with 14+ years of professional experience, recently quit my job hoping to work on something EA related, currently trying out a few side projects, considering a specific job, but also open to waiting for better opportunities to come up.</p><h1>What do I want?</h1><p>People are bad at answering this question correctly, but here\u2019s my attempt:</p><h2>EA / Improving the world</h2><p>TL;DR: Meta causes and rationality.</p><p>I think longtermism is important but that I\u2019m not a good fit for helping there.</p><p>I care about rationality and systematic change.</p><h3>Direct impact, not earning to give</h3><p>This is also a <a href=\"https://twitter.com/benskuhn/status/1374794487784824832\"><u>motivational thing</u></a> for me.</p><h2>Having a good life</h2><h3>No remote work</h3><p>I don\u2019t think I\u2019d enjoy it for social reasons.</p><p>I\u2019m not totally against trying, especially if I have an unusually good connection with the team, but it seems unpromising.</p><p>Another option would be setting up logistics where I\u2019d spend all my time sitting with friends here while working remotely. It's a possibility, but I\u2019ve tried it before, it\u2019s challenging, and now we even have a pandemic.</p><h3>No relocation (I live in Israel)</h3><p>All of my family and social circle are here.</p><h3>Probably not founding a startup</h3><p>A lot of things in this path do attract me, but not the stress.</p><h3>Doing something where I have added value</h3><p>See next section</p><h1>What am I good at + what do I like doing?</h1><h2>Software development</h2><p>I learned how to program when I was 13 and got addicted to it immediately. I've been doing it professionally since age 18, and I\u2019m 32 years old.&nbsp;</p><p>I won\u2019t go over my entire <a href=\"https://www.linkedin.com/in/yonatancale/\"><u>CV</u></a>, but the job that was closest to my heart was being a \u201cCTO As A Service\u201d, which means I was a freelancer who met founders with a startup idea, and I\u2019d build it for them, focusing on how to go to market quickly.&nbsp;</p><h2>Early stage projects (product)</h2><p>The mindset of The Lean Startup and Y Combinator really resonates with me: Launching quickly without overthinking things and getting feedback from the real world.&nbsp;</p><p>On a personal note, this was a big change in me. I used to be the person who teaches software design and creates great plans about how they make everything generic and scalable. No more. Well, unless there\u2019s actually a good reason.</p><h2>Coaching</h2><p>Maybe not a core skill, but I really enjoy it, and I think I'm ok at it.</p><p>I also ask for advice a lot, but I admit this very public post is outside my comfort zone.</p><h1>What am I already doing</h1><p>Beyond taking a break from work for the first time in my life, these are my side projects:</p><h2>Mentoring EA Software Developers (\"Effective Developers\")</h2><p>A new project, see my <a href=\"https://forum.effectivealtruism.org/posts/FkWHn6WaFGzrzqb9P/i-m-offering-free-coaching-for-software-developers-in-the-ea\"><u>post</u></a>.</p><p>On a personal note, younger-me made such uninformed career decisions. Did you know that when I had 8 years of professional experience in c/c++, I wasn\u2019t sure if anyone would accept me as a junior web developer? I wish I could send younger-me a note, but at least I can help my friends at home, and now, EAs too.</p><h2>A volunteers\u2019 board (\u201cimpact colabs\u201d)</h2><p>We connect people looking for projects - with projects looking for people.&nbsp;</p><p>See <a href=\"https://forum.effectivealtruism.org/posts/giKLP5snkAiKGut4t/announcing-the-launch-of-ea-impact-colabs-beta-request-for\"><u>post</u></a>.</p><h2>Looking for the most impactful tech companies in Israel</h2><p>This aims to:</p><ol><li>Give concrete options to software developers that want to improve the world while still working in Israel.</li><li>Create a public discussion about how to measure the impact of a company. Today, the level of discussion is something like \u201cmedical? good!\u201d or \u201cgambling? bad!\u201d. I think this can be improved easily. I also think technical people (who like numbers and graphs) could be good early adopters for this way of thinking, as part of the project of spreading EA in Israel. Also, the Israeli tech community is pretty competent and creative. I have hope that if they\u2019ll be better at measuring what actually improves the world, some promising startups will pop up. Also, I\u2019m a good fit for talking to them.</li></ol><p>There are lots of things to discuss about this (such as \u201cisn\u2019t it really hard to evaluate companies?\u201d).</p><p>I could write a post describing how I think about it if people are interested.</p><p>Bottom lines:&nbsp;</p><ul><li>The recommendations will be nowhere as good as 80000 hours\u2019.</li><li>I still think getting people to talk about measuring impact could be useful. I also think moving some people to the top 1% most effective companies in Israel is doable and still useful.</li><li>I have internal doubts. If you want to discuss (or collaborate on?) this project, let me know.</li></ul><h1><strong>What other options do I have?</strong></h1><h2><strong>Monday\u2019s social impact team - please help me analyze this!</strong></h2><p><a href=\"https://monday.com/\"><u>Monday</u></a> have a good CRM/project-management tool.&nbsp;</p><p>I think they actually care about improving the world.&nbsp;</p><p>They have a brand new social impact team with a huge (!) budget and no developers yet.&nbsp;</p><p>Their vision is to help nonprofits get the most out of technology. [Edit: I removed all the examples]</p><p>Open questions:</p><h3>How useful is it to help a large number of<a href=\"https://www.givingwhatwecan.org/charity-comparisons/\"><u> ineffective charities</u></a>?</h3><p>How to even approach this calculation?</p><h3>Should I expect to find ways to help charities with things (A) they already want and (B) also actually improve the world a lot?</h3><p>For example, if I help them measure ESGs, which isn\u2019t my metric of choice, but I do it at scale, how much impact would that have?</p><h1>How can you help?</h1><h2>Things \u201cI know I don\u2019t know\u201d</h2><ul><li>Help me estimate the Monday option</li><li>Suggest other employment options I didn\u2019t think about</li><li>Find ideas to improve (or reasons to discard?) one of my projects</li></ul><h2>Things \u201cI don\u2019t know I don\u2019t know\u201d</h2><ul><li>Perhaps there are things that I want and I\u2019m not taking into account?</li><li>Maybe there\u2019s a different way of thinking about the problem?</li></ul>", "user": {"username": "hibukki"}}, {"_id": "re6FsKPgbFgZ5QeJj", "title": "Effective strategies for changing public opinion: A literature review", "postedAt": "2021-11-09T14:09:28.893Z", "htmlBody": "<p><i>Many thanks to Janet Pauketat, Courtney Dillard, Jacy Reese Anthis, and Ilana Rudaizky for reviewing and providing feedback.</i></p><h1><strong>ABSTRACT</strong></h1><p>Social movements often try to affect public opinion as a lever for legislation and other social change. This report provides a broad overview of research relating to (1) whether advocates can successfully influence public opinion and how they can do so most effectively, and (2) what the other causes of public opinion change are and how advocates can harness them. Key findings include that direct persuasion attempts to change attitudes (rather than behavior) tend to have \u201csmall\u201d or \u201cvery small\u201d short-term effects,&nbsp;but advocates may be able to have lasting indirect effects on public opinion via policy change or reframing the issues. We list factors that affect&nbsp;how persuasive messages are&nbsp;and make tentative suggestions for how advocates can cost-effectively leverage external influences on public opinion such as the media, celebrities, politicians, and policy.</p><h1><strong>INTRODUCTION</strong></h1><p>Public opinion&nbsp;\u2014 \u201cthe preferences of the adult population on matters of relevance to government\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt1\"><sup>[1]</sup></a>&nbsp;\u2014 is important for social change: While historical causation is always difficult to assess, our previous research projects indicate a significant effect of public opinion on legislative outcomes.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt2\"><sup>[2]</sup></a>&nbsp;Political&nbsp;scientist Alan Monroe (1998) found that in 70% of policy decisions between 1980 and 1993 in which the public favored the status quo, the US government maintained it.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt3\"><sup>[3]</sup></a>&nbsp;Sentience Institute\u2019s \u201cSummary of Evidence for Foundational Questions in Effective Animal Advocacy\u201d lists a number of other instances where public opinion could be important (citations omitted):</p><ul><li>\u201cPublic opinion could play an important role in affecting whether legislation is preserved or subsequently overturned\u2026</li><li>The decision-making of the Supreme Court of the United States seems to be substantially influenced by public opinion.</li><li>When pre-decision public opinion&nbsp;is more closely aligned with a Supreme Court decision, the risk of legislative backlash is lower and the effects of the ruling on public opinion seem likely to be more positive\u2026</li><li>Corporate welfare campaigns have been partly dependent upon mobilizing the public to express dissatisfaction with a particular practice in animal farming, such as the caging of layer hens. It may be crucial that the public is already opposed to a practice for such campaigns to be successful.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt4\"><sup>[4]</sup></a></li></ul><p>Hence, although much animal advocacy research has treated behavior change, especially individual diet change,&nbsp;as the main outcome of interest,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt5\"><sup>[5]</sup></a>&nbsp;understanding the causes of public opinion change also seems useful for cost-effectively encouraging social change.</p><p>The research reviewed here is relevant for evaluating the usefulness of moral circle expansion (i.e., advocacy to increase the number of beings given moral consideration, such as legal protection) and other forms of values spreading&nbsp;as strategies for influencing the long-term future. The impact of these strategies rests on fundamental questions, such as<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt6\"><sup>[6]</sup></a>:</p><ul><li>Does advocating for a value system increase or reduce the number of people with these values?</li><li>Do persuaded individuals take action in support of their new values, such as further advocacy?</li><li>Do the actions of persuaded individuals tend to align with the intentions of the original advocates?</li><li>Will the values that are spread be durable or will they revert to an equilibrium?</li></ul><h2><strong>SUMMARY OF STRATEGIC IMPLICATIONS</strong></h2><p>Below are a number of strategic claims supported by the evidence in this review:</p><ul><li>Direct advocacy and persuasive messages focused on&nbsp;attitudes&nbsp;seem likely to have effect sizes conventionally interpreted as \u201csmall\u201d or \u201cvery small.\u201d Though not the norm, persuasive messages can sometimes cause attitudes to change in the opposite direction to intended (\u201cboomerang\u201d attitude change), e.g. when the difference between the message\u2019s position and the receiver\u2019s position is very large.</li><li>There are a number of communicator, message, and receiver factors that have small or moderate effects on how persuasive messages are.&nbsp;For example:</li><li>More communicator credibility (expertise, trustworthiness, and caring), likeability, attractiveness, communicator-receiver similarity, and authority usually enhance persuasiveness. These factors tend to be more influential when the receiver is not thinking carefully about the issue, e.g. if it is of low relevance to them.</li><li>Advocates can increase their effectiveness if they make emotional appeals&nbsp;to fear, guilt, anger, or disgust. However, these tactics can reduce message effectiveness or lead to boomerang effects if they are used&nbsp;incorrectly&nbsp;or excessively.</li><li>Describing but refuting opposing arguments can increase the effectiveness of persuasive messages and make audiences more resistant to opposing persuasion attempts.</li><li>Persuasive messages tend to be more effective if the audience is less familiar with or opinionated about an issue.&nbsp;This suggests that advocates can influence public opinion towards less salient attributes or sub-topics (e.g. a specific farming practice) if they are able to shape the initial media coverage or share persuasive messages widely. It also suggests that tactics that are aimed at increasing salience may be counterproductive (by making subsequent attitude change more difficult) on issues where public opinion is currently unfavorable.</li><li>Framing variations can influence attitudes through different mechanisms to direct persuasion attempts by encouraging audiences to place more weight on some considerations than others.</li><li>Protests and social movement events can&nbsp;influence public opinion as well as the public\u2019s perceptions of the importance of certain issues.</li><li>The media can cause public opinion change through persuasive messaging or one-sided coverage. Additionally, if it highlights certain attributes of an issue over others, the media can influence overall public opinion without persuasion, similarly to how advocates can influence attitudes through framing manipulations.&nbsp;If advocates can focus media attention on more favorable attributes of an issue, they may be able to indirectly influence public opinion.</li><li>High media coverage tends to make the public think that an issue is more important.</li><li>Politicians and celebrities can act as advocates, influencing the attitudes of their audience, though they are not always more persuasive than alternative spokespeople. They can also influence media coverage and public perceptions of the importance of certain topics or attributes. This suggests that if advocates can encourage supportive public comments from these individuals, this could be worthwhile, but efforts would be counterproductive if they encouraged hostile comments.</li><li>If advocates can secure policy change, public opinion will tend to move towards support for those policies.</li><li>Intergroup contact&nbsp;and several other prejudice reduction strategies can encourage moral consideration of outgroups, though it may be difficult for advocates to cost-effectively utilize these strategies on a large scale.</li><li>The effects on attitudes from exposure to single messages \u2014 whether from persuasive messaging, framing manipulations, or media coverage \u2014 are most pronounced immediately after exposure to the message. The effects diminish with time, though at least some effects have been found weeks afterwards.&nbsp;For advocates to cost-effectively cause lasting&nbsp;public opinion change, they therefore probably need to encourage some sort of self-perpetuating mechanism such as new legislation, social norms, or framings used by the media.</li><li>A number of indirect or long-term factors can influence public opinion, such as demographic changes and major events beyond the control of advocates.</li></ul><h1><strong>METHODOLOGY</strong></h1><p>Search terms&nbsp;were inputted into Google Scholar, seeking to identify meta-analyses, textbooks, and summary articles relevant to the causes of public opinion change, especially those that might be relevant for advocacy strategy. The citations of and by some of the most important and relevant items were also searched. There were no strict inclusion or exclusion criteria. For example, although the focus was usually on meta-analyses and textbooks, if these were not available for topics of interest, individual studies were sometimes sought. Items were included if they seemed useful and relevant in some way to the topic, even if they did not meet a certain definition of the term \u201cpublic opinion\u201d (of which there are several<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt7\"><sup>[7]</sup></a>). For example, this review does not usually distinguish between attitudes and public opinion, so includes research on both.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt8\"><sup>[8]</sup></a>&nbsp;Research on the causes of changes in less relevant outcomes (e.g. behavior or knowledge) was usually excluded, but sometimes discussed briefly for comparison to research on public opinion.</p><p>This topic is too broad to be suited to a single systematic review and the outcomes used are too diverse for an overview of reviews to be appropriate either.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt9\"><sup>[9]</sup></a>&nbsp;This report does not attempt to comprehensively review the literature on any specific topic, but rather to identify and summarize a variety of research findings of interest to advocates and researchers of social change.&nbsp;The reviewed research comes from a number of disciplines, including communication studies, political science, psychology, and sociology.</p><p>When reporting on the effect sizes from included meta-analyses, the metrics provided by the original reviewers are used, most commonly standardized mean difference (SMD) / Cohen\u2019s <i>d</i>&nbsp;and Pearson\u2019s <i>r</i>&nbsp;correlation coefficients. Guidelines exist for interpreting some of these different forms of outcome measures, most notably Jacob Cohen\u2019s definitions of <i>r</i>&nbsp;= 0.1 or <i>d</i>&nbsp;= 0.2 as a \u201csmall effect size,\u201d <i>r</i>&nbsp;= 0.3 or<i>&nbsp;d</i>&nbsp;= 0.5 as a \u201cmedium effect size,\u201d and <i>r</i>&nbsp;= 0.5 or <i>d</i>&nbsp;= 0.8 as a \u201clarge effect size.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt10\"><sup>[10]</sup></a></p><h1><strong>FINDINGS</strong></h1><h2><strong>CAN ADVOCATES SUCCESSFULLY INFLUENCE PUBLIC OPINION AND IF SO, HOW CAN THEY DO SO MOST EFFECTIVELY?</strong></h2><p>This section focuses on tactics that advocates can use to influence public opinion fairly directly, e.g. via direct advocacy and persuasion; opportunities for more indirect influence are reviewed in the following section.</p><h3><strong>Direct advocacy and persuasion</strong></h3><p>Although findings are mixed, a number of studies suggest that social movements can influence public opinion in the direction that they intend through public campaigns and persuasive messaging.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt11\"><sup>[11]</sup></a>&nbsp;For example, several studies on anti-abortion and anti-death penalty attitudes suggest that educational interventions can have positive effects, but the effects may be small and short-term.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt12\"><sup>[12]</sup></a></p><p>Rains et al. (2018) review and summarize \u201c149 meta-analyses exploring human communication phenomena\u201d (not all focusing on attitude change) which have a mean effect size of <i>r</i>&nbsp;= .21, a small effect size.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt13\"><sup>[13]</sup></a>&nbsp;Marketing scholar Jacob Hornik and colleagues (2016) combined 2,276 different effect sizes from advertising studies in meta-analysis and found a mean weighted effect size on persuasion outcomes&nbsp;(including \u201cattitude toward the product, attitude toward the brand, purchase intention, and product choice\u201d) of <i>r</i>&nbsp;= .19.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt14\"><sup>[14]</sup></a>&nbsp;Although neither of these reviews focused exclusively on attitude outcomes, meta-analyses with a narrower focus on attitude change tend to find small or very small effects.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt15\"><sup>[15]</sup></a>&nbsp;For example:</p><ul><li>A&nbsp;meta-analysis of 36 experiments found that \u201ccommunicating evidence of a policy\u2019s effectiveness increased support for the policy (SMD = 0.11, 95% CI [0.07, 0.15], p &lt; 0.0001)\u201d and \u201c[c]ommunicating evidence of ineffectiveness decreased policy support (SMD = \u22120.14, 95% CI [\u22120.22, \u22120.06], p &lt; 0.001).\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt16\"><sup>[16]</sup></a></li><li>A meta-analysis found significant effects on attitudes from 10 experiments and quasi-experiments of descriptive social norms manipulations, which focus on \u201cthe perceived prevalence of a behavior\u201d (<i>d </i>= 0.17, 95% CI [0.06, 0.27]) and 4 studies of injunctive norms manipulations, which focus on \u201cthe social (dis)approval of one\u2019s actions\u201d (<i>d </i>= 0.34, 95% CI [0.17, 0.50]).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt17\"><sup>[17]</sup></a></li><li>A meta-analysis of 30 studies found that \u201c[f]act-checking has a significantly positive overall influence on political beliefs (<i>d</i>&nbsp;= 0.29).\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt18\"><sup>[18]</sup></a></li><li>A meta-analysis of 49 field experiments of political campaign advertising and outreach actually found an average effect of zero on US voting choices.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt19\"><sup>[19]</sup></a>&nbsp;</li></ul><p>Many of these effects come from studies using only short messages in artificial contexts, so they may not be very informative about the effects we should expect from real-world advocacy contexts. More extensive interactions (e.g. a lengthy conversation, a documentary, a book) or repeated exposure to similar arguments (e.g. via shifts in media coverage, discussed below) may have larger effects.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt20\"><sup>[20]</sup></a></p><p>However, there is evidence from a number of meta-analyses on health behavior (e.g. persuading people to eat more healthily, exercise more, or smoke less) that education or information only interventions, social norms interventions, mass media campaigns, social marketing, and advertising also tend to have small or very small effect sizes.&nbsp;These studies tend to focus on more realistic interventions, such as lessons conducted over a number of weeks in schools or large-scale mass media campaigns visible to the general public.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt21\"><sup>[21]</sup></a>&nbsp;Attitude and behavior change are correlated,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt22\"><sup>[22]</sup></a>&nbsp;so this provides a rough guide for expected effect sizes of changes in attitudes from targeted individuals in comparable interventions focused on public opinion. Some reviews of mass media campaigns find changes in attitudes that are larger than the changes in behaviors, though they are still sometimes very small.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt23\"><sup>[23]</sup></a>&nbsp;Even if advocates are able to provide repeated persuasive messages to their audience, encouraging attitude change is difficult.</p><p><strong>Moderators</strong></p><p>If advocates design and deliver messages in the most effective manner possible, the effects on attitudes may still be small.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt24\"><sup>[24]</sup></a>&nbsp;Nevertheless, there are a number of factors that make small or moderate differences to the effectiveness of persuasive messages.</p><p><i><strong>Theories of persuasion moderators</strong></i></p><p>In chapters 2 to 8 of <i>Persuasion: Theory and Research </i>(2015), communication scholar Daniel J. O\u2019Keefe outlines various theories that help to explain outcomes from persuasive messages and the factors that moderate how effective they are.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt25\"><sup>[25]</sup></a>&nbsp;For example:</p><ul><li>Chapter 2 describes \u201csocial judgement theory,\u201d where, \u201cthe effect of a persuasive communication depends upon the way in which the receiver evaluates the position it advocates.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt26\"><sup>[26]</sup></a>&nbsp;A key prediction of this theory, supported by numerous studies, is that the relationship between message discrepancy (\u201cthe difference between the message\u2019s position and the receiver\u2019s position\u201d) and attitude change \u201cis suggested to be something like an inverted-U-shaped curve.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt27\"><sup>[27]</sup></a>&nbsp;In other words, advocates need to carefully evaluate how radical to make their messages in order to achieve maximum effect \u2014 too conservative and the attitude change produced will be in the intended direction but small, too radical and the attitude change produced could be small, non-existent, or even in the opposite direction to intended (\u201cboomerang\u201d attitude change).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt28\"><sup>[28]</sup></a>&nbsp;The ideal message will vary depending on the audience and their current view.&nbsp;</li><li>Social judgement theory also predicts that&nbsp;when someone has less extreme views on a topic, e.g. because it has little personal relevance to them, a persuader might be able to successfully advocate for a more discrepant position.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt29\"><sup>[29]</sup></a>&nbsp;This suggests that persuasion attempts relating to less salient topics&nbsp;will be more tractable, as will those relating to public policy and other institutional changes&nbsp;rather than, say, a recipient\u2019s dietary choices.&nbsp;However, a number of strategies may facilitate effective persuasion for audiences with strong attitudes.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt30\"><sup>[30]</sup></a></li><li>Chapter 3 describes \u201cfunctional approaches to attitude,\u201d where, \u201c[t]he basic idea is that attitudes may serve various functions.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt31\"><sup>[31]</sup></a>&nbsp;This is relevant because \u201csubstantial evidence suggests that persuasive appeals that are matched to the receiver\u2019s attitude function will be more persuasive than mismatched appeals.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt32\"><sup>[32]</sup></a>&nbsp;This implies that it is useful to understand why your audience holds certain attitudes and tailor messages towards their attitude function where possible.</li><li>Chapter 5 summarizes and evaluates cognitive dissonance theory which entails that attitudes can change because \u201cpersons seek to maximize the internal psychological consistency of their cognitions (beliefs, attitudes, etc.).\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt33\"><sup>[33]</sup></a>&nbsp;The theory has many implications for persuasion. One key point is that people may resolve cognitive dissonance \u2014 discomfort from conflicting thoughts \u2014 in a number of ways, e.g. by changing their attitudes to justify their new behavior (if you manage to change behavior without changing attitudes first), or changing their behavior to bring it in line with their new attitudes (if you manage to persuade them of something).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt34\"><sup>[34]</sup></a>&nbsp;This provides an explanation for why attitude change relating to personal behaviors is so hard \u2014 people will want their attitudes to match their behaviors, so may reject arguments that you present to them.</li><li>Chapter 8 analyzes the Elaboration Likelihood Model (ELM), which \u201csuggests that important variations in the nature of persuasion are a function of the likelihood that receivers will engage in elaboration of (that is, thinking about) information relevant to the persuasive issue.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt35\"><sup>[35]</sup></a>&nbsp;There is evidence that, \u201cattitudes shaped under conditions of high elaboration will (compared with attitudes shaped under conditions of low elaboration) display greater temporal persistence, be more predictive of intentions and subsequent behavior, and be more resistant to counterpersuasion.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt36\"><sup>[36]</sup></a>&nbsp;All else being equal, it is clearly preferable to encourage attitude change under conditions of high elaboration, though this may be more difficult. There is also evidence supporting ELM\u2019s implication that, under conditions of low elaboration, such as when a topic has little personal relevance to the receiver,&nbsp;heuristics such as the communicator\u2019s perceived credibility (discussed below) become especially important as determinants of attitude change, relative to argument quality.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt37\"><sup>[37]</sup></a></li></ul><p><i><strong>Communicator factors</strong></i></p><p>There are a number of \u201ccommunicator factors\u201d that make persuasive efforts more or less effective. These have been identified in numerous studies and summarized in chapter 10 in O\u2019Keefe\u2019s <i>Persuasion </i>(2015) and chapter 8 in <i>The Dynamics of Persuasion </i>(2017), another textbook on persuasion by communications scholar Richard Perloff<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt38\"><sup>[38]</sup></a>:</p><ul><li>Although high credibility communicators tend to be more persuasive, surprisingly, \u201cat least sometimes low-credibility communicators are significantly more effective than high-credibility communicators.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt39\"><sup>[39]</sup></a>&nbsp;One notable finding is that, \u201c[w]ith a counterattitudinal message [i.e. one which opposes the receiver\u2019s current view], the high-credibility communicator will tend to have a persuasive advantage over the low-credibility source; with a&nbsp;proattitudinal message, however, the low-credibility communicator appears to enjoy greater persuasive success than the high-credibility source.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt40\"><sup>[40]</sup></a>&nbsp;The perceived \u201ccredibility\u201d of the communicator is influenced by their perceived \u201cexpertise,\u201d e.g. education, occupation, and experience; citation of evidence sources (rather than providing vague claims); and \u201cnonfluencies in delivery\u201d (like saying \u201cuh\u201d a lot).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt41\"><sup>[41]</sup></a>&nbsp;Credibility is also influenced by perceived trustworthiness<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt42\"><sup>[42]</sup></a>&nbsp;and perceived caring (goodwill), though the latter is less consistently identified as important.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt43\"><sup>[43]</sup></a>&nbsp;Credibility is especially important under conditions of low elaboration and if the communicator\u2019s identity is clear before a message is delivered.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt44\"><sup>[44]</sup></a></li><li>As with credibility, communicators who are more liked by their audience are usually more persuasive, especially under conditions that prompt low elaboration. Again, however, sometimes disliked communicators are more effective, typically when the audience has \u201cfreely chosen to listen to the message.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt45\"><sup>[45]</sup></a>&nbsp;There is also evidence that, \u201c[t]he effects of liking can apparently be overridden by credibility.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt46\"><sup>[46]</sup></a></li><li>Physical attractiveness and communicator-receiver similarity can both affect persuasiveness, but operate indirectly such as through effects on liking and credibility.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt47\"><sup>[47]</sup></a></li><li>Authority can induce compliance (i.e. outward conformity), but not necessarily private acceptance.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt48\"><sup>[48]</sup></a>&nbsp;It is unclear whether compliance will be sufficient for advocacy purposes.</li></ul><p>There are many complexities affecting when being evaluated positively on each of these factors is more or less useful. As a general rule, however, more credibility (expertise, trustworthiness, and caring), likeability, attractiveness, communicator-receiver similarity, and authority usually enhance persuasiveness.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt49\"><sup>[49]</sup></a></p><p><i><strong>Message factors</strong></i></p><p>Scholars have also identified a number of \u201cmessage factors\u201d that affect persuasiveness:</p><ul><li>Narratives can sometimes be more persuasive than nonnarrative messages, especially if the audience identifies with the characters or is \u201ctransported by\u201d (\u201ccaught up in, or carried away by\u201d) the story.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt50\"><sup>[50]</sup></a>&nbsp;</li><li>The use of evidence also enhances persuasion, either by improving the strength of the arguments or by acting as a cue of the communicator\u2019s credibility.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt51\"><sup>[51]</sup></a>&nbsp;Meta-analyses examining whether statistical evidence or narratives are more effective have come to conflicting conclusions.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt52\"><sup>[52]</sup></a>&nbsp;Statistical evidence may be most effective in specific contexts, e.g. under conditions of high elaboration.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt53\"><sup>[53]</sup></a></li><li>More broadly, one meta-analysis has found that \u201cvividness\u201d of persuasive appeals has small positive effects on attitudes<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt54\"><sup>[54]</sup></a>&nbsp;and another has found that emotional appeals tend to be more persuasive than rational appeals in advertising.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt55\"><sup>[55]</sup></a>&nbsp;</li><li>A meta-analysis of 38 studies found that, overall, metaphors have a very small persuasive advantage over literal messages (r = .07), which rose to a moderate effect size (r = .42) \u201cunder optimal conditions, when a single, nonextended metaphor was novel, had a familiar target, and was used early in a message.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt56\"><sup>[56]</sup></a></li><li>Intense language may be effective at enhancing enthusiasm among supporters, but ineffective at persuading those with contrasting views.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt57\"><sup>[57]</sup></a></li><li>There are usually stronger effects for messages that explicitly state their recommendation, rather than omitting it.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt58\"><sup>[58]</sup></a>&nbsp;Relatedly, \u201cmessages with more specific descriptions of the recommended action are more persuasive than those providing general, nonspecific recommendations.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt59\"><sup>[59]</sup></a></li><li>Fear appeals can be effective \u2014 they tend to have small positive effects on attitudes, intentions, and behaviors.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt60\"><sup>[60]</sup></a>&nbsp;However, to have the intended effects, they need to first successfully \u201cconvince message recipients that they are susceptible to negative outcomes,\u201d that \u201cthe recommended response will alleviate the threat,\u201d and that the recipients are capable of achieving the recommended response.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt61\"><sup>[61]</sup></a>&nbsp;They also must not induce so much fear that recipient becomes incapacitated.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt62\"><sup>[62]</sup></a></li><li>Guilt appeals may be even more effective \u2014 one meta-analysis found \u201ca strong positive overall effect of guilt (r = .49, 95% CI 0.31\u20130.64)\u201d on \u201chealth-related attitudes and intentions.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt63\"><sup>[63]</sup></a>&nbsp;Like fear appeals, they may be effective \u201conly if certain conditions are met. Research suggests that guilt appeals can work if: (a) the message induces empathy, (b) instills a sense of social, normative responsibility to help, and (c) convinces individuals that the recommended behavior will reduce guilt or repair the problem. However, if the message goes too far, eliciting reactance [a perception of threat] and making people angry, it can backfire.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt64\"><sup>[64]</sup></a>&nbsp;There is evidence that encouraging shame may also be persuasive, though perhaps more likely to backfire than encouraging guilt.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt65\"><sup>[65]</sup></a></li><li>A meta-analysis of 36 studies found no significant overall effects of using intentional evocations of anger in persuasive messaging focused on attitudes. However, there were small positive effects when the anger evoked was relevant to the message and when the message was combined with strong arguments or an appeal that reassures the individuals that the recommended behavior will address the problem.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt66\"><sup>[66]</sup></a></li><li>Some studies (including two focused on opposition to animal exploitation) suggest that disgust may enhance persuasion.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt67\"><sup>[67]</sup></a>&nbsp;However, as with guilt and fear appeals, other studies suggest negligible or counterproductive effects.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt68\"><sup>[68]</sup></a></li><li>Although persuasive messages that explicitly encourage positive emotions such as pride and joy are less well studied than those that encourage negative emotions like fear and guilt, such messages may also be effective.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt69\"><sup>[69]</sup></a></li><li>Hornik et al.\u2019s (2016) meta-analysis of advertising studies found that humor had the second largest persuasive effect size of the seven tested appeal types (<i>r</i>&nbsp;= .35), above fear appeals and various more rational persuasion types.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt70\"><sup>[70]</sup></a>&nbsp;A more recent meta-analysis focusing on a broader range of persuasion studies (not limited to advertising) found that the use of humor in persuasion has significant effects on knowledge (<i>r </i>= .23, k = 29), attitudes (<i>r</i>&nbsp;= .12, k = 58), and behavioral intent (<i>r</i>&nbsp;= .09, k = 29). However, the effects were insignificant and close to zero for each of these outcomes for both \u201cpolitical topics (k = 21), such as gun control and social security\u201d and \u201chealth topics (k = 27), such as cervical cancer and mouth hygiene,\u201d suggesting that humor is not likely to make messages relating to serious ethical issues either consistently more or less persuasive.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt71\"><sup>[71]</sup></a>&nbsp;That said, if humor helps to increase public attention to a persuasive message (even if it does not increase its persuasiveness per se), then it could sometimes still be useful as long as it does not trivialize the movement or have other indirect negative effects.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt72\"><sup>[72]</sup></a></li><li>Similarly, Hornik et al. (2016) found sex appeals to have the largest persuasive effect size of all tested advertising appeal types (<i>r </i>= .46),<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt73\"><sup>[73]</sup></a>&nbsp;but communications scholar Ronald D. Smith (2017) notes that a \u201cconsistent finding from persuasion research is that sex appeal should not be used simply for shock value. It is far more effective when the sexual theme has a legitimate association with the product (such as lingerie, perfume, or condoms) or with the cause (such as birth control or responsible sexual behavior).\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt74\"><sup>[74]</sup></a>&nbsp;A similar effect may occur in social advocacy. Indeed, a study testing the effects of PETA advertisements suggests that sexual ads will backfire for animal advocates.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt75\"><sup>[75]</sup></a></li><li>Fast speech rate may enhance persuasion by acting as a cue of credibility under conditions of low elaboration and may capture attention. But it may reduce the communicator\u2019s apparent goodwill (so could be counterproductive for sensitive issues)<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt76\"><sup>[76]</sup></a>&nbsp;and comprehension by the audience.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt77\"><sup>[77]</sup></a></li><li>There are a number of factors that can make imagery persuasive,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt78\"><sup>[78]</sup></a>&nbsp;though a meta-analysis of 12 studies found that, overall, adding \u201cvisual images to verbal texts had no significant effect on persuasion.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt79\"><sup>[79]</sup></a></li><li>\u201cRefutational two-sided messages\u201d (those which describe but refute opposing arguments) \u201care dependably more persuasive than one-sided messages; nonrefutational two-sided messages, on the other hand, are slightly less persuasive than their one-sided counterparts.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt80\"><sup>[80]</sup></a>&nbsp;There are a number of caveats and exceptions where one-sided messages can be more effective.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt81\"><sup>[81]</sup></a></li></ul><p>Relatedly, \u201cshowing receivers refutations of weak opposing arguments makes receivers more resistant to persuasion (by subsequent attack messages) than they otherwise would have been.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt82\"><sup>[82]</sup></a>&nbsp;This \u201cinoculation\u201d strategy is one of several possible strategies for preventing unwanted attitude change. A meta-analysis suggests that, \u201cproviding people with arguments and information supporting their current views\u201d can confer resistance to persuasion, but that these \u201csupportive\u201d treatments are less effective than \u201cinoculation.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt83\"><sup>[83]</sup></a>&nbsp;Warning a receiver that they are about to hear a message intended to persuade them also stimulates resistance to persuasion.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt84\"><sup>[84]</sup></a></p><p>Although not easily employed to alter public opinion, there are a number of techniques that have been found to increase the effectiveness of&nbsp;interpersonal persuasion efforts&nbsp;aimed at changing behaviors, such as the \u201cfoot-in-the-door,\u201d \u201cdoor-in-the-face,\u201d \u201cthat\u2019s-not-all,\u201d \u201clow-balling,\u201d \u201cfear-then-relief,\u201d \u201cbut-you-are-free,\u201d and \u201cdisrupt-then-reframe\u201d techniques.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt85\"><sup>[85]</sup></a>&nbsp;Techniques focused on persuasion within groups could potentially be applied at the level of whole organizations or even social movements, such as the finding that a member of a group with a minority view can sometimes persuade the majority to change their view by either \u201cconforming with the group and then deviating\u201d or \u201cconsistently disagreeing with the group.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt86\"><sup>[86]</sup></a>&nbsp;There is also evidence that certain advertising techniques \u2014 such as increased exposure to a product and association between the product and certain images or attributes \u2014 can positively influence attitudes toward the product. The success of these techniques is influenced by a variety of factors, but it seems plausible that they could sometimes be employed to influence public opinion towards policies or social issues.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt87\"><sup>[87]</sup></a></p><p><i><strong>Other factors</strong></i></p><p>Persuasion can also be affected by \u201creceiver factors\u201d such as demographic factors or the receiver\u2019s mood,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt88\"><sup>[88]</sup></a>&nbsp;which will often be difficult for advocates to account for but suggest that optimal persuasive messages will be tailored to their audiences.&nbsp;Some of this research has more generalizable advocacy-relevant implications. For example:</p><ul><li>It may sometimes be possible to encourage receptivity to persuasive messages before sharing the message itself, such as by asking people to reflect on their values.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt89\"><sup>[89]</sup></a>&nbsp;</li><li>\u201cReactance\u201d is a state created by a perception of threat to the receiver\u2019s freedom, which makes the receiver more likely to reject a view advocated to them. It can be reduced by avoiding forceful claims (e.g. \u201cit is impossible to deny all the evidence\u201d) and emphasizing the receiver\u2019s freedom of choice.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt90\"><sup>[90]</sup></a></li><li>It is harder to persuade someone if they are already knowledgeable about the topic.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt91\"><sup>[91]</sup></a>&nbsp;This suggests that persuasive interventions focusing on professionals with relevant expertise are less likely to result in attitude change than interventions focusing on the general population. It also suggests that first impressions matter; advocates should ensure that awareness-raising tactics are persuasive to their audience, since it will be harder to change attitudes once knowledge has increased.</li></ul><p>Psychologists Stuart Oskamp and P. Wesley Schultz (2005) summarize research findings as demonstrating that, \u201c[p]rint media (books, magazines, and newspapers)... produce better comprehension and retention of complex material [as well as higher attitude change] than other media, but that this advantage does not hold for simple material\u2026 [P]eople\u2019s knowledge of current affairs is more closely related to their use of print media.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt92\"><sup>[92]</sup></a>&nbsp;They also summarize that, \u201c[t]here is general agreement that personal communication usually has a stronger influence on people\u2019s attitudes and behavior than does mass communication.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt93\"><sup>[93]</sup></a>&nbsp;Of course, this does not necessarily mean that efforts to alter public opinion through personal communication will be more cost-effective, since they may also be more resource-intensive.</p><p>There is a substantial amount of research demonstrating a \u201cthird-person effect,\u201d&nbsp;where people believe that a persuasive message will have a stronger influence on others than on themselves. For example, Paul et al.\u2019s (2000) meta-analysis found an effect size of r = 0.50, i.e. substantially \u201cgreater perceived effects on others than on oneself,\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt94\"><sup>[94]</sup></a>&nbsp;though Sun et al. (2008) found a smaller effect size (<i>d</i>&nbsp;= 0.65).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt95\"><sup>[95]</sup></a>&nbsp;In some contexts, it may be sufficient for advocates to encourage the perception that an issue is important or an attitude is widely held;<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt96\"><sup>[96]</sup></a>&nbsp;the third-person effect suggests that doing so is tractable.</p><p><strong>Duration</strong></p><p>When advocates successfully encourage attitude change, a substantial proportion of the change may endure for at least several weeks, as Oskamp and Schultz (2005) summarize:</p><blockquote><p>After 4 to 6 weeks, the amount of attitude change retained may be from one-third to two-thirds of the initial change, which of course may have been small to begin with. In a study of five different TV documentaries shown to college students, Fitzsimmons and Osburn (1968) found that only one retained a significant attitudinal effect after 4 weeks. However, many experiments have found attitude changes lasting as long as 6 months, and a very impressive classroom study by Rokeach (1971) showed significant attitude changes lasting well over one year.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt97\"><sup>[97]</sup></a></p></blockquote><p>This is promising, given that, \u201call of these findings stem from studies in which the persuasive message was delivered only once\u201d and that, \u201c[r]esearch has shown that repeated re-exposures to a persuasive message will strengthen and prolong any prior opinion change.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt98\"><sup>[98]</sup></a>&nbsp;More recently, political scientist Seth J. Hill and colleagues have noted that, \u201c[s]cholars do not usually test for the duration of the effects of mass communication.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt99\"><sup>[99]</sup></a>&nbsp;Nevertheless, a \u201chandful of recent studies [have] found that persuasion effects can be quite shortlived, decaying in a few weeks or even a few days.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt100\"><sup>[100]</sup></a>&nbsp;A review of nine brief interventions intended to reduce implicit racial preferences found that \u201call nine interventions immediately reduced implicit preferences\u201d but \u201cnone were effective after a delay of several hours to several days.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt101\"><sup>[101]</sup></a>&nbsp;Two experimental studies of a documentary \u201cthat presents the health, environmental, and animal welfare motivations for reducing consumption of meat and animal products\u201d found that it \u201cdid not meaningfully affect any of the\u2026 exploratory attitude outcomes\u201d at follow-up after two weeks.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt102\"><sup>[102]</sup></a>&nbsp;Hill et al.\u2019s own study (which uses political advertising data rather than an experimental design) and a subsequent experiment suggest that over half of the attitude change observed from persuasive interventions decayed within weeks, but that non-negligible proportions of the effects lasted for longer periods.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt103\"><sup>[103]</sup></a>&nbsp;Regression across numerous studies of social norms manipulations found no significant effects of \u201c[t]he time in days between message exposure and final assessment\u201d on attitudes outcomes.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt104\"><sup>[104]</sup></a></p><p>A natural experiment&nbsp;from Germany suggests that one-sided persuasion campaigns may have effects on social norms, beliefs, and behaviors&nbsp;that last for years.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt105\"><sup>[105]</sup></a>&nbsp;An observational study&nbsp;found that viewers of an advertisement designed to increase opposition to Canadian seal hunts still had significantly higher opposition two months after exposure than beforehand, though it had levelled off relative to their opposition immediately after viewing.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt106\"><sup>[106]</sup></a>&nbsp;This provides some evidence that persuasive interventions may cause long-term attitude change, but it also seems clear that most of the attitude change they cause will be temporary. For advocates to cost-effectively cause long-term public opinion change, they therefore probably need to encourage some sort of self-perpetuating mechanism such as new legislation, social norms, or framings used by the media.</p><p><strong>Prejudice reduction strategies</strong></p><p>Research on advocacy and persuasion efforts that focus specifically on reducing prejudice is especially relevant to social movements targeting moral circle expansion. Psychologist Levy Paluck and colleagues (2021) reviewed and meta-analyzed \u201c418 experiments reported in 309 manuscripts from 2007 to 2019 to assess which approaches work best and why.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt107\"><sup>[107]</sup></a>&nbsp;Table 1 is a summary of their findings.</p><p>&nbsp;</p><p>Table 1: Effects of prejudice reduction strategies, summarized from Paluck et al. (2021)<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt108\"><sup>[108]</sup></a></p><figure class=\"table\"><table style=\"background-color:rgb(255, 255, 255);border-bottom:1px solid rgb(33, 33, 33);border-right:1px solid rgb(33, 33, 33)\"><tbody><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\"><strong>Intervention</strong></td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\"><strong>Definition</strong></td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\"><i><strong>d</strong></i></td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\"><strong>LL</strong></td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\"><strong>UL</strong></td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\"><strong>n</strong></td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Entertainment</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">\u201c[E]ntertainment interventions have tested interactive narratives that allow individuals to participate in the construction of stories about outgroups, films made by and for Black audiences, pro-integration music lyrics, and entertainment education that incorporates educational messages about prejudice into an entertaining storyline of a soap opera or film.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.43</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.27</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.59</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">12</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Value consistency and self-worth</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\"><p>\u201cThese interventions include reminders of</p><p>individuals\u2019 or their group\u2019s egalitarian preferences or history in order to inspire consistency with that history in the present moment, remind people of moral exemplars, and provoke introspection about one\u2019s existing beliefs and prejudices.\u201d</p></td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.41</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.23</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.6</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">35</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Extended and imaginary contact</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">\u201c[T]he majority of studies testing the extended contact hypothesis used fictional friends or characters in books or movies that belong to the same ingroup as the audience member to test whether the fictional character\u2019s contact with an outgroup member would reduce prejudice.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.37</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.3</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.44</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">137</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Social categorization</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">These interventions \u201cencourage participants to rethink group boundaries or to prioritize common identities shared with specific outgroups.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.37</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.27</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.46</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">59</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Overall</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.36</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.31</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.4</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">416</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Cognitive and emotional training</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">\u201cInterventions categorized as cognitive and emotional training share the idea that individuals can be trained to use thinking and emotion regulation strategies to fight off their personal implicit or explicit prejudices.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.34</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.25</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.43</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">104</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Multicultural, antibias, moral education</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">\u201cAntibias education and multicultural education draw variously on theories addressing the socialization of prejudice, cognitive and moral development, and learning. The form of these interventions also ranges widely.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.30</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.18</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.42</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">20</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Diversity trainings</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">\u201cThe notion of diversity training encompasses a wide category of interventions that are \u2018designed to attack bias\u2019 among managers and workers.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.30</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">-0.12</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.71</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">6</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Interpersonal contact</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">Contact between members of groups. E.g. included studies \u201crandomized criminology students to have contact with individuals incarcerated for serious crimes\u201d or \u201crandomly assigned Jewish and Arab Israelis to meet one another on peace encounters.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.28</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.17</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.38</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">29</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Peer influence, discussion / dialogue</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">\u201cThis category of interventions is united by the idea that people who share important identities, peers, or ingroup members have a powerful influence over one another\u2019s impression of the attitudes and behaviors that are typical, desirable, and correct. The interventions in this category wield peer influence in various ways to reduce prejudice.\u201d</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.27</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.13</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.41</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">39</td></tr><tr><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">Other</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:346.788px\" colspan=\"1\" rowspan=\"1\">&nbsp;</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.24</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.01</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">0.47</td><td style=\"border-left:1px solid rgb(33, 33, 33);border-top:1px solid rgb(33, 33, 33);padding:2px;width:116.438px\" colspan=\"1\" rowspan=\"1\">24</td></tr></tbody></table></figure><p><i>LL = 95% confidence interval lower limit; UL = 95% confidence interval upper limit; n = number of outcomes.</i></p><p>&nbsp;</p><p>They found similar sized effects for \u201cexplicit attitudes or beliefs\u201d (<i>d </i>= 0.35, 95% CI [0.3, 0.39], <i>n</i>&nbsp;= 335), behavior (<i>d </i>= 0.42, 95% CI [0.29, 0.55], <i>n </i>= 50), and other outcome types, though Table 1 combines all the outcome types they measured.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt109\"><sup>[109]</sup></a></p><p>They find evidence of substantial publication bias, noting that, \u201cthe average effect size drops 48%, to 0.187, when we focus solely on the top quintile of sample sizes\u201d and that their analysis suggests that \u201ca study large enough to generate a standard error of approximately zero would, on average, produce no change in prejudice at all.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt110\"><sup>[110]</sup></a></p><p>The outcome measures tend to focus on prejudice itself, which is arguably less useful for advocates than measures of support for policies that affect marginalized groups.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt111\"><sup>[111]</sup></a>&nbsp;However, we might expect many interventions to affect both types of outcomes. For example, there is evidence that intergroup contact affects both.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt112\"><sup>[112]</sup></a></p><p>Some of the reviewed intervention types, such as intergroup contact and various training types, seem likely to be very expensive. It therefore remains unclear whether these mechanisms could be employed to cost-effectively encourage widespread attitude change.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt113\"><sup>[113]</sup></a>&nbsp;Paluck et al. note that 76% of their included studies evaluate \u201ctreatments that are easy to implement, brief (under 10 minutes), inexpensive, and thought to have lasting effects,\u201d but highlight that there is little evidence about the long-term effects of these interventions.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt114\"><sup>[114]</sup></a></p><h3><strong>Framing</strong></h3><p>\u201cFraming\u201d variations might influence public opinion through different psychological mechanisms&nbsp;to direct persuasion efforts. Political scientist Thomas E. Nelson and colleagues (1997) explain that, by highlighting certain aspects of a topic over others, \u201c[f]rames may supply no new information\u201d and have no effect on the recipient\u2019s beliefs about the topic, \u201cyet their influence on our opinions may be decisive through their effect on the perceived relevance [\u201cweight\u201d] of alternative considerations.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt115\"><sup>[115]</sup></a>&nbsp;In contrast, traditional persuasion influences attitudes by providing new information and altering beliefs without influencing the weight of those beliefs.</p><p>For example, when presented with a choice of options that involve risk, the option that people are most likely to select varies substantially according to whether the positives (e.g. \u201clives saved\u201d) or negatives (e.g. \u201clives lost\u201d) are emphasised by the question, even when the options presented are logically identical. A meta-analysis of 136 empirical papers found a small effect (<i>d</i>&nbsp;= 0.31) from such framing variations, with respondents more likely to avoid risk when the positives are emphasized.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt116\"><sup>[116]</sup></a></p><p>Various other small differences in survey question wording (or perhaps ballot language<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt117\"><sup>[117]</sup></a>) that highlight certain aspects of a topic can elicit substantially different levels of support. A common survey question on the death penalty is Gallup\u2019s \u201cAre you in favor of the death penalty for persons convicted of murder?\u201d \u2014 this tends to receive majority approval in the US, but support for the death penalty can fall by 30% or more when the question instead asks whether the respondents support the death penalty or life without parole for convicted murderers.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt118\"><sup>[118]</sup></a>&nbsp;Political scientists Dennis Chong and James N. Druckman (2007) note that variations in framing have been demonstrated to&nbsp;affect policy preferences in \u201cexperiments, surveys, and case studies across a range of issues.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt119\"><sup>[119]</sup></a>&nbsp;</p><p>Such framing effects could occur at the level of societal discourse of a topic. As discussed in the section on \u201cThe media\u2019s agenda-setting effects\u201d below, advocates could influence such discourse via the media. Encouraging widespread adoption of a particular frame could take a long time but have substantial effects on policy preferences.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt120\"><sup>[120]</sup></a>&nbsp;It therefore seems useful for advocates to identify the frames that are most persuasive to their audiences and then to apply these in their messages.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt121\"><sup>[121]</sup></a></p><p><strong>Moderators</strong></p><p>Chong and Druckman (2007) cite evidence that framing efforts are more effective if:</p><ul><li>The audience perceives the connection being made between the frame and the issue as valid,</li><li>The audience does not have strong values that contradict the frame,</li><li>The frame is \u201cdelivered by credible sources,\u201d or</li><li>The frame \u201cinvoke[s] longstanding cultural values.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt122\"><sup>[122]</sup></a></li></ul><p>Given that framing can operate through different mechanisms to persuasion, it might have different moderators. For example, Nelson et al.\u2019s (1997) experiment found evidence that framing variations have stronger effects for people with high \u201cdomain-specific knowledge about the arguments surrounding an issue,\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt123\"><sup>[123]</sup></a>&nbsp;whereas other studies have found that the opposite tends to be the case for persuasion attempts.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt124\"><sup>[124]</sup></a>&nbsp;So even if persuasion attempts fail, reframing the issue might still successfully shift overall attitudes, and vice versa.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt125\"><sup>[125]</sup></a></p><p><strong>Duration</strong></p><p>As with research focused on the duration of the persuasive messages, some studies provide evidence that at least some attitude change caused by a single positively or negatively framed article can persist for several weeks, but that the size of the effect will diminish during that time.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt126\"><sup>[126]</sup></a>&nbsp;Presumably then, for a new frame to have substantial, long-lasting effects on attitudes, advocates would need to engage in long-term campaigns with repeated messaging using the new frame (which could be very expensive), or successfully encourage the media, politicians, or other influencers to adopt the frame as well. Indeed, a review of 16 longitudinal, experimental studies testing how long news framing effects last found that \u201c[s]tudies focusing on repetitive framing are [not] conclusive but suggest that repetitive news frame exposure strengthens the framing effect to some extent.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt127\"><sup>[127]</sup></a>&nbsp;They also found that:</p><ul><li>\u201cStudies focusing on competitive framing often conclude that news framing effects persist beyond initial exposure, but are relatively easily altered, sometimes only one day later, by competitive exposure.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt128\"><sup>[128]</sup></a></li><li>\u201c[S]tudies using non-salient issues&nbsp;are able to detect longer-lasting framing effects, probably because there is less exposure to issue-relevant news in the interim period between initial and delayed post-tests.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt129\"><sup>[129]</sup></a></li><li>\u201c[N]egative news frames are likely to have stronger and therefore longer-lasting effects on opinions than positive news frames\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt130\"><sup>[130]</sup></a></li></ul><h3><strong>Protest and social movement events</strong></h3><p>Protest could presumably influence public opinion either by spreading persuasive arguments or affecting how social issues are framed and reported in the media. A recent observational study found that activism by the women\u2019s movement has had substantial cumulative effects on gender attitudes such as support for female presidents.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt131\"><sup>[131]</sup></a>&nbsp;However, other studies suggest that the cumulative effect of social movement protest and mobilization on public opinion is not always significant and positive, as sociologists Edwin Amenta and Francesca Polletta (2019) summarize:</p><p>Research indicates that protest may not budge public opinion, as was the case with the Occupy movement and anti\u2013Vietnam War protests. A movement\u2019s impact may be canceled out by the impact of a counter movement, as was the case with environmentalists on climate change. Or movement action may backfire, leading to more negative views of the group or issue, as was the case for nuclear freeze proponents. Movements\u2019 influence on public opinion may depend on their being endorsed by more mainstream political elites. Anti\u2013Vietnam War sentiment was limited until political leaders and reporters began to criticize the war, and public support for the Equal Rights Amendment in Oklahoma declined after legislators rejected it. At a cross-national level, public opinion on issues of importance to a movement\u2019s constituency may be driven more by political regime type, demographics, religion, and economic development than by the existence of a movement.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt132\"><sup>[132]</sup></a>&nbsp;</p><p>Some studies find that protests can improve attitudes towards movements\u2019 intended beneficiaries<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt133\"><sup>[133]</sup></a>&nbsp;and attitudes towards the movements themselves,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt134\"><sup>[134]</sup></a>&nbsp;though violent protest can have negative effects on such attitudes.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt135\"><sup>[135]</sup></a></p><p>Activism can be widely covered in the media,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt136\"><sup>[136]</sup></a>&nbsp;which can in turn influence the public\u2019s assessment of how important affected issues are.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt137\"><sup>[137]</sup></a>&nbsp;Relatedly, a number of studies have found that social movement activism can have significant effects on the political agenda, encouraging hearings and the introduction of legislation relevant to the targeted issues.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt138\"><sup>[138]</sup></a></p><h2><strong>WHAT ARE SOME OTHER CAUSES OF PUBLIC OPINION CHANGE AND HOW CAN ADVOCATES HARNESS THEM?</strong></h2><p>This section does not attempt to review all possible causes of public opinion change; the focus is on factors that seem especially well-studied in the academic literature or especially relevant to social movements targeting moral circle expansion.</p><h3><strong>The media\u2019s persuasive effects</strong></h3><p>Psychologists Stuart Oskamp and P. Wesley Schultz (2005) summarize that the scholarly consensus on whether and when the media tends to affect public opinion or not has changed:</p><blockquote><p>The first, a powerful effects model, was dominant from the 1920s through the 1940s, as illustrated in the deep fear of the possibly irresistible effects of propaganda on a defenseless public\u2026 It was followed by the minimal effects model, articulated by Klapper (1960), based on the many empirical studies which found no effects or very limited effects of the media in changing people\u2019s beliefs, attitudes, and behavior. More recently, a model of powerful effects under limiting conditions has gained more adherents. It denies the early all-powerful view of the media, but stresses that they have important effects in particular circumstances and with particular individuals. Thus current research is apt to focus on the interacting variables and contingent conditions under which media effects will emerge most clearly\u2014for instance, under conditions of heavy viewing and weak prior predispositions. Furthermore, current conceptions include a wide range of media effects\u2014not just changing attitudes, but also forming new attitudes, beliefs, or behaviors, reinforcing already existing ones, and crystallizing previously vague or unstated beliefs or attitudes.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt139\"><sup>[139]</sup></a></p></blockquote><p>They summarize that there is evidence that, \u201cmass communication usually serves to reinforce existing attitudes and opinions\u201d and \u201c[w]hen mass communication does produce attitude change, minor change in the extremity or intensity of the attitude is much more common than is \u2018conversion\u2019 from one side of an issue to the other side.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt140\"><sup>[140]</sup></a></p><p>This does not preclude the possibility that advocates could encourage substantial public opinion changes in usual circumstances, such as if they manage to encourage sustained shifts in the overall tone of media coverage. Indeed, there is evidence that where \u201ccoverage of a public issue is unbalanced in a pro or con direction, public opinion is likely to shift in that direction subsequently.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt141\"><sup>[141]</sup></a>&nbsp;This may be tractable if advocates can strategically raise the salience of certain attributes (see \u201cAttribute salience and effects on public opinion\u201d below) or create newsworthy persuasive materials such as documentaries or exposes that shift the balance of what news is being covered.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt142\"><sup>[142]</sup></a></p><p>There are several theories about how the media influences public opinion. These could be relevant to advocates, but tend to make broad claims and have mixed results in empirical tests.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt143\"><sup>[143]</sup></a>&nbsp;For example, in the \u201ctwo-step flow\u201d theory, \u201ca small minority of \u2018opinion leaders\u2019... act as intermediaries between the mass media and the majority of society,\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt144\"><sup>[144]</sup></a>&nbsp;suggesting that it may be most cost-effective to focus resources on changing the opinions of influential audience members. However, some research suggests that, at least in some circumstances, it could be more cost-effective to ignore supposed opinion leaders and focus on whoever can be persuaded most easily.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt145\"><sup>[145]</sup></a></p><p><strong>Moderators</strong></p><p>Oskamp and Schultz (2005) note that, \u201c[m]ass communication can be quite effective in changing attitudes in areas where people\u2019s existing opinions are weak\u201d and \u201ccan be quite effective in creating opinions on new issues where there are no existing predispositions to reinforce.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt146\"><sup>[146]</sup></a>&nbsp;Presumably then, advocates could influence public opinion towards less well-known attributes&nbsp;or sub-topics (e.g. a specific farming practice) if they are able to shape the initial media coverage. It also suggests that tactics that are aimed at increasing awareness&nbsp;without necessarily being persuasive may be counterproductive for issues where public opinion is currently unfavorable because they may make subsequent attitude change more difficult.</p><p>Many of the other moderators described in the section above on \u201cDirect advocacy and persuasion\u201d likely hold in the context of media messaging.</p><p><strong>Duration</strong></p><p>Many of the studies of the duration of persuasive effects described in the section above on \u201cDirect advocacy and persuasion\u201d were tested using media articles or data; the effects from a single exposure to persuasive media content will diminish with time, though there may be some lasting effects, and repeated exposure to similar content may encourage longer-term attitude change.</p><h3><strong>The media\u2019s agenda-setting effects</strong></h3><p>Agenda-setting research finds evidence that, when the news media covers certain issues, the public tends to increase its evaluation of how important those issues are, i.e. that the media \u201ccan have strong, direct effects in the short term by influencing not what people think, but what they think about.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt147\"><sup>[147]</sup></a>&nbsp;This effect has been identified using a wide variety of methodologies, issue foci, geographical foci, and media types, but the prototypical design is that the media\u2019s agenda is assessed through content analysis, the public\u2019s agenda is assessed through survey questions, and the correlation between the two is then estimated.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt148\"><sup>[148]</sup></a>&nbsp;Luo et al.\u2019s (2019) meta-analysis found that the mean correlation from the 67 included studies was 0.49.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt149\"><sup>[149]</sup></a>&nbsp;Maxwell McCombs and Sebastian Valenzuela, prominent scholars of the media\u2019s agenda-setting effects, summarize in their book <i>Setting the Agenda </i>(2021) evidence that these correlations are usually mostly explained by the media\u2019s effect on the public, rather than by the public\u2019s effect on the media or some other factor:</p><ul><li>Experiments have also demonstrated the media\u2019s agenda-setting effects,</li><li>Comparisons between the media and public agenda at different time points tend to suggest that the media changes focus first, with the public following afterwards, and</li><li>Studies have found correlations between the media and public agendas even when controlling for plausible lurking variables.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt150\"><sup>[150]</sup></a></li></ul><p>There is also evidence that the media has various other effects related to its public agenda-setting function: it can influence the political agenda,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt151\"><sup>[151]</sup></a>&nbsp;the public\u2019s knowledge and perceptions of reality,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt152\"><sup>[152]</sup></a>&nbsp;and the public\u2019s views about which criteria should be used to evaluate politicians.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt153\"><sup>[153]</sup></a></p><p>The agenda-setting hypothesis&nbsp;suggests that advocates can probably influence the public agenda if they are able to increase media attention to an issue.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt154\"><sup>[154]</sup></a>&nbsp;The logical next questions are therefore: can social movements influence the media agenda, and if so, how? These questions are not the focus of this review, though McCombs and Valenzuela summarize some evidence that public relations professionals have a substantial influence on the media agenda<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt155\"><sup>[155]</sup></a>&nbsp;and some studies have found that social movement protests have influenced media coverage and the public agenda.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt156\"><sup>[156]</sup></a></p><p><strong>Moderators</strong></p><p>McCombs and Valenzuela provide evidence that the media tends to have weaker effects if the public has high personal experience of an issue or is otherwise already relatively certain about an issue\u2019s importance.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt157\"><sup>[157]</sup></a>&nbsp;For example, studies have found lower correlations between media coverage and the public\u2019s perceived importance of issues like crime and the cost of living \u2014 which affect the public very directly \u2014 than between media coverage and the public\u2019s perceived importance of issues like pollution, drug abuse, and energy.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt158\"><sup>[158]</sup></a>&nbsp;One implication of these findings for social movements is that successfully attracting media coverage of institutional campaigns may have a stronger agenda-setting effect than successfully attracting media coverage of individual diet change topics.</p><p>Relatedly, McCombs and Valenzuela summarize one study which seems to suggest that media coverage may have lower agenda-setting effects if the public already has higher awareness about a topic.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt159\"><sup>[159]</sup></a>&nbsp;This suggests that if certain topics have been on the agenda for some time already, further efforts to raise their salience may be less effective.</p><p>Demographic variables tend to have little or no moderating effect on agenda setting. There is evidence from several studies that people with more years of education more closely mirror the media agenda, though the difference is very small.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt160\"><sup>[160]</sup></a></p><p><strong>Duration</strong></p><p>McCombs and Valenzuela summarize that, \u201cthe point of decay of agenda-setting effects, defined as the point in time where significant correlations between the media agenda and the public agenda disappear, ranges from eight to twenty-six weeks.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt161\"><sup>[161]</sup></a>&nbsp;This suggests that advocates should usually not attempt to increase the salience of certain topics via the media unless they suspect that they will be able to sustain high media attention, since, if they are only successful in temporarily raising media attention, the effects on the public\u2019s perceptions of its importance will also be short-lived.</p><p>Advocates might also seek to increase salience if they have specific, strategic goals in mind for why higher salience will be useful at a particular time, e.g. to support a legislative campaign on an issue with high public support. McCombs and Valenzuela note, however, that, \u201cthe span of time involved in the transfer of issue salience from the media to the public agenda is generally in the range of four to eight weeks,\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt162\"><sup>[162]</sup></a>&nbsp;so advocates need to account for this time lag in their planning.</p><p><strong>Attribute salience and effects on public opinion</strong></p><p>The \u201cfirst level\u201d of agenda setting research focuses on assessing whether and how media attention influences the salience and perceived importance of certain \u201cobjects\u201d (i.e. topics) among the public, while the \u201csecond level\u201d assesses whether and how media attention influences the&nbsp;salience&nbsp;and perceived importance of more specific attributes of those objects.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt163\"><sup>[163]</sup></a>&nbsp;Luo et al. (2019) found that the correlations tended to be larger in studies of first level agenda-setting than studies of second-level agenda-setting \u2014 this difference was significant at p &lt; .10 (\u03b2 = 0.53).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt164\"><sup>[164]</sup></a>&nbsp;McCombs and Valenzuela also note that the media only seems to have much of a second level agenda-setting effect where \u201cboth the political system and the news media are reasonably open and free.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt165\"><sup>[165]</sup></a></p><p>There is some evidence that increases in the salience of certain attributes can have disproportionately large effects on the salience of the broader object.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt166\"><sup>[166]</sup></a>&nbsp;More importantly, there is also evidence that certain attributes tend to be covered in a more positive tone in the media than other attributes; if those attributes (and their corresponding more or less positive coverage<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt167\"><sup>[167]</sup></a>) become more salient, then this can affect public opinion on the topic as a whole.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt168\"><sup>[168]</sup></a>&nbsp;This suggests that advocates can potentially alter both the public\u2019s perceived importance of a topic and public opinion on that topic by identifying specific attributes that tend to be seen more or less positively, then strategically raising the salience of certain attributes while avoiding raising the salience of others.</p><h3><strong>Politicians and celebrities</strong></h3><p>Politicians and celebrities can advocate particular ideas \u2014 presumably their persuasion efforts may be successful and are subject to the same moderators as other communicators.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt169\"><sup>[169]</sup></a>&nbsp;A number of studies have directly evaluated the persuasive and agenda-setting effects of politicians and celebrities too.</p><p>Several studies suggest that politicians can increase approval for specific policies by framing them as directed towards broad goals that the public support,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt170\"><sup>[170]</sup></a>&nbsp;and numerous others suggest that presidents can intentionally shape public opinion through persuasive messaging.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt171\"><sup>[171]</sup></a>&nbsp;Benoit et al.\u2019s (2003) meta-analysis found that viewing US presidential debates had an effect size of .14 (95% CI .03\u2013.32, k = 4) on \u201cpreference for one candidate\u2019s issue positions over another\u2019s.\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt172\"><sup>[172]</sup></a>&nbsp;So presidents can influence public opinion, though the effects may be small.</p><p>There is also evidence that political parties\u2019 positions influence the attitudes of partisan supporters.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt173\"><sup>[173]</sup></a>&nbsp;This influence may be stronger when party positions are polarized<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt174\"><sup>[174]</sup></a>&nbsp;and when the public is less informed about an issue.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt175\"><sup>[175]</sup></a>&nbsp;Cumulatively, repeated cues from political elites pushing in a certain direction could have substantial effects on public opinion, whereas competing partisan cues in polarized debates could roughly balance each other out and potentially even drown out the effects of other persuasion efforts.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt176\"><sup>[176]</sup></a></p><p>In addition to altering attitudes through persuasion or reframing, politicians can affect the public\u2019s perceptions of the importance of issues via comments, actions, and press releases that are reported in the media.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt177\"><sup>[177]</sup></a>&nbsp;Benoit et al.\u2019s (2003) meta-analysis found that US presidential debates had a mean weighted agenda-setting effect size of .29 (95% CI .22\u2013.44, k = 3).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt178\"><sup>[178]</sup></a></p><p>There is evidence that celebrities can have substantial effects on attitudes, though they are not always more effective than alternative spokespeople.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt179\"><sup>[179]</sup></a>&nbsp;Some advertising studies have found support for the \u201cmatchup hypothesis,\u201d where \u201c[c]elebrities or other endorsers are particularly apt to enhance&nbsp;consumer attitudes when their characteristics \u2018match up with\u2019 or are relevant to the product being promoted,\u201d<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt180\"><sup>[180]</sup></a>&nbsp;and we might expect a similar effect to hold for promotion of policies.</p><p>Even if celebrities or politicians are not seen as credible to speak on a particular issue, their involvement could be beneficial if it helps to bring attention to certain issues and arguments, either through a second-level agenda-setting effect<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt181\"><sup>[181]</sup></a>&nbsp;or because it increases the prevalence of persuasive arguments pushing in a certain direction.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt182\"><sup>[182]</sup></a>&nbsp;But their involvement could backfire for the same reasons, so it seems more important to focus on getting these public figures promoting the right messages than on maximizing their general discussion of an issue.</p><h3><strong>Policy change</strong></h3><p>Oskamp and Schultz (2005) summarize several studies showing that public opinion often follows US foreign policy quite closely.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt183\"><sup>[183]</sup></a>&nbsp;There is evidence from observational analyses and social movement case studies that public opinion changes can occur from policies affecting social issues and the breadth of the moral circle, too. For example, numerous countries have seen public support for the death penalty decline since it was abolished.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt184\"><sup>[184]</sup></a>&nbsp;Some studies suggest that international policies and policies in neighboring jurisdictions can also affect public opinion.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt185\"><sup>[185]</sup></a>&nbsp;When the Supreme Court makes a decision, this tends to cause public opinion to move towards the opinion implied by that decision, though this does not always happen.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt186\"><sup>[186]</sup></a>&nbsp;This all suggests that if advocates can encourage policy change, public opinion will tend to move towards support for those policies. Nevertheless, political scientist James Stimson (2015) presents evidence that public preferences regarding the general direction of <i>further </i>government action sometimes shift in the opposite direction to trends in government policy itself.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt187\"><sup>[187]</sup></a></p><h3><strong>Indirect or long-term factors</strong></h3><p>Reliably changing public opinion is less tractable if the main causes of fluctuations are slow-moving and long-term, or only have an indirect relationship to change and are therefore difficult to predict and control.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt188\"><sup>[188]</sup></a></p><p>Referring to the impact of events on public opinion about foreign policy, Oskamp and Schultz (2005) summarize that:</p><blockquote><p>[E]ven spectacular events have no effect on attitudes, or they may cause only a brief fluctuation followed by a return to the preexisting attitude\u2026 When rapid attitude shifts occur, they are usually related to major events in international affairs or in the economy, such as the improvement in U.S.-Soviet relations that took place under Gorbachev, and particularly the fall of the Berlin Wall, which signaled the end of the Cold War. As a contrasting example, after the Chinese army's massacre of student protesters in Tiananmen Square in 1989, the percentage of Americans who expressed favorable opinions of China plunged briefly from 72% to 31%. However, even the most dramatic changes in political alignments usually involve attitude changes by only 20% to 30% of the population, and such changes almost always involve a combination of spectacular events and cumulative events. Either type of event alone is apt to produce attitude changes of no more than 10%.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt189\"><sup>[189]</sup></a></p></blockquote><p>External events have been found to affect a range of other social and political attitudes, such as hurricanes affecting support for pro-environmental politicians, financial crisis affecting support for conservative economic policies, and nuclear disaster decreasing support for the use of nuclear power.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt190\"><sup>[190]</sup></a>&nbsp;However, as with the events affecting foreign policy opinions, their effects are often small or temporary.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt191\"><sup>[191]</sup></a></p><p>Public opinion surveys usually find that some demographic variables are statistically significant predictors. For example, Sentience Institute\u2019s US surveys find that women, younger people, more liberal people, Democrats, black and Hispanic people, people from the Northeast of the US, and vegetarians and vegans have significantly higher Animal Farming Opposition than other respondents.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt192\"><sup>[192]</sup></a>&nbsp;We should expect, then, that demographic trends might lead to changes in overall public opinion,<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt193\"><sup>[193]</sup></a>&nbsp;though such changes might be slow and difficult to detect.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt194\"><sup>[194]</sup></a></p><p>Of course, even if advocates cannot easily influence indirect or long-term factors, they may be able to take advantage of them. For example, it might be possible to implement institutional changes during a temporary spike in public support for certain policies.</p><h1><strong>LIMITATIONS</strong></h1><ul><li>Where I have cited textbooks in this review, I have not tended to look up the individual citations for the claims made. Often, the textbooks will provide a general characterization of certain streams of research and then provide a small number of illustrative examples, rather than rigorously reviewing the strength of evidence for various claims. In this sense, it is possible that some of the characterizations here do not actually have much supporting evidence.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt195\"><sup>[195]</sup></a></li><li>Reviewed research items often discussed the strengths and weaknesses of different research approaches (e.g. field studies vs. laboratory experiments) and whether the key assumptions of various theories had been tested directly or indirectly, but rarely assessed the risk of bias&nbsp;in included studies.</li><li>I have attempted to avoid selective reporting of findings, but I have not attempted to make the coverage of particular topics in this review systematic or comprehensive. It is possible that some of the findings cited here have been critiqued and challenged by other research that I have not seen.</li><li>Given the many different streams of research relating to the causes of public opinion change, it is likely that some have accidentally been missed. Others have been intentionally excluded in the interests of brevity (e.g. if they had unclear implications for advocacy).</li><li>This review mostly focuses on strategies or factors influencing public opinion that are well-studied in the academic literature. It may therefore miss effective but less well-studied strategies.</li></ul><h1><strong>FURTHER RESEARCH</strong></h1><ul><li>This review has focused on the causes of public opinion change under the assumption that public opinion is important for at least some institutional and social movement outcomes. This assumption has been evaluated by sociologists and political scientists \u2014 a review of relevant research on the effects of public opinion change could be informative both for testing the assumption and assessing how to most effectively encourage the intended outcomes.&nbsp;It could also shed further light on the questions examined in this review, since in asking how advocacy and public opinion affect policy, researchers sometimes implicitly evaluate how advocacy affects public opinion.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt196\"><sup>[196]</sup></a></li><li>This review has covered a wide range of topics fairly briefly; more systematic, in-depth reviews of some of these topics could be valuable. For example, there does not currently seem to be a meta-analysis or detailed review of the effects of protests and activism on public opinion, even though a number of studies exist on this topic. Additionally, a more comprehensive comparison of the effect sizes of various intervention types on public opinion could be valuable, albeit time consuming.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt197\"><sup>[197]</sup></a></li><li>Experiments could be conducted to more directly assess the relative effectiveness of different influences on public opinion. For example, which matters more: the frames used in media coverage (e.g. animal protection vs. environmental vs. human health focus), or the tone (positive or negative)? Which has a larger effect: a story about a favorable speech by a politician or a story about a well-attended public protest?</li><li>Experiments could be conducted that test out findings from persuasion research specifically in the context of social movements focusing on moral circle expansion. For example, are refutational two-sided messages more effective than one-sided messages at building support for institutional reforms that affect animals, like bans on factory farming? Which sources are seen as more or less credible (and hence more or less persuasive in certain contexts) when encouraging moral concern for artificial sentience: AI researchers, philosophers, cognitive scientists, robots, politicians, or members of the public?</li><li>American public opinion has changed substantially on a number of issues, such as the right to gay marriage, legalisation of marijuana, desegregation, and whether the public would vote for qualified women or African Americans for president.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt198\"><sup>[198]</sup></a>&nbsp;Case studies of these issues could reveal insight into the causes of public opinion change.</li><li>The research on framing and second-level agenda setting suggests that media coverage and persuasive messages will vary substantially in their persuasiveness depending on their content. There are a wide variety of research types that could be conducted to shed light on which frames and attributes specific social movements should prioritize, including content analyses, focus groups, surveys, experiments, and more informal exploration.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt199\"><sup>[199]</sup></a></li><li>It might theoretically be possible to influence attitude formation through repeated positive initial exposure to certain ideas or&nbsp;shaping which attitudes parents pass on to their children (e.g. via educational or supportive services).<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt200\"><sup>[200]</sup></a>&nbsp;In general, such factors shaping initial attitude formation seem likely to be difficult to influence,&nbsp;so this review has mostly ignored them, but a review of relevant research could be useful.<a href=\"https://www.sentienceinstitute.org/public-opinion#ftnt201\"><sup>[201]</sup></a></li><li>A review of studies with outcomes different from but related to attitudes \u2014 such as knowledge, attention, memory, emotion, identity and so on \u2014 could be helpful, e.g. for social marketing purposes.</li></ul><h1><strong>BIBLIOGRAPHY AND FOOTNOTES</strong></h1><p>See the <a href=\"https://www.sentienceinstitute.org/public-opinion\">version on Sentience Institute's website</a>.</p>", "user": {"username": "Jamie_Harris"}}, {"_id": "5T9eJWdSo6XwLrENC", "title": "Business Coaching/Mentoring For EA Organisations", "postedAt": "2021-11-09T11:11:53.632Z", "htmlBody": "<p><i><strong>TL;DR</strong>: if you're a CEO or senior decision maker at an EA org I'd like to invite you to </i><a href=\"https://thebusinessgrowthagency.actioncoach.co.uk/coaching-scholarships/\"><i>apply for our pro bono business coaching service</i></a><i>. If you know somebody who could help connect EA orgs and business coaches/mentors please point me in their direction.</i></p><p>I'm the COO (Chief Operating Officer) at <a href=\"https://thebusinessgrowthagency.actioncoach.co.uk/\">a business coaching practice in London</a>. Over the past year I've been running a few EA side projects to try and use some of the influence I have at work to direct money and resources towards EA causes.&nbsp;</p><p>As a company we now donate a proportion of our profits to GiveWell charities and I've given presentations and worked individually with our CEO clients in London introducing them to basic EA concepts and supporting them in setting up giving pledges.</p><p>The next thing I want to try is providing pro bono coaching/mentoring to leaders at EA organisations.</p><p>Some reasons why we might be able to help...</p><ul><li>We coach clients on productivity, goal setting and accountability but also practical business skills like management, operations, recruitment, culture, negotiation, marketing, ... etc. which I think could be relevant to many EA orgs as they grow</li><li>Each of our coaches have a decade plus of experience either in senior management positions at large organisations or founding and growing successful&nbsp;companies themselves</li><li>All our coaches have gone through rigorous coaching certification as well as coaching, between them, thousands of CEOs over the past decade</li><li>As a coaching practice with a team of 8 people we have a back office team and various different programmes that we've developed over the years. As well as 1-to-1 coaching sessions we do things like weekly group accountability coaching, workshops on specific business topics, coaching for emerging leaders in organisations and&nbsp;an entrepreneurs' book club. I'd be open to offering places at any of these free of charge to the right people.</li></ul><p>If this sounds interesting to you and you would like to talk to one of our business coaches <a href=\"https://thebusinessgrowthagency.actioncoach.co.uk/coaching-scholarships/#application-form\">please fill out an application form at the bottom of this page</a>.</p><p>If you know somebody who might benefit please forward them a link to this post or <a href=\"https://thebusinessgrowthagency.actioncoach.co.uk/coaching-scholarships/\">the application/explainer page on my website</a>.</p><p>If you can put me in touch with someone who has a clever way of connecting coaches/mentors and EA orgs that I haven't thought of please let me know. (I've reached out to a few meta-EA organisations by email this morning but I'm sure there's others I'm not aware of or situations where I won't get anywhere without someone first making an initial introduction.)</p><p>If you have any feedback or advice on anything I've written above please let me know in the comments or via private message.</p><p>Thanks so much.</p>", "user": {"username": "TheBusinessGrowthAgency"}}, {"_id": "SvbZtETGenTkZni8C", "title": "Where does most of the suffering from eating meat come from?", "postedAt": "2021-11-09T03:09:27.588Z", "htmlBody": "<p>Like a lot of folks eating primarily or only plants, I dislike eating animals because I empathize with animals and feel bad about their suffering from raising them for food.</p><p>That said, how much of the suffering involved in meat consumption actually comes from the animal whose meat is consumed?</p><p>What I'm thinking about is that <a href=\"https://awellfedworld.org/feed-ratios/\">eating animals is O(10) times less efficient at providing calories than eating plants</a>. This suggests that if more than 0.1 units of suffering (assuming the animal being eaten suffers 1 unit) are produced in the production of plants for food, then the suffering caused by eating meat is dominated not by the suffering of the animal being eaten but by the suffering caused in order to produce the food.</p><p>Obviously some of this is going to be hard to pin down. For example, depending on how you weigh the suffering of insects and how much pesticides are used to grow feed stock of the meat being consumed may cause wild swings in estimates, but I'd nonetheless be interested in seeing what models people have of how much suffering is caused. This might also make a suffering-oriented case for better meat choices among those who eat meat anyway. For example, maybe organic beef causes O(100) times less suffering than conventional beef because 100 times fewer insects suffer in its production?</p><p>So, any thoughts on this, what I might call the \"iceberg\" of suffering caused by eating meat?</p>", "user": {"username": "gworley3"}}, {"_id": "8Z3zFnDKRhGku3qje", "title": "Robin Hanson's Grabby Aliens model explained - part 2", "postedAt": "2021-11-09T17:43:36.383Z", "htmlBody": "<p><i>This article is the script of the Rational Animations video linked above. This time, we go into the power law that constitutes the \"core\" of this model, how to generate simulations of the model, and how to generate predictions from simulations. The predictions covered are many, such as when we'll meet grabby aliens, how many galaxies they can grab, our chances of hearing alien messages, and becoming an interplanetary civilization ourselves. The model answers the Fermi paradox while estimating grabby civilizations' expansion speed: we don't see them in our skies because they travel very fast. High speeds constitute another selection effect: if we could see grabby aliens, they would be here now instead of us.</i></p><p><a href=\"https://www.lesswrong.com/posts/trPP6GdkeXjKtEhza/robin-hanson-s-grabby-aliens-model-explained-part-2\">LessWrong cross-post</a></p><p><strong>Introduction/summary of the previous video</strong></p><p>In the previous video, we talked about why humanity seems to have appeared <i>very</i> early in the universe: among the first civilizations that could ever appear. This estimate is the result of two considerations:</p><p>First: Before becoming a civilization, life has to go through a series of difficult phases called \u201chard steps\u201d that are very unlikely to be completed as early as we did.<br>And second: Star systems with lifetimes of trillions of years might be habitable, making the overwhelming majority of future civilizations develop in those star systems in the trillions of years to come.&nbsp;</p><p>Earliness constitutes a riddle. It makes humanity\u2019s situation seem rather mysterious. Robin Hanson, who introduced the idea of the great filter, solves this riddle by postulating that civilizations that he calls \u201cgrabby\u201d will soon fill the universe. They have three characteristics:</p><ol><li>They expand from their origin planet at a fraction of the speed of light.</li><li>They make significant and visible changes wherever they go, and</li><li>They last a long time.</li></ol><p>Once a grabby civilization has occupied a volume of space, it prevents other civilizations from being born in that volume. Therefore grabby civilizations set a deadline for other civilizations to appear. Every non-grabby civilization can only appear early because later, all the habitable planets will already be taken. This constitutes a selection effect.</p><p>Hypothesizing grabby aliens explains our apparent extreme earliness. It makes us look typical amongst the other non-grabby civilizations, which can only follow our same path of appearing early and then potentially becoming grabby and expanding into space.</p><p>As anticipated, grabby aliens are the main assumption of a detailed model describing how such aliens expand and distribute in the universe. The model makes many interesting predictions, such as when we\u2019ll meet grabby aliens, our chances of hearing alien messages, and becoming an interplanetary civilization ourselves. It also answers why we don\u2019t see aliens yet, considering the universe\u2019s large number of stars and galaxies.&nbsp;</p><p><strong>A model simulation&nbsp;</strong></p><p><a href=\"https://youtu.be/oLvzFJLLfCY\"><u>Here\u2019s</u></a> a simulation of the model in an expanding spherical region of space, starting with a diameter of half a billion light-years and containing 400 million galaxies. See these colored regions expanding? They represent the grabby civilizations. In this simulation, they are born at a particular rate, and then they expand at half of the speed of light. When they meet, they simply stop expanding. The model doesn\u2019t tell us anything about what they\u2019ll do after meeting each other.&nbsp;</p><p>Civilizations that are born later take up less space, and if they are born very late, they occupy the crevices between bigger civilizations. Conversely, the first civilizations to appear end up controlling larger volumes.</p><p>At the end of the simulation, there are 87 grabby civilizations, 25 billion years have passed, and the diameter of the spherical region of space has expanded to 25 billion light-years. This is because the universe intrinsically expands. The distance between any two parts of the universe that are not gravitationally bound gets larger with time.</p><p><strong>The power law and its variables: how we estimate the variables and their effects on the model.</strong></p><p>The mathematical model underlying the simulation you just saw is relatively simple. It takes just three parameters: the rate at which grabby civilizations are born, the speed at which they expand, and the number of hard steps they have to go through. Hard steps are difficult phases that life has to go through before becoming a grabby civilization. Each of these steps has an extremely low probability of being completed per unit of time. For example, hard steps could be the creation of the first self-replicating molecules, the passage from prokaryotic cells to eukaryotic cells, and multicellularity.&nbsp;</p><p>The appearance rate of grabby civilizations is denoted with \u201ck\u201d and the number of hard steps with \u201cn\u201d. They are parameters in a simple power law: (t/k)^n. This law takes a time t and returns the chance per co-moving volume that a grabby civilization has appeared by that time. A co-moving volume is a volume of space that expands at the same rate as the rest of the universe.</p><p>Note that as the number of galaxies per co-moving volume stays roughly constant in time, you can interpret this law as giving a chance per galaxy per unit time. Note also that this power law approximates the complicated function we showed in the previous video. Because this is just an approximation, the power \u201cn\u201d is close to but isn\u2019t exactly the same as the number of hard steps.&nbsp;</p><p>The power law and the expansion speed \u201cs\u201d are all the ingredients you need for the model. Once you choose a value for \u201ck\u201d, \u201cn\u201d, and \u201cs\u201d, you can run simulations such as the one you saw earlier, and you\u2019re able to tell how grabby aliens expand over time and therefore make predictions.</p><p>We can estimate these parameters from data, which enables us to ground the model in real-world observations.</p><p>Some things are clear: we don\u2019t seem to be in a part of the universe that has been visibly changed by aliens. Less clearly, but still plausibly, we seem to be on a path to perhaps becoming grabby ourselves. Let\u2019s say that if this happens, it will happen in less than 10 million years. We don\u2019t have any reason to believe that our location in space and time might be unusual compared to other grabby civilizations\u2019 origin locations. From here, we can tentatively say that we can treat our current date as a random sample from all the dates at which a grabby civilization could be born. Therefore, in expectation, our origin date falls in the middle between the other possible origin dates. The fact that we expect to be in the middle of the distribution of grabby aliens\u2019 origin dates constrains the rate at which they must be born, giving us an estimate for k within roughly a factor of two.&nbsp;</p><p>Now let\u2019s talk briefly about the number of hard steps again. As we saw in the previous video, their number can be easily estimated considering two hard step candidates. First: the time between now and when Earth will first become uninhabitable, which is 1.1 billion years. This leads to an estimate of 3.9 hard steps. And second: The time between when Earth first became habitable to when life first appeared, which is 0.4 billion years. This would lead to an estimate of 12.5 hard steps. These two durations suggest a middle number of at least n = 6.</p><p>So, now we have k and n. How do we estimate \u201cs\u201d, the speed at which grabby civilizations expand? Surprisingly, we can estimate it by simply observing that we don\u2019t see any trace of other civilizations. The model predicts that, on average, when a grabby civilization is born, a third to a half of the universe is already filled with other grabby civilizations. Since we expect our date to be among grabby aliens\u2019 origin dates, we should see a significant portion of the universe already occupied by other civilizations. And yet, we don\u2019t see that. As we are about to explain, from this observation alone, assuming that grabby aliens exist, we infer that they expand <i>very</i> fast. In short, we don\u2019t see them because if we could have seen them, they would be here now instead of us.&nbsp;</p><p>This is a selection effect in the same vein as the reasoning for why we\u2019re early. Let\u2019s dive into it in more detail.</p><p>This is our backward light cone:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/y7evhlvohnfzag41kwcq.png\"></figure><p>&nbsp;</p><p>Each point falling on the red lines is a coordinate in space-time: simply a date and a place in the universe. From Earth, we can only see the coordinates falling on the red lines. For example, we can now see events that happened three years ago, three light-years away from us. But we have already seen events that happened four years ago and two light-years away, and we still have to see events that happened two years ago, but four light-years away. Inside the cone, there are events that we have already seen. Outside of the cone, there are events that we haven\u2019t seen yet.&nbsp;</p><p>Now, let\u2019s add a grabby civilization that expands fast, almost at the speed of light.&nbsp;<br><br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/l2wrtupr5utmnybku5gt.png\"></figure><p><br>&nbsp;</p><p>See the yellow region? All of the coordinates in that region can\u2019t be origins for other grabby civilizations. If they were, those grabby civilizations would be here instead of us. They would have prevented our existence.&nbsp;</p><p>For us to be able to see other grabby civilizations, they would have had to be born at the coordinates in these smaller green regions.</p><p>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/vhv1zvtrjjjobvu5dwos.png\"></figure><p>&nbsp;</p><p>The faster the aliens are, the bigger the yellow region and the smaller the green regions. Therefore the faster the aliens are, the smaller is the chance that we could see them.</p><p>Since we don\u2019t see them, we conclude that they are probably expanding at higher speeds.</p><p>Look at this graph:</p><p>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/gtvt2xoc1btrm5pab63f.png\"></figure><p><br>&nbsp;</p><p>The colors and the numbers here represent likelihood ratios. They tell us how well each combination of speed and number of hard steps predict our evidence of \u201cseeing no aliens\u201d compared to the other combinations. The combinations of \u201cn\u201d and \u201cs\u201d falling into bluer areas poorly predict the evidence that we see no aliens relative to the other combinations. That means that those combinations of <i>s</i> and <i>n</i> make seeing no aliens more unlikely. Colors that are more yellow mean the opposite: combinations that correspond to them better predict the evidence that we see no aliens. That means that they make seeing no aliens more likely.</p><p>For example, the probability of seeing no aliens assuming 8 hard steps and that they travel at 10% of the speed of light is more or less a hundred times smaller than the probability that we see no aliens assuming 8 hard steps and speeds of 70% of the speed of light.</p><p>Therefore the hypothesis that grabby civilizations travel at 70% of the speed of light gains a lot of credence relative to the hypothesis that aliens travel at 10% of the speed of light.</p><p>Notice how speed influences the colors way more than the number of hard steps here. Colors are going from blue to yellow horizontally much more than vertically. This is also clear by looking at the numbers. A takeaway from this graph is that speeds of less than one-third of the speed of light predict our evidence of \u201cseeing no aliens\u201d very poorly. That means that they place our chances of seeing no aliens very low. Therefore, they lose <i>a lot</i> of credence compared to higher speeds.</p><p>This whole reasoning about speed is similar to the reasoning at the basis of the model. The grabby aliens model explains why we are early because if we weren\u2019t, grabby aliens would have already occupied Earth instead of us. In the same way, we don\u2019t see aliens because otherwise, they would have already occupied our planet. Both hypotheses, grabby aliens and high speeds, make us look more typical: not seeing aliens is more common if grabby civilizations expand at high speeds.</p><p>Of course, if you believe speeds higher than one-third of the speed of light to be highly unrealistic in the first place, then you have a reason not to believe at least some parts of the grabby aliens model. For example, you might disbelieve that grabby civilizations ought to make very visible changes wherever they go and say that this is why we don\u2019t currently see them in our skies. Maybe they produce only subtle changes that we will notice in the future. The Grabby Aliens model would still apply, but we wouldn\u2019t be able to say much about grabby civilizations\u2019 expansion speed.&nbsp;</p><p>If Grabby Aliens produce changes that we will only notice with better tools, then the model predicts that we will see many civilizations.</p><p>And they would be big! Most grabby-aliens-occupied volumes we could see would appear to us to be much larger than the full moon.</p><p><strong>Predictions and how to generate them from the simulations</strong></p><p>Now that we have this simple law and know how to roughly estimate the parameters, we can describe and simulate how grabby civilizations expand and distribute in the universe. This allows us to make many specific predictions.&nbsp;</p><p>For example, there\u2019s something interesting to say about how fast grabby aliens appear in the universe. The power law implies that they\u2019ll all appear in a relatively short time window.&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/ikbwmbspbzzuyovpzoja.png\"></figure><p><br>This graph means that they all appear within a time window between five and fifty billion years long, depending on the power \u201cn\u201d we choose to plug in the model.&nbsp;</p><p>The horizontal axis represents grabby civilizations origin times, the vertical axis represents the percentile.</p><p>Here\u2019s an example that will help interpret it: let\u2019s say we assume 3 hard steps. Let\u2019s take 20 billion years as an origin date on the horizontal axis. It corresponds to a percentile of almost 0.8. That means that 80% of civilizations will have appeared by the time the universe is 20 billion years old. The other 20% will appear later: till the universe is at least 50 billion years old. A percentile of 0.8 also means that any random grabby civilization has an 80% chance of having appeared by the time the universe is 20 billion years old.</p><p>See how sharply these functions rise? The reason is that the power-law implies that civilizations tend to appear just before the deadline. In general, they are very unlikely to appear; therefore, the few lucky ones that do, appear as late as possible. And soon after, they preclude everyone else from appearing, and the functions become flat again.&nbsp;</p><p>It is interesting to understand how such graphs are generated from simulations.&nbsp;</p><p>In general, a simulation of the model, such as the one you saw at the beginning of the video, is generated in this way: potential grabby civilization origins are placed uniformly within a volume at random positions. The positions are paired with random times drawn from a power law (t/k)^n with its own k and n. So, basically, you generate random origins in space and time, but there are more origin points at the times where the power law rises faster. Then, all the origins that would be precluded by the expansion of the other grabby civilizations are eliminated. And then you run the simulation.</p><p>Now, let\u2019s try to understand how the graph you just saw is generated from simulation runs. You pick one value for \u201cn\u201d, for example 6, and then you run many simulations with n set to 6. Then you average out all of these simulation runs to get an average simulation. The resulting curve you see in the graph is generated from this average simulation.</p><p>Consider this one-dimensional version of a simulation:<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/dwict2qssrx1qhmkqgi0.png\"></figure><p><br>Space has just one dimension, the horizontal axis. Time is the vertical axis. The \u201ccones\u201d you see are the grabby civilizations. They are born at a certain point, and then they expand till they meet another civilization. Now consider the curve we just saw. You can link each origin point in this one-dimensional simulation to a point in the function. Each of the points corresponds to a different time and percentile. But this is a simplified example. We are doing this with a one-dimensional simulation, but the graphs are generated using 3D simulations.</p><p>Having understood all this, we can also better characterize how the parameter \u201ck\u201d of the model is estimated. Maybe you\u2019ve noticed that we haven\u2019t ever mentioned a specific value for \u201ck\u201d during the video. This is because we don\u2019t need to calculate \u201ck\u201d explicitly to generate predictions from simulations. Given a particular simulation, we can iterate through each of its civilization origins, and set that date to be our current date of 13.8 billion years, adjusting all the other dates in that simulation proportionally. This iteration gives us a distribution of answers to whatever question we might be asking. This process in effect assumes that humanity\u2019s rank among the grabby aliens is equally likely to be any particular rank. That is, we are just as likely to be at rank 18% as at rank 72% Therefore \u201ck\u201d is given implicitly using this method.</p><p>Now that we have understood more or less how the model predictions are generated, let\u2019s see a few more of them:<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/ikobagjll2awbt89haip.png\"></figure><p><br>This graph shows how much of the universe grabby civilizations will collectively control at each particular origin date rank. For example, suppose our origin date is in the middle among other origin dates. In that case, that means other grabby civilizations have already taken about 30% to a half of the universe, depending on the number of hard steps we have plugged into the model.&nbsp;</p><p>We can also say how many galaxies each grabby civilization controls when it meets another grabby civilization. In this case, \u201cgalaxy\u201d means a clump of more than 1 million stars. The average galaxy has a hundred million stars, and our milky way galaxy has 100 billion stars.<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/yty1f2blpbisstnpaw4n.png\"></figure><p><br>As you can see, grabby civilizations will control between 10 and 10 billion galaxies by the time they meet another one, depending on how lucky they are and the number of hard steps. Without including the more unlikely extremes, the number falls between 1000 and 1 billion galaxies. So we can expect to become quite a big civilization ourselves by the time we meet aliens - provided that we don\u2019t suffer an extinction event or something that permanently curtails our potential.&nbsp;<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/q5kw8oqxxch0eupsa6l0.png\"></figure><p><br>The earlier a grabby civilization appears, the more galaxies it can grab. The earliest civilizations will be able to take as much as 10 billion galaxies. Civilizations appearing very late will have to settle for the crevices between other grabby civilizations, which could mean way less than a single galaxy to 1 million galaxies, depending on the number of hard steps.</p><p>Given our appearance date of 13.8 billion years, and if the number of hard steps is our middle estimate of six, we can expect to take between ten thousand and 100 million galaxies. Cosmic-humanity might be quite vast!&nbsp;</p><p>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994828/mirroredImages/8Z3zFnDKRhGku3qje/puywpij4g7lmpum2yssj.png\"></figure><p><br>When will this happen? The model predicts that we\u2019ll meet another grabby civilization in about 50 million to 50 billion years. One hundred million to 10 billion years if we exclude the extreme cases.</p><p><strong>SETI and probability of becoming grabby&nbsp;</strong></p><p>Another two very important things that we can try to understand are:&nbsp;</p><p>1. How likely is it that we will become grabby? And 2. How likely is it that we will observe signs of other non-grabby civilizations? Such as alien messages or signatures of alien technology? It turns out these questions are intimately related.</p><p>Let\u2019s say that in the universe, the number of non-grabby civilizations is R times the number of grabby civilizations. Then, R is the ratio between non-grabby and grabby civilizations.</p><p>The number of non-grabby civilizations includes all the non-grabby civilizations ever to exist, even the ones that at some point transition to grabby. For the purpose of calculating this ratio R, the set of non-grabby civilizations contains the set of grabby civilizations.</p><p>Therefore, we can say that 1 out of R is the chance any non-grabby civilization becomes grabby in the absence of other information.</p><p>For example, if non-grabby civilizations are 100 times more common than grabby ones, then each non-grabby civilization has only a one in a hundred chance of becoming grabby.</p><p>The more non-grabby civilizations there are, the higher is R, because the number of grabby civilizations is fixed by our choice of model parameters. But the more non-grabby civilizations there are, the higher is our chance to succeed in the search for extraterrestrial intelligence.</p><p>This means that the higher our chances of finding signs of aliens are, the lower is our chance of becoming grabby ourselves. Thus It would be <i>very</i> bad news to hear alien messages or find other non-grabby civilizations, for example, by spotting signatures of alien technology. That would mean R is high and that our chance of becoming grabby is low.</p><p>Why should we care about our chances of becoming grabby being low? Because many scenarios in which we don\u2019t become grabby happen because we become extinct or suffer a catastrophe that permanently limits our potential.&nbsp;</p><p>This reasoning reminds us of the original great filter paper and of arguments <a href=\"https://www.overcomingbias.com/2021/07/what-shows-smaller-future-filter.html\"><u>already made by Hanson</u></a>. He argues that finding any sort of extraterrestrial life that is not yet big and visible on an astronomical scale would increase our probability estimate that the great filter is ahead of us. Therefore it would increase our probability estimate that we\u2019re headed for extinction.&nbsp;</p><p>Consider this: Let\u2019s pretend for a moment that grabby aliens expand at the speed of light. Then, if we estimate six hard steps, to expect any non-grabby civilization to have <i>ever</i> been active in our galaxy, a ratio of over 10-thousand is required. This ratio gets smaller with lower speeds. For example, assuming our lower-bound speed of \u2153 of the speed of light, the ratio would be 27 times smaller: roughly 400. Still pretty high. But if we allow even lower speeds, then R becomes even smaller, and the conflict between the probability of finding signs of aliens and of surviving becomes less and less pronounced.</p><p>But there\u2019s more: to expect just one other non-grabby civilization to be active now in our galaxy, a ratio of over one million is required. This is valid assuming that non-grabby civilizations last 1 million years. If they last 10 times less, then a ten times larger ratio is required, and if they last more, the ratio required goes down. But it doesn\u2019t get lower than the ratio required for another non-grabby civilization to have ever been active in our galaxy.</p><p>Therefore, if higher speeds are possible and if you believe that we are not alone in our galaxy, you also need to accept that the chance to become grabby ourselves is vanishingly small. And if you believe that our chances of becoming grabby are good, then we are almost surely alone in our galaxy. Pick one.</p><p><strong>Conclusion</strong></p><p>And this is it. Now, you should have a pretty clear picture. Notice how this is not just speculation. The strength of this model is that its basic assumption, grabby civilizations, stems from a very precise observation: that humanity looks early. And as we\u2019ve seen, the mathematics of the model can be grounded in observations too. To strongly reject the model, you need to reject that we\u2019re early or find a much better explanation than grabby aliens. Otherwise, the model looks reasonably likely. My gut tells me that grabby aliens have a decent chance of actually being a correct assumption.&nbsp;</p><p>If grabby aliens as described in this video are true, then the universe is about to enter a new phase. Up until now, cosmology has been about dead stuff. But soon, everything might be filled with life that restructures everything it touches according to its goals. What will become of the universe will be decided by life itself rather than by dead processes.&nbsp;</p><p>If this is all true, I hope that humanity will survive to join the feast. But if we don\u2019t manage to survive, grabby aliens might come across our ruins, so remember to write respectful comments, just in case they discover this video.</p>", "user": {"username": "Writer"}}, {"_id": "qPvZJkcqvGAeFt3o6", "title": "Takeaways on US Policy Careers (Part 2): Career Advice", "postedAt": "2021-11-08T08:42:57.801Z", "htmlBody": "<h1>Summary / Key Takeaways</h1>\n<p>Over the summer, about a dozen very cool policy professionals shared their takes on impactful US policy careers. The sessions were not recorded, so\u2014in hopes that others find the content as useful as I did\u2014here\u2019s Part 2 of what I learned (companion post <a href=\"https://forum.effectivealtruism.org/posts/z9hzAB9mfgcpXmcze/takeaways-on-us-policy-careers-part-1-paths-to-impact-and\">here</a>). My main personal takeaways are in this section: suggestions for people at various stages in their careers, corrections of a few misconceptions, and a list of resources/compilations in this post.</p>\n<p><strong>Moves to seriously consider at various career stages:</strong> Here are some options that are often among the best things to do at various stages of your career, if you\u2019re optimizing for US policy jobs and don\u2019t need to worry about US work authorization (although there are plenty of exceptions, and you can very much be fine if you skip some steps or do them differently):</p>\n<ul>\n<li><strong>1) If you\u2019re an undergrad</strong>, DC-based internships are very useful, and Semester in Washington programs are often very useful for getting these internships (and for additional benefits).\n<ul>\n<li>Your undergraduate degree matters little for most policy jobs; internships, connections, learning, etc. matter more.</li>\n</ul>\n</li>\n<li><strong>2) If you just finished your undergrad</strong>, spend about 1-2 years interning/working in Congress or DC think tanks.\n<ul>\n<li>This can both set you up well for a policy-relevant graduate degree and inform you about your fit for it.</li>\n<li>On breaking into policy work:\n<ul>\n<li>The hardest part of getting into policy work is getting a foot in the door; then, you can use your network from your first job to get the next job. So if you want to go into policy, just get started somehow\u2014even if your first job isn\u2019t very high-impact.</li>\n<li>If you\u2019re sympathetic to this community and looking to break into policy work (or connect with likeminded policy professionals), get in touch with the DC EA community.</li>\n<li>Get your application materials thoroughly reviewed by people who know what application reviewers are looking for. Call in favors / draw on connections to get this very useful help.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>3) If you\u2019ve worked in US policy for ~1-2 years\u2014or without doing that</strong>, if you have trouble getting those jobs)\u2014a great next move is often going to graduate school to get a policy master\u2019s degree (in a DC-based program) or a law degree (in DC or a top program). If you can, intern / work (part-time / in summers) in Congress or think tanks as you do your degree.\n<ul>\n<li>\n<p>Broadly, high-impact jobs in government disproportionately require some sort of graduate study.</p>\n</li>\n<li>\n<p>Top options for policy-oriented master\u2019s degrees include:</p>\n<ul>\n<li><a href=\"https://sfs.georgetown.edu/\">Georgetown\u2019s School of Foreign Service</a> / <a href=\"https://mccourt.georgetown.edu/\">McCourt School of Public Policy</a></li>\n<li><a href=\"https://sais.jhu.edu/\">Johns Hopkins\u2019 School of Advanced International Studies</a><sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-1\" id=\"fnref-H9rB9jFyxokwFBvnj-1\">[1]</a></sup></li>\n</ul>\n</li>\n<li>\n<p>Law degrees are often very useful, although you typically shouldn\u2019t start a law program if you\u2019re set on doing think tank research (they\u2019re not as useful for that).</p>\n<ul>\n<li>(Some people may have the impression that they need to go to e.g. Harvard Law School to do the most impactful US policy work. Anecdotally, this seems to be incorrect.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-2\" id=\"fnref-H9rB9jFyxokwFBvnj-2\">[2]</a></sup>)</li>\n</ul>\n</li>\n<li>\n<p>While relevant PhDs can be useful, the time/opportunity cost is huge, so starting a (US-based) PhD is often not as good for a career in US policy as doing a shorter graduate degree followed by several years of policy work.</p>\n<ul>\n<li>Getting a PhD might be a great move if you\u2019ve already started a PhD program, if you do it in the UK (much lower time costs), if you\u2019re targeting jobs in US executive agencies for which PhDs are very helpful, or if you\u2019re very interested in certain jobs outside of US policy.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-3\" id=\"fnref-H9rB9jFyxokwFBvnj-3\">[3]</a></sup></li>\n</ul>\n</li>\n<li>\n<p>It\u2019s very valuable if the graduate program you do is located in DC, for work and networking opportunities.</p>\n</li>\n<li>\n<p>You can often <a href=\"https://forum.effectivealtruism.org/posts/DqwxrdyQxcMQ8P2rD/list-of-ea-funding-opportunities\">get funding</a> to use a top degree to tackle existential risks.</p>\n</li>\n<li>\n<p>When it comes to school, consider <a href=\"https://mindingourway.com/half-assing-it-with-everything-youve-got/\">\u201cHalf-assing it with Everything You've Got.\u201d</a></p>\n</li>\n</ul>\n</li>\n<li><strong>4) If you (will soon) have a grad degree (especially in STEM) or years of STEM work experience</strong>, use a full-time policy fellowship program (or connections from earlier work) to get a job in Congress, think tanks, or the executive branch.\n<ul>\n<li>See <a href=\"https://docs.google.com/document/d/1cSQYcOzd5ZLdFwizNYlJqrgegfytfNhUcCZPV6jZSRc/edit#heading=h.kpkngtxuq611\">this section</a> for a non-comprehensive list and comparison of these fellowship programs.</li>\n</ul>\n</li>\n<li><strong>5) If you\u2019re already working in the policy world</strong>, use connections from your job to advance in your career. Meanwhile, carve out and be deeply protective of time for thinking strategically about how you can be most impactful.</li>\n<li><strong>As soon as you can in your career:</strong> If EA resonates strongly with you, find collaborators in EA.</li>\n</ul>\n<p><strong>Contrary to some misconceptions:</strong></p>\n<ul>\n<li>You don\u2019t need a strong technical background to get many high-impact jobs in US technology policy.</li>\n<li>If you\u2019re legally a US permanent resident but not yet a US citizen, you can still get impactful policy jobs outside government (e.g. at think tanks) and some jobs in Congress.</li>\n<li>Anecdotally, DC policy folks tend to be very friendly (in ways that, at least so far, have felt authentic).</li>\n</ul>\n<p><strong>Resources in this post:</strong> This post contains several (hopefully) useful compilations\u2014check out whichever seem useful for you.</p>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Fellowships___List_and_Comparison\">A list and comparison of US policy fellowships</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Reading___podcast_recommendations\">Recommended readings and podcasts, by subject</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Funding__jobs__and_other_resources\">A compilation of funding/job/other resources for (prospective) graduate students</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Tips_for_getting_a_security_clearance\">Tips for getting a security clearance</a></li>\n<li>Advice for people with various other specific backgrounds:\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Advice_for_undergraduates\">Undergraduates</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Advice_for_non_US_citizens\">Non-US citizens</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Career_options_in_US_policy_for_people_with_strong_technical_backgrounds\">People with STEM backgrounds</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Career_options_in_US_technology_policy_for_people_without_strong_technical_backgrounds\">People without STEM backgrounds</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Advice_for_program_event_organizers\">Program/event organizers</a></li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Various_additional_career_tips\">Academics seeking journalists\u2019 coverage</a></li>\n</ul>\n</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice#Additional_Takeaways\">A summary of additional key takeaways (next section)</a></li>\n</ul>\n<h2>Additional Takeaways</h2>\n<ul>\n<li>On <strong>career paths/mindsets:</strong>\n<ul>\n<li>Many impact-driven people, e.g. impact-driven people working in US policy, should not be confident about any detailed long-term career plan, or even spend much time making detailed long-term plans. In many US policy careers, the best choices are very hard to plan far in advance.</li>\n<li>It\u2019s very common for policy professionals to move between policy jobs in different parts of the policy ecosystem.</li>\n<li>You\u2019ll likely be changing jobs a lot\u2014aim for growing/learning/self-investment more than immediate impact in your first job.</li>\n</ul>\n</li>\n<li>Advice applicable <strong>outside of work hours</strong>:\n<ul>\n<li>Build and really value relationships.</li>\n<li>Being a pleasant person is very valuable\u2014people want pleasant/good-humored/happy colleagues.</li>\n<li>Value contributors to good long-term physical and emotional health (long-term relationships, exercise, sleep, etc.). These are very useful for life-long productivity and for having a low risk of burnout. If you are experiencing mental health difficulties, consider getting therapy.</li>\n<li>Be very aware of and careful with what might cause irreversible reputational harms or slow down a security clearance. Keeping a low profile online seems usually worthwhile for keeping options open.</li>\n<li>If you prioritize problems that are mostly not funding-constrained, consider time your most valuable resource\u2014be willing to spend money to save time.</li>\n</ul>\n</li>\n<li>On fitting in <strong>culturally</strong>:\n<ul>\n<li>DC is relatively hierarchical in terms of age and job responsibilities. Being seen as someone who\u2019s respectful and trying to learn will come across as way better than trying to be the Silicon Valley-style kid genius who doesn\u2019t need to listen and has figured it out.</li>\n<li>Learning to speak the \u201cpolicy language\u201d is useful for getting policy jobs\u2014listening to this language a lot (e.g. through the reading and podcast recommendations) is very useful for learning it.</li>\n</ul>\n</li>\n<li>There is much (at least superficial) <strong>interest from this community</strong> in high-impact US policy careers; the speaker series got over 500 signups.</li>\n</ul>\n<h2>Context</h2>\n<p>This is Part 2 of a 2-part series; see the companion post <a href=\"https://forum.effectivealtruism.org/posts/z9hzAB9mfgcpXmcze/takeaways-on-us-policy-careers-part-1-paths-to-impact-and\">here</a>.</p>\n<ul>\n<li><strong><a href=\"https://forum.effectivealtruism.org/posts/z9hzAB9mfgcpXmcze/takeaways-on-us-policy-careers-part-1-paths-to-impact-and\">Part 1</a></strong> mostly gives descriptive context about US policy careers (as well as some advice that is difficult to separate from these descriptions).</li>\n<li><strong>Part 2</strong> (this part) is fully focused on sharing advice.</li>\n</ul>\n<p>Most of the information here is my understanding of what was shared in this summer\u2019s <a href=\"https://forum.effectivealtruism.org/posts/E55hbrjfvBHrAfwzj/us-policy-careers-speaker-series-summer-2021\">US policy careers speaker series</a>. To respect speakers\u2019 privacy, the sources of information here have all been anonymized, and mixed with a few other readings and conversations.</p>\n<p>Discussion focuses on US policy careers, but I\u2019d guess there are significant similarities to policy careers in other countries.</p>\n<p>Caveats:</p>\n<ul>\n<li>Panelists in the speaker series are not necessarily affiliated with this forum or related organizations, communities, and ideas.</li>\n<li>This is extremely far from being comprehensive.</li>\n<li>It\u2019s very possible that I misheard/misunderstood some things.\n<ul>\n<li>If something here is critical for a career or advocacy decision, please get it at least double-checked before you use the information.</li>\n</ul>\n</li>\n</ul>\n<p>I\u2019m thankful to the speakers and my co-organizer for sharing their time and experience, as well as to Fiona Pollack, Felipe Calero, and especially <a href=\"https://forum.effectivealtruism.org/users/kuhanj\">Kuhan Jeyapragasan</a> for note-taking and comments.</p>\n<h1>General advice on US policy career planning</h1>\n<p><em>If you\u2019re here, you might be planning on reading this from top to bottom. I suggest instead skipping around to sections you find especially relevant/interesting.</em></p>\n<p>We\u2019ll start with more widely relevant advice and insights for career planning.</p>\n<h2>Advice on whether to go into US policy</h2>\n<p>An impact-driven policy professional advices:</p>\n<ul>\n<li>Many more EAs could and should go into policy because:\n<ul>\n<li>The potential for impact is large scale; the US federal government...\n<ul>\n<li>Directs a huge budget: ~$4 trillion per year\n<ul>\n<li>Caveat: only ~$1.5 trillion per year of that is discretionary / able to be moved</li>\n</ul>\n</li>\n<li>Has unique authority to regulate</li>\n<li>Is the world\u2019s largest intelligence enterprise</li>\n</ul>\n</li>\n<li>Much of the impact is counterfactual\u2014the large majority of policy professionals (including the person you\u2019d likely be replacing) aren't focused on the issues of greatest concern to this community.</li>\n<li>Policy can absorb many people.</li>\n<li>Policy work offers good (transferable) career capital\n<ul>\n<li>E.g. you can fairly easily pivot between AI &amp; biosecurity; you can even work on multiple cause areas at the same time.</li>\n</ul>\n</li>\n<li>Many EAs are plausibly good personal fits for policy.\n<ul>\n<li>There are many kinds of jobs (EAs often focus on just a few and then mistakenly self-select out).</li>\n<li>E.g. there are lots of non-extroverted people at some think tanks doing good work.</li>\n<li>People who really don\u2019t want to self-censor can be advocates, journalists, etc (although they\u2019d still need to keep audiences in mind).</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Advice on career mindsets</h2>\n<p>A few speakers suggested that many impact-driven people, e.g. impact-driven people working in US policy, should not be confident about any detailed long-term career plan, or even spend much time making detailed long-term plans. As discussed below, in many careers, including many in US policy, the best choices are very hard to plan far in advance. That\u2019s because future opportunities are very hard to predict\u2014they involve collaborators / employers you might not have met yet, or organizations that might not yet exist.</p>\n<p>More broadly, the possibility of finding / creating new opportunities is a reason for many people to not be totally committed to a very specific long-term career plan.</p>\n<p>(This is presumably less true for some other careers that require people to take pre-established long-term paths, e.g. some careers in academia.)</p>\n<p>Some other impact-motivated policy professionals also shared the following advice:</p>\n<ul>\n<li>\n<p>Before starting a graduate degree, it\u2019s often useful to spend ~1-2 years working in DC if you can find decent options\u2014these can help you test fit and prepare you for graduate degrees.</p>\n<ul>\n<li>Working in Congress or think tanks can be especially great during this time (since it\u2019s also recommended to max out your credentials before coming into the executive branch.)</li>\n</ul>\n</li>\n<li>\n<p>If there\u2019s something that seems valuable to you, but you have e.g. a slightly different slant on it from others in EA, at least nurture that and take it seriously.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-4\" id=\"fnref-H9rB9jFyxokwFBvnj-4\">[4]</a></sup></p>\n</li>\n<li>\n<p>Everyone interested in policy should consider spending time in government, but not everyone should spend all their time in government\u2014it\u2019s valuable for your career and society to transfer lessons/knowledge between parts of society.</p>\n<ul>\n<li>So consider a stint of a few years in government, then try something else, and then maybe come back.</li>\n</ul>\n</li>\n<li>\n<p>Two speakers spent much of their college years working low-wage jobs, and they have still gotten to high-impact career paths; strategically pursuing impact early on helps, but if you haven\u2019t done so yet, you still have a shot.</p>\n</li>\n</ul>\n<h2>What do US policy career paths often look like?</h2>\n<p>It\u2019s very common for policy professionals to move between policy jobs in different parts of the policy ecosystem.</p>\n<p>To illustrate this, here are examples of some people\u2019s US policy career paths (just in their first several years, for some of them):</p>\n<ul>\n<li>Think tank \u2192 executive branch \u2192 executive branch (a different part)</li>\n<li>Congress \u2192 nonprofit (lobbying)</li>\n<li>Think tank \u2192 executive branch</li>\n<li>Academia \u2192 executive branch \u2192 executive branch (a different part)</li>\n<li>Congress \u2192 executive branch \u2192 Congress (a different part)</li>\n<li>Think tank \u2192 journalism</li>\n<li>Nonprofit (research) \u2192 Congress \u2192 nonprofit (organizing) \u2192 nonprofit (lobbying)</li>\n</ul>\n<p>Congress is one good way to get a good job in an agency. Many of the influential jobs in agencies are political appointments; the White House often pulls from Congressional staff of their party to make these appointments.</p>\n<p>An exception: it\u2019s hard to transition from journalism to policy; employers may be suspicious of you.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-5\" id=\"fnref-H9rB9jFyxokwFBvnj-5\">[5]</a></sup></p>\n<h2>Advice on your first job in policy</h2>\n<p>One policy professional advises: the hardest part of getting into policy work is getting a foot in the door; then, you can use your network from your job to get the next job. Things build on themselves after your first job.</p>\n<p>For example, think tank researchers can interact with and impress people in Congress and the executive branch, and this gives them job opportunities.</p>\n<p>This means getting your first job in policy is especially important. Here\u2019s some advice for doing that:</p>\n<ul>\n<li>Use fellowships to enter policy work during / right after a graduate program (see below for a list of these fellowships).</li>\n<li>If you plan to get a graduate degree, get it before going into the executive branch so that you\u2019re not coming in too junior\u2014max out your credentials before you enter the executive branch.</li>\n<li>Where to get your first job?\n<ul>\n<li>The large career capital that your first DC policy job will offer you means that, if you want to go into policy, just get started somehow\u2014even if your first job isn\u2019t very high-impact.</li>\n<li>One Congressional staffer thinks Congress is a good place to begin. They explain: you\u2019ll make lots of connections, likely go on to other government positions, get a sense of how Congress works, and learn about a relatively wide range of policy issues.</li>\n</ul>\n</li>\n<li>How to get your first job?\n<ul>\n<li>Doing a part-time masters program in DC (see the section on graduate school for more details) can be a good way to get the connections and opportunity to get your first policy job.</li>\n<li>Consider volunteering for projects and doing pro-bono policy work if you are transitioning into a new area.\n<ul>\n<li>(I\u2019m guessing that this advice is geared toward people who have undergraduate degrees or are more senior, and that non-governmental organizations such as think tanks and advocacy nonprofits will be more receptive to volunteers.)</li>\n<li>One policy professional spent their early career doing many part-time-consultancies/volunteer projects (in parallel), for their resume and mostly because they were very unsure what they wanted to do.\n<ul>\n<li>They would just go to an organization they were interested in, tell them what they were interested in, and offer to volunteer for a few months if they were interested.\n<ul>\n<li>A handful of those turned into paid jobs.</li>\n<li>They found this structure more interesting than that of a formal internship.</li>\n<li>Use other money (e.g. from paid time or grants) to fund this time.</li>\n<li>To get started, cold-emailing people is a decent option.</li>\n</ul>\n</li>\n<li>(This might often be a better approach than the above, since such funding is often available in the community.)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>How to get a new job if that requires pivoting jobs?\n<ul>\n<li>It\u2019s not that hard, one professional advises\u2014be guileless enough to reach out to someone and ask for help.</li>\n<li>For figuring out whether/how to pivot, think more deeply earlier about what the most important problems in the world are\u2014one professional wishes they had done more of that thinking earlier.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>What to focus on in your first job?\n<ul>\n<li>You\u2019ll likely be changing jobs a lot\u2014aim for growing/learning/self-investment more than immediate impact in your first job.</li>\n</ul>\n</li>\n</ul>\n<h2>Advice on picking which problems and projects to work on</h2>\n<p>A few impact-driven policy professionals recommend:</p>\n<ul>\n<li>\n<p>Work on neglected problems\u2014see footnote for why.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-6\" id=\"fnref-H9rB9jFyxokwFBvnj-6\">[6]</a></sup></p>\n</li>\n<li>\n<p>Try to focus on projects that are interesting and let you work with people you really admire (e.g. think are ethical and can learn a lot from).</p>\n</li>\n<li>\n<p>Aim for being more of a generalist.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-7\" id=\"fnref-H9rB9jFyxokwFBvnj-7\">[7]</a></sup></p>\n</li>\n<li>\n<p>For jobs and career paths whose paths to impact are less well-established / explored, talk with lots of people and identify gaps (things others aren\u2019t trying to do which you can do).</p>\n</li>\n</ul>\n<h2>On career paths involving specific parts of the policy ecosystem</h2>\n<p>Congress</p>\n<ul>\n<li>Work in Congress offers good exit options\u2014many employers like legislative experience.</li>\n<li>Ways for mid-career people with advanced degrees to break into Congressional staffing:\n<ul>\n<li>There are good fellowship opportunities (see the section that lists fellowship opportunities):\n<ul>\n<li>There\u2019s room for mid-career people to get into mid-level roles in Congress, or even into committees if they have lots of expertise.</li>\n</ul>\n</li>\n<li>Congressional support agencies also offer options for people with more expertise.</li>\n</ul>\n</li>\n</ul>\n<p>Think tank work</p>\n<ul>\n<li>People sometimes treat think tank work as an alternative to working in government, but the revolving door dynamic is really important\u2014individuals usually switch between time thinking and time implementing policy (i.e. they switch between working in think tanks and working in government).\n<ul>\n<li>So one speaker doesn\u2019t think of think tank work as a career on its own\u2014the career path is more typically working in policy / in some policy area.</li>\n<li>People often switch out of government because they burn out or get replaced by new administrations.</li>\n<li>Although it\u2019s less common, some people who are less good fits for government work stay in think tanks.</li>\n</ul>\n</li>\n</ul>\n<h2>Career options for people with various backgrounds</h2>\n<p>This section discusses career options in US policy for non-US-citizens, for people who don\u2019t have strong STEM backgrounds, and for people who do have strong STEM backgrounds.</p>\n<h3>Advice for non-US-citizens</h3>\n<p>Comparing job options in US policy for non-US-citizens:</p>\n<ul>\n<li>These US policy jobs are often open to non-US-citizens who are legally US permanent residents:\n<ul>\n<li>Working at a think tank</li>\n<li>Policy journalism</li>\n<li>Congressional staffing</li>\n<li>Advocacy</li>\n<li>Political data analytics</li>\n<li>Supporting others\u2019 careers (e.g. advising, operations)</li>\n</ul>\n</li>\n<li>US policy jobs that are significantly more difficult for non-US-citizens (although often feasible):\n<ul>\n<li>Working in executive branch jobs (especially tough in the national security space\u2014getting a security clearance will be hard)</li>\n</ul>\n</li>\n<li>US policy jobs that are off the table for non-US-citizens:\n<ul>\n<li>Running for federal office (constitutionally limited to US citizens<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-8\" id=\"fnref-H9rB9jFyxokwFBvnj-8\">[8]</a></sup>)</li>\n</ul>\n</li>\n</ul>\n<p>Non-citizens can get permanent residency (and thus get on a path to citizenship) via employer sponsorship or through marriage / family ties.</p>\n<h3>Career options in US technology policy for people without strong technical backgrounds</h3>\n<p>Even in technology policy, people can make decent policy without extensive technical backgrounds; a very wide range of backgrounds can work. (See footnote for why.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-9\" id=\"fnref-H9rB9jFyxokwFBvnj-9\">[9]</a></sup>)</p>\n<p>Anecdotally, one professional with a policy job that is very focused on technology only did a STEM minor.</p>\n<h3>Career options in US policy for people with strong technical backgrounds</h3>\n<ul>\n<li>Personal fit considerations for people with strong STEM backgrounds:\n<ul>\n<li>Even people who are world experts at something are unlikely to actually use that expertise in government (except e.g. if they\u2019re in a lab or working on some subject that is more often used in policymaking, like cryptography).</li>\n</ul>\n</li>\n<li>What should people with strong STEM backgrounds do?.\n<ul>\n<li>Use that background as a springboard to get in (even if that\u2019s to a job that doesn\u2019t have much direct impact right away.)\n<ul>\n<li>Fellowship programs for STEM folks are especially promising ways to break in\u2014see the section listing and comparing fellowship opportunities to see which are most interested in STEM folks.</li>\n</ul>\n</li>\n<li>A broad background/advanced degree in STEM is definitely enough to work at the intersection of technology and national security, and in the executive branch more broadly (although it\u2019s not necessary for any of that).</li>\n<li>STEM PhD backgrounds are very useful for becoming a program manager at IARPA or DARPA, where you may be able to create and fund impactful research programs with budgets in the tens of millions (?) of dollars.</li>\n<li>Another potential option can be the Office of Science and Technology Policy, which has many STEM PhDs.</li>\n</ul>\n</li>\n<li>See footnote for advice on getting into policy journalism.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-10\" id=\"fnref-H9rB9jFyxokwFBvnj-10\">[10]</a></sup></li>\n</ul>\n<h1>General advice for being very positively impactful</h1>\n<h2>On prioritizing and planning well</h2>\n<p>Everyone should \u201cwaste an hour per week so they don\u2019t waste 1,000 hours per year.\u201d Reserving time for thinking strategically about how you can be most impactful often has very high returns.</p>\n<p>Carve out and be deeply protective of time for strategic thinking; one policy professional ensures they have time blocked to think about whether they\u2019re losing sight of the most important problems they\u2019re working on.</p>\n<h2>On networking often and well</h2>\n<p>Some policy professionals advise:</p>\n<ul>\n<li>Build and really value relationships.\n<ul>\n<li>This includes relationships with people who think differently from you/have different priorities from you.</li>\n<li>Try to build a positive relationship with everyone you encounter, even if that\u2019s over a bagel at a conference.</li>\n</ul>\n</li>\n<li>If EA resonates strongly with you, find collaborators in EA early.\n<ul>\n<li>One policy professional would have told their past self to reach out to this EA community, even though they seem busy. They suggest that this would have helped them start highly promising work much earlier, and feel much less alone.</li>\n<li>Ways you can start getting connected include <a href=\"https://80000hours.org/speak-with-us/\">80,000 Hours career advising</a> and the <a href=\"https://www.effectivealtruismdc.org/connect\">EA DC community</a>.</li>\n</ul>\n</li>\n<li>Consider keeping a list of people whose work you really admire and cold-calling / cold-emailing them; people often reply.\n<ul>\n<li>If you can\u2019t find someone\u2019s email, you can guess.</li>\n</ul>\n</li>\n<li>Being a pleasant person is very valuable\u2014people want pleasant/good-humored/happy colleagues.\n<ul>\n<li>\n<p>This is especially true in DC.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-11\" id=\"fnref-H9rB9jFyxokwFBvnj-11\">[11]</a></sup></p>\n</li>\n<li>\n<p>Conversely, especially abrasive people tend to do worse.</p>\n</li>\n<li>\n<p>The direct critical feedback of the rationality community won\u2019t fly\u2014people who are used to those norms should temper them to succeed in DC (i.e. more often be diplomatic than critical).</p>\n</li>\n</ul>\n</li>\n<li>Leaning on curiosity and empathy is very valuable.\n<ul>\n<li>They will help you be willing to spend the time doing the following, which can be very useful:\n<ul>\n<li>When talking to someone, think about: What are they thinking? What are they stressed about? What have they spent their day doing? What does their boss want?\n<ul>\n<li>Putting yourself in people\u2019s shoes is valuable for connecting and being better at emphasizing how things you ask for connect with what others care about.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>On personal health and well-being</h2>\n<p>Value long-term relationships, exercise, sleep, and other contributors to good long-term physical and emotional health. These are very useful for life-long productivity and for having a low risk of burnout. If you are experiencing mental health difficulties, consider getting therapy.</p>\n<h2>On saving valuable time</h2>\n<p>If you prioritize problems that are mostly not funding-constrained, consider time your most valuable resource\u2014be willing to spend lots of money to save time (e.g. getting a short commute, reducing cooking time).</p>\n<p>Relatedly, don\u2019t over-optimize for things other than your top priority\u2014just use satisficing heuristics.</p>\n<ul>\n<li>Don\u2019t spend much time thinking about something unless it\u2019s highly consequential/irreversible.\n<ul>\n<li>(Things can be highly consequential even if you don\u2019t yet know exactly how\u2014please don\u2019t ignore things that seem highly important on the basis of this advice.)</li>\n</ul>\n</li>\n<li>Manage your \u201canxiety budget.\u201d</li>\n</ul>\n<h1>Advice on grad degree choice and what to do in grad school</h1>\n<p><em>Caveat: I\u2019m personally still very unsure about what grad degrees would make sense for people in various situations (or what the crucial considerations even are), so the advice here is especially far from being comprehensive. Still, I think there\u2019s useful stuff here.</em></p>\n<h2>General advice</h2>\n<ul>\n<li>If you want to work in DC but are struggling to get a job, you should first consider getting a professional policy-focused master\u2019s degree. They also make recommendations about specific programs that are especially good.</li>\n<li>Value of grad education:\n<ul>\n<li>High-impact jobs in policy disproportionately require some sort of graduate study.\n<ul>\n<li>As supporting evidence, well over half of senior positions in Congress, the executive branch, and think tanks seem to be held by people with graduate degrees.</li>\n</ul>\n</li>\n<li>At least in the context of executive branch work, graduate degrees can be useful because they (or at least certain ones) will:\n<ul>\n<li>Make you be listened to more often</li>\n<li>Help you discern if some info is relevant to you (which is very useful, since you\u2019ll be a \u201cfact taker, not fact maker\u201d)</li>\n<li>Help you set up processes to test views you hear</li>\n<li>Help you learn new things more quickly (e.g. through quantitative fluency)</li>\n<li>Help you with knowing who knows relevant facts and be able to draw from them, which is useful in (technology) policy</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>It\u2019s very valuable if the graduate program you do is located in DC, for work and networking opportunities.</li>\n<li>It\u2019s typically better to get a graduate degree when you\u2019re young (although not necessarily straight out of undergrads) because the payoffs can accumulate each year that you have the degree.</li>\n<li>Fellowships are great for people who have graduate degrees to start their policy careers\u2014see the section below on fellowships.</li>\n<li>Consider <a href=\"https://mindingourway.com/half-assing-it-with-everything-youve-got/\">\u201cHalf-assing it with Everything You've Got\u201d</a></li>\n</ul>\n<h2>Advice on what degrees to get for specific parts of the policy world</h2>\n<ul>\n<li>\n<p>What graduate degree to get for (senior) executive branch jobs? We can get imperfect<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-12\" id=\"fnref-H9rB9jFyxokwFBvnj-12\">[12]</a></sup> information about that by looking at what degrees people in these jobs tend to hold\u2014it varies a lot by agency:</p>\n<ul>\n<li>\n<p>Some places have lots of law degrees:</p>\n<ul>\n<li>\n<p>Among a few top national security roles (Secretary of State, Secretary of Homeland Security, and Director of National Intelligence), as well as for the Secretary of Commerce and Secretary of Agriculture, law degrees are very common\u2014they\u2019re the most advanced degrees held by 60%-80% of the last 5 people<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-13\" id=\"fnref-H9rB9jFyxokwFBvnj-13\">[13]</a></sup> to hold each of these positions. (Most of the rest had master\u2019s degrees.)</p>\n</li>\n<li>\n<p>The National Security Council is around half people with law degrees (and around half policy master\u2019s).<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-14\" id=\"fnref-H9rB9jFyxokwFBvnj-14\">[14]</a></sup></p>\n</li>\n</ul>\n</li>\n<li>\n<p>Some places have lots of technical advanced degrees:</p>\n<ul>\n<li>\n<p>The last 5 (non-acting) directors of a few science/technology agencies (<a href=\"https://en.wikipedia.org/wiki/Office_of_Science_and_Technology_Policy#Directors\">OSTP</a>, <a href=\"https://www.darpa.mil/attachments/DARPA_Directors_Sheet_20201.pdf\">DARPA</a>, and <a href=\"https://en.wikipedia.org/wiki/Intelligence_Advanced_Research_Projects_Activity#Directors\">IARPA</a>) virtually all have STEM PhDs,<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-15\" id=\"fnref-H9rB9jFyxokwFBvnj-15\">[15]</a></sup> as do many OSTP staff<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-16\" id=\"fnref-H9rB9jFyxokwFBvnj-16\">[16]</a></sup> and majorities of <a href=\"https://www.darpa.mil/about-us/people\">DARPA</a> and <a href=\"https://www.iarpa.gov/index.php/our-program-managers\">IARPA</a> program managers.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-17\" id=\"fnref-H9rB9jFyxokwFBvnj-17\">[17]</a></sup></p>\n</li>\n<li>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Rochelle_Walensky\">last</a> <a href=\"https://www.cdc.gov/about/history/pastdirectors.htm\">5</a> directors of the CDC all have MDs; 3 also have master\u2019s degrees in public health.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>The most advanced degrees of the last 5 US Secretaries of Defense are a mix of non-law degrees.</p>\n</li>\n<li>\n<p>See footnote for a more detailed count of the above.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-18\" id=\"fnref-H9rB9jFyxokwFBvnj-18\">[18]</a></sup></p>\n</li>\n<li>\n<p>(I haven\u2019t looked into other roles.)</p>\n</li>\n</ul>\n</li>\n<li>\n<p>What graduate degree (if any) to get for Congressional staffing?</p>\n<ul>\n<li>A professional with experience in Congress guesses that:\n<ul>\n<li>About 60% of senior staff in Congress members\u2019 personal offices have advanced (JDs/master\u2019s) degrees</li>\n<li>About 80% of senior staff in Congressional committees have advanced (JDs/master\u2019s) degrees</li>\n<li>Almost all senior staff in Congressional support agencies have advanced (JDs/master\u2019s) degrees</li>\n</ul>\n</li>\n<li>Additional advice from that professional:\n<ul>\n<li>Graduate degrees are very useful for one\u2019s reputation/credibility.</li>\n<li>The practicality of education partly determines how useful it is.\n<ul>\n<li>E.g. Georgetown is very practical; others are more academic.</li>\n<li>This affects, for example, how much time / opportunities for policy-focused networking you will have.</li>\n</ul>\n</li>\n<li>MPPs (Master of Public Policy degrees) are useful, law degrees are super useful, and PhDs are often not useful (maybe for some specialized positions)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>What graduate degree to get for think tank research?\n* A few graduate students with relevant interests seemed to agree that a masters degree (not a PhD, and not a law degree) is typically the best option for going into think tank research.</p>\n</li>\n</ul>\n<h2>Law degrees</h2>\n<p><em>This section is mostly based on the advice of a single impact-motivated lawyer. For another lawyer\u2019s perspective, see Cullen O'Keefe\u2019s talk, <a href=\"https://forum.effectivealtruism.org/posts/Fk8wMe95Tzva6h98t/cullen-o-keefe-doing-the-most-good-with-a-law-degree\">\u201cDoing the most good with a law degree.\u201d</a></em></p>\n<p><em>For transparency\u2019s sake, one reviewer mentioned several things in this section are off, so take it with extra grains of salt. (The absence of a similar statement at the top of other sections doesn\u2019t mean they were extensively reviewed.)</em></p>\n<p>Paths to impact:</p>\n<ul>\n<li>Broadly:\n<ul>\n<li>Law backgrounds can be useful for improving how policies are implemented (and good implementation is often critical for policies to go well).</li>\n<li>Law degrees help for working in Congress, because lawyers are very particular about the words they choose\u2014there's an art form of having a good sense of how word choice affects the eventual implementation of a law through the executive branch.</li>\n</ul>\n</li>\n<li>Paths in DC with a law background?\n<ul>\n<li>One recent law graduate sees these as some potentially promising places to work:\n<ul>\n<li>LPP, Office of Legal Counsel (Supreme Court position, very hard to get), NSC, DoD (e.g. Office of Net Assessment)</li>\n<li>Any general counsel\u2019s office might get you some good subject-specific expertise, e.g. the Department of State for expertise on unifying executive agreements.</li>\n<li>Office of Management and Budget\n<ul>\n<li>Within that is OIRA, where they do cost-benefit analysis</li>\n</ul>\n</li>\n<li>DARPA, BARDA (?), OSTP</li>\n<li>Anywhere in Congress</li>\n<li>Skills from going into a firm</li>\n</ul>\n</li>\n<li>Law work for mitigating catastrophic risks is an extremely novel path. That said, there is a lot of opportunity to create your own path, and the <a href=\"https://www.legalpriorities.org/\">Legal Priorities Project</a> is working on some questions about how the law can help longtermist issues. The field of lawyers helping with longtermist issues is novel, and many in EA are realizing everything they want to do requires lawyers.</li>\n</ul>\n</li>\n</ul>\n<p>Some considerations relevant to personal fit:</p>\n<ul>\n<li>Like PhD programs, law school can be a bad choice for people who\u2019ve struggled with mental health.</li>\n<li>The first year of classes was less relevant to the interests of one student (who wanted to use law skills to inform legislation), because those classes were litigation-focused.</li>\n</ul>\n<p>Where to do law school:</p>\n<ul>\n<li>Some options recommended by a law student interested in catastrophic risks:\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Law_school_rankings_in_the_United_States#List_of_T14_law_schools\">\u201cTop 14\u201d (T14) law schools</a>\n<ul>\n<li>Should you do a T14 law degree if you\u2019re worried about debt? One law student interested in catastrophic risks advises: If you want to use a law degree to tackle existential risks, there\u2019s lots of resources to cover that, so probably take the debt and aim for the best you can find.\n<ul>\n<li>Apply for EA/OpenPhil funding sources (listed in the section \u201cFundings, jobs, and other resources\u201d below) if they\u2019re relevant for you.</li>\n<li>Also see resources on the bottom of <a href=\"https://80000hours.org/articles/us-ai-policy/#:~:text=At%2080%2C000%20Hours%2C%20we%20think,considers%20more%20advanced%20AI%20systems.\">80,000 Hours\u2019 AI policy page</a>.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>National security law schools:\n<ul>\n<li>In DC: Georgetown, George Washington, George Mason</li>\n<li>Others: University of Maryland, University of Virginia</li>\n<li>(Caveat: one commenter <a href=\"https://forum.effectivealtruism.org/posts/qPvZJkcqvGAeFt3o6/takeaways-on-us-policy-careers-part-2-career-advice?commentId=e37EYjrEGe2gkWpXu\">points out</a> that top national security jobs, when they go to lawyers, have in the last few decades usually gone from lawyers from a top few law schools.)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>What classes to take:</p>\n<ul>\n<li>One law student interested in catastrophic risks recommends courses in these topics: national security law, administrative law, procurement law, foreign relations law, intellectual property law, international trade law, nuclear security law, tort law, liability for risky research</li>\n<li>They also recommend doing paper courses in national security law or some administrative law</li>\n</ul>\n<h2>PhDs</h2>\n<p><em>Most of the advice here comes from one PhD who has the most experience with poli sci PhDs\u2014they recommend talking with someone in your field.</em></p>\n<ul>\n<li>Advantages of doing a PhD:\n<ul>\n<li>Credentialing:\n<ul>\n<li>It\u2019s necessary for being a tenured professor, e.g. for rotating between Georgetown University and government positions.</li>\n<li>It can let you go into government starting higher (although perhaps not higher than where you would be if you had spent your PhD years climbing ladders in government).\n<ul>\n<li>A PhD can be very helpful for becoming a program manager at IARPA and DARPA.</li>\n</ul>\n</li>\n<li>It makes it easier to become a go-to expert.</li>\n</ul>\n</li>\n<li>Learning:\n<ul>\n<li>It\u2019s great for learning about your subject and developing research skills, partly since it includes a guided multi-year expert survey of your field.</li>\n<li>It gives you time to develop your own views.</li>\n</ul>\n</li>\n<li>It helps you build an academic network.</li>\n</ul>\n</li>\n<li>Disadvantages of doing a PhD:\n<ul>\n<li>It takes <em>lots</em> of time, e.g. 5.5 years average in one student\u2019s program.</li>\n<li>Depression is very common\u2014in the ballpark of 40% [<a href=\"https://www.insidehighered.com/news/2015/04/22/berkeley-study-finds-high-levels-depression-among-graduate-students\">1</a>] [<a href=\"https://news.bloomberglaw.com/us-law-week/legal-education-needs-a-wellness-reckoning\">2</a>]\u2014among a wide range of graduate degrees. Rates of stress/anxiety also seem very high.\n<ul>\n<li>(From a small amount of searching, I didn\u2019t come across surveys suggesting that mental health is far worse for a given year in PhD programs than in other graduate degrees, but at minimum, they involve more years of often-poor mental health. On a more optimistic note, maybe the social support and career options/security that EA can offer help.)</li>\n<li>One piece of advice for mitigating this is keeping your friends from outside the PhD program.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Advice and considerations for learning more about your personal fit:\n<ul>\n<li>Try working as an RA for a professor,</li>\n<li>Read a bunch of articles in the top journal of your field of interest (e.g. International Security for security studies), see their opening remarks.\n<ul>\n<li>See if you are interested by them / could see yourself working on similar topics.</li>\n<li>See if methodology is cool, see if you can see yourself generating similar things?</li>\n<li>Good at producing original thought compared to networking/other useful skills</li>\n</ul>\n</li>\n<li>Good signs include:\n<ul>\n<li>If you have a high hedonic set point</li>\n<li>If you have always done fairly well academically</li>\n</ul>\n</li>\n<li>One concern that\u2019s (at least anecdotally) less well-founded is that PhD programs quickly pigeon-hole people into very narrow areas without leaving room for exploration. To the contrary, one PhD student hasn\u2019t felt that at all, and notes that most people will give you very general credit for knowing your field, regardless of your specific focus.</li>\n<li>Format varies between types of PhD programs, e.g. STEM PhDs are often centered on one big thesis project (for which there\u2019s sometimes little/rare feedback), while political science PhD requirements are more often fulfilled by doing multiple papers.</li>\n<li>A useful heuristic can be considering whether you have a comparative advantage in academics (relative to others working to address the problems you see as most pressing).</li>\n</ul>\n</li>\n<li>Where to do PhDs (in the context of political science PhDs):\n<ul>\n<li>Top programs for getting on tenure track in academic political science (professors are important):\n<ul>\n<li>Columbia, Stanford, Berkeley, UChicago, Yale</li>\n</ul>\n</li>\n<li>Top PhD programs for getting into policy:\n<ul>\n<li>Harvard Kennedy School, Princeton International Affairs, Security Studies at MIT</li>\n</ul>\n</li>\n<li>\u201cUK PhDs\u201d: DPhils (UK PhD equivalents) only take 3 years (!) because they don\u2019t involve an initial barrage of classes\n<ul>\n<li>Downsides include that it makes it harder to be a US professor, and you don\u2019t get a guided multi-year expert survey of the field.</li>\n<li>Oxford is a common choice, since FHI is there (as is GPI).</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Funding, jobs, and other resources</h2>\n<ul>\n<li>Additional grad school funding opportunities, for people focused on improving the long-term future / biosecurity / AI policy:\n<ul>\n<li><a href=\"https://funds.effectivealtruism.org/funds/far-future\">Long-Term Future Fund - EA Funds</a></li>\n<li><a href=\"https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future\">Open Philanthropy early-career funding for individuals interested in improving the long-term future</a></li>\n<li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity/open-philanthropy-project-early-career-funding-global-catastrophic-biological-risks\">Open Philanthropy Early-Career Funding for Global Catastrophic Biological Risks</a></li>\n</ul>\n</li>\n<li>General resources on grad programs:\n<ul>\n<li><a href=\"https://docs.google.com/document/d/1bbL3PknQsaUOK0Fxw9s70Fhzdl_agheIhmQlYFpgnos/edit#\">Science and technology policy opportunities</a> - grad school funding opportunities, fellowship and work opportunities, and more</li>\n<li>General job opportunities/resources for (recent) grad students:\n<ul>\n<li><a href=\"https://docs.google.com/document/d/1uLBi1gow9Y0MIt88lK55am3SU8CX2IQ7f2JAUqB4B3Y/edit\">Advice on preparing for Federal Jobs</a>\n<ul>\n<li>Includes advice on things to do well before you apply</li>\n</ul>\n</li>\n<li>See the fellowships section below</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Resources relevant to PhDs:\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/sYG75GFstqmiFWaBe/economics-phd-application-support-become-a-mentee\">Economics PhD application support - become a mentee!</a></li>\n</ul>\n</li>\n</ul>\n<h1>Fellowships - List and Comparison</h1>\n<p>For graduate students who will soon complete their degrees, people with advanced degrees, and in some cases people with only undergraduate degrees, full-time policy fellowships tend to be great opportunities to kickstart US policy careers, or to springboard to higher-leverage positions. (This is especially the case for people with advanced STEM degrees or professional experience with technology, partly because regular entry-level jobs in these areas are often incredibly competitive.)</p>\n<ul>\n<li>Opportunities:\n<ul>\n<li><a href=\"https://docs.google.com/document/d/1bbL3PknQsaUOK0Fxw9s70Fhzdl_agheIhmQlYFpgnos/edit\">Science and technology policy opportunities</a></li>\n<li><a href=\"https://docs.google.com/document/d/1-S407AQIu0cZI0HASAer1mBmZd57KXFMhKk79JnzBYk/edit#heading=h.muvcq3speh5h\">List of US Science and Technology Fellowships</a></li>\n<li>Your particular professional or demographic group may also have dedicated fellowship programs; do some searching to see if they exist. E.g., from <a href=\"https://crsreports.congress.gov/product/pdf/RL/98-654\">this list</a>:\n<ul>\n<li>Programs for demographic groups include:\n<ul>\n<li><a href=\"https://www.apaics.org/congressional-fellowship\">Asian Pacific American Institute for Congressional Studies Congressional Fellowship</a></li>\n<li><a href=\"https://www.cbcfinc.org/programs/fellowships/\">Congressional Black Caucus Foundation Fellowship</a></li>\n<li><a href=\"https://chci.org/programs/public-policy-fellowship-program/\">Congressional Hispanic Caucus Institute Public Policy Fellowship Program</a></li>\n<li><a href=\"https://www.wcpinst.org/our-work/congressional-fellows/\">Women\u2019s Congressional Policy Institute Congressional Fellowships</a></li>\n<li><a href=\"https://victoryinstitute.org/programs/victory-congressional-fellowship-2/\">Victory Congressional Fellowship</a>, for LGBTQ young professionals</li>\n</ul>\n</li>\n<li>Programs for professional groups include:\n<ul>\n<li><a href=\"https://connect.apsanet.org/centennialcenter/the-public-service-program/\">American Political Science Association (APSA) Public Service Program</a></li>\n<li><a href=\"https://gai.georgetown.edu/courses-programs/capitol-hill-fellowship/\">Georgetown Government Affairs Institute Capitol Hill Fellowship</a>, for executives and managers</li>\n<li><a href=\"https://www.brookings.edu/fellowships-programs/legis/\">Brookings LEGIS Congressional Fellowship</a>, for people with significant government/management experience</li>\n<li><a href=\"https://www.apa.org/about/awards/congress-fellow\">American Psychological Association Congressional Fellowship</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1>Advice on US policy job applications (including things to do well in advance)</h1>\n<h2>Context - This can be hard</h2>\n<p>Many entry-level roles in DC are very competitive. For example, it\u2019s common for entry-level think tank roles to get hundreds of applicants (including many more qualified candidates than they have capacity to admit). \\</p>\n<p>(If you go / went to a top university for undergrad, that might help, but employers won\u2019t be that impressed\u2014you won\u2019t be a shoo-in for a prominent DC think tank on the basis of where you did your undergrad.)</p>\n<p>The competitiveness of these roles implies that being strategic in your approach, and persevering when necessary, are very useful.</p>\n<h2>Preparing for applications</h2>\n<h3>Tips for transitioning into policy from another field (and more generally)</h3>\n<ul>\n<li>See this doc: <a href=\"https://docs.google.com/document/d/1uLBi1gow9Y0MIt88lK55am3SU8CX2IQ7f2JAUqB4B3Y/edit\">Advice on preparing for Federal Jobs</a></li>\n<li>Build relationships with professors, friends, colleagues\u2014people will talk to you if you ask.</li>\n<li>Draw on others for help\u2014that\u2019s very useful.</li>\n<li>Apply to things as they come up if they seem relevant, because they\u2019re competitive.</li>\n<li>Advice for transitioning from another field into think tanks:\n<ul>\n<li>If you have a technical background, getting some piece published (in a college newspaper, some other place, or even a blog) is good for signaling your interest in policy and technical skills.</li>\n<li>Often, there\u2019s interest in people who aren\u2019t political science majors from well-known colleges\u2014having that typical background isn\u2019t particularly helpful for many opportunities.</li>\n<li>Have writing samples ready in advance.\n<ul>\n<li>Writing these can also be useful for assessing fit.</li>\n</ul>\n</li>\n<li>If you\u2019re looking to transition into a think tank role, RA roles can be useful, since they\u2019re very legibly adjacent.\n<ul>\n<li>There may be more RA opportunities now since many things are remote.</li>\n<li>(As discussed above, DC-based, policy-focused master\u2019s degrees might be even better for transitioning.)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Tips on cultural acclimation</h3>\n<p>Like many other groups of people, DC policy professionals tend to have particular styles of communicating (specific terms, metaphors, norms, shared context, etc). Communicating similarly is useful, both for getting jobs and for getting listened to. (This advice was given in the context of applying to think tank jobs, but I\u2019m guessing that it generalizes.)</p>\n<p>As with other languages, you learn to speak the \u201cpolicy language\u201d by hearing it a lot, and with time. For hearing it a lot, see the reading &amp; podcast recommendations below.</p>\n<p>(See some additional discussion of this in the section \u201cHow to apply.\u201d)</p>\n<p>Also, DC is relatively hierarchical in terms of age and job responsibilities. Being seen as someone who\u2019s respectful and trying to learn will come across as way better than trying to be the Silicon Valley-style kid genius who doesn\u2019t need to listen and has figured it out.</p>\n<h3>Tips for getting a security clearance</h3>\n<p>Getting a security clearance means getting permission to access certain classified information. While plenty of high-impact policy jobs <em>don\u2019t</em> require a security clearance, some do, especially in the national security space.</p>\n<p>Here are one professional\u2019s tips for getting a security clearance (non-comprehensive):</p>\n<ul>\n<li>Don\u2019t be a spy</li>\n<li>Don\u2019t be blackmailable</li>\n<li>Don\u2019t have committed recent felonies</li>\n<li>Don\u2019t have smoked marijuana in the last year. Don't have done harder drugs in at least the last 5 years, maybe longer. In general, don't do drugs.</li>\n<li>Be careful around foreign contacts</li>\n<li>(See the section on reputation risks for more relevant advice.)</li>\n<li>Check out SF-86 form for security clearances in advance of doing stuff\u2014it\u2019s a common entry requirement.\n<ul>\n<li>See SF-86 <a href=\"https://www.opm.gov/forms/pdf_fill/sf86.pdf\">full form</a> and <a href=\"https://nationalsecurity.gmu.edu/wp-content/uploads/2018/02/A-Short-Primer-on-Security-Clearances-15.0-2.23.183.pdf\">primer</a>.</li>\n</ul>\n</li>\n<li>[Added] Don't lie in the process of applying for a security clearance!</li>\n</ul>\n<h3>Tips on reputation risks</h3>\n<p>Advice from an experienced policy professional:</p>\n<p>Think a lot about what might cause irreversible reputational harms or slow down a security clearance. After all, those are really useful.</p>\n<p>Risky activities include drugs and saying regretful stuff online. For people considering US policy careers, keeping a low profile online seems usually worthwhile for keeping options open (there\u2019s always the possibility of anonymously / pseudonymously publishing your blog posts). Even if some particular content or affiliation doesn\u2019t pose serious reputation risks now, that could change in the future.</p>\n<p>Generally, be really cautious. Policy professionals are generally really cautious around their reputations, e.g. whom they\u2019re seen in rooms with.</p>\n<h2>Where to apply</h2>\n<p><em>(This section is especially far from being comprehensive.)</em></p>\n<ul>\n<li>You can work in and have lots of positive impact in lots of places.\n<ul>\n<li>The <a href=\"https://www.legalpriorities.org/\">Legal Priorities Project</a> is a global research project founded by researchers from Harvard. They conduct legal research that tackles highly pressing problems, with a long-term lens.</li>\n<li>Lawyers also tend to do relatively well in running for office.</li>\n</ul>\n</li>\n<li>For political data analytics, one relevant professional considers these organizations especially impactful (there\u2019s few of them):\n<ul>\n<li><a href=\"https://boards.greenhouse.io/openlabs\">OpenLabs</a> for Democratic data analytics\n<ul>\n<li>There\u2019s a number of other impact-focused people who joined the team to have impact through it.</li>\n<li>Employees need really advanced statistical skills.</li>\n</ul>\n</li>\n<li><a href=\"https://www.dataforprogress.org/\">Data for Progress</a> (mix of polling and advocacy)</li>\n</ul>\n</li>\n<li>A few places to find job postings:\n<ul>\n<li>Blindly applying to things on USAjobs won\u2019t get you far, some professionals suggest.</li>\n<li>Congress - there\u2019s secret job postings you need to know about:\n<ul>\n<li><a href=\"http://Bradtravers.com\">Bradtravers.com</a></li>\n<li>There\u2019s code words for jobs that are posted but are basically already settled</li>\n</ul>\n</li>\n<li>Ask relevant contacts.</li>\n</ul>\n</li>\n<li>(As mentioned earlier, this section is especially far from being comprehensive\u2014it doesn\u2019t even cover e.g. journalism or the executive branch, not to mention policy jobs that don\u2019t involve applications.)</li>\n</ul>\n<h2>How to apply</h2>\n<ul>\n<li>Get more help than you probably think is necessary in your application materials.\n<ul>\n<li>Expect to get help with this! Get your application materials thoroughly reviewed by people who know what these people are looking for.\n<ul>\n<li>Even as a smart and well-qualified person, you likely won\u2019t be fluent in the application language.</li>\n<li>It\u2019s like your first touch, and you\u2019re trying to hang out to a very quickly-moving thing\u2014getting that initial contact right is very difficult and very useful.</li>\n</ul>\n</li>\n<li>Call in favors\u2014draw on connections to get this very useful help.</li>\n<li>If you\u2019re asked to talk about issues in your cover letter / interview, you can do some issue-specific cramming on how policy people think and talk about certain issues. Read mainstream publications / listen to mainstream podcasts relevant to policy, then mirror the language used in mainstream discourse about the topics you\u2019re most interested in.\n<ul>\n<li>This is useful because people use imperfect heuristics to hire, including \u201care you using the right language?\u201d as a proxy for implicit knowledge.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>In one professional\u2019s experience (as an applicant and later as an applicant reviewer), think tanks aren\u2019t looking for extensive background when they hire for junior research roles; they\u2019re looking for someone who\u2019s smart and has demonstrated curiosity.\n<ul>\n<li>So showing you are curious and can learn fast / independently is a huge bonus.\n<ul>\n<li>Demonstrate (intellectual) passions in your job application/interview if the vibe seems right.</li>\n</ul>\n</li>\n<li>Still, be able to tell good story about fit/experiences\n<ul>\n<li>Help from others is very useful for this.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>If you tend to do things last-minute, keep in mind that DC application deadlines are in Eastern Time by default.\n<ul>\n<li>(So if you submit an application shortly before midnight in Pacific Time, that\u2019s too late. Not that I would know.)</li>\n</ul>\n</li>\n</ul>\n<h1>Additional advice</h1>\n<h2>Connect with the professional EA DC community</h2>\n<p>(The following are broader impressions from 15 or so hours of informal interactions with people in the professional DC EA community. Many of these hours were not through the speaker series\u2014speakers had varied interests, and none of them are necessarily involved with or sympathetic to this community. Another caveat: I\u2019ve mostly interacted with people who focus on emerging technology policy, so I\u2019m not sure how well this generalizes to other parts of the DC EA community.)</p>\n<p>Before meeting policy folks, part of me wondered if they\u2019d live up to stereotypes of ruthless, sleazy professionals. Fortunately, I\u2019ve only seen the opposite of that; the policy professionals I\u2019ve met so far all seem friendly (in a way that feels, not snaky, but genuine / warm / pleasant to interact with). It\u2019s like some other EA hubs, but with stronger social skills.</p>\n<p>(I hear, from a relevant professional, that this is a broader trend\u2014sleaziness is generally rare in the federal US policy space; people who are unpleasant to work with tend to get less far. This matches my anecdotal experience: I\u2019ve met a few dozen policy folks so far, and my most common experience has been \u201cthis person is so friendly!\u201d)</p>\n<p>These professionals also seem to be very busy, very cautious about reputation risks, and\u2014I hear\u2014quite emotionally and professionally supportive of one another.</p>\n<p>On a slightly more personal note, before interacting with this community, I had some hesitations about moving away from my existing connections to go work in DC. I feel much more excited about that possibility now, knowing there\u2019s a very friendly, supportive, inspiring community there.</p>\n<p>(I also already knew I tend to enjoy hanging out with impact-driven folks, and I was lucky to have mutual friends with some of these people, so it might have been easier for me to have this experience than it would be for some people in different situations.)</p>\n<p>Long story short: definitely <a href=\"https://www.effectivealtruismdc.org/connect\">connect</a> <a href=\"https://www.facebook.com/groups/effective.altruism.dc/?ref=pages_profile_groups_tab&amp;source_id=1524355844510913\">with</a> the EA DC community if you\u2019re working in DC and very EA-sympathetic. (Not doing so would probably be a big mistake.)</p>\n<h2>Advice for undergraduates</h2>\n<p>A few policy professionals advise:</p>\n<ul>\n<li>Semester in DC programs / internships are great (especially for getting mentors/sponsors).\n<ul>\n<li>Or volunteer for a campaign if you\u2019re interested in learning more about what it\u2019s like to run for office.</li>\n</ul>\n</li>\n<li>For running for office, your degree (at least your undergraduate degree) roughly doesn\u2019t matter at all.</li>\n<li>For other policy jobs, your (undergraduate?) degree also matters little; STEM degrees are nice boosts to your credibility on STEM topics because they\u2019re rare in the policy world. A policy (masters?) degree is more helpful for networking and the writing sample than for the resume line.\n<ul>\n<li>So take the time you would have spent stressing over a packed courseload in undergrad and spend it doing <a href=\"https://forum.effectivealtruism.org/posts/F9E8MwsPLiqSYhXJm/lessons-from-running-stanford-ea-and-seri\">other useful things</a> (which very much can and I\u2019d guess should also include learning)!\n<ul>\n<li>(I started writing more about this here, then I realized there was more I wanted to say that would fit nicely in a single bullet point, so now this is its own post\u2014see <a href=\"https://forum.effectivealtruism.org/posts/hgiLaE3eL76ovcfdH/many-undergrads-should-take-light-courseloads\">\u201cMany Undergrads Should Take Light Courseloads.\u201d</a>)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>One policy professional took all CS and math classes and realized they could have lived in a different city and worked full-time and would have just had 4 years to do valuable things like learning what kinds of things they would be good at.\n<ul>\n<li>Not everyone should do that, they suggest, but also not everyone should spend 4 years focused on getting good grades.</li>\n</ul>\n</li>\n<li>(My personal takeaway from this is that people interested in US policy careers should typically choose their undergraduate degrees on the basis of considerations other than what degree seems most useful for policy job applications. E.g. What degree will let you spend more time on useful things other than classes, such as <a href=\"https://forum.effectivealtruism.org/posts/F9E8MwsPLiqSYhXJm/lessons-from-running-stanford-ea-and-seri\">getting more people to go into high-impact careers</a>? Or what degree will best give you skills/credentials that prepare you well for career options you\u2019re considering outside of public policy? Or what degree will best help you learn useful things? And, as suggested before, pay more attention to choices other than your degree, e.g. how you spend your time beyond the minimum time needed to get your degree.)</li>\n</ul>\n<h2>Advice for program/event organizers</h2>\n<p>Takeaways from my own experience helping organize the speaker series:</p>\n<ul>\n<li>There is much interest from this community in high-impact US policy careers; the speaker series got over 500 signups.</li>\n<li>If you\u2019re looking for a speaker who can speak about some policy area, lean toward reaching out to people at government-adjacent organizations such as think tanks or academia, rather than people in government.\n<ul>\n<li>Getting speakers who work in government is pretty hard; people in government tend to be super busy, and people at some government organizations need to go through a bunch of approval processes to speak at events. On top of that, some government employees often have to keep interesting things confidential.</li>\n<li>This consideration may be outweighed by e.g. an event being sufficiently large.</li>\n<li>Relatedly, policy professionals seem to like short emails.</li>\n</ul>\n</li>\n<li>If you\u2019re running a Zoom event, ask speakers to show up 5-10 minutes in advance (or more if there\u2019s more things to go over / higher stakes). (A longer buffer might be useful for in-person events, to be safe even if e.g. there\u2019s traffic.)\n<ul>\n<li>That extra time can be great for rehashing an event plan (which you hopefully already discussed with them earlier), connecting with them, and working out any technical issues in advance of your event.</li>\n</ul>\n</li>\n<li>Follow-up emails containing resources for event attendees can be useful, both for the resources\u2019 direct usefulness and for encouraging people to act on suggestions made in the event.</li>\n<li>Logistically, we used a Zoom webinar with the following settings, among others: there was a Q&amp;A (with attendees typing and upvoting questions), only a few panelists with cameras turned on were visible to attendees, and attendees could join anonymously but couldn\u2019t use the chat. These settings aren\u2019t best for every event, but if they make sense for your event, <a href=\"https://docs.google.com/document/d/1VqvAonp2D5LtcH1GX4P0DdRPkvIihwm2Qw7eEySvogE/edit#heading=h.uwher8sjt22g\">this doc</a> details the exact settings we used.</li>\n</ul>\n<h2>Various additional career tips</h2>\n<ul>\n<li>\n<p>Don\u2019t be shy about making an open-ended request for help if you have a connection and don\u2019t know quite what you want.</p>\n<ul>\n<li>If you\u2019re working with deadlines, ask well in advance.</li>\n</ul>\n</li>\n<li>\n<p>Consider crowd-sourcing your career decisions.</p>\n<ul>\n<li>Poll people you trust/admire, ask for estimates of value, crucial considerations, and advice\u2014that advice may be better than your own intuitions.</li>\n<li>One policy professional has done this for most of their last several jobs, and it seems to have gone well for them.</li>\n</ul>\n</li>\n<li>\n<p>To become better positioned to be a federal political candidate, in addition to <a href=\"https://forum.effectivealtruism.org/posts/z9hzAB9mfgcpXmcze/takeaways-on-us-policy-careers-part-1-paths-to-impact-and#Personal_fit_for_running_for_office_\">reaching out</a>, get involved with local (e.g. state) politics.</p>\n</li>\n<li>\n<p>In Congress, some things you didn't think would work out end up informing legislation later (remembering this may help keep failures from being too discouraging).</p>\n</li>\n<li>\n<p>Which semester to start graduate school in? One graduate student thinks it probably doesn\u2019t make much difference.</p>\n</li>\n<li>\n<p>When do dual degrees (e.g. masters + law) make sense? It could help with getting subject-matter expertise + law expertise.</p>\n</li>\n<li>\n<p>Read biographies of people\u2019s careers at places where you\u2019ll be working, to learn more about the institutions and advancing within it.</p>\n<ul>\n<li>E.g. reading Bill Burns\u2019 autobiography on his time in the State Department can be helpful.</li>\n</ul>\n</li>\n<li>\n<p>Advice relevant to journalism:</p>\n<ul>\n<li>What makes great unsolicited articles for news outlets? See footnote.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-19\" id=\"fnref-H9rB9jFyxokwFBvnj-19\">[19]</a></sup></li>\n<li>How can academics get their reports seen by influential people?\n<ul>\n<li>Getting covered by a news outlet is very helpful for both reaching and persuading people.</li>\n<li>To get covered often, you\u2019re going to have to turn everything into very concise/shareable/soundbite/single-line version.\n<ul>\n<li>I.e. you have to learn how to turn everything into a high level summary, something quotable, something understandable to people without any background knowledge.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>How can academics get attention from journalists? See footnote.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-20\" id=\"fnref-H9rB9jFyxokwFBvnj-20\">[20]</a></sup></li>\n<li>How much should journalists focus on making video vs text content? See footnote.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-21\" id=\"fnref-H9rB9jFyxokwFBvnj-21\">[21]</a></sup></li>\n</ul>\n</li>\n<li>\n<p>If someone is interested in pursuing a graduate program in China, how do the benefits trade off against the potential increased difficulty of getting a security clearance? See footnote.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-22\" id=\"fnref-H9rB9jFyxokwFBvnj-22\">[22]</a></sup></p>\n</li>\n<li>\n<p>What level of Chinese fluency is needed for \"sufficient\" understanding of AI work in China for different types of work? See footnote.<sup class=\"footnote-ref\"><a href=\"#fn-H9rB9jFyxokwFBvnj-23\" id=\"fnref-H9rB9jFyxokwFBvnj-23\">[23]</a></sup></p>\n</li>\n<li>\n<p>Read! See the next section for specific reading/podcast recommendations and advice on ways to approach learning.</p>\n</li>\n</ul>\n<h1>Reading &amp; podcast recommendations</h1>\n<ul>\n<li><a href=\"https://80000hours.org/\">80,000 Hours</a> is great\n<ul>\n<li>80,00 Hours\u2019 reviews of careers you\u2019re interested in are worth reading closely and attentively; their writers (apparently) try to write articles that are both very informative and not super long, so important points often get just one sentence.</li>\n<li>See resources below</li>\n<li>Multiple policy professionals recommended 80,000 Hours for the general accuracy of its information on US policy careers.</li>\n</ul>\n</li>\n<li>Learning more about executive branch work:\n<ul>\n<li>80,000 Hours podcast interviews with former government officials: <a href=\"https://80000hours.org/podcast/episodes/beth-cameron-pandemic-preparedness/\">Beth Cameron</a>,<a href=\"https://80000hours.org/podcast/episodes/tom-kalil-government-careers/\"> Tom Kalil</a>,<a href=\"https://80000hours.org/podcast/episodes/tom-inglesby-health-security/\"> Tom Inglesby</a>,<a href=\"https://80000hours.org/podcast/episodes/ambassador-bonnie-jenkins-peace-arms-control/\"> Bonnie Jenkins</a>, <a href=\"https://80000hours.org/podcast/episodes/samantha-pk-nuclear-security/\">Samantha Pitts-Kiefer</a>, and <a href=\"https://80000hours.org/podcast/episodes/andy-weber-rendering-bioweapons-obsolete/\">Andy Weber</a>\n<ul>\n<li>The interview with Kalil is especially focused on what working in government is like (rather than object-level discussion of issues).</li>\n</ul>\n</li>\n<li>Book with great examples of individual impact in government work (an engineer counterfactually bringing about safer decisions): <a href=\"https://www.amazon.com/Command-Control-Damascus-Accident-Illusion/dp/0143125788\">Command and Control</a> by Eric Schlosser</li>\n<li>Books with more background on the executive branch:\n<ul>\n<li><em><a href=\"https://www.amazon.com/National-Security-Enterprise-Navigating-Labyrinth/dp/1626164401/ref=asc_df_1626164401/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312643571004&amp;hvpos=&amp;hvnetw=g&amp;hvrand=12168734761593113638&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9061285&amp;hvtargid=pla-579173914047&amp;psc=1\">The National Security Enterprise: Navigating the Labyrinth - George and Rishikof</a></em></li>\n<li><em><a href=\"https://www.amazon.com/Federal-Budget-Politics-Policy-Process/dp/0815777256\">The Federal Budget: Politics, Policy, Process - Schick</a></em></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Learning more about Congressional staffing:\n<ul>\n<li>80,000 Hours\u2019 career profile on <a href=\"https://80000hours.org/career-reviews/congressional-staffer/\">Congressional staffing</a></li>\n<li>Don\u2019t forget about the Congressional support agencies: the <a href=\"https://crsreports.congress.gov/\">Congressional Research Service</a>, <a href=\"https://www.gao.gov/\">Government Accountability Office</a>, and <a href=\"https://www.cbo.gov/\">Congressional Budget Office</a></li>\n<li>Additional materials:\n<ul>\n<li>Bazelon and Yglesias on the \u201c<a href=\"https://www.slowboring.com/p/the-rise-and-importance-of-secret\">Secret Congress</a>\u201d</li>\n<li>_<a href=\"https://www.amazon.com/Act-Congress-Americas-Essential-Institution/dp/030770016X/ref=sr_1_1?crid=1XYACYH9FFKDY&amp;dchild=1&amp;keywords=act+of+congress+by+robert+kaiser&amp;qid=1626227652&amp;sprefix=act+of+congress%2Caps%2C137&amp;sr=8-1\">Act of Congress: How America\u2019s Essential Institution Works, and How It Doesn\u2019t</a> _by Robert Kaiser (on Congress as a whole)</li>\n<li><em><a href=\"https://www.amazon.com/Surviving-Inside-Congress-Mark-Strand/dp/099665240X\">Surviving Inside Congress</a></em> by Mark Strand et al. (on life as a Congressional staffer)</li>\n<li>Podcasts: <a href=\"https://www.aei.org/tag/understanding-congress-podcast/\">Understanding Congress</a> and <a href=\"https://kilmer.house.gov/news/podcasts\">Quick Questions about Congress with Kilmer</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Learning more about think tank research:\n<ul>\n<li>80,000 Hours\u2019 career profile on <a href=\"https://80000hours.org/career-reviews/think-tank-research/\">think tank research</a></li>\n<li>Additional readings:\n<ul>\n<li>James McGann (2016), <em>The Fifth Estate: Think Tanks, Public Policy, and Governance</em> (Brookings Institution Press), also <a href=\"https://www.jstor.org/stable/10.7864/j.ctt1gpccjc\">available online on JSTOR</a></li>\n<li>Tevi Troy (2012), \u201c<a href=\"https://www.nationalaffairs.com/publications/detail/devaluing-the-think-tank\">Devaluing the Think Tank</a>,\u201d <em>National Affairs</em></li>\n<li>Peter W. Singer (2010), \u201c<a href=\"https://www.brookings.edu/articles/washingtons-think-tanks-factories-to-call-our-own/\">Washington\u2019s Think Tanks: Factories to Call Our Own</a>,\u201d <em>Brookings Institution</em></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Learning more about policy journalism:\n<ul>\n<li><a href=\"https://80000hours.org/podcast/episodes/kelsey-piper-important-advocacy-in-journalism/\">Kelsey Piper interview - 80,000 Hours Podcast</a></li>\n<li>See <a href=\"https://link.vox.com/view/608adc4191954c3cef037d04eqpve.3gm/3f24c49f\">this Vox article by Dylan Matthews</a> for an overview of times journalism had major impacts on policy. The article references these works, among others:\n<ul>\n<li><a href=\"https://www.newyorker.com/magazine/1963/01/19/our-invisible-poor\">Dwight MacDonald article on poverty in 1963</a>\n<ul>\n<li><a href=\"https://www.smithsonianmag.com/history/how-a-new-yorker-article-launched-the-first-shot-in-the-war-against-poverty-17469990/\">Jill Lepore on its influence on the White House</a></li>\n<li>More detail in <a href=\"https://www.jstor.org/stable/446920\">this piece</a> on MacDonald and Michael Harrington's influence</li>\n</ul>\n</li>\n<li><a href=\"https://www.newyorker.com/magazine/2009/06/01/the-cost-conundrum\">Atul Gawande piece on health costs</a>\n<ul>\n<li>On its influence - <a href=\"https://www.nytimes.com/2009/06/09/us/politics/09health.html?ref=todayspaper\">New York Times piece</a></li>\n<li>Gawande did <a href=\"https://www.newyorker.com/magazine/2015/05/11/overkill-atul-gawande\">a retrospective finding</a> that McAllen's costs were sharply reduced as a result of ACA reforms.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Future Perfect: <a href=\"https://www.vox.com/future-perfect\">website</a> and <a href=\"https://www.vox.com/future-perfect\">newsletter</a>\n<ul>\n<li><a href=\"https://www.vox.com/future-perfect/2018/10/15/17948062/pandemic-flu-ebola-h1n1-outbreak-infectious-disease\">\u201cA pandemic killing tens of millions of people is a real possibility \u2014 and we are not prepared for it\u201d</a> - Ron Klain (now Biden\u2019s Chief of Staff), 2018, on Vox\u2019s Future Perfect</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Grad programs:\n<ul>\n<li>General job opportunities/resources for (recent) grad students:\n<ul>\n<li><a href=\"https://docs.google.com/document/d/1uLBi1gow9Y0MIt88lK55am3SU8CX2IQ7f2JAUqB4B3Y/edit\">Advice on preparing for Federal Jobs</a>\n<ul>\n<li>Includes advice on things to do well before you appl</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>PhD programs:\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/posts/B7AQF7HNiLRbKMKJt/how-to-phd\">How to PhD</a></li>\n</ul>\n</li>\n<li>Additional readings:\n<ul>\n<li><a href=\"https://mindingourway.com/half-assing-it-with-everything-youve-got/\">\u201cHalf-assing it with Everything You've Got\u201d - Minding Our Way</a></li>\n<li><a href=\"https://80000hours.org/career-decision/article/\">How to make tough career decisions - 80,000 Hours</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>General recommended readings:\n<ul>\n<li><a href=\"https://homepages.inf.ed.ac.uk/wadler/papers/firbush/hamming.pdf\">You and Your Research</a> by Richard Hamming on looking for especially important problems</li>\n<li>On getting things done behind the scenes: <a href=\"https://www.slowboring.com/p/the-rise-and-importance-of-secret\">Secret Congress</a> by Bazelon and Yglesias</li>\n<li>Relevant to a panelist\u2019s point about not spending unnecessary effort on school and being more creative: <a href=\"https://mindingourway.com/half-assing-it-with-everything-youve-got/\">\u201cHalf-assing it with Everything You've Got\u201d - Minding Our Way</a></li>\n<li>On imposter syndrome: <a href=\"https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/\">Having a Successful Career with Depression, Anxiety and Imposter Syndrome</a> - 80,000 Hours podcast episode</li>\n<li>On government/policy and catastrophic risks:\n<ul>\n<li>Additional <a href=\"https://80000hours.org/\">80,000 Hours</a> resources:\n<ul>\n<li>Profile on <a href=\"https://80000hours.org/articles/us-ai-policy/#further-reading\">AI policy</a> (includes lists of fellowship opportunities at the end of the profile)</li>\n<li>Profile on <a href=\"https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/\">global catastrophic biological risks</a></li>\n</ul>\n</li>\n<li><em><a href=\"https://www.amazon.com/Gambling-Armageddon-Roulette-Hiroshima-1945-1962/dp/0307266885/ref=sr_1_1?crid=1UMCCBFKAESAA&amp;dchild=1&amp;keywords=gambling+with+armageddon&amp;qid=1626119394&amp;sprefix=gambling+with+armagedd%2Caps%2C160&amp;sr=8-1\">Gambling with Armageddon</a></em> by Martin Sherwin and <em><a href=\"https://www.amazon.com/Bomb-Presidents-Generals-History-Nuclear/dp/1982107308/ref=sr_1_1?dchild=1&amp;keywords=the+bomb+book+fred+kaplan&amp;qid=1626119432&amp;sr=8-1\">The Bomb</a></em> by Fred Kaplan (see also some books listed in footnote 1 <a href=\"https://80000hours.org/2019/04/career-advice-i-wish-id-been-given-when-i-was-young/\">here</a>)\n<ul>\n<li>If you like these books and are interested in learning about biosecurity, you might also enjoy <em><a href=\"https://www.amazon.com/Dead-Hand-Untold-Dangerous-Legacy/dp/0307387844/ref=sr_1_1?crid=32RA5417QQINV&amp;dchild=1&amp;keywords=dead+hand+book&amp;qid=1626119312&amp;sprefix=dead+hand%2Caps%2C239&amp;sr=8-1\">Dead Hand</a></em> by David Hoffman.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Additional books to read and podcasts to listen to (for learning how many in the policy community talk/write about certain topics\u2014which can be helpful for getting hired\u2014as well as for learning about the topics):\n<ul>\n<li>Bill Burns (2019), <em><a href=\"https://www.amazon.com/Back-Channel-American-Diplomacy-Renewal/dp/0525508864\">The Back Channel: A Memoir of American Diplomacy and the Case for Its Renewal</a></em></li>\n<li>Richard Neustadt and Ernest May (1988), <em><a href=\"https://www.amazon.com/Thinking-Time-Uses-History-Decision-Makers/dp/0029227917\">Thinking in Time: The Uses of History for Decision-Makers</a></em> \u2014 \u201ca classic look at some policymaking dynamics\u201d</li>\n<li><a href=\"https://www.nytimes.com/column/ezra-klein-podcast\">Ezra Klein's podcast</a> \u2014 \u201cany episodes on a policy issue will give you a sense of how the issue is framed in the mainstream, and also usually some thoughtful critique of that framing.\u201d</li>\n<li><a href=\"https://slate.com/podcasts/political-gabfest\">Slate Political Gabfest</a> \u2014 \u201cfor a flavor of what (lefty) DC is talking about that week. (Could be good ahead of an interview or while adjusting to a new job from a different space.)\u201d</li>\n</ul>\n</li>\n<li>A few policy professionals\u2019 advice on reading and on learning more generally:\n<ul>\n<li>Read.</li>\n<li>Read lots.\n<ul>\n<li>Make protected, dedicated reading time\n<ul>\n<li>You can make reading time both for discovering new readings and for catching up on your reading list.</li>\n<li>You can use apps like <a href=\"https://en.wikipedia.org/wiki/Pocket_(service)\">Pocket</a> for collecting articles.</li>\n<li>This is especially useful if you want to pursue somewhat independent/self-directed priorities/learning.</li>\n<li>It doesn\u2019t have to have an agenda.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Read history.\n<ul>\n<li>One technology policy professional sees themselves as getting more value from reading histories of disasters/near-disasters (i.e. how we got to those thickets, and how we navigated out of them, including with an institutional focus) than from reading more on technology.</li>\n<li>Find the biographies of people whose lives/professional paths you found especially valuable, and try to reverse-engineer them.\n<ul>\n<li>Others\u2019 paths aren\u2019t always reproducible, or they shouldn\u2019t always be imitated, due to randomness. But there\u2019s enough signal to find e.g. which degrees and offices are useful.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Read things that the people around you are not reading.\n<ul>\n<li>This gets you useful info that you likely wouldn\u2019t get otherwise, and it can be professionally valuable in other ways.</li>\n<li>Use <a href=\"https://www.google.co.uk/alerts\">Google Alerts</a> to get news on important yet obscure topics.</li>\n<li>Try to avoid reading stuff that\u2019s urgent but not long-term important.</li>\n</ul>\n</li>\n<li>Really get to know the issues that you care about and the issues that they touch\u2014get to know their context.\n<ul>\n<li>Especially if your issue of interest is new/doesn\u2019t have an established constituency (presumably since in that case, finding allies from intersecting issues is especially useful).</li>\n<li>That helps people take you seriously, and it helps you understand your issue.</li>\n<li>Every issue you might be interested in is really complicated\u2014connects with many other issues and has complex issues.\n<ul>\n<li>E.g. US-China competition on AI is influenced by many other US-China issues and the broader history of US-China technology relations (trade war under Trump, bans on Tiktok, history of relationship/history of technology, controversy over satellite sales, China joining world trade, etc).</li>\n<li>Understanding these things is really important.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1>Addendum to Part 1</h1>\n<p>In writing this post, I realized some content I had aimed to include here would fit better in the previous post. It\u2019s there now, in <a href=\"https://forum.effectivealtruism.org/posts/z9hzAB9mfgcpXmcze/takeaways-on-us-policy-careers-part-1-paths-to-impact-and#Addendum\">a new section</a> (so you can find the new content without needing to search through the old content).</p>\n<h1>What\u2019s Next?</h1>\n<p>That\u2019s all for now! (If you have additional perspectives/advice/corrections you\u2019d like to share in the comments, please do so!)</p>\n<p>To recap some key points: there\u2019s tons of room for people who are strong fits to have impact through US policy work, there are many different jobs in the US policy space, and (depending on your interests and career stage) there are many things you might find useful to do next\u2014for example, consider whether it makes sense for you to:</p>\n<ul>\n<li>Get in touch with the EA DC community</li>\n<li>Network with a bunch of other DC folks</li>\n<li>Prepare/apply for a policy masters (perhaps a DC-based one) or law degree</li>\n<li>Apply for US policy internships (perhaps through a \u201cSemester-in-DC\u201d program), US policy fellowships, and/or US policy jobs</li>\n<li>Dive into the further reading/podcast recommendations</li>\n<li>Have lots of impact through a current US policy job</li>\n</ul>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-H9rB9jFyxokwFBvnj-1\" class=\"footnote-item\"><p>I was initially confused about how Johns Hopkins could offer the benefits of a DC location if Google says they\u2019re in Baltimore, Maryland. Turns out their School of Advanced International Studies is in DC. <a href=\"#fnref-H9rB9jFyxokwFBvnj-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-2\" class=\"footnote-item\"><p>Of the first handful of people I thought of who seem to be doing especially impactful work in, say, US AI policy, over half went to relatively accessible grad programs, and likewise for undergrad. <a href=\"#fnref-H9rB9jFyxokwFBvnj-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-3\" class=\"footnote-item\"><p>Of the several community members I\u2019ve heard of who are pursuing PhDs for US AI policy work, at least one of these exceptions apply to all of them. <a href=\"#fnref-H9rB9jFyxokwFBvnj-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-4\" class=\"footnote-item\"><p>E.g. a speaker thinks they would have benefited from trusting their gut about their interest in / the importance of politics, and their fit for it; it took them too long to figure that out. <a href=\"#fnref-H9rB9jFyxokwFBvnj-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-5\" class=\"footnote-item\"><p>One journalist was told that they were turned down from a Congressional internship because they thought this journalist would leak to another journalist. The person who shared this anecdote thinks this transition is harder to pull off in the US than in some other countries. <a href=\"#fnref-H9rB9jFyxokwFBvnj-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-6\" class=\"footnote-item\"><p>Reasons for working on neglected problems:</p>\n<ul>\n<li>The popularity of a problem is evidence that it\u2019s hard to solve.</li>\n<li>And it\u2019s harder to make further contributions to popular problems\u2014more people will have taken low-hanging fruit.</li>\n<li>And there tends to be more competition to break into roles that focus on popular problems.</li>\n</ul>\n <a href=\"#fnref-H9rB9jFyxokwFBvnj-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-7\" class=\"footnote-item\"><p>One professional advises: Develop a toolkit for thinking about technology risk and policy (in the abstract). That\u2019s just as\u2014or more\u2014marketable in places like the White House or Congressional offices, where people typically work on a broad variety of issues. (I suspect this piece of advice in particular is controversial, and plenty of thoughtful people will be more excited about specialists, especially given the current portfolio in this community. Maybe the best of both worlds is being a specialist with regard to your skills and a generalist with regard to the problems you know about?) <a href=\"#fnref-H9rB9jFyxokwFBvnj-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-8\" class=\"footnote-item\"><p>US House members <a href=\"https://history.house.gov/Institution/Origins-Development/Constitutional-Qualifications/\">must</a> have been US citizens for 7 years, US senators <a href=\"https://www.senate.gov/senators/qualifications_termsofservice.htm#:~:text=The%20Constitution%20sets%20three%20qualifications,represents%20at%20time%20of%20election.\">must</a> have been US citizens for 9 years, and the US President <a href=\"https://www.usa.gov/presidents#:~:text=Requirements%20to%20Hold%20Office,United%20States%20for%2014%20years.\">must</a> be a natural-born US citizen and have been a US resident for 14 years. <a href=\"#fnref-H9rB9jFyxokwFBvnj-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-9\" class=\"footnote-item\"><p>Reasons why technical backgrounds are not that important for working in technology policy: Knowing who knows the right thing, and drawing from them, is good enough (and it allows you to access much more information than what any individual could know). Relatedly, unlike in academia, it\u2019s encouraged to \u201clift from other people\u2019s work\u201d; some academic skills are not very useful for excelling in policy. Government does not care at all about whether you\u2019ve made original intellectual contributions\u2014\u201cit is very much a team sport,\u201d explains one professional. <a href=\"#fnref-H9rB9jFyxokwFBvnj-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-10\" class=\"footnote-item\"><p>Advice for going from technical backgrounds into policy journalism:</p>\n<ul>\n<li>You can leverage that unique expertise to be a technology / AI journalist. Deep understanding there is rare in journalism, so lean into that. (This scarcity of tech-experienced tech journalists is because software engineering pays much better than journalism.)</li>\n<li>You can also aim for journalism jobs that require some software skills.</li>\n<li>Human management and synthesis skills from certain jobs in tech might transfer over well.</li>\n</ul>\n <a href=\"#fnref-H9rB9jFyxokwFBvnj-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-11\" class=\"footnote-item\"><p>Reputation in DC is highly based on networks of trust and likeability, since measuring technical merit in policy-making is much harder than in some other fields (presumably due to there being less clear links between individuals\u2019 contributions and policy outcomes). <a href=\"#fnref-H9rB9jFyxokwFBvnj-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-12\" class=\"footnote-item\"><p>Imperfect because it\u2019s hard to tell how much correlation between a degree and a job comes from causation, as opposed to, say, self-selection. <a href=\"#fnref-H9rB9jFyxokwFBvnj-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-13\" class=\"footnote-item\"><p>\u201cLast 5,\u201d here and later, is relative to Wikipedia\u2019s lists on Sep. 17, 2021. <a href=\"#fnref-H9rB9jFyxokwFBvnj-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-14\" class=\"footnote-item\"><p>Source: a policy professional who is familiar with them <a href=\"#fnref-H9rB9jFyxokwFBvnj-14\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-15\" class=\"footnote-item\"><p>The only unclear case is one director\u2019s applied economics PhD. <a href=\"#fnref-H9rB9jFyxokwFBvnj-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-16\" class=\"footnote-item\"><p>Source: a policy professional who is familiar with them <a href=\"#fnref-H9rB9jFyxokwFBvnj-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-17\" class=\"footnote-item\"><p>These OSTP directors all also served as university professors before becoming OSTP directors. <a href=\"#fnref-H9rB9jFyxokwFBvnj-17\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-18\" class=\"footnote-item\"><p>Degrees of various very senior US executive branch officials.</p>\n<ul>\n<li>In some roles that (I think) deal extensively with national security:\n<ul>\n<li>Among the last 5 (non-acting) <a href=\"https://en.wikipedia.org/wiki/List_of_secretaries_of_state_of_the_United_States\">US Secretaries of State</a>, 4 had JDs. (None had other graduate degrees.)</li>\n<li>The highest degrees of the last 5 (non-acting) <a href=\"https://en.wikipedia.org/wiki/United_States_Secretary_of_Defense#List_of_secretaries_of_defense\">US Secretaries of Defense</a> were 2 doctorates, 2 master\u2019s, and 1 bachelor\u2019s.</li>\n<li>Among the last 5 (non-acting) <a href=\"https://en.wikipedia.org/wiki/United_States_Secretary_of_Homeland_Security#List_of_secretaries_of_homeland_security\">US Secretaries of Homeland Security</a>, the highest degrees were JDs for 4 of them, and a master\u2019s for 1.</li>\n<li>Among the last 5 (non-acting) <a href=\"https://en.wikipedia.org/wiki/Director_of_National_Intelligence#Directors\">US Directors of National Intelligence</a>, the highest degrees were 3 JDs and 2 master\u2019s.</li>\n</ul>\n</li>\n<li>Among the last 5 (non-acting) <a href=\"https://en.wikipedia.org/wiki/United_States_Secretary_of_Agriculture\">US Secretaries of Agriculture</a>, the highest degrees were 3 JDs, 1 veterinarian degree, and 1 master\u2019s.\n<ul>\n<li>(This is double-counting the one who served two non-continuous terms.)</li>\n</ul>\n</li>\n<li>Among the last 5 (non-acting) <a href=\"https://en.wikipedia.org/wiki/United_States_Secretary_of_Commerce#List_of_secretaries_of_commerce\">US Secretaries of Commerce</a>, the highest degrees were 3 JDs, 2 master\u2019s, and 1 doctorate.\n<ul>\n<li>(This adds to 6 because one Secretary has both a JD and a doctorate.)</li>\n</ul>\n</li>\n</ul>\n <a href=\"#fnref-H9rB9jFyxokwFBvnj-18\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-19\" class=\"footnote-item\"><p>Advice from a few journalists (from one particular news outlet):</p>\n<ul>\n<li>Probably don\u2019t try to send a finished piece; when they do publish, it usually requires going back and forth editing, and it\u2019s also often not a good fit.</li>\n<li>A piece that the news outlet\u2019s own journalists couldn\u2019t write that you can, because of your unique experiences, can be especially interesting to them.</li>\n<li>Then there\u2019s the separate skill of being good at being edited.</li>\n<li>Tailor your piece to the outlet you\u2019re writing for; different outlets like different kinds of op-eds (and you can get a sense of their style by reading their op-eds).</li>\n</ul>\n <a href=\"#fnref-H9rB9jFyxokwFBvnj-19\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-20\" class=\"footnote-item\"><p>Advice from a few journalists:</p>\n<ul>\n<li>Be callable: get back promptly on short turnarounds, and have solid and quotable things to say.</li>\n<li>There\u2019s reasonable reasons to be hesitant about this role, but there\u2019s lots of room to get good at it.</li>\n<li>Someone counted that 40% of references to political scientists were references to one guy who literally sat by the phone all day.</li>\n<li>You might need to unlearn (or at least temporarily not use) academic writing articles.\n<ul>\n<li>How to do that: practice; imitating things you see in other good articles.</li>\n</ul>\n</li>\n</ul>\n <a href=\"#fnref-H9rB9jFyxokwFBvnj-20\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-21\" class=\"footnote-item\"><p>Response from a few journalists:</p>\n<ul>\n<li>More mediums would be good, but do the stuff you\u2019re good at\u2014match is very important.</li>\n<li>It takes more time/money typically to make videos than text content.</li>\n<li>You\u2019re also able to fit less text / abstract ideas into e.g. a 5-min video (that\u2019s about 200 words; you can do pictures/visuals but that\u2019s harder for some things).</li>\n</ul>\n <a href=\"#fnref-H9rB9jFyxokwFBvnj-21\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-22\" class=\"footnote-item\"><p>One relevant professional suggests: it\u2019s hard to give a blanket answer. A few points:</p>\n<ul>\n<li>In general, they\u2019d expect spending time in China not to prevent you from getting a clearance, but quite likely to slow down your clearance process. This could change though, e.g. if US-China relations deteriorate (if that happens, time in China could be looked on worse).</li>\n<li>Different parts of government handle clearances differently. Spending significant time in China is more likely to be a problem for working in the intelligence community than the State department, for example.</li>\n<li>If you do go, make sure to keep records of where you live and who you have close contact with.</li>\n<li>How much this will impact your clearance will also depend on how long ago it was by the time you apply - if you apply right after getting back, that\u2019s super different than if you apply, say, 4 years later. It will also matter how many \u201cclose, continuing conflicts\u201d you have who are Chinese nationals when you apply.</li>\n</ul>\n <a href=\"#fnref-H9rB9jFyxokwFBvnj-22\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-H9rB9jFyxokwFBvnj-23\" class=\"footnote-item\"><p>One relevant professional\u2019s response: they think you can make basically any level of fluency work. If you don\u2019t read Chinese, then you shouldn\u2019t set yourself up to be doing work that would require that, but if you\u2019re willing to be enterprising, use machine translation, and so on, you can do quite a bit. If you actively feel interested in learning Chinese then I think that\u2019s a great idea, and they\u2019ve really enjoyed investing time in that, but if that feels like pulling teeth then they don\u2019t think it needs to prevent you from working in the area, as long as you\u2019re aware of your limitations. <a href=\"#fnref-H9rB9jFyxokwFBvnj-23\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Mauricio"}}, {"_id": "tvB4HnweNJbdu7LeA", "title": "\"Intergenerational communication\": could something like \"writing letters to the future\" suscitate interest in long-termism?", "postedAt": "2021-11-07T23:35:29.248Z", "htmlBody": "<h3>(I mean, <i>ersatz</i> intergenerational communication)</h3><p>I was intrigued by this project, <a href=\"https://www.deartomorrow.org/\">Dear Tomorrow</a>, where people are invited to write letters to the future to commit to take action against climate change. T. Shum's <a href=\"https://link.springer.com/article/10.1007/s10584-021-03002-6\">paper</a> suggests this increases \"the willingness to donate to climate change mitigation efforts\" - by raising the concern with the prospects of future generations. This fits my priors - and it reminded me of the impact Bostrom's <a href=\"https://www.nickbostrom.com/utopia.html\"><i>Letter from Utopia</i></a><i> </i>had on me.</p><p>I wonder if one could extrapolate this idea to long-termism in general - e.g., inviting people to write messages to their descendants in different future periods, regarding distinct global issues... Or even the opposite: make them write to an ancestor (as if they are the utopians in Bostrom's tale), commenting on the latter's &nbsp;nuisances and on the pros and cons of the present. Maybe there's something like that already, and I just don't know how to google it? Maybe it would be cool for us to try something like that in EA groups and workshops?</p>", "user": {"username": "Ramiro"}}, {"_id": "ZtA8wgRcPwtewjGoW", "title": "Experience with Donation Matching Initiatives?", "postedAt": "2021-11-08T07:01:02.554Z", "htmlBody": "<p>Hi all!</p><p>I'm hoping to begin an initiative to push for donation matching at my company and wanted to see if anyone here has any experience doing similar things and has any suggestions / things to keep in mind to make the initiative a success.</p>", "user": {"username": "sreers"}}, {"_id": "eiezKWZzEw3Ca7ctq", "title": "Climate Change: Prevention vs Preperation", "postedAt": "2021-11-07T15:09:03.279Z", "htmlBody": "<p>Why is the EA focus of climate change on prevention rather than preparation? It seems to me, perhaps wrongly, that preventing climate change is increasingly unlikely and the best we can really achieve is reducing the impact. Should the focus of EA therefore not be on prevention - which is the focus point of so many groups anyway - but preparation assuming the worst-case scenarios come true? This appears to be an underfunded area because people don't like acknowledging the possibility climate change can't be prevented, it seems defeatist in public policy. On the other hand the benefit of such work is very high! Does anyone have a list of effective groups tackling the problems that could happen if our current climate goals fail?&nbsp;</p>", "user": {"username": "edwardhaigh"}}, {"_id": "w4u2WksZjJdn2AKyB", "title": "Why Undergrads Should Take History Classes", "postedAt": "2021-11-07T14:25:56.390Z", "htmlBody": "<p>I really liked <a href=\"https://forum.effectivealtruism.org/users/mauricio\">Mauricio</a>'s recent post, <a href=\"https://forum.effectivealtruism.org/posts/hgiLaE3eL76ovcfdH/many-undergrads-should-take-light-courseloads\">Many Undergrads Should Take Light Courseloads</a>. I agree that the majority of the value of college is outside of the classes, and that taking too many classes can detract from other very important things you could be doing.</p><p>Still, as a student, you do have to take classes in order to graduate. Most of those classes will be determined by your major and what you intend to do after college. But, at least in the US, some of your classes won't be related to your major. What should you take?</p><p>As a computer science major, I have taken some non-CS classes that were far more valuable than the average CS class. Most of them were history classes. In general, I think that history classes can be quite useful, especially for people in EA. In this post, I'll share some lessons I've learned from history.</p><p>First, a couple caveats:</p><ul><li>I'm generally interested in history so this is likely to be biased. This might not be the advice for you if you've always hated history classes (but again, maybe you just haven't had the right history class).</li><li>Yale (the university I attend) is supposed to have one of the best history departments in the world (according to dubious news sources). It's possible this isn't good advice at some universities with weaker history departments, but I doubt this.</li></ul><p>With those out of the way, I'll share what I've learned from history classes:</p><ul><li><strong>Well-meaning people can do very bad things.</strong> History is filled with people who we would now call immoral, because of the terrible actions that they took. While some of them acted purely out of self-interest, others were actually trying to do good.</li><li><strong>The ends often don't pan out.</strong> Consequentialism is obviously quite important in EA, for very good reason. That being said, history is filled with examples of consequentialism going wrong. Why? Because somebody was overconfident in their predictions about the value or probability of the ends. They put very high probability on very high value outcomes when they shouldn't have. The imagined ends might actually have justified the actual means, but history shows there is a difference between the imagined ends and the realized ends.</li><li><strong>The future is hard to predict.</strong> Humans like to make predictions about the future, and there are many historical examples. Lots of people end up being very wrong. Some end up being right, but not for the reasons they said. Others predict things eerily well. It's sometimes useful to think about these successes and failures when making your own prediction about the future.</li><li><strong>Morality changes, a lot. </strong>What people consider good and bad varies tremendously across time and space. Many EA's are probably familiar with the idea of <a href=\"https://en.wikipedia.org/wiki/The_Expanding_Circle\">the expanding circle</a>, but this is a relatively simplistic way of describing the fact that morality can change quite significantly and rapidly. What we believe is right today might not last.</li><li><strong>Ideologies come and go. </strong>The majority of ideologies do not last as they were originally articulated. Many simply die; others are subsumed into other ideologies; others evolve with the times. Projects driven by all-encompassing ideologies sometimes succeed, but they are often eventually coopted by other ideologies.</li><li><strong>Things have not always been this way.</strong> There are a lot of things that many of us take for granted in modern society, especially in Western countries. Bureacracies, secularism, peace at home, capitalism, borders, nation states... the list goes on. And yet, many of these things didn't exist, or existed very differently, not very long ago. This knowledge can help with the realization that immense change could still be on the horizon.</li></ul><p>Lots of these things are somewhat obvious. Still, taking the time to process history allows you to actually process them, in a way that I think is very important.</p><p>You might wonder what kinds of history classes to take. In the past, I've taken: European Intellectual History since Neitzsche, Eastern European History To 1914, A History of South Africa, and now a class on American history in the 20th century. Here are a few things I value in a history class:</p><ul><li><strong>Primary source readings</strong>. The best way to learn history is often from those who lived it. Stay away from classes that teach from textbooks and look for classes that teach from the primary sources.</li><li><strong>Engaging lectures.</strong> Most history classes you'll take are likely to be lectures. You will learn far more if they are engaging. Even if a class is on an area of history you find fascinating, you won't learn much if the professor has a monotone style and slides with 200 words each.</li><li><strong>Research assignments.</strong> Research is a good way to learn actively rather than passively. Also, in many cases, you can probably make the research on something that is relevant to EA. For instance, I am now writing a paper on the biological weapons convention.</li><li><strong>Not much emphasis on memorization.</strong> Memorizing history facts is very unimportant. You want the key themes, not the key dates. You might have to memorize some facts for your classes, but the classes should be mostly not about memorizing facts.</li></ul><p>Lastly, I will close by saying that like every professor, history professors have agendas. They have their own view of history and how they want to present it. The best history professors tell stories, and those stories often have morals. Don't always take them for granted.</p><p>The next time you sit down to consider classes, and realize you need to satisfy a requirement or you have a blank space in your schedule, remember this post: consider history.</p>", "user": {"username": "ThomasWoodside"}}, {"_id": "HfdeFgFnNtbnkWgqk", "title": "How do EAs deal with having a \"weird\" appearance?", "postedAt": "2021-11-07T00:49:24.700Z", "htmlBody": "<p><i>Background info: I'm male, black, and considering academia as a career path.&nbsp;</i><br><br>My hair has recently gotten a bit longer and I started experimenting with different hairstyles. (E.g. cornrows.) And given that there <a href=\"https://www.bbc.com/news/uk-england-london-13367631\">might</a> <a href=\"https://en.wikipedia.org/wiki/Afro-textured_hair#Modern_perceptions_and_controversies\">be</a> <a href=\"https://www.pbs.org/newshour/show/how-hair-discrimination-impacts-black-americans-in-their-personal-lives-and-the-workplace\">a </a><a href=\"https://en.wikipedia.org/wiki/Cornrows#Controversy\">price</a> <a href=\"https://www.forbes.com/sites/nickmorrison/2019/04/05/black-students-face-racial-bias-in-school-discipline/\">to</a> <a href=\"https://www.thetrendspotter.net/unprofessional-hairstyles-for-men/\">pay</a> in terms of \"<a href=\"https://forum.effectivealtruism.org/posts/MH9suFZbxXCYsr5Z5/you-have-a-set-amount-of-weirdness-points-spend-them-wisely\">weirdness points</a>\", I realized I will probably shave it off. Mostly because I value career opportunities a lot more than I do \"self-expression\". (At least regarding this issue.)<br><br>I was wondering what other EAs think about trade-offs such as these. Not only about \"weird\" hair but other forms of (somewhat controllable) \"weirdness\" like being openly/visibly LGBT+, or having a <a href=\"https://forum.effectivealtruism.org/posts/qf6pGhm9a7vTMFLtc/english-as-a-dominant-language-in-the-movement-challenges\">non-standard accent</a>.</p>", "user": {"username": "SamiM"}}, {"_id": "7heEBq7oZnMvbSbJa", "title": "WEIRD morality", "postedAt": "2021-11-07T05:41:53.166Z", "htmlBody": "<h1>Moral Ought</h1><p>A lot of people from WEIRD (Western, Educated, Industrialized, Rich, Democratic) societies I've met seem to think that \"ought\" necessarily implies morality. I believe this is either true or false depending on one's domain-of-moral-ought.&nbsp;</p><h1>Domain-of-moral-ought</h1><p>What is a domain-of-moral-ought? A person's domain-of-moral-ought is whatever counts as a moral issue in their worldview. &nbsp;Moral ought can refer either to moral norms or moral values. A domain-of-moral-ought is contingent upon the idiosyncratic worldview specific to a person, therefore, it differs from one person to another. A clear example of how domains-of-moral-ought can differ between people is on the issue of science's moral status. According to some people's worldviews, science never falls within their domain-of-moral-ought whereas for others, science itself is intrinsically amoral but sometimes may be forced to grapple with moral problems in its practical applications; still, some others believe that scientific theorizing is intrinsically moral rather than being only incidentally moral, and therefore, all of science falls under the purview of morality.</p><h1>Moral Foundations Theory</h1><p>One's domain-of-moral-ought is closely linked to one's intuitive moral foundations. According to Moral Foundations Theory (Haidt &amp; Bjorklund, 2008; Haidt &amp; Kesebir, 2010), the \u2018moral domain\u2019 comprises a limited number of areas of social concern. These areas of moral relevance are norms and values pertaining to:</p><ol><li>care/harm</li><li>fairness/cheating</li><li>liberty/oppression</li><li>loyalty/betrayal</li><li>authority/subversion</li><li>sanctity/degradation</li></ol><p>Jonathan Haidt explains the differing moral domains by analogy to taste buds. We have taste receptors for sweet, sour, salty, bitter, and umami. But just like we all have the same taste buds yet different tastes in food, it is also true that we all have the same moral taste buds yet different tastes in morality. As a result different groups are more sensitive to different moral taste buds over others. This is a big factor that shapes the differences in our moral ought beliefs.</p><h1>WEIRD domain-of-moral-ought</h1><p>Much of WEIRD morality seems to focus exclusively on welfare and justice. EAs in particular seem to be have theories of value that are single-mindedly focused on wellbeing and suffering-reduction. The welfarism (belief that welfare is the only thing with intrinsic value) that seems to be so prevalent in the EA movement ignores non-welfarist theories that recognize other sources of value, such as fairness, equality, or beauty. Welfarism is what happens when one's domain-of-moral-ought is based only on harm/care norms without regard for the other norms that exist; sometimes even going so far as to deny the existence of these other norms. Other values such as justice and liberty are merely thought as means to achieving the ultimate moral value of wellbeing or suffering-reduction.</p><p>Going back to the moral taste receptor metaphor, WEIRD moralities are like cuisines that only try to activate one or two of these receptors:&nbsp;</p><p>Imagine a restaurant which reasoned that since the activation of sweet receptor produces the strongest surge of dopamine in the brain, it is highly efficient in terms of units per of pleasure per calorie to consume sweeteners. This sort of restaurant aims exclusively to stimulate this one taste receptor. Naturally, this would be off-putting to people who are more used to having more than just one taste receptor being satisfied. Likewise, a morality that concerns itself exclusively with suffering-reduction or harm-minimization can ignore important moral issues like justice or liberty.</p><p>It is believed that the attempt to ground all of morality on a single principle leads to societies that are unsatisfying to most people and at high risk of becoming inhumane because they ignore so many other moral principles.</p><h1>Prudential Ought</h1><p>In WEIRD societies, where the domain-of-moral-ought is almost exclusively limited to concerns of welfare (care norms) and justice (fairness norms), only norm-violations such as harm and injustice are moralized and, therefore, considered to fall within the purview of moral ought. Norms pertaining to cleanliness, beauty, loyalty, politeness, obedience and sanctity are not intrinsically valuable according to WEIRD morality and so belong to prudential ought. Prudential oughts are adhered to not because they are moral obligations, but because they are instrumental norms which are valued insofar as they contribute to intrinsic values such as care. For instance, I once heard a distinctly WEIRD argument that cleanliness may be justified as a moral norm insofar as it contributes to eradicating disease and reducing the harm that arises from it. This, I feel, to be a stark contrast to non-WEIRD attitudes which value cleanliness in itself, without recourse to welfare. The distinction between moral and prudential ought seems to exist only in WEIRD societies where norms such as \"you ought to be loyal to me\" or \"I ought to be polite to you\" are no longer considered moral but are recognized to have some instrumental value if we abide by them. A more pertinent example of a norm that used to be moralized but is now a prudential norm \u2014 at least in WEIRD societies \u2014 are norms about belief-formation. Belief in a deity's existence as a matter of moral ought is a clear-cut example of how belief-formation is moralized in many religious people's worldviews but is largely a matter of prudential ought in WEIRD societies. Neither to believe nor to disbelieve is a matter of moral ought for WEIRD DOMO.</p><h1>Non-WEIRD domain-of-moral-ought</h1><p>In contrast to this, many non-WEIRD societies (including my own) moralize many of these so-called prudential norms and consider them to be moral norms. To use the taste buds analogy again, non-WEIRD societies tend to rely on all six moral foundations in tasting the cuisine of their domain-of-moral-ought. In such a worldview, adherence to the law is in itself a moral act independent of its contribution to welfare.</p><h1>Conclusion</h1><p>The domain-of-moral-ought differs between WEIRD and non-WEIRD societies. Different DOMOs might be evoked in discussion when morality is evoked because different people have different scopes of the moral domain. In other words, their domain-of-moral-ought differs in what it counts as moral. Part of the reason why these discussions about values can be so heated with so much people talking past each other is because there is no consensus of DOMO. Hopefully this clarification allows for better disagreements between people of different DOMOs.</p><h1>&nbsp;</h1>", "user": {"username": "imperfectscout"}}, {"_id": "ikyF2nrR9qDdMYm9z", "title": "Upcoming Effective Environmentalism talks ", "postedAt": "2021-11-06T12:06:48.767Z", "htmlBody": "<p>Hi all! We, <a href=\"https://www.effectiveenvironmentalism.org/\">Effective Environmentalism</a>, are organising upcoming talks from those tackling climate change using an EA (or adjacent) approach. We've got three quite exciting talks lined up over the next two months so if anyone is interested in learning more, do sign up below.</p><p>We're always looking for new speakers so if you might be interested or have any suggestions for potentially interesting speakers, please comment below and let me know!</p><p>&nbsp;</p><p><a href=\"https://www.eventbrite.com/e/how-to-decarbonise-the-financial-system-universal-ownership-vs-esg-tickets-196956992667\"><strong>How to Decarbonise the Financial System: Universal Ownership vs ESG - </strong></a><strong>November 21st, 7-8pm GMT</strong></p><p>An exploration of the merits of universal ownership vs ESG for decarbonising our financial system, with <a href=\"https://www.cser.ac.uk/team/ellen-quigley/\">Dr Ellen Quigley</a> from University of Cambridge Centre for the Study of Existential Risk (CSER). In this talk, Dr &nbsp;Quigley will describe universal ownership as an approach to real-economy decarbonisation, and go over the ways in which ESG is ill-equipped to decarbonise the financial sector (and therefore the real economy).</p><p>&nbsp;</p><p><a href=\"https://www.eventbrite.com/e/the-changing-landscape-of-high-impact-climate-philanthropy-tickets-191542939087\"><strong>The changing landscape of high-impact climate philanthropy - </strong></a><strong>November 28th, 6-7pm GMT</strong></p><p>Johannes Ackva, a Senior Researcher from the Founders Pledge Climate Fund, will discuss recent research, updates to their fund strategy and charity recommendations going forward. Come along to hear some exciting updates on how the focus of the Founders Pledge Climate Fund is shifting and evolving.</p><p>&nbsp;</p><p><a href=\"https://www.eventbrite.com/e/good-news-on-climate-change-what-is-a-worst-case-scenario-tickets-203871744877\"><strong>Good news on climate change + what is a worst-case scenario?</strong></a><strong> - Sunday December 19th, 6-7pm GMT</strong></p><p>Dr John Halstead from the Forethought Foundation will be discussing some recent positive climate science as well as some potential worst-case scenarios.<strong> </strong>In this talk, John will firstly discuss some good news on climate change: on current policy, emissions look set to be lower than once feared, as is the risk of very high climate sensitivity. Secondly, John will discuss a worst-case scenario in which we burn all of the fossil fuels: how many fossil fuels are there, how likely we are to burn them, how we might do so if we did, the warming that would produce, and what that might mean for life on Earth.</p><p>&nbsp;</p><p>You can also stay in touch with our events via our <a href=\"https://www.effectiveenvironmentalism.org/get-involved\">newsletter</a> (highest recommendation!), <a href=\"https://calendar.google.com/calendar/u/0?cid=anNlZDZ1bjZuNjUxYjZndjQzYmRjb2JvN2NAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ\">Google Calendar</a>, <a href=\"https://www.facebook.com/groups/effectiveenvironmentalism\">Facebook page</a> (some good discussions here) and <a href=\"https://join.slack.com/t/effectiveenvironment/shared_invite/zt-fw634it0-Sy60JGwJyvF88m~C80th~w\">Slack Channel</a>.</p>", "user": {"username": "JamesOz"}}, {"_id": "uaJXi6vPKt2rw97wb", "title": "Seeing the effects of your donation and making incremental choices", "postedAt": "2021-11-05T21:46:39.173Z", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/oYvmPnc72npdtFuPg/effects-of-being-able-to-see-effects-of-donation-first-hand\">Samuel Shadrach asked </a></p>\n<blockquote>\n<p>Have there been any studies on the effects of being able to see the effects of your donation to an EA cause first-hand?  ... it could matter.  if it's not just first-hand but personalised and non-fungible. Like \"this specific child or alcoholic is being sponsored by you\"</p>\n</blockquote>\n<p>I've thought a lot about this in the past, also considering making this apparent by allowing the donor to 'personalize' the outcome in some un-interfering way, like painting the sanitary facilities blue. I considered work on the impact of 'donor choice' as a tool for boosting effective giving.  For example, in <a href=\"http://repository.essex.ac.uk/10009/1/dp749.pdf\">a 2014 essay</a> I wrote an essay <sup class=\"footnote-ref\"><a href=\"#fn-PWGJA8xE5DB2EsmJ5-1\" id=\"fnref-PWGJA8xE5DB2EsmJ5-1\">[1]</a></sup>(<a href=\"https://www.degruyter.com/document/doi/10.1515/klio-2017-0013/html\">https://www.degruyter.com/document/doi/10.1515/klio-2017-0013/html</a>) ... in which I stated</p>\n<blockquote>\n<p>When the giver retains some use, experience, or control over the gift, she shares in the consumption of it ...\nLarger organizations like UNICEF have also depersonalized giving, re- moving the connection between giving, empathy, and a feeling of ownership. The [ancient[ liturgy institution may have preserved this while taking away the voluntary aspect. There is evidence that visible impact, a personal connection, and the ability to make choices are important drivers of giving, and that these are crucial to the emotional benefits of giving. [citations removed]</p>\n</blockquote>\n<p><strong>Considerations:</strong></p>\n<blockquote>\n<p>Is this idea special to EA? This may help reduce the <a href=\"https://daaronr.github.io/ea_giving_barriers/aware-distance.html\">\"distance\" concern</a> by making the gift more tangible? This practice generally comes at the cost of effectiveness. However, if it motivates substantially more giving, it may be worth it. For international poverty relief this is sometimes very hard to do credibly because of the distance and mediating channels, but easier because objects (e.g., a toilet) can be built at relatively low cost.</p>\n</blockquote>\n<blockquote>\n<p>This also relates to the discussion of identifiable victims, and Peter Singer's discussion in The Life You Can Save (under \"Creating a Culture of Giving\")</p>\n</blockquote>\n<p><img src=\"https://freewheeldesign.com/wp-content/uploads/2017/06/Plan-WelcomeBrochure2016_pg3.jpg\" alt=\"\"></p>\n<blockquote>\n<p>[Plan International] does its best to retain the appeal of the identifiable child by continuing to invite potential donors to \u201cSponsor a Child\u201d for between \u00a312 and \u00a317 ($24 to $34) per month, and the sponsors can write and receive letters, visit their sponsor child, and send \u201csmall gifts.\u201d But potential sponsors are also told: \u201cYour money does not go to the individual child that you sponsor. So that Plan can make efficient use of funds, the money is pooled with contributions from other sponsors to support programs benefiting communities worldwide.\"</p>\n</blockquote>\n<p><em>But does the latter part of the message, even if it allows 'more efficient use' of the funds, undermine the emotional impact and reduce the amount donated?</em></p>\n<h1>Academic work or other studies?</h1>\n<p>I recall seeing some academic studies that get at this (pairing potential donors with particular identified recipients), but not in a very meaningful way.<sup class=\"footnote-ref\"><a href=\"#fn-PWGJA8xE5DB2EsmJ5-2\" id=\"fnref-PWGJA8xE5DB2EsmJ5-2\">[2]</a></sup>. People typically knew they were in an experiment and the amounts were small or low probability, and the designs are often 'weird' in other ways.</p>\n<p><a href=\"https://users.nber.org/~kesslerj/papers/Kessler_etal_RichandPowerful_2017.pdf\">\"Getting The Rich and Powerful to Give\"</a> involves a large field experiment where they give donors the \"option to express their charitable giving priorities on the reply card\", and find a substantial positive effect, but this is a framing change, not actual earmarked giving. (And this was for an ivy league  university alumni fundraiser, perhaps the polar opposite to an effective charity.)</p>\n<p>There's also a  lot on the <a href=\"https://www.tandfonline.com/doi/full/10.1080/15534510.2016.1216891\">'identifiable victim effect'</a>, but these experiments essentially alter the framing, not the <em>actual conditionality or donor-recipient connection.</em></p>\n<h1>My take</h1>\n<p>I've thought about this for a while, and I've long been wanting (someone) to engineer a 'good trial of this'. It's challenging, but might be possible with greater internet access and new payment mechanisms.</p>\n<p>*Responding to <a href=\"https://forum.effectivealtruism.org/posts/oYvmPnc72npdtFuPg/effects-of-being-able-to-see-effects-of-donation-first-hand\">Samuel's post</a>... Samuel suggested I make this a standalone post. I reprise/excerpt my response below...</p>\n<h2>The 'two sided market', tangibility and agency</h2>\n<p>EA donors  are special creatures, but to make donations sustainable in the large community one might need to think about <em>both</em></p>\n<ul>\n<li><strong>the gains to the 'beneficiaries', and</strong></li>\n<li><strong>the internal emotional and social benefits for the donor.</strong></li>\n</ul>\n<p>I.e., a <strong>'two-sided market'.</strong> One needs to consider the 'donor as consumer'.  As I allude to above,  we might want to consider some reduction in former if it greatly increases the latter.</p>\n<p>The latter certainly involves tangibility, the feeling of having done something that you can see changes the world in a visible way, 'agency', and the feeling of having a particular attachment to somebody you can help.</p>\n<p>People buy fireworks <em>even when their town has a big show</em> because they get more pleasure from \"lighting fireworks and controlling their direction\" than from \"seeing a display\".</p>\n<p><img src=\"https://lakewoodchamber.com/wp-content/uploads/2019/06/FireworkSafety2019_728.jpg\" alt=\"\"></p>\n<h2>Tangibility, incrementally, and donor-choice: turning a weakness of effective charities into a strength?</h2>\n<p>The problem is that the forms of generosity that you can more easily 'control and see the tangible effects of' tend to be more local to the wealthy (by global standards) donors, less neglected, and thus less effective at the margin.</p>\n<p>Furthermore, ensuring and enabling this 'specific donation' can itself be costly in terms of administration and communication. It can also lead to some departure from 'giving to the most needy' if the 'most needy person' is harder to communicate with.</p>\n<p>But I think there is <strong>potential to try to harness tangibility and incrementally in the effective giving space</strong>.</p>\n<p>There is the idea of 'sponsor an individual child or family or village'. My impression is that many charities in fact do this in their marketing and communication but the actual donations are not directly tied to particular beneficiaries.  And I expect donors realize this. When I spoke with these charities they say that there are both practical and ethical issues making this undoable (see below).</p>\n<h2>My proposal sketch</h2>\n<p>...Something like telling people</p>\n<blockquote>\n<p>We are linking each potential donor to a particular household (or village etc).  You are linked to the ZJHGUH household in SHMZPLA.</p>\n</blockquote>\n<blockquote>\n<p>Do you want to donate to provide the  ZJHGUH household with education, medicine, and clean water?  You are the only potential donor linked to ZJHGUH by our organization.</p>\n</blockquote>\n<blockquote>\n<p>If you donate to build a new sanitary facility, you will be able to choose which color it is painted, and we'll send photos</p>\n</blockquote>\n<p>The idea is close to what are you are suggesting. Furthermore, technology could allow us now to provide some pictures and feedback directly linked to the beneficiary household.</p>\n<h2>Objections and responses</h2>\n<p><strong>The strongly-stated objections to this I have heard:</strong></p>\n<ol>\n<li>\"This is unethical/unfair to the beneficiary households not targeted\"</li>\n<li>\"This is too manipulative of potential donors\"</li>\n<li>\"This would be impossible to implement\"</li>\n<li>\"This is too 'white-saviour vs victim'-ish\"</li>\n</ol>\n<p>These objections are somewhat reasonable (but some could also be rebutted to an extent... see below). Anyways,  I suspect that if done right, the benefits will outweigh the costs, in terms of generating substantial amounts of donations  (as well as bringing connections between people on both sides of the global divide, which may have additional benefits).</p>\n<h2>Why do I think it could be so effective at motivating donations?</h2>\n<p>(Repeating the above a bit)</p>\n<p>Humans in general (including the global wealthy) devote huge shares of our income to ...</p>\n<ul>\n<li>our family</li>\n<li>people whom we interact with, and in our community (see '<a href=\"https://daaronr.github.io/ea_giving_barriers/aware-distance.html#in-group\">parochialism</a>')</li>\n<li>public goods that we can have a tangible impact on (e.g., fireworks, public art, gardens)</li>\n</ul>\n<p>If helping the worlds' poorest people can be made into something that is tangible, incremental, and more 'direct' I think it will leverage our innate desires...</p>\n<ol>\n<li>To help those we feel we have an 'obligation to' (because no one else will help them),</li>\n<li>To have a connection to people whom we can help, and</li>\n<li>To have agency and see the impact of our actions.</li>\n</ol>\n<p>You might object to the first point,  saying</p>\n<blockquote>\n<p>this is inaccurate ... how can you say 'no one else will help them'</p>\n</blockquote>\n<blockquote>\n<p>how can you link a single donor to a single beneficiary and otherwise deny that beneficiary the opportunity?</p>\n</blockquote>\n<p>But there is a sense in which the <em>standard</em> charity and aid does this anyways, only in a more probablistic sense. We don't give enough to provide for all the world's poor. Some are going to be denied opportunities, and an individual donation <em>does</em> make this difference. It's just that it is not completely traceable.</p>\n<p>By making it traceable and tangible we might unlock a vast \"supply of generosity\".</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-PWGJA8xE5DB2EsmJ5-1\" class=\"footnote-item\"><p>Tied to a conference and book on gifts and the 'embeded economy' in the ancient world <a href=\"#fnref-PWGJA8xE5DB2EsmJ5-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-PWGJA8xE5DB2EsmJ5-2\" class=\"footnote-item\"><p>E.g., this <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S004727271000160X?via%3Dihub\">\"Truth in Giving\"</a> study involves pairing up (lab subject) donors with particular recipients and considers willingness to pay for information about deservingness, but they don't vary the presence of the connection.  <a href=\"https://doi.org/10.1016%2Fj.jpubeco.2010.06.018\">\"Warm glow, information, and inefficient charitable giving\"</a> is also marginally relevant <a href=\"#fnref-PWGJA8xE5DB2EsmJ5-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "david_reinstein"}}, {"_id": "cFRbLmhCEu74wcJ3D", "title": "[Discussion] Best intuition pumps for AI safety", "postedAt": "2021-11-06T08:11:17.165Z", "htmlBody": "<p>When I introduce people to AI safety I usually get one of three responses:&nbsp;<br>a) \u201cthat makes a lot of sense. What can we do about it?\u201d,&nbsp;<br>b) \u201cI get it rationally, but intuitively I don\u2019t feel it\u201d,<br>c) \u201cI just don\u2019t buy it - I don\u2019t think machines can\u2019t be smarter than humans\u201d, \u201cI still think that we can just program them the way we want\u201d or something along these lines.&nbsp;</p><p>I get the last response even after giving the standard arguments for why a stop button won\u2019t work, why superhuman intelligence is plausible or why intelligence doesn\u2019t imply morality. So my hypothesis is that they find the thought of unaligned superhuman AI so unintuitive that they are unwilling to actually consider the arguments.&nbsp;</p><p>Thus, my question is: <i>What are the best intuition pumps for AI safety?</i></p><p>I\u2019m personally looking for Carl Shulman-style common sense arguments similar to those of his <a href=\"https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/\"><u>80K podcast appearance</u></a>. He argues that buying insurance for a gain-of-function lab would probably cost billions of dollars which gives us a better intuition about the risk involved.&nbsp;</p><p>I have recently started making the following argument. If you think that AI won\u2019t be smarter than humans but agree that we cannot perfectly control AI in the same way that we cannot perfectly control humans, then you should be willing to pay as much money towards aligning AI as society spends on aligning humans, e.g. terror defense, prisons, police, and the justice system.&nbsp;</p><p>According to <a href=\"https://www.investopedia.com/articles/investing/061215/what-countries-spend-antiterrorism.asp\"><u>Investopedia,</u></a> the US alone spends 175$ Billion on counterterrorism and 118$ Billion on <a href=\"https://boingboing.net/2021/04/20/u-s-policing-budgets-would-rank-as-the-worlds-third-highest-military-expenditure.html\"><u>police</u></a> per year.</p><p><a href=\"https://www.ojp.gov/ncjrs/virtual-library/abstracts/what-does-world-spend-criminal-justice\"><u>This paper</u></a> from 2004 estimates that 70 rich nations spend more than 360$ Billion combined on the justice system in 1997.&nbsp;</p><p>Thus, if we adjust for inflation and missing countries we will likely get a lower bound of at least 1 Trillion Dollars spend per year on aligning humans. What we currently spend on AI safety is many orders of magnitude away from this.&nbsp;</p><p>Do you think this argument makes sense? Feedback and further suggestions are welcome. Your argument can also address different concerns that people typically have about AI safety.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "mariushobbhahn"}}, {"_id": "DDzak76SYYGQbb4jE", "title": "Who are your role models?", "postedAt": "2021-11-05T12:41:28.168Z", "htmlBody": "<p>Many of the paths EAs take towards impact are difficult, confusing, and very heavy-tailed: the most effective people on that path can achieve far more impact than the median. Under those circumstances, it is especially useful to have <i>role models</i>: examples of people achieving very high impact doing roughly the thing you are trying to do.</p><p>Finding role models, however, can be easier said than done. It's not enough for very inspiring people to exist: you need to learn about them, identify them as people you could benefit from emulating, and access key information about the things you might want to emulate (traits, habits, decisions, et cetera). The usual way to do this is via <i>media</i> (podcasts, biographies, et cetera) \u2013 but if your ideal role model isn't famous, or doesn't appear in media you consume, they are likely to pass you by. The main alternative to media is <i>gossip</i> \u2013 but gossip is quite low-bandwidth and is often least accessible to those who are in greatest need of role models.</p><p>EAs seeking role models have two broad places they can look: within the EA community, or outside it. Role models <i>within</i> the community are ideal in that they (i) are likely to more closely match the kind of thing you are trying to do, (ii) align with your values in a way that helps you emulate them more whole-heartedly, and (iii) are more likely to appear in media you already consume (especially the 80,000 Hours podcast).&nbsp;</p><p>However, the EA community is relatively small, and high-status public figures within the community are highly enriched for certain groups at the expense of others. If you want to do AI safety research, or global priorities research, or grantmaking, there are a decent number of well-known and super-impressive people in the community for you to emulate; if instead you want to do operations, or community building, or a host of other important paths, you will often need to identify role models on your own.</p><p>Role models <i>outside</i> the community are far more numerous and diverse, but also far less discoverable: one person on a given path might identify and benefit greatly from a particular role model, but another person on the same path might completely miss them, simply because they listen to different podcasts or read different books. Under these circumstances, it's valuable to create a place where people can share information on their favourite role models, so that other people with similar goals can discover them more easily.</p><hr><p>So, with all that said, <strong>who are your role models?</strong> Whose example inspires you, and <strong>why</strong> do you find them inspiring? What <strong>resources</strong> (books, podcasts, articles, et cetera) can other people use to learn about those role models, and hopefully be similarly inspired?</p><p>I'm particularly interested in collating information on <strong>lesser-known role-models</strong>: either inspirational people from outside the EA community, or role models from within the community whose example should be better known. Role models for career paths that tend to be <strong>under-represented among public figures</strong> (e.g. operations roles, community builders, event organisers, civil servants...) are especially welcome.</p>", "user": {"username": "willbradshaw"}}, {"_id": "FXPaccMDPaEZNyyre", "title": "A Model of Patient Spending and Movement Building", "postedAt": "2021-11-08T18:00:17.481Z", "htmlBody": "<p><i>This project began during Nu\u00f1o's 2020 summer research fellowship at FHI. Phil was the project mentor.</i></p><h2><strong>Motivation</strong></h2><p>The EA movement has tradeoffs to make about where to deploy its capital and labor. However, for now, these decisions seem like they are mostly made heuristically and intuitively.</p><p>To make those decisions more robust, we have set up a reasonably general <a href=\"https://philiptrammell.com/static/Labor__Capital__and_Patience_in_the_Optimal_Growth_of_Social_Movements.pdf\">model</a> to try to capture the most important dynamics. We hope that the model is informative enough to influence decisions directly, and that it motivates more gathering and systematization of empirical data about variables that the model finds crucial (rate of expropriation, the shape of returns to movement building, etc.).&nbsp; We also hope that it inspires further modelling work.</p><h2><strong>Setup</strong></h2><p>The model looks something like:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995062/mirroredImages/FXPaccMDPaEZNyyre/wscggjmx5tegb5fvdehu.png\"></figure><p>That is, the social movement has access to labor and capital.</p><p>In combination, they can be:</p><ul><li>Allocated to paid movement-building efforts, which return more labor</li><li>Allocated to direct work, which returns goods in the world (malaria nets, etc.)</li></ul><p>Alone, capital can be:</p><ul><li>Transformed into more capital with time</li><li>Transformed into labor through hiring (only possible in one of the two models; this is why this step is greyed out&nbsp; in the diagram)</li></ul><p>Alone, labor can be:</p><ul><li>Left alone to produce more labor, or decay, depending on the specifics of the model</li><li>Allocated to earning to give, which returns more capital (only possible in one of the two models; this is why this step is grayed out in the diagram)</li></ul><p>Note that the diagram only lays out the possible flows of labor and capital, but many parameters and functions determine how exactly that flow looks in practice. The paper defines these in more detail, but some which turn out to be important are:</p><ul><li>Utility is isoelastic in \"direct work\", meaning that as the quantity of \"direct work\" (e.g., malaria nets delivered) increases, the utility function is assumed to have constant curvature, in a certain sense. For instance, as the number of malaria nets delivered increases, they get sent to places where the need for them is less great: this would imply a curvature that is less than linear. In our model, this curvature is represented by <i>\u03b7</i>. <i>\u03b7</i> = 1 defines a logarithmic utility function, <i>\u03b7</i> &gt; 1 defines a function which exhibits sharper diminishing returns than the logarithm, and <i>\u03b7</i> &lt; 1 defines a function that exhibits returns which diminish more slowly. (<i>\u03b7</i> = 0 defines linear utility.)</li></ul><figure class=\"image image_resized\" style=\"width:46.57%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995062/mirroredImages/FXPaccMDPaEZNyyre/uh12kf97robnqrjyhynl.png\"><figcaption>Diminishing returns under some values for <i>\u03b7</i> in a isoelastic utility function</figcaption></figure><ul><li>Capital has a rate of return r</li><li>\u03b4 is the \u201cdiscount rate\u201d (more technically, the time preference), i.e. the rate of intrinsically caring less about the future (pure time preference) we have\u2014if any\u2014plus the annual rate at which we collectively face risks of expropriation, value drift, existential catastrophe, etc.</li><li>Labor, if left alone, depreciates (i.e., movement participants leave or die), at a rate <i>d</i></li><li>Labor productivity grows at a rate <i>\u03b3</i>, to reflect the growing labor productivity seen in the economy as a whole</li></ul><p>Using these functions and parameters, we set up a system in terms of rather general functions for the production of direct work and recruitment. We then solve it to arrive at the optimal solution, either across all points in time (if we allow for both earning to give and hiring) or only asymptotically (if we don't.)</p><p>If you are familiar with what the terms \"isoelastic\" and \"constant elasticity of substitution\" mean, you might want to just<a href=\"https://philiptrammell.com/static/Labor__Capital__and_Patience_in_the_Optimal_Growth_of_Social_Movements.pdf\"> <u>read the document</u></a>.</p><h2><strong>Main results</strong></h2><p>If the social movement is \"patient\" (in that <i>\u03b4</i> &lt; <i>r</i> \u2212 <i>\u03b3\u03b7</i>), then under some reasonable assumptions about diminishing returns to movement building (see <a href=\"https://forum.effectivealtruism.org/posts/FXPaccMDPaEZNyyre/a-model-of-patient-spending-and-movement-building?commentId=e2pPbGihSLGd2EwWB\">this comment</a>), our model finds that total labor (i.e., total movement size) approaches a constant value. The fractions of labor that are allocated to direct work and movement building also approach a constant.</p><p>Spending on movement building and on direct work both grow exponentially, but at different rates (except under a knife-edge condition). Thus, eventually, close to all of the movement's spending will be directed to either movement building or direct work. This is because only one of the two can be the most efficient one at allowing capital to substitute for labor. That is, the movement eventually reaches either a Ponzi-esque state\u2014where labor is much more scarce than capital and the best way to increase the absolute number of movement participants doing direct work is to use asymptotically all of its spending on movement-building\u2014or a single-minded state where asymptotically all of its spending goes towards direct work.</p><p>We also make a tentative argument that the EA movement may not want to earn to give in the limit. The argument begins with the observation that, if both earning to give and hiring external labor are allowed, patient social movements generally want\u2014in the long run\u2014to hire external labor and have their participants do either direct work or movement building, but not earning to give. This suggests that, in the worlds where earning to give is feasible but hiring is not (at least for some tasks), patient movements will in the limit not want to engage in any earning to give.</p><h2><strong>Applications to Effective Altruism and Future Work</strong></h2><p>Though the results of the model are fairly straightforward, there are complications to consider when trying to apply them to real-world EA movement strategy. For instance, we are modelling movement participants as homogeneous. In reality, even though the model concludes that earning to give doesn't make sense in general in the limit, it might make sense for participants whose capacity to earn is far greater than their capacity to engage in direct work or movement-building. Also, note that any conclusion about the undesirability of earning to give is only asymptotic: we do not dispute that earning to give can be worthwhile early in a patient movement's timeline, if it does not yet possess enough capital.</p><p>In short, we think this model supports the intuitive idea that as capital accumulates, earning to give should eventually be phased out, though as noted we are uncertain about whether EA has reached that point already. Ultimately, our results depend on empirical parameters (value drift rate, the production function for movement building, salary rates, etc.), about which we are also uncertain. But uncertainty about parameter values can be<a href=\"https://forum.effectivealtruism.org/posts/eRQe4kkkH2pPzqvam/more-empirical-data-on-value-drift\"> <u>reduced</u></a>, and this should eventually allow our model to produce more concrete recommendations.</p><p>Another neglected complication is that our model doesn't address movement strategy when the movement can non-negligibly lower <i>\u03b4</i>, e.g. by mitigating existential risks in the near term.&nbsp; It also fails to model the impact of doing research that makes future direct work and/or movement-building more efficient. We believe that extending the model to account for these complexities would be a valuable subject for future work.</p><p>Despite these caveats, the model has produced at least one important update for us. As the stock of EA capital has grown more quickly than the stock of EA labor, it has been widely claimed that earning to give is less valuable, relative to direct work, than it used to be. On a March 2020 episode of the 80,000 Hours podcast, Phil had argued that this claim was mistaken, on the grounds that the EA \"capital to labor ratio\" should simply be expected to fluctuate over time, suggesting that we had no reason to expect a long-run trend in either direction. Earning to give is thus still highly valuable, he argued, in light of the opportunity to invest for a time in which EA projects are again more capital-constrained. The results of our model suggest to us that this particular argument for earning to give was incorrect. It is at least plausible that, relative to direct work, earning to give has indeed grown less valuable, and\u2014temporary fluctuations notwithstanding\u2014will continue to do so.</p>", "user": {"username": "NunoSempere"}}, {"_id": "PwH5hKfAdwYGE8Jdh", "title": "Disagreeables and Assessors: Two Intellectual Archetypes", "postedAt": "2021-11-05T09:01:58.207Z", "htmlBody": "<p><i>Epistemic status: uncertain. I removed most weasel words for clarity, but that doesn't mean I'm very confident. Don\u2019t take this too literally. I'd love to see a cluster analysis to see if there's actually a trend, this is a rough guess.[1] I'm interested in feedback on this; if it matches the intuitions of others reading this, and if there are important aspects I've missed or really messed up.</i></p><p>I separate a lot of interesting intellectuals into <i>disagreeables </i>and <i>assessors</i>.[2]</p><p>Disagreeables are highly disagreeable. They're quick to call out bullshit and are excellent at coming up with innovative ideas. Unfortunately, they produce a whole lot of false positives; they're pretty overconfident and wrong a great deal of the time. They're \"idea detectors\" with the sensitivity turned up to 11.</p><p>Disagreeables often either work alone or on top of a structure that believes (almost) everything they say.</p><p>Assessors are well-calibrated. If/when they participate in forecasting tournaments, they do really well. Assessors don't try to come up with many brilliant new ideas and don't spend particularly much effort questioning the most deeply held group assumptions, but they're awfully good at <i>not being wrong</i>. When they say X is true, X is indeed very likely to be true.</p><p>Both disagreeables and assessors are frustrated by mainstream epistemics, but for different reasons.</p><p>Disagreeables tend to make dramatic statements like,</p><ul><li>\u201cThere\u2019s a big conspiracy that everyone is in on\u201d</li><li>\u201cEveryone is just signaling all the time, they\u2019re not even trying to be right\u201d</li><li>\u201cThis organization is a huge Ponzi scheme\u201d</li></ul><p>Assessors would make more calm clarifications like,</p><ul><li>\u201cYea, disagreeable person said X is a huge Ponzi scheme. There\u2019s some truth there, but it\u2019s a big oversimplification\u201d</li><li>\u201cI\u2019m quite sure that Y\u2019s paper is unlikely to replicate, after closely looking at the related papers\u201d</li></ul><p>If the emperor really has no clothes, disagreeables would be the first to (loudly) call that out. If the emperor has a large set of broadly reasonable policies, but a few that are subtly mistaken, the assessors would be a better fit to diligently identify and explain these.</p><p>Some disagreeables include Socrates, Nietzsche, Wittgenstein, Nassim Taleb, Robin Hanson, early Steve Jobs, other tech founders, angry public figures, professional gurus of all kinds.</p><p>Assessors include David Hume[3], Bertrand Russell, Robert Caro, Scott Alexander, Superforecasters, some of late Steve Jobs, good CEOs, many not-particularly-angry politicians (Eisenhower/Clinton/Obama come to mind).</p><p>To the disagreeables, assessors seem like boring <a href=\"https://www.lesswrong.com/posts/zcriHTKgKNehSSdyG/against-blankfaces\">blankfaces</a> and bureaucrats who maintain the suffocating status quo. To assessors, disagreeables often seem like reckless cult leaders who go around creating epistemic disarray.</p><p>Disagreeables value boldness and novelty; being the most interesting person in the room, making a big statement. Assessors value nuance, clarity, discernment. Getting things really right, even if the truth is tedious or boring.</p><p>I feel like Rationalists lean disagreeable, and Effective Altruists lean assessment.</p><p>The ideal is a combination of both. Have disagreeables come up with ideas and assessors assess them. But this is really hard to do!</p><p>Disagreeable normally don't exactly pronounce they are disagreeable; they often have compelling sounding arguments why <i>everyone else</i> is wrong (including all the assessors). Disagreeables often really truly and absolutely believe their inaccuracies. Meanwhile, accessors can be very soft-spoken and boring.[4] Great accessors who are unsure about X, even after a lot of research, can seem a whole lot like regular people who are unsure about X.</p><p>I wish we had a world where the people with great ideas are also all well-calibrated. But we don't live in that world. As such, I can rarely easily recommend interesting books, I need to condition my reviews.</p><blockquote><p>\"These books are very interesting, but I'd only recommend them if you're already familiar with the topic and surrounding debate, otherwise they might cause you more harm than good.\"</p></blockquote><p>Or with people,&nbsp;</p><blockquote><p>\"These people have the cool ideas, but you can't take them too seriously. You instead have to wait for these other people to review the ideas, but honestly, you\u2019ll likely be waiting a while.\"</p></blockquote><h2>Summary Table</h2><figure class=\"table\"><table><tbody><tr><td>&nbsp;</td><td>Disagreeables</td><td>Assessors</td></tr><tr><td>Goal</td><td>Innovation</td><td><i>Not being wrong</i></td></tr><tr><td>Traits</td><td>Disagreeable, innovative, interesting, individualistic, unreasonable, (occasionally) angry</td><td>Calibrated, nuanced, clear, strong discernment, reasonable, calm</td></tr><tr><td>Examples</td><td>Socrates, Nietzsche, Wittgenstein, &nbsp;Nassim Taleb, Robin Hanson, early Steve Jobs, other tech founders, angry public figures, professional gurus of all kinds</td><td>David Hume[1], Bertrand Russell, Robert Caro, Scott Alexander, Superforecasters, some of late Steve Jobs, good CEOs, many not-particularly-angry politicians (Eisenhower/Clinton/Obama)</td></tr><tr><td>Example Quotes</td><td><p>\u201cThere\u2019s a big conspiracy that everyone is in on\u201d</p><p>\u201cEveryone is just signaling all the time, they\u2019re not even trying to be right\u201d</p><p>\u201cThis organization is a huge Ponzi scheme\u201d</p></td><td><p>\u201cYes, disagreeable person said X is a huge Ponzi scheme. There\u2019s some truth there, but it\u2019s a big oversimplification\u201d</p><p>\u201cI\u2019m quite sure that Y\u2019s paper is unlikely to replicate, after closely looking at the related papers\u201d</p></td></tr><tr><td>Failure modes</td><td>Wild overconfidence, convincing the public that personal\"pet theories\" are either widely accepted or self-evident</td><td>Too quiet to draw attention, focuses on being accurate on things that don't even matter that much</td></tr><tr><td>Great for</td><td>Idea generation, calling out huge mistakes, big simplifications (when justified), tackling hard problems in areas with stigmas</td><td>Prioritization, filtering the ideas of disagreeables, catching many moderate-sized mistakes</td></tr></tbody></table></figure><hr><p>[1] This work has flavors of personality profiling tools like the Enneagram and Myers-Briggs. If you hate those things, you should probably be suspicious of this.</p><p>[2] These aren't all the types, but they're the main ones I think of. Another interesting type is \"bulldogs\", who dogmatically champion one or two ideas over several decades. Arguably \"philosopher king/queen/ruler\" is a good archetype, though it overlaps heavily with disagreeables and assessors.&nbsp;</p><p>[3] I'm not really sure about Hume, this is just my impression of him from several summaries.</p><p>[4] See <a href=\"https://www.youtube.com/watch?v=6POQjSjIXWk\">this set of interviews</a> of superforecasters for an idea. I think these people are interesting, but I could easily imagine overlooking them if I just heard them speak for a short period.</p>", "user": {"username": "oagr"}}, {"_id": "LoSojczPKA5kMchFA", "title": "CEA grew a lot in the past year", "postedAt": "2021-11-05T09:01:50.801Z", "htmlBody": "<p><em>For CEA's Q3 update, we're sharing multiple posts on different aspects of our work.</em></p>\n<p>Over the past year, we\u2019ve doubled down on the <a href=\"https://www.centreforeffectivealtruism.org/strategy/\">strategy we set out last year</a>. The key metrics we were targeting increased significantly (often more than doubling), and we made many strong hires to <a href=\"https://forum.effectivealtruism.org/posts/KyesB9Ee6TxbzkmFE/cea-s-headcount-nearly-doubled-in-2021-and-other-updates\">nearly double our headcount</a>.</p>\n<p>So, unless you\u2019ve been paying a lot of attention, CEA is probably somewhat different from what you think.<sup class=\"footnote-ref\"><a href=\"#fn-JCrszCLd7FeXK4jbq-1\" id=\"fnref-JCrszCLd7FeXK4jbq-1\">[1]</a></sup></p>\n<h2>Our strategy</h2>\n<p>We think that humanity will have a better chance of surviving this century, and sharply reducing present suffering, if there are many more highly-engaged EAs (\u201cHEAs\u201d). By this, we mean people who are motivated in part by an impartial care for others<sup class=\"footnote-ref\"><a href=\"#fn-JCrszCLd7FeXK4jbq-2\" id=\"fnref-JCrszCLd7FeXK4jbq-2\">[2]</a></sup>, who are thinking very carefully about how they can best help others, and who are taking some significant actions to help (most likely through their careers).<sup class=\"footnote-ref\"><a href=\"#fn-JCrszCLd7FeXK4jbq-3\" id=\"fnref-JCrszCLd7FeXK4jbq-3\">[3]</a></sup></p>\n<p>In the recent past, people we\u2019d consider \u201chighly engaged\u201d have done a lot to improve human lives, reduce the suffering of animals, develop our understanding of risks from emerging technologies, and build up the effective altruism community.</p>\n<p>To increase the number of HEAs working on important problems, we are nurturing discussion spaces: places where people can come together to discuss how to effectively help others, and where they can motivate, support, and coordinate with each other.</p>\n<p>In particular, we do this via university groups and conferences, both of which have a strong track record of getting people deeply interested in EA ideas, and then helping them find ways to pursue impactful work (as evidenced, for instance, by <a href=\"https://forum.effectivealtruism.org/posts/NP5B6yNMoZiZbmEQ8/open-phil-ea-lt-survey-2020-introduction-and-summary-of\">OpenPhil\u2019s recent survey</a>).</p>\n<h2>Recent progress</h2>\n<p>Some recent progress:</p>\n<ul>\n<li>For front-facing programs, the key metrics we focus on have (more than) doubled in the last 12-24 months. For instance:\n<ul>\n<li>The events team is on track to facilitate <a href=\"https://forum.effectivealtruism.org/posts/zP4jebzvdtBr6mxdz/cea-s-events-team-capacity-building-and-mistakes\">roughly twice as many new connections</a> as they did in 2019. We hope this means that many more people are getting mentorship, advice, and career opportunities.</li>\n<li>We had as many calls with group leaders in the last three months as we did in the whole of last year.</li>\n<li>More generally, it seems that <a href=\"https://forum.effectivealtruism.org/posts/GZn2xGR37or7N9zLm/many-groups-metrics-have-grown-by-100-to-400-in-the-past\">the activities of EA groups are growing rapidly</a> (maybe as much as 400% in some areas), and our support (via retreats, funding, etc.) is contributing to this.</li>\n<li>The number of hours people spent logged in to the EA Forum <a href=\"https://forum.effectivealtruism.org/posts/orcHMGjvzEKC59cGy/ea-forum-engagement-doubled-in-the-last-year\">doubled in the last year</a>. Many more people are regularly engaging with some of the community\u2019s best new ideas and resources.</li>\n</ul>\n</li>\n<li>We introduced and grew some new products and programs to complement our previous products:\n<ul>\n<li><a href=\"https://www.effectivealtruism.org/virtual-programs/\">Virtual Programs</a>, which have helped over 1000 people learn more about EA ideas in the past year. This has helped to seed new groups, get people more involved in EA, and ensure that high-fidelity versions of EA ideas are shared worldwide.\n<ul>\n<li>The latest <a href=\"https://forum.effectivealtruism.org/posts/cN9Zu7dowefAfmNnH/the-effective-altruism-handbook\">EA Handbook</a>, a set of readings based on the curriculum for our introductory virtual program, which has helped hundreds of additional readers work through similar content at their own pace.</li>\n</ul>\n</li>\n<li><a href=\"https://forum.effectivealtruism.org/groupsAndEvents\">A new groups/events platform</a>, which we hope will make it much easier for people to transition from online engagement to in-person engagement.</li>\n</ul>\n</li>\n</ul>\n<p>We think this type of progress is critical, because it means that more people are being exposed to and then engaging deeply with the ideas of effective altruism. We are in the process of assessing how well this progress has translated into more people taking action to help others in the last year, but given previous data, we expect to see a strong connection between these figures and the number of people who proceed to work on important problems.</p>\n<p>As for CEA\u2019s internal progress:</p>\n<ul>\n<li>Our team size nearly doubled, and we were especially pleased with <a href=\"https://forum.effectivealtruism.org/posts/KyesB9Ee6TxbzkmFE/cea-s-headcount-nearly-doubled-in-2021-and-other-updates#Hires\">the hires we made this year</a>.</li>\n<li>Less specifically, when we held our team retreat in September, I felt that:\n<ul>\n<li>There was a lot more clarity about what we\u2019re doing, and how everyone\u2019s work fits together (in the past, CEA sometimes struggled with a <a href=\"https://www.centreforeffectivealtruism.org/our-mistakes#running-too-many-projects-2016---2020\">lack of strategic clarity</a>).</li>\n<li>I\u2019m more excited about the current team than any previous CEA team I\u2019ve been a part of, due to a combination of the people and the culture. (Though I\u2019m also excited to see if we can make further improvements here.)</li>\n</ul>\n</li>\n</ul>\n<h2>Mistakes and reflections</h2>\n<p>I think that the key specific mistakes we made during this period were:</p>\n<ul>\n<li>The Meta Coordination Forum (a retreat for leaders in the EA meta space) was less valuable than it could have been due to a variety of mistakes we made. We plan to make major changes to address these issues. (More details in the <a href=\"https://forum.effectivealtruism.org/posts/zP4jebzvdtBr6mxdz/cea-s-events-team-capacity-building-and-mistakes#Meta_Coordination_Forum\">events post</a>.)</li>\n<li>For at least one hiring round (but not all rounds), I think we should have communicated more promptly with applicants and given them more detailed feedback. Assistants are now supporting hiring managers with emails, and we have updated towards giving more substantive feedback for applicants that make it far in our process.</li>\n</ul>\n<p>I also plan to spend part of the next few months reflecting on questions like:</p>\n<ul>\n<li>Should we be more ambitious, and aim to move more quickly than we currently are?</li>\n<li>What can we do to make sure we\u2019re not displacing even better community building efforts? And if others do begin to offer similar services, what are the best ways to collaborate effectively with them?</li>\n</ul>\n<p><strong>If you are interested in helping us, <a href=\"mailto:max@centreforeffectivealtruism.org\">let me know</a>:</strong> finding the right people to hire will help us move forward on many of these improvements, and we\u2019re always keen to diversify our funding base.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-JCrszCLd7FeXK4jbq-1\" class=\"footnote-item\"><p>This probably applies to most organizations you\u2019re not tracking closely: but I think the scale of change is maybe greater with CEA. <a href=\"#fnref-JCrszCLd7FeXK4jbq-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-JCrszCLd7FeXK4jbq-2\" class=\"footnote-item\"><p>Without regard to factors like someone\u2019s nationality, birthdate or species, except insofar as those things might actually be morally relevant. <a href=\"#fnref-JCrszCLd7FeXK4jbq-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-JCrszCLd7FeXK4jbq-3\" class=\"footnote-item\"><p>For each of these attributes, we set quite a high bar. And when we evaluate whether we\u2019d think of someone as \u201chighly engaged\u201d, we either interview them or look for other strong evidence (such as their having been hired by an organization with high standards and a strong connection to the EA movement). <a href=\"#fnref-JCrszCLd7FeXK4jbq-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Maxdalton"}}, {"_id": "fcyFwik4XRemw5YLL", "title": "Is there a need for a curated list of resources on basic biology/immunology/epidemiology for those wanting to work GCBRs? ", "postedAt": "2021-11-05T02:44:36.136Z", "htmlBody": "<p>From reading Dr Gregory Lewis's <a href=\"https://docs.google.com/document/d/14xGN4yv_hvk6lAH-EWE3svyYEKcqiJiH03Bipmyj-gY/edit#heading=h.yg9ajofvlo9p\">GCBR reading list</a> (see the section on 'Technical knowledge/Basic sceince') it looks as if it might be useful for there to be a curated list of resources (or course) on basic cell and molecular biology, immunology and epidemiology for people pursuing a career in GCBRs.&nbsp;</p><p>So, my question has two parts:&nbsp;</p><p>1) Is there a significant number of people from non-biology backgrounds pursuing a career in GCBRs that could benefit from a curated list of resources?&nbsp;</p><p>2) And if there is, is there much of a need for a curated list or is it easy enough for people to pick up the basics in those areas through Wikipedia and the like?</p>", "user": {"username": "AndyMorgan"}}, {"_id": "CKB5rSbwkseJvDErs", "title": "EA Organization Updates: October 2021", "postedAt": "2021-11-04T22:24:50.637Z", "htmlBody": "<p>These monthly posts originated as the \"Updates\" section of the EA Newsletter. Organizations submit their own updates, which we edit for clarity.</p><p>You can see previous updates in our<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives/\"> <u>repository of past newsletters</u></a>.</p><h2><strong>80,000 Hours</strong></h2><p>This month, Rob Wiblin spoke to <a href=\"https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/\"><u>Carl Shulman on the common-sense case for existential risk work and its practical implications</u></a>. He also spoke with <a href=\"https://80000hours.org/podcast/episodes/varsha-venugopal-vaccinations-children-india/\"><u>Varsha Venugopal on using gossip to help vaccinate every child in India</u></a>.</p><h2><strong>Anima International</strong></h2><p>Anima International published the results of a new survey on Russians\u2019 attitudes towards plant-based alternatives that, among other results, showed a 3-fold increase in consumption in the last 18 months and found that the label \u201c100% plant-based product\u201d is 11 times more attractive compared to \u201c100% vegan product\u201d. You can read more about the study <a href=\"https://animainternational.org/blog/Good-news-from-Russia\"><u>here</u></a> or see <a href=\"https://eatingbetter.ru/blog/new-survey-2021\"><u>the original in Russian</u></a>.<br><br>Otwarte Klatki in Poland <a href=\"https://www.youtube.com/watch?v=SRHZXEYCZyw\"><u>released the results of an investigation on mink farms</u></a>, including interviews with breeders and farm workers, who discuss the presence of disease on farms. They also worked with the Auchan supermarket chain, which signed an agreement to improve the welfare of chickens raised to supply meat for its private-label products; this is the most important broiler chicken commitment they\u2019ve achieved so far.</p><h2><strong>Animal Advocacy Careers</strong></h2><p>Animal Advocacy Careers has launched applications for their free, introductory <a href=\"https://www.animaladvocacycareers.org/course\"><u>online course</u></a>.</p><p>AAC ran a <a href=\"https://forum.effectivealtruism.org/posts/EGoLAH7xaMKt9QHyA/evidence-from-two-studies-of-ea-careers-advice-interventions\"><u>study</u></a> of this online course last year, and the results suggested that participants were more likely than a control group to make career plan changes and find roles that seemed promising for helping animals.</p><p>The course walks you through some of the key considerations and research involved in working out which are the most cost-effective strategies for you to help animals. It will also help you identify specific opportunities and plan your next steps. The course takes 1-2 hours per week, spread out over a couple of months.&nbsp;</p><p>If this sounds helpful to you, <a href=\"https://www.animaladvocacycareers.org/course\"><u>see details on their website</u></a> and apply via the link there. <strong>Applications are due by 10 November.</strong></p><h2><strong>Animal Ethics</strong></h2><p>Animal Ethics has been supporting the postdoc research of Jara<strong> </strong>Guti\u00e9rrez and others over the last few years.<strong> </strong>Jara and her professor Javier de Miguel recently published an article in the <i>European Journal of Ecology</i> focusing on the effects of wildfires on animals and ways to reduce such harms. The full paper can be openly accessed at<a href=\"https://journals.ku.edu/EuroJEcol/article/view/14643/13965\"> <u>Fires in nature: a review of the challenges for wild animals</u></a>. The<a href=\"https://www.animal-ethics.org/a-new-article-fires-in-nature-a-review-of-the-challenges-for-wild-animals/\"> <u>abstract</u></a> of the paper is available on the Animal Ethics website.</p><p>Animal Ethics also published<a href=\"https://www.animal-ethics.org/the-development-of-sentience-in-juvenile-animals/\"> <u>The development of sentience in juvenile animals</u></a>, which includes two short case studies of the type necessary to eventually settle the question about which animals are sentient and at what stage in development an animal develops sentience.</p><h2><strong>Centre for Effective Altruism</strong></h2><p>CEA released a series of EA Forum posts summarizing their progress over the last quarter. These include updates from the <a href=\"https://forum.effectivealtruism.org/posts/zP4jebzvdtBr6mxdz/cea-s-events-team-capacity-building-and-mistakes\"><u>Events team</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/GZn2xGR37or7N9zLm/many-groups-metrics-have-grown-by-100-to-400-in-the-past\"><u>Groups team</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/orcHMGjvzEKC59cGy/ea-forum-engagement-doubled-in-the-last-year\"><u>Online team</u></a>, and <a href=\"https://forum.effectivealtruism.org/posts/KyesB9Ee6TxbzkmFE/cea-s-headcount-nearly-doubled-in-2021-and-other-updates\"><u>Executive Director</u></a>.</p><h2><strong>Centre for the Governance of AI (GovAI)</strong></h2><p>GovAI has now officially <a href=\"https://forum.effectivealtruism.org/posts/5vGsFzWvh8Snet6sK/the-centre-for-the-governance-of-ai-has-relaunched\"><u>relaunched as a nonprofit</u></a>. Their mission remains the same: to build a global research community, dedicated to helping humanity navigate the transition to a world with advanced AI. However, owing to their new structure, they will also be exploring new ways to expand our impact.&nbsp;</p><p>They are hiring <a href=\"https://www.governance.ai/post/research-fellows\"><u>Research Fellows</u></a> and a <a href=\"https://www.governance.ai/post/chief-of-staff\"><u>Chief of Staff</u></a>, and taking applications for their 3-month <a href=\"https://www.governance.ai/opportunities/fellowships\"><u>Summer Fellowship Program</u></a> (apply before January 1st). More info at <a href=\"http://governance.ai/\"><u>governance.ai</u></a> and in the <a href=\"https://www.governance.ai/post/the-centre-for-the-governance-of-ai-has-relaunched\"><u>announcement post</u></a>.</p><h2><strong>Centre for the Study of Existential Risk (CSER)</strong></h2><p>CSER announced a <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe-9Iuzvc_IVpmhAMxJ7SjcxEhM3O6LdC-2uiQw-PlygB0dUA/viewform\"><u>call for lightning talks</u></a> as part of their 2022 conference <strong>(apply by 15 November)</strong>. Talks will be delivered remotely during special sessions on the afternoons (BST) of 19-20 April.</p><p>In a new <a href=\"https://www.cser.ac.uk/news/paper-why-and-how-governments-should-monitor-ai-de/\"><u>working paper</u></a>, Jess Whittlestone and Jack Clark present a proposal for improving the regulation of AI by investing in government initiatives to measure and monitor aspects of AI research, deployment, and impacts.</p><p>Se\u00e1n \u00d3 h\u00c9igeartaigh and Toby Ord published a <a href=\"https://www.cser.ac.uk/news/uk-government-set-out-its-national-ai-strategy/\"><u>comment</u></a> on the UK Government's National AI Strategy, welcoming its initiatives and especially the government's commitment to working with researchers to safely advance AI and mitigate catastrophic risks.</p><p>CSER published a <a href=\"https://www.cser.ac.uk/resources/video-who-creating-existential-risk-why-and-why-should-we-care/\"><u>video</u></a> of their recent panel on the causes of different existential hazards, the sources of existential risk in the past and present, what could change in the future, and what this means for the world.</p><h2><strong>Charity Entrepreneurship</strong></h2><p>Charity Entrepreneurship (CE) <a href=\"https://forum.effectivealtruism.org/posts/dAqrQQK42PpjHYoZs/presenting-2021-incubated-charities-charity-entrepreneurship\"><u>has announced</u></a> the launch of five new EA-aligned charities that, thanks to generous funders (EA Funds, Open Philanthropy, and the CE Seed Network), were provided with $537,000 in grants. The 2021 incubated charities are:&nbsp;</p><ul><li><a href=\"https://www.trainingforgood.com/\"><strong><u>Training For Good</u></strong></a> - delivering a range of training programs to fill important EA capability gaps and raise the utilization rate of EA talent (<a href=\"https://forum.effectivealtruism.org/posts/eKK2ryF9cDmvyAW33/introducing-training-for-good-tfg\"><u>EA forum announcement</u></a>)</li><li><a href=\"https://www.highimpactprofessionals.org/\"><strong><u>High Impact Professionals</u></strong></a> - enabling working professionals to have the biggest positive impact possible (<a href=\"https://forum.effectivealtruism.org/posts/M2KrWyG2X8iQaHGJv/introducing-high-impact-professionals\"><u>EA forum announcement</u></a>)</li><li><a href=\"https://www.shrimpwelfareproject.org/\"><strong><u>Shrimp Welfare Project</u></strong></a> - improving the lives of hundreds of millions of farmed shrimp in Southeast Asia</li><li><a href=\"https://www.healthierhens.com/\"><strong><u>Healthier Hens</u></strong></a> - improving the welfare of farmed egg-laying hens via a cost-effective intervention focused on feed fortification</li><li><a href=\"https://www.alcoholpolicysolutions.center/\"><strong><u>Center for Alcohol Policy Solutions</u></strong></a> - saving lives and promoting well-being through alcohol taxation</li></ul><p>CE has introduced <a href=\"https://charityentrepreneurship.aidaform.com/CEQuiz\"><u>a quiz</u></a> that can help you determine whether starting an effective nonprofit might be a good fit for you.</p><h2><strong>Effective Altruism Funds</strong></h2><p>The <a href=\"https://funds.effectivealtruism.org/funds/far-future\"><u>Long-Term Future Fund</u></a> and the <a href=\"https://funds.effectivealtruism.org/funds/ea-community\"><u>EA Infrastructure Fund</u></a> are once more calling for applications. Please submit your applications for projects that could address important bottlenecks for the EA community or improve the long-term future!&nbsp;</p><p>They\u2019re especially excited about ambitious ideas that could have a large-scale impact if successful, with specific plans for how you might achieve those successes. <a href=\"https://funds.effectivealtruism.org/apply-for-funding\"><u>Apply now</u></a>.</p><h2><strong>Faunalytics&nbsp;</strong></h2><p>In the latest edition of their <a href=\"https://youtube.com/playlist?list=PL24QgdIOV-lE8i3bjvGeAi4Vv6VciyPVa\"><u>Faunalytics Explains</u></a> video series, Faunalytics examines a report from the beef industry that can also serve as a tactical guide for advocates. They\u2019ve also updated their <a href=\"https://faunalytics.org/library/\"><u>research library</u></a> with several articles on topics related to rethinking food systems, including <a href=\"https://faunalytics.org/aquaculture-the-lesser-of-two-evils/\"><i><u>Aquaculture: The Lesser Of Two Evils?</u></i></a><i>, </i><a href=\"https://faunalytics.org/technical-outrage-innovating-to-reduce-animal-use/\"><i><u>Technical Outrage: Innovating To Reduce Animal Use</u></i></a><i>, </i>and <a href=\"https://faunalytics.org/towards-a-just-food-future-reimagining-dairy-systems/\"><i><u>Towards A Just Food Future: Reimagining Dairy Systems</u></i></a><i>. </i>Additionally, their new blog <a href=\"https://faunalytics.org/what-is-animal-law/\"><i><u>What Is Animal Law?</u></i></a> provides an overview of animal law and how advocates can (and do) employ it in creative ways.</p><h2><strong>Fish Welfare Initiative</strong></h2><p>In September, FWI secured its <a href=\"https://www.fishwelfareinitiative.org/post/_sage\"><u>first corporate commitment</u></a>. Although the scale of the commitment is relatively small (affecting ~500 fish/year), FWI believes it sets an important precedent; as far as they are aware, this is the first corporate commitment ever for farmed fish in India.</p><p>Additionally, they are hiring a <a href=\"http://fwi.fish/comms-lead\"><u>Communications Lead</u></a>. If you know any candidates in India who might be a good fit, please share!</p><h2><strong>Giving What We Can</strong></h2><p>Giving What We Can released a new <a href=\"https://www.givingwhatwecan.org/get-involved/share-our-ideas/guide-to-talking-about-effective-altruism/\"><u>guide to talking about effective altruism, effective giving, and Giving What We Can</u></a> and <a href=\"https://www.givingwhatwecan.org/post/2021/10/podcast-geetanjali-basarkod/\"><u>recorded an interview</u></a> with one of the co-authors, Geetanjali Basarkod, discussing key concepts and tips.</p><p>They also announced their <a href=\"https://givingwhatwecan.org/pledge-drive\"><u>Pledge Drive</u></a> and <a href=\"https://www.givingwhatwecan.org/post/2021/10/effective-giving-advocacy-challenge-2021/\"><u>Effective Giving Advocacy Challenge</u></a> for 2021 \u2014 by participating, you can help to promote effective giving.</p><p>Giving Season is fast approaching, and GWWC would be very happy to help with events related to effective giving (see <a href=\"https://www.givingwhatwecan.org/events/guides/\"><u>some event ideas</u></a>) in collaboration with local, university, industry and workplace groups. Please <a href=\"http://givingwhatwecan.org/contact\"><u>contact them</u></a> if you would like help organising an event for the 2021 giving season, or <a href=\"https://calendly.com/luke-gwwc/talks\"><u>pick a time</u></a> if you\u2019d like them to give a talk online or help to facilitate a <a href=\"https://www.givingwhatwecan.org/events/guides/giving-games/\"><u>giving game</u></a>.</p><h2><strong>Global Catastrophic Risk Institute</strong></h2><p>GCRI Executive Director Seth Baum contributed to a new paper on<a href=\"http://gcrinstitute.org/artificial-intelligence-systemic-risks-and-sustainability/\"> <u>\u201cArtificial Intelligence, Systemic Risks, and Sustainability</u></a>\u201d. AI is increasingly used in agriculture and forestry practices processing large datasets from environmental sensors. However, this creates major risks from cyberattacks, large-scale mis-optimization, and extreme cascading failure modes.&nbsp;</p><p>The paper was led by Victor Galaz, the Deputy Director of the<a href=\"https://www.stockholmresilience.org/\"> <u>Stockholm Resilience Centre</u></a>, in conjunction with the<a href=\"https://risk.princeton.edu/\"> <u>Global Systemic Risk</u></a> group at Princeton.</p><h2><strong>GiveWell&nbsp;</strong></h2><p>GiveWell's number-one priority is increasing the number of opportunities to which it can direct funding. It's seeking Senior Researchers to join its team and fill the most important role needed to achieve this goal. It hopes you'll consider applying or sharing the listing with your network.</p><p>Senior Researchers are the intellectual leaders who guide GiveWell's exploration of how it can do the most good. Senior Researchers will be responsible for finding, researching, and ultimately recommending high-impact giving opportunities. As part of a small team, they will be significant contributors to decisions about how hundreds of millions of dollars will be spent with the goal of saving and improving the lives of people living in the lowest-income communities in the world. Remote work is possible. More information is available <a href=\"https://www.givewell.org/about/jobs/senior-researcher\"><u>here</u></a>.&nbsp;</p><p>GiveWell also shared its <a href=\"https://blog.givewell.org/2021/10/08/initial-thoughts-on-malaria-vaccine-approval/\"><u>initial thoughts on the malaria vaccine</u></a> that was recently recommended by the World Health Organization.</p><h2><strong>Happier Lives Institute</strong></h2><p>The Happier Lives Institute (HLI) published <a href=\"https://www.happierlivesinstitute.org/overview.html\"><u>new meta-analyses comparing</u></a> the cost-effectiveness of <a href=\"https://happierlivesinstitute.us19.list-manage.com/track/click?u=e759f3a724b8709250fb153c2&amp;id=37bed2357e&amp;e=e4235c884d\"><u>cash transfers</u></a> and <a href=\"https://happierlivesinstitute.us19.list-manage.com/track/click?u=e759f3a724b8709250fb153c2&amp;id=c76c51180f&amp;e=e4235c884d\"><u>psychotherapy</u></a> in low-income countries in terms of subjective well-being. Their key finding is that <a href=\"https://strongminds.org/\"><u>StrongMinds</u></a> is 12 times (95% CI: 4, 24) more cost-effective than <a href=\"https://www.givedirectly.org/\"><u>GiveDirectly</u></a> in terms of subjective well-being. This puts it roughly on par with the top deworming charities recommended by GiveWell.</p><p>This is the first in-depth charity evaluation to compare poverty alleviation and mental health interventions using the same units (subjective well-being) rather than income and DALYs. The analysis is built upon a much broader evidence base rather than evaluating individual charities in isolation. HLI also used <a href=\"https://happierlivesinstitute.us19.list-manage.com/track/click?u=e759f3a724b8709250fb153c2&amp;id=9b62a6af92&amp;e=e4235c884d\"><u>Monte Carlo</u></a> simulations, rather than point estimates, to account for their uncertainty in their final estimates.&nbsp;</p><p>HLI Director Michael Plant discussed these results, HLI\u2019s broader approach, and a number of other topics, such as EA\u2019s cause prioritisation methodology, on Spencer Greenberg\u2019s <a href=\"https://clearerthinkingpodcast.com/?ep=076\"><u>Clearer Thinking podcast</u></a>.</p><ul><li><a href=\"https://happierlivesinstitute.us19.list-manage.com/track/click?u=e759f3a724b8709250fb153c2&amp;id=8ce1ccc6ec&amp;e=e4235c884d\"><u>Read the full report</u></a></li><li><a href=\"https://happierlivesinstitute.us19.list-manage.com/track/click?u=e759f3a724b8709250fb153c2&amp;id=8e45a2e35b&amp;e=e4235c884d\"><u>Discuss on the EA Forum</u></a></li><li><a href=\"https://happierlivesinstitute.us19.list-manage.com/track/click?u=e759f3a724b8709250fb153c2&amp;id=fd9505d571&amp;e=e4235c884d\"><u>Listen to the podcast</u></a></li></ul><h2><strong>One for the World</strong></h2><p>One for the World has conducted corporate talks about giving at Microsoft, Blackrock, and Mott MacDonald this month, raising around $6000 so far. They are also launching their Giving Tuesday campaigns in the next month.</p><h2><strong>Open Philanthropy</strong></h2><p>Open Philanthropy announced grants including<a href=\"https://www.openphilanthropy.org/focus/us-policy/macroeconomic-policy/dezernat-zukunft-general-support-and-regranting\"> <u>$4M</u></a> to Dezernat Zukunft to support work on monetary and fiscal policy in Europe,<a href=\"https://www.openphilanthropy.org/focus/other-areas/global-priorities-institute-general-support-2021\"> <u>$3.3M</u></a> to the Global Priorities Institute for general support,<a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity/center-security-and-emerging-technology-biosecurity-research\"> <u>$3.3M</u></a> to the Center for Security and Emerging Technology to support a biosecurity research project, and<a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/mercy-animals-corporate-campaigns-2021\"> <u>$3M</u></a> to Mercy for Animals to support corporate engagement on animal welfare. Open Philanthropy co-CEO Holden Karnofsky also<a href=\"https://www.nytimes.com/2021/10/05/opinion/ezra-klein-podcast-holden-karnofsky.html\"> spoke with the <i>New York Times\u2019</i> Ezra Klein</a> about how to do the most good, why we may be living in the most important century, the merits of worldview diversification, and more.&nbsp;</p><h2><strong>Rethink Priorities</strong></h2><p><a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a> (RP) welcomed several new employees.&nbsp;</p><p>New members of the Global Health and Development team:</p><ul><li><a href=\"https://www.linkedin.com/in/rubydickson/\"><u>Ruby Dickson</u></a>, Researcher</li><li><a href=\"https://www.linkedin.com/in/brucetsai/\"><u>Bruce Tsai</u></a>, Researcher</li><li><a href=\"https://www.linkedin.com/in/jenny-kudymowa/\"><u>Jenny Kudymowa</u></a>, Researcher</li><li><a href=\"https://www.linkedin.com/in/greer-gosnell/\"><u>Greer Gosnell</u></a>, Senior Environmental Economist</li></ul><p>New members of the Animal Welfare team:</p><ul><li><a href=\"https://www.linkedin.com/in/samaramendez/\"><u>Samara Mendez</u></a>, Senior Researcher</li><li><a href=\"https://www.linkedin.com/in/peacockjacob/\"><u>Jacob Peacock</u></a>, Senior Research Manager</li><li><a href=\"https://www.linkedin.com/in/william-mcauliffe-423b909a/\"><u>William McAuliffe</u></a>, Senior Research Manager</li></ul><p>RP also published several new <a href=\"https://forum.effectivealtruism.org/tag/rethink-priorities?sortedBy=new\"><u>EA Forum posts</u></a>. Principal Research Director <a href=\"https://www.linkedin.com/in/david-moss-789a9616/\"><u>David Moss</u></a>&nbsp; presented the results concerning <a href=\"https://forum.effectivealtruism.org/posts/zMKxgK4wbSywnkFrn/ea-survey-2020-geography\"><u>geography</u></a> from the EA Survey 2020; it examined where EAs are located, how the number of EAs in different areas is increasing or decreasing, and explored differences across regional, country, and city divides. Lastly, Researcher <a href=\"https://www.linkedin.com/in/michael-aird-3a3709133/\"><u>Michael Aird</u></a> gave a presentation on \u201cNuclear risk research, forecasting, and impact\u201d, of which there is a <a href=\"https://forum.effectivealtruism.org/posts/eFW2uYcNyCuuKrzJs/nuclear-risk-research-forecasting-and-impact-presentation\"><u>summary</u></a>, <a href=\"https://docs.google.com/presentation/d/1Kr8EQiEPncLgadrFLJJgIHOCebw86RDV9Lj1mSVVyxk/edit#slide=id.ge44d99aa4e_0_0\"><u>slides</u></a>, and a <a href=\"https://www.youtube.com/watch?v=z3BNer81wHM\"><u>video recording of the Q&amp;A session</u></a>.</p><h2><strong>WANBAM</strong></h2><p>WANBAM released a new blog post: \u201c<a href=\"https://www.wanbam.com/blog/wanbam-successes-and-challenges-from-our-first-two-years\"><u>WANBAM: Successes and challenges from our first two years</u></a>.\u201d The post summarizes their activities, initial results, and steps they took to mitigate risk. They additionally onboarded their next round of mentees for their fourth round.</p><h1><strong>Add your own update</strong></h1><p>If your organization isn't shown here, you can provide an update in a comment.</p><p>You can also <a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><u>email me</u></a> if you'd like to be one of the organizations I ask for updates each month. (I may not accept all such requests. Whether I include an org depends on its size, age, focus, track record, etc.)</p>", "user": {"username": "aarongertler"}}, {"_id": "cJzRuLQYatkL2LSXv", "title": "What is the maximum number of days of pig confinement that you would exchange in order to save one piglet from being crushed?", "postedAt": "2021-11-04T21:06:06.334Z", "htmlBody": "<p>In the EU the vast majority of female breeding pigs are confined in farrowing crates for around 35 days around the period that they give birth (usually from around 1 week beforehand to 4 weeks afterwards). This practice is clearly harmful to the mother pig's welfare since she can only lie down and stand up. Crating is claimed to reduce piglet mortality rates due to the mother pig being less likely to crush her piglets.&nbsp;</p><p>One alternative system to farrowing crates is temporary crating where the mother pig is only confined in the crate for around 1 week and another is zero-confinement pens where the sow is free to move within the pen (examples of these systems are shown at the bottom of this post). The research in this area is very mixed but generally a shorter period of confinement is associated with higher piglet mortality rates.&nbsp;</p><p>Piglets are most often crushed when they are 0-5 days old and it usually takes roughly 0-4 minutes for a piglet to suffocate when being crushed. With the \"End the Cage Age\" initiative there is now significant momentum towards banning farrowing crates. However, in order to make a judgement about whether zero-confinement pens or temporary crates are the optimal alternative for welfare, a judgement must be made on how to trade off sow confinement against piglets being crushed.&nbsp;</p><p>I want to test my own intuitions about this so what is the maximum number of days of mother pig confinement that would you be willing to trade in order to save one piglet from being crushed? I would find any answers helpful as data points (even if you have only spent a few seconds thinking about it)</p><p>Answer here or in the comments if you prefer: <a href=\"https://www.supersurvey.com/QQV89YYI0\">https://www.supersurvey.com/QQV89YYI0</a>&nbsp;</p><p>&nbsp;</p><p>Farrowing crate: <a href=\"https://www.ciwf.com/media/1167620/Sow-standing-in-farrowing-crate.jpg?center=0.46175989508628845,0.37496721744537354&amp;mode=crop&amp;width=730&amp;height=275&amp;rnd=130851689570000000\">https://www.ciwf.com/media/1167620/Sow-standing-in-farrowing-crate.jpg?center=0.46175989508628845,0.37496721744537354&amp;mode=crop&amp;width=730&amp;height=275&amp;rnd=130851689570000000</a></p><p>Temporary crate: <a href=\"https://www.freefarrowing.org/wp-content/uploads/2021/08/360-open-resized-1024x680.png\">https://www.freefarrowing.org/wp-content/uploads/2021/08/360-open-resized-1024x680.png</a>&nbsp;</p><p>Zero-confinement farrowing pen: <a href=\"https://www.pig-world.co.uk/wp-content/uploads/2017/07/PigSAFE-lactation.jpg\">https://www.pig-world.co.uk/wp-content/uploads/2017/07/PigSAFE-lactation.jpg</a>&nbsp;</p>", "user": {"username": "fullham"}}, {"_id": "QDWoBgDKpRrG8u76y", "title": "What high-level change would you make to EA strategy?", "postedAt": "2021-11-04T16:31:18.905Z", "htmlBody": "<p>Imagine you are a powerful person within EA. Maybe a large donor or someone with great connections. What high level change would you make in EA strategy?&nbsp;</p>", "user": {"username": "nathan"}}, {"_id": "e65BsYQLycLebxzpn", "title": "Global vaccine equity (covid-19) giving opportunities?", "postedAt": "2021-11-04T16:16:00.472Z", "htmlBody": "<p>Global covid-19 vaccine equity seems to be a frequent talking point in terms of highlighting disparities between the developed and developing world. (It was even mentioned in the last TLYCS newsletter.) It's also super easy to see how increasing global vaccinations could have huge ramifications - not only preventing lots of deaths and suffering in countries currently affected by covid-19 but also lessening the chance of a new variant emerging. But...who is working on this and does anyone have any giving recommendations in this vein? Is it worth donating to Gavi/COVAX, even though it seems like that initiative has not been very successful? What about orgs engaging in lobbying campaigns like PHR? Are the systemic issues (distribution challenges, etc.) at play here too much of a barrier or are there actions I can take as an individual? Educate me, please!!&nbsp;</p>", "user": {"username": "AHF"}}, {"_id": "xDFxwiNBB6hcgRLHd", "title": "Free workspace for EA researchers in London", "postedAt": "2021-11-04T15:34:39.415Z", "htmlBody": "<p>Hello all! I'm trying to ascertain demand for a new EA organisation in central London before I apply for funding.</p><p>I'm calling it EAL (Effective Altruism Laboratory) for now, until I think of a sufficiently punny acronym.</p><p>The hope is to create a workspace for independent researchers to enjoy the benefits of... not being independent. Think serendipitous conversation, discussion, collaboration, mentorship, a small community of smart, passionate, ambitious individuals exercising their intellectual autonomy and doing world class research - with free food.</p><p>I think membership could be automatically offered to London-based EA research grant recipients, with other researchers, professionals and academics in EA and closely related fields also able to apply for access.</p><p>If you are a London-based researcher and/or would like to be considered for membership if EAL happens, please help me by filling in a short form:</p><p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfhd3CHLkN6-MG8I4tRrGWYCjNnqI73guOyfgIIEFfl2qa7Nw/viewform?fbzx=4387633687430203649\">https://docs.google.com/forms/d/e/1FAIpQLSfhd3CHLkN6-MG8I4tRrGWYCjNnqI73guOyfgIIEFfl2qa7Nw/viewform?fbzx=4387633687430203649</a></p>", "user": {"username": "JessicaCooper"}}, {"_id": "LNeLEys5k7PaNnjNq", "title": "How can we make Our World in Data more useful to the EA community?", "postedAt": "2021-11-04T14:02:41.260Z", "htmlBody": "<p>I work at <a href=\"https://ourworldindata.org\">Our World in Data</a>, where we try to make research and data on the world's largest problems more accessible and understandable.</p>\n<p>I attended EA Global this past weekend, where I received very interesting input from many lovely people on potential improvements. But I thought it'd also be worth asking here to get wider feedback. I'm interested in all the following:</p>\n<ul>\n<li>\n<p>Low-hanging 'data fruits': simple datasets or charts that you know to be readily available somewhere and that would add significant value, but that aren't already listed <a href=\"https://ourworldindata.org/charts\">here</a>.</p>\n</li>\n<li>\n<p>High-hanging fruits: things we could add to the website in the medium term with a lot more work (new subjects, larger datasets, data that needs a lot of cleaning, etc.)</p>\n</li>\n<li>\n<p>Imaginary fruits: what you'd like to see on OWID in your wildest dreams (e.g. global population projections to the year 10,000 under various scenarios).</p>\n</li>\n</ul>\n<p>Thank you!</p>\n", "user": {"username": "EdMathieu"}}, {"_id": "orcHMGjvzEKC59cGy", "title": "EA Forum engagement doubled in the last year", "postedAt": "2021-11-04T10:44:33.247Z", "htmlBody": "<p><em>For CEA's Q3 update, we're sharing multiple posts on different aspects of our work.</em></p>\n<p>The <a href=\"https://forum.effectivealtruism.org/\">Effective Altruism Forum</a> aims to be the central place for collaborative discussion about how to do the most good.</p>\n<h2>Metric status</h2>\n<p>Hours of engagement are up by 100.2% year-over-year as of October 1. This meets our target of 100%, and means we\u2019ve doubled engagement hours this year.</p>\n<p>We weren't tracking this metric until 2020, but based on other data we have, we estimate that the previous year's growth rate was 90%, for two-year growth of ~380%.</p>\n<p><img src=\"https://i.ibb.co/XSyQQWV/image-6.png\" alt=\"\"></p>\n<p>This chart shows daily engagement hours for 2020 (yellow) and 2021 (blue). The red line shows our daily \"target\" (the numbers we'd need to hit to reach 100% YoY growth if our engagement grew linearly).</p>\n<p>In response to a comment, we'll also share our graph for monthly active users (people who viewed at least one page while logged in):</p>\n<p><img src=\"https://i.ibb.co/VmyLCzD/MAUs.png\" alt=\"\"></p>\n<p>We went from 727 MAUs in 10/2019 to 1140 in 10/2020 and 1954 in 10/2021.</p>\n<h2>Progress this quarter</h2>\n<p>Key progress:</p>\n<ul>\n<li>We onboarded 2 new engineers: <a href=\"https://www.centreforeffectivealtruism.org/team/sarah-cheng/\">Sarah Cheng</a> and <a href=\"https://www.centreforeffectivealtruism.org/team/jonathan-mustin/\">Jonathan Mustin</a>.</li>\n<li>We published the <a href=\"https://forum.effectivealtruism.org/community\">community and events page</a> and hired a contractor to make sure the list of events is up to date every week.</li>\n<li>We cross-posted hundreds of older articles from EA sources to the Forum, to make them taggable and searchable within the site. This includes blog posts from Open Philanthropy, papers from the Global Priorities Institute, and podcast transcripts from 80,000 Hours.</li>\n<li>We helped <a href=\"https://www.openphilanthropy.org/about/team/holden-karnofsky\">Holden Karnofsky</a> cross-post a series of successful posts from his Cold Takes blog, starting with <a href=\"https://forum.effectivealtruism.org/posts/TwQzyP3QgttmuTHym/all-possible-views-about-humanity-s-future-are-wild\">this one</a>.</li>\n<li>We launched a <a href=\"https://forum.effectivealtruism.org/posts/JFiHewypFB2niuy2G/ea-forum-creative-writing-contest-usd10-000-in-prizes-for\">creative writing contest</a>.</li>\n</ul>\n<p>Other progress:</p>\n<ul>\n<li>We implemented several new <a href=\"https://forum.effectivealtruism.org/posts/HKkPWQrw4E3dkfJqC/forum-update-new-features-october-2021\">user-visible features</a>:\n<ul>\n<li>Post analytics</li>\n<li>Forum Digest subscription reminder</li>\n<li>Sidebar banner</li>\n</ul>\n</li>\n<li>We got <a href=\"https://www.cypress.io/\">Cypress testing</a> working, with a handful of initial tests. This won\u2019t have a direct impact on users, but will make it easier for us to test our code. This was a large project \u2013 LessWrong <a href=\"https://github.com/LessWrong2/Lesswrong2/pull/1579\">started working on it</a> over 2 years ago.</li>\n<li>~150 people have ordered books from the <a href=\"https://airtable.com/shr6KgESHHZcc5rYH\">order link</a> we posted in the <a href=\"https://forum.effectivealtruism.org/posts/cN9Zu7dowefAfmNnH/the-effective-altruism-handbook\">EA Handbook</a>.</li>\n</ul>\n<h2>Reflections</h2>\n<p>I estimate that engineering output tripled in September (e.g. all three of the user-visible features I mentioned were released in September.) This is in line with the tripling of team size. Given that Sarah just started this month, I\u2019m pretty happy with that increase in productivity, though I also think that I could have made the onboarding process even smoother.</p>\n", "user": {"username": "Ben_West"}}, {"_id": "KyesB9Ee6TxbzkmFE", "title": "CEA\u2019s headcount nearly doubled in 2021 (and other updates)", "postedAt": "2021-11-04T07:51:19.615Z", "htmlBody": "<p><em>For CEA's Q3 update, we're sharing multiple posts on different aspects of our work.</em></p>\n<h2>People</h2>\n<p>At this time one year ago, we had 16 people on our core team. Accounting for confirmed arrivals and departures, we now have 29 people on our core team.</p>\n<p>Overall, we think we\u2019ve managed to grow the team rapidly while also raising our bar for new hires.</p>\n<h3>Role Changes</h3>\n<p><a href=\"https://www.centreforeffectivealtruism.org/team/nicole-ross\">Nicole Ross</a> is transitioning to the role of manager of the Community Health team. Katie Glass, who was previously the team\u2019s interim manager, is now focused on developing hiring systems and helping to build systems for tracking metrics.</p>\n<h3>Hires</h3>\n<p>In Q3, we made the following hires:</p>\n<ul>\n<li><a href=\"https://de.linkedin.com/in/jonathan-michel-9b2008151\">Jonathan Michel</a> (Operations Associate, incoming in December)</li>\n<li><a href=\"https://www.linkedin.com/in/lizka-vaintrob\">Lizka Vaintrob</a> (Events Generalist, started in September)</li>\n<li><a href=\"https://uk.linkedin.com/in/ollie-base-970922135?trk=people-guest_people_search-card\">Ollie Base</a> (Community Events Manager, incoming in November)</li>\n<li><a href=\"https://www.linkedin.com/in/jessica-mccurdy?trk=people-guest_people_search-card\">Jessica McCurdy</a> (Scalable Uni Support, part-time)</li>\n<li><a href=\"https://uk.linkedin.com/in/will-payne-9138481a3\">Will Payne</a> (Groups Associate: Campus Specialist Manager)</li>\n<li><a href=\"https://www.linkedin.com/in/kuhanj\">Kuhan Jeyapragasan</a> (Groups Associate: Campus Specialist Manager, part-time)</li>\n</ul>\n<p>We had been planning to hire about five people in Q3. We surpassed this target, but some of our new joiners had already been working with us in a contracting capacity, and are simply increasing their hours or becoming more integrated with the team.</p>\n<p>For at least one hiring round, I think we should have communicated more promptly with applicants and given them more detailed feedback. We\u2019ve increased tracking of applicant communications, and email support for hiring managers, to try to ensure prompt responses and substantive feedback for applicants that make it far in our process.</p>\n<h3>Hiring plans</h3>\n<p>We\u2019re currently hiring for an <a href=\"https://www.centreforeffectivealtruism.org/careers/executive-assistant\">Executive Assistant</a> and a <a href=\"https://www.centreforeffectivealtruism.org/careers/finance-associate\">Finance Associate</a>. We\u2019re also welcoming general expressions of interest, as well as <a href=\"https://www.centreforeffectivealtruism.org/careers/#expressions-of-interest\">expressions of interest</a> for a Full Stack Engineer.</p>\n<h3>Departures</h3>\n<p>Barry Grimes, Harri Besceli, and Aaron Gertler will transition out of CEA during Q4. Sky Mayhew is transitioning from being a member of the Community Health team to contracting with CEA. Louis Dixon also departed CEA at the end of Q3, replaced in his role as our finance lead by Litawn Gan (as mentioned in a previous update).</p>\n<h3>Morale</h3>\n<p>The average morale reported for Q3 was 7.17/10, compared with 7.03/10 in Q2.</p>\n<h3>Org chart</h3>\n<p><img src=\"https://i.ibb.co/5WxybZP/Org-Chart.png\" alt=\"\"></p>\n<h3>Team retreat</h3>\n<p>We ran our first team retreat since the pandemic in late August/September. 16 people came together in a venue near Oxford. The average likelihood to recommend the event was 9.07/10, and attendees thought the event was 7x more valuable than the counterfactual use of their time (geometric mean).  Broadly, we think that this helped a lot with onboarding new hires, and that people left the event feeling excited about the impact we can have together.</p>\n<h2>Operations</h2>\n<p>The Operations team aims to provide the financial, legal, administrative, grantmaking, logistical, and fundraising support that enables CEA, <a href=\"https://80000hours.org/\">80,000 Hours</a>, <a href=\"https://www.forethought.org/\">the Forethought Foundation</a>, <a href=\"https://funds.effectivealtruism.org/\">EA Funds</a>, <a href=\"https://governance.ai/\">the Centre for the Governance of AI</a> and <a href=\"https://givingwhatwecan.org/\">Giving What We Can</a> to run efficiently.</p>\n<ul>\n<li><strong>Operations Associate hiring:</strong> We hired <a href=\"https://www.linkedin.com/in/jonathan-michel-9b2008151/\">Jonathan Michel</a> to run operations in our Oxford office. Jonathan will start in December.</li>\n<li><strong>Office refurbishment:</strong> The office is now fully up and running. We\u2019re still making small improvements to lighting and catering, but users are happy.</li>\n<li><strong>Improvements to financial systems:</strong>\n<ul>\n<li>We adjusted our financial year in the USA so that it aligns with the UK entity, which will allow us to present consolidated accounts going forward.</li>\n<li>Audits were completed for both the UK and the US.</li>\n</ul>\n</li>\n<li><strong>Customer Relationship Management (CRM) software:</strong> We have completed the discovery phase with our Salesforce developer partner, and they have begun to build the CRM.\n<ul>\n<li>In Q4, our primary focus will be working with the groups team and group organisers to make sure that the CRM meets their needs; achieving this is fundamentally important for the success of the project.</li>\n</ul>\n</li>\n<li><strong>Feedback:</strong> We set up a form for continuously gathering feedback on our work (which is shared e.g. in our email signatures).</li>\n<li><strong>Cybersecurity:</strong> We stood up well to penetration testing performed by our consultants, and are making some further improvements based on their feedback.</li>\n<li><strong>Wiki/staff handbook:</strong>\n<ul>\n<li>We began to build onboarding wikis for the Forethought Foundation and Giving What We Can.</li>\n<li>We added additional guides to the staff handbook to cover private medical insurance, pensions, and visas.</li>\n</ul>\n</li>\n</ul>\n<p>The Ops team met for their first annual team retreat. The team left feeling more cohesive and well-coordinated on future plans.</p>\n<h2>Community Health</h2>\n<p>The Community Health team aims to address key issues related to the EA community that are not the focus of other projects and organizations. This includes scanning for issues that other groups aren\u2019t working on and developing plans for us, or partner organizations, to address them. In the past, this has caused us to work on fostering a healthy culture, improving demographic diversity, mitigating harm from conflict or interpersonal problems in the community, advising community members who are doing media interviews about EA, and supporting and reducing risks in early field-building.</p>\n<p>Given the low capacity on the team this quarter, we\u2019ve mostly prioritized hiring and EA brand work.</p>\n<h3>Staffing</h3>\n<p>Nicole Ross has taken over management of the Community Health team from Katie Glass, who is transitioning to work on some of CEA\u2019s internal systems.</p>\n<p>Julia Wise returned from parental leave in August. Sky Mayhew left CEA, after having handed off her previous responsibilities. We plan for her to act as a consultant for us in the future.</p>\n<p>We are open to hiring an excellent generalist who could work across a range of areas. So we plan to replace our current expressions of interest with a single general form.</p>\n<p>Catherine Low of the groups team is also providing part-time capacity on community health, supporting group organizers on difficult situations they encounter in their groups.</p>\n<p>We gave <a href=\"https://www.linkedin.com/in/isaac--dunn/\">Isaac Dunn</a> a career development contract to allow them to further develop their skills, while doing some projects for the Community Health team and helping other people within the community building sphere.</p>\n<h3>Reactive work</h3>\n<p>We handled:</p>\n<ul>\n<li>16+ inquiries or cases regarding media stories about EA</li>\n<li>7 concerns around interpersonal problems such as sexual harassment</li>\n<li>9 cases where we advised on situations in early field-building (geographical areas or academic/professional fields where EA is just getting established)</li>\n<li>6 \u201corganizational health\u201d cases where we advised organizations or projects on conflicts of interest, conflicts between staff, best practices in HR, etc.</li>\n<li>9 situations where we advised individuals on situations involving personal or mental health problems</li>\n<li>11 other situations where we advised groups, organizations, and individuals on situations like conflicts between staff or group members, online conflicts, and improving diversity, equity, and inclusion</li>\n</ul>\n<p>Examples of different types of cases from this quarter:</p>\n<ul>\n<li>We coordinated and coached spokespeople for a press article about EA and careers.</li>\n<li>We provided support and advice to two group organizers who were concerned about members struggling with personal/mental health problems.</li>\n</ul>\n<h3>Proactive projects</h3>\n<p>We\u2019ve been working with some key stakeholders and advisors to develop a plan for more accurately sharing information about EA and longtermism with the general public.</p>\n<p>We have written a research brief outlining key questions within DE&amp;I that we are interested in. Eventually, we would like to find someone to conduct projects in these areas. This project is currently on pause due to a lack of capacity.</p>\n", "user": {"username": "Maxdalton"}}, {"_id": "Xa3qB6xBrmnGDmQCW", "title": "Yale EA House Seeking Housemates", "postedAt": "2021-11-04T03:29:29.384Z", "htmlBody": "<p>A number of us (Thomas Woodside, Matt Burtell, Miriam Huerta, and Nat Irwin) have been living together in New Haven, Connecticut, for the past few months.&nbsp;</p><p>As a result of the upcoming December graduation of two of our group members, we have two open spaces in our 6-person house next semester (January-July, 2022). Since this is an abnormal time for students to move off campus, we thought we would open up the search to students and early-career professionals at other universities. We think this would be a great opportunity for people who are uncertain about their current living situations or are especially interested in helping us with Yale EA.</p><h2>Who might be a good fit?</h2><p><strong>Current students on leaves of absence</strong>: If you are a student on a leave of absence working remotely, this could be a great opportunity for you! It's a chance for you to meet Yale EA members and hang out with other students.</p><p><strong>Recently graduated students between jobs</strong>: If you've recently graduated, but don't have a job yet, this is a relatively low-cost place to live compared with other US cities (rent is $637.50/month).</p><p><strong>Early-career professionals with remote jobs</strong>: We have observed that especially at schools outside of Oxford/Cambridge/Bay Area, there is often not much connection between EA professionals and students. We think that these connections can really matter for increasing students' enthusiasm and potential for impact, and you could help us achieve that.</p><h2>Who might not be a good fit?</h2><p>We are clean, conscientious, and reliable. That being said, we are college students, and our social sphere consists of college students, and if you'd prefer a different social scene, you probably shouldn't live here. Lastly, New Haven is a small city, and there isn't all that much to do outside of Yale, so if you want a city that never sleeps, the house probably isn't for you.</p><h2>Why live here?</h2><p>Living in New Haven might let you do community-building at Yale. We think this is an especially promising opportunity, as Yale is one of the most developed university groups and we are currently growing rapidly. If you have skills or knowledge relevant to community building, like cause-area specific expertise, community building experience, or anything else that you think would be useful, we especially would love for you to live here. Though you would not be required to participate in community building, we would love for you to share your experiences and help high-potential Yale students maximize their impact.</p><p>&nbsp;</p><p><strong>You can find more about the house </strong><a href=\"https://docs.google.com/document/d/1q9iJPbFkuPu4kK6whVukU7yk890kAtN0Y3JiD2CIXCY/edit?usp=sharing\"><strong>here</strong></a><strong>, and register your interest </strong><a href=\"https://forms.gle/H5wET7ZZM1xLMaCv6\"><strong>here</strong></a>. We will reach out if we think you would be a good fit.</p>", "user": {"username": "ThomasWoodside"}}, {"_id": "Z2jPENrHpY9QSQBDQ", "title": "Proposal: alternative to traditional academic journals for EA-relevant research (multi-link post)", "postedAt": "2021-11-03T20:16:02.421Z", "htmlBody": "<p><em>Caveats:</em> This is mainly a linkpost; the linked posts make the arguments more carefully. Some content comes from my comments on other posts.</p>\n<p>I realize  I may need to make a stronger case for 'why we should care about academic research and bringing academic feedback and credibility to EA research'.  If this doesn't seem apparent, you might consider the below as \"to the extent that EA-aligned researchers are seeking this, here's a proposal for how to do it better\".</p>\n<p><strong>Update Feb/March 2022: LTFF funding received, progress being reported in the <a href=\"https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/\">collaborative gitbook space</a></strong> -- I will make an updated backlink post soon.</p>\n<h1>We can help 'slay the journals', make research better, and nudge academics towards considering EA-relevant issues</h1>\n<p><a href=\"https://forum.effectivealtruism.org/posts/gfDApmMiKhxFL28gR/help-please-integrating-ea-ideas-into-large-research\">Lauren's post</a> on bringing EA ideas into large research organizations, reminded me that EA organizations and researchers can \"make research better, and in doing so, bring academics into our fold\".  Our <a href=\"https://www.centreforeffectivealtruism.org/ceas-guiding-principles\">shared principles and values</a>,  and our lack of ties to traditional systems can help us break the collective-action problems.</p>\n<h1>EA researchers and orgs need an alternative to traditional journals</h1>\n<p>In the <a href=\"https://docs.google.com/document/d/1GFISlF5TieCuA6jDYkYlNWaEpuEYrr_zTmaVpTfBg4A/edit#heading=h.iqq0k5uqyg8x\">Slaying the journals</a> discussion I argue:</p>\n<blockquote>\n<p>Global priorities and EA research organizations are looking for \u2018feedback and quality control\u2019, dissemination, and external credibility.  We would gain substantial benefits from supporting, and working with [journal-independent  peer-evaluation systems], rather than (only) submitting our work to traditional journals. We should also put some direct value on results of open science and open access, and the strong impact we may have in supporting this.</p>\n</blockquote>\n<p>I am eager for us to take concrete steps towards an alternative to 'traditional academic journals' process. As I argue in <a href=\"https://bit.ly/unjournal\">the 'unjournal' link</a>, the traditional model</p>\n<ul>\n<li>lets publishers extract rents and makes research less accessible,</li>\n<li>inhibits innovation and open science practices (especially dynamic docs),</li>\n<li>(most substantially) leads to tremendous wasted effort and risk, as it encourages researchers to focus on gamesmanship and often requires us to submit papers to a long process sequence of journals with 0/1 outcomes.</li>\n</ul>\n<h1>\"Plan of action\", crucial steps</h1>\n<p>I set up a space where I propose a Plan of Action <a href=\"https://app.gitbook.com/o/-MfFk4CTSGwVOPkwnRgx/s/-MkORcaM5xGxmrnczq25/plan-of-action\">HERE</a> in the Gitbook format. I would appreciate your feedback and suggestions.</p>\n<p><strong>I think the crucial steps are</strong></p>\n<ol>\n<li>\n<p>Set up an \"experimental space\" e.g.,  <a href=\"https://prereview.org/\">on PREreview</a> allowing us to include additional, more quantitative metrics (they have offered this as a possibility), and to focus on content and approaches that are relevant to EA and global priorities.</p>\n</li>\n<li>\n<p><strong>Most crucially:  Get funding and support</strong>  and commitments (from GPI, RP, etc)</p>\n</li>\n</ol>\n<ul>\n<li>... for people to do reviewing, rating, and feedback activities in our space in PREreview</li>\n<li>...   for 'editorial' people to oversee which research projects are relevant and assign relevant reviewers</li>\n</ul>\n<ol start=\"3\">\n<li>Link arms with Cooper Smout and the <a href=\"https://freeourknowledge.org/\">\"Free our Knowledge\"</a> pledges and initiatives like <a href=\"https://github.com/FreeOurKnowledge/website/issues/40\">this one</a> as much as possible. Note that this is very close to Cooper's mission, and he has time funded/allotted for this.</li>\n</ol>\n<h1>Asides and caveats</h1>\n<h2>My 'rated list of tools and partners'</h2>\n<p>In <a href=\"https://airtable.com/shraTY0WcwsjJSANs\">this Airtable view</a> I give my rough opinion about the value of existing outlets including innovative OA journals, places to host preprints and research projects, and, most importantly IMO, journal-independent peer review and rating tools.</p>\n<h2>Do we need an actual OA journal?</h2>\n<p>I don't think setting up an OA journal with an impact factor is necessary. I think \"credible quantitative peer review\" is enough, and in fact the best mode. But I am also supportive of open-access journals with good feedback/rating models like <a href=\"https://scipost.org/about\">SciPost</a>. It might be nice to have an EA-relevant place like this. Cooper Smout is more enthusiastic about the idea of starting best-practice OA journals that 'give every acceptable paper a rating' ... see our discussion after my post <a href=\"https://onscienceandacademia.org/t/moving-science-beyond-closed-binary-static-journals-a-proposed-alternative-how-the-effective-altruist-and-nontraditional-nonprofit-sector-can-help-make-this-happen/1490\">here</a>.</p>\n<h2>I recognize that open-access/open science in some fields can raise X-risks</h2>\n<p>We give a rough outline of the arguments <a href=\"https://forum.effectivealtruism.org/posts/DcNB2Z2tKLe6migqk/why-scientific-research-is-less-effective-in-producing-value\">here</a>. But I think its pretty in most cases whether or not this is relevant.</p>\n<p>To be a bit glib... Microbiology of diseases: Yes.  General AI: Probably. Development Economics: No. Psychology: No.</p>\n<h2>This is a 'small but big' step</h2>\n<p>My proposal may not 'fix the biggest problems of research alignment and productivity' (see, e.g.,  discussions <a href=\"https://forum.effectivealtruism.org/posts/DcNB2Z2tKLe6migqk/why-scientific-research-is-less-effective-in-producing-value\">here</a> and <a href=\"https://forum.effectivealtruism.org/posts/Rjtezb4N7fyqGQrk6/prioritization-in-science-current-view\">here</a> nor make a tremendous contribution to humanity.</p>\n<p>But it would make research somewhat more efficient, transparent, and accessible.\nIt would make the researchers' careers less stressful and less random. They would appreciate us for that.</p>\n<p>And it would help EA-aligned researchers and organizations do better, more credible research.</p>\n", "user": {"username": "david_reinstein"}}, {"_id": "ut4fFk9zE6tPh4Hg7", "title": "If I have a strong preference for remote work, should I focus my career on AI or on blockchain?", "postedAt": "2021-11-04T07:05:32.306Z", "htmlBody": "<p>Hello Effective Altruists,</p><p>I am familiar with the ideas of Effective Altruism as I have read the <a href=\"https://80000hours.org/career-guide/\">80,000 Hours career guide.</a> I think it is a great guide and it definitely put a new perspective on the way I view my career.</p><p><strong>A bit of my background:&nbsp;</strong></p><p>I have a master's degree in computer science. I am currently working remotely as a machine learning engineer.</p><p><strong>Here is a list of the things that I am looking for in my career, ordered from most important to least important:</strong></p><ol><li>Remote work</li><li>High salary</li><li>Impact</li></ol><p>Maybe I'm not the paragon of Effective Altruism values, but if I'm being honest, I value remote work and high salary more than impact. Impact has the 3rd place, but it is still a factor.</p><p><strong>Now onto my question:</strong></p><p>A few years ago I read <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742\">Superintelligence</a> and got scared that AGI might make humanity go extinct. I then started focusing on machine learning and after graduating I ended up as a machine learning engineer, where I'm working currently.</p><p><strong>Recently, however, I began questioning whether what I was doing is the right thing to do impact-wise.</strong> I believe blockchain to be a great technology as well (even though we are in a bubble right now). Fundamentally, I think blockchain is going to bring \"power &nbsp;to the people\" and I think that's great. It's got it weaknesses now, sure, but over time I think they'll get ironed out.</p><p><strong>Here are my top three reasons why I think I should switch to blockchain:</strong></p><ol><li><strong>Given my strong remote work preferences, I don't think I will make any impact in anything AI safety related.</strong> I think that the main discoveries are being made in companies such as OpenAI and DeepMind and they all require going to the office. Since I don't want to go to the office (my remote work preference is higher than my impact preference), I don't think I will be a part of a team that reaches a fundamental breakthrough. With blockchain, on the other hand, most jobs are remote and I could therefore contribute more.</li><li><strong>I am not 100% convinced that AI safety is an existential risk.</strong> There are some indications toward this (such as <a href=\"https://www.youtube.com/watch?v=zkbPdEHEyEI\">this one</a>), but I think that it may very well be that worrying about AGI safety (as in it's an existential risk for <strong>all</strong> humans) is the same as worrying that aliens will come and destroy Earth or something similar. I am not denying the problems with current AI systems, but what I am saying is that I don't see a clear path to AGI and I think there's a lot of hand waving that goes on when talking about AGI safety at this point in time.</li><li>One could make the argument that I should do machine learning engineering jobs and wait for AI safety related jobs to become remote. I would then be working on making some AI system safe. Here's the problem with this perspective:<strong> I'm not sure when we will come to a point where there are remote AI safety jobs available.</strong> What if there's no fundamental breakthrough in AI for another 30-40 years and I keep working on some non-AI safety related remote machine learning jobs to \"keep my skills sharp in case they're needed\", only to find myself never using them on actual AI safety problems.</li></ol><p><strong>Fundamentally, the only reason I'm interested in AI is because of AGI safety. And right now I'm not sure that AGI safety is a real existential threat and even if it is, given my remote work preferences I will probably have low to no impact on AGI safety. Blockchain, on the other hand, is already changing and will most likely continue to change the way we use the internet and is much more remote friendly.</strong></p><p><strong>What are your 2 cents?</strong> I'd like to bounce off perspectives off of others to see if I'm missing anything in my train of thought.</p><p>P.S. I cross-posted this on LessWrong to get more perspectives.</p>", "user": {"username": "Malleable_shape"}}, {"_id": "4GvpaerCKLgXXdssu", "title": "Can we get #TeamHumans going", "postedAt": "2021-11-03T15:05:24.584Z", "htmlBody": "<p>Question to EA Funds, EA Giving Tuesday, anyone in content creation, marketing, etc.:</p><p>Can we have this for EA?</p><figure class=\"image\"><img src=\"https://assets01.teamassets.net/assets/images/teamseas-tm-logo.png\"><figcaption><a href=\"https://teamseas.org/\"><u>https://teamseas.org/</u></a></figcaption></figure><p>Mr. Beast: <a href=\"https://youtu.be/cV2gBU6hKfY\"><u>https://youtu.be/cV2gBU6hKfY</u></a>&nbsp;<br>Mark Rober: <a href=\"https://youtu.be/pXDx6DjNLDU\"><u>https://youtu.be/pXDx6DjNLDU</u></a>&nbsp;</p><p>Just coming out of EAG and its motto of \u201cbe ambitious\u201d, so could we pull this off? I am thinking of having a fundraiser by the whole YouTube community in the next one to two years? I think this should actually be achievable by quite the small team.</p><p>The hurdle to convince Mr.Beast to donate to GiveDirectly should essentially be 0, and likewise, Mark Rober could cover the Against Malaria Foundation or something even more engineering-focused. The impact would be more than the 30 million raised, this could really increase the amount of EA content out there (if this is good, I don\u2019t know, would probably contain some good and some bad content), as well as widespread media coverage and exposure to large scale philanthropists.</p><p>Am interested in ideas, connections to Mark and Mr. Beast, and potential downsides/counterarguments. If this seems worth doing, I might help coordinate this in the earliest stages.</p>", "user": {"username": "Heye Gro\u00df"}}, {"_id": "Saza3y7tEdgCZ47kK", "title": "Complexity Science, Economics and the Art of Zen.", "postedAt": "2021-11-04T07:05:40.983Z", "htmlBody": "<p><i>\u201cEconomics can be harder than rocket science: the Soviet Union was great at rocket science\u201d</i></p><p>EAG is a brilliant conference, and like many other <a href=\"https://en.wikipedia.org/wiki/Building_20\">examples</a> throughout history it shows the value of gathering a large amount of smart, driven people together from many fields together. You\u2019re never quite sure what the <a href=\"https://www.newyorker.com/magazine/2012/01/30/groupthink\">chaos will produce</a>, but it will always be interesting.</p><p>For me, one of the more fascinating talks at the conference happened off the agenda, when a group gathered together to discuss complexity science, and its implications for EA.&nbsp;</p><p>There was far too much discussed there for a detailed breakdown, and many are already incorporating complexity science and theories into their work, so this post may be of only partial value to many. However, the implications of complexity interacting with long termism in particular are serious, and if we are working on systems as complicated as humanity as a whole we ignore them at our peril.</p><p><a href=\"https://taylorpearson.me/complexity-science/\">Complexity science</a> covers many forms of analysis, broadly tied to a central thesis: \"The whole is the greater than the sum of its parts\". It applies to animals, mathematics, the movement of planets, the functioning of politics, and any other system where the sum of seemingly linear systems and logical correlations suddenly produce emergent behaviour, often in fascinating and counterintuitive ways.</p><p>However, I am an economist, and while I have no great insight into chaos theory mathematics we do think about these issues of complexity in our own way, which are sometimes very different to other fields.</p><p>There was a classic challenge to economics, to name something <a href=\"https://ifreetrade.org/?/article/a_punters_guide_to_a_true_but_non_obvious_proposition_in_economics\">true and non-trivial that the discipline has provided</a>. Samuelson\u2019s answer was Ricardo\u2019s theory of trade, which is deeply counterintuitive to many, especially at the time, and profoundly changed the world. It is also an example where letting go of control and planning, in favour of chaos, led to vast gains in human material prosperity.</p><p>The original argument is elegantly simple. The United Kingdom can produce wool, which it is well suited to, and grow wine, with low yields and poor taste due to our <a href=\"https://www.statista.com/statistics/610677/annual-raindays-uk/\">unique climate</a>. Meanwhile Portugal can make excellent wine, but is unsuited to sheep varieties with heavy wool.&nbsp;</p><p>From the point of view of the United Kingdom there are two ways to make wine. Firstly, you can divert labour and capital away from sheep farming at great cost and with limited results, or secondly you could load wool onto a ship, which returns a month later loaded with far more wine than you could ever produce from the same resources. It may as well be a factory for turning wool into wine for all the United Kingdom cares, and importantly this process works even if <i>Portugal is both better at making wine and wool.</i></p><p>Governments today still struggle with this, attempting to micromanage the chaos of trade by adjusting tariffs and setting complicated quotas on <a href=\"https://unstats.un.org/unsd/tradekb/Knowledgebase/Harmonized-Commodity-Description-and-Coding-Systems-HS\">thousands</a> of line items. I once contracted on a project for a government in Sub Saharan Africa which was doing exactly this. The country had set an import quota for sugar, managed by their state trading company, in an attempt to improve their balance of payments situation. When this led to high prices due to shortages they had set price limits, when this led to hoarding and smuggling they took control of sugar retailing and distribution. Smuggling profits rose even higher, leading to a situation where <a href=\"https://pure.diis.dk/ws/files/1269212/DIIS_WP_2017_11.pdf\">sugar smuggling funds terrorist operations</a> and normal people experience rationing, at prices well above world market rates. Unfortunately, the game of policy twister is still underway, with an expansion of state-owned sugar production to bridge the deficit. This is not going well. It almost certainly did not even improve their balance of payments, their original goal in some distantly remembered past.</p><p>The concept that letting go to achieve more is deeply counterintuitive, and runs against our instincts, but is a foundational aspect of core parts of classical economics. &nbsp;Chaos there is not just a force to be fought, but something a force to be supported and gently guided the service of social goals. As a result, you end up in an almost state of zen, doing less to achieve more, and finding a way to let the fury of the river do the work for you rather than damming it and making a serene canal.</p><p>There is a great tendency in the world today to see a problem, then sit down and reason almost in a dark room for how you will solve it scientifically via policy and commands. This can drive great progress, and EA was founded on this to a degree. However, the failure modes of this thinking can be catastrophic, especially with radical and ambitious plans: many in the past have sat down with honest intentions for adding order to chaos for the betterment of man but ended up doing the precise opposite. These ideas <a href=\"https://slatestarcodex.com/2019/06/06/asymmetric-weapons-gone-bad/\">seemed obvious at the time</a>, <a href=\"https://slatestarcodex.com/2017/03/16/book-review-seeing-like-a-state/\">promised to provide better lives through science</a>, <a href=\"https://en.wikipedia.org/wiki/Great_Leap_Forward\">or raise agricultural output</a>. The reality did not conform to their expectations: they were plans designed for robots, not the humans and natural world standing before them in their vast complexities.</p><p>However, while chaos cannot and should not be contained completely, and while we should not presume to understand it fully, we also simply cannot leave it alone. This is true in economics and its failures of the past and present; it is utterly foundational to the concerns around existential risks. Plans must be made for when the market fails: your social supports, antitrust laws, policy on externalities and systems to prevent financial crashes and bank runs must be in place. These economic policies were almost all built upon the pain of past experiences, and scientific thinking, evidence and rationality were a key element of their design. For existential risks this raises an even larger problem, as we cannot work from past experiences: it will simply be too late, and grand plans here are required.</p><p>So where does this leave us? Are we stuck between chaos being uncontrollable and often useful up until the point where it levels our world?&nbsp;</p><p>I would argue not, while I\u2019m perhaps stretching the zen analogy here I believe it is a case where we need both the yin and the yang of chaos and order.&nbsp;</p><p>We need to be modest in what we know, understand that the world is not a chess board and that making use of chaos can be helpful: plans should be able to adjust, we should design systems to make use knowledge no matter where it comes from, we should be humble in our own understanding and constantly attempt to question our positions. A physicist may not be able to predict the outcome of a football match, but they may be able to make predictions of what happens when the ball is kicked, and if it will leave the stadium. That can still be useful.</p><p>At the same time we need to keep working towards the large goals attached to the movement, and accept that we must impose order on chaos, sometimes in situations where we only have reason to guide us. Here we must still look at the mistakes of the past, in order to not repeat them, we may only have one opportunity to do so.</p><p>This is a fairly chaotic post, rather fittingly. I do not have all the answers or even that many, I am working on one small part of EA looking at food systems, where producing enough food is a necessary but not sufficient step in feeding everyone. It would be great to start a conversation on complexity across the movement, how we deal with it well, how we deal with it poorly, and where the example of others can help.&nbsp;</p><p>If you have any thoughts please post them below, and we should not stop having grand ambitions or even ivory tower thinking.</p><p>However, at the same time I would suggest embracing the following thoughts, wherever possible:</p><ul><li>Design systems that scale from a small base. If you have a grand plan that requires a complete system to function, but no way of getting there, you do not have a plan.</li><li>Systems must fail gracefully. We will be wrong many times, and must build that into our thinking.</li><li>Look at history and other studies, people have done surprisingly weird things in the past which may be relevant for you today. Want to see how a society with <a href=\"https://slatestarcodex.com/2017/11/13/book-review-legal-systems-very-different-from-ours/\">a judiciary, a legislature but no executive functioned</a>? Want to know what happens when we <a href=\"https://en.wikipedia.org/wiki/1815_eruption_of_Mount_Tambora#Impact_of_the_eruption\">lose a good chunk of our sunlight</a>? Of course the world has changed, but history and its examples can prove a very interesting start for our thinking, something Will Macaskill highlighted in his recent EAG speech.</li><li>The world is not a chessboard. We do not know all of the rules and probably never can, we cannot assume we can move the pieces at will, and I would be deeply concerned of a system that allowed that level of control.</li></ul>", "user": {"username": "Michael Hinge"}}, {"_id": "rjFJYREBZZscvDwLu", "title": "A Framework for Technical Progress on Biosecurity", "postedAt": "2021-11-03T10:57:41.853Z", "htmlBody": "<figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/qw9ogxtcdokorwqgytxr\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/kwyvertvqrnwsab5ckjk 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/nodyvzqizesxtxra0lxy 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/dl8skbcmbzjikvjrjmhf 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/xeudmktshvgql2gfsfb3 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/t8iiok21m602hitdyige 1000w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/unxe8mwn2zyuh7msdoga 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/aow9lgkfnbe4ssjydcef 1400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/dhzkzwqmwn5sffmwg2kc 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/c7cik7y5nri3ypgfzurp 1800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rjFJYREBZZscvDwLu/tbdyitwyw9joihnb5l6g 1910w\"><figcaption>The Problem Space (aka \"Biosecurity Bingo\")</figcaption></figure><h1>Introduction</h1><p>Urbanization, globalization, climate change, political instability, and health system fragility all contribute to the vulnerability of the modern world to catastrophic damage from pandemic diseases. Of particular concern is the emergence of a pathogen or pathogens so severe that they threaten the global population\u2019s collective ability to respond, survive, and recover. Even if the probability of such an event is low, the scale of damage and potential for irreparable harm to society warrant focused consideration extending beyond the scope of traditional public health programs and research.</p><p>Technical innovations like vaccination and antibiotic development have dramatically improved the resilience of human society to the threat of infectious disease. A variety of existing, emerging, and yet-undeveloped technologies could provide additional layers of protection and mitigate the risks posed by potentially catastrophic pathogens. The goal of this post is to establish a framework for conceptualizing the technical innovations needed for improving biosecurity and pandemic preparedness. The relevant technologies can be grouped into six core categories: monitoring, forensics, barriers, diagnostics, prophylactics, and therapeutics. Across all of these categories, there is a conserved set of key goals that must be pursued to improve biosecurity. These goals are to make the relevant technologies fast, general, cheap, robust, and scalable.&nbsp;</p><p>Considered together, these six categories of technology and five key goals create a matrix of thirty individual objectives (i.e. improve the generality of diagnostics, lower the costs of prophylactics, increase the speed of therapeutics development, etc.) that cover the majority of the technical biosecurity problem space. With few exceptions, pursuing these objectives has potential to catalyze general improvements to global health in addition to mitigating the risk of a catastrophic pandemic or biological attack. Here, I introduce the key goals and core technology areas and highlight the unique importance of each for improving biosecurity.</p><h1>Key technology development goals</h1><h2>Overview</h2><p>The innovations needed for improving biosecurity and pandemic preparedness include diverse technologies across a broad spectrum of engineering and biotechnology disciplines, from vaccine development to personal protective equipment (PPE) to diagnostics design and manufacturing. Across all these technologies, there is a need for solutions to move from expensive to cheap, from slow to fast, from specific to generalizable, from small to large scale, and from fragile to robust. While inevitably interconnected, each of these goals has a unique and important role to play in improving pandemic preparedness and avoiding catastrophic damage from biological threats. Here, I introduce each of these objectives, highlight their importance for biorisk reduction, and use genetic sequencing as a case study to illustrate their value.</p><h2>Slow to fast</h2><p>Speed is a critical factor in pandemic prevention and response. The ability to quickly identify a novel pathogen or outbreak, develop reliable diagnostics, and produce a vaccine or other countermeasure is essential for containment and mitigation. The average time required for the development of a new therapeutic is more than 10 years, from discovery to commercialization. Vaccine development and deployment are similarly sluggish: it takes an estimated 5\u201318 years to bring a vaccine from the research phase to the international market. While various strategies are used to accelerate these timelines amidst disease outbreaks with pandemic potential, current technologies still impose a limit on the pace of countermeasure development. Innovative approaches to drug and vaccine discovery and manufacturing are needed to dramatically reduce the lag time between identification of a pathogen and successful deployment of countermeasures.&nbsp;</p><p>Progress in genome sequencing exemplifies the improvement needed in this area. The human genome project (HGP) took 13 years to produce the first human genome sequence. Today, reaching the same endpoint with a novel sample requires only a day or two. The value added by this increased efficiency goes far beyond convenience. Rapid sequencing enables near real-time analysis, giving clinicians and researchers invaluable information in routine investigations and amidst public health crises. Reducing the development and use times for other biosecurity-relevant technologies, including diagnostics, therapeutics, and vaccines, would similarly improve biothreat preparedness.</p><h2>Specific to generalizable</h2><p>The traditional paradigm in drug development is a one-bug-one-drug approach, where a specific therapeutic is developed for each individual pathogen. Vaccines, diagnostics, and pathogen surveillance systems are largely analogous: the technology used for each is typically pathogen dependent and poorly generalizable. This is a problem for biosecurity, as it is impossible to predict which pathogen will be implicated in the next outbreak. There is a need across all of these domains to move from a suite of strategies tailored to specific agents, to platforms capable of responding to a broad variety of different pathogens. This goal is particularly important in light of advances in synthetic biology that enable the engineering of novel pathogens capable of evading current countermeasures.&nbsp;</p><p>Sequence-based diagnostics are one area where vital developments have improved generalizability, exemplified by progress from single-pathogen PCR tests to clinical metagenomics. PCR tests are an important tool for infectious disease detection and diagnosis; however, dependence on specific primers for each query limits their utility to a defined set of pathogens. In contrast, clinical metagenomics is a pathogen-agnostic strategy involving comprehensive analysis of all genetic material from both the host and any associated microbes. This approach allows identification of potential novel pathogens in addition to diagnosis of any known infectious diseases.</p><h2>Expensive to cheap</h2><p>The development and deployment of advanced biotechnologies is notoriously expensive, which severely limits their scope of use. As new technologies emerge, they are often confined to the limited research institutions and companies capable of absorbing the high costs of equipment, reagents, and maintenance. The development and use of such expensive technologies contributes to the exorbitant prices of drug development, vaccine manufacturing, and infectious disease surveillance. Many new and existing technologies require broad deployment to substantially improve biosecurity, which is not feasible while costs remain high. Finding innovative ways to lower costs for the production and use of relevant technologies, or developing entirely new technologies at low price points with overlapping functionality, will thus be essential efforts in the process of leveraging advanced biotechnologies for the reduction of biorisks.&nbsp;</p><p>Whole-genome sequencing is an excellent example of the type of cost reduction necessary to make new technology applicable in the biosecurity context. The HGP produced the first finished sequence of the human genome in 2003 at an estimated cost of ~$450 million. By 2006, the cost to produce an equivalent sequence had dropped more than one order of magnitude, to $20\u201325 million. Relentless innovation over the past decade and a half has continued to drive dramatic cost reductions; the cost for obtaining a complete genome sequence now approaches $1,000/genome. This progress has been fueled by a synergistic combination of incremental improvements (e.g. a steady reduction in the price of reagents and consumables) and dramatic paradigm shifts (e.g. the advent of next-generation sequencing (NGS)). Similar progress is needed for many other biorisk-related technologies to reach sufficient affordability and achieve the level of coverage essential for improving security.&nbsp;</p><h2>Fragile to robust</h2><p>Detecting and responding to pandemic threats requires global deployment of relevant technologies. The utility of many important tools, from diagnostics platforms to vaccines, are currently limited by their fragility: they often depend on sensitive reagents and samples, or bulky equipment that requires frequent maintenance. In order to deploy these measures in field environments or amidst a catastrophe when infrastructure, labor, and supply chains are severely compromised, their robustness must be improved. Specific needs vary depending on the technology. For diagnostics and detection systems, helpful goals include miniaturization of equipment for ease of transport, minimization of sample prep requirements and reagent complexity, and simplification of protocols to allow operation by non-experts. For vaccines and therapeutics, achieving cold-chain independence and improving shelf life, even in harsh conditions, are top priorities.&nbsp;</p><p>The successful development of field-deployable genome sequencing systems represents one important step in this direction. The MinION sequencer (Oxford Nanopore Technologies, Oxford, UK) is a handheld sequencing device that weighs less than 100g and connects to a laptop through a standard USB port. Despite a higher error rate than existing sequencing strategies, the robustness of the MinION has contributed to its successful use for pathogen surveillance in the field during major disease outbreaks. Analogous progress is needed in other areas of biotechnology to ensure that even the most remote and resource-constrained environments have access to robust surveillance, diagnostics, immunization, and therapeutics.&nbsp;</p><h2>Small to large scale</h2><p>Pandemics are global events and are likely to require comprehensive, global approaches to prevention, detection, and response. Many relevant biotechnologies are poorly suited to meet this need, as they are developed predominantly for small-scale manufacturing and low-throughput use. Even for technologies already deployed at a large scale, like vaccines, the systems and processes used for scale-up and mass production are fraught with challenges and inefficiencies, often causing late-stage product failure, high production costs, and/or supply shortages. Improving scalability for the full suite of technologies with biosecurity applications is thus an essential goal in ensuring pandemic preparedness. This will require 1) novel manufacturing systems to facilitate scale-up of technologies currently produced in small batches, 2) improvements to mass manufacturing paradigms for vaccines, therapeutics, and other products, and 3) more accurate models for predicting industrial performance of laboratory processes to guide research and development.&nbsp;</p><p>In the case of genome sequencing, improvements in process scale and throughput have enabled large-scale genome sequencing studies that provide unprecedented levels of insight into complex genetic relationships and host-pathogen interactions. While not yet at the scale required for universal coverage, this success represents tremendous progress. Identifying high-priority technologies for global biodefense and building systems for comprehensive coverage by increasing the efficiency and reliability of scale-up will be crucial in preparing for the next pandemic.&nbsp;</p><h1>Core technology areas</h1><h2>Monitoring</h2><p>Early detection of an emerging pathogen or biological attack is essential for effective response and containment. Clinical diagnostics have an important role to play in identifying outbreaks, however it is much preferable to detect novel pathogens prior to human exposure. To accomplish this, global monitoring systems are needed for real-time surveillance of both natural disease reservoirs and high-probability areas for an accidental or deliberate pathogen release. Identifying novel pathogens or strains of existing pathogens with potential to affect the human population allows proactive measures to be taken for closer monitoring, containment, and pre-emptive countermeasure development and deployment.</p><p>Example project: <a href=\"https://arxiv.org/pdf/2108.02678.pdf\"><u>a global nucleic acid observatory</u></a></p><h2>Forensics</h2><p>If and when a novel pathogen is identified, forensic efforts are needed to identify its provenance. A major question to be answered is whether a particular pathogen evolved or emerged naturally, or whether it was the product of deliberate engineering. If the former, robust forensic tools are needed to determine the specific region and mechanism of emergence. If the latter, we need tools capable of determining where, and by whom, the pathogen was developed. Such knowledge will be useful for improving our understanding of the emergence and behavior of threatening pathogens, informing response efforts, and, perhaps most importantly, denying anyone the possibility of releasing a novel pathogen with impunity.</p><p>Example project: <a href=\"https://www.nature.com/articles/s41467-020-19612-0\"><u>genetic engineering attribution</u></a></p><h2>Barriers</h2><p>Barriers are systems for preventing exposure to an infectious agent. They include tools for protecting individual people from exposure (e.g. personal protective equipment like masks, gloves, and protective suits), as well as systems for preventing penetration of a pathogen into physical spaces like bunkers or buildings. Such systems can be based on physical removal of pathogens through filtration, destruction of pathogens through sterilization, or a combination of both. There is tremendous need for improvements to barrier technologies, as they have historically been neglected relative to other areas of technology development. Ideal barrier solutions would make it a trivial matter for large numbers of people to remain completely safe from infection, even amidst a serious outbreak.&nbsp;</p><p>Example project: <a href=\"https://www.challenge.gov/challenge/mask-innovation-challenge/\"><u>mask innovation challenge</u></a></p><h2>Diagnostics</h2><p>The ability to rapidly and accurately diagnose patients during an outbreak is essential for mounting an adequate response. Of particular interest is the ability to quickly diagnose infections by previously unseen pathogens anywhere in the world. Various issues with traditional approaches to this challenge have made it an elusive goal, including low sensitivities, high costs, and extensive sample prep requirements of conventional diagnostic tests. These bottlenecks are especially problematic in resource-constrained environments that lack central laboratory facilities and modern molecular biology equipment. Fortunately, novel strategies have shown promise for overcoming the limitations of conventional diagnostics and may soon play a central role in the mitigation of biothreats.</p><p>Example project: <a href=\"https://www.nature.com/articles/s41591-020-1105-z\"><u>metagenomic diagnostics</u></a>&nbsp;</p><h2>Prophylactics</h2><p>Vaccines and other prophylactics are an essential tool for protecting individuals, communities, and the global population against infectious diseases. However, despite tremendous progress and unprecedented success in improving global health over the past two centuries, modern technologies for vaccine development, manufacturing, and delivery remain insufficient for addressing the threat of novel pathogens with catastrophic pandemic potential. Innovations in each of these areas are needed to ensure capacity for rapid development, production, and distribution of vaccines in response to outbreaks. The use of mRNA vaccines in fighting the COVID-19 pandemic has been an astonishing success, and hopefully represents only the beginning of next-generation vaccine technologies.&nbsp;</p><p>Example project: <a href=\"https://www.modernatx.com/modernas-work-potential-vaccine-against-covid-19\"><u>mRNA vaccines</u></a></p><h2>Therapeutics</h2><p>When someone is infected with a pathogen, therapeutics are among the best tools for treatment and can help prevent further spread. A variety of therapeutic classes, including small molecules, peptides, proteins, and antibodies, likely have a role to play in ensuring effective responses to emerging pathogens. In some cases, broad-spectrum therapeutics can be developed that are capable of treating most or all pathogens of an entire category (e.g. gram-positive bacteria, coronaviruses), even ones that have never been seen before. In other cases, it may be possible to develop rapid therapeutic design systems capable of creating new therapeutics for emerging pathogens quickly enough to stop an outbreak.</p><p>Example project: <a href=\"https://med.stanford.edu/news/all-news/2020/10/grant-to-develop-broad-spectrum-drugs.html\"><u>broad-spectrum antivirals</u></a></p><h1>Conclusion</h1><p>The vast majority of technical innovation needed for improving biosecurity and pandemic preparedness involves pushing these six technology areas toward these five goals. The result, if successful, will be a suite of technologies that function together to provide low-cost, broad-spectrum, rapid, effective, and dependable protection against the emergence or deployment of novel pathogens anywhere on the planet. They will also function collectively as a powerful deterrent, dissuading bad actors from ever pursuing bioweapons as a strategy for destruction.</p><p>While it is useful to consider each of these goals and technologies independently and highlight their unique contributions to biorisk reduction and pandemic preparedness, in practice they are intimately connected. Improvements in one technology along a single dimension can empower innovations in diverse areas, and, conversely, a vulnerability anywhere in this network is a vulnerability everywhere. The true power of these technologies comes from their synergies, and their potential will only be fully realized through strategic, coordinated development.</p><p>In choosing specific projects to prioritize and deciding how to pursue them, it is essential to consider not just the largest unmet needs, but also the dual-use potential of relevant biotechnologies. Responsible technology development in many of these areas requires an abundance of caution and may look significantly different than a cursory analysis would suggest.&nbsp;</p><p>If you\u2019re interested in working in this space, please get in touch! I will connect you with people in the biosecurity community who are laser-focused on building these technologies as safely and quickly as possible.&nbsp;</p><p>&nbsp;</p><p><i>Thanks to Andrew Snyder-Beattie, Ethan Alley, Grigory Khimulya, and Sofia-Davis Fogel for comments on drafts of this post.</i></p><p><i>This project was supported by the EA Long-Term Future Fund.</i><br>&nbsp;</p>", "user": {"username": "kyle_fish"}}, {"_id": "sadugJrbaa9v2z9zg", "title": "Urban wildlife in South Africa - Cape baboons", "postedAt": "2021-11-03T10:20:38.893Z", "htmlBody": "<h2><i><strong>Introduction&nbsp;</strong></i></h2><p>This is the third post on research exploring the interventions to improve the lives of urban wild vertebrate animals in South African cities. We are grateful for the support of a grant from <a href=\"https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants\"><u>EA Animal Welfare Fund</u></a>. The aim of this post is to catalogue existing methods for managing the population of Cape chacma baboons living in the Cape peninsula, with a focus on welfare impacts for the baboons. Where appropriate, we indicate which existing methods might be prioritised or modified, to improve baboons\u2019 welfare. Since urban baboon populations are not widespread globally, many of these insights are particular to the local context. However, some insights may be generalised to contexts where urban wildlife populations share certain characteristics with the Cape chacma baboon.</p><p>Relative to more numerous urban wild animal populations considered \u201cpests\u201d (e.g. rats and pigeons), we think that the importance of interventions to improve welfare of Cape baboons is low. Recent data (in the past ~5 years) indicates that annual baboon deaths are relatively low (and have been decreasing). The baboon population seems to be stabilizing around a suggested carrying capacity of ~480 baboons. Fewer baboons die from (direct or indirect) human causes than from natural causes (including infanticide).&nbsp;</p><p>Neglectedness is also relatively low compared to rats and pigeons. The problem seems relatively well-recognised and researched. The City of Cape Town has devoted resources to the existing Urban Baboon Programme (see below), which seems to have been fairly effective at keeping baboons out of urban areas. The issue has received&nbsp;<a href=\"http://www.icwild.uct.ac.za/cape-peninsula-baboons\"><u>research</u></a> and <a href=\"https://www.dailymaverick.co.za/article/2021-07-19-cape-towns-baboon-programme-successful-coexistence-between-wildlife-and-urban-communities-through-trial-and-error/\"><u>media attention</u></a>.&nbsp;</p><p>Nevertheless, Cape baboon welfare matters, and efforts to improve their welfare are worthwhile. Improving their welfare seems relatively tractable, e.g. there seems to be reasonably broad \u201cbuy in\u201d for humane methods of keeping humans and baboons separate.&nbsp;</p><h2><i><strong>Rationale</strong></i></h2><p>There are 12 troops of Cape chacma baboons in the Cape peninsula, with a total population of approximately 450-500, which is comfortably within the bounds of the suggested ~480 carrying capacity for the area. These baboons live close to urban areas and are often in contact and in conflict with human beings. This proximity and associated conflict is driven largely by the historical culling of baboon predators, reduction in baboon habitat due to urbanisation, and expanding baboon populations over the past approximately ten years.&nbsp;</p><p>The majority of human-baboon conflict arises when baboons enter low-lying urban areas to source calorie-rich human food (\u201craiding\u201d). Raiding is typically performed by lone male baboons, and research suggests that most raiding is undertaken by only a few members of a few of the Cape peninsula troops. These troops reside in areas with steep mountain slopes adjacent to urban areas (baboons typically need 2000-2500 hectares of \u201cbuffer\u201d natural habitat, and prefer more digestible vegetation in low-lying areas). Human-baboon conflict has resulted in human-inflicted injury and death to baboons, and less frequently, baboon-inflicted injuries to humans. The leading causes of death and injuries for baboons in these conflicts are dog attacks, car accidents, poisoning, and shooting (Human Wildlife Solutions, 2020).</p><p>Although the most significant welfare considerations for baboons arise from human-baboon conflict, human-baboon spatial overlap also carries relatively low health risks to both humans and baboons due to the possible transmission of parasites&nbsp;(<i>Trichuris</i>) or&nbsp;hepatitis A virus&nbsp;between the species (Ravasi et al, 2012).&nbsp;</p><h2><i><strong>Current interventions</strong></i></h2><p>Research indicates (typically lone young male) baboons continue to raid despite the risk of deterrence, because the rewards of calorie-rich human food are so great. Therefore it is widely accepted that (i)&nbsp;baboons spending more time in their natural habitats is associated with fewer human-caused injuries and deaths; and (ii) reducing contact with humans requires&nbsp;active monitoring and the use of aversive deterrents from human food/waste. Because the reward of human-food is so great, multiple interventions are required to make raiding undesirable to baboons.</p><p>The problem of human-baboon conflict in the Cape peninsula has received significant public attention (O\u2019Riain, 2021). In 2009, the City of Cape Town implemented the well-resourced Urban Baboon Programme, which, via an outsourced service provider, manages the local baboon population, with the central goal of keeping baboons and human beings as separate as possible. The programme follows a set of guidelines and standard operating procedures which are the product of deliberation between public and private participants including research institutions, affected communities, and the organisations mandated to manage local and national parks (City of Cape Town, 2019).&nbsp;</p><p>The baboon troops are actively monitored, which includes tracking their population, movement, and behaviour. Interventions such as electric fencing and \u201cvirtual&nbsp;fences\u201d (predator sounds played via loud-speakers when baboons approach particular locations) are used in \u201cno-go\u201d buffer zones around the urban edge in order to make it more difficult for baboons to travel into urban areas (Human Wildlife Solutions, 2019).&nbsp;</p><p>When necessary, baboon rangers are trained and permitted to use a variety of methods and tools to prevent conflict between humans and baboons and deter baboons from entering urban areas. These include the use of paintball markers, bear bangers and pepper balls.</p><ul><li>Paintball markers are shot from paintball guns in order to encourage a change in direction of baboons approaching urban areas, and afford the operator a wider sphere of influence (10-20 metres from the operator). When baboons are within a buffer zone,&nbsp; rangers will prioritise firing no more than two warning shots into the ground in front of adult baboons. If warning shots fail to cause the baboons to retreat, operators may fire at adult baboons, but only at their central back and rump, with caution not to hit the facial region. If the baboons continue to approach or enter an urban area, the protocol changes. These changes however do not allow for more indiscriminate firing on baboon targets, but rather pertain to firing in such a way as to prevent the dispersion of the baboon troop as it is expelled from the area, as well as the prevention of harm to human beings.</li><li>Bear bangers are \u201ccartridges that are fired into the air\u2026[which] explode with a loud bang that scare baboons\u201d (Human Wildlife Solutions, 2015).</li><li>Pepper balls are projectiles from paintball guns which \u201cbreak upon impact and release a super-irritant powder called PAVA (capsaicin II) pepper. They are only used to chase baboons in extreme circumstances\u201d (Human Wildlife Solutions, 2015), presumably because of the pain and suffering caused to baboons.</li></ul><p>The above methods are designed to inflict as little physical harm to baboons as possible while achieving the desired effect of deterring them from urban areas.</p><p>In addition to active management and monitoring , legislation exists to reduce human-induced death and injury to baboons in the Cape peninsula, as well as to curb human behaviour that encourages human-baboon contact and conflict. It is illegal to feed,&nbsp; hunt, poison, and shoot at baboons (including with non-lethal implements such as paintball guns), or otherwise intentionally harm baboons, without a permit (Cape Nature, 2010).</p><p>Apart from legal mechanisms to discourage inappropriate treatment of baboons, the Urban Baboon Programme also engages in routine public education campaigns to promote responsible human behaviour aimed at reducing human-baboon conflict. Communities are given information regarding how to make their environments less appealing and accessible to baboons, such as by installing baboon-proofing on windows, doors, and bins, and by planting indigenous trees instead of invasive garden plants that are appealing food sources for baboons. Electric fencing is also recommended in areas where baboon raiding occurs, especially for properties where food waste attracts baboons, such as restaurants and waste sites. The City of Cape Town requires residents in baboon raiding areas to use baboon-proof bins fitted with two padlocks. Such bins are made available by local government, and although they tend to be readily adopted, residents use the padlocks inconsistently, diminishing the overall efficacy of the intervention.&nbsp;</p><p>Research by Fehlman et al (2017) used GPS tracking collars attached to baboons to show that&nbsp;raiding male baboons significantly changed their behaviours in urban spaces (relative to baboons living in more remote areas who adopted time-expensive low-risk foraging strategies), in order to exploit calorie-rich human food sources. Specifically, raiders spent almost all of their time at the urban edge, engaging in short, high-activity forays into the urban space. Despite the risks of being deterred by rangers, occasionally getting into urban areas to raid is incentive enough to persist with raiding attempts.&nbsp;This suggests that the above baboon-proofing interventions are essential in tandem with the ranger activities outlined above, in order to deter raiding baboons.&nbsp;</p><p>The programme also includes protocols for the euthanasia of baboons who are identified as habitual raiders and this is used as a last resort. Baboons who have been seriously injured by human-interactions are also sometimes euthanised under this protocol. &nbsp;</p><p>Research indicates that these interventions have been fairly effective in deterring baboons from entering urban areas. Between 2013 and 2020 managed troops spent around 95% of their time outside of urban areas (Human Wildlife Solutions, 2020).&nbsp;</p><p>In addition, fewer baboons are currently being euthanised under the programme\u2019s protocols compared to when the programme commenced, even though the baboon population has grown. For example, between 2013 and 2015, 35 habitual raiders were euthanised, as opposed to only 17 being euthanised in the period between 2018-2020, while the population has grown by 25.7% (Human Wildlife Solutions, 2020). &nbsp;</p><h2><i><strong>Potential improvements on current interventions</strong></i></h2><p>The current management strategy implemented under the Urban Baboon Programme has been largely successful where it has been enforced, and is routinely scrutinised and revised both internally and via external stakeholder engagement (O\u2019Riain, 2021). One reasonable improvement, therefore,&nbsp; is to extend the programme\u2019s operational sphere to other areas in the Western Cape province (and South Africa) with significant human-baboon interactions. Another improvement might be to introduce audits or other mechanisms to ensure community compliance with the programme. Another&nbsp; urgent additional intervention will be contraceptives and sterilisation in order to control the Cape peninsula population as it approaches and/or breaches the carrying capacity of the region. Finally, further research could be productive in assessing new and existing interventions to better understand their efficacy and short- and long-term impacts on baboon welfare.</p><h3><i><strong>Consider contraceptives and sterilisation options</strong></i></h3><p>Due to the historic culling of baboon predators (and their resultant absence), it is possible that in the future the baboon population may grow to exceed the region\u2019s carrying capacity. This may in turn increase competition for resources and lead to an increase in human-baboon conflict (or increase non-anthropogenic baboon suffering due to e.g. starvation or intra-troop conflict) . &nbsp;</p><p>There is some evidence to suggest that the Cape baboon population might already be approaching its carrying capacity. A report by Human Wildlife Solutions (the company &nbsp;contracted to manage the Urban Baboon Programme at the time), noted that baboon deaths caused directly by interactions with human beings (such as being killed by a dog or a car) doubled between 2017 and 2020 (Human Wildlife Solutions, 2020). In the same period, infanticides committed by baboons also doubled, suggesting that human-baboon conflict and intra-baboon conflict are both increasing. Hoffmann and O\u2019Riain (2012) suggest that capturing and removing troops is not recommended as a viable long-term solution to this conflict, because neighbouring troops are likely to usurp the territory vacated by removed troops.</p><p>Sterilisation and contraceptive measures for baboon populations were considered in response to the damage baboons have caused to commercial plantations, especially in Mpumalanga (Institute for Commercial Forestry Research, 2012). A number of significant welfare consequences of these interventions were raised. It was noted that sterilisation by surgical means is exceptionally costly, would cause significant distress to the baboon being sterilised as well as its troop, and may cause unforeseen behavioral and social consequences such as the dissolution of troops (Institute for Commercial Forestry Research, 2012).&nbsp;</p><p>The use of contraceptives also has unknown social consequences for baboons, but may be more cost-effective and easier to implement. Effective use of contraceptives would require repeated administration of hormones to known individual baboons, which in the Cape peninsula may be reasonable as the troops are very well-monitored. Further research is required to assess baboon welfare effects and &nbsp;potential costs and benefits.</p><h3><i><strong>Extend formal protection of baboons and baboon management programmes</strong></i></h3><p>A notable shortcoming of the current strategy is its scope. Outside of the rural and urban areas of the Cape peninsula, the active management of baboon troops is generally absent, and there are fewer legal constraints on the treatment and killing of baboons who are in conflict with human beings (Winchester, 2019). In other parts of the Western Cape, such as the Cape winelands, hunting season for baboons runs throughout the year, and farmers are permitted to kill one baboon per day. Unlike in the Cape peninsula, communities have largely been left to navigate the management of local baboon troops without support or coordination, which has resulted in inflated baboon deaths caused by humans (Winchester 2019).</p><p>A first step to decreasing the number of baboons being killed by humans in these regions is to extend the formal protection of baboons that exists in the Cape peninsula, thereby prohibiting members of the public from killing any baboons without a permit. However, in absence of a concrete management strategy, human-baboon conflict is likely to persist in these regions.</p><p>Extending the Urban Baboon Programme, or establishing similar regional programmes in other parts of the Western Cape, would ameliorate this chaotic situation, but would require significant investment by the provincial government.</p><h3><i><strong>Create alternatives to euthanasia for habitual raiders</strong></i></h3><p>One of the most significant welfare costs to baboons of the present programme is the euthanasia of habitual raiders. While it is not recommended to relocate entire troops, relocating these habitual raiders to appropriate wildlife sanctuaries or rehabilitation centers (that meet the regulatory requirements of the Cape nature conservation authority, CapeNature) may be a feasible alternative.</p><h3><i><strong>Conduct more concrete field-based trials of baboon deterrents</strong></i></h3><p>Marginal improvements to baboon welfare could be achieved by conducting more concrete field-based trials of new baboon deterrents, to assess marginal effectiveness over existing deterrents; and making the results of such trials available to decision-makers in a centralised database. Ideally, new deterrents should <i>prima facie&nbsp;</i>be (i) at least as effective at keeping baboons out of human areas; and (ii) have a lower impact on baboon welfare.</p><h3><i><strong>Audit and draft legislation to improve compliance with guidelines for reducing food attractants for baboons</strong></i></h3><p>As underscored by Fehlman et al. (2017), the high-reward of calorie-rich human food for raiding baboons is so high that a critical aspect of mitigating baboon presence in urban areas is to decrease the amount of human food accessible to baboons, and&nbsp; increase the cost for baboons of accessing these human food sources (City of Cape Town, 2021).&nbsp;</p><p>The City of Cape Town could perform audits in baboon hotspots to ensure that baboon-proofing is in place on human properties, and that waste management procedures are conducted so as to minimise baboons being lured and having access to waste as a food source. Areas already identified as having low rates of adoption of baboon-proofing should be prioritized for auditing. In addition, the city may consider introducing by-laws or other legislative mechanisms to ensure compliance with these measures.&nbsp;</p><h3><i><strong>Expand use of baboon-proof electric fencing&nbsp;</strong></i></h3><p>While we could find little existing research regarding the efficacy of electric fencing as a deterrent to raiding baboons, their installation has been frequently recommended in the annual and monthly reports produced by the Urban Baboon Programme, especially in situations where paintball markers are not being used. This suggests that adequate roll-out of electric fencing may be a viable alternative to paintball markers and other interventions that impose greater welfare costs to baboons.&nbsp;</p><h2><i><strong>Lessons for other cities</strong></i></h2><p>A key driver of human-baboon conflict in the Cape peninsula is the lack of natural baboon predators due to historic culling of these species. Other cities should be keenly aware of the importance of preserving existing predator populations and biodiversity in order to minimise conflict between humans and urban wildlife species (especially wildlife populations that interact/conflict frequently with humans).</p><p>In the case of baboons, research has shown that most conflict with humans is caused by particular troops and particular individuals within those troops. This may be true of other urban wildlife species who come into conflict with human beings. It is therefore advisable to conduct monitoring and research to identify problematic groups or individual animals that may be driving conflict, such that management interventions can be targeted at these animals, as opposed to applied generically. This is especially important in the case of more extreme measures such as culling.</p><h2><i><strong>References</strong></i></h2><p>Beamish, E. 2009. Causes and consequences of mortality and mutilation in the Cape Peninsula baboon population, South Africa. University of Cape Town.</p><p>Bentley S., Kaplan, M., Justin O'Riain \"Shedding Light on Reflective Prisms as Potential Baboon (<i>Papio ursinus</i>) Deterrents in the Cape Peninsula, South Africa,\" African Journal of Wildlife Research, 45(1), 117-121, (1 April 2015).&nbsp;</p><p>Cape Nature. (2010). <i>Understanding Baboons [Pamphlet]. </i>https://www.kbrc.org.za/imgs/biodiversity/baboons/understanding-baboons-a3-leaflet.pdf</p><p>City of Cape Town. (November, 2019). <i>2019 Guidelines for Baboon Management. </i><a href=\"https://www.baboons.org.za/images/Protocols/Guidelines_for_Baboon_Management_Nov_2019.pdf\"><u>https://www.baboons.org.za/images/Protocols/Guidelines_for_Baboon_Management_Nov_2019.pdf</u></a></p><p>City of Cape Town. (May, 2021). <i>Urban Baboon Programme May 2021 Monthly Report</i>. <a href=\"https://resource.capetown.gov.za/documentcentre/Documents/City%20research%20reports%20and%20review/Baboon-Mngt-monthly-report-NCC_2021-05.pdf\">h<u>ttps://resource.capetown.gov.za/documentcentre/Documents/City%20research%20r</u>eports%20and%20review/Baboon-Mngt-monthly-report-NCC_2021-05.pdf.</a></p><p>Drewe, J. A., O'Riain, M. J., Beamish, E., Currie, H., &amp; Parsons, S. (2012). Survey of infections transmissible between baboons and humans, Cape Town, South Africa. <i>Emerging infectious diseases</i>, <i>18</i>(2), 298\u2013301. https://doi.org/10.3201/eid1802.111309</p><p>Fehlmann, G., O\u2019Riain, M.J., Kerr-Smith, C. <i>et al.</i> Extreme behavioural shifts by baboons exploiting risky, resource-rich, human-modified environments. <i>Sci Rep</i> <strong>7,&nbsp;</strong>15057 (2017). https://doi.org/10.1038/s41598-017-14871-2</p><p>Hoffman, T. S. and M. Justin O'Riain. 2012. Monkey management: using spatial ecology to understand the extent and severity of human\u2013baboon conflict in the Cape Peninsula, South Africa. <i>Ecology and Society</i> <strong>17</strong>(3): 13. <a href=\"http://dx.doi.org/10.5751/ES-04882-170313\"><u>http://dx.doi.org/10.5751/ES-04882-170313</u></a></p><p>Human Wildlife Solutions. (2015). <i>Tools used by HWS</i> [Pamphlet]. <a href=\"http://www.baboons.org.za/index.php/2015-04-02-12-21-19/brochures/send/2-brochures/114-tools-used-by-hws\"><u>http://www.baboons.org.za/index.php/2015-04-02-12-21-19/brochures/send/2-brochures/114-tools-used-by-hws</u></a></p><p>Human Wildlife Solutions. (August, 2019) <i>Baboon Management: Past, Present and Future</i>. <a href=\"https://resource.capetown.gov.za/documentcentre/Documents/Graphics%20and%20educational%20material/PresentationCARBS-BaboonMngt_PastPresentFuture_2019-08-29.pdf\">https://resource.capetown.gov.za/documentcentre/Documents/Graphics%20and%20educational%20material/PresentationCARBS-BaboonMngt_PastPresentFuture_2019-08-29.pdf</a></p><p>Human Wildlife Solutions. (2020) <i>Cape Peninsula Baboon Management Annual Report July 2019 to June</i> 2020. <a href=\"https://resource.capetown.gov.za/documentcentre/Documents/City%20research%20reports%20and%20review/Baboon-Mngt-2019-20-annual-report-HWS.pdf\"><u>https://resource.capetown.gov.za/documentcentre/Documents/City%20research%20reports%20and%20review/Baboon-Mngt-2019-20-annual-report-HWS.pdf</u></a></p><p>Institute for Commercial Forestry Research. (August, 2012). <i>Causes, consequences and solutions to baboon induced damage within commercial plantations in southern Africa&nbsp;</i>[Workshop summary]. <a href=\"https://www.forestry.co.za/uploads/File/Industry%20News/2013/Oct%202013%20-%20Baboon%20damage%20symposium%202012.pdf\">https://www.forestry.co.za/uploads/File/Industry%20News/2013/Oct%202013%20-%20Baboon%20damage%20symposium%202012.pdf</a></p><p>Kaplan, B.S., O\u2019Riain, M.J., van Eeden, R. <i>et al.</i> A Low-Cost Manipulation of Food Resources Reduces Spatial Overlap Between Baboons (<i>Papio ursinus</i>) and Humans in Conflict. <i>Int J Primatol</i> <strong>32,&nbsp;</strong>1397\u20131412 (2011). https://doi-org.ezproxy.uct.ac.za/10.1007/s10764-011-9541-8</p><p>O\u2018Riain, J. (2021) Cape Town\u2019s baboon programme. <i>Daily Maverick</i> (19 July 2021). &nbsp;<a href=\"https://www.dailymaverick.co.za/article/2021-07-19-cape-towns-baboon-programme-successful-coexistence-between-wildlife-and-urban-communities-through-trial-and-error/\">https://www.dailymaverick.co.za/article/2021-07-19-cape-towns-baboon-programme-successful-coexistence-between-wildlife-and-urban-communities-through-trial-and-error/</a></p><p>Ravasi DF, O\u2019Riain MJ, Davids F, Illing N (2012) Phylogenetic Evidence That Two Distinct Trichuris Genotypes Infect both Humans and Non-Human Primates. PLoS ONE 7(8): e44187.&nbsp;<a href=\"https://doi.org/10.1371/journal.pone.0044187\"><u>https://doi.org/10.1371/journal.pone.0044187</u></a>.&nbsp;</p><p>Winchester, Madelyn. (July, 23, 2019). Barrydale at war over growing baboon problem. <i>IOL.</i> &nbsp;<a href=\"https://www.iol.co.za/capeargus/news/barrydale-at-war-over-growing-baboon-problem-29682582\"><u>https://www.iol.co.za/capeargus/news/barrydale-at-war-over-growing-baboon-problem-29682582</u></a>.</p>", "user": {"username": "ajmfisher"}}, {"_id": "zP4jebzvdtBr6mxdz", "title": "CEA\u2019s events team: capacity building and mistakes", "postedAt": "2021-11-03T08:08:17.433Z", "htmlBody": "<p><em>For CEA's Q3 update, we're sharing multiple posts on different aspects of our work.</em></p>\n<p><em>This post was written before EA Global: London 2021, and refers to that event in future tense.</em></p>\n<hr>\n<p>Events enable attendees to make new connections, learn about core concepts, share and discuss new research, and coordinate on projects.</p>\n<p>In summary:</p>\n<ul>\n<li>We put a lot of effort into building up our team.</li>\n<li>We ran two well-received medium-sized events.</li>\n<li>We made some significant mistakes with one of these events, which means that the counterfactual impact might have been negative. We plan to reflect on this after EA Global is complete, and we expect to make significant changes to address this.</li>\n<li>In the past few weeks (as of 10/26/21), we\u2019ve managed to roughly double the capacity of EA Global: London, in response to an unexpectedly high number of very strong applications. We\u2019re currently focused on this event.</li>\n<li>Depending on how  EA Global: London goes, we expect to increase the total number of new connections (our key metric) relative to previous years. Optimistically, we might double the number of connections relative to 2019 (though the increase will likely be somewhat lower).</li>\n</ul>\n<h2>Metric: connections</h2>\n<p>Past surveys (e.g. <a href=\"https://forum.effectivealtruism.org/posts/NP5B6yNMoZiZbmEQ8/open-phil-ea-lt-survey-2020-introduction-and-summary-of\">Open Phil\u2019s survey</a>) suggest that connections between individuals are the key source of impact from our events. So we focus on the number of new connections we make at our events.</p>\n<p>Currently, we calculate this as follows: number of attendees x average number of self-reported new connections. We define a new connection as \u201csomeone you feel comfortable reaching out to to ask for a favor\u201d, so this is a relatively high bar. We expect that this is an overestimate, since people who engage a lot with the event are more likely to fill in our post-event survey, and also more likely to make lots of connections. However, we think it allows for some comparison between events and years.</p>\n<p>We\u2019re currently working on fine-tuning this metric, finding better ways of measuring it, and thinking through other important outcomes to consider for measuring event outcomes.</p>\n<p>Broadly, we think that we\u2019re on track to nearly double new connections compared to 2019. This would be a slight increase from 2020. However, we think this methodology somewhat overestimates growth relative to 2019, and underestimates growth relative to 2020:</p>\n<ul>\n<li>We think this methodology overestimates the number of connections at virtual events relative to in-person events<sup class=\"footnote-ref\"><a href=\"#fn-oxjEtvkgS5LMcSfHC-1\" id=\"fnref-oxjEtvkgS5LMcSfHC-1\">[1]</a></sup>; all events in 2020 were virtual, and some events in 2021 were virtual, while all 2019 events were in-person.</li>\n<li>2020 figures include EAGx, whereas 2019 and 2021 figures don\u2019t.</li>\n</ul>\n<p><img src=\"https://i.ibb.co/fHs4XCK/Events-table-1.png\" alt=\"\"></p>\n<p>See footnote #2 for details on how we came up with our \"projected\" figure.<sup class=\"footnote-ref\"><a href=\"#fn-oxjEtvkgS5LMcSfHC-2\" id=\"fnref-oxjEtvkgS5LMcSfHC-2\">[2]</a></sup></p>\n<h2>Staffing</h2>\n<p>Overall, we\u2019re currently in a transition period for the team: we\u2019ve made some very strong hires who we are now onboarding, or who are joining later this year. During this onboarding process, we\u2019re still understaffed. But we think the team will be stronger in about 6 months.</p>\n<p>Additionally, Amy is transitioning from a role focused on direct work on events to a role more purely focused on management, with support from Max (Executive Director).</p>\n<ul>\n<li>We hired\n<ul>\n<li><a href=\"https://www.linkedin.com/in/lizka-vaintrob/\">Lizka Vaintrob</a> for the <a href=\"https://www.centreforeffectivealtruism.org/careers/events-generalist/\">Events Generalist</a> role: we expect her to focus on communications and help out with impact analysis.</li>\n<li><a href=\"https://www.linkedin.com/in/ollie-base-970922135/\">Ollie Base</a> as our <a href=\"https://www.centreforeffectivealtruism.org/careers/community-events-manager/\">Community Events Manager</a>. He will help us to develop our overall events portfolio, with a particular focus on expanding our community-led events.</li>\n<li><a href=\"https://www.linkedin.com/in/patricia-magcalas-738256218/\">Pat Magcalas</a> as a Personal Assistant for Amy Labenz at the start of October.</li>\n</ul>\n</li>\n<li><a href=\"https://www.centreforeffectivealtruism.org/team/barry-grimes\">Barry Grimes</a> is transitioning to the <a href=\"https://www.happierlivesinstitute.org/\">Happier Lives Institute</a> starting in November. We\u2019re grateful for all of his hard work on events over the past 2.5 years.</li>\n<li>We\u2019re working with <a href=\"https://www.centreforeffectivealtruism.org/team/anine-havn-andresen\">Anine Andresen</a> (previously a contractor) to increase her hours, in the hope that she might become a full-time team member.</li>\n<li>We expect to bring on at least one additional hire in the next 3-6 months. In the meantime, we have hired some additional contractors:\n<ul>\n<li><a href=\"https://www.linkedin.com/in/ivan-burduk-387916118/\">Ivan Burduk</a> to support Admissions and Stewardship</li>\n<li><a href=\"https://www.linkedin.com/in/ashleyylin/\">Ashley Lin</a> to support Event Production, starting in October</li>\n</ul>\n</li>\n</ul>\n<h2>Picnics</h2>\n<p>We ran the EA Picnic in San Francisco (July 11).</p>\n<ul>\n<li>Outcomes:\n<ul>\n<li>191 attendees</li>\n<li>Attendees who filled out the survey:\n<ul>\n<li>averaged 4.6 new connections formed through the Picnic</li>\n<li>rated the Picnic as 8.38/10 on likelihood to recommend (LTR)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>We considered not running this event due to capacity limitations, but elected to do so anyway as a means of testing a new format. We\u2019re not sure if this was the right call: it meant we were more stretched in the run-up to EA Global and the Meta Coordination Forum.</li>\n</ul>\n<p>We supported two similar events on the US East Coast (in NYC on October 2nd, and in Boston on October 9th), since EAGx East Coast was delayed. However, we think the majority of the impact from those events is attributable to their organizers, who don\u2019t work for CEA.</p>\n<h2>Meta Coordination Forum</h2>\n<p>In September, we ran the EA Meta Coordination Forum, which had 40 attendees. The event was focused on facilitating ad hoc collaboration between key people working in the EA \u201cmeta\u201d space. (This is a successor to past series of Leaders Forum events.)</p>\n<p>So far, feedback from attendees has been positive on average:</p>\n<ul>\n<li>At time of writing, we have 22/40 responses to the survey.</li>\n<li>Median new connections = 4 (Mean = 5.33).</li>\n<li>Self-reported comparisons with counterfactual value of time\n<ul>\n<li>All but 3 people found that the event was a better use of their time than the counterfactual.</li>\n<li>Most of the respondents (9) told us that the time spent at the forum was 3-10x as valuable as the counterfactual.</li>\n<li>A rough (and conservative) interpretation of responses gives 10.77x the counterfactual as the average response. This value is similar to that of the last in-person Leaders Forum (2019) and higher than that of the last virtual Leaders Forum (2020).</li>\n</ul>\n</li>\n</ul>\n<p>However, we made some mistakes in the run-up to the event, which meant that some key attendees could not attend (due to US restrictions on who could enter the US). We think that we should have done more contingency planning for the scenario where US restrictions stayed in place longer than we anticipated. We also should have asked attendees for their preferences about timing and location earlier than we did.</p>\n<p>We also could have done more to support attendees who were travelling from abroad and dealing with difficult COVID restrictions, and we failed to communicate promptly with some invitees. Less significantly, there were some operational issues onsite: for instance, making snacks easily accessible to attendees and ensuring that food was produced on time.</p>\n<p>For this reason, we think that the event was valuable, but that it was probably less valuable (maybe significantly less valuable) than it could have been. We\u2019re still a bit unsure about this, because we think that delaying or moving the event might have caused us to lose <em>different</em> key attendees (and that delays would have delayed some of the learnings/plan changes produced by the event, which might be important).</p>\n<p>We think these events are important, and we value attendees\u2019 time very highly. As a result, we plan to reflect carefully on these mistakes, and we are contemplating making some major changes (such as changing who is responsible for the event in the future).</p>\n<h2>Future events</h2>\n<p>Most of our focus is on running <a href=\"https://www.eaglobal.org/events/london2021/\">EA Global: London</a> (in October). We were pleasantly surprised by the number of strong applications. This led us to increase the capacity of the event from around 500 to 1000 in the past few weeks (as of 10/26/21). We also created a concurrent virtual event, which has hundreds of attendees registered. We think that this might roughly double the value of the event, which would be quite exciting. This change does increase the risk of logistical issues, as well as the risk of COVID spread, but we think these risks are outweighed by the additional value, and we\u2019re doing our best to anticipate and mitigate these risks.</p>\n<p>The quality of applications also meant that we had to waitlist or reject some very strong applicants, because of limits on how many people we could accommodate even after increasing our capacity. Where possible, we\u2019ve tried to point these applicants to other events or resources, so that they can continue to engage with EA.</p>\n<p>We are also supporting <a href=\"https://www.eaglobal.org/events/eagxprague-2021/\">EAGx Prague</a> (in December).</p>\n<p>The schedule for 2022 is still being developed, but with our increased capacity, we are considering running 3 large EA Global conferences in 2022. Additionally, we will be supporting EAGxBoston, EAGxOxford, EAGxSingapore, EAGxAustralia, and a virtual Student Summit.</p>\n<h2>Reflections</h2>\n<p>Overall, the events team is in the middle of a difficult transition, but I expect us to be stronger on the other side.</p>\n<p>We\u2019ve had greatly reduced capacity while we focused on hiring and onboarding. We still managed to run multiple events alongside this, but we made some mistakes in those events. We\u2019re hopeful that our investments will pay off, and that we\u2019ll be able to run more and better events in future years.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-oxjEtvkgS5LMcSfHC-1\" class=\"footnote-item\"><p>We expect virtual events to overreport total numbers of connections relative to in-person events.\nAs mentioned in the body, this figure is calculated as the number of attendees x average number of self-reported new connections. And we expect that this is an overestimate, since people who engage a lot with the event are more likely to fill in the survey, and also more likely to make lots of connections.\nWe expect this effect to be bigger for virtual events, because there\u2019s more variation in how much people engage with the event, and because fewer people complete the survey (causing more bias). Therefore, we expect this method to broadly overestimate the number of connections made in 2020 (and to some extent 2021) versus 2019 <a href=\"#fnref-oxjEtvkgS5LMcSfHC-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-oxjEtvkgS5LMcSfHC-2\" class=\"footnote-item\"><p>This assumes:</p>\n<ul>\n<li>950 people at EAG: London in person, 850 people attending virtually.</li>\n<li>Average connection numbers for those are similar to those at past in-person events (8.3) and slightly lower than those at previous virtual events (4, relative to a historic average of 4.4) (respectively).</li>\n</ul>\n<p>This gives a total of 950 * 8.3 + 850 * 4 = 7885+3400 = 11,285 connections from EAG.</p>\n<p>We also expect around 200 connections from the Coordination Forum, which adds a total of 11,485 to the 6,144 number above. <a href=\"#fnref-oxjEtvkgS5LMcSfHC-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "AmyLabenz"}}, {"_id": "ikYdJALsmqzYnbwKf", "title": "How to maximize a career in psychology/therapy for the most good?", "postedAt": "2021-11-03T07:42:52.597Z", "htmlBody": "<p>Hey all, I'm preparing for Master's studies in Counseling Psychology (but am also considering eventually pursuing research, organizational, etc. if I find I can have higher general impact through those). I would love to study trauma psych and effective approaches to working with vulnerable/dispersed/refugee communities, and currently plan to focus my studies around that.</p><p>Are there any resources that people here could offer in regards to using a psych career in an EA framework? Thanks!</p>", "user": {"username": "Jon Massmann"}}]