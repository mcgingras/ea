[{"_id": "AMufjtGhdFNsrLCAw", "postedAt": "2023-12-01T07:14:31.053Z", "postId": "N4zqrvFnK59TwBnj8", "htmlBody": "<p>Hey Andreas! Thanks for writing this up, it was a really interesting read and I'm glad you shared it!&nbsp;</p><p>Some quick rambling thoughts after reading:</p><p>I think some of the distinctions might be semantic - some of what you describe would fall under <a href=\"https://www.safe.ai/ai-risk\">misuse risk/malicious use</a>, which could indeed be a real problem (If an AI is causing harm because it its values are aligned with a malicious human, is it aligned or misaligned overall? I'm not sure, but the <a href=\"https://www.lesswrong.com/search?contentType=Tags&amp;query=human%20alignment&amp;page=1\">human alignment problem</a> seems to be the issue here) - and I'm not sure how to weight that against the risk of unaligned AI. I think given that we are nowhere close to solving the alignment problem, people tend to assume that if we have AGI, it will be misaligned \"by definition\". In terms of s-risks, I would really recommend checking out the work of <a href=\"https://longtermrisk.org/research-agenda\">CLR</a>, as they seem to be the ones who spent most time thinking about s-risks. &nbsp;I think they also have a course on s-risks coming up sometime!</p>", "parentCommentId": null, "user": {"username": "gergogaspar"}}, {"_id": "RfvR9zL3uX9zWCacR", "postedAt": "2023-12-01T12:28:19.989Z", "postId": "N4zqrvFnK59TwBnj8", "htmlBody": "<p>There's a fair amount of discussion in AI alignment about what outer alignment requires, and how it's not just pursuing goals of a single person who is supposed to be in control.<br><br>As a few examples, you might be interested in some of these:<br><a href=\"https://www.alignmentforum.org/posts/Cty2rSMut483QgBQ2/what-should-ai-owe-to-us-accountable-and-aligned-ai-systems\">https://www.alignmentforum.org/posts/Cty2rSMut483QgBQ2/what-should-ai-owe-to-us-accountable-and-aligned-ai-systems</a></p><p><a href=\"https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ASIMOV2021-REUTH.pdf\">https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ASIMOV2021-REUTH.pdf</a>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/Tmvvvx3buP4Gj3nZK/learning-societal-values-from-law-as-part-of-an-agi\">https://www.lesswrong.com/posts/Tmvvvx3buP4Gj3nZK/learning-societal-values-from-law-as-part-of-an-agi</a>&nbsp;</p>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "FWycavoLbwjBX7hga", "postedAt": "2023-12-01T13:16:29.299Z", "postId": "N4zqrvFnK59TwBnj8", "htmlBody": "<p><strong>Executive summary</strong>: The risk of suffering from an aligned AI controlled by a profit-seeking entity may be higher than the extinction risk from a misaligned AI.</p><p><strong>Key points:</strong></p><ol><li>An aligned AI controlled by a corporation risks being used to maximize profits without checks and balances. This could lead to dystopia.</li><li>Absolute power granted by an aligned AI risks corrupting those in control, with no way to transfer power safely.</li><li>Today's corporations already control governments; an aligned AI would remove any remaining checks on their power.</li><li>Random all-powerful individuals with an aligned AI may be more dangerous than a misaligned AI.</li><li>More analysis is needed on the potential suffering enabled by aligned AI rather than just extinction risks.</li><li>The author is new to AI safety and wants feedback, especially from technical experts, on these ideas and questions.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "RvoJYj3tWSvKo4aFc", "postedAt": "2023-12-05T04:29:27.446Z", "postId": "N4zqrvFnK59TwBnj8", "htmlBody": "<p>I appreciate you sharing these! I've already started to read them</p>", "parentCommentId": "RfvR9zL3uX9zWCacR", "user": {"username": "Andreas P"}}, {"_id": "GjQFZqZHmGqruvXTq", "postedAt": "2023-12-05T04:37:42.477Z", "postId": "N4zqrvFnK59TwBnj8", "htmlBody": "<p>Awesome thank for the links and thoughts. I have actually been debating applying to the s risk fellowship, but with your mention finally applied.</p><p>Agreed that the big picture falls under the human alignment / malicious human use. It's likely the area which has been more researched historically, and I need to delve deeper into it. I've been putting off being more involved in LessWrong, but I will now make an account there as well with your highlight.&nbsp;</p><p>Thank you</p>", "parentCommentId": "AMufjtGhdFNsrLCAw", "user": {"username": "Andreas P"}}]