[{"_id": "8h6q7cHC5BrFmpF8L", "postedAt": "2024-02-26T10:36:03.466Z", "postId": "QBsCLkiEMpNmjPmzN", "htmlBody": "<p>Let me try to steelman this fear (which I mostly disagree with):</p><ol><li>Social media was originally thought to be a radical force for democratic change - see the Arab Spring, for instance.</li><li>The objective of disinformation was never to change minds, but to reduce trust in anonymous online interactions. See Russia's human-based propaganda methods.</li><li>Thus, disinformation blunts the value proposition of social media platforms in allowing individuals to coordinate political action.&nbsp;</li></ol><p>So it's really an opportunity cost we're talking about here in preventing social media from achieving its full potential - which may have been oversold in the first place.</p><p>My own view is that very few actors will attempt to target \"political trust\" as an abstract force. Instead, we should be significantly more concerned about financially-motivated scams targeting individuals.&nbsp;</p>", "parentCommentId": null, "user": {"username": "David Stinson"}}, {"_id": "6CqZkKX9sHXRTKKCs", "postedAt": "2024-02-26T13:21:57.134Z", "postId": "QBsCLkiEMpNmjPmzN", "htmlBody": "<p><strong>Executive summary</strong>: The widespread alarmism around AI-based disinformation threatening Western democracies is greatly overstated due to mistaken beliefs about persuasion, media, and establishment power.</p><p><strong>Key points</strong>:</p><ol><li>Online disinformation is not actually the root cause of most modern political problems.</li><li>Political persuasion is extremely difficult, even with sophisticated targeting.</li><li>Media competes for limited attention, requiring building reputation and audience trust.</li><li>Establishment forces will have access to more advanced AI than anti-establishment ones.</li><li>Counter-establishment content mainly preaches to a minority choir rather than persuading.</li><li>The alarmism around AI and disinformation neglects how people, media, and power actually function.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "9AgZqyLnH4bqSHcjn", "postedAt": "2024-02-27T07:53:01.551Z", "postId": "QBsCLkiEMpNmjPmzN", "htmlBody": "<p>Great post, Dan! Relatedly, readers may want to check <a href=\"https://forum.effectivealtruism.org/posts/KLL2YwK5i8ry2FJwx/180-why-gullibility-and-misinformation-are-overrated-hugo\">#180 \u2013 Why gullibility and misinformation are overrated (Hugo Mercier on the 80,000 Hours Podcast)</a>.</p><blockquote><p><i>There are now dozens, if not hundreds, of experiments showing that in the overwhelming or the quasi-entirety of the cases, when you give people a good argument for something, something that is based in fact, some authority that they trust, then they are going to change their mind. Maybe not enough, not as much as we\u2019d like them to, but the change will be in the direction that you would expect. In a way, that\u2019s the sensible thing to do.</i></p><p><i>And you\u2019re right that both laypeople and professional psychologists have been and still are very much attracted to demonstrations that human adults are irrational and a bit silly, because it\u2019s more interesting. We are attracted by mistakes, by errors, by kind of silly behaviour, but that doesn\u2019t mean this is representative at all.</i></p><p>- Hugo Mercier</p></blockquote>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "Kn9ySEozQbArBqm9M", "postedAt": "2024-03-01T12:28:53.149Z", "postId": "QBsCLkiEMpNmjPmzN", "htmlBody": "<p>In general I agree (but I already did before reading the arguments) there's probably a hype around AI-based disinformation and argument 2, i.e., that we actually don't have a lot of evidence of large change in behavior or attitudes due to misinformation, is the strongest of the arguments. There is evidence from the truth effect that could be used to argue that a flood of disinformation may be bad anyway; but the path is longer (truth effect leads to slightly increased belief than then has to slightly influence behavior; probably something we can see with a population of 8 billions, but nothing with losses as large as war or environmental pollution/destruction). The other arguments are significantly weaker, and I'd note the following things:&nbsp;</p><ol><li>It's interesting that, in essence, the central claim relies on the idea that there's widespread misinformation about the fact that misinformation does not impact people's attitudes, behavior, etc., that much. [Edit: In general, I'd note that most studies on misinformation <strong>prevalence</strong> depend on a) <strong>representativeness</strong> of the data collected and b) on our ability to <strong>detect</strong> misinformation. As for the first, we don't know whether most studies are representative (although it is easy to suspect they aren't due to their recruitment methods; e.g., MTurk and Prolific) nor in terms of the experiences these already-probably-not-representative people have (they mostly focus on one platform, with X, formerly Twitter, dominating most research). In terms of the second, self-reported misinformation exposure and lists of URL \"poor quality websites\" are probably very noisy (the latter depending on how exhaustive the URL lists are and relying on the assumption that good quality news don't ever share misinformation and that bad quality news don't ever share actual information; and of course this does not allow the detection of misinformation that is not shared through an URL link but by user's own words, images, videos, etc.), and alternatives such as human fact checkers and fact-check databases are also not perfect, as they reflect the biases (e.g., in the selection of what to fact-check in the first place) and lack of knowledge that the human curators naturally have. In sum: We should have a good amount of uncertainty around our estimates of misinformation prevalence.]</li><li>The dig at the left with \"aligns better with the prevailing sensibility and worldview of the liberal commentariat\" is unnecessarily inflammatory, particularly when no evidence of difference in perspective between left and right is advanced (and everyone remembers how Trump weaponized the term \"fake news\"; regardless, left-right asymmetries aren't settled with anecdotes, but with actual data; or even better, instead of focusing on \"who does it more\" focusing on \"stopping to do it\").&nbsp;</li><li>It sounds odd to me to assume in 4. that what people fear is only or even mostly about anti-establishment propaganda. In part because, without any evidence, this is just mind-reading, in part because, given modern levels of affective polarization, the most likely is that when one's party is not in power (or has only recently left), we are more likely to believe that the president (we don't like) is using their powers for the purposes of propaganda, and so are more worried about establishment propaganda as it then becomes salient (e.g., in the US context: https://edition.cnn.com/2021/01/24/politics/trump-worst-abuses-of-power/index.html | https://www.nytimes.com/2023/06/14/business/media/fox-news-biden-dictator-trump.html).</li></ol>", "parentCommentId": null, "user": {"username": "CristinaM"}}, {"_id": "TEPawWvSdscAqEWCA", "postedAt": "2024-03-06T06:12:50.350Z", "postId": "QBsCLkiEMpNmjPmzN", "htmlBody": "<p>I appreciate you writing this, it seems like a good and important post. I'm not sure how compelling I find it, however. Some scattered thoughts:</p><ul><li>In point 1, it seems like the takeaway is \"democracy is broken because most voters don't care about factual accuracy, don't follow the news, and elections are not a good system for deciding things; because so little about elections depends on voters getting reliable information, misinformation can't make things much worse\". You don't actually say this, but this appears to me to be the central thrust of your argument \u2014 to the extent modern political systems are broken, it is not in ways that are easily exacerbated by misinformation.&nbsp;</li><li>Point 3 seems to be mainly relevant to mainstream media, but I think the worries about misinformation typically focus on non-mainstream media. In particular, when people say they \"saw X on Facebook\", they're not basing their information diet on trustworthiness and reputation. You write, \"As noted above (#1), the overwhelming majority of citizens get their political information from establishment sources (if they bother to get such information at all).\" I'm not sure what exactly you're referencing here, but it <a href=\"https://www.pewresearch.org/journalism/fact-sheet/news-platform-fact-sheet/\">looks to me</a> like people are getting news from social media about \u2154 as much as from news websites/apps (see \"News consumption across digital platforms\"). This is still a lot of social media news, which should not be discounted.&nbsp;</li><li>I don't think I find point 4 compelling. I expect the establishment to have access to slightly, but not massively better AI. But more importantly, I don't see how this helps? If it's easy to make pro-vaccine propaganda but hard to make anti-vax propaganda, I don't see how this is a good situation? It's not clear that propaganda counteracts other propaganda in an efficient way such that those with better AI propaganda will win out (e.g., insularity and people mostly seeing content that aligns with their beliefs might imply little effect of counter-propaganda existing). You write \"<i>Anything that anti-establishment propagandists can do with AI, the establishment can do better</i>\", but propaganda is probably not a zero-sum, symmetric weapon.&nbsp;</li><li>Overall, it feels to me like these are decent arguments about why AI-based disinformation is likely to be less of a big deal than I might have previously thought, but they don't feel super strong. They feel largely handwavy in the sense of \"here is an argument which points in that direction\", but it's really hard to know how hard they push that direction. There is ample opportunity for quantitative and detailed analysis (which I generally would find more convincing), but that isn't made here, and is instead obfuscated in links to other work. It's possible that the argument I would actually find super convincing here is just way to long to be worth writing.&nbsp;</li><li>Again, thanks for writing this, I think it's a service to the commons.&nbsp;</li></ul>", "parentCommentId": null, "user": {"username": "Aaron_Scher"}}, {"_id": "aLv2KEyLNFqTbbSpp", "postedAt": "2024-03-10T19:44:34.055Z", "postId": "QBsCLkiEMpNmjPmzN", "htmlBody": "<p>Elaborating on point 1 and the \"misinformation is only a small part of why the system is broken\" idea:&nbsp;</p><p>The current system could be broken in many ways but at some equilibrium of sorts. Upsetting this equilibrium could have substantial effects because, for instance, people's built immune response to current misinformation is not as well trained as their built immune response to traditionally biased media.&nbsp;</p><p>Additionally, intervening on misinformation could be far more tractable than other methods of improving things. I don't have a solid grasp of what the problem is and what makes is worse, but a number of potential causes do seem much harder to intervene on than misinformation: general ignorance, poor education, political apathy. It can be the case that misinformation makes the situation merely 5% worse but is substantially easier to fix than these other issues.&nbsp;</p>", "parentCommentId": "TEPawWvSdscAqEWCA", "user": {"username": "Aaron_Scher"}}, {"_id": "ryWpgjD2pSwArwYJN", "postedAt": "2024-03-19T10:19:27.680Z", "postId": "QBsCLkiEMpNmjPmzN", "htmlBody": "<p>Thanks for this. A few points:&nbsp;</p><ul><li>Re. point 1: Yes, I agree with your characterisation in a way: democracy is already a kind of epistemological disaster. Many treatments of dis/misinfo assume that people would be well-informed and make good decisions if not for exposure to dis/misinfo. That's completely wrong and should affect how we view the marginal impact of AI-based disinformation.&nbsp;</li><li>On the issue of social media, see my post: <a href=\"https://www.conspicuouscognition.com/p/debunking-disinformation-myths-part.\">https://www.conspicuouscognition.com/p/debunking-disinformation-myths-part.</a> Roughly: People tend to greatly overestimate how much people are informing themselves about social media in my view.&nbsp;</li><li>Re. point 4: I'm not complaining it's a good thing if establishment propaganda outcompetes anti-establishment propaganda. As I explicitly say, I think this is actually a genuine danger. It's just different from the danger most people focus on when they think about this issue. More generally, in thinking about AI-based disinformation, people tend to assume that AI will only benefit disinformation campaigns, which is not true, and the fact it is not true should be taken into consideration when evaluating the impact of AI-based disinformation.&nbsp;</li></ul><p>You write of my arguments: \"They feel largely handwavy in the sense of \"here is an argument which points in that direction\", but it's really hard to know how hard they push that direction. There is ample opportunity for quantitative and detailed analysis (which I generally would find more convincing), but that isn't made here, and is instead obfuscated in links to other work.\"&nbsp;</p><p>It's a fair point. This was just written up as a blog post as a hobby in my spare time. However, what really bothers me is the asymmetry here: There is a VAST amount of alarmism about AI-based disinformation that is guilty of the problems you criticise in my arguments, and in fact features much less reference to existing empirical research. So it felt important for me to push back a bit, and more generally I think it's important that arguments for and against risks here aren't evaluated via asymmetric standards.&nbsp;</p>", "parentCommentId": "TEPawWvSdscAqEWCA", "user": {"username": "Dan Williams"}}]