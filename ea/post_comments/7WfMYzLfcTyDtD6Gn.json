[{"_id": "JntrE4HemjzP6Aiep", "postedAt": "2023-10-10T17:37:34.765Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>Thanks for sharing, Scott! For reference, your post had already been <a href=\"https://forum.effectivealtruism.org/posts/ZAxaXaakgdQK3ACqY/pause-for-thought-the-ai-pause-debate-astral-codex-ten\">linkposted</a>, but it may be fine to have the whole post here as well. I think it makes sense to contact the author before linkposting.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "5b6grb3dufp6kmfSh", "postedAt": "2023-10-10T18:07:35.840Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>(I suggested to Scott that he do this crosspost. I think it was nice of David to do the link post, but I like having the full text available on the forum, and under the original author's name.)</p>", "parentCommentId": "JntrE4HemjzP6Aiep", "user": {"username": "Ben_West"}}, {"_id": "hscbLWrku5w7BLhZN", "postedAt": "2023-10-11T09:31:22.219Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>I think that the evidence you cite for \"careening towards Venezuela\" being a significant risk comes nowhere near to showing that, and that as someone with a lot of sway in the community you're being epistemically irresponsible in suggesting otherwise. &nbsp;<br><br>Of the links you cite as evidence:<br><br>The first is about the rate of advance slowing, which is not a collapse &nbsp;or regression scenario. At most it could contribute to such a scenario if we had reason to think one was otherwise likely.&nbsp;<br><br>The second is describing an all-ready existing phenomenon of cost disease which while concerning has been compatible with high rates of growth and progress over the past 200 years.<br><br>The third is just a blog post about how some definitions of \"democratic\" are theoretically totalitarian in principle, and contains 0 argument (even bad) that totalitarianism risk is high, or rising, or will become high.<br><br>The fourth is mostly just a piece that takes for granted that some powerful American liberals and some fraction of American liberals like to shut down dissenting opinion, and then discusses inconclusively how much this will continue and what can be done about it. But this seem obviously insufficient to cause the collapse of society, given that, as you admit, periods of liberalism where you could mostly say what you like without being cancelled have been the exception not the rule over the past 200 years, and yet growth and progress have occurred. Not to mention that they have also occurred in places like the Soviet Union, or China from the early 1980s onward, that have been pretty intolerant of ideological dissent.&nbsp;<br><br>The fifth is a highly abstract and inconclusive discussion of the possibility that having a bunch of governments that grow/shrink in power as their policies are successful/unsuccessful, might produce better policies than an (assumed) status quo where this doesn't happen*, combined with a discussion of the connection of this idea to an obscure far-right wing Bay Area movement of <i>at most</i> a few thousand people. It doesn't actually argue for the idea that dangerous popular ideas will eventually cause civilization regression at all; it's mostly about what would follow <i>if</i> popular ideas tended to be bad in some general sense, and you could get better ideas by having a \"free market for governments\" where only successful govs survived.&nbsp;<br><br>The last link on dysgenics and fertility collapse largely consist of you arguing that these are not as threatening as some people believe(!). In particular, you argue that world population will still be slightly growing by 2100 and it's just really hard to project current trends beyond then. And you argue that dysgenic trends are real but will only cause a very small reduction in average IQ, even absent a further Flynn effect (and \"absent a further Flynn effect\" strikes me as unlikely if we are talking about <i>world</i> IQ, and not US.) Nowhere does it argue these things will be bad enough to send progress into reverse.<br><br>This is an incredibly slender basis to be worrying about the idea that the general trend towards growth and progress of the last 200 years will reverse absent one particular transformative technology.&nbsp;<br><br><br><br>*It plausibly does happen to some degree. The US won the Cold War partly because it had better economic policies than the Soviet Union.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Dr. David Mathers"}}, {"_id": "nkAdZjmibgs83pKSC", "postedAt": "2023-10-11T12:28:36.881Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>Thanks for writing this up, I was skeptical about Scott\u2018s strong take but didn\u2019t take the time to check the links he provided as proof.</p>\n", "parentCommentId": "hscbLWrku5w7BLhZN", "user": {"username": "Simon_Grimm"}}, {"_id": "8tbDgKq73Mmh6irvu", "postedAt": "2023-10-11T13:07:41.678Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>\"eventually technology will advance to the point where you can train an AI on anything\"</p>\n<p>Assuming this means AGI, this is a very strong claim that doesn't get any justification. It may be theoretically true if \"eventually\" means \"within 100 billion years\", but it's not obvious to me that this will be true on more practical time scales (10-300 years).</p>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "HMeCPy4dHnZ5okuo4", "postedAt": "2023-10-11T13:10:04.141Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>\"Fourth, there are many arguments that a pause would be impossible, but they mostly don\u2019t argue against trying.\"</p>\n<p>I think this is a really important point</p>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "y7WqTCLMXNABLxP4t", "postedAt": "2023-10-12T15:33:57.806Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>I think this is a good and useful post in many ways, in particular laying out a partial taxonomy of differing pause proposals and gesturing at their grounding and assumptions. What follows is a mildly heated response I had a few days ago, whose heatedness I don't necessarily endorse but whose content seems important to me.</p>\n<p>Sadly this letter is full of thoughtless remarks about China and the US/West. Scott, you should know better. Words have power. I recently wrote <a href=\"https://forum.effectivealtruism.org/posts/xABJoccsRyfXGNDEA/careless-talk-on-us-china-ai-competition-and-criticism-of\">an admonishment to CAIS for something similar</a>.</p>\n<blockquote>\n<p>The biggest disadvantage of pausing for a long time is that it gives bad actors (eg China) a chance to catch up.</p>\n</blockquote>\n<p>There are literal misanthropic 'effective accelerationists' in San Francisco, some of whose stated purpose is to train/develop AI which can surpass and replace humanity. There's Facebook/Meta, whose leaders and executives have been publicly pooh-poohing discussion of AI-related risks as pseudoscience for years, and whose actual motto is 'move fast and break things'. There's OpenAI, which with great trumpeting announces its 'Superalignment' strategy without apparently pausing to think, 'But what if we can't align AGI in 5 years?'. We don't need to invoke bogeyman 'China' to make this sort of point. Note also that the CCP (along with EU and UK gov) has so far been <em>more</em> active in AI restraint and regulation than, say, the US government, or orgs like Facebook/Meta.</p>\n<blockquote>\n<p>Suppose the West is right on the verge of creating dangerous AI, and China is two years away. It seems like the right length of pause is 1.9999 years, so that we get the benefit of maximum extra alignment research and social prep time, but the West still beats China.</p>\n</blockquote>\n<p>Now, this was in the context of paraphrases of others' positions on a pause in AI development, so it's at least slightly <a href=\"https://en.wikipedia.org/wiki/Use%E2%80%93mention_distinction\">mention-flavoured (as opposed to use)</a>. But as far as I can tell, the precise framing here has been introduced in Scott's retelling.</p>\n<p>Whoever introduced this formulation, this is bonkers in at least two ways. First, who is 'the West' and who is 'China'? This hypothetical frames us as hivemind creatures in a two-player strategy game with a single lever. Reality is <em>a lot more porous</em> than that, in ways which matter (strategically and in terms of outcomes). I shouldn't have to point this out, so this is a little bewildering to read. Let me reiterate: governments are not currently pursuing advanced AI development, only companies. The companies are somewhat international, mainly headquartered in the US and UK but also to some extent China and EU, and the governments have thus far been unwitting passengers with respect to the outcomes. Of course, these things can change.</p>\n<p>Second, <em>actually think</em> about the hypothetical where 'we'<sup class=\"footnote-ref\"><a href=\"#fn-bcyC6ohGe3nSQFCtA-1\" id=\"fnref-bcyC6ohGe3nSQFCtA-1\">[1]</a></sup> are 'on the verge of creating dangerous AI'. For sufficient 'dangerous', the only winning option for humanity is to take the steps we can to prevent, or at least delay<sup class=\"footnote-ref\"><a href=\"#fn-bcyC6ohGe3nSQFCtA-2\" id=\"fnref-bcyC6ohGe3nSQFCtA-2\">[2]</a></sup>, that thing coming into being. This includes advocacy, diplomacy, 'aggressive diplomacy' and so on. I put forward that the right length of pause then is 'at least as long as it takes to make the thing not dangerous'. You don't win by capturing the dubious accolade of nominally belonging to the bloc which directly destroys everything! To be clear, I think Scott and I agree that 'dangerous AI' here is shorthand for, 'AI that could defeat/destroy/disempower all humans in something comparable to an extinction event'. We already have weak AI which is dangerous to lesser levels. Of course, if 'dangerous' is more qualified, then we can talk about the tradeoffs of risking destroying everything vs 'us' winning a supposed race with 'them'.</p>\n<p>I'm increasingly running with the hypothesis that many anglophones are mind-killed on the inevitability of contemporary great power conflict in a way which I think wasn't the case even, say, 5 years ago. Maybe this is how thinking people felt in the run up to WWI, I don't know.</p>\n<p>I wonder if a crux here is some kind of general factor of trustingness toward companies vs toward governments - I think extremising this factor would change the way I talk and think about such matters. I notice that a lot of American libertarians seem to have a warm glow around 'company/enterprise' that they don't have around 'government/regulation'.</p>\n<p>[ <a href=\"https://forum.effectivealtruism.org/posts/xABJoccsRyfXGNDEA/careless-talk-on-us-china-ai-competition-and-criticism-of\">In my post</a> about this I outline some other possible cruxes and I'd love to hear takes on these ]</p>\n<p>Separately, I've got increasingly close to the frontier of AI research and AI safety research, and the challenge of ensuring these systems are safe remains very daunting. I think some policy/people-minded discussions are missing this rather crucial observation. If you expect it to be easy (and expect others to expect that) to control AGI, I can see more why people would frame things around power struggles and racing. For this reason, I consider it worthwhile repeating: <em>we don't know how to ensure these systems will be safe, and there are some good reasons to expect that they won't be by default</em>.</p>\n<p>I repeat that the post as a whole is doing a service and I'm excited to see more contributions to the conversation around pause and differential development and so on.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-bcyC6ohGe3nSQFCtA-1\" class=\"footnote-item\"><p>Who, me? You? No! Some development team at DeepMind or OpenAI, presumably, or one of the current small gaggle of other contenders, or a yet-to-be-founded lab. <a href=\"#fnref-bcyC6ohGe3nSQFCtA-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-bcyC6ohGe3nSQFCtA-2\" class=\"footnote-item\"><p>If it comes to it, extinction an hour later is better than an hour sooner. <a href=\"#fnref-bcyC6ohGe3nSQFCtA-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "parentCommentId": null, "user": {"username": "Oliver Sourbut"}}, {"_id": "Z8RuFRkNtKQH46p2Q", "postedAt": "2023-10-13T19:49:32.006Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<blockquote><p>The second is describing an all-ready existing phenomenon of cost disease which while concerning has been compatible with high rates of growth and progress over the past 200 years.</p></blockquote><p>I want to add further that cost disease is not only <i>compatible</i> with economic growth, cost disease itself is a <i>result</i> of economic growth, at least in the usual sense of the word. The Baumol effect -- which is what people usually mean when they say cost disease -- is simply a side effect of some industries becoming more productive more quickly than others. Essentially the only way to avoid cost disease is to have uniform growth across all industries, and that's basically never happened historically, except during times of total stagnation (in which growth is ~0% in every industry).</p>", "parentCommentId": "hscbLWrku5w7BLhZN", "user": {"username": "Matthew_Barnett"}}, {"_id": "Pv25bcDkAswWoof2b", "postedAt": "2023-10-20T15:31:44.571Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>I'm curating this post. This is a well-written summary of the AI Pause Debate, and I'm excited for our community to build on that conversation, through distillation and more back-and-forth.</p>", "parentCommentId": null, "user": {"username": "jpaddison"}}, {"_id": "txrtCMizsAzKufqTZ", "postedAt": "2023-10-20T17:50:01.014Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<blockquote><p>Zach writes in an email: \u201cMuch/most of my concern about China isn't <i>China has worse values than US</i>&nbsp;or even <i>Chinese labs are less safe than Western labs</i>&nbsp;but rather <i>it's better for leading labs to be friendly with each other (mostly to better coordinate and avoid racing near the end), so (a) it's better for there to be fewer leading labs and (b) given that there will be Western leading labs it's better for all leading labs to be in the West, and ideally in the US</i>&nbsp;[\u2026]</p></blockquote><p>I'm curious why Zach thinks that it would be ideal for leading AI labs to be in the US. I tried to consider this from the lens of regulation. I haven't read extensively on comparisons of what regulations there are for AI in various countries, but my impression is that the US federal government is sitting on their laurels with respect to regulation of AI, <a href=\"https://www.wired.com/story/us-failed-to-pass-ai-regulation-new-york-city-stepping-up/\">although state and municipal governments provide a somewhat different picture</a>, and whilst the intentions of each are different, the EU and the UK have been moving much more swiftly than the US government.</p><p>My opinion would change if regulation doesn't play a large role in how successful an AI pause is, eg if industry players could voluntarily practice restraint. There are also other factors that I'm not considering.</p>", "parentCommentId": null, "user": {"username": "Naoya Okamoto"}}, {"_id": "sRKXo5hu7PqA8iatW", "postedAt": "2023-10-25T07:05:17.186Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>Climate change is wrecking the planet, &nbsp;Putin is trying to start World War Three and the middle east is turning into a blood bath. Mean while some people hide from reality and worry about a perceived threat from the latest tools that humanity has invented. &nbsp;</p><p>Is their intelligent life on earth? I see little evidence to support that argument.</p>", "parentCommentId": null, "user": {"username": "Trev Prew"}}, {"_id": "zF29ioYvBeitPeK46", "postedAt": "2023-10-25T10:47:47.837Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<blockquote><p>I think there\u2019s a <a href=\"https://www.astralcodexten.com/p/the-extinction-tournament\">~20%</a>&nbsp;chance of AI destroying the world.&nbsp;</p></blockquote><p>I'd like to see more fleshed out reasoning on where this number is coming from. Is it based on an aggregate of expert views from people you trust? Or is there an actual <a href=\"https://forum.effectivealtruism.org/posts/idjzaqfGguEAaC34j/if-your-agi-x-risk-estimates-are-low-what-scenarios-make-up\">gears-level</a> mechanism for why there is non-doom over ~80% of future worlds with AGI? (Also, 20% is more than enough to be shouting \"<a href=\"https://twitter.com/So8res/status/1715380167911067878\">fucking stop</a>[!]\"...)</p><blockquote><p>But if we don\u2019t get AI, I think there\u2019s a 50%+ chance in the next 100 years we end up dead or careening towards Venezuela.&nbsp;</p></blockquote><p>Also would be good to see more justification for this! As per Dr. David Mathers' <a href=\"https://forum.effectivealtruism.org/posts/7WfMYzLfcTyDtD6Gn/pause-for-thought-the-ai-pause-debate?commentId=hscbLWrku5w7BLhZN\">comment</a> below. (And also: \"<a href=\"https://twitter.com/So8res/status/1715380167911067878\">Find some other route to the glorious transhuman future</a>[!]\")</p><blockquote><p>That doesn\u2019t mean I have to support AI accelerationism because 20% is smaller than 50%. Short, carefully-tailored pauses could improve the chance of AI going well by a lot, without increasing the risk of social collapse too much.</p></blockquote><p>Good that you don't support AI accelerationism, but I remain unconvinced by the reasoning for having carefully-tailored pauses. It seems far too risky to <a href=\"https://forum.effectivealtruism.org/posts/E6CahapSad7psvqx4/timelines-are-short-p-doom-is-high-a-global-stop-to-frontier\">me</a>.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "Qtw5mexDDHLKgGbXs", "postedAt": "2023-10-25T10:50:52.773Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<blockquote><p>There\u2019s no easy way to enforce it once technology gets so good that you can train an AI on your laptop, and (<strong>absent much wider adoption of x-risk arguments</strong>) government\u2019s won\u2019t have the stomach for hard ways. [my emphasis]</p></blockquote><p>I think that we&nbsp;<i>can</i> get that much wider adoption of x-risk arguments (indeed we are&nbsp;<a href=\"https://twitter.com/gcolbourn/status/1711637692377711034\"><u>already seeing it</u></a>), and a taboo on AGI / superhuman AI to go along with it, which will go a long way toward making enforcement of frontier model training run caps manageable.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "w7m645hnhjJ2GXJFY", "postedAt": "2023-10-25T10:57:24.934Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>What is wrong with the reasoning <a href=\"https://forum.effectivealtruism.org/posts/E6CahapSad7psvqx4/timelines-are-short-p-doom-is-high-a-global-stop-to-frontier\">here</a>? Yes there's a lot of things wrong with the world, but the extinction (total - no survivors) we're actually likely to get is from AI, this decade, unless we do something to stop it.&nbsp;</p>", "parentCommentId": "sRKXo5hu7PqA8iatW", "user": {"username": "Greg_Colbourn"}}, {"_id": "92FPsFn2qo5s22Dan", "postedAt": "2023-10-26T06:01:24.014Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>Thanks for for reply. The only threat to humanity comes from humanity. AI, like any other tool such as atomic weapons or dynamite will be used for good or bad by humans. AI is powerful and because its a new technology, it's impact on the future debatable, but this has been the case ever since humans invented flint tools.&nbsp;</p><p>I say the fundamental problem is how to steer humanity away from improper use of technology, which can be achieved by first understanding human behaviors and &nbsp;motivations, then by the widespread dispersion of this knowledge and finally by exposing the futility of such behaviour in our globalised, interconnected and interdependent society.&nbsp;</p><p>If the end of humanity does happen, it will not be due to AI, pandemics or atomic weapons, it will because one group of humans, decided it wanted to get an advantage over another group of humans and ignored all other considerations. Understand why and we may be able to find a solution.&nbsp;</p>", "parentCommentId": "w7m645hnhjJ2GXJFY", "user": {"username": "Trev Prew"}}, {"_id": "2ZXMAwztM9LBKp2Gj", "postedAt": "2023-10-26T13:00:22.916Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>No. AGI is different. It will have it's own goals and agency. It's more akin to a new alien species than a \"tool\". What we are facing here is basically better thought of as a (digital) alien invasion, facilitated (or at the last accidentally unleashed) by the big AI companies. Less intelligent species don't typically fare well when faced with competing more intelligent species.</p>", "parentCommentId": "92FPsFn2qo5s22Dan", "user": {"username": "Greg_Colbourn"}}, {"_id": "o6siTjJg2t4MvFwAH", "postedAt": "2023-10-27T05:45:22.362Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>Thanks Greg, I'm sixty years old and grew up when every one said the world was going to be destroyed in a thermonuclear war, then it was acid rain, then it was nano technology (covering the world in a layer of scum!), then it was the millennium bug, currently its climate change and it looks like people are starting to worry about AI. Even the Prime minister is at it, perhaps as a cover for his failed short term policies. Humans are fundamentally neurotic - perhaps it gives us an evolutionary edge, always being on the lookout for new threats, but if you step back and take an overview of humanity, maybe you will see what the real problems are. &nbsp;</p><p>However, my point is, take care of today (with an eye on the mid term), the current problems and the future will look after itself. Who can predict the future with any degree of certainty anyway, so why worry? &nbsp;Its correct that long term thinking is needed to tackle climate change, but not problems like Palestine / Israel or Putin's and Xi Jinping's ideology that threatens Europe and Asia or Trumps attack on democracy, all of which are trying to drag us back to repeat past failures. &nbsp;Long term thinking should not be used to avoid tackling short term problems.&nbsp;</p><p>From what I've read of science, biology, neurology, psychology, politics, economics, history, philosophy we are on the verge of a breakthrough in new thought and maybe because AI can pull vast pools of knowledge together and perhaps eliminate our biases and prejudices, bring about great change for the better. This is not something to be afraid of, but something to embrace, but of course caution is needed and a simple fail safe button should be built in &nbsp;if we don't like the outputs.&nbsp;</p><p>Thanks for reading.&nbsp;</p><p>Regards and good luck with your endeavours. Never stop learning, but keep it real.</p>", "parentCommentId": "2ZXMAwztM9LBKp2Gj", "user": {"username": "Trev Prew"}}, {"_id": "Bh5dHomwG39PxwEXw", "postedAt": "2023-10-27T07:58:29.242Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>'No. AGI is different. It will have it's own goals and agency.' Only if we choose to build it that way: <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf\">https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf</a> &nbsp;(Though Bengio was correct when he pointed out that even if lots of people build safer tools, that doesn't stop a more reckless person building an agent instead.)</p>", "parentCommentId": "2ZXMAwztM9LBKp2Gj", "user": {"username": "Dr. David Mathers"}}, {"_id": "6pzir3NYNnWd9DQAG", "postedAt": "2023-10-27T09:19:48.640Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>Unfortunately it's no longer a long term problem, it's 0-5 years away. Very much short term!</p>", "parentCommentId": "o6siTjJg2t4MvFwAH", "user": {"username": "Greg_Colbourn"}}, {"_id": "BPvHE9uZCTJeAL6yQ", "postedAt": "2023-10-27T09:20:51.002Z", "postId": "7WfMYzLfcTyDtD6Gn", "htmlBody": "<p>People are very much <a href=\"https://github.com/Significant-Gravitas/AutoGPT\">choosing</a> to build it that way unfortunately!&nbsp;</p>", "parentCommentId": "Bh5dHomwG39PxwEXw", "user": {"username": "Greg_Colbourn"}}]