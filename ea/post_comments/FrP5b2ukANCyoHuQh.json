[{"_id": "vRuyoeLnGikAip2HD", "postedAt": "2023-12-21T08:24:04.182Z", "postId": "FrP5b2ukANCyoHuQh", "htmlBody": "<p>Thanks so much for this article, I feel like I understand LLMs and their potential trajectory far better than I did before. The framings of \"drunk expert\" vs. \"interns\" was a little bit of a lightbulb moment for me.</p><p>As a non-technical person I've tried reading a lot of theoretical stuff about LLMs and AI both here and on LessWrong. I usually start reading then at some point (in the first third/half of the article) I hit a point where I genuinely don't understand what's going on, stop reading then go back and read something else usually in my global health comfort zone :D.</p><p><i><strong>For whatever reason I managed to read this whole article while at least feeling like I understood every paragraph and concept. I think the bar for understanding here is lower than many AI explainer articles, which I greatly appreciated and I would encourage other non-AI-technical people to read this in full. I would consider myself likely in the bottom 20% of forum readers when it comes to AI understanding as well.</strong></i><br><br><i><strong>I'm sure its not easy to walk the line between adequately explaining these concepts while still enabling basic people like me to understand ;).</strong></i></p><p>So well done.</p>", "parentCommentId": null, "user": {"username": "NickLaing"}}, {"_id": "xXbMheGKkum3g4cfW", "postedAt": "2023-12-24T20:06:49.245Z", "postId": "FrP5b2ukANCyoHuQh", "htmlBody": "<blockquote><p>In their most straightforward form (\u201cfoundation models\u201d), language models are a technology which naturally scales to something in the vicinity of human-level (because it\u2019s about emulating human outputs), not one that naturally shoots way past human-level performance</p><ul><li>i.e. it is a mistake-in-principle to imagine projecting out the GPT-2\u2014GPT-3\u2014GPT-4 capability trend into the far-superhuman range</li></ul></blockquote><p>Surprised to see no pushback on this yet. I do not think this is true; I've come around to thinking that Eliezer is basically right that the limit of next token prediction on human generated text is superintelligence. Now how this latent ability manifests is a hard question, but it's <i>there</i> to be used by the model for its own ends or elicited by humans for ours, or both.</p><p>Also worth adding (guessing this point has been made before) that non human-generated text (e.g. regression outputs from a program) are in the training data, so merely predicting those gets you superhuman performance in some domains.</p>", "parentCommentId": null, "user": {"username": "aaronb50"}}, {"_id": "TovYmCSmNhB6KSbCH", "postedAt": "2023-12-24T22:49:32.668Z", "postId": "FrP5b2ukANCyoHuQh", "htmlBody": "<p>Sorry, I think you're reading me as saying something like \"language models scaled naively up don't do anything superhuman\"? Whereas I'm trying to say something more like \"language models scaled naively up break the trend line in the vicinity of human level, because the basic mechanism for improved capabilities that they had been using stops working, so they need to use other mechanisms (which probably move a bit slower)\".</p><p>If you disagree with that unpacking, I'm interested to hear it. If you agree with the unpacking and think that I've done a bad job summarizing it, I'm interested if you want to propose alternate wording.</p><p>I do discuss the stuff you're talking about in several places in the doc, especially Sections 2.3, 4.1, and 6.2.</p>", "parentCommentId": "xXbMheGKkum3g4cfW", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "mb7KXSj8FcpJk5L2h", "postedAt": "2023-12-24T20:08:01.293Z", "postId": "FrP5b2ukANCyoHuQh", "htmlBody": null, "parentCommentId": "xXbMheGKkum3g4cfW", "user": null}]