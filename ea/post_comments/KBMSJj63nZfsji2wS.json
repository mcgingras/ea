[{"_id": "r2wb6SmnF56TLtFjB", "postedAt": "2023-06-08T13:15:56.144Z", "postId": "KBMSJj63nZfsji2wS", "htmlBody": "<p>Thank you for the post!&nbsp;</p><p>I just want to add some pointers to the literature which also add to the uncertainty regarding whether current or near-future AI may be conscious:&nbsp;</p><p>VanRullen &amp; Kanai have made reasonably concrete suggestions on how deep learning networks could implement a form of global workspace: <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0166223621000771\">https://www.sciencedirect.com/science/article/abs/pii/S0166223621000771</a>&nbsp;</p><p>Moreover, the so-called \"small network\" or \"trivial realization\" argument suggests that most computational theories of consciousness can be implemented by very simple neural networks which are easy to build today:&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0893608007001530?via%3Dihub\">https://www.sciencedirect.com/science/article/abs/pii/S0893608007001530?via%3Dihub</a></p><p><a href=\"http://henryshevlin.com/wp-content/uploads/2015/04/Trivial-Realisation-Argument.pdf\">http://henryshevlin.com/wp-content/uploads/2015/04/Trivial-Realisation-Argument.pdf</a></p>", "parentCommentId": null, "user": {"username": "LeonardDung"}}, {"_id": "Dfx55ANSTHBBRpShf", "postedAt": "2023-06-08T14:55:33.827Z", "postId": "KBMSJj63nZfsji2wS", "htmlBody": "<p>Thanks.&nbsp;</p>", "parentCommentId": "r2wb6SmnF56TLtFjB", "user": {"username": "Dr. David Mathers"}}, {"_id": "qCBAQaCv7CjSBHbfY", "postedAt": "2023-06-15T13:10:49.215Z", "postId": "KBMSJj63nZfsji2wS", "htmlBody": "<p>Thanks for clarifying!</p><p>It seems like it would be good if the discussion moved from the binary-like question \"is this AI system sentient?\" to the spectrum-like question \"what is the expected <a href=\"https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates\">welfare range</a> of this AI system?\". I would say any system has a positive expected welfare range, because welfare ranges cannot be negative, and we cannot be 100 % sure they are null. If one interprets sentience as having a positive expected welfare range, AI systems are already sentient, and so the question is how much.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "JeDDLGzcc8xTevorj", "postedAt": "2023-06-15T13:33:37.248Z", "postId": "KBMSJj63nZfsji2wS", "htmlBody": "<p>I think something like this is right, but I'm not entirely sure what an expected welfare range is. Suppose I think that all conscious &nbsp;things with pleasurable/painful experiences have the same welfare range, but there is only a 1 in 1000 chance that a particular AI systems has conscious pains and pleasures. What would it's expected welfare range be?&nbsp;</p>", "parentCommentId": "qCBAQaCv7CjSBHbfY", "user": {"username": "Dr. David Mathers"}}, {"_id": "7jij4s8wkz52PRpxq", "postedAt": "2023-06-15T15:11:25.785Z", "postId": "KBMSJj63nZfsji2wS", "htmlBody": "<p>The expected welfare range can be calculated from \"probability of welfare range being positive\"*\"expected welfare range if it is positive\", and is usually assumed to be 1 for humans. So it would be 10^-4 for the case you described, i.e. having 10 k such AI systems experiencing the best possible state instead of the worst would produce as much welfare as having 1 human experiencing the best possible state instead of the worst.</p>", "parentCommentId": "JeDDLGzcc8xTevorj", "user": {"username": "vascoamaralgrilo"}}, {"_id": "sa8Q9cdYhyS3MWGwq", "postedAt": "2023-06-15T17:49:14.400Z", "postId": "KBMSJj63nZfsji2wS", "htmlBody": "<p>Okay, that makes sense I guess.&nbsp;</p>", "parentCommentId": "7jij4s8wkz52PRpxq", "user": {"username": "Dr. David Mathers"}}]