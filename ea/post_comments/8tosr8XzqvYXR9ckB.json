[{"_id": "ijNHq9if8dtGEZQro", "postedAt": "2022-10-06T13:40:43.126Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<blockquote><p>I have some sympathy for the second view, although I'm skeptical that sane advisors have significant real impact. I'd love a way to test it as decisively as we've tested the \"government (in its current form) responds appropriately to warning shots\" hypotheses.</p><p>On my own models, the \"don't worry, people will wake up as the cliff-edge comes more clearly into view\" hypothesis has quite a lot of work to do. In particular, I don't think it's a very defensible position in isolation anymore....if you want to argue that we <i>do</i> need government support but (fortunately) governments will start behaving more reasonably after a warning shot, it seems to me like these days you have to pair that with an argument about why you expect the voices of reason to be so much louder and more effectual in 2041 than they were in 2021.</p><p>(Which is then subject to a bunch of the usual skepticism that applies to arguments of the form \"surely my political party will become popular, claim power, and implement policies I like\".)</p></blockquote><p>I think the second view is basically correct for policy in general, although I don't have a strong view yet of how it applies to AI governance specifically. One thing that's become clear to me as I've gotten more involved in institution-focused work and research is that large governments and other similarly impactful organizations are huge, sprawling social organisms, such that I think EAs simultaneously underestimate and overestimate the amount of influence that's possible in those settings. The more optimistic among us tend to get too excited about isolated interventions (e.g., electing a committed EA to Congress, getting a voting reform passed in one jurisdiction) that, even if successful, would only address a small part of the problem. On the other hand, skeptics see the inherent complexity and failures of past efforts and conclude that policy/advocacy/improving institutions is fundamentally hopeless, neglecting to appreciate that critical decisions by governments are, at the end of the day, made by real people with friends and colleagues and reading habits just like anyone else.</p><p>Viewed through that lens, my opinion and one that I think you will find is shared by people with experience in this domain is that the reason we have not seen more success influencing large-scale bureaucratic systems is that we have have been under-resourcing it as a community. &nbsp;By \"under-resourcing it\" I don't just mean in terms of money, because as the Flynn campaign showed us it's easy to throw millions of dollars at a solution that hits rapidly diminishing returns. I mean that we have not been investing enough in strategic clarity, a broad diversity of approaches that complement one another and collectively increase the chances of success, and the patience to see those approaches through. In the policy world outside of EA, activists consider it normal to have a 6-10 year timeline to get significant legislation or reforms enacted, with the full expectation that there will be many failed efforts along the way. But reforms <i>do</i> happen -- just look at the success of the <a href=\"https://www.slowboring.com/p/ten-years-of-yimbyism-have-accomplished\">YIMBY movement</a>, which Matt Yglesias wrote about today, or recent legislation to allow Medicare to negotiate prescription drug prices, which was in no small part the result of an <a href=\"https://www.arnoldventures.org/newsletter/we-werent-supposed-to-win-this-fight\">8-year, $100M campaign by Arnold Ventures</a>.</p><p>Progress in the institutional sphere is not linear. It is indeed disappointing that the United States was not able to get a pandemic preparedness bill passed in the wake of COVID, or that the NIH is still funding ill-advised research. But we should not confuse this for the claim that we've been able to do \"approximately nothing.\" The overall trend for EA and longtermist ideas being taken seriously at increasingly senior levels over the past couple of years is strongly positive. Some of the diverse factors include the launch of the Future Fund and the emergence of SBF as a key political donor; the publication of Will's book and the resulting book tour; the networking among high-placed government officials by EA-focused or -influenced organizations such as Open Philanthropy, &nbsp;CSET, CLTR, the Simon Institute, Metaculus, fp21, Schmidt Futures, and more; and the natural emergence of the initial cohort of EA leaders into the middle third of their careers. Just recently, I had one senior person tell me that Longview Philanthropy's hiring of Carl Robichaud, a nuclear security grantmaker with 20 years of experience, is what got them to pay attention to EA for the first time. All of it is, by &nbsp;itself, not enough to make a difference, and judged on its own terms will look like a failure. But all of it <i>combined</i> is what creates the possibility that more can be accomplished the next time around, and all of the time in between.</p>", "parentCommentId": null, "user": {"username": "IanDavidMoss"}}, {"_id": "oR99WpsMyXqFkdLgh", "postedAt": "2022-10-06T14:47:11.053Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>Can't we produce really good text-to-image educational videos? E.g. Eliezer's fictional writings are really fun, and have introduced many of us into this topic. Bonus point if these videos accurately predict the future, gaining us some sort of reputation.</p>", "parentCommentId": null, "user": {"username": "L3opard"}}, {"_id": "EPxbiBFk9KMdauKvL", "postedAt": "2022-10-06T19:40:28.184Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<blockquote><p>And\u2014despite valiant effort!\u2014we've been able to do approximately nothing.</p></blockquote><p>Why not?</p><p>I apologize for an amateur question but: what all have we tried and why has it failed?</p>", "parentCommentId": null, "user": {"username": "Susan II"}}, {"_id": "viYtmiLWj86FZ8adK", "postedAt": "2022-10-06T20:51:31.082Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>It's possible there's a more comprehensive writeup somewhere, but I can offer two data points regarding the removal of $30B in pandemic preparedness funding that was originally part of Biden's Build Back Better initiative (which ultimately evolved into the Inflation Reduction Act):</p><ul><li>I had an opportunity to speak earlier this summer with a former senior official in the Biden administration who was one of the main liaisons between the White House and Congress in 2021 when these negotiations were taking place. According to this person, they couldn't fight effectively for the pandemic preparedness funding because it was not something that representatives' constituents were demanding.</li><li>During his presentation at EA Global DC a few weeks ago, Gabe Bankman-Fried from Guarding Against Pandemics said that Democratic leaders in Congress had polled Senators and Representatives about their top three issues as Build Back Better was being negotiated in order to get a sense for what could be cut without incurring political backlash. Apparently few to no members named pandemic preparedness as one of their top three. (I'm paraphrasing from memory here, so may have gotten a detail or two wrong.)</li></ul><p>The obvious takeaway here is that there wasn't enough attention to motivating grassroots support for this funding, but to be clear I don't think that is always the bottleneck -- it just seems to have been in this particular case.</p><p>I also think it's true that if the administration had wanted to, it probably could have put a bigger thumb on the scale to pressure Congressional leaders to keep the funding. Which suggests that the pro-preparedness lobby was well-connected enough within the administration to get the funding on the agenda, but not powerful enough to protect it from competing interests.</p>", "parentCommentId": "EPxbiBFk9KMdauKvL", "user": {"username": "IanDavidMoss"}}, {"_id": "wsGvjny5vw5MySDEx", "postedAt": "2022-10-06T22:26:04.273Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>I would be curious how much the pandemic preparedness stuff is actually a crux. E.g. if gain of function research is restricted within the next year, would that noticeably change your estimate of how helpful a warning shot will be?</p><p>I think this is kind of testing your argument (2) \u2013 EA advisors might possibly become more influential within the next year. (And also it just takes forever to do anything in policy.)</p>", "parentCommentId": null, "user": {"username": "Ben_West"}}, {"_id": "ARYobrjHRru6j3rKQ", "postedAt": "2022-10-06T22:27:55.052Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>This post makes the case that warning shots won't change the picture <i>in policy</i> &nbsp;much, but I could imagine a world where &nbsp;some warning shot makes the leading AI labs decide to focus more on safety, or agree to slow down their deployment, without policy change occurring. Maybe this could buy a couple of years time for safety researchers?</p><p>This isn't a well developed thought, just something that came to mind while reading.</p>", "parentCommentId": null, "user": {"username": "harrygietz@gmail.com"}}, {"_id": "PMm4w3iru8A5gxxKi", "postedAt": "2022-10-06T22:57:40.238Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>Readers might be interested in the comments over <a href=\"https://www.lesswrong.com/posts/idipkijjz5PoxAwju/warning-shots-probably-wouldn-t-change-the-picture-much#comments\">here</a>, especially <a href=\"https://www.lesswrong.com/posts/idipkijjz5PoxAwju/warning-shots-probably-wouldn-t-change-the-picture-much?commentId=EcS2P4XGbmKMXGgbe\">Daniel K.'s comment</a>:</p>\n<blockquote>\n<p>The only viable counterargument I've heard to this is that the government can be competent at X while being incompetent at Y, <em>even if X is objectively harder than Y</em>. The government is weird like that. It's big and diverse and crazy. Thus, the conclusion goes, we should still have some hope (10%?) that we can get the government to behave sanely on the topic of AGI risk, especially with warning shots, despite the evidence of it behaving incompetently on the topic of bio risk despite warning shots.</p>\n</blockquote>\n<blockquote>\n<p>Or, to put it more succinctly: The COVID situation is just one example; it's not overwhelmingly strong evidence.</p>\n</blockquote>\n", "parentCommentId": null, "user": {"username": "Mauricio"}}, {"_id": "nB2xk9GGYX6dxbKwv", "postedAt": "2022-10-07T00:27:46.493Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<blockquote><p>And\u2014despite valiant effort!\u2014we've been able to do approximately nothing.</p></blockquote><p>This should update judgements on whether GOF research is as easy to influence as was thought in 2021.</p><p>Some resources I recommend on GOF research are the first two chapters of Mearshimer's Tragedy of Great Power Politics (2014) and the first two chapters of Schelling's Arms and Influence (1966).</p>", "parentCommentId": null, "user": {"username": "trevorw96"}}, {"_id": "gyYsdKeBC5hXKEQRa", "postedAt": "2022-10-07T01:00:40.524Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>Surely grassroots support for pandemic preparedness wouldn't be too hard to get, would it? Is anyone working on this? Should someone work on this?</p>\n", "parentCommentId": "viYtmiLWj86FZ8adK", "user": {"username": "Peter_Hurford"}}, {"_id": "2fw4bN3J67d87vcQw", "postedAt": "2022-10-07T01:46:25.163Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>As opposed to speaking with Congressmen, is \"prepare a scientific report and meet with the NIH director/his advisors\" an at-all plausible mechanism for shutting down the <a href=\"https://twitter.com/WilliamAEden/status/1576387289336336386\">specific research grant</a> Soares linked?</p><p>Or if not, becoming NIH <a href=\"https://grants.nih.gov/grants/peer/becoming_peer_reviewer.htm\">peer reviewers</a>?</p>", "parentCommentId": "viYtmiLWj86FZ8adK", "user": {"username": "Susan II"}}, {"_id": "6dFchNa6MvEifkLLg", "postedAt": "2022-10-07T01:51:10.959Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>Random meta point: You can now crosspost posts to the EA Forum from LW and vice-versa, which automatically adds a link to the crosspost to the top, and adds a link to the comment section on the other side to the bottom of the comment section (together with a counter of the number of comments). Seems like this would have been a bit nicer for this case.</p>", "parentCommentId": null, "user": {"username": "Habryka"}}, {"_id": "JAXgqTLmefE89qm43", "postedAt": "2022-10-07T02:53:40.937Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>I'm not aware of anyone working on it really seriously!</p>", "parentCommentId": "gyYsdKeBC5hXKEQRa", "user": {"username": "IanDavidMoss"}}, {"_id": "ZQpS6tGbKCnjz45py", "postedAt": "2022-10-07T15:08:46.701Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<blockquote><p>\"I think the second view is basically correct for policy in general, although I don't have a strong view yet of how it applies to AI governance specifically. One thing that's become clear to me as I've gotten more involved in institution-focused work and research is that large governments and other similarly impactful organizations are huge, sprawling social organisms, such that I think EAs simultaneously underestimate and overestimate the amount of influence that's possible in those settings.\"</p></blockquote><p>&nbsp;</p><p>This is a problem I've spoken often about, and I'm currently writing an essay on for this forum based on some research I co-authored.&nbsp;</p><p>People <strong>wildly</strong> underestimate how hard it is to not only pass governance, but make sure it is abided to, and to balance the various stakeholders that are required. The AI Governance field has a <strong>massive</strong> sociological, socio-legal, and even ops-experience gap that means a lot of very good policy and governance ideas die in their infancy because no-one who wrote them have any idea how to enact them feasibly. My PhD is on the governance end of this and I do a bunch of work within government AI policy, and I see a<strong> lot</strong> of very good governance pitches go <i>splat </i>against the complex, ever-shifting beast that is the human organisation purely because the researchers never thought to consult a sociologist, or incorporate any socio-legal research methods.</p>", "parentCommentId": "ijNHq9if8dtGEZQro", "user": {"username": "Luke Chambers"}}, {"_id": "fXytuHk9kTmB2geLF", "postedAt": "2022-10-09T14:39:06.969Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>To be honest, Nate\u2019s analysis about the hope for government action sometimes comes across as if it\u2019s from someone who never studied political economy or other poli-sci topics, assumed governments would act rational, and then concludes governments are hopeless when that assumption is flawed.</p>\n<p>If you started out with the assumption that governments are nationally rational and fast-acting, then yes you obviously need to update away from that in light of the lack of COVID response (and many other examples predating COVID). And yes you do need to have some plausible causal chains for success.</p>\n<p>But COVID definitely shouldn\u2019t be assumed as proof of hopelessness (or other pessimistic claims like \u201cWarning Shots Probably Wouldn't Change The Picture Much\u201d), if only given examples of where government did take sensible or at least strong actions in response to threats/events (sometimes without them even occurring). See for example: 9/11,  Y2K, Nunn-Lugar Cooperative Threat Reduction program, the asteroid tracking system, etc. Some of these have even been analyzed in posts here on the EA Forum! (Plausibly also worth learning about MADD: Mothers Against Drunk Driving)</p>\n<p>Ultimately, factors such as \u201cProbability of affecting the decision-makers personally,\u201d \u201cexisting options for response,\u201d \u201ctemporal sharpness of harm,\u201d \u201cgood political narratives,\u201d \u201cinfluential advisors,\u201d etc. matter. Are those things guaranteed for AI? No, but I don\u2019t think this COVID situation is a good/sufficient reason to assume that we can\u2019t act rationally in response to a clear warning shot for AI.</p>\n<p>Models of success should reflect this uncertainty clearly in their reasoning/modeling, in case later analysis shows this to be false (i.e., warning shots for AI probably won\u2019t matter).</p>\n", "parentCommentId": null, "user": {"username": "Harrison D"}}, {"_id": "PgxDcyBSXuJCbDko7", "postedAt": "2022-10-13T02:28:50.611Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>I'll just note that I have a prediction market on this <a href=\"https://manifold.markets/IsaacKing/will-there-be-a-fire-alarm-for-agi\">here</a>, which is currently at a 7% chance of some prominent event causing mainstream AI capabilities researchers to start taking the risk more seriously by 2028.</p>", "parentCommentId": null, "user": {"username": "Isaac King"}}, {"_id": "6FSbjTNtvPnhG7Rkq", "postedAt": "2022-10-15T19:16:35.456Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<blockquote><p>To be honest, Nate\u2019s analysis about the hope for government action sometimes comes across as if it\u2019s from someone who never studied political economy or other poli-sci topics, assumed governments would act rational, and then concludes governments are hopeless when that assumption is flawed.</p></blockquote><p>Nate thinks we should place less of our hope and focus on governments, and more of it on corporations; but corporations obviously aren't perfect rational actors either.</p><p>This isn't well predicted by \"perfect rational actor or bust\", but it's well predicted by \"Nate thinks the problem is at a certain (high) level of difficulty, and the best major governments are a lot further away from clearing that difficulty bar than the best corporations are\".</p><p>From Nate's perspective, AGI is a much harder problem than anything governments have achieved in the past (including the good aspects of our response to nuclear, Y2K, 9/11, and asteroids). In order to put a lot of our hope in sane government response, there should be clear signs that EA intervention can cause at least one government to perform <i>better than any government ever has in history</i>.</p><p>COVID's relevance here isn't \"a-ha, governments failing on COVID proves that they never do anything right, and therefore won't do AGI right\"; it's \"we plausibly won't get any more opportunities (that are at least this analogous to AGI risk) to test the claim that EAs can make a government perform dramatically better than they ever have before; so we should update on what data we have (insofar as we even need more data for such an overdetermined claim), and pin less of our hopes on government outperformance\".</p><p>If EAs can't even get governments to perform as well as they have <i>on other problems</i>, in the face of an biorisk warning shot, then we've failed much more dramatically than if we'd <i>merely</i> succeeded in making a government's response to COVID as sane as its response to the Y2K bug or the collapse of the Soviet Union.</p><p>(This doesn't mean that we should totally give up on trying to improve government responses \u2014 marginal gains might help in some ways, and unprecedented things do happen sometimes. But we should pin less of our hope on it, and treat it as a larger advantage of a plan if the plan doesn't require gov sanity as a point of failure.)<br><br>Are there other things you think show Nate is misunderstanding relevant facts about gov, that aren't explained by disagreements like \"Nate thinks the problem is harder than you do\"?</p>", "parentCommentId": "fXytuHk9kTmB2geLF", "user": {"username": "RobBensinger"}}, {"_id": "FHJJop3wBkozcWqxg", "postedAt": "2022-10-15T19:43:30.144Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>Re \"AGI is a harder problem\", see Eliezer's <a href=\"https://www.lesswrong.com/posts/cCrpbZ4qTCEYXbzje/ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts\">description</a>:</p><blockquote><p>[...] I think there's a valid argument about it maybe being more possible to control the supply chain for AI training processors if the global chip supply chain is narrow (also per Carl).</p><p>It is in fact a big deal about nuclear tech that uranium can't be mined in every country, as I understand it, and that centrifuges stayed at the frontier of technology and were harder to build outside the well-developed countries, and that the world ended up revolving around a few Great Powers that had no interest in nuclear tech proliferating any further.</p><p>Unfortunately, before you let that encourage you too much, I would also note it was an important fact about nuclear bombs that they did not produce streams of gold and then ignite the atmosphere if you turned up the stream of gold too high with the actual thresholds involved being unpredictable.</p><p>[...]</p><p>I would be a lot more cheerful about a few Great Powers controlling AGI if AGI produced wealth, but more powerful AGI produced no more wealth; if AGI was made entirely out of hardware, with no software component that could be keep getting orders of magnitude more efficient using hardware-independent ideas; and if the button on AGIs that destroyed the world was clearly labeled.</p><p>That does take AGI to somewhere in the realm of nukes.</p></blockquote>", "parentCommentId": "6FSbjTNtvPnhG7Rkq", "user": {"username": "RobBensinger"}}, {"_id": "sX6T7GqBTrfzcYTZo", "postedAt": "2022-10-23T22:57:21.037Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>I think the Pandemic Prevention Network, formerly No More Pandemics, is active in this space. (Definitely in the UK, maybe some work in the US?) More info on them from:</p><ul><li><a href=\"https://www.pandemicpreventionnetwork.org\">Their website</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/pnsxzyLEp9RiNqNpM/no-more-pandemics-a-grassroots-group?view=postCommentsNew&amp;postId=pnsxzyLEp9RiNqNpM\">This EA Forum post</a> by founder Sanjay Joshi&nbsp;</li><li><a href=\"https://www.youtube.com/watch?v=FsJsp3Ubw28\">Sanjay's talk</a> at EAGxOxford in March 2022</li></ul><p>On the other hand, as of October 23 2022, &nbsp;<a href=\"https://www.pandemicpreventionnetwork.org/volunteer\">this page</a> on their site states \"Our operations are currently on hold,\" so maybe they're not active at the moment.</p>", "parentCommentId": "gyYsdKeBC5hXKEQRa", "user": {"username": "ryanywho"}}, {"_id": "p5YrMYbLthBitaKyW", "postedAt": "2022-11-01T08:14:28.967Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>I just wanted to share as my experience was so radically different from yours. Based in the UK during the pandemic &nbsp;I felt like:</p><ul><li><strong>No one in was really doing anything to try to \"make the government sane around biorisk\"</strong>. I published <a href=\"https://www.cser.ac.uk/media/uploads/files/Risk_Report_FINAL_-_Nov_2021.pdf\">a paper</a> targeted at government on managing risks. I remember at the time (in 2020) it felt like no one else was shifting to focus on policy change on lessons learned from COVID.</li><li><strong>When I tried doing stuff it went super well. </strong>As mentioned <a href=\"https://forum.effectivealtruism.org/posts/xX2JKae8GyzdY98Zn/appg-for-future-generations-impact-report-2020-2021#A_government_commitment_review_the_approach_to_risk_identification_\">here&nbsp;</a> (and <a href=\"https://forum.effectivealtruism.org/posts/AWKk9zjA3BXGmFdQG/appg-on-future-generations-impact-report-raising-the-profile-1#Improving_the_UK_s_risk_management___ongoing\">here</a>) this work went much better than expected. The government seemed willing to update and commit to being better in future.</li></ul><p>&nbsp;I came away from the situation with a feeling that influencing policy was easy and impactful and neglected and hopefully about what policy work could achieve \u2013 but just disappointed that not more was being done to &nbsp;\"make the government sane around biorisk\".<br>&nbsp;</p><p>This leads me to questions <strong>Why are our experiences so different? </strong>Some hypothesis that I have are:</p><ul><li>Luck / randomness \u2013 maybe I was lucky or US advocates were unlucky and we should assume the truth lies in the middle.</li><li>Different country \u2013 the US is different, harder to influence, or less sane than some (or many) other places.</li><li>Different methodology \u2013 The standard policy advocacy sector really sucks, it is not evidence based and there is little M&amp;E. It might be that advocacy run in an impact-focused way (like was happening in the UK) is just much better than funding standard advocacy organisations (which I guess was happening in the US). See discussion on this <a href=\"https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal?commentId=LSu8wkjeHHHSnnc9C\">here</a>.</li><li>Different amount of work \u2013 y<strong>our post mentions a \"valiant effort\" was made, but does not evidence this. </strong>This makes it hard to form an opinion on what works and why. Would be great to get an answer to this (see <a href=\"https://forum.effectivealtruism.org/posts/8tosr8XzqvYXR9ckB/warning-shots-probably-wouldn-t-change-the-picture-much?commentId=EPxbiBFk9KMdauKvL\">Susan's comment</a>) e.g. links to a few campaigns in this space.&nbsp;</li></ul><p>Grateful for your views.</p>", "parentCommentId": null, "user": {"username": "weeatquince"}}, {"_id": "jbhwPqHaA8zGuxbQu", "postedAt": "2022-11-01T08:30:02.534Z", "postId": "8tosr8XzqvYXR9ckB", "htmlBody": "<p>I don\u2019t follow the US pandemic policy but wasn\u2019t some $bn (albeit much less than $30bn) still approved for pandemic preparedness and isn't more still being discussed (a very quick google points to $0.5b <a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2022/06/30/statement-by-president-biden-on-pandemic-preparedness-prevention-and-response-fund-at-the-world-bank/\">here</a> and $2b <a href=\"https://bioprocessintl.com/bioprocess-insider/global-markets/biden-pledges-2bn-to-launch-us-biomanufacturing-initiative/\">here</a> etc and I expect there is more)? If so that seems like a really significant win.<br><br>Also your reply was about government, not about EA or adjacent organisations. I am not sure anyone in this post / thread has given any evidence of any \"a valiant effort\" yet, such as listing campaigns run or even policy papers written etc. The only post-COVID policy work I know of (in the UK, see comment below) seemed very successful and I am not sure it makes sense to update against \"making the government sane\" without understanding what the unsuccessful campaigns have been. (Maybe also <i>Guarding Against Pandemics</i>, are they doing stuff that people feel ought to have had an impact by now, and has it?)</p>", "parentCommentId": "viYtmiLWj86FZ8adK", "user": {"username": "weeatquince"}}]