[{"_id": "rrhsdZzEkT3Mzbsxc", "postedAt": "2023-12-23T09:07:53.870Z", "postId": "Jdx9TJu6rgnpKB4kh", "htmlBody": "<p>I don't know who DeBoer is, but I enjoyed reading about why being weird is practically inevitable even from a historical perspective if you really want to do good. Thanks!</p>\n", "parentCommentId": null, "user": {"username": "MathieuSpil"}}, {"_id": "TrC6R2SMJxAafysKv", "postedAt": "2023-12-23T10:51:01.710Z", "postId": "Jdx9TJu6rgnpKB4kh", "htmlBody": "<p>Thanks for writing this - I was planning to but as usual my Forum post reach exceeded my grasp. I also just found it to be a bad piece tbh, and DeBoer to be quite nasty (see Freddie's <a href=\"https://open.substack.com/pub/benthams/p/freddie-deboer-is-wrong-about-effective?utm_campaign=comment-list-share-cta&amp;utm_medium=web&amp;comments=true&amp;commentId=44484410\">direct \"response\"</a> for example)</p><p>But I think what I find most astounding about this, and the wave of EA critiques in the wider blogosphere (inc big media) over the last few months is how much they make big claims that seem obviously false, or provide no evidence to back up their sweeping claims. Take this example:</p><blockquote><p>If you\u2019d like a more widely-held EA belief that amounts to angels dancing on the head of a pin, you could consider effective altruism\u2019s turn to an obsessive focus on \u201c<a href=\"https://www.vox.com/future-perfect/23298870/effective-altruism-longtermism-will-macaskill-future\">longtermism</a>,\u201d in theory an embrace of future lives over present ones and in practice a fixation on the potential dangers of apocalyptic artificial intelligence.</p></blockquote><p>Like this paragraph isn't argued for. It just states and provides no evidence for EA focusing on longtermism, that focus being obsessive, that theoretically it leads one to embrace future lives over the present, and than in practice it leads to a fixation of AI. Even if you think he's right, you have to provide some goddamn evidence.</p><p>Then later:</p><blockquote><p>Still, utilitarianism has always been subject to simple hypotheticals that demonstrate its moral failure. Utilitarianism insists...</p></blockquote><p>What follows, I'm afraid, is not a thorough literature review of Meta/Normative/Applied Ethics, or intellectual histories of arguments for Utilitarianism, or as you (and Richard Chappell) point out the difference between Utilitarianism, Consequentialism, and Beneficentrism.&nbsp;</p><p>Or later:</p><blockquote><p>I will, however, continue to oppose the tendentious insistence that any charitable dollars for the arts, culture, and beauty are misspent.</p></blockquote><p>I have no idea what claim this is responding to or the idea that this is representative of EA somehow?</p><p>All in all Freddie seems to mix up some just simply false empirical claims (in practice EAs do mostly <i>X</i>, or that trying to do good impartially is super common behaviour for everyone around the world) or just appeals to his own moral intuition (this set of stuff <i>Y </i>that EA does is good but you don't need EA, and all this other stuff is just self-evidently ridiculous and wrong)</p><p>Funnily enough, it's an example of EA criticism being a shell game, rather than EA. Like in the comments of DeBoer's article and in <a href=\"https://www.astralcodexten.com/p/contra-deboer-on-movement-shell-games/comments\">Scott's reply</a> people are having tons of arguments about EA, but very few are litigating the merits of DeBoer's actual claims. It reminds me of the inconsistency between Silicon Valley e/acc critiques of EA and more academic/leftist critiques of EA. The former calls us communists and the other useful idiots of the expoitative capitalist class. We can't be both!</p><p>Anyway, just wanted to add some ammunition to your already good post. Keep on pushing back on rubbish critiques like this where you can Omnizoid, I'll back you up where I can.</p>", "parentCommentId": null, "user": {"username": "JWS"}}, {"_id": "abyiHpKy5LJygrXtX", "postedAt": "2023-12-23T17:04:00.305Z", "postId": "Jdx9TJu6rgnpKB4kh", "htmlBody": "<blockquote><p>quickly you discover that [the specifics of the EA program] are a series of tendentious perspectives on old questions, frequently expressed in needlessly-abstruse vocabulary and often derived from questionable philosophical reasoning that seems to delight in obscurity and novelty</p></blockquote><p>He doesn't talk or quote specifics, as if to shield his claim from analysis. \"tendentious\"? \"abstruse\"? He's complaining that I, as an EA, am \"abstruse\" meaning obscure/difficult to understand, but <strong>I'm</strong> the one that has to look up <strong>his</strong> words in the dictionary. As for how EAs \"seem\", if one doesn't try to understand them, they may \"seem\" different than they are.</p><blockquote><p>EA leads people to believe that hoarding money for interstellar colonization, is more important than feeding the poor.</p></blockquote><p>Hmm, I've been around here awhile and I recall no suggestions to hoard money for interstellar colonization. Technically I haven't been <i>feeding</i> the poor \u2015 I just spent enough on AMF to statistically save one or two children from dying of malaria. But I'm also trying to figure out how AGI ruin might play out and whether there's a way to stop it, so I assume deBoer doesn't like this for some reason. The implication of the title seems to be that because I'm interested in the second thing, I'm engaged in a \"Shell Game\"?</p><blockquote><p>researching EA leads you to debates about how sentient termites are</p></blockquote><p>I haven't seen any debates about that. Maybe deBoer doesn't want the question raised at all? Like, when he squishes a bug, it bothers him that anyone would wonder whether pain occurred? I've seen people who engage in \"moral obvious-ism\": \"whatever my moral intuitions may be, they are obviously right and yours are obviously wrong\". deBoer's anti-EA stance might be simply that.</p><blockquote><p>I\u2019ve pointed to the EA argument, which I assure you <a href=\"https://blog.practicalethics.ox.ac.uk/2023/03/why-preventing-predation-can-be-a-morally-right-cause-for-effective-altruism/\">sincerely exists</a>, that we should push all carnivorous species in the wild into extinction, in order to reduce the negative utility caused by the death of prey animals. (This would seem to require a belief that prey animals dying of disease and starvation is superior to dying from predation, but ah well.) I pick this, obviously, because it\u2019s an idea that most people find self-evidently ludicrous</p></blockquote><p>The second sentence there is surely inaccurate, but the third is the crux of the matter: he claims it's \"self-evidently ludicrous\" to think extinction of predators is preferable to the suffering and death of prey. It's an appeal-to-popularity fallacy: since the naturalistic fallacy is very popular, it is right. But also, deBoer implies, since <i>one</i> EA argues this, it's evidence that the <i>entire movement</i> is mad. Like, is debate not something intellectuals should be doing?</p><blockquote><p>\u201cwhat\u2019s distinctive about EA is that\u2026 its whole <i>purpose</i> is to shine light on important problems and solutions in the world that are being neglected.\u201d But that isn\u2019t distinctive at all! Every do-gooder I have ever known has thought of themselves as shining a light on problems that are neglected. So what?</p></blockquote><p>So, maybe he's never met anyone who did mainstream things like give to cancer research or local volunteering. But it's a straw man anyway, since he simply ignores key elements of EA like tractability, comparing different causes with each other via cost-effectiveness estimates and prioritization specialists, etc.</p><blockquote><p>\u201cLet\u2019s be effective in our altruism,\u201d \u201clet\u2019s pursue charitable ends efficiently,\u201d \u201clet\u2019s do good well\u201d - however you want to phrase it, that\u2019s not really an intellectual or political or moral project, because no one could object to it. There is no content there</p></blockquote><p>Yet he <strong>is</strong> objecting to it, and there are huge websites filled with the EA content which... counts as \"no content\"?</p><p>But oh well, haters gonna hate. Wait a minute, didn't <a href=\"https://www.astralcodexten.com/p/contra-deboer-on-movement-shell-games\">Scott Alexander respond to this already</a>?</p>", "parentCommentId": null, "user": {"username": "dpiepgrass"}}, {"_id": "LeFEFvXDS5kkLuHoZ", "postedAt": "2024-01-02T14:03:24.831Z", "postId": "Jdx9TJu6rgnpKB4kh", "htmlBody": "<p>I don't fully agree with DeBoer but am much more sympathetic to his views than yourself. Some respectful pushpack on some of your points:</p><p>&nbsp;</p><blockquote><p>But if you look at what EAs actually recommend, they very much do not recommend defrauding lots of people</p></blockquote><p>It's true EA does not reccommend fraud but I think we underappreciate EAs role in having encouraged SBF (and the other EAs assosciated with him) to walk down that path. By all accounts, SBF was a very moral person who cared strongly about animals and was set on a career in animal welfare before he was persuaded by William MacAskill to work in crypto on EA grounds. MacAskill made that encouragement and stuck by him even though</p><ul><li>Many cryptocurrencies have high carbon emissions</li><li>cryptocurrency having questionable utility to society</li><li>FTX advertising complex financial products to unsophisticated retail investors in an unethical way (super bowl ads with celebrities)</li></ul><p>&nbsp;</p><blockquote><p>If everyone supports effective charities, why does the Against Malaria Foundation get such a small percentage of charitable funding?</p></blockquote><p>I don't disagree with the gist of your point about people being ineffective, but I think this specific example doesn't work because by definition the most effective charities have to be receiving a small percentage of funding, otherwise they would no longer be neglected.</p><p>&nbsp;</p><blockquote><p>Is it really plausible that huge numbers of people have looked into it and concluded that the GiveWell top charities are ineffective?</p></blockquote><p>Many people just don't know GiveWell exists. Or in fact, they think they are using something as good as GiveWell (e.g. Charity Navigator).</p><p>&nbsp;</p><blockquote><p>DeBoer\u2019s final point involves questioning why one should align oneself with the movement. Why not just like do charitable things effectively? This is, I think, less important than most of his critique. If you don\u2019t call yourself an effective altruist but give 10% of your income to effective charities, take a high-impact career, are vegan, and give away your kidney, I don\u2019t think you\u2019re doing anything wrong. In fact, I\u2019d consider you to be basically an EA in spirit, even if not in name.</p></blockquote><p>I disagree that this is not an important part of DeBoer's critique. DeBoer is stressing that by disassosciating yourself from \"Effective Altruism\", you can continue doing the <i>good parts of</i> effective altruism, without the baggage of the <i>bad parts</i>. The bad parts DeBoer describes as things like longtermism, hypotheticals, book promotions, the castle. If you are someone like DeBoer who sees those things as the bad parts of EA, then there probably is value in distancing yourself from EA, as it lets you continue your good deeds without inadvertently supporting the parts of EA you think are misguided.</p>", "parentCommentId": null, "user": {"username": "Mohammad Ismam Huda"}}]