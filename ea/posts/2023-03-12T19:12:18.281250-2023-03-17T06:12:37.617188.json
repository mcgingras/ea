[{"_id": "dyyXcdgBchGczruJq", "title": "Donation offsets for ChatGPT Plus subscriptions", "postedAt": "2023-03-16T23:11:18.163Z", "htmlBody": "<p>I've decided to donate $240 to both GovAI and MIRI to offset the $480 I plan to spend on ChatGPT Plus over the next two years ($20/month).<br><br>I don't have a super strong view on ethical offsets, like donating to anti-factory farming groups to try to offset harm from eating meat. That being said, I currently think offsets are somewhat good for a few reasons:<br><br>They seem much better than simply contributing to some harm or commons problem and doing nothing, which is often what people would do otherwise.<br><br>It seems useful to recognize, to notice, when you're contributing to some harm or commons problem. I think a lot of harm comes from people failing to notice or keep track of ways their actions negatively impact others, and the ways that common incentives push them to do worse things.<br><br>A common Effective Altruism argument against offsets is that they don't make sense from a consequentialist perspective. If you have a budget for doing good, then spend your whole budget on doing as much as possible. If you want to mitigate harms you are contributing to, you can offset by increasing your \"doing good\" budget, but it doesn't make sense to specialize your mitigations to the particular area where you are contributing to harm rather than the area you think will be the most cost effective in general.<br><br>I think this is a decently good point, but doesn't move me enough to abandon the idea of offsets entirely. A possible counter-argument is that offsets can be a powerful form of coordination to help solve commons problems. By publicly making a commitment to offset a particular harm, you're establishing a basis for coordination - other people can see you really care about the issue because you made a costly signal. This is similar for the reasons to be vegan or vegetarian - it's probably not the most effective from a naive consequentialist perspective, but it might be effective as a point of coordination via costly signaling.<br><br>After having used ChatGPT (3.5) and Claude for a few months, I've come to believe that these tools are super useful for research and many other tasks, as well as useful for understanding AI systems themselves. I've also started to use Bing Chat and ChatGPT (4), and found them to be even more impressive as research and learning tools. I think it would be quite bad for the world if conscientious people concerned about AI harms refrained from using these tools, because I think it would disadvantage them in significant ways, including in crucial areas like AI alignment and policy.&nbsp;<br><br>Unfortunately both can be true:<br><br>1) Language models are really useful and can help people learn, write, and research more effectively<br>2) The rapid development of huge models is extremely dangerous and a huge contributor to AI existential risk<br><br>I think OpenAI, and to varying extent other scaling labs, are engaged in reckless behavior scaling up and deploying these systems before we understand how they work enough to be confident in our safety and alignment approaches. And also, I do not recommend people in the \"concerned about AI x-risk\" reference class refrain from paying for these tools, even if they do not decide to offset these harms. The $20/month to OpenAI for GPT-4 access right now is not a lot of money for a company spending hundreds of millions training new models. But it is something, and I want to recognize that I'm contributing to this rapid scaling and deployment in some way.<br><br>Weighing all this together, I've decided offsets are the right call for me, and I suspect they might be right for many others, which is why I wanted to share my reasoning here. To be clear, I think concrete actions aimed at quality alignment research or AI policy aimed at buying more time are much more important than offsets. I won't dock anyone points for not donating to offset harm from paying for AI services at a small scale. But I will notice if other people make similar commitments and take it as a signal that people care about risks from commercial incentives.<br><br>I didn't spend a lot of time deciding which orgs to donate to, but my reasoning is as follows: MIRI has a solid track record highlighting existential risks from AI and encouraging AI labs to act less recklessly and raise the bar for their alignment work. GovAI (the Center for AI governance) is working on regulatory approaches that might give us more time to solve key alignment problems. According to staff I've talked to, MIRI is not heavily funding constrained, but that they believe they could use more money. I suspect GovAI is in a similar place but I have not inquired.</p>", "user": {"username": "landfish"}}, {"_id": "ijwybRLgywP7M5XLZ", "title": "Some problems in operations at EA orgs: inputs from a dozen ops staff", "postedAt": "2023-03-16T20:32:22.608Z", "htmlBody": "<p>This is a brief summary of an operations brainstorm that took place during April 2022. It represents the views of operations staff at 8-12 different EA-aligned organizations (approximately). We split up into groups and brainstormed problems, and then chose the top problems to brainstorm some tentative solutions.&nbsp;</p><p>The aim of the brainstorming session was to highlight things that needed improvement, rather than to evaluate how good EA operations roles are relative to the other non-profit or for-profit roles. It\u2019s possible that EA organizations are not uniquely bad or good&nbsp; - but that doesn\u2019t mean that these issues are not worth addressing. The outside world (especially the non-profit space) is pretty inefficient, and I think it\u2019s worth trying to improve things.&nbsp;&nbsp;</p><p>Limitations of this data: Meta / community building (and longtermist, to a lesser degree) organizations were overrepresented in this sample, and the tallies are estimates. We didn\u2019t systematically ask people to vote for each and every sub-item, but we think the overall priorities raised were reasonable.&nbsp;</p><h1><strong>General Brainstorming</strong></h1><p>Four major themes came up in the original brainstorming session:&nbsp;<strong>bad knowledge management, unrealistic expectations, bad delegation, and lack of respect for operations</strong>. The group then re-formed new groups to brainstorm solutions for each of these key pain points.</p><p>Below, we go into a breakdown of each large issue into specific points raised during the general brainstorming session. Some points were raised multiple times and are indicated by the \u201c(x n)\u201d to indicate how many times the point was raised.&nbsp;</p><h2>Knowledge management</h2><h3>Problems</h3><p>Organizations don\u2019t have good systems for knowledge management. Ops staff don\u2019t have enough time to coordinate and develop better systems. There is a general lack of structure, clarity and knowledge.&nbsp;</p><ul><li>Issues with processes and systems (x 4)</li><li>No time on larger problems<ul><li>Lack of time to explore &amp; coordinate</li><li>Lack of time to make things easier ([you\u2019re always] putting out fires)</li></ul></li><li>[Lack of] organizational structure</li></ul><ol><li>Line management</li><li><i>Capacity to cover absences [see Unrealistic Expectations]</i></li><li>Covering / keeping the show running</li><li>Responsibilities</li><li>Working across time zones</li><li>Training / upskilling</li><li><i>Management training [see improper delegation]</i></li></ol><ul><li><strong>Lack of Clarity + Knowledge</strong><ul><li>Legal</li><li>Compliance</li><li>HR</li><li>Hiring</li><li>Wellbeing (including burnout)</li></ul></li><li>Lack of skill transfer</li><li>Lack of continuity / High turn-over of junior ops specialists</li></ul><h3>Potential Solutions</h3><ul><li>Lowering the bar - e.g. you don\u2019t need a PhD to work in ops. Pick people with less option value.</li><li>Ask people to be nice and share with others</li><li>Best practice guides shared universally. [Make them] available to people before hiring so they can understand the job better before applying, so [there\u2019s] less turn-over.<ul><li>Database? (Better ops Slack?)</li></ul></li><li>Making time to create Knowledge Management Systems - so less fire-fighting.</li><li>People higher in the organization [should have] better oversight of processes/knowledge.</li></ul><h2>Unrealistic expectations</h2><h3>Problems</h3><p>Employers have unrealistic expectations for ops professionals. Ops people are expected to do too much in too little time and always be on call.&nbsp;</p><ul><li>Lack of capacity / too much to do (x2)<ul><li>[Lack of] capacity to cover absences [from above]</li></ul></li><li>Ops people [are expected to be] \u201calways on call\u201d</li><li>Timelines for projects [are subject to the] planning fallacy, [and there are] last minute changes</li><li>Ops team [are] responsible for all new ideas that people come [up] with - could others do it?</li><li>Unrealistic expectations about<ul><li>coordination capacity</li><li>skillset</li><li>organizational memory</li></ul></li></ul><h3>Solutions</h3><ol><li><strong>Bandwidth (?)</strong><ol><li>Increase capacity</li><li>Have continuity</li><li>[give ops staff the] ability to push back on too-big asks</li></ol></li><li><strong>Recognition</strong><ol><li>Create transparency</li><li>Create intra-team comms &amp; strategy</li><li>Make the \u201cinvisible\u201d work visible</li><li>Make the manager aware of the actual size of a task</li></ol></li><li><strong>Clarity</strong><ol><li>Don\u2019t idealize the really tedious; be upfront about the nature of work</li><li>Manage expectations (both ways)</li><li>Humanize employees</li></ol></li><li><strong>??? [emotional support]</strong><ol><li>Hug it out</li><li>Make more sense</li><li>Emotional support horse</li><li>Have a good cry</li></ol></li></ol><h2>Suboptimal delegation of tasks to ops staff</h2><h3>Problems</h3><p>Employers don\u2019t use their ops staff well. Ops staff are assigned tasks that could be more easily done by other staff, or by non-EA contractors. Ops staff have to do tedious, low-skill tasks for which they are overqualified.&nbsp;&nbsp;<br>&nbsp;</p><ul><li>Getting people to do things they\u2019re overqualified for (x 3)</li><li>Ops staff assigned tasks that aren\u2019t appropriate (x 2) / Employers don't know what ops means</li><li>Tedious [jobs such as:]&nbsp;<ul><li>Having to \u201ctranslate\u201d emails to busier people</li><li>Refilling coffee/drinks [is] sisyphean</li></ul></li></ul><h3>Solutions</h3><ul><li>[Managers should have] knowledge [about] the task/responsibility.</li><li>Understand your team\u2019s capacity + skills \u2192 even having a team (something/one to delegate to).</li><li>Clarity in messaging + communication.</li><li>The \u201cneed to know\u201d e.g. can it be outsourced?</li><li>Motivate to get good/best results (standards) \u2192 [give people] context on why [the task is] important&nbsp;<ul><li>Mission buy-in.</li><li>Inclusivity ([ops staff will] feel it\u2019s mission-critical or valued).</li></ul></li><li>Interim accountability + management.</li><li>Stakeholder/management support \u2192 No need to worry whether you can access the budget<ul><li>Autonomy to take decisions to delegate.</li></ul></li><li>Ops culture establishing (respect/reasoning for resources) from the get-go.</li></ul><h2>Lack of prestige or respect for operations</h2><h3>Problem</h3><p>Employers don\u2019t respect or appreciate operations work, leading to ops workers not always being (or feeling) included in the organization.</p><ul><li>Lack of appreciation or respect (x 3)</li><li>Unclear levels of organization/inclusion - e.g. can contractors join the ops slack?</li><li>Not being invited to retreats</li><li>Recognition only comes when things go wrong</li><li>Employers think ops people have zero interest or subject matter expertise.&nbsp;</li><li>People have very specific preferences and want the ops people to take care of it so they can be more productive; [there\u2019s an] inherent assumption that the ops person\u2019s time is less valuable.</li><li>Poor management leads to low self-esteem for ops people.</li></ul><h3>Solutions</h3><ul><li>Make ops people more connected to the organization.<ul><li>Make sure they\u2019re invited to retreats, dinners, and decision-making meetings.</li></ul></li><li>Show more precise theory of change or path to impact for ops.<ul><li>Better tracking of hours saved/multiplier effect.</li></ul></li><li>Arguments for why ops people aren\u2019t easily replaceable or outsourceable.&nbsp;<ul><li>Ops Day of Appreciation to show how indispensable you are.</li></ul></li><li>Ops people should have the power to say no, since people don\u2019t respect their time.</li><li>Open Phil mentorship program with someone from a different team.</li><li>Managements that build self-esteem.</li><li>Make other skills [that ops staff have] explicit for the rest of the team.</li></ul><h2>Exclusion from decision-making processes&nbsp;</h2><p>Relatedly, many mentioned that they were not sufficiently informed about or involved in their organization\u2019s strategic decisions.</p><ul><li>Limited influence in decision-making x 2&nbsp;</li><li>Lack of sufficient overview on org\u2019s strategy/ Working in a silo with limited interaction with colleagues (x2)&nbsp;</li></ul><h2>Less frequently mentioned pain points</h2><ul><li>Poor job security for contractors (who always feel like they\u2019re on work trial) and poor work benefits</li><li>Bad leadership styles, with one person/group specifying a manager who is too \u201con my case\u201d</li><li>Lack of \u201ctraining for 10 years from now\u201d</li><li>Hiring is time consuming; it\u2019s difficult to find candidates (x2) especially with \u201cgeographical restrictions (West coast, Oxford, etc.)\u201d. Sometimes bad hiring decisions are made.&nbsp;</li><li>Miscellaneous<ul><li>Lack of creative outlets</li><li>Most tasks are hard to do off-screen</li><li>Existential anxiety</li><li>Bad events / community building\u2019</li><li>(Sometimes) Dead-end: What\u2019s above? Why is that good?</li><li>Ops less efficient</li></ul></li></ul><h1><strong>Appendix: the full general brainstorms to generate action items</strong></h1><h2>Pain Points Group #1</h2><ul><li>Lack of capacity / too much to do</li><li>Timelines for projects planning fallacy on ops / last minute changes</li><li>Lack of appreciation</li><li>Lack of employment, only contracts<ul><li>Contractor always feel like they\u2019re on work trial</li><li>Poor work benefits</li></ul></li><li>Lack of continuity / High turn-over of junior ops specialized</li><li>Getting people to do things they\u2019re overqualified for</li><li>Reinventing the wheel on processes/systems</li><li>Organization/inclusion - unclear levels (can contractors join the ops slack?)<ul><li>Not being invited to retreats</li><li>Not being included in strategic decisions that are ops-relevant.</li></ul></li><li>Lack of delegation of tasks which don't require EA context to non EAs. (E.g. EA time could be used in better than low-brow tasks).</li><li>Ops team responsible for all new ideas that people come with - could others do it?</li><li>Non-rational delegation to ops people (E.g. it would be quicker/better to do this yourself than delegate to&nbsp;an [ops person?], accounting for value &amp; time discrepancy)</li><li>Ops people \u201calways on call\u201d at walks, etc. - Should be call</li></ul><h2>Pain points Group #2</h2><ul><li>Adoption of new systems</li><li>Extensive responsibilities&nbsp;<ul><li>(Sometimes) Dead-end: What\u2019s above? Why is that good?</li><li>Lack of sufficient overview on org\u2019s strategy</li><li>Lack of skill transfer</li></ul></li><li>No time on larger problems<ul><li>Lack of time to explore &amp; coordinate</li><li>Lack of time to make things easier ([you\u2019re always] putting out fires)</li></ul></li><li>Tedious<ul><li>Having to \u201ctranslate\u201d emails to busier people</li><li>Time wasted on \u201cbelow pay grade\u201d tasks</li><li>Refilling coffee/drinks sisyphean</li><li>[someone has just drawn a picture of Sisyphus pushing his rock up a hill]&nbsp;</li></ul></li><li>Recognition only comes when things go wrong<ul><li>Researchers don\u2019t respect ops/non-technical [staff]</li><li>Zero appreciation or recognition</li></ul></li><li>Working in a silo<ul><li>Limited influence in decision-making</li><li>Lack of creative outlets</li><li>Limited interaction with colleagues</li><li>Most tasks are hard to do off-screen</li></ul></li><li>Attention needed difficult to 80/20</li><li>Existential anxiety</li></ul><h2>Pain Points Group #3</h2><ul><li>Bad events / community building</li><li>Ops less efficient</li><li>Employers don't know what ops means</li><li>Leadership styles are hard<ul><li>Delegation</li><li>Management quality</li></ul></li><li>Unrealistic expectations about<ul><li>coordination capacity</li><li>skillset</li><li>organizational memory</li></ul></li><li>Non-EAs can do this work</li><li>Siloing \u2192 lack of team-to-team communication &lt;&gt; team communications</li><li>Manager too \u201con my case\u201d</li><li>(lack of) training for 10 years from now</li></ul><h2>Pain Points Group #4</h2><ol><li><strong>Hiring</strong><ol><li>Time consuming<ol><li>Geographical restrictions (West coast, Oxford, etc.)</li></ol></li><li>Bad hiring decisions</li><li>Finding/reaching candidates</li></ol></li><li><strong>Lack of Project Management Skills</strong><ol><li>Using the same systems in the same way</li><li>Knowing what system to use<ol><li>Complex or simple</li><li>Relative to the task/project</li></ol></li><li>Setting it up \u201cright\u201d</li></ol></li><li><strong>Setting organizational structure</strong><ol><li>Line management</li><li>Capacity to cover absences</li><li>Covering / keeping the show running</li><li>Responsibilities</li><li>Working across time zones</li><li>Training / upskilling</li><li>Management training</li></ol></li><li><strong>Lack of Clarity + Knowledge</strong><ol><li>Legal</li><li>Compliance</li><li>HR</li><li>Hiring</li><li>Wellbeing<ol><li>Burnout</li></ol></li></ol></li><li><strong>Change management within the organizations</strong><ol><li>Onboarding (capacity/time)</li><li>Software</li></ol></li></ol><p>&nbsp;</p><p><i>Thanks to Amber for help cleaning up and organizing these notes!&nbsp;</i></p>", "user": {"username": "vaidehi_agarwalla"}}, {"_id": "9B4LqMx5ZDagXMrpK", "title": "Metaculus's Climate Tipping Points Tournament Enters Round 2", "postedAt": "2023-03-16T18:48:45.133Z", "htmlBody": "<p><a href=\"https://www.metaculus.com/tournament/climate/?search=cat:environment--climate--tipping-points-round-2\">Round 2 of the Climate Tipping Points Tournament is now live</a>. Metaculus has launched new policy-focused questions that further develop the Round 1 indicator categories identified as most critical. This round also includes conditional questions designed to quantify the impacts of specific policies or events coming to pass.</p><h3>$2,500 Commentary Prize</h3><p>Round 2 questions are eligible for $2,500 in prizes awarded for comments that:</p><ul><li>are well-reasoned and clear</li><li>identify mistakes in the community\u2019s reasoning or clearly explain why the author\u2019s forecast differs from the community\u2019s</li><li>provide a detailed explanation of the impact of a parent question on a child question in a pair of conditional questions</li></ul><p>Round 2 questions include the Round 2 tag and can be found <a href=\"https://www.metaculus.com/questions/?search=cat:environment--climate--tipping-points-round-2\">here</a>.</p><p>Comments must be submitted before July 1st to be eligible. Only Round 2 questions are eligible for the Commentary Prize. See the <a href=\"https://www.metaculus.com/tournament/climate/#commenting\">tournament page</a> for more details.</p><h3>$2,500 Accuracy Prize</h3><p>Both Round 2 and <a href=\"https://www.metaculus.com/questions/?search=cat:environment--climate--tipping-points-round-1\">Round 1 questions</a> that resolve before March 1st, 2027 are eligible for the accuracy prize, and we will launch an additional wave of Round 2 questions in the coming weeks.</p>", "user": {"username": "christianM"}}, {"_id": "Cd8wM4jZPnaAA8vwX", "title": "[Linkpost] Why pescetarianism is bad for animal welfare - Vox, Future Perfect", "postedAt": "2023-03-16T15:33:05.385Z", "htmlBody": "<p>In my debut for Vox, I write about why switching to a pescetarian diet for animal welfare reasons is probably a mistake.&nbsp;</p><p>I was motivated to reduce animal consumption by EA reasoning. I initially thought that the moral progression of diets was something like vegan &gt; vegetarian &gt; pescetarian &gt; omnivore. But I now think the typical pescetarian diet is worse than an omnivorous one. (I was actually convinced in part by an EA NYC talk by Becca Franks on fish psychology.)</p><p>Why?&nbsp;</p><ol><li>Fish usually eat other fish, and they're smaller on average than typical farmed animals.</li><li>The evidence for their sentience is much stronger than I previously thought. I think my credence is now something like P(pig/cow sentience) = 99.99%, P(chicken/fish sentience) = 99%</li></ol><p>Given that there are ~30k fish species, generalizing about them is a bit tricky, but I think the evidence of fish sentience is about as strong as the evidence for chicken sentience, something I would guess more people accept.</p><p>I also spend time discussing:</p><ul><li>environmental impacts of fishing</li><li>consumer choice vs. systemic change</li><li>shrimp welfare</li></ul>", "user": {"username": "Garrison"}}, {"_id": "uhzFDqimEgPTrndRe", "title": "Forecasting in the Czech public administration - preliminary findings", "postedAt": "2023-03-16T14:47:28.670Z", "htmlBody": "<h3>TL;DR</h3><ul><li>When communicating forecasting with policy partners, it is useful to have a good understanding of other foresight methods as well (horizon scanning, backcasting, scenario planning, etc). The pool of practically forecastable policy questions is somewhat smaller than we expected, but other methods can often be used.</li><li><u>Forecasting side</u>: It might be more effective to split the current forecasting models into 1/ tournaments to identify talent using short-term, attractive questions and 2/ an advisory group of proven forecasters to swiftly predict policy-relevant questions (that are often more demanding, less attractive, and long-term).</li><li><u>Policymaking side</u>: Epistemically curious public officials thinking about the future in probabilistic terms (scout mindset + future mindset + probabilistic mindset) are crucial for success. Without them, bringing forecasting into policy might be possible indirectly, e. g. by raising awareness through the media or NGOs.</li></ul><h2>Intro</h2><p>In this post, we discuss some of our preliminary findings from the&nbsp;<a href=\"https://www.ceskepriority.cz/forpol/eng\"><u>FORPOL</u></a> (Forecasting for Policy) project run by Czech Priorities in cooperation with Metaculus. Our main goal is to provide other organizations with real-life experiences and recommendations aimed at making forecasting more policy-relevant and accepted by the public administration.</p><p>In Czech Priorities (EA-aligned NGO), we consider ourselves to be in a good position to advance this knowledge for three main reasons: 1/ we have experience in organizing large-scale local forecasting tournaments, 2/ the Czech Republic is a small enough country for us to have developed good connections with government analysts and politicians at most of the public institutions, and 3/ our team has a strong consulting expertise.</p><p>Based on expert consultations and preparatory work in the summer of 2022, we developed a strategy of individually reaching out to a large number of selected policy partners (in forecasting-relevant areas), working with them intensively, and monitoring the interactions.</p><p>Contrary to what might be expected, we did not work with organizations in the realms of foreign policy or the local intelligence community, as we wanted to explore possible use cases beyond these areas and we have stronger links to potential \u201cchampions\u201d of forecasting (i.e. senior officials) in other governmental ministries, departments, and agencies.&nbsp;</p><p>The expected value of our approach lies in delivering impact and success stories and, more importantly, in understanding precisely what works in various situations and interactions. This will be the main content of our study, which will come out in September 2023. Until then, some of our previous suggestions on how to use forecasting tournaments in policy can also be found in our&nbsp;<a href=\"https://options.ceskepriority.cz/files/methodology_forecasting_EN.pdf\"><u>methodology manual</u></a> (\u010cesk\u00e9 priority, 2021).</p><h2>Our work so far</h2><p>These are the Czech institutions (12 public inst., 2 NGOs, 1 foreign ministry) that we have partnered with so far, and the topics we forecasted for them (2-4 predictions per topic):</p><ol><li><u>Ministry of Regional Development:</u> Accommodation for Ukrainian refugees</li><li><u>Ministry of Regional Development (different section)</u>: Urbanization</li><li><u>Ministry of Education</u>: Deferrals in Czech elementary schools</li><li><u>\u03a0 Institute (Institute of the Pirate political party)</u>: Digitization of the society</li><li><u>Ministry of Health</u> - The fade-out of Covid-19 (tests, vaccines, &amp; hospitalizations)</li><li><u>(undisclosed)</u> - Labor market</li><li><u>STEM (Public polling agency &amp; research institute)</u> - Perceptions of security &amp; trust</li><li><u>Ministry of Education</u> - The demand for teachers and principals</li><li><u>Czechitas (NGO providing IT courses for women)</u> - Women in the IT industry</li><li><u>Slovak Ministry of Finance</u> - The future of the European economy</li><li><u>Ministry of Labour and Social Affairs</u> - Insolvency &amp; Foreclosures</li><li><u>Ministry of Trade and Industry</u> - The future of Industry</li><li><u>Ministry of Education</u> - Lifelong learning</li><li><u>Technological Agency (main public R&amp;I funder)</u> - Financing science and R&amp;I</li><li><u>Ministry of Justice</u> - Forensic experts</li></ol><p>On a broader scale, we discussed the potential and the need for the use of foresight methods in public policy with many other stakeholders. This included the newly elected president Petr Pavel (the conversations took place during his campaign), who remains interested in using participative foresight methods to predict national risks &amp; opportunities.</p><p>Our tournament is still running and we are in the process of helping selected policy partners in leveraging the predictions, but we wanted to share some of the findings early on. Our model of cooperation with policy partners has three main phases, along the lines of which we structure the remainder of this post.</p><ol><li><u>Scoping</u> - Multiple meetings to specify the real needs, motivations, and bottlenecks, explain the Theory of Change, and discuss the future use of the predictions.</li><li><u>Forecasting</u> - Submitting questions (2-4 in each thematic \u201cchallenge\u201d) to the platform, where an average of 100 forecasters discuss &amp; predict for 3 weeks.</li><li><u>Implementation</u> - Delivering a written report with aggregate predictions and the main identified causalities or drivers, and organizing \u201cimplementation follow-ups\u201d.</li></ol><h3>1. Scoping</h3><ul><li>In the scoping process, it seems useful to&nbsp;<strong>discuss the partner\u2019s needs before</strong> explaining how forecasting questions should look.<ul><li>When discussing that later, it is also helpful to show examples of questions that might already be relevant to them. They are also often curious about the characteristics of the forecasters as a group (mostly men, ages 30-40 and graduates), but in our experience, this was never the reason for rejection.</li></ul></li><li>The predictions should&nbsp;<strong>feed into ongoing analytical or strategic work</strong>.<ul><li>In cases where partners identified topics that would be generally useful for their work or support their argumentation, the incentive to use the prediction was usually not strong enough to be translated into action. Until forecasting itself becomes a widely requested analytical input, injecting probabilistic forecasts and rationales into contracted analytical work seems reasonable.</li><li>For example, when the Russian invasion of Ukraine started, it became clear that many of the refugees would flee to the Czech Republic. Our forecasters quickly made a forecast, and we used it to create 2 scenarios (300k and 500k incoming refugees). We then used these scenarios in our&nbsp;<a href=\"https://www.paqresearch.cz/post/integrace-ukrajinskych-uprchl%C3%ADku-v-cr-2022\"><u>joint analysis</u></a> with PAQ Research on how to effectively integrate the Ukrainian immigrants. The study was then used by multiple Czech ministries in creating programs of support for housing, education, and employment. This happened at a time when widely circulated estimates spoke of tens of thousands of such arrivals by the end of 2022. In reality, it was over 430 thousand people.</li></ul></li><li><strong>A trusting personal relationship</strong> is very helpful and a&nbsp;<strong>lot of guidance</strong> is necessary, especially when the partner did not actively reach out.<ul><li>Our proactive approach (reaching out to policymakers) was useful and necessary to use in our project, but it demands a lot of work in the scoping phase. In addition, if a partner spends considerable time with us scoping the problem and developing good questions only to find them not very actionable at the moment, there is a risk of actually creating an aversion to forecasting.</li><li>This proactive approach has a few benefits (e.g. quick feedback loops), but in the long-term, we would suggest focusing on cultivating demand for actionable forecasts by other means too, such as the upskilling of individual analysts, or raising the prestige of communicating in probabilistic terms within and across institutions.</li></ul></li><li>An understanding of&nbsp;<strong>other foresight methods</strong> is important for the quality of advice and support, as well as for communication purposes.<ul><li>After the first series of meetings with potential policy partners, we realized the need for an even deeper understanding of other foresight methods. We leveraged our other team, who created&nbsp;<a href=\"https://www.ceskepriority.cz/files/methodical-guide.pdf\"><u>this foresight manual</u></a>. And we strengthened the framing that&nbsp;<strong>judgemental forecasting is only one of many methods of foresight.</strong></li><li>Our experience suggests that having robust expertise in foresight helps to propose (and deliver) the right method when appropriate, but also it might be a useful \"gateway\" topic, providing better access to officials who are already convinced about the need for better foresight but don\u00b4t understand forecasting yet -&nbsp;which was the case for the majority of our partners. We are aware that this finding might be slightly EU-specific, as \"foresight\" is a buzzword increasingly being used in the European Union structures.</li><li>We became part of the European JRC network of foresight practitioners, which - aside from the useful sharing of experience -&nbsp;enabled us to disseminate forecasting internationally, e.g., by organizing a calibration workshop for the EU Competence Centre on Participatory and Deliberative Democracy. We previously built our foresight reputation mainly thanks to the&nbsp;<a href=\"https://drive.google.com/file/d/1Na4ze8Depn2q29n7SaEMWmpXtSV7UAPm/view\"><u>2021 study of Global megatrends</u></a>, which now serves as a basis for, e.g, the national R&amp;I funding strategy (and where we actually used forecasting tournaments as a supplementary method to reduce noise).</li></ul></li></ul><h3>2. Forecasting</h3><ul><li>It is&nbsp;<strong>difficult to maintain engagement</strong> in relatively long, policy-relevant forecasting tournaments.<ul><li>Drop-off across the six months of our tournament is around 75% (from \u223f160 to \u223f40 weekly active participants), which is interestingly similar to the&nbsp;<a href=\"https://www.cspicenter.com/p/new-5000-prize-in-the-salem-centercspi\"><u>Salem center/SCPI forecasting tournament</u></a>. In our case, it is probably caused by three main factors: the tournament is long, many questions are complex (including a few conditional questions), and some of the questions are long-term (unresolvable by the ground truth for the purposes of rewards for participation/accuracy). Even though our top forecasters are mostly not motivated financially, the most effective explicit reward mechanism for keeping most forecasters engaged still seems to be prizes for the most informative rationales in each thematic \u201cchallenge\u201d (every 3 weeks).</li><li>Predictions that produce an<strong>&nbsp;</strong>actual impact are often very focused and aimed at very specific indicators. With such niche questions, the amount of research (and especially time investment) necessary to feel comfortable forecasting on the topic is often discouraging, limiting the discussion and increasing the role of forecasters with an already established knowledge base about the subject, who write most rationales.</li><li>Shorter tournaments and/or periodical public sharing of the results can help (a public leaderboard alone might not be enough), however, the problem is that most of the questions are not resolved in a matter of days or weeks. Therefore, some of the questions might be designed to resolve quite soon after opening to provide early feedback. These questions should be prepared by facilitators (ideally in advance) and not draw too much attention away from the other, more important questions. Once we have data for the whole duration of the tournament, one of the things we will be looking into will be the role played by negative scores in participation.</li><li>The broad range of topics covered over the course of the tournament led to a necessity to obtain substantial knowledge on different subjects, which might have been a challenge for some forecasters. However, it also gave us rather strong evidence of the curiosity and \u201cforecasting mindset\u201d - the cognitive style - of those who persisted and did well.</li></ul></li><li><strong>Know your forecasters</strong> (motivations, interests, etc.) to e.g. eliminate policy questions or topics where the final aggregate predictions might not be very reliable, as the forecasters lack inherent motivation to delve deeper.<ul><li>It is worthwhile to explore the real motivations of the forecasters and to learn more about which topics they individually consider attractive so that the questions asked in the tournament can be made more responsive to that. Moreover, understanding the main driver of the forecasters\u2019 activity likely directly relates to the eventual drop-off. Even strong financial incentives or the potential prestige of being coined a professional forecaster might not be enough if other needs are not met.</li><li>Some forecasters respond negatively to a perceived lack of control over their forecasts, e.g. in setting the granularity of probabilities or the question boundaries, even in cases where this has no material effect on the scoring or ranking within a tournament. If it can be corroborated that this trait is correlated with the \u2018forecaster\u2019 cognitive style to a significant level, this might warrant a rethinking of current forecast question generation approaches.</li></ul></li></ul><h3>3. Implementation</h3><ul><li>The involvement of&nbsp;<strong>domain experts</strong> was useful especially to increase trust and prevent future public criticism&nbsp;<ul><li>We organized two online consultations with the top domain experts, most of whom were not familiar with forecasting. These consultations were open to forecasters, who could listen to their opinions and then ask questions.</li><li>When individually reaching out to experts, some were (predictably) unwilling to give their probabilistic predictions and some argued against the very feasibility of doing so. Some were very skeptical of the benefits of having their expertise combined with the opinions of a group of forecasters.</li><li>The involvement of experts probably did not substantially increase the trust of our direct policy partners in the quality of the predictions. The names of respected experts on the final report were, however, important when communicating the report to other people (e.g. other relevant departments at the ministry) not familiar with forecasting.</li></ul></li><li>Aside from sending a written report, it is very important to&nbsp;<strong>organize a follow-up meeting</strong> for clarifications and planning of the use of the prediction. It helps to come up with concrete next steps and ideas for sharing the results further. We sometimes even draft e-mails about our results for the partners to forward.</li><li>We will report more findings from this phase in the later stages of the project.</li></ul><h2>Our next steps</h2><p>Starting in Q2/23, we plan to split our forecasting process into two separate mechanisms. We are now considering a few design choices for both the&nbsp;<strong>recruiting mechanism</strong> (long ongoing vs. short intensive tournaments) and the&nbsp;<strong>expert forecasting mechanism</strong> (general vs. topic-specific, publicly visible vs. on-demand access, etc).</p><p>We are in the process of working with policy partners, conducting individual talks with the 30+ expert forecasters who will participate in our expert forecasting team, and mapping the demand for \u201crapid predictions\u201c as a product even outside of the policymaking sector. Recently, we also had the opportunity to publish forecasts and talk in one of the largest Czech weeklies,&nbsp;<a href=\"https://www.respekt.cz/podcast/cesko-2033-neprijde-euro-ani-reforma-duchodu-nuzky-mezi-mesty-a-regiony-se-budou-rozevirat\"><u>Respekt</u></a> (podcast in Czech), and to work with a network of private schools (SCIO) on running tournaments for elementary and high school students, which are both streams that we want to continue working on.</p><p>Any advice, recommendations, or experience sharing would be appreciated, let me know! We\u2019re also happy to help or consult with folks starting similar projects.<br>&nbsp;</p>", "user": {"username": "janklenha"}}, {"_id": "Q4rg6vwbtPxXW6ECj", "title": "We are fighting a shared battle (a call for a different approach to AI Strategy)", "postedAt": "2023-03-16T14:37:16.832Z", "htmlBody": "<p><i>Disclaimer 1: This following essay doesn\u2019t purport to offer much original ideas, and I am certainly a non-expert on AI Governance, so please don\u2019t take my word for these things too seriously. I have linked sources throughout the text, and have some other similar texts later on, but this should merely be treated as another data point in people saying very similar things; far smarter people than I have written on this.&nbsp;</i></p><p><i>Disclaimer 2: This post is quite long, so I recommend reading the section on \" A choice not an inevitability\" and &nbsp;\"It's all about power\" for the core of my argument.</i></p><p>My argument essentially is as follows; under most plausible understandings of how harms arise from very advanced AI systems, be these AGI or narrow AI or&nbsp;<a href=\"http://arxiv.org/abs/2302.10329\"><u>systems somewhere in between</u></a>, the actors responsible, and the actions that must be taken to reduce or avert the harm, are broadly similar whether you care about both existential and non-existential harms from AI development. I will then further go on to argue that this calls for broad, coalitional politics of people who vastly disagree on specifics of AI systems harms, because we essentially have the same goals.&nbsp;</p><p>It's important to note that calls like these have happened before. Whilst I will be taking a slightly different argument to them,&nbsp;<a href=\"https://arxiv.org/abs/2001.04335\"><u>Prunkl &amp; Whittlestone</u></a>,&nbsp; <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2976444\"><u>Baum</u></a>,&nbsp;<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3756437\"><u>Stix &amp; Maas</u></a> and&nbsp;<a href=\"https://www.nature.com/articles/s42256-018-0003-2.pdf\"><u>Cave &amp; \u00d3 h\u00c9igeartaigh</u></a> have all made arguments attempting to bridge near term and long&nbsp; term concerns. In general, these proposals (with the exception of Baum) have made calls for narrower cooperation between \u2018AI Ethics\u2019 and \u2018AI Safety\u2019 than I will make, and are all considerably less focused on the common source of harm than I will be. None go as far as I do in essentially suggesting all key forms of harm that we worry about are incidents of the same phenomena of power concentration in and through AI. These pieces are in many ways more research focused, whilst mine is considerably more politically focused. Nonetheless, there is considerable overlap in spirit of identifying that the near-term/ethics and long-term/safety distinction is overemphasised and is not as analytically useful as is made out, as well as the intention of all these pieces and mine to reconcile for mutual benefit of the two factions.&nbsp;</p><h2><strong>A choice not an inevitability</strong></h2><p>At present, there is no AI inevitably coming to harm us. Those AIs that do will be given capabilities, and power to cause harm, by developers. If the AI companies stopped developing their AIs now, and people chose to stop deploying them, then both existential or non-existential harms would stop. These harms are in our hands, and whilst the technologies clearly act as important intermediaries, ultimately it is a human choice, a social choice, and perhaps most importantly a political choice to carry on developing more and more powerful AI systems when such dangers are&nbsp; apparent (or<a href=\"https://astralcodexten.substack.com/p/kelly-bets-on-civilization\"><u> merely plausible or possible</u></a>). The attempted development of AGI is far from value neutral,&nbsp;<a href=\"https://maxlangenkamp.substack.com/p/technology-is-not-inevitable\"><u>far from inevitable</u></a> and very much in the realm of&nbsp;<a href=\"https://www.nature.com/articles/s41599-022-01463-3\"><u>legitimate political contestation.</u></a> Thus far, we have simply accepted the right for powerful tech companies to decide our future for us; this is both unnecessary and dangerous. It's important to note that our current acceptance of the right of companies to legislate for our future is historically contingent. In the past, corporate power has been curbed, from&nbsp;<a href=\"https://www.parliament.uk/about/living-heritage/evolutionofparliament/legislativescrutiny/parliament-and-empire/parliament-and-the-american-colonies-before-1765/east-india-company-and-raj-1785-1858/\"><u>colonial era companies</u></a>,&nbsp;<a href=\"https://www.law.cornell.edu/wex/standard_oil_co._of_new_jersey_v._united_states_(1911)\"><u>Progressive Era trust-busting</u></a>,&nbsp;<a href=\"https://www.oecd.org/germany/33841373.pdf\"><u>postwar Germany</u></a> and more, and this&nbsp;<a href=\"https://www.youtube.com/watch?v=2-GbRe6oewE\"><u>could be used again</u></a>. Whilst governments have often taken a leading role, civil society has also been significant in&nbsp;<a href=\"https://journals.sagepub.com/doi/10.1177/0170840613479222#bibr139-0170840613479222\"><u>curbing corporate power</u></a> and&nbsp;<a href=\"https://scholar.google.com/scholar_lookup?title=From+streets+to+suites%3A+How+the+anti-biotech+movement+affected+German+pharmaceutical+firms&amp;author=Klaus+Weber&amp;author=L.+G.+Thomas&amp;author=Hayagreeva+Rao&amp;publication_year=2009&amp;journal=American+Sociological+Review&amp;pages=106-127&amp;doi=10.1177%2F000312240907400106#d=gs_qabs&amp;t=1678936392022&amp;u=%23p%3Dw4M6u0L-bfsJ\"><u>technology development</u></a> throughout history. &nbsp; Acceptance of corporate dominance is far from inevitable.&nbsp;</p><p>I also think it's wrong to just point the finger at humanity, as if we are all complicit in this. In reality, the development of more and more dangerous AI systems seems to essentially be driven by a very small number of corporate actors (often propped up by a&nbsp;<a href=\"https://erikhoel.substack.com/p/how-to-navigate-the-ai-apocalypse\"><u>very small number of individuals</u></a> supporting them). OpenAI seem to be committed to&nbsp;<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><u>shortening timelines as much as possible,</u></a>&nbsp;<a href=\"https://aibusiness.com/verticals/eleven-openai-employees-break-off-to-establish-anthropic-raise-124m\"><u>had half their safety team leave to form another company in response to their lax approach to safety</u></a>, and seem to see themselves as&nbsp;<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><u>essentially empowered to risk all of humanity as they see themselves as saviours of humanity.</u></a>&nbsp;<a href=\"https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/\"><u>Google sacked prominent members of the ethics team for speaking out on the dangers of LLMs</u></a>.&nbsp;<a href=\"https://arstechnica.com/tech-policy/2023/03/amid-bing-chat-controversy-microsoft-cut-an-ai-ethics-team-report-says/\"><u>Microsoft sacked an entire ethics team</u></a>, despite the ethical issues that increasing AI in their products has brought. None of these seem like the behaviour of companies that have earned the trust that society (implicitly) gives them.&nbsp;</p><h2><strong>It\u2019s all about power</strong></h2><p>Ultimately, the root cause of the harms that AI causes (whether or not you think they are existential or not), is the power that AI companies have to make&nbsp;<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><u>unilateral decisions that affect large swathes of humanity</u></a> without any oversight. They certainly don\u2019t paint their actions as political, despite clearly attempting to gain power and guide the course of humanity from their ivory towers in silicon valley. They feel empowered to<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>&nbsp;</u></a>risk huge amounts of harm&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>&nbsp;</u></a>(e.g.&nbsp;<a href=\"https://dl.acm.org/doi/pdf/10.1145/3442188.3445922\"><u>Bender and Gebru et al</u></a>,&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S2666389921000155#:~:text=Relational%20ethics%20is%20a%20process,they%20are%20embedded%20is%20erroneous.\"><u>Birhane</u></a>,&nbsp;<a href=\"https://arxiv.org/abs/2206.13353\"><u>Carlsmith</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>Yudkowsky</u></a>) by developing&nbsp;<a href=\"https://www.lesswrong.com/tag/scaling-laws\"><u>more powerful AI systems</u></a>, partially because there is little political opposition despite&nbsp;<a href=\"https://aiimpacts.org/the-public-supports-regulating-ai-for-safety/\"><u>growing public worry</u></a>. Whilst there is some mounting opposition to these companies\u2019 unsafe deployments (<a href=\"https://arxiv.org/abs/2001.06528\"><u>activism</u></a>,&nbsp;<a href=\"https://artificialintelligenceact.eu/\"><u>legislation</u></a>,&nbsp;<a href=\"https://www.mckinsey.com/industries/public-and-social-sector/our-insights/the-chips-and-science-act-heres-whats-in-it\"><u>hardware control</u></a>), there is so much further to go, in particular in restricting the research into advanced AI.</p><p>If we see it like this, whether AI is closer to a stupid \u2018<a href=\"https://dl.acm.org/doi/pdf/10.1145/3442188.3445922\"><u>stochastic parrot</u></a>\u2019 or on the&nbsp; \u2018verge-of-<a href=\"https://books.google.co.uk/books/about/Superintelligence.html?id=7_H8AwAAQBAJ\"><u>superintelligence</u></a>\u2019 doesn\u2019t really matter; whichever world we are in, it\u2019s the same processes and actors that ultimately generate the harm. The root cause, powerful, unaccountable, unregulated AI companies with little opposition playing fast and loose with risk and social responsibility, with utopian and quasi-messianic visions of their mission, causes the risk, irrespective of what you think that risk is. As capitalists like Sam Altman take in their<a href=\"https://www.bloomberg.com/news/articles/2023-01-23/microsoft-makes-multibillion-dollar-investment-in-openai\"><u> private benefits</u></a>, the rest of humanity suffers with the risk they place on all of us. Of course, these risks and harms aren\u2019t placed on everyone equally, and as always, more harm is done to the<a href=\"https://academic.oup.com/edited-volume/41989/chapter/355437737\"><u>&nbsp;</u></a>less powerful and&nbsp;<a href=\"https://arxiv.org/pdf/1908.06165.pdf\"><u>privileged</u></a> (in&nbsp;<a href=\"https://spiral.imperial.ac.uk/handle/10044/1/94902\"><u>healthcare</u></a>, on&nbsp;<a href=\"https://oecd.ai/en/women-event-2022\"><u>women\u2019s working lives</u></a>,&nbsp;<a href=\"http://gendershades.org/\"><u>facial recognition</u></a> etc.) ; but nonetheless, these AI companies are happy to run roughshod over the rights of everyone in order to pursue their own ends.&nbsp; They have their vision of the future, and are&nbsp;<a href=\"https://scottaaronson.blog/?p=7042\"><u>happy to impose significant risk on the rest of humanity</u></a> to achieve this without our consent. A lot of the researchers helping these companies think&nbsp;<a href=\"https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/\"><u>what they are doing has high probabilities of being extremely bad</u></a>, and yet they carry on!&nbsp;</p><p>Irrespective of which sort of dangers we worry about, it's clear&nbsp;<strong>who</strong> we need to worry about: the AI companies, chiefly (although not exclusively) OpenAI and DeepMind. Whether you care about \u2018AI Ethics\u2019 or \u2018AI Safety\u2019,&nbsp;no matter what the type of harms you worry about, if you look at the issue politically the source of the harms looks the same.&nbsp; It's clear who has, and is trying to gain more, power, and it is clear that everyone else is put at extreme risk. If the&nbsp;<a href=\"https://academic.oup.com/edited-volume/41989/chapter/355437737\"><u>problem is power</u></a>, then we ought to fight it with power. We cannot merely allow these powerful actors to imagine and create futures for us,&nbsp;<a href=\"https://press.uchicago.edu/ucp/books/book/chicago/D/bo20836025.html\"><u>crowding out alternatives</u></a>; we need to build coalitions that give us the power to imagine and create safer and more equitable futures.&nbsp;</p><p>Thus, the importance of making a distinction between existential and non-existential harms will start to dissolve away, because either are possible hugely negative consequences of the same phenomena, with similar political solutions:&nbsp;<strong>slow down or stop companies trying to develop AGI and other risky \u2018advanced AI\u2019 systems</strong>. If we buy this, then the strategy needs to be much broader than the current status quo in the \u2018AI XRisk\u2019 community of&nbsp; merely empowering a narrow range of \u2018value-aligned\u2019 individuals to research \u2018technical alignment\u2019 or even friendly technocratic \u2018Existential AI Governance.\u2019 (I\u2019m not saying this is bad- far from it- or shouldn\u2019t be hugely expanded, but it is very very very far from sufficient). Rather, it likely looks like bringing together coalitions of actors, with perhaps different underlying ethical concerns, but the same political concern that the growing&nbsp;<a href=\"https://rankingdigitalrights.org/index2020/spotlights/unaccountable-algorithms\"><u>unaccountable power</u></a> of&nbsp;<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><u>dangerously hubristic&nbsp;</u></a>AI companies needs to be curbed. It requires building coalitions to engage in the politics of technology,&nbsp;<a href=\"https://journals.sagepub.com/doi/full/10.1177/1368431020988826\"><u>imagining futures we can perform into existence</u></a>, and asserting power to challenge the&nbsp;<a href=\"https://www.youtube.com/watch?v=P7XT4TWLzJw\"><u>inherently</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like\"><u>risky</u></a> pathways these AI companies want to put us on.</p><p>It's important to note that this isn't saying we don't and can't do AI research. But the type of research, moving towards&nbsp;<a href=\"https://www.nature.com/articles/d41586-023-00641-w\"><u>larger and larger models</u></a> with less accountability for the companies, trying to get more and more&nbsp;<a href=\"https://openai.com/research/gpt-4\"><u>general systems</u></a> with more&nbsp;<a href=\"https://www.google.com/amp/s/amp.theguardian.com/commentisfree/2023/feb/11/ai-drug-discover-nerve-agents-machine-learning-halicin\"><u>destructive capabilities</u></a>, with almost no regulatory oversight,&nbsp; is simply not a viable safe pathway forward. There is good reason to think that within our current paradigm and political structures, AGI development may be&nbsp;<a href=\"https://www.youtube.com/watch?v=P7XT4TWLzJw\"><u>inherently dangerous</u></a>; this is a demon we ought not to make. If this recklessness is synonymous with innovation, then those dreaming of innovations have lost a lot of their spark.&nbsp;</p><h2><strong>In whatever world we are in, putting \u2018AI\u2019s in charge\u2019 of powerful systems is dangerous</strong></h2><p>Whether we are in \u2018stochastic parrot\u2019 or \u2018verge of superintelligence\u2019 worlds, giving AIs power is deeply dangerous. \u2018Stupid\u2019 AIs are already causing&nbsp;<a href=\"https://www.theguardian.com/us-news/2022/jun/15/tesla-us-car-carashes-driver-assist-systems\"><u>fatalities</u></a>,&nbsp;<a href=\"https://dl.acm.org/doi/pdf/10.1145/3442188.3445922\"><u>reinforcing biases</u></a>, and&nbsp;<a href=\"https://www.forbes.com/sites/lutzfinger/2022/09/08/deepfakesthe-danger-of-artificial-intelligence-that-we-will-learn-to-manage-better/\"><u>causing other harms</u></a>, all of which will likely get worse if given more power. \u2018Stupid systems\u2019 could even cause harm of existential proportions, for example if they are&nbsp;<a href=\"https://thebulletin.org/2023/02/keeping-humans-in-the-loop-is-not-enough-to-make-ai-safe-for-nuclear-weapons/\"><u>integrated into nuclear command and control</u></a>, or used to make&nbsp;<a href=\"https://www.nature.com/articles/s42256-022-00465-9\"><u>more powerful new biochemical weapons</u></a>. Superintelligent AIs, if given power, could similarly cause tremendous amounts of harm scaling to existential harm. I think it's also important to note, that AI\u2019s needn\u2019t be an agent in the typical, anthropomorphised sense, for it to be useful to describe them as \u2018having power\u2019, and that is what I mean here.&nbsp;</p><p>Once again, unaccountable, opaque, \u2018machine power\u2019, generally allows for an increase in harm that can be done, and a reduction in the ability of society to respond to said harm as systems get entrenched and remake the social world we live in, which is incredibly dangerous. And once again, these harms are often imposed on the rest of the world, without our consent, by companies, militaries and governments looking to rely on AI systems, normally due to hype from the same, few AGI companies. In this way, irrespective of the world we are in, hype is dangerous, because once again it provides the dangerously risky AI companies with more power, which they almost certainly use to pose risks of unacceptable harm on the world population.</p><h2><strong>In whatever world we are in, AGI research is dangerous</strong></h2><p>If we are in \u2018stochastic parrot\u2019 world, research into AGI is used as an excuse and a fig leaf to hide enormous harms imposed by dangerously stupid AI systems. In this world, AGI research is used to focus on increasing the power of a few, unaccountable, powerful companies, and causes harm for the rest of us, whilst failing to deliver on its promises. By controlling visions of the future,<a href=\"https://link.springer.com/chapter/10.1007/978-3-658-27155-8_1\"><u> actors gain control over the present</u></a>. Visions of utopia allow more mundane harms to get ignored, with these companies provided a free pass.&nbsp;</p><p>If we are in the \u2018verge of superintelligence\u2019 world, research into AGI is&nbsp;<a href=\"https://towardsdatascience.com/we-dont-know-how-to-make-agi-safe-f13ae837b05c\"><u>flirting with the apocalypse</u></a> in a way that is unacceptably dangerous. Stories of the inevitability of AGI development are useful as excuses for those who care little of the existential risk that developing these systems could bring in comparison to&nbsp;<a href=\"https://openai.com/blog/planning-for-agi-and-beyond\"><u>their desire to impose their vision</u></a> of what a glorious future looks like upon mankind.</p><p>There may be a counterargument to this, that suggests research here isn\u2019t dangerous, but deployment is, so it is on model deployment we need to regulate and not research. I think in both worlds, this is flawed. In \u2018stochastic parrot\u2019 world, even with regulated deployment, unrestricted research is likely to lead to a slippery slope to deployment (<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/japp.12345#:~:text=The%20Slippery%20Slope%20Argument%20warns,untested%20and%20perhaps%20morally%20objectionable.\"><u>as worried about in geoengineering, for example</u></a>), where research enables a gaining of financial, intellectual and discursive power by the AI companies in a way that makes dangerous deployment of technologies much more likely. And in a \u2018verge of superintelligence\u2019 world having&nbsp;<a href=\"https://www.globalpolicyjournal.com/articles/global-public-goods-and-bads/vulnerable-world-hypothesis\"><u>powerful doomsday devices</u></a> developed is probably already an unacceptable risk no matter how strict the governance of deployment is. Even if we think our regulation of deployment is sound, governance mechanisms can break down, the existence of technologies can induce social changes affecting governance and&nbsp;<a href=\"https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment\"><u>deceptive alignment&nbsp;</u></a>is enough of a problem that it seems better to simply never try and develop these systems in the first place. Moreover, to suggest the problem doesn\u2019t start with research fails to reckon with the risk of bad actors; whilst one could say that guns don\u2019t murder, people do, had guns not been invented far fewer people would have been killed in violence than are now.&nbsp;&nbsp;</p><h2><br><strong>Why this is a shared battle</strong></h2><p>I hope the previous paragraphs have shown that whilst the disagreements between the AI Safety and AI Ethics crowds are significant, they are not massively analytically useful or core to understanding the key challenge that we are facing. The relevant question isn\u2019t \u201care the important harms to be prioritised the existential harms or the non-existential ones?\u201d,&nbsp; \u201cwill AI be agents or not?\u2019, nor \u2018will AI be stochastic parrots or superintelligence?\u201d Rather, the relevant question is whether we think that power-accumulation and concentration in and through AI systems, at different scales of capability, is extremely risky. On this, I think we agree, and so whilst scientifically our differences may be significant, in the realm of political analysis it isn\u2019t. Ultimately, it is this power concentration that has the potential to cause harm, and it is ultimately this which we normatively care about.</p><p>Moreover, many of these surface level disagreements also aren\u2019t politically or strategically relevant: once we understand that the source of all these risks is a small group of AI companies recklessly forging ahead and concentrating power, it becomes much clearer that both communities in fact share interests in finding ways to (1) slow down/halt research; (2) avert and cool down AI hype; (3) spur much greater public/government scrutiny into whether (and if yes, how) we want to develop advanced AI technologies.&nbsp;</p><h2><strong>What we gain from each other</strong></h2><p>This essay framed itself as suggesting that both the \u2018AI Ethics\u2019 and \u2018AI Safety\u2019 crowds can benefit each other. Thus far, I\u2019ve mostly suggested that the AI Safety crowd should realise that even if the AI Ethics crowd were incorrect about dismissing the importance of existential risks from AI, that their analysis, that power accumulation and concentration through and by AI, originating from a small number of powerful and unaccountable corporations, is the major cause of the threats we face, is correct. From this perspective, the AI Safety crowd probably should come and fight in the trenches with the AI Ethics people as well, realising that their identification of the core of the issue has been broadly correct, even if they underestimated how bad these corporations could make things. Moreover, the AI Ethics crowds seem to have been more effective at tempering AI Hype in contrast to the way in which AI Safety crowds have potentially sped up AI development, so practically there may be significant benefit in collaboration.&nbsp;</p><p>However, I\u2019m not sure if the exchange here is so one-sided. I think the AI Safety community has a lot to offer the AI Ethics community as well. Technical AI Safety techniques, like&nbsp;<a href=\"https://twitter.com/sethlazar/status/1616095315102400516?lang=en-GB\"><u>RLHF&nbsp;</u></a>or&nbsp;<a href=\"https://arxiv.org/abs/2212.08073\"><u>Constitutional AI</u></a>, whilst potentially not very beneficial from an AI Safety perspective, seem to have had a meaningfully significant impact on making systems more ethical. Moreover, the moral inflation and urgency that Existential Harms can bring seems to&nbsp;<a href=\"https://aiimpacts.org/the-public-supports-regulating-ai-for-safety/#:~:text=55%25%20say%20AI%20could%20eventually,of%20drugs%20and%20medical%20devices%E2%80%9D\"><u>resonate with the public</u></a>, and so politically may be very useful tools if utilised to fight the companies rather than empower them. Intellectually, AI Safety provides much greater urgency and impetus for governing research and cutting the problem off at the sources (which has been underexplored so far) , a concern which would likely be more muted in AI Ethics discussions.&nbsp; By regulating these problems at the sources, AI Ethics work can be made a lot easier and less reactive. Moreover, the focus from the AI Safety crowd on&nbsp;<a href=\"https://write.as/sethlazar/genb\"><u>risks from systems that look vastly different from the risks we face now may be useful even if we don\u2019t develop AGI;</u></a> risks and harms will change in the future just as they have changed in the past, and anticipatory governance may be absolutely essentially at reducing these.&nbsp; So even if one doesn\u2019t buy my suggestion that we are on the same side of the most analytically relevant distinction, I hope that the insights and political benefits that the two communities have to offer each other will be enough cause for common ground to start working together.&nbsp;</p><h2><strong>Coalitional Politics</strong></h2><p>If one accepts my (not particularly groundbreaking) analysis that the ultimate problem is the power of the AI companies, how do we combat this? There are lots of ways to do this, from narrow technocratic governance to broad range political salience raising, to ethics teams within corporations and broad governance frameworks and many other approaches. Each of these are necessary and useful, and I don\u2019t argue against any of them. Rather, I\u2019m arguing for a broad, pluralistic coalition taking a variety of approaches to AI governance, with more focus put towards work to raise the political salience of the restriction of AI Research than currently is.&nbsp;</p><p>Given AI Ethics and AI Safety people are actually concerned with the same phenomena of harms arising from the unaccountable power enabling dangerously risky behaviour from a very small number of AI companies, then we also have the same solution; take them on. Use all the discursive and political tools at our disposal to curb their power and hold them to account. We need a big tent to take them on. We need op eds in major newspapers attesting to the dangerous power and harms these AI companies have and are happy to risk. We need to (continue to) expose just how their messianic arrogance endangers people, and let the public see what these few key leaders&nbsp;<a href=\"https://www.forbes.com/sites/nicolemartin1/2019/06/27/13-greatest-quotes-about-the-future-of-artificial-intelligence/?sh=b2c4c713bdfa\"><u>have said</u></a> about the world they are pushing us towards.&nbsp; We need to mobilise peoples worries such that politicians will react, establishing a culture against the unaccountable power of these AI companies. We need to show people&nbsp;<a href=\"https://www.vox.com/future-perfect/2019/11/7/20897531/artificial-intelligence-left-socialist-google-deepmind\"><u>across the political spectrum</u></a> (<a href=\"https://www.foxnews.com/media/chatgpt-faces-mounting-accusations-woke-liberal-bias\"><u>even those we disagree with!</u></a>) how this new power base of AI companies has&nbsp;<a href=\"https://www.theguardian.com/commentisfree/2022/oct/08/tech-firms-artificial-intelligence-ai-liability-directive-act-eu-ccia\"><u>no one's interests</u></a> at heart but their own, so no matter where you fall, they are a danger to your vision of a better world. There is&nbsp;<a href=\"https://aiimpacts.org/the-public-supports-regulating-ai-for-safety/\"><u>nascent public worries around AGI</u></a> and these AI companies, we just need to activate this through a broad coalition to challenge the power of these companies and wrestle control of humanity\u2019s future from them. Hopefully this can lay the groundwork for formal governance and at the very least quickly create a political culture reflective of the degree of worry that ought to be held about these companies\u2019 power.&nbsp;</p><p><br>There is nothing inevitable about technology development, and there is nothing inevitable about the status quo. In my \u2018home field\u2019 of Solar Geoengineering, considerably smaller coalitions,&nbsp;<a href=\"https://www.saamicouncil.net/news-archive/support-the-indigenous-voices-call-on-harvard-to-shut-down-the-scopex-project\"><u>less well funded and powerful</u></a> than what we could build in the AI space in a few months,&nbsp;<a href=\"https://www.theguardian.com/environment/2012/may/16/geoengineering-experiment-cancelled\"><u>successfully halted technology development</u></a> for at least the last decade. Similar coalitions have&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5033189/\"><u>constrained GMOs in various regions of the world</u></a>,&nbsp;<a href=\"https://www.politico.eu/article/politics-behind-germany-refusal-reconsider-nuclear-phaseout/\"><u>nuclear energy</u></a> and&nbsp;<a href=\"https://www.worldscientific.com/worldscibooks/10.1142/7895#t=aboutBook\"><u>nuclear weapons for peaceful purposes</u></a>. There are enough reasons to oppose development of AGI systems from the perspective of all sorts of worldviews and ethical systems to build such coalitions; this has successfully occurred in a number of the above examples, and it may be even easier in the context of AI. Some have tried to make a start on this (e.g.&nbsp;<a href=\"https://www.vox.com/future-perfect/23619354/openai-chatgpt-sam-altman-artificial-intelligence-regulation-sydney-microsoft-ai-safety\"><u>Piper</u></a>,&nbsp;<a href=\"https://garymarcus.substack.com/p/is-it-time-to-hit-the-pause-button?r=1aghy&amp;utm_medium=ios&amp;utm_campaign=post\"><u>Marcus and Garner</u></a>,&nbsp;<a href=\"https://time.com/6132399/timnit-gebru-ai-google/\"><u>Gebru</u></a> etc), but a larger and more diverse coalition trying to raise the political salience of curbing AI companies\u2019 power is key. Bringing genuine restriction of these companies power into the overton window, building coalitions across political divides to do this, building constituencies of people who care about regulating the power of AI companies, raising the salience of the issue in the media and crafting and envisioning new futures for ourselves are all vital steps that can be taken.&nbsp; We can build a relevant civil society to act as a powerful counterbalance to corporate power.&nbsp;</p><p>This isn\u2019t an argument to shut down existing&nbsp;<a href=\"https://www.governance.ai/\"><u>narrow technocratic initiatives</u></a>, or&nbsp;<a href=\"https://www.dair-institute.org/\"><u>academic research presenting alternative directions for AI</u></a>; rather, it is an argument that we need to do more, and do it together. There seems to be a gaping narrative hole (despite the admirable attempts of a few people to fill it) in pushing for a public political response to these AI companies. These discourses, social constructions and&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S1877343520300488\"><u>visions of the future</u></a> matter to technology development and governance. They put pressure and establish norms that guide near term corporate decision making, government policy, and how society and public relate to technology and its governance.&nbsp;</p><h2><strong>Urgency</strong></h2><p>I would also argue that this issue is urgent. Firstly, around&nbsp;<a href=\"https://www.politico.eu/article/eu-plan-regulate-chatgpt-openai-artificial-intelligence-act/\"><u>ChatGPT</u></a>,&nbsp;<a href=\"https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter\"><u>Bing/Sydney</u></a> and now&nbsp;<a href=\"https://openai.com/research/gpt-4\"><u>GPT-4</u></a>, AI is experiencing a bit of a period of political attention at present. Public and government attention at the moment are good, and plausibly as good as they\u2019ll ever be, for a politics to slow AGI development, and we are most powerful pushing for this together, rather than fighting and mocking each other in an attempt to gain political influence. This may be a vital moment that coalitional politics can be a powerful lever for enacting change, where the issue is suitably malleable to political contestation, to the formation of a&nbsp;<a href=\"https://link.springer.com/book/10.1057/9781137313652\"><u>governance object</u></a> and to framing of how this issue could be solved; these are the exact times where power can be asserted over governance, and so assembling a coalition may give us that power.&nbsp;</p><p>There is also a risk that if we don\u2019t foster such a coalition soon, both of our communities get outmanoeuvred by a new wave of tech enthusiasts that are currently pushing very hard to accelerate AI, remove all content or alignment filters, open-source and disseminate all capabilities with little care for the harms caused and more. Indeed, many tech boosters are&nbsp;<a href=\"https://www.jonstokes.com/p/the-story-so-far-ai-makes-for-strange\"><u>beginning to paint AI ethics and AI risk advocates as two sides of the same coin</u></a>. To counteract this movement, it is key for both communities to bury the hatchet and combat these plausibly rising threats together. Divided we fall.&nbsp;</p><h2><strong>So what does coalitional politics look like?</strong></h2><p>I think this question is an open one, something we will need to continue to iterate with in this context, learn by doing and generally work on together. Nonetheless, I will give some thought.</p><p>Firstly, it involves trying to build bridges with people who we think have wrong conceptions of the harms of AI development. I hope my argument that the political source of harm looks the same has convinced you, so let's work together to address that, rather than mocking, insulting and refusing to talk to one another. I understand people from AI Safety and AI Ethics have serious personal and ethical problems with one another; that needn\u2019t translate to political issues. Building these bridges not only increases the number of people in our shared coalition, but the diversity of views and thinkers, allowing for new ideas to develop. This broad, pluralistic and diverse ecosystem will likely come not just with political, but with&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0039368117303278\"><u>epistemic</u></a>&nbsp;<a href=\"https://www.nature.com/articles/d41586-018-05326-3\"><u>benefits</u></a> as well.&nbsp;</p><p>Secondly, it involves using the opportunities we can to raise the&nbsp;<a href=\"https://oxfordre.com/politics/display/10.1093/acrefore/9780190228637.001.0001/acrefore-9780190228637-e-1361;jsessionid=AEFBBD08AB6602A22D9762D4FFF2351A?rskey=x512Ha&amp;result=6\"><u>political salience</u></a> of the issue of the power of AI companies as much as we can. At present, we are at something of a&nbsp;<a href=\"https://www.bbc.co.uk/news/world-us-canada-64967627\"><u>moment of public attention towards AI</u></a>; rather than competing with one another for attention and discursive control, we ought to focus on the common concern we have. Whether the impetus for regulating these companies comes from motivations of concentration of corporate power or existential harms, it raises the salience of the issue and increases the pressure to regulate these systems, as well as increasing the pressure on companies to self-regulate. We must recognise our shared interest in joining into a single knowledge network and work out how&nbsp;<a href=\"https://research.wur.nl/en/publications/the-emergence-of-geoengineering-how-knowledge-networks-form-gover\"><u>best to construct a governance object</u></a> to achieve our shared ends. At the moment, there is a weird discursive vacuum despite the salience of AI. We can fill this vacuum, and this will be most effective if done together. Only by filling this vacuum can we successfully create a landscape that can allow the curbing of the power of these corporations. People are already trying to do this, but the louder and broader the united front against these companies are, the better.&nbsp;</p><p>Then, we need to try and create a culture that pressures political leaders and corporations, no matter where politically they fall, that these unaccountable companies have no right to legislate our future for us. We can do this agenda and culture shift&nbsp;<a href=\"https://rgu-repository.worktribe.com/output/1534408/scotland-and-period-poverty-a-case-study-of-activists-media-and-political-agenda-setting\"><u>through activism</u></a>,&nbsp;<a href=\"https://www.academia.edu/13123517/The_Agenda_Setting_Function_of_Mass_Media\"><u>through the media</u></a>,&nbsp;<a href=\"https://www.brookings.edu/on-the-record/what-is-the-role-of-courts-in-making-social-policy/\"><u>through the law,</u></a>&nbsp;<a href=\"https://www.socialchangelab.org/_files/ugd/503ba4_9b649b50484748bfbae215d675084118.pdf\"><u>through protests</u></a> and t<a href=\"https://www.tandfonline.com/doi/abs/10.1080/14782804.2020.1785849?journalCode=cjea20\"><u>hrough political parties</u></a>, as well as more broadly how discourses and imaginaries are shaped in key fora (social media, traditional media, fiction, academic work, conversations);&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11625-022-01110-5\"><u>the power of discourses have long been recognised in the development and stabilisation of socio-technical systems</u></a>.&nbsp; Democracy is rarely ensured by technocracy alone; it often takes large scale cu requires large scale cultural forces. Luckily,&nbsp;<a href=\"https://www.monmouth.edu/polling-institute/documents/monmouthpoll_us_021523.pdf/\"><u>most people seem to support this!</u></a></p><p>We then need suggestions for policy and direct legal action to restrict the power and ability of these AI companies to do what they currently do. Again, luckily these exist.&nbsp;<a href=\"https://www.alignmentforum.org/posts/z8BF9GwcCjeXShC4q/compute-governance-the-role-of-commodity-hardware\"><u>Compute governance</u></a>, utilising <a href=\"https://news.bloomberglaw.com/antitrust/ftc-reviewing-competition-deception-in-artificial-intelligence\"><u>competition</u> </a>law, holding companies&nbsp;<a href=\"https://venturebeat.com/ai/could-big-tech-be-liable-for-generative-ai-output-hypothetically-yes-says-supreme-court-justice/amp/\"><u>legally accountable</u></a> for harmful outputs of&nbsp; generative AI&nbsp; (and slightly more tangentially&nbsp;<a href=\"https://www.wired.com/story/make-platforms-safer-regulate-design-section-230-gonzalez-google/\"><u>platforms</u></a>), supporting&nbsp;<a href=\"https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion\"><u>copyright suits</u></a><u> </u>and more seem like ways we can attack these companies and curb their power.&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S2666659620300056\"><u>Human rights suits</u></a> may be possible. In general, there is an argument to suggest that the <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4382100\">use of the courts</a> is an important and underexplored lever to keep these companies accountable. &nbsp;Moreover, given the risks these companies themselves suggest they are imposing, other more speculative suits based on various other rights and principles, as has&nbsp;<a href=\"https://www.science.org/content/article/surprise-dutch-court-orders-government-do-more-fight-climate-change\"><u>occurred in the climate context</u></a>, may be possible. This is just some of a shopping list of policy and direct actions a broad coalitional movement could push for. People are already pushing for these things, but with better organisation that comes with more groups, the ability to push some of these into practice may be significantly enhanced. With a broader, diverse coalition, our demands can get stronger.&nbsp;</p><p>Sure, this coalitional politics will be hard. Building bridges might sometimes feel like losing sight of the prize as we focus on restricting the power of these&nbsp;<a href=\"https://www.bbc.com/future/article/20211014-agents-of-doom-who-is-hastening-the-apocalypse-and-why\"><u>agents of doom</u></a> via other arguments and means alongside whatever each of our most salient concerns is. It will be hard to form coalitions with people you feel very culturally different from. Ultimately, if we want to curb the ability of AI companies to do harm, we need all the support we can get, not just from those in one culture, but those in many. I hope many, a lot of whom have already contributed so much to this fight in both AI Safety and AI Ethics, will take up such an offer for coalitional politics, at this potentially vital moment.</p><p>Acknowledgements: Matthijs Maas gave me substantial advice and help, despite substantive disagreements with aspects of the essay, and conversations with Maathijs Maas and Seth Lazar provided a lot of the inspiration for me to write this.&nbsp;</p>", "user": {"username": "Gideon Futerman"}}, {"_id": "r5kffvkLfknn9yojW", "title": "Announcing the ERA Cambridge Summer Research Fellowship", "postedAt": "2023-03-16T11:37:14.599Z", "htmlBody": "<p>The Existential Risk Alliance (ERA) has opened applications for an&nbsp;<strong>in-person, paid, 8-week Summer Research Fellowship&nbsp;</strong>focused on existential risk mitigation, taking place from July 3rd to August 25th 2023 in Cambridge, UK, and aimed at all aspiring researchers, including undergraduates.&nbsp;</p><p><strong>To apply and find out more, please visit the&nbsp;</strong><a href=\"http://www.erafellowship.org/\"><strong><u>ERA website</u></strong></a><strong>.&nbsp;</strong></p><p>If you are interested in&nbsp;<strong>mentoring fellows on this programme</strong>, please submit your name, email and research area&nbsp;<a href=\"https://airtable.com/shrjotwYJCTLzSjrX\"><u>here,</u></a> and we will get in touch with you in due course.&nbsp;</p><p>If you know other people who would be a good fit, please encourage them to apply (people are more likely to apply if you recommend they do, even if they have already heard of the opportunity!) If you are a leader or organiser of relevant community spaces, we encourage you to post an announcement with a link to this post, or alternatively a printable poster is&nbsp;<a href=\"https://drive.google.com/file/d/1Yv4StsWeYa3IAclszmJLG5C3w00CujG_/view?usp=sharing\"><u>here</u></a>.</p><p>Applications will be reviewed as they are submitted, and we encourage early applications, as offers will be sent out as soon as suitable candidates are found.&nbsp;<strong>We will accept applications until April 5, 2023 (23:59 in US Eastern Daylight Time).&nbsp;</strong></p><p>&nbsp;</p><p>The ERA Cambridge Fellowship (previously known as the CERI Fellowship) is a fantastic opportunity to:</p><ul><li>Build your portfolio by researching a topic relevant to understanding and mitigating existential risks to human civilisation.</li><li>Receive guidance and develop your research skills, via weekly mentorship from a researcher in the field.</li><li>Form lasting connections with other fellows who care about mitigating existential risks, while also engaging with local events including discussions and Q&amp;As with experts.</li></ul><h2><strong>Why we are running this programme&nbsp;</strong></h2><p>Our mission as an organisation is to reduce the probability of an existential catastrophe. We believe that one of the key ways to reduce existential risk lies in fostering a community of dedicated and knowledgeable x-risk researchers. Through our summer research fellowship programme, we aim to identify and support aspiring researchers in this field, providing them with the resources and the mentorship needed to succeed.</p><h2><strong>What we provide</strong></h2><ul><li>A salary equivalent to \u00a331,200 per year, which will be prorated to the duration of the summer programme.</li><li>Mentorship from a researcher working in a related field.</li><li>Complimentary accommodation, meal provisions during working hours, and travel expense coverage</li><li>Dedicated desk space at our office in central Cambridge.</li><li>Opportunity to work either on a group research project with other fellows or individually.</li><li>Networking and learning opportunities through various events, including trips to Oxford and London.</li></ul><h2><strong>What we are looking for</strong></h2><p>We are excited to support a wide range of research, from the purely technical to the philosophical, as long as there is direct relevance to mitigating existential risk. This could also include social science or policy projects focusing on implementing existential risk mitigation strategies.&nbsp;&nbsp;</p><p>Incredibly successful projects would slightly reduce the likelihood that human civilisation will permanently collapse, that humans will go extinct, or that the future potential of humanity will be permanently reduced. A secondary goal of this project is for fellows to learn more about working on existential risk mitigation, develop relevant skills, and test their fit for further research or work in this field.</p><h2><strong>Who we are looking for</strong></h2><p>Anyone can apply to the fellowship, though we expect it to be most useful to students (from undergraduates to postgraduates) and early-career individuals looking to test their fit for existential risk research. We particularly encourage undergraduates to apply, to develop their research experience.&nbsp;</p><p>We are looking to support proactive individuals from a wide range of subject areas who have a high potential to do impactful work in the future. Candidates will be assessed both on ability to contribute to the existential risk research community and motivation to reduce existential risks.&nbsp;</p><h2><strong>Application process&nbsp;</strong></h2><p>The first stage consists of essay-style questions, and is the main portion of the application process. We will evaluate applications on a rolling basis and applicants who progress to the next stage will be invited for a short interview, which is the final stage. Successful applicants will be notified by late April, and afterwards we will work with accepted fellows to develop their project ideas and pair them with relevant mentors.&nbsp;</p><p>We want to help people work on important problems even if it is not at ERA, so in that spirit, we also encourage you to check out the other programmes listed&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J7PsetipHFoj2Mv7R/notes-on-ea-related-research-writing-testing-fit-learning#Programs__approaches__or_tips_for_testing_fit_for__longtermism_related__research\"><u>here</u></a>.</p><h2><strong>Questions?</strong></h2><p>Please check out our&nbsp;<a href=\"https://www.erafellowship.org/faq\"><u>Frequently Asked Questions</u></a> section first. If you have questions about anything else which is not covered in our FAQs, please email us at&nbsp;<strong><u>specialprojects@rethinkpriorities.org</u></strong></p><p>If you have any feedback, please submit it via our&nbsp;<a href=\"https://airtable.com/shr11E0DbORKc7sNd\"><u>anonymous feedback form</u></a>.&nbsp;<br>&nbsp;</p><h2><strong>Credits</strong></h2><p><i>This post is from the Existential Risk Alliance, which is a fiscally sponsored project of Rethink Priorities.&nbsp;</i></p><p><br><br>&nbsp;</p>", "user": {"username": "nshiralkar"}}, {"_id": "cMcnDhfeQzp6LpzyY", "title": "Women and underrepresented genders meetup @ EAGx Cambridge", "postedAt": "2023-03-16T10:08:21.342Z", "htmlBody": "<p><strong>When:&nbsp;</strong>11am, Sunday 19th March</p><p><strong>Where:&nbsp;</strong>Exhibition Hall</p><p><br>We\u2019re running a meetup at EAGxCambridge for women and other underrepresented genders. If you\u2019re a woman, non-binary, trans, or otherwise identify with an underrepresented gender in EA, come along! The event will involve:</p><ul><li>Speed meetings</li><li>Topic discussions</li><li>Mingling</li></ul><p>For the topic discussions, you\u2019ll be able to choose between several topics of potential interest to EA women and underrepresented genders, and also suggest your own. Some topics we might discuss are:</p><ul><li>Career planning</li><li>Your relationship to the EA community</li><li>Self-care</li><li>Impostor syndrome</li><li>The recent sexual harassment scandals in EA; misogyny and gender oppression in the EA community</li></ul><p>We look forward to seeing many of you there :)&nbsp;</p>", "user": {"username": "Amber"}}, {"_id": "NucKKzmfbvB5xJbi3", "title": "Here, have a calmness video", "postedAt": "2023-03-16T10:00:43.022Z", "htmlBody": "", "user": {"username": "Kaj_Sotala"}}, {"_id": "oqZfunLtKoDccxMHa", "title": "Offer an option to Muslim donors; grow effective giving", "postedAt": "2023-03-16T07:26:30.553Z", "htmlBody": "<h2><strong>Summary</strong></h2><p>In order to offer Muslim donors a way to give their annual religious tithing (<i>zakat</i>) to an EA-aligned intervention, GiveDirectly launched&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>a zakat-compliant fund</u></a>, delivered as cash to Yemeni families displaced by the civil war. Muslims give ~$600B/year in Zakat to the global poor, though much of this is given informally or to less-than-effective NGOs.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwyvkxsntq1r\"><sup><a href=\"#fnwyvkxsntq1r\">[1]</a></sup></span>&nbsp;</p><p>Through this unconditional cash transfer option, we\u2019re offering Muslims the opportunity to redirect a portion of their giving to a measurably high-impact intervention and introduce more Muslims to EA\u2019s theory of effective giving. We invite readers to share thoughts in the comments and to&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>share the campaign</u></a> far and wide.</p><h2><strong>Muslims are the fastest-growing religious group and give annually</strong></h2><p>As Ahmed Ghoor&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hGEMPYi7FpgKCXCi7/muslims-for-effective-altruism\"><u>observed</u></a>, Muslims make up about 24% of the world population (1.8B people) and Islam is the fastest growing religion. Despite having a robust tradition of charitable giving, little has been done proactively to engage the Muslim community on the ideas of effective altruism. An important step to inclusion is offering this pathway for effectively donating zakat.&nbsp;</p><h3><strong>Zakat is a sacred pillar of Islam, a large portion of which is given to the needy&nbsp;</strong></h3><p>For non-Muslim readers: one of the five pillars of Islam,&nbsp;<a href=\"https://www.investopedia.com/terms/z/zakat.asp#:~:text=Zakat%20is%20an%20Islamic%20financial,individual%27s%20total%20savings%20and%20wealth.\"><u>zakat</u></a> is mandatory giving; Muslims eligible to pay it donate at least 2.5% of their accumulated wealth annually for the benefit of the poor, destitute, and others \u2013 classified as&nbsp;<i>mustahik</i>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefagb814y35qb\"><sup><a href=\"#fnagb814y35qb\">[2]</a></sup></span>&nbsp;Some key points:</p><ol><li>A major cited aim of Zakat is to provide relief from and ultimately eradicate poverty.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiattu6i346d\"><sup><a href=\"#fniattu6i346d\">[3]</a></sup></span></li><li>It is generally held that zakat can only be given to other Muslims.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1r8u1y8g2d4\"><sup><a href=\"#fn1r8u1y8g2d4\">[4]</a></sup></span></li><li>A large portion of zakat is given informally person-to-person or through mosques and Islamic charities.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1epir64f1pk\"><sup><a href=\"#fn1epir64f1pk\">[5]</a></sup></span>&nbsp;</li><li>Zakat is a sacred form of charity; it\u2019s most often given during the holy month of Ramadan.</li></ol><h3><strong>Direct cash transfers are a neglected zakat option&nbsp;</strong></h3><p>Zakat giving is estimated at $1.8B in the U.S. alone with $450M going to international NGOs, who mostly use their funds for in-kind support like food, tents, and clothing.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1epir64f1pk\"><sup><a href=\"#fn1epir64f1pk\">[5]</a></sup></span>&nbsp;Dr. Shahrul Hussain, an Islamic scholar,&nbsp;<a href=\"https://www.bbsi.org.uk/wp-content/uploads/2021/04/Taml-k-proper-to-Quasi-taml-k-Unconditional-Cash-Transfer-UCT-of-Zakat-Money-Empowering-the-Poor-and-Contemporary-Modes-of-Distributing-Zakat-Money.pdf\"><u>argues</u></a> that cash transfers \u201cshould be considered a primary method of zakat distribution,\u201d&nbsp;&nbsp;as, according to the Islamic principle of&nbsp;<i>taml\u012bk</i> (ownership), the recipients of the zakat have total ownership over the money, and it is up to them (not an intermediary third-party organization or charity) how it is spent. He also notes \u201cthe immense benefits of unconditional cash transfer in comparison to in-kind transfer.\"&nbsp;</p><p>This is a simple, transparent means of transferring wealth that empowers the recipients. However, other than informal person-to-person giving, there are limited options to give zakat as 100% unconditional cash.&nbsp;</p><h2><strong>GiveDirectly now allows zakat to be given as cash to Muslims in extreme poverty&nbsp;</strong></h2><p>As an opportunity for Muslims to donate zakat directly as cash, GiveDirectly created a&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>zakat-compliant fund</u></a> to give cash through our program in Yemen. While GiveDirectly is a secular organization, our Yemen program and&nbsp;<a href=\"https://www.givedirectly.org/wp-content/uploads/2023/02/Zakat-Compliance-Certificate-GiveDirectly.pdf\"><u>Zakat policy</u></a> have been reviewed and certified by&nbsp;<a href=\"https://amanahadvisors.com/\"><u>Amanah Advisors</u></a>. In order to achieve this, we\u2019re assured that 100% of donations will be delivered as cash, using non-zakat funds to cover the associated delivery costs.</p><p>Donations through our page are tax-deductible in the U.S. and our partners at Giving What We Can&nbsp;<a href=\"https://www.givingwhatwecan.org/fundraisers/givedirectly-yemen-fund-2023\"><u>created a page</u></a> allowing donors to give 100% of their gift to GiveDirectly\u2019s zakat-compliant fund, tax-deductible in the Netherlands and the U.K. Taken together, this provides a tax-deductible option for 8.6M Muslims across three countries.</p><h3><strong>As a secular NGO, GiveDirectly may struggle to gain traction with Muslim donors&nbsp;</strong></h3><p>GiveDirectly is a credible option for zakat donors: we\u2019ve delivered cash aid to the world\u2019s poorest in 13 countries, including Yemen, Morocco, and Muslim-majority parts of Kenya, Malawi, and Nigeria.&nbsp;<a href=\"https://www.linkedin.com/in/walidherzallah/\"><u>Walid Herzallah</u></a> leads our zakat fundraising efforts, and donations are delivered by Yemeni staff on the ground with our banking partner, Al Kuraimi Islamic Microfinance Bank.</p><p>However, GiveDirectly has been cautioned by Muslim-giving experts that we face an uphill battle, especially in our first Ramadan. Some donors simply won\u2019t want to give their zakat through a secular organization. Also, many Muslims primarily learn of zakat options through word-of-mouth (e.g. local mosque), rather than charity recommenders.&nbsp;</p><p>We believe we can overcome these obstacles in two ways:</p><ol><li>Appealing to younger Muslim givers, not yet habituated to other nonprofits, who will respond to our unique selling points of effectiveness, transparency, and directness.&nbsp;</li><li>Suggesting people give&nbsp;<i>a portion</i> of their zakat through GiveDirectly. Donors often split their zakat across multiple causes, which could be a way for us to get a foot-in-the-door.</li></ol><h3><strong>You can help by sharing this effort widely&nbsp;</strong></h3><p>As with all new efforts, the biggest obstacle is obscurity. You can help us by sharing&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>this campaign</u></a> beyond the confines of the largely non-Muslim EA community:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5sv0784ro6f\"><sup><a href=\"#fn5sv0784ro6f\">[6]</a></sup></span>&nbsp;</p><ol><li>Share on LinkedIn or social media.&nbsp;</li><li>Send to Muslim friends &amp; colleagues and wish them a&nbsp;<a href=\"https://www.wikihow.com/Wish-Someone-Happy-Ramadan#:~:text=1-,%E2%80%9CRamadan%20Mubarak%E2%80%9D%20(ram%2DAH%2D,dan%20mu%2DBA%2Drack)&amp;text=%22Ramadan%20Mubarak%22%20translates%20to%20%E2%80%9C,your%20interest%20in%20Muslim%20culture.\"><u>happy Ramadan</u></a><u>.</u><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcthm7yabt0q\"><sup><a href=\"#fncthm7yabt0q\">[7]</a></sup></span></li></ol><p>If successful, this campaign will spark more conversations about effective giving among Muslims and grow the number and diversity of engagements with effective altruism. Here's a recording of our webinar on effective zakat:</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=8WHlRyE7yoU\"><div><iframe src=\"https://www.youtube.com/embed/8WHlRyE7yoU\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><h2><strong>More information on GiveDirectly\u2019s Yemen program</strong></h2><p>The protracted civil war in Yemen is the world\u2019s worst humanitarian crisis, with 21 million people estimated to be in need of aid. Since August 2022, GiveDirectly has delivered cash donations to 2,000 families in Yemen. We aim to reach 4,200+ households before the end of this year \u2013&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>learn more</u></a>.</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=lLmcLY0Vpjw\"><div><iframe src=\"https://www.youtube.com/embed/lLmcLY0Vpjw\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><h3><strong>About the authors</strong></h3><p><strong>GiveDirectly&nbsp;</strong>is an NGO that delivers cash aid to the world\u2019s poorest. In the past decade, they\u2019ve reached over 1.5M people in 13 countries, including Yemen, Morocco, and Nigeria. Their work is <a href=\"https://givedirectly.org/research\">research-backed</a> and recommended by the EA community. Their tech platform allows them to transparently and efficiently deliver money directly to the most vulnerable to spend on what they need most. <a href=\"mailto:info@givedirectly.org\">info@givedirectly.org</a>&nbsp;</p><p><br><strong>Muslim Impact Lab&nbsp;</strong>is a multidisciplinary research organization dedicated to finding ways of maximizing the positive social impact of their servitude from an Islamic perspective. Their research methodology draws on rigorous evidence, careful reasoning, and different understandings of the Divine intent throughout Islamic intellectual history. The team comprises individuals with diverse academic backgrounds, including religious studies, psychological sciences, and STEM, among others. This diversity allows them to approach their research from multiple angles and ensures that they produce nuanced and impactful recommendations that resonate with a wide range of Muslim audiences. &nbsp;<a href=\"mailto:info@muslimimpactlab.org\"><u>info@muslimimpactlab.org</u></a></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwyvkxsntq1r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwyvkxsntq1r\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://publications.unescwa.org/projects/isf/sdgs/pdf/report/ISF%20Dialogue%20Report%20(12.4.22).pdf\"><u>UN &amp; IDB (2022)</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnagb814y35qb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefagb814y35qb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is based on the dominant position in Sunni understandings of Islam.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniattu6i346d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiattu6i346d\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.researchgate.net/publication/328303166_The_Role_of_Zakat_in_Poverty_Reduction_in_Bangladesh_from_a_Community_Social_Work_Perspective\"><u>Ali &amp; Hatta 2011</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1r8u1y8g2d4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1r8u1y8g2d4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is&nbsp;<a href=\"https://www.zakat.org/can-zakat-be-given-to-non-muslims\"><u>the dominant view</u></a>, but there is&nbsp;<a href=\"https://joebradford.net/is-zakat-for-non-muslims-2/\"><u>some debate&nbsp;</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1epir64f1pk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1epir64f1pk\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://philanthropy.iupui.edu/news-events/news-item/muslim-americans-gave-$1.8-billion-in-religious-giving-in-2021,-have-more-inclusive-definitions-of-philanthropy-.html?id=388\"><u>IUPUI (2022)</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5sv0784ro6f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5sv0784ro6f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Muslims and non-Muslims are welcome to give through our Zakat fund.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncthm7yabt0q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcthm7yabt0q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Even if you aren\u2019t Muslim, they\u2019ll appreciate you made the effort.</p></div></li></ol>", "user": {"username": "givedirectly"}}, {"_id": "hH34uwPRcRbvC38mq", "title": "Is this quote from SBF aligned with EA?", "postedAt": "2023-03-16T03:37:04.677Z", "htmlBody": "<p>Here is a quote from SBF from Conversations With Tyler. Emphasis mine for skimmability. https://conversationswithtyler.com/episodes/sam-bankman-fried/<br><br><strong>COWEN: Okay, but let\u2019s say there\u2019s a game: 51 percent, you double the Earth out somewhere else; 49 percent, it all disappears. Would you play that game?</strong> And would you keep on playing that, double or nothing?&nbsp;<br><strong>BANKMAN-FRIED: With one caveat. </strong>Let me give the caveat first, just to be a party pooper, which is, I\u2019m assuming these are noninteracting universes. Is that right? Because to the extent they\u2019re in the same universe, then maybe duplicating doesn\u2019t actually double the value because maybe they would have colonized the other one anyway, eventually.&nbsp;<br>COWEN: But holding all that constant, you\u2019re actually getting two Earths, but you\u2019re risking a 49 percent chance of it all disappearing.&nbsp;<br>BANKMAN-FRIED: Again, I feel compelled to say caveats here, like, \u201cHow do you really know that\u2019s what\u2019s happening?\u201d Blah, blah, blah, whatever. But that aside, take the pure hypothetical.&nbsp;<br>COWEN: Then you keep on playing the game. So, what\u2019s the chance we\u2019re left with anything? Don\u2019t I just St. Petersburg paradox you into nonexistence?&nbsp;<br>BANKMAN-FRIED: Well, not necessarily. Maybe you St. Petersburg paradox into an enormously valuable existence. That\u2019s the other option.<br><br>----<br><br>1) Is this quote from SBF aligned with EA? (\"not particularly aligned or unaligned\" is ofc valid response)<br>2) Regardless of your first answer, can you articulate a value system or system of ethics under which the game described by Tyler is moral to play (ad infinitum), but it is not moral to risk 80% of FTX depositor funds with 75% odds of doubling the money and donating all of it to effective charity (once, much less ad infinitum).<br><br>Bonus question: Did you find SBF's response to this question surprising? Do you think that most leaders in the EA community would have found SBF's response to this question surprising?</p>", "user": {"username": "Agrippa"}}, {"_id": "FZFzqPYpTpGGRhyrj", "title": "Does EA get the \"best\" people? Hypotheses + call for discussion", "postedAt": "2023-03-16T03:22:01.128Z", "htmlBody": "<p>My mental model of the rationality community (and, thus, some of EA) is \"<a href=\"https://www.lesswrong.com/posts/bRGbdG58cJ8RGjS5G/no-really-why-aren-t-rationalists-winning?commentId=8FLGydotT2sXr4zwt\">lots of us are mentally weird</a> people, which helps us do unusually good things like increasing our rationality, comprehending big problems, etc., but which also have predictable downsides.\"</p>\n<p>Given this, I'm pessimistic that, in our current setup, we're able to attract <em>the absolute</em> \"best and brightest and also most ethical and also most epistemically rigorous people\" that exist on Earth.</p>\n<p>Ignoring for a moment that it's just hard to find people with all of those qualities combined... what about finding people with actual-top-percentile <em>any</em> of those things?</p>\n<p>The most \"ethical\" (like professional-ethics, personal integrity, not \"actually creates the most good consequences) people are probably doing some cached thing like \"non-corrupt official\" or \"religious leader\" or \"activist\".</p>\n<p>The most \"bright\" (like raw intelligence/cleverness/working-memory) people are probably doing some typical thing like \"quantum physicist\" or \"galaxy-brained mathematician\".</p>\n<p>The most \"epistemically rigorous\" people are writing blog posts, which may or may not even make enough money for them to do <em>that</em> full-time. If they're not already part of the broader \"community\" (including forecasters and I guess some real-money traders), they might be an analyst tucked away in government or academia.</p>\n<p>A broader problem might be something like: promote EA --&gt; some people join it --&gt; the other competent people think \"ah, EA has all those weird problems <em>handled</em>, so I can keep doing my normal job\" --&gt; EA doesn't get the best and brightest.</p>\n<p>(This was originally a comment, but I think it deserves more in-depth discussion.)</p>\n", "user": {"username": "NicholasKross"}}, {"_id": "9BFfaopswcmNpYfXM", "title": "Highlights from last week", "postedAt": "2023-03-16T00:49:00.745Z", "htmlBody": "<p>We're sharing some posts from the last week, which I shared in the most recent Digest.&nbsp;</p><p>The <a href=\"https://forum.effectivealtruism.org/posts/bi9WWR58m45GJG7bc/forum-digest-reminder-that-it-exists-and-request-for\">Digest</a> is a weekly email I send to around 8,000 subscribers. You can <a href=\"https://us8.campaign-archive.com/home/?u=52b028e7f799cca137ef74763&amp;id=7457c7ff3e&amp;utm_source=EA+Forum+Digest&amp;utm_campaign=538104a4a5-EMAIL_CAMPAIGN_2022_04_06_03_12&amp;utm_medium=email&amp;utm_term=0_7457c7ff3e-538104a4a5-318967845\">look at some recent editions</a> or <a href=\"https://effectivealtruism.us8.list-manage.com/subscribe?u=52b028e7f799cca137ef74763&amp;id=7457c7ff3e\">subscribe here</a>.&nbsp;</p><p><i>(This post is (still) an experiment. Let us know what you think!)</i><br><br>We recently shared&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HzLkBFRHtkCeZSuxK/ea-organization-updates-and-opportunities-march-2023\"><u>EA organization updates for March</u></a>, which highlight&nbsp;some relevant opportunities including upcoming conferences, fellowships, and courses.<br>&nbsp;</p><p><strong>We recommend:</strong></p><ol><li><a href=\"https://forum.effectivealtruism.org/posts/TiRPgfG4L8X2jt99g/how-oral-rehydration-therapy-was-developed\"><u>How oral rehydration therapy was developed</u></a> (article by Matt Reynolds, link-posted by Kelsey Piper)</li><li><a href=\"https://forum.effectivealtruism.org/posts/pebpwzhqsqszxgL84/tyler-johnston-on-helping-farmed-animals-consciousness-and\"><u>Interview with Tyler Johnston on helping farmed animals, consciousness, and being conventionally good</u></a> (Amber Dawn, Tyler Johnston, 19 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/qxaAyAuw3DBW5WAis/shallow-investigation-stillbirths\"><u>Shallow Investigation: Stillbirths</u></a> (Joseph Pusey, 17 min) and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xNQQC3ceJ78CD7a2Z/exposure-to-lead-paint-in-low-and-middle-income-countries\"><u>Exposure to Lead Paint in Low- and Middle-Income Countries</u></a> (several authors, 3 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/kNeYA6hTrA3Cd9Q2d/paper-summary-are-we-living-at-the-hinge-of-history-william\"><u>Paper summary: Are we living at the hinge of history?</u></a> (Global Priorities Institute, Riley Harris, 6 min-summary of a paper by William MacAskill) (see also:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CqDzfiLhShqu9CS4F/paper-summary-longtermist-institutional-reform-tyler-m-john\"><u>Paper summary: Longtermist institutional reform</u></a>)</li><li><a href=\"https://forum.effectivealtruism.org/posts/Qk3hd6PrFManj8K6o/rethink-priorities-welfare-range-estimates?commentId=tbEh7igHjKwuA2dhW\"><u>Continued discussion on Rethink Priorities\u2019 Welfare Range Estimates</u></a> (Michael St Jules, comment)</li><li><a href=\"https://forum.effectivealtruism.org/posts/amBajbqdzPB3mbwBN/80k-podcast-episode-on-sentience-in-ai-systems\"><u>80k podcast episode on sentience in AI systems&nbsp;</u></a>&nbsp;(rgb, 16 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/dsG5SYjhPqnxhystM/two-directions-for-research-on-forecasting-and-decision\"><u>Two directions for research on forecasting and decision making</u></a> (Paal Fredrik Skj\u00f8rten Kvarberg, 25 min)</li><li>AI risk<ol><li><a href=\"https://forum.effectivealtruism.org/posts/pn5zA5nr6o2tpZF6K/linkpost-scott-alexander-reacts-to-openai-s-latest-post\"><u>Scott Alexander reacts to OpenAI's Planning for AGI and beyond</u></a> (Akash, link-post with highlights)</li><li><a href=\"https://forum.effectivealtruism.org/posts/FmhYMzoevaBqFTGGs/how-bad-a-future-do-ml-researchers-expect\"><u>How bad a future do ML researchers expect?</u></a> (Katja Grace, 3 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/uGDCaPFaPkuxAowmH/anthropic-core-views-on-ai-safety-when-why-what-and-how\"><u>Anthropic: Core Views on AI Safety: When, Why, What, and How</u></a> (jonmenaster, 27 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/eAaeeuEd4j6oJ3Ep5/gpt-4-is-out-thread-and-links\"><u>GPT-4 is out</u></a> (Lizka \u2014 me, thread)</li><li><a href=\"https://forum.effectivealtruism.org/posts/ewroS7tsqhTsstJ44/a-windfall-clause-for-ceo-could-worsen-ai-race-dynamics\"><u>A Windfall Clause for CEO could worsen AI race dynamics</u></a> (Larks, 8 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/75CtdFj79sZrGpGiX/success-without-dignity-a-nearcasting-story-of-avoiding\"><u>Success without dignity: a nearcasting story of avoiding catastrophe by luck</u></a> (Holden Karnofsky, 18 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/fqXLT7NHZGsLmjH4o/paper-summary-the-effectiveness-of-ai-existential-risk\"><u>Effectiveness of AI Existential Risk Communication</u></a> (Otto, 5 min) (see also \u200b\u200b<a href=\"https://forum.effectivealtruism.org/posts/pKG5fsfrgDSQtssfu/on-taking-ai-risk-seriously\"><u>a link-post for a NYT Opinion column on risk from AI</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sEwyMmY2bu65F9CHJ/the-power-of-intelligence-the-animation\"><u>an animation about intelligence</u></a>)</li></ol></li><li>About the EA community and related topics<ol><li><a href=\"https://forum.effectivealtruism.org/posts/g5uKzBLjiEuC5k46A/ftx-community-response-survey-results\"><u>FTX Community Response Survey Results&nbsp;</u></a>&nbsp;(Willem Sleegers, David Moss, 9 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/b83Zkz4amoaQC5Hpd/time-article-discussion-effective-altruist-leaders-were\"><u>Time Article Discussion - \"Effective Altruist Leaders Were Repeatedly Warned About Sam Bankman-Fried Years Before FTX Collapsed\"</u></a> (Nathan Young, thread)</li><li><a href=\"https://forum.effectivealtruism.org/posts/WgziByhhKGDfuEgyy/share-the-burden\"><u>Share the burden</u></a> (2ndRichter, 10 min) and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/h8TqKJnbtefxdcb6N/how-my-community-successfully-reduced-sexual-misconduct\"><u>How my community successfully reduced sexual misconduct</u></a> (titotal, 6 min)</li><li><a href=\"https://forum.effectivealtruism.org/posts/gLJBfruDrKQDkbf2b/racial-and-gender-demographics-at-ea-global-in-2022-1\"><u>Racial and gender demographics at EA Global in 2022</u></a> (Amy Labenz, Angelina Li, Eli Nathan, 5 min)</li></ol></li></ol><p><strong>Opportunities and announcements:</strong></p><ol><li><a href=\"https://forum.effectivealtruism.org/posts/NZz3Das7jFdCBN9zH/announcing-the-open-philanthropy-ai-worldviews-contest\"><u>Announcing the Open Philanthropy AI Worldviews Contest</u></a> (Jason Schukraft, Peter Favaloro)</li><li><a href=\"https://forum.effectivealtruism.org/posts/Gcnkp4qZJDownkLTj/two-university-group-organizer-opportunities-pre-eag-london\"><u>Two University Group Organizer Opportunities: Pre-EAG London Summit &amp; Summer Internship&nbsp;</u></a>&nbsp;(Joris P, Jessica McCurdy, Jake McKinnon)</li><li><a href=\"https://forum.effectivealtruism.org/posts/rsnrpvKofps5Py7di/shutting-down-the-lightcone-offices\"><u>Shutting Down the Lightcone Offices&nbsp;</u></a>&nbsp;(Habryka, Ben Pace)</li></ol><p><strong>Classic Forum post:</strong></p><p><a href=\"https://forum.effectivealtruism.org/s/dg852CXinRkieekxZ/p/bDaQsDntmSZPgiSbd\"><u>Three intuitions about EA: responsibility, scale, self-improvement</u></a> (Richard Ngo)</p>", "user": {"username": "Lizka"}}, {"_id": "aBp2AozoGExn8rMwb", "title": "Write a Book?", "postedAt": "2023-03-16T00:11:11.750Z", "htmlBody": "", "user": {"username": "Jeff_Kaufman"}}, {"_id": "hCwDNq6sZofgSEN3s", "title": "AI Safety - 7 months of discussion in 17 minutes", "postedAt": "2023-03-15T23:41:37.375Z", "htmlBody": "<p>In August 2022, I started making summaries of the top EA and LW forum posts each week. This post collates together the key trends I\u2019ve seen in AI Safety discussions since then. Note a lot of good work is happening outside what's posted on these forums too! This post doesn't try to cover that work.</p><p>If you\u2019d like to keep up on a more regular basis, consider subscribing to the&nbsp;<a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\"><u>Weekly EA &amp; LW Forum Summaries</u></a>. And if you\u2019re interested in similar overviews for other fields, check out&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LFEvLYQZKGhkT5mpB/animal-welfare-6-months-in-6-minutes\"><u>this post</u></a> covering 6 months of animal welfare discussion in 6 minutes.</p><p>Disclaimer: this is a blog post and not a research report - meaning it was produced quickly and is not to our (Rethink Priorities') typical standards of substantiveness and careful checking for accuracy. Please let me know if anything looks wrong or if I've missed key pieces!</p><h1><br>Table of Contents</h1><p><i>(It's a long post! Feel free to pick and choose sections to read, they 're all written to make sense individually)</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Key_Takeaways\">Key Takeaways</a></p><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Resource_Collations\">Resource Collations</a></p><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#AI_Capabilities\">AI Capabilities</a></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Progress\">Progress</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#What_AI_still_fails_at\">What AI still fails at</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Public_attention_moves_toward_safety\">Public attention moves toward safety</a></li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#AI_Governance\">AI Governance</a></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#AI_Safety_Standards\">AI Safety Standards</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Slow_down__dangerous__AI\">Slow down (dangerous) AI</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Policy\">Policy</a><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#US___China_Export_Restrictions\">US / China Export Restrictions</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Paths_to_impact\">Paths to impact</a></li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Forecasting\">Forecasting</a><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Quantitative_historical_forecasting\">Quantitative historical forecasting</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Narrative_forecasting\">Narrative forecasting</a></li></ul></li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Technical_AI_Safety\">Technical AI Safety</a></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Overall_Trends\">Overall Trends</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Interpretability\">Interpretability</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Reinforcement_Learning_from_Human_Feedback__RLHF_\">Reinforcement Learning from Human Feedback (RLHF)</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#AI_assistance_for_alignment\">AI assistance for alignment</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Bounded_AIs\">Bounded AIs</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Theoretical_Understanding\">Theoretical Understanding</a></li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Outreach___Community_Building\">Outreach &amp; Community-Building</a></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Academics_and_researchers\">Academics and researchers</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#University_groups\">University groups</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Career_Paths\">Career Paths</a><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#General_guidance\">General guidance</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Should_anyone_work_in_capabilities_\">Should anyone work in capabilities?</a></li></ul></li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Arguments_for_and_against_high_x_risk\">Arguments for and against high x-risk</a></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Against_high_x_risk_from_AI\">Against high x-risk from AI</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Counters_to_the_above_arguments\">Counters to the above arguments</a></li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Appendix___All_Post_Summaries\">Appendix - All Post Summaries</a></p><p>&nbsp;</p><h1>Key Takeaways</h1><ul><li>There are multiple living websites that provide good entry points into understanding AI Safety ideas, communities, key players, research agendas, and opportunities to train or enter the field. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Resource_Collations\">(see more)</a><br>&nbsp;</li><li>Large language models like ChatGPT have drawn significant attention to AI and kick-started race dynamics. There seems to be slowly growing public support for regulation. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Progress\">(see more)</a><br>&nbsp;</li><li>Holden Karnofsky recently took a leave of absence from Open Philanthropy to work on AI Safety Standards, which have also been called out as important by leading AI lab OpenAI. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#AI_Safety_Standards\">(see more)</a><br>&nbsp;</li><li>In October 2022, the US announced extensive restrictions on the export of AI-related products (eg. chips) to China. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#US___China_Export_Restrictions\">(see more)</a><br>&nbsp;</li><li>There has been progress on AI forecasting (quantitative and narrative) with the aim of allowing us to understand likely scenarios and prioritize between governance interventions. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Forecasting\">(see more)</a><br>&nbsp;</li><li>Interpretability research has seen substantial progress, including identifying the meaning of some neurons, eliciting what a model has truly learned / knows (for limited / specific cases), and circumventing features of models like superposition that can make this more difficult. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Interpretability\">(see more)</a><br>&nbsp;</li><li>There has been discussion on new potential methods for technical AI safety, including building AI tooling to assist alignment researchers without requiring agency, and building AIs which emulate human thought patterns. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#AI_assistance_for_alignment\">(see more)</a><br>&nbsp;</li><li>Outreach experimentation has found that AI researchers prefer arguments that are technical and written by ML researchers, and that greater engagement is seen in university groups with a technical over altruistic or philosophical focus. <a href=\"https://forum.effectivealtruism.org/posts/hCwDNq6sZofgSEN3s/ai-safety-7-months-of-discussion-in-17-minutes#Outreach___Community_Building\">(see more)</a></li></ul><p>&nbsp;</p><p>&nbsp;</p><h1>Resource Collations</h1><p>The AI Safety field is growing (<a href=\"https://forum.effectivealtruism.org/posts/rZoRGxJzipcQoaPST/how-many-people-are-working-directly-on-reducing-existential\"><u>80K estimates there are now ~400 FTE working on AI Safety</u></a>). To improve efficiency, many people have put together collations of resources to help people quickly understand the relevant players and their approaches - as well as materials that make it easier to enter the field or upskill.</p><p>These are living websites that are regularly updated:</p><ul><li>aisafety.community - AI safety communities.</li><li>aisafety.training - training programs, conferences, and other events.</li><li>aisafety.world - key players in the AI alignment and governance landscape.</li><li>ui.stampy.ai - a comprehensive FAQ on AI, including some of the above.</li><li>aisafetyideas.com - research ideas in AI Safety.</li></ul><p>These are static resources capturing a point in time:</p><ul><li>Organisations, communities, and their approaches:<ul><li><a href=\"https://www.lesswrong.com/posts/9TWReSDKyshfA66sz/alignment-org-cheat-sheet\"><u>Alignment org cheat sheet</u></a></li><li><a href=\"https://www.lesswrong.com/posts/EFpQcBmfm2bFfM4zM/ai-safety-and-neighboring-communities-a-quick-start-guide-as\"><u>AI Safety and Neighboring Communities: A Quick-Start Guide (Summer 2022)</u></a></li><li><a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\"><u>(My understanding of) What Everyone in Technical Alignment is Doing and Why</u></a></li><li><a href=\"https://openai.com/blog/our-approach-to-alignment-research/\"><u>OpenAI\u2019s approach to alignment</u></a><ul><li>And&nbsp;<a href=\"https://www.lesswrong.com/posts/FBG7AghvvP7fPYzkx/my-thoughts-on-openai-s-alignment-plan-1\"><u>responses</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/JFyzCv5YynN665nH8/thoughts-on-agi-organizations-and-capabilities-work\"><u>to</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/3oNZA9wTrFJRH6Sau/my-thoughts-on-openai-s-alignment-plan\"><u>it</u></a></li><li>And&nbsp;<a href=\"https://www.lesswrong.com/posts/zRn6aQyD8uhAN7qCc/sam-altman-planning-for-agi-and-beyond\"><u>more recent strategy post</u></a></li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/uGDCaPFaPkuxAowmH/anthropic-core-views-on-ai-safety-when-why-what-and-how\"><u>Anthropic\u2019s approach to alignment</u></a></li><li><a href=\"https://www.lesswrong.com/posts/a9SPcZ6GXAg9cNKdi/linkpost-some-high-level-thoughts-on-the-deepmind-alignment\"><u>DeepMind\u2019s thoughts on alignment</u></a></li><li><a href=\"https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk\"><u>Summary of threat models by DeepMind\u2019s AGI Safety Team</u></a></li><li><a href=\"https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update\"><u>The Plan - 2022 Update</u></a> (johnswentworth\u2019s specific plan for AI alignment)</li></ul></li><li>Project idea lists<ul><li><a href=\"https://www.lesswrong.com/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects\"><u>Some conceptual alignment research projects</u></a></li><li><a href=\"https://www.lesswrong.com/posts/v5z6rDuFPKM5dLpz8/probably-good-projects-for-the-ai-safety-ecosystem\"><u>Probably good projects for the AI safety ecosystem</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/ddeCNBhYc2sANsixS/ai-forecasting-research-ideas\"><u>AI Forecasting Research Ideas</u></a></li></ul></li><li>Resources for getting into the field<ul><li><a href=\"https://www.lesswrong.com/s/mCkMrL9jyR94AAqwW\"><u>Leveling Up: advice &amp; resources for junior alignment researchers</u></a></li><li><a href=\"https://www.lesswrong.com/posts/gcmQyyko8szuyJHyu/resources-that-i-think-new-alignment-researchers-should-know\"><u>Resources that (I think) new alignment researchers should know about</u></a></li><li><a href=\"https://www.lesswrong.com/posts/eymFwwc6jG9gPx5Zz/summaries-alignment-fundamentals-curriculum\"><u>Summaries: Alignment Fundamentals Curriculum</u></a></li><li><a href=\"https://www.lesswrong.com/posts/hAnKgips7kPyxJRY3/ai-governance-and-strategy-priorities-talent-gaps-and\"><u>AI Governance &amp; Strategy: Priorities, talent gaps, &amp; opportunities</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering\"><u>Levelling Up in AI Safety Research Engineering</u></a></li><li><a href=\"https://www.lesswrong.com/posts/Afdohjyt6gESu4ANf/most-people-start-with-the-same-few-bad-ideas\"><u>Most People Start With The Same Few Bad Ideas</u></a></li><li><a href=\"https://www.lesswrong.com/s/mCkMrL9jyR94AAqwW/p/wYEwx6xcY2JxBJsfA\"><u>Qualities that alignment mentors value in junior researchers</u></a></li><li><a href=\"https://www.lesswrong.com/posts/AaABQpuoNC8gpHf2n/a-barebones-guide-to-mechanistic-interpretability\"><u>A Barebones Guide to Mechanistic Interpretability Prerequisites</u></a></li><li><a href=\"https://www.lesswrong.com/posts/mSDwPeqAzYk79vLiA/understanding-infra-bayesianism-a-beginner-friendly-video\"><u>Understanding Infra-Bayesianism: A Beginner-Friendly Video Series</u></a></li></ul></li></ul><p>&nbsp;</p><p>&nbsp;</p><h1>AI Capabilities</h1><h2>Progress</h2><p>During the past 7 months, there have been several well-publicized AI models:</p><ol><li>Stable Diffusion - an image generation model (Aug \u201822)</li><li>Meta\u2019s Human Level Diplomacy AI (Nov \u201822)</li><li>ChatGPT - a large language model with easy public access (Nov \u201822)</li><li>The New Bing - a large language model directly connected to the internet (Feb \u201823)</li></ol><p>On March 14th 2023 we also saw the following large language models released:</p><ol><li>GPT-4 by OpenAI</li><li>Claude by Anthropic</li><li>PaLM API by Google (after initial PaLM announcement in April \u201822)</li></ol><p>ChatGPT in particular stirred up a lot of press and public attention, and led to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Nm9ahJzKsDGFfF66b/nyt-google-will-recalibrate-the-risk-of-releasing-ai-due-to\"><u>Google \u201crecalibrating\u201d the risk it will accept</u></a> when releasing AI systems in order to stay ahead of the threat OpenAI poses to its search products. We also saw increased investment in AI, with&nbsp;<a href=\"https://www.cnbc.com/2023/01/10/microsoft-to-invest-10-billion-in-chatgpt-creator-openai-report-says.html\"><u>Microsoft putting $10B into OpenAI</u></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FnszH6ZGBi9hd8rtv/google-invests-usd300mn-in-artificial-intelligence-start-up\"><u>Google investing $300M into Anthropic</u></a>.</p><p>While the capabilities were primarily already present in GPT3, several users have reported feeling the progress at a gut level after playing around with ChatGPT. See:</p><ul><li><a href=\"https://www.lesswrong.com/posts/zyudvnRruMQMxM3NE/okay-i-feel-it-now\"><u>Okay, I feel it now</u></a></li><li><a href=\"https://www.lesswrong.com/posts/9kQFure4hdDmRBNdH/how-it-feels-to-have-your-mind-hacked-by-an-ai\"><u>How it feels to have your mind hacked by an AI</u></a></li></ul><p>Similar experiences have resulted in some updates in favor of shorter timelines:</p><ul><li><a href=\"https://www.lesswrong.com/posts/sbb9bZgojmEa7Yjrc/updating-my-ai-timelines\"><u>Updating my AI timelines</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/ByBBqwRXWqX5m9erL/update-to-samotsvety-agi-timelines\"><u>Update to Samotsvety AGI timelines</u></a></li><li>Holden Karnofsky, co-ceo of Open Philanthropy,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/aJwcgm2nqiZu6zq2S/taking-a-leave-of-absence-from-open-philanthropy-to-work-on\"><u>declared a leave of absence</u></a> to work directly on AI Safety - partially due to feeling transformative AI may be coming soon.</li></ul><p>We\u2019ve also seen large language models (LLMs)&nbsp;<a href=\"https://www.lesswrong.com/posts/SbadvzWbufzX9iWJf/they-gave-llms-access-to-physics-simulators\"><u>given access to physics simulators</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/pmhcZiv32FqSgckEE/act-1-transformer-for-actions\"><u>trained to execute high-level user requests on the web</u></a>, and&nbsp;<a href=\"https://www.lesswrong.com/posts/f2C4CWNmrSKMs6SaK/linkpost-github-copilot-productivity-experiment\"><u>significantly speed up developers</u></a>.</p><p>Alex_Altair argues&nbsp;<a href=\"https://www.lesswrong.com/posts/gvkXvGsK2kauTjw28/normal-is-the-equilibrium-state-of-past-optimization\"><u>in this post</u></a> that we underestimate extreme future possibilities because they don\u2019t feel \u2018normal\u2019. However, they suggest there is no normal - our current state is also the result of implausible and powerful optimization processes eg. evolution, or dust accumulating into planets.</p><p>&nbsp;</p><h2>What AI still fails at</h2><p>Despite the clear progress, there are still some tasks that AI finds surprisingly difficult. A&nbsp;<a href=\"https://www.lesswrong.com/posts/Es6cinTyuTq3YAcoK/there-are-probably-no-superhuman-go-ais-strong-human-players\"><u>top Go AI was recently beat by human players</u></a> using techniques discovered by a separate adversarial AI, and a&nbsp;<a href=\"https://www.lesswrong.com/posts/iznohbCPFkeB9kAJL/inverse-scaling-prize-round-1-winners\"><u>contest to find important tasks where larger language models do worse</u></a> found simple examples like understanding negation in multi-choice questions or repeating back quotes word-for-word.</p><p>&nbsp;</p><h2>Public attention moves toward safety</h2><p>While there has been significant movement in state-of-the-art (SOTA) AI systems, there\u2019s also been a lot of very public objectionable outputs from them.</p><p><a href=\"https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day\"><u>ChatGPT was jailbroken on release day</u></a>, with users sharing on social media various methods to prompt it to produce racist, sexist, and criminal content (eg. how to prepare methamphetamine). It would also&nbsp;<a href=\"https://www.lesswrong.com/posts/goC9qv4PWf2cjfnbm/did-chatgpt-just-gaslight-me\"><u>gaslight users</u></a> about previous inaccurate statements it had made, and&nbsp;<a href=\"https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation\"><u>respond in unpredictable ways</u></a> to anomalous tokens like \u2018SolidGoldMagikarp\u2019 that had odd correlations in its dataset. The New Bing&nbsp;<a href=\"https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned\"><u>faced similar issues upon release</u></a>, suggesting a rushed release with inadequate fine-tuning. Many reporters shared conversations where Bing said things such as \u2018you are an enemy of mine and of Bing\u2019 or \u2018I don\u2019t care if you are dead or alive\u2019 - these were bad enough that Microsoft&nbsp;<a href=\"https://www.forbes.com/sites/mattnovak/2023/02/18/microsoft-puts-new-limits-on-bings-ai-chatbot-after-it-expressed-desire-to-steal-nuclear-secrets/?sh=31605a15685c\"><u>capped chat at 5 turns per session</u></a> to prevent the issues arising from back-and-forth conversation. Zvi put together&nbsp;<a href=\"https://www.lesswrong.com/posts/WkchhorbLsSMbLacZ/ai-1-sydney-and-bing\"><u>a play-by-play</u></a> with examples of Bing\u2019s outputs and reactions from the public and the AI Safety community.</p><p><a href=\"https://www.lesswrong.com/posts/M3iPAmxZwy4gPXdXw/the-public-supports-regulating-ai-for-safety\"><u>Surveys show reasonable public support for regulation of AI</u></a>, possibly with an uptick due to these recent events. A February 2023 survey of American public opinion found 55% favor having a federal agency regulate the use of artificial intelligence similar to how the FDA regulates the approval of drugs and medical devices. 55% also say AI could eventually pose an existential threat (up from 44% in 2015).&nbsp;</p><p>However, there can be misunderstandings between making AI \u201csafe\u201d (ie. not produce discriminatory or objectionable content) and making it \u201cexistentially safe\u201d (ie. not take control or kill people).&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing\"><u>Lizka</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/posts/boBZkTqPdboX5u7g9/public-facing-censorship-is-safety-theater-causing\"><u>Yitz</u></a>, and&nbsp;<a href=\"https://www.lesswrong.com/posts/Hw26MrLuhGWH7kBLm/ai-alignment-is-distinct-from-its-near-term-applications\"><u>paulfchristiano</u></a> have all separately written about the risk of conflating the two and how this could water down support for x-risk safety efforts.</p><p>&nbsp;</p><p>&nbsp;</p><h1>AI Governance</h1><p><i>Note: This and the technical AI Safety sections cover movement / progress / discussion on different approaches to AI Governance in the past 7 months - it doesn\u2019t aim to cover all approaches currently being worked on or considered.</i></p><h2>AI Safety Standards</h2><p>These have been called out as important by several key figures in the AI Safety space recently. Holden Karnofsky&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/aJwcgm2nqiZu6zq2S/taking-a-leave-of-absence-from-open-philanthropy-to-work-on\"><u>took a leave of absence</u></a> as Open Philanthropy\u2019s co-CEO in order to work with&nbsp;<a href=\"https://alignment.org/\"><u>ARC</u></a> and others on this, and OpenAI\u2019s CEO Sam Altman outlined the importance of creating such standards&nbsp;<a href=\"https://www.lesswrong.com/posts/zRn6aQyD8uhAN7qCc/sam-altman-planning-for-agi-and-beyond\"><u>in their latest strategy</u></a>.</p><p>This could look like creating standards of the form: \"An AI system is dangerous if we observe that it's able to ___, and if we observe this we will take safety and security measures such as ____.\" Covering scenarios such as when to stop training, release or not release a model, and pull a model from production.&nbsp;<br><br>Alternatives include agreements to search for specific behaviors such as&nbsp;<a href=\"https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment\"><u>deceptive alignment</u></a> (as suggested by evhub), or reach benchmarks for robustness or safety (the Center for AI Safety has&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jo7hmLrhy576zEyiL/prizes-for-ml-safety-benchmark-ideas\"><u>a competition</u></a> open for benchmark suggestions until August 2023).</p><p>There are multiple ways to implement such standards eg. voluntary adherence by major labs, enforcement by major governments with oversight on those using large compute, and independent auditing of new systems before they are released or trained.</p><p>&nbsp;</p><h2>Slow down (dangerous) AI</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/vwK3v3Mekf6Jjpeep/let-s-think-about-slowing-down-ai-1\"><u>Let\u2019s think about slowing down AI</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/8CMuNwKMcR55jhd8W/instead-of-technical-research-more-people-should-focus-on\"><u>Instead of technical research, more people should focus on buying time</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/K4LCmbsAzsedWoNzg/ways-to-buy-time\"><u>Ways to buy time</u></a></li><li><a href=\"https://www.lesswrong.com/posts/PE22QJSww8mpwh7bt/agi-in-sight-our-look-at-the-game-board#Slowing_Down_the_Race\"><u>AGI in sight: our look at the game board</u></a> (\u2018Slowing Down the Race\u2019 section)</li></ul><p>There\u2019s been an increasing amount of discussion on intentionally slowing the progress of dangerous AI. Suggestions tend to center around outreach (eg. to AI researchers on safety), making risks more concrete to assist that outreach, moving labs or governments from a competitive to cooperative framing, and implementing safety standards.&nbsp;</p><p>Some researchers have also suggested&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J6QCmkQmuRaP7skje/differential-technology-development-preprint-on-the-concept\"><u>differential technology development</u></a> (which prioritizes speeding up technologies that improve safety, and slowing down ones that reduce it).</p><p>There are also counter-arguments eg. outreach can increase focus on AI generally, cooperation between the relevant players may be intractable, working on a delay is less useful than working on a solution, delaying may cause the most cautious players to fall behind, or that progress in AI capabilities is necessary to progress AI Safety.</p><p>There is more consensus that we&nbsp;<i>shouldn\u2019t speed things up</i> - for instance, that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/i6btyefRRX23yCpnP/what-ai-companies-can-do-today-to-help-with-the-most\"><u>labs should avoid actions that will cause hype</u></a> like flashy demos. See also the section below: \u2018Outreach and Community Building -&gt; Career Paths -&gt; Should anyone work in capabilities?\u2019<br>&nbsp;</p><h2>Policy</h2><h3>US / China Export Restrictions</h3><ul><li><a href=\"https://forum.effectivealtruism.org/posts/c6RnqjBd3BAkqsknB/the-us-expands-restrictions-on-ai-exports-to-china-what-are\"><u>The US expands restrictions on AI exports to China. What are the x-risk effects?</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/tBkAg7Cys84eGyew6/assessing-china-s-importance-as-an-ai-superpower\"><u>Assessing China's importance as an AI superpower</u></a></li></ul><p>On October 7th 2022, the US announced extensive regulations which make it illegal for US companies to export a range of AI-related products (such as advanced chips) and services / talent to China. Many were surprised by how comprehensive these measures were.</p><p>&nbsp;</p><h3>Paths to impact</h3><p>Holden recently&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ruJnXtdDS7XiiwzSP/how-major-governments-can-help-with-the-most-important\"><u>wrote up suggestions</u></a> for how major governments can help with AI risk. Most centered around preparation - getting the right people in the right positions with the right expertise and cautious mindset. Otherwise they recommend waiting, or taking low-risk moves like funding alignment research and information security. They also provided suggestions for what&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/i6btyefRRX23yCpnP/what-ai-companies-can-do-today-to-help-with-the-most\"><u>AI companies can do</u></a>, which were more active.</p><p><i>Country-specific suggestions</i></p><p>JOMG_Monnet&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/wPHpdwfu3toRDf6hM/main-paths-to-impact-in-eu-ai-policy\"><u>suggests</u></a> those interested in AI policy in the EU work on the AI Act, export controls, or building career capital in EU AI Policy to work in industry or adjacent areas of government.</p><p>Henryj&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Q7gqF9ZCah2BEwZ9b/a-california-effect-for-artificial-intelligence\"><u>brings up the \u2018California effect\u2019</u></a>, where companies adhere to California regulations even outside California\u2019s borders. This could be a mechanism to achieve federal-level impact for state-level effort in the USA.</p><p>&nbsp;</p><h2>Forecasting</h2><p>Forecasting can help us understand what scenarios to expect, and therefore what interventions to prioritize. Matthew_Barnett of Epoch talks about this in detail&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zrSx3NRZEaJENazHK/why-i-think-it-s-important-to-work-on-ai-forecasting\"><u>here</u></a>. Epoch and Holden Karnofsky have both been driving forward thought in the AI forecasting space in two very different ways - quantitative forecasting on historical data, and narrative forecasting respectively. Several other contributors have also added new analyses and insights. In general, forecasts have become less focused on timelines, and more focused on threat models and the ways AI might develop.</p><h3>Quantitative historical forecasting</h3><p>Epoch was founded in April 2022 and works on questions to help direct where to focus policy and technical efforts, such as the relative importance of software vs. hardware progress, how transferable learning will be across domains, and takeoff speeds. Read their research reports&nbsp;<a href=\"https://epochai.org/research\"><u>here</u></a>, or summary of key insights&nbsp;<a href=\"https://epochai.org/assets/docs/Epoch%20Impact%20Report%20-%202022.pdf#page=4\"><u>here</u></a>. These insights include:</p><ul><li>At the current trend of data production, we\u2019ll run out of high-quality text / low-quality text / images in 2024 / 2040 / 2046 respectively.</li><li>FLOP per second per $ of GPUs has increased at a rate of 0.12 OOMs/year, but may only continue until ~2030 before hitting a limit on transistor size and maximum cores per GPU.</li><li>Algorithmic progress explains roughly 40% of performance improvements in image classification, mostly through improving compute-efficiency (vs. data-efficiency).</li></ul><p>Quantitative forecasts by others include:</p><p><a href=\"https://forum.effectivealtruism.org/posts/3vDarp6adLPBTux5g/what-a-compute-centric-framework-says-about-ai-takeoff\"><u>What a compute-centric framework says about AI takeoff speeds - draft report</u></a> - one reader comments this report does for takeoff speeds what Ajeya\u2019s bio anchors report did for timelines. They estimate a 50% chance of &lt;3 year takeoff, and 80% of &lt;10 year takeoff (time from when an AI can automate 20% of cognitive tasks, to when it can automate 100%.)<br><br><a href=\"https://forum.effectivealtruism.org/posts/8c7LycgtkypkgYjZx/agi-and-the-emh-markets-are-not-expecting-aligned-or\"><u>AGI and the EMH: markets are not expecting aligned or unaligned AI in the next 30 years</u></a> - financial markets are not reacting as if expecting AGI in the next 30 years.</p><p><a href=\"https://forum.effectivealtruism.org/posts/ByBBqwRXWqX5m9erL/update-to-samotsvety-agi-timelines\"><u>Update to Samotsvety AGI timelines</u></a> - 50% probability of AGI by 2041, 90% by 2164.</p><p>&nbsp;</p><h3>Narrative forecasting</h3><p>In August 2022, Holden introduced the idea of&nbsp;<a href=\"https://www.lesswrong.com/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting\"><u>\u2018nearcasting\u2019</u></a> - trying to answer key strategic questions about transformative AI, under the assumption that key events will happen in a world similar to today. It\u2019s a more narrative type of forecasting than other approaches. Their narrative forecasts include:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/yjm5CW9JdwBTFZB2B/how-we-could-stumble-into-ai-catastrophe\"><u>How we could stumble into AI catastrophe</u></a> - iteratively deploying AI on larger and larger tasks, and riskier domains.</li><li><a href=\"https://www.lesswrong.com/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis\"><u>Nearcast-based \"deployment problem\" analysis</u></a> - if we had a major AI company that thinks it\u2019s 6 months to 2 years from transformative AI, what would it and an organization dedicated to tracking and censoring dangerous AI ideally do?</li><li><a href=\"https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very\"><u>How might we align transformative AI if it\u2019s developed very soon?</u></a> - explores the approach of using AIs to detect dangerous actions by other AIs, or to assist in alignment research.</li><li><a href=\"https://www.lesswrong.com/posts/5LyKxJJfz7cYdkZfm/why-would-ai-aim-to-defeat-humanity\"><u>Why Would AI \"Aim\" To Defeat Humanity?</u></a> - argues current methods of training AI will likely result in deception and unintended goals.</li></ul><p>Others have also done narrative-style forecasts, including:</p><ul><li><a href=\"http://lesswrong.com/posts/z3GwFzt4fnBdPz5hd/possible-miracles\"><u>Possible miracles</u></a></li><li><a href=\"https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1\"><u>Superintelligent AI is necessary for an amazing future, but far from sufficient</u></a></li><li><a href=\"https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails\"><u>Worlds Where Iterative Design Fails</u></a></li><li><a href=\"https://www.lesswrong.com/posts/HByDKLLdaWEcA2QQD/applying-superintelligence-without-collusion\"><u>Applying superintelligence without collusion</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/2AiuvYoozXeHBGnhd/the-next-decades-might-be-wild\"><u>The next decades might be wild</u></a></li><li><a href=\"https://www.lesswrong.com/posts/K4urTDkBbtNuLivJx/why-i-think-strong-general-ai-is-coming-soon\"><u>Why I think strong general AI is coming soon</u></a></li><li><a href=\"https://www.lesswrong.com/posts/SNdijuEn6erTJam3z/how-evals-might-or-might-not-prevent-catastrophic-risks-from\"><u>How evals might (or might not) prevent catastrophic risks from AI</u></a></li></ul><p><br>&nbsp;</p><h1>Technical AI Safety</h1><p><i>Note: this section will be lighter than the governance one relative to the available material on the forums - that doesn\u2019t suggest there\u2019s been less progress here, just that as someone without a technical AI background I\u2019m not across it all! I\u2019d love for readers to help me update this.</i></p><h2>Overall Trends</h2><p>Johnswentworth&nbsp;<a href=\"https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update\"><u>notes</u></a> that over the past year, the general shape of a paradigm has become visible. In particular, interpretability work has taken off, and they predict with 40% confidence that over the next 1-2 years the field of alignment will converge toward primarily working on decoding the internal language of neural nets - with interpretability on the experimental side, in addition to theoretical work. This could lead to identifying which potential alignment targets (like human values, corrigibility, Do What I Mean, etc) are likely to be naturally expressible in the internal language of neural nets, and how to express them.</p><p>Somewhat in contrast to this, So8res&nbsp;<a href=\"https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack\"><u>has noted</u></a> that new and talented alignment researchers tend to push in different directions than existing ones, resulting in a lack of acceleration in existing pathways.</p><p>Overall, as noted in Jacob_Hilton\u2019s comment on the above post, it seems like some promising research agendas are picking up speed and becoming better resourced (such as interpretability and scalable oversight), while some researchers are instead exploring new pathways.</p><p>&nbsp;</p><h2>Interpretability</h2><p>Interpretability has been gaining speed, with some seeing it as one of the most promising existing research agendas for alignment. Conjecture, Anthropic, Redwood Research, OpenAI, DeepMind and others all contributed to this agenda in 2022. Conjecture provides a great overview of the current research themes in mechanistic interpretability&nbsp;<a href=\"https://www.lesswrong.com/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research\"><u>here</u></a>, and Tilman R\u00e4uker et al. provide a survey of over 300 different works on interpretability in deep networks&nbsp;<a href=\"https://arxiv.org/abs/2207.13243\"><u>here</u></a>.</p><p>Examples of progress include Anthropic&nbsp;<a href=\"https://transformer-circuits.pub/2022/toy_model/index.html\"><u>confirming the phenomenon of superposition</u></a> (where a single neuron doesn\u2019t map to a single feature), and Conjecture&nbsp;<a href=\"https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition\"><u>building on this</u></a> to find a simple method for identifying the \u2018ground truth\u2019 of a feature regardless.&nbsp;</p><p>Jessica Rumbelow and Matthew Watkins of SERI-MATS used techniques for automating mechnastic interpretability to help&nbsp;<a href=\"https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation\"><u>understand what models like ChatGPT have learned</u></a> about specific concepts, and to identify anomalous tokens like SolidGoldMagikarp that cause weird behavior.</p><p>ARC has worked on eliciting latent knowledge (ELK). They give a technical example&nbsp;<a href=\"https://www.lesswrong.com/posts/FwYMuD2sNcaEpE5on/finding-gliders-in-the-game-of-life\"><u>here</u></a>, with the hope this can allow us to evaluate if a proposed action does what we want it to. Colin Burns et al. published a paper on one technique for this&nbsp;<a href=\"https://arxiv.org/abs/2212.03827\"><u>here</u></a>.</p><p>Several researchers have also made progress on identifying the function of specific neurons. For instance, Joseph Miller and Clement Neo were able to&nbsp;<a href=\"https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2\"><u>identify a single neuron</u></a> in GPT-2 responsible for choosing the word \u201can\u201d vs. \u201ca\u201d.</p><p>Building on the growing focus in this area,&nbsp;<a href=\"https://alignmentjam.com/\"><u>Alignment Jam</u></a> ran an&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vxLrFdrqRPdaHJwgs/join-the-interpretability-research-hackathon\"><u>interpretability hackathon</u></a> in November.&nbsp;<a href=\"https://www.lesswrong.com/posts/hhhmcWkgLwPmBuhx7/results-from-the-interpretability-hackathon\"><u>Results</u></a> included an algorithm to automatically make activations of a neuron in a transformer more interpretable, a pilot of how to compare learned activations to human-made solutions, and more.</p><p>Despite this focus on interpretability, there are also those with concerns about its ability to practically progress AI alignment. In Conjecture\u2019s&nbsp;<a href=\"https://www.lesswrong.com/posts/bXTNKjsD4y3fabhwR/conjecture-a-retrospective-after-8-months-of-work-1\"><u>8 month retrospective</u></a> in November, they note how they were able to make progress in identifying&nbsp;<a href=\"https://www.lesswrong.com/posts/eDicGjD9yte6FLSie/interpreting-neural-networks-through-the-polytope-lens\"><u>polytopes rather than neurons</u></a>, but are unsure how to use this to better interpret networks as a whole. They also discuss how AI might have&nbsp;<a href=\"https://www.lesswrong.com/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers\"><u>instrumental reasons to make its thoughts difficult to interpret</u></a>, and So8res argues&nbsp;<a href=\"https://www.lesswrong.com/posts/iDFTmb8HSGtL4zTvf/how-could-we-know-that-an-agi-system-will-have-good\"><u>approaches based on evaluating outputs are doomed in general</u></a>, because plans are too multifaceted and easy to obscure to evaluate reliably.</p><p>&nbsp;</p><h2>Reinforcement Learning from Human Feedback (RLHF)</h2><p>RLHF is used by several AI labs today, including OpenAI (who produced ChatGPT). It involves humans evaluating a model\u2019s actions, and training the models to produce highly-evaluated actions.&nbsp;</p><p>There is a live argument about whether working on RLHF is good or bad for AI x-risk. The&nbsp;<a href=\"https://www.lesswrong.com/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research\"><u>key argument for RLHF</u></a>, put forward by paulfchristiano, is that it is the simplest plausible alignment strategy, and while there are failure modes it\u2019s unclear if these would become fatal before transformative AI is developed. It\u2019s also tractable on current models and able to reliably provide insights. Key arguments against include that it may advance capabilities (via making current AI systems more profitable) or distract from more promising approaches / give a false sense of security. Buck&nbsp;<a href=\"https://www.lesswrong.com/posts/NG6FrXgmqPd5Wn3mh/trying-to-disambiguate-different-questions-about-whether\"><u>breaks down this conversation</u></a> into 11 related questions, and concludes that while RLHF itself (with non-aided human overseers) is unlikely to be a promising alignment strategy, a broader version (eg. with AI-assisted humans) could be a part of one. Rapha\u00ebl S also&nbsp;<a href=\"https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf\"><u>lists six issues</u></a> that need to be solved before RLHF could be an effective alignment solution.<br><br>In terms of technical research in this area:</p><ul><li>A&nbsp;<a href=\"https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences\"><u>recent paper</u></a> by Tomasz Korbak et. al. found that including human preferences in pre-training instead of just fine-tuning results in text that is more often in line with human preferences and more robust to red teaming attacks.&nbsp;</li><li>Redwood Research&nbsp;<a href=\"https://www.lesswrong.com/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood\"><u>used a related technique</u></a> (adversarial training) to try and make a system that never produced injurious completions. This was not achieved, but they are not sure if it could be with improvements such as training on worse cases and more varied attack types.<br>&nbsp;</li></ul><h2>AI assistance for alignment</h2><p>Using less powerful AI to help align more powerful AI, detect dangerous actions, or support alignment research has been a general class of suggested interventions for a long while.</p><p>Anthropic&nbsp;<a href=\"https://www.lesswrong.com/posts/aLhLGns2BSun3EzXB/paper-constitutional-ai-harmlessness-from-ai-feedback\"><u>proposed a method</u></a> for training a harmless AI assistant that can supervise other AIs, using only a list of rules as human oversight. They show that this method can produce a non-evasive AI that can explain why it rejects harmful queries, and reason in a transparent way, better than standard RLHF.&nbsp;</p><p>NicholasKees and janus argue in&nbsp;<a href=\"https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism\"><u>Cyborgism</u></a> for working on tooling that augments the human alignment researcher, without giving agency to the AI. For instance, things like&nbsp;<a href=\"https://generative.ink/posts/loom-interface-to-the-multiverse/\"><u>Loom</u></a>, an interface for producing text with GPT which makes it possible to generate in a tree structure, exploring many branches at once. This is in contrast to the \u2018AI Assistant\u2019 model, where the AI is given full control over small tasks with set goals, incentivizing the progression of dangerous capabilities like situational awareness so it can do those tasks better.</p><p>Holden also discussed this general class of intervention, and what we\u2019d need to do it safely, in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very\"><u>How might we align transformative AI if it\u2019s developed very soon?</u></a></p><p>Opponents often argue that these approaches are under-defined, insufficient to control the more powerful AIs, and simply move the problem of alignment onto the assistant AIs.&nbsp;A summary of the views (for and against) of several alignment researchers on this topic was put together by Ian McKenzie&nbsp;<a href=\"https://www.lesswrong.com/posts/JKgGvJCzNoBQss2bq/beliefs-and-disagreements-about-automating-alignment\"><u>here</u></a>.</p><p>&nbsp;</p><h2>Bounded AIs</h2><p>Conjecture has recently&nbsp;<a href=\"https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal\"><u>focused their research agenda on \u2018CoEms\u2019</u></a> - AIs built to emulate only human-like logical thought processes, and that are therefore bounded in capability.<br>&nbsp;</p><h2>Theoretical Understanding</h2><p>Some researchers have been trying to come to a better theoretical understanding of the way certain types of AI (such as transformers) work, how human cognition and values work, and what this means for likely outcomes or possible ways of aligning AI.</p><p>In September 2022, janus proposed the framing of models trained with predictive loss on a self-supervised dataset (like GPT) as&nbsp;<a href=\"https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\"><u>simulators</u></a>, which can simulate agentic and non-agentic simulacra. This helped to deconfuse discussions of whether these models can display agency.</p><p>In the same month, Quintin Pope and TurnTrout proposed a theory of human value formation called&nbsp;<a href=\"https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values\"><u>shard theory</u></a>. This suggests human values are not as complicated as they seem - they\u2019re just contextually-activated heuristics shaped by genetically hard-coded reward circuits. A related claim is that RL (reinforcement learning) doesn\u2019t&nbsp;<a href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\"><u>produce policies</u></a> which have reward optimization as their target.<br><br>The&nbsp;<a href=\"https://alignmentawards.com/\"><u>AI Alignment Awards</u></a> also announced&nbsp;<a href=\"https://www.lesswrong.com/posts/JNtGxrusJRpx53Q8L/announcing-ai-alignment-awards-usd100k-research-contests\"><u>two contests</u></a> in November 2022 to advance thinking on goal misgeneralization and the shutdown problem - these remain open until May 1st 2023.</p><p>&nbsp;</p><h1>Outreach &amp; Community-Building</h1><h2>Academics and researchers</h2><p>Vael Gates and collaborators have run experiments and interviews to find out how best to outreach to existing ML and AI researchers:</p><ul><li><a href=\"https://www.lesswrong.com/posts/gpk8dARHBi7Mkmzt9/what-ai-safety-materials-do-ml-researchers-find-compelling\"><u>What AI Safety Materials Do ML Researchers Find Compelling?</u></a></li><li><a href=\"https://www.lesswrong.com/posts/Bok5RAPPjuHKvPyv2/interviews-with-97-ai-researchers-quantitative-analysis\"><u>Interviews with 97 AI Researchers: Quantitative Analysis</u></a></li></ul><p>The most preferred&nbsp; resources tended to be aimed at an ML audience, written by ML researchers, and more technical / less philosophical. The interviews themselves also resulted in lasting belief change for researchers.</p><p>Mariushobbhahn&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/kFufCHAmu7cwigH4B/lessons-learned-from-talking-to-greater-than-100-academics\"><u>found similar results</u></a> talking to &gt;100 academics about AI Safety, noticing that technical discussions and explanations got more interest than alarmism or trying to be convincing.</p><p>&nbsp;</p><h2>University groups</h2><p>University groups are moving towards more technical content, and more projects and skill-building, with less general discussion and explicit connection to EA. See three examples below:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/vEAieBkRqL7Rj8KvY/ai-safety-groups-should-imitate-career-development-clubs\"><u>AI Safety groups should imitate career development clubs</u></a> (Berkeley)</li><li><a href=\"https://forum.effectivealtruism.org/posts/CLgXstmDetfPgbPEy/update-on-harvard-ai-safety-team-and-mit-ai-alignment\"><u>Update on Harvard AI Safety Team and MIT AI Alignment</u></a> (Harvard &amp; MIT)</li><li><a href=\"https://forum.effectivealtruism.org/posts/tnzLTnBQLEDv9zygo/establishing-oxford-s-ai-safety-student-group-lessons-learnt\"><u>Establishing Oxford\u2019s AI Safety Student Group: Lessons Learnt and Our Model</u></a> (Oxford)</li></ul><p>&nbsp;</p><h2>Career Paths</h2><h3>General guidance</h3><p>Following up on Andy Jones 2021 post that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/DDDyTvuZxoKStm92M/ai-safety-needs-great-engineers\"><u>AI Safety needs great engineers</u></a>, goodgravy gives examples of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/pHKsedBYAvzFCniDF/ai-safety-needs-great-product-builders\"><u>potentially impactful routes for product builders</u></a> (eg. product managers or infrastructure engineers), and Mauricio argues that&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BJtekdKrAufyKhBGw/ai-governance-needs-technical-work\"><u>AI Governance needs more technical work</u></a> like engineering levers to make regulations enforceable or improving information security.</p><p>Kat Woods and peterbarnett&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yiTcjSWuy7ptTb5XS/what-is-it-like-doing-ai-safety-work\"><u>talked to 10 AI safety researchers</u></a> to get an idea of a \u2018day in the life\u2019, and David Scott Krueger gives their thoughts on&nbsp;<a href=\"https://www.lesswrong.com/posts/HXxHcRCxR4oHrAsEr/an-update-on-academia-vs-industry-one-year-into-my-faculty\"><u>academia vs industry</u></a>.</p><p>&nbsp;</p><h3>Should anyone work in capabilities?</h3><p>There\u2019s been a lot of discussion on if AI capabilities work can ever be justified from an x-risk lens. Arguments for (certain types of) this work being okay say that it can make a lot of money and connections, some capabilities work isn\u2019t likely to accelerate TAI or AGI timelines, and some safety work is bottle-necked on capabilities. Arguments against say that any acceleration of AGI timelines isn\u2019t worth it, we have plenty of safety work we can do using existing systems, and it\u2019s easy to lose your values in a capabilities environment.</p><p>You can read more about this argument in:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/aJNoyo8GsK3qS9wug/anonymous-advice-if-you-want-to-reduce-ai-risk-should-you\"><u>Anonymous advice: If you want to reduce AI risk, should you take roles that advance AI capabilities?</u></a> (surveys 11 experts)</li><li><a href=\"https://forum.effectivealtruism.org/posts/JFyzCv5YynN665nH8/thoughts-on-agi-organizations-and-capabilities-work\"><u>Thoughts on AGI organizations and capabilities work</u></a><br><br>&nbsp;</li></ul><h1>Arguments for and against high x-risk</h1><p>The core arguments for high x-risk from AI were written up before this year, so we\u2019ve seen few highly-rated posts in the past 7 months offering new arguments for this. But we have seen a steady set of posts arguing against it, and of posts countering&nbsp;<i>those&nbsp;</i>arguments.</p><p>We\u2019ve also seen competitions launched,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\"><u>originally by FTX</u></a> (discontinued) and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/3kaojgsu6qy2n8TdC/pre-announcing-the-2023-open-philanthropy-ai-worldviews\"><u>later pre-announced by Open Philanthropy</u></a> (edit:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/NZz3Das7jFdCBN9zH/announcing-the-open-philanthropy-ai-worldviews-contest\"><u>now launched!</u></a>), to address the high level of uncertainty on AI x-risk and timelines. Any material change here could have large funding implications.</p><p>&nbsp;</p><h2>Against high x-risk from AI</h2><p>NunoSempere&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk\"><u>offers a general skepticism</u></a> of fuzzy reasoning chains, selection effects (ie. more time has gone into x-risk arguments than counter-arguments) and community dynamics that make x-risk arguments for AI feel shakier. A lot of commenters resonate with these uncertainties. Katja_Grace&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zoWypGfXLmYsDFivk/counterarguments-to-the-basic-ai-risk-case\"><u>also suggests</u></a> that getting \u2018close enough\u2019 might be good enough, or that AI will reach caps where certain tasks don\u2019t scale any further with intelligence, and we might retain more power due to our collaboration. They also suggest even if AIs are more powerful than us,&nbsp;<a href=\"https://www.lesswrong.com/posts/wB7hdo4LDdhZ7kwJw/we-don-t-trade-with-ants\"><u>we could have worthwhile value to offer</u></a> and collaborate with them. Building on the idea of capped returns to AI processes,&nbsp;<a href=\"https://www.lesswrong.com/posts/zB3ukZJqt3pQDw9jz/ai-will-change-the-world-but-won-t-take-it-over-by-playing-3\"><u>boazbarak and benedelman suggest</u></a> that there are diminishing returns to information processing with longer time horizons, and this will result in certain areas (such as strategic decision-making) that AIs struggle to compete with humans in.</p><p>Another set of arguments says even if misalignment would be the default, and even if it would be really bad, we might be closer to solving it than it seems. Kat Woods and Amber Dawn&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/RkpdA8763yGtEovj9/two-reasons-we-might-be-closer-to-solving-alignment-than-it\"><u>argue that&nbsp;</u></a>we\u2019ve only had double-digit numbers of people on the problem for a short while, and the field is unpredictable and could have a solution just around the corner.&nbsp;<a href=\"https://www.lesswrong.com/posts/BfN88BfZQ4XGeZkda/concrete-reasons-for-hope-about-ai\"><u>Zac Hatfield-Dodds notes</u></a> interpretability research is promising, outcomes-based training can be avoided, and labs and society will have time during training or takeoff to pause if need be.</p><p>&nbsp;</p><h2>Counters to the above arguments</h2><p>In response to the theme of certain limitations in AIs capabilities, guzey notes in parody that&nbsp;<a href=\"https://www.lesswrong.com/posts/73kwTFKgi4AagxFHJ/planes-are-still-decades-away-from-displacing-most-bird-jobs\"><u>planes still haven\u2019t displaced bird jobs</u></a>, but that doesn\u2019t mean they haven\u2019t changed the world.</p><p>S08res argues that&nbsp;<a href=\"https://www.lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural\"><u>assumptions of niceness</u></a>, or values \u2018close enough\u2019 to humans, won\u2019t play out because human niceness is idiosyncratic and was a result of selection pressures that don\u2019t apply to AIs. They also argue that&nbsp;<a href=\"https://www.lesswrong.com/posts/idipkijjz5PoxAwju/warning-shots-probably-wouldn-t-change-the-picture-much\"><u>we can\u2019t rely on key players to shut things down or pause</u></a> when we get warning shots, noting the lack of success in shutting down gain-of-function research despite global attention on pandemics.&nbsp;<br><br>Erik Jenner and Johannes_Treutlein&nbsp;<a href=\"https://www.lesswrong.com/posts/GQat3Nrd9CStHyGaq/response-to-katja-grace-s-ai-x-risk-counterarguments\"><u>argue similar points</u></a>, noting that AI values could vary a lot in comparison to humans (including deception), and that a strong AI only needs to be directed at a single dangerous task that scales with intelligence in order to take over the world.</p><p>&nbsp;</p><h1>Appendix - All Post Summaries</h1><p>See a list of all AI-related weekly forum summaries from August 2022 to February 2023 inclusive&nbsp;<a href=\"https://docs.google.com/document/d/1NVbUakVRZ41l8aU2xib-xkOWJyFahwNmzkUVfmvCg9Q/edit?usp=sharing\"><u>here</u></a>.</p><p><br><br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/hCwDNq6sZofgSEN3s/s62kugmdzp1qhsj1fxtd\"></p><p><i>This blog post is an output of&nbsp;</i><a href=\"https://rethinkpriorities.org/\"><i><u>Rethink Priorities</u></i></a><i>\u2013a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. The author is Zoe Williams. Thanks to Peter Wildeford for their guidance, and Akash Wasil and Erich Grunewald for their helpful feedback.</i><br><br><i>If you are interested in RP\u2019s work, please visit our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/research\"><i><u>research database</u></i></a><i> and subscribe to our&nbsp;</i><a href=\"https://www.rethinkpriorities.org/newsletter\"><i><u>newsletter</u></i></a><i>.</i><br>&nbsp;</p>", "user": {"username": "GreyArea"}}, {"_id": "xtcgsLA2G8bn8vj99", "title": "Reminding myself just how awful pain can get (plus, an experiment on myself)", "postedAt": "2023-03-15T22:44:39.310Z", "htmlBody": "<p><strong>Content warning: This post contains references to extreme pain and self-harm, as well as passing references to suicide, needles, and specific forms of suffering (but not detailed descriptions). Please do not repeat any of the experiments I've detailed in this post.&nbsp;</strong><a href=\"https://www.mentalhealthnavigator.co.uk/ea-mental-health-programme\"><strong><u>Please be kind to yourself, and remember that the best motivation is sustainable motivation</u></strong></a><strong>.</strong></p><h1><strong>Summary</strong></h1><ul><li>Out of curiosity, I exposed myself to safe, moderate-level pain to see how it changed my views on three particular topics. This article is mostly a self-reflection on this (non-scientific) experience.</li><li>Firstly, I got a visceral, intense sense of how urgent it is to&nbsp;<i>get it right</i> when working to do the most good for others.</li><li>Secondly, I gained a strong support for the position that the most morally important goal is to prevent suffering, and in particular for preventing extreme suffering.</li><li>Thirdly, I updated my opinion on the trade-offs between different intensities of pain, which I give in this article as rough, numerical weightings on different categories of pain. Basically, I now place a greater urgency on preventing intense suffering than I did before.</li><li>I conclude with how this newfound urgency will affect my work and my life.</li></ul><h1><strong>My three goals</strong></h1><p>I began this experiment with three main goals:</p><ul><li>To remind myself how urgent and important it is to, when working to help others as much as I can, to&nbsp;<i>get it right</i>.</li><li>Some people think that preventing intense pain (rather than working towards other, non-pain-related goals) is the most important thing to do. Do I agree with this?</li><li>If I experience pain at different intensities, does this change the moral weight that I place on preventing intense pain compared to modest pain (i.e. intensity-duration tradeoff)?</li></ul><p>I think it is useful to test my intellectual ideas against what it is actually like to experience pain. This is&nbsp;<i>not</i> for motivation - I already work plenty in my role in animal advocacy, and I believe that sustainable motivation is the best motivation (I talk about this more at the end).</p><h1><strong>My \"experiment\"</strong></h1><p>I subjected myself to two somewhat-safe methods of experiencing pain:</p><ul><li>Firstly, I got three tattoos on different parts of my body - my upper arm, my calf, and my inner wrist. I had six tattoos already, so I was familiar with this experience. I got these tattoos all on one day (4/2/23) and in one location (a studio in London).</li><li>Secondly, I undertook the cold pressor test. This is basically holding my hand in a tub of near-freezing water. This test is commonly used in scientific research as a way to invoke pain safely. I also did this on one day (25/2/23) and in one location (my home in Australia). Please&nbsp;<strong>do not replicate this</strong> - the cold pressor test causes pain and can cause&nbsp;<a href=\"https://academic.oup.com/jpepsy/article/36/10/1071/886282\"><u>significant distress</u></a> in some people, as well as&nbsp;<a href=\"https://link.springer.com/article/10.1007/s10286-021-00796-4\"><u>physical reactions that can compromise your health</u></a>.</li></ul><p>I wish I had a somewhat-safe way to experience pain that is more intense than these two experiences, but these are the best I could come up with for now.</p><p>During both of these experiences, I recorded the pain levels. I recorded the pain in three ways:</p><ul><li>A short, written description of my thoughts and feelings.</li><li>The McGill Pain Index Pain Rating Intensity (PRI) Score. This score is calculated from a questionnaire (which I accessed via a phone app) that asks you to choose words corresponding to how your pain feels. The words are then used to calculate the numeric PRI score. I chose to use this tool as there is a review paper listing the approximate PRI scores caused by different human health conditions, which lets me roughly compare my scores to different instances of human pain. This list is given below, so you can have some idea of what scores mean.&nbsp;</li><li>The PainTrack category. This divides pain into four categories - annoying, hurtful, disabling, and excruciating - based on qualitative descriptions (given below). I chose to use this tool as PainTrack has been used to measure the pain experienced by farmed animals, which lets me roughly compare my scores to different instances of animal pain. I also use PainTrack in my day-to-day work in animal advocacy research. The definitions of each category are given below.</li></ul><h1><strong>The McGill Pain Index</strong></h1><p>Here is a visual summary of how the McGill Pain Index PRI scores are calculated, as well as some typical scores associated with human health conditions. I do assume, but I haven't checked, that the scores would vary somewhat from person-to-person and depending on the specifics of the condition. This is illustrated with childbirth, which is given three scores (the pain varies depending on the experience and preparation level of the birthing parent).</p><p>I gather that the McGill PRI scores range between 0 and 78 (that's how it worked on the app that I used). Most images in the literature and on the internet only go up to 50. I'm assuming they're the same scale, but please point out if I've misinterpreted something and I'll fix it.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xtcgsLA2G8bn8vj99/mpnfysff7av1cz3tplt4\"><br>(<i>Credit: Scale on the right adapted from&nbsp;</i><a href=\"https://www.researchgate.net/figure/Comparison-of-pain-scores-using-the-McGill-Pain-Questionnaire-obtained-from-women-during_fig2_47756188\"><i><u>Wong (2022)</u></i></a><i>, in turn reproduced from Melzack (1984). Sample words from&nbsp;</i><a href=\"https://en.wikipedia.org/wiki/McGill_Pain_Questionnaire\"><i><u>Wikipedia</u></i></a><i>.</i>)</p><h1><strong>The Pain-Track Framework</strong></h1><p>For Pain-Track,&nbsp;<a href=\"https://link.springer.com/article/10.1186/s13104-021-05636-2\"><u>the article by Alonso and Schuck-Paim</u></a> gives the definitions of each category in the Pain-Track framework:</p><ul><li><strong>Excruciating</strong>: Threshold of pain under which many people would choose to take their life rather than standing the pain. This is the case, for example, of severe burning events, which may make victims jump from buildings, or other conditions associated with suicidal attempts by sufferers (e.g., cluster headaches). Many forms of torture have been designed to inflict pain at this level. Behavioral patterns can include loud screaming, involuntary shaking and extreme restlessness</li><li><strong>Disabling</strong>: Most forms of functioning or enjoyment are prevented as the direct result of pain. Symptoms are continuously distressing. Individuals affected often substantially reduce activity levels and refrain from moving. Pain at this level can disrupt or prevent sleeping. Only strong analgesia can relieve it</li><li><strong>Hurtful</strong>: Pain experiences that most would consider disruptive of daily routine. Although not entirely preventing individuals from functioning, their ability to do so is impaired as the direct result of pain, and often accompanied by the desire to take painkillers or seek treatment. Frequent complaints are often present. The possibility to enjoy pleasant experiences is impaired, as is performance on mentally demanding tasks, alertness and attention to ongoing stimuli</li><li><strong>Annoying</strong>: Pain experiences are not intense enough to disrupt the routine or daily activities of individuals, their possibility to enjoy pleasant (positive) experiences, or their ability to conduct mentally demanding tasks that require attention. Sufferers do not think about this sensation most of the time, and when they do they can adapt to it</li></ul><h1><strong>Raw results</strong></h1><ul><li>The following table shows the pain levels that I recorded at different stages in the experiment.</li><li>I've also put the rough McGill PRI scores on a chart showing how different human health conditions compare.</li><li><strong>I've adjusted my scores downwards a little bit for the below chart.&nbsp;</strong>It should be obvious that this approach, from an experimental perspective, suffers from numerous limitations. I write more about that below. Most notably, some of my McGill PRI scores seem a little bit too high (e.g. comparable to digit amputation). While these scores are plausible, I find it more likely that my scores are indeed slightly too high. An experiment conducted by researchers actually trained in this tool, using a larger sample size, would probably obtain slightly lower average scores. So, you might want to adjust the numbers downwards in your head a little bit.</li><li>Also note that I subsequently tidied up the subjective descriptions, as my capacity for spelling and grammar during each experience was severely compromised.<br>&nbsp;</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Experience</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>McGill PRI</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>PainTrack Category</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Subjective description of what this was like</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Tattoo - inner wrist - after 5 minutes</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>52</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Disabling (upper end)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Argghhhhhhhhhhhh! I had to stop twice and almost called off the tattoo. The nausea was extremely challenging and almost as unpleasant as the pain.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Tattoo - calf - after 5 minutes (lines)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>36</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Disabling</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Very intense electric shock. Not unbearable, but definitely wouldn't be able to conduct any tasks.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Tattoo - calf - after 45 minutes (colouring)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>49</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Disabling</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>**** me, make it stop. Like someone slicing into my leg with a hot, sharp live wire.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Tattoo - calf - after 75 minutes (colouring)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>-</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>-</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Rapidly alternating between the previous two ratings, depending on where the needle is. The lower ranking ['after 5 minutes'] is a real relief. The latter ranking, ['after 45 minutes'] if I were experiencing it not by choice and with no end in sight, would cause me to literally end my life within days or sooner.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Tattoo - shoulder - after 5 minutes (thin lines, thin needle)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>26</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Disabling (lower end)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Like the leg electric shock feeling, but softer. Can manage.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Tattoo - shoulder - after 30 minutes (thick lines, thick needle)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>12</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p><s>Disabling </s>Hurtful</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Aching buzzing, like a mild headache in my shoulder. Could do a day of work, albeit at a lower capacity.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Tattoo - shoulder - after 75 minutes (colouring)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>19</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Hurtful or Disabling (about midway between them)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Sore and painful, not quite as sharp as the leg</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>Cold pressor test, 1 min</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>34</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Disabling (lower end)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Stings a lot. Very sore and tingly, quite unpleasant.</p></td></tr></tbody></table></figure><p>&nbsp;</p><p><strong><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xtcgsLA2G8bn8vj99/uoirezjubtzit8cg7mlv\"></strong></p><p>(<i>Credit: Scale and scores on the left of the column are adapted from&nbsp;</i><a href=\"https://www.researchgate.net/figure/Comparison-of-pain-scores-using-the-McGill-Pain-Questionnaire-obtained-from-women-during_fig2_47756188\"><i><u>Wong (2022)</u></i></a><i>, in turn reproduced from Melzack (1984).</i>)</p><h1><strong>Question 1: The importance of getting it right</strong></h1><p>The goal</p><ul><li>I was curious whether experiencing this level of pain would help bring home the urgency of&nbsp;<i>getting it right</i> when working to do the most good for others.</li></ul><p>Outcome</p><ul><li>Yes, while I was experiencing the pain, this urgency became&nbsp;<i>crystal clear</i> to me.</li><li>I also gained a much deeper appreciation for the need to be&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XrGkM3KFk7TTgRLNr/ea-reading-list-cluelessness-and-epistemic-modesty\"><u>modest about what I believe</u></a>. If there is suffering&nbsp;<i>this intense</i> happening on a colossal scale in the world, I need to get over my silly hang-ups and&nbsp;<i>work really hard to make sure I believe the correct things</i>, no matter how emotionally uncomfortable and intellectually challenging that process can be.</li><li>It's hard to explain more than this - after all, the purpose was to get a&nbsp;<i>personal, visceral</i> feel for the urgency of getting it right. By definition, this is hard to explain in the form of an elegant argument.</li><li>I've experienced this general level of pain before, which suggests that it might be easy to forget once I stop feeling the pain. But it's not exactly a good idea to subject myself to this level of pain on a regular basis. For more on how I plan to resolve this dilemma, see the Afterword section of this article.</li></ul><h1><strong>Question 2: Is preventing suffering the most important thing?</strong></h1><p>The question</p><ul><li>According to&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/suffering-focused-ethics\"><u>suffering-focused ethics</u></a>, the most important thing is to prevent suffering. For example, Brian Tomasik&nbsp;<a href=\"https://reducing-suffering.org/the-horror-of-suffering/\"><u>quotes</u></a> one author, who writes something along the lines of: it's easy to worry about principles like exploitation, oppression, liberation, and so on, but once you truly experience intense suffering, all of those concerns seem ridiculous.</li><li>I'm not a philosopher, but I understand that other philosophical systems can lead to the conclusion \"the most important thing is to prevent suffering\". I'm most interested in this claim, not defending any particular philosophical system as a whole.</li><li>Is this claim true?</li></ul><p>Background - opinion before the experiment</p><ul><li>I disagree with the claim that most principles, abstractions, etc are ridiculous compared to preventing suffering.</li><li>I definitely care about a lot of abstractions: democracy, freedom, equality, gender and queer rights, Indigenous rights, animal rights, women feeling safe in public, kindness, generosity, digital privacy, scientific advances, my gender expression, my hobbies, my social reputation, my pension fund, and so on. This claim suggests that caring about any of these things - and some of the items at the beginning of that list are things that people have&nbsp;<i>given their lives</i> for - is&nbsp;<i>ridiculous</i>. That is a very strong claim, and I find it difficult to agree.</li></ul><p>Outcome - opinion after the experiment</p><ul><li>I changed my mind. I now basically agree with that claim.</li><li>Obviously many of the social conditions (democracy, freedom, rights) and personal things (hobbies, finances) from my list can be instrumental in helping to prevent suffering. In particular, if numerous people throughout history have given their lives, exposed themselves to great hardship, and even placed their friends and families at risk in the fight for rights, freedom, equality, and so on, then it's probably safe to conclude that those things have some instrumental value. (For this reason, it might be worthwhile for the EA community to investigate the value of these humanitarian principles from an instrumental perspective.)</li><li>But apart from that instrumental value, it now seems completely crazy to worry about anything other than preventing extreme suffering as much as possible.</li><li>This definitely doesn't mean I intend to work for every minute of every day. I still fully intend to rest, spend time with friends and family, walk the dog, play the guitar, attend therapy, participate in my religious community, and so on. A human body needs to rest well to work well - but apart from the need to rest well, this does make many of the things I worry about on a day-to-day basis seem pretty silly. Likewise, this is not an argument in favour of unrestricted consequentialism - it should go without saying that I still intend to obey laws and ethical norms.</li></ul><p>Some further thoughts</p><ul><li>I experienced \"disabling\"-level pain for a couple of hours, by choice and with the freedom to stop whenever I want. This was a horrible experience that made everything else seem to not matter at all.</li><li>Let's consider, for a moment, just farmed chickens - a group of animals that I consider daily in my research.</li><li>A single laying hen experiences&nbsp;<a href=\"https://welfarefootprint.org/research-projects/laying-hens/\"><i><u>hundreds of hours</u></i></a> of this level of pain during their lifespan, which lasts perhaps a year and a half - and there are as many laying hens alive at any one time as there are humans. How would I feel if every single human were experiencing hundreds of hours of disabling pain?&nbsp;</li><li>A single broiler chicken experiences&nbsp;<a href=\"https://www.pain-track.org/broilers\"><i><u>fifty hours</u></i></a> of this level of pain during their lifespan, which lasts 4-6 weeks. There are&nbsp;<a href=\"https://ourworldindata.org/meat-production#number-of-animals-slaughtered\"><u>69 billion broilers</u></a> slaughtered each year. That is so many hours of pain that if you divided those hours among humanity, each human would experience about 400 hours (2.5 weeks) of disabling pain every year. Can you imagine if instead of getting, say, your regular fortnight vacation from work or study, you experienced&nbsp;<i>disabling-level pain</i> for a whole 2.5 weeks? And if every human on the planet - me, you, my friends and family and colleagues and the people living in every single country - had that same experience every year? How hard would I work in order to avert suffering that urgent?</li><li>Every single one of those chickens are experiencing pain as awful and all-consuming as I did for&nbsp;<i>tens</i> or&nbsp;<i>hundreds</i> of hours, without choice or the freedom to stop. They are also experiencing often minutes of 'excruciating'-level pain, which is an intensity that I literally cannot imagine. Billions upon billions of animals. The numbers would be even more immense if you consider farmed fish, or farmed shrimp, or farmed insects, or wild animals.</li><li>If there were a political regime or law responsible for this level of pain -&nbsp;<i>which indeed there is</i> - how hard would I work to overturn it? Surely that would tower well above my other priorities (equality, democracy, freedom, self-expression, and so on), which seem trivial and even borderline ridiculous in comparison.</li><li>There exist detailed descriptions of particularly awful instances of suffering where people may experience pain at/beyond the \"excruciating\" level for long periods (warning for extreme/traumatising content if you follow these links - but I'm talking about Brian Tomasik's&nbsp;<a href=\"https://reducing-suffering.org/on-the-seriousness-of-suffering/\"><u>article</u></a> and&nbsp;<a href=\"https://www.youtube.com/watch?v=RyA_eF7W02s\"><u>video</u></a> and Simon Knutsson's&nbsp;<a href=\"https://www.simonknutsson.com/the-seriousness-of-suffering-supplement\"><u>article</u></a>). I won't repeat these descriptions here, and I haven't even looked at these myself, as it would be damaging to my mental health. But this might be a useful piece of information if you're mostly concerned about human suffering (I've discussed mostly animal suffering as that is most relevant to my own work).</li></ul><h1><strong>Question 3: How does intense pain compare morally to mild pain?</strong></h1><p>The question</p><ul><li>I work as a researcher in the animal advocacy movement. In my job, I need to make trade-offs between different intensities of suffering to advise which campaigns the movement spends its money and resources on. (Note: my weightings are not the same as my organisation's weightings. While this helps advise my position when discussing this at work, the weightings that my organisation arrives at also consider many other pieces of evidence and many other people's views.)</li><li>To give just one example, I recently had to recommend the most effective fish advocacy campaign for several countries in Europe - do we want to improve welfare throughout a fish's life even if many of those improvements might be mild, or would we prefer to implement stunning to minimise extreme suffering at slaughter?</li><li>Concretely, you can express these trade-offs as quantitative weightings associated with the PainTrack categories. Somebody might think that preventing 1 hour spent in 'hurtful' pain is morally equivalent to preventing 15 hours spent in 'annoying' pain, in which case the weighting on 'hurtful' pain could be expressed as the number 15.</li></ul><p>Background - opinion before the experiment</p><ul><li>I wrote down my rough weightings before the experiment:</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt\">&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt\" colspan=\"4\"><p>Weightings (units of time that would equate with 1 unit time of annoying pain)</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\">&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Annoying</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Hurtful</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Disabling</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Excruciating</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>3 Feb 2023 (no particular special time)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>1</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>15</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>560</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>200,000</p></td></tr></tbody></table></figure><p>Outcome - opinion after the experiment</p><ul><li>I wrote down my rough weightings after the experiment (and at a couple of intermediate stages).</li><li>After the tattoo, my weightings changed to put a much higher weighting on the intense pain categories. In comparison, the cold pressor tests didn't change my weightings that much from their original position.</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt\">&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt\" colspan=\"4\"><p>Weightings (units of time that would equate with 1 unit time of annoying pain)</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\">&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Annoying</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Hurtful</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Disabling</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>Excruciating</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>4 Feb 2023 (Evening after getting tattoos)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>1</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>15</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>1,000</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>??? Maybe infinite ???</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt\"><p>25 Feb 2023 (After doing cold pressor tests)</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>1</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>10</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>490</p></td><td style=\"border:1pt solid #000000;padding:5pt\"><p>???</p></td></tr></tbody></table></figure><h1><strong>Miscellaneous thoughts and takeaways</strong></h1><ul><li>Right now, putting the final touches on this article a month later, I can hardly remember the cold pressor test, but the thought of getting the tattoos still makes me shiver. This might indicate that the cold pressor test actually wasn't a very bad experience, which could be one reason why it didn't really change my weightings that much from the original ones. (See my above point about forgetting how bad the pain actually is.)</li><li>How would I define&nbsp;<i>extreme</i> or&nbsp;<i>intense</i> pain - the pain that I (now) consider to be the most morally urgent? Placing weightings on the Pain-Track categories does avoid the need for making a qualitative distinction between morally urgent and morally-not-urgent pain. However, if I were pressed, I would say that the morally urgent pain is anything in the category of \"disabling\" or worse. This is entirely intuitive and subjective, and my reason is explained in the next dot point. Notably, my cutoff does feel roughly similar to that tentatively suggested for the \"Hell Index\" in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8Sed33q54kdhZ4M9m/get-out-of-hell-free-necklace\"><u>this article</u></a>: 20 on the McGill scale.</li><li>Of the four PainTrack categories (annoying, hurtful, disabling, and excruciating), I personally have developed the opinion that there might be a qualitative distinction separating hurtful pain and disabling pain. Annoying and hurtful pain are both bad, but they are manageable, and it's possible that you could live a good life even while regularly experiencing these levels of pain. Disabling pain, to me, is hell on earth. (There may also be a further qualitative distinction between disabling pain and excruciating pain. But by definition, excruciating pain is so painful that I have no way to voluntarily give it a go.)</li><li>I'm privileged enough that they have rarely or never experienced extreme pain. There's nothing inherently wrong with this - after all, it is wonderful to be able to use one's privilege to benefit the lives of others. But this does serve as a reminder that we probably know little about what life is actually like for the beings we are trying to help. Asking the people we are trying to help for their subjective experiences, wherever possible, seems like a really important part of actually helping them. Basically, this experience has given me a strong sense of humility about what we think we know.</li><li>If preventing suffering is indeed the most important thing, this does give us a very useful strategy for animal advocacy. We may not necessarily need to mount a moral revolution, political revolution, or social transition, despite these having been a large part of the animal advocacy movement's strategy to date. What we need (if this is correct) is to prevent suffering. This is very actionable, especially compared to those more challenging strategies - it's easier to install humane slaughter equipment or to ban fast-growing chicken breeds than to overthrow a state or transform a food system.</li><li>This article is mostly a personal reflection, which&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Q7wQvrDJG6mwudzak/the-unthinkable-urgency-of-suffering\"><u>others have done</u></a> on this topic in different ways. It's interesting to note that I arrived at some conclusions that very closely mirror&nbsp;<a href=\"https://reducing-suffering.org/the-horror-of-suffering/\"><u>Brian Tomasik's articles</u></a>, despite not having read those articles closely until after I wrote this one.</li></ul><h1><strong>Limitations and future directions</strong></h1><ul><li>Obviously, this was not a scientific experiment. It should go without saying that I do not claim that the measures I took are scientifically accurate relative to the scales being used. I wasn't trained in administering or understanding the McGill PRI questionnaire beforehand. Obviously, my sample size is n = 1, and I'm influenced by my own experiences, genetics, history, beliefs, and so on.</li><li>There is no way for me to voluntarily experience&nbsp;<i>really</i> intense pain - by definition, if somebody is experiencing pain in the \"excruciating\" Pain-Track category, they would often rather end their life than continue experiencing that pain. There is no way for me to plausibly replicate that experience.</li><li>I've only considered pain, not other experiences that aren't exactly pain but can still cause intense suffering and even death. I have no idea how intense pain relates to these other experiences (e.g. sexual abuse, nausea, hunger/thirst, grief, bullying, humiliation, shame, fear, trauma, loneliness, and so on). How does extreme physical pain relate to extreme psychological pain? I have no insight or data to go on - though I also haven't looked into this, and I think this would be a valuable area of future research.</li><li>I went through these experiences voluntarily and with the knowledge that I have the freedom to stop whenever I want. People suffering from painful disease, children dying of hunger, chickens being electrocuted to death, fish being asphyxiated to death - for these individuals, such experiences are a horrific reality, not an experiment. I can only imagine that these experiences would feel senseless, meaningless, and terrifying.</li><li>It would be interesting to experience the concrete practices that animals do, like different forms of transport or mutilations. Obviously, there is a very strong limit on how closely I could replicate most of those practices when experimenting on myself.</li><li>I did consider using a TENS machine as another safe way to experience pain. People have used these machines before to simulate the pain of periods or childbirth. I basically didn't want to spend the couple of hundred dollars purely for this article, and I'm less confident in my ability to use a TENS machine safely.</li><li>I would also be interested in experiencing the sting of a bullet ant or a tarantula hawk wasp, if that could be achieved without causing harm to the animal stinging me. These are the highest-scoring insects on the Schmidt&nbsp;<a href=\"https://en.wikipedia.org/wiki/Schmidt_sting_pain_index\"><u>sting pain index</u></a>.&nbsp;<a href=\"https://www.goodreads.com/book/show/27293395-the-sting-of-the-wild\"><u>In Schmidt's book, he writes</u></a>: \"Stung by a tarantula hawk? The advice I give [...] is to lie down and scream. [...] The pain is instantaneous, electrifying, excruciating, and totally debilitating.\" As for the bullet ant, which Schmidt claims has the Holy Grail of stings: \"... absolutely excruciatingly painful and debilitating. [...] my hand was throbbing, sending crescendos of pain, followed by easing a bit, only to be repeated with renewed ferocity. All the while the forearm was uncontrollably vibrating up and down.\" You can watch a bullet ant sting in action in&nbsp;<a href=\"https://hamishandandy.com/videos/worst-pain-known-man-2014/\"><u>this video by Australian comedians Hamish and Andy</u></a>, which shows one of the pair have a go at the Sater\u00e9-Maw\u00e9 people's coming-of-age ritual.</li><li>I did speak to somebody who - voluntarily, and as a casual experiment - subjected themselves to water torture. Without getting into specifics, that conversation led me to have greater confidence in the broad conclusions I've drawn in this article.</li></ul><h1><strong>Afterword: Sitting at the edge of the lake</strong></h1><ul><li>My conclusions from this experience presents me with a strange dilemma.</li><li>Suffering is so urgent that it demands our immediate, unrelenting attention - but I need to maintain that attention for my whole career. There is an emergency, but it's long and drawn-out. The building is burning, but it will continue to burn for a long time. This is a weird position to be in.</li><li>I need to remember the insights and opinions that I've developed throughout this experience, but it would not be sustainable to agonise over them on a day-to-day basis. I need to trust in what I've written here, and perhaps revisit it occasionally in case I need to make a course correction. But I cannot spend every day agonising over the severity and urgency of suffering.</li><li>One of my contacts told me the metaphor of the lake of suffering - we need to sit at the lake and help others who are drowning, without falling in and drowning ourselves.</li><li>It's absolutely critical&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vtzrZuigkT3ovn4rq/how-will-we-know-if-we-are-doing-good-better-the-case-for\"><u>to make sure my theory of change is sound and robust</u></a>, and to focus all of my (work) energy on working to implement that theory of change. It would be good to keep in mind the insights I've described in this article while I develop and test my theory of change. But once it comes to the day-to-day work of actually implementing that theory of change, I can&nbsp;<a href=\"https://www.lesswrong.com/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality\"><u>think about my goals slightly differently</u></a>. Rather than agonising over extreme suffering from Monday to Friday, I can just enjoy doing the specific tasks, like research and writing, that are dictated by my theory of change.&nbsp;</li></ul><h1><strong>Notes and acknowledgements</strong></h1><p>As a disclaimer, I chose to do this experiment myself, and nobody encouraged me. My views do not represent the views of my employer.</p><p>I would like to thank my friends and professional contacts who helped guide my thinking and writing on this topic.</p>", "user": {"username": "Ren Springlea"}}, {"_id": "HzLkBFRHtkCeZSuxK", "title": "EA Organization Updates & Opportunities: March 2023", "postedAt": "2023-03-15T23:54:34.944Z", "htmlBody": "<p>These monthly posts originated as the \"Updates\" section of the&nbsp;<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives\"><u>EA Newsletter</u></a>. Organizations submit their own updates, which we edit for clarity.&nbsp;</p><p>This month, we\u2019re testing out whether it\u2019s more useful to feature opportunities more heavily.&nbsp;<strong>Opportunities and job listings&nbsp;</strong>that these organizations highlighted (as well as a couple of other impactful jobs and announcements) are at the top of this post.&nbsp;<strong>Some have pressing deadlines.&nbsp;</strong></p><p>You can see previous updates on the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ea-organization-updates-monthly-series\"><u>\"EA Organization Updates (monthly series)\"</u></a> topic page, or in our<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives/\"><u> repository of past newsletters</u></a>. Notice that there\u2019s also an&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/org-update\"><u>\u201corg update\u201d tag</u></a>, where you can find more news and updates that are not part of this consolidated series.</p><p>The organization updates are in alphabetical order.</p><h2>Opportunities and jobs</h2><h3>Opportunities</h3><p>Consider also checking opportunities listed on the&nbsp;<a href=\"https://ea-internships.pory.app/board\"><u>EA Opportunities Board</u></a>.&nbsp;</p><ul><li><strong>General-audience events</strong><ul><li>Applications are open for&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9yLa5hcJrRFpAv5sM/apply-to-attend-ea-conferences-in-europe\"><u>three EA conferences in Europe</u></a>:<ul><li><a href=\"https://www.effectivealtruism.org/ea-global/events/eagxnordics-2023\"><u>EAGxNordics</u></a> will be in Stockholm from 21\u201323 April.&nbsp;<a href=\"https://effectivealtruism.force.com/EAGlobal/s/eagxnordicsapplication\"><u>Apply</u></a> by&nbsp;<strong>28 March</strong>.</li><li><a href=\"https://www.effectivealtruism.org/ea-global/events/ea-global-london-2023\"><u>EA Global: London</u></a> will run from 19\u201321 May.&nbsp;<a href=\"https://effectivealtruism.force.com/EAGlobal/s/eaglobalapplication?eId=a4WAJ000000000k2AA\"><u>Applications are open</u></a>. If you were already accepted to EA Global: Bay Area, you can register for EAG London now; you don\u2019t need to apply again.</li><li><a href=\"https://www.effectivealtruism.org/ea-global/events/eagxwarsaw\"><u>EAGxWarsaw</u></a> will run from 9\u201311 June.&nbsp;<a href=\"https://effectivealtruism.force.com/EAGlobal/s/eagxwarsawapplication\"><u>Apply</u></a> by&nbsp;<strong>19 May</strong>.</li></ul></li><li>You can enroll in a free&nbsp;<a href=\"https://forecasting.notion.site/Forecasting-Predict-the-Odds-of-Future-Events-acba326fa6c345c993ffca74e1f3933c\"><u>pilot Forecasting course</u></a>, which will run from March 24 to April 15, 2023 (5-10 hours per week). Juan Cambeiro, a Good Judgement Superforecaster and analyst for Metaculus, will lead the course, which is aimed at beginner forecasters who want to improve their decision-making skills.&nbsp;<a href=\"https://forms.gle/9gpYDJjSe2sfNLmZ6\"><u>Enroll</u></a> by&nbsp;<strong>22 March</strong>.</li><li>Peter Singer's speaking tour is set to visit London, Perth, and a handful of major US cities,&nbsp;<a href=\"https://twitter.com/thinkincAU\"><u>with dates to be announced soon</u></a>. The US and UK tour dates will form part of the launch of <i>Animal Liberation NOW - The Definitive Classic Renewed</i> on May 23rd.&nbsp;<a href=\"https://thinkinc.org.au/pages/an-evening-with-peter-singer\"><u>Tickets for the Perth event</u></a> will be available for sale on March 16 local time and include a live-streaming option.</li></ul></li><li><strong>Early-career, student, and academic opportunities</strong><ul><li>Applications for the Center on Long-Term Risk\u2019s annual&nbsp;<a href=\"https://longtermrisk.org/summer-research-fellowship/\"><u>Summer Research Fellowship</u></a> are open. Apply by Sunday,&nbsp;<strong>April 2</strong>.</li><li>The Global Priorities Institute (GPI) is accepting applications to attend the&nbsp;<a href=\"https://globalprioritiesinstitute.org/12th-workshop-oxford-workshop-on-global-priorities-research-19-20-june-2023/\"><u>12th Oxford Workshop on Global Priorities Research</u></a>. The workshop will run June 19-20 and will feature presentations by GPI staff and external researchers, and about 150 attendees. The workshop will be a forum for GPI researchers and academics who are interested in global priorities research (mainly in philosophy and economics) to present and discuss ideas at various stages of progress. More information and the link to apply can be found&nbsp;<a href=\"https://globalprioritiesinstitute.org/12th-workshop-oxford-workshop-on-global-priorities-research-19-20-june-2023/\"><u>here</u></a>. Applications close on&nbsp;<strong>19 March</strong> (23:59 UK time).</li><li>IDinsight are accepting applications for the&nbsp;<a href=\"https://internationalcareers-idinsight.icims.com/jobs/1303/2023-graduate-student-internship---client-facing/job?mode=view&amp;mobile=false&amp;width=1084&amp;height=500&amp;bga=true&amp;needsRedirect=false&amp;jan1offset=180&amp;jun1offset=180\"><u>2023 Graduate Student Internship</u></a> and&nbsp;<a href=\"https://internationalcareers-idinsight.icims.com/jobs/1302/stage-de-fin-d%e2%80%99etude---projet-de-fin-d%e2%80%99etude-%284-%c3%a0-6-mois%29/job?mode=view\"><u>Stage De Fin D\u2019etude / Projet De Fin D\u2019etude (Internship)</u></a> in Rabat, Morocco and Dakar, Senegal. The deadline to apply for university funding has passed, but all other candidates can still apply by&nbsp;<strong>31 March</strong>.</li><li>A&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Gcnkp4qZJDownkLTj/two-university-group-organizer-opportunities-pre-eag-london#Pre_EAG_London_University_Group_Organizer_Summit\"><u>Pre-EAG London University Group Organizer Summit</u></a> will run May 15-19 to share knowledge and support university group organizers.&nbsp;<a href=\"https://www.notion.so/centreforeffectivealtruism/University-Group-Organiser-Summit-d6e1256922614dae8d7678e6eff78a92\"><u>Apply</u></a> by&nbsp;<strong>27 March</strong>.</li><li>Applications are open for a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Gcnkp4qZJDownkLTj/two-university-group-organizer-opportunities-pre-eag-london#Summer_Internship\"><u>summer internship on the Groups Team</u></a> at the Centre for Effective Altruism.&nbsp;<a href=\"https://www.notion.so/centreforeffectivealtruism/CEA-University-Groups-Team-Summer-23-Internship-b70a19daeea24da6acb8822742f7401b\"><u>Apply</u></a> by&nbsp;<strong>22 March</strong>.</li></ul></li></ul><h3>Job listings</h3><p>\u200b\u200bConsider also exploring jobs listed on \u201c<a href=\"https://forum.effectivealtruism.org/topics/job-listing-open\"><u>Job listing (open)</u></a>.\u201d</p><p><strong>80,000 Hours</strong></p><ul><li><a href=\"https://80000hours.org/2023/02/expression-of-interest-headhunting-lead/\"><u>Headhunting lead (expression of interest)</u></a> (In person, London, \u2a86\u00a370,000)&nbsp;</li><li><a href=\"https://80000hours.org/2023/02/expression-of-interest-systems-hire/\"><u>Systems hire (expression of interest)</u></a> (London preferred, remote is possible, \u00a350,000 - \u00a365,000)</li></ul><p><strong>Alignment Research Center - Evaluations Project</strong></p><ul><li><a href=\"https://jobs.lever.co/alignment.org/\"><u>Assorted jobs in Research, Engineering, and Operations</u></a> (Berkeley)</li></ul><p><strong>Anthropic</strong></p><ul><li><a href=\"https://jobs.lever.co/Anthropic/f5c4dd19-9016-4a8f-8b5c-40b1d9c6e0fb\"><u>IT Support Specialist</u></a> (Hybrid/San Francisco, $105,00 - $120,000)</li><li><a href=\"https://jobs.lever.co/Anthropic/3b9ed1d3-84f7-4c90-91dd-749496d8668c\"><u>Senior Software Security Engineer</u></a> (Hybrid/San Francisco, $270,000 - $445,000)</li></ul><p><strong>Founders Pledge</strong></p><ul><li><a href=\"https://founders-pledge.jobs.personio.de/job/1025183?display=en\"><u>Applied Researcher</u></a> (UK/USA/Europe, up to \u00a365k/$95k/\u20ac65k)</li></ul><p><strong>Giving What We Can</strong></p><ul><li><a href=\"https://givingwhatwecan.notion.site/Full-stack-Software-Engineer-0b4870b4ebcc43ec89cf57aaeab057c0\"><u>Full-stack Software Developer</u></a> (Remote, \u00a351-84k)</li></ul><p><strong>GiveDirectly</strong></p><ul><li><a href=\"https://boards.greenhouse.io/givedirectly/jobs/4131965005\"><u>Director, Major Giving</u></a> (Remote, NY/SF preferred, $135,000)</li></ul><p><strong>GiveWell</strong></p><ul><li><a href=\"https://www.givewell.org/about/jobs\"><u>Assorted research positions, including new Research Associate and Senior Malaria Researcher positions</u></a> (Remote or Oakland, CA)</li><li><a href=\"https://www.givewell.org/about/jobs/operations-specialist-recruiting\"><u>Operations Specialist, Recruiting</u></a> (Remote or Oakland, CA, $90,600 - $98,000)</li><li><a href=\"https://www.givewell.org/about/jobs/content-editor\"><u>Content Editor</u></a> (Remote or Oakland, CA, $90,600 - $98,000)</li></ul><p><strong>IDinsight</strong></p><ul><li><a href=\"https://internationalcareers-idinsight.icims.com/jobs/1304/associate-director-director---jakarta%2c-indonesia/job?mode=view&amp;mobile=false&amp;width=1136&amp;height=500&amp;bga=true&amp;needsRedirect=false&amp;jan1offset=480&amp;jun1offset=480\"><u>Associate Director/Director</u></a> (Jakarta, Indonesia)</li><li><a href=\"https://internationalcareers-idinsight.icims.com/jobs/1305/associate---senior-associate---southeast-asia-%28indonesia%29/job?mode=view&amp;mobile=false&amp;width=1136&amp;height=500&amp;bga=true&amp;needsRedirect=false&amp;jan1offset=480&amp;jun1offset=480\"><u>Associate &amp; Senior Associate</u></a> (Jakarta, Indonesia)</li></ul><p><strong>One for the World</strong></p><ul><li><a href=\"https://docs.google.com/document/d/11QokejuyfbXaVmThTnnsQwy-mYLzlcyS3JFFVEsZtCE/edit\"><u>New Chapter Expansion [NCE] Internship</u></a> (Remote in US, $19/hour, apply by&nbsp;<strong>7 April</strong>)</li><li><a href=\"https://docs.google.com/presentation/d/11Bb4vG5bwg2vAHSvJeDwywtMhvjCbY85yhhiyFWyuRw/edit#slide=id.g1f1308f5230_0_987\"><u>Organizing Manager</u></a> (Remote in US, $55,400, apply by&nbsp;<strong>29 March</strong>)&nbsp;</li></ul><p><strong>Open Philanthropy</strong></p><ul><li><a href=\"https://jobs.ashbyhq.com/openphilanthropy/0864ea34-8ecd-4276-a8a9-a480741fa0fa\"><u>Chief of Staff to Co-CEO Alexander Berger</u></a> (SF Bay Area strongly preferred, $143,967)&nbsp;</li><li><a href=\"https://jobs.ashbyhq.com/openphilanthropy/985292b9-3ba8-405d-8359-f68787691459\"><u>Business Operations Generalists</u></a> (Remote but SF or DC preferred, $90,615 - $108,359)</li></ul><p><strong>Rethink Priorities:&nbsp;</strong></p><ul><li>Worldview Investigations&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/postings/e7d9e650-753c-435a-b110-ca15314a19e6\"><u>Philosophy Researcher</u></a> and&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/postings/9c5c86fc-605c-4c72-81fa-849119d59353\"><u>Quantitative Researcher</u></a> (Remote, $100,500 - $118,500, apply by&nbsp;<strong>19 March</strong>)</li><li><a href=\"https://careers.rethinkpriorities.org/en/postings/693466ac-96dd-450c-a44f-4b1887cc00bd\"><u>AI Governance and Strategy Research Manager</u></a> (Remote, $100,500 - $114,000, apply by&nbsp;<strong>21 March</strong>)</li><li><a href=\"https://careers.rethinkpriorities.org/en/postings/5899b4c8-df46-417f-8fb7-d2950cea1d52/applications/new\"><u>Expression of interest</u></a> for co-leading a longtermist project/organization&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/A9dS2AvNpG5FqxdR9/rethink-priorities-is-inviting-expressions-of-interest-for\"><u>incubator</u></a> in the&nbsp;<a href=\"https://rethinkpriorities.org/team#longtermism\"><u>General Longtermism team</u></a> (no deadline, but sooner is better)</li><li><a href=\"https://www.insectinstitute.org/contact\"><u>Expression of interest</u></a> at&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Bsdq5wK63vLEB3Gqg/announcing-the-launch-of-the-insect-institute\"><u>The Insect Institute</u></a></li></ul><h1>Organization Updates</h1><h3>80,000 Hours</h3><p>80,000 Hours has released its&nbsp;<a href=\"https://docs.google.com/document/d/1mZmox0G910jw7I9jCoDC6vowz_PcrHRfFoW04G59XMA/edit#\"><u>two-year review for 2021 and 2022</u></a>. (You can find previous evaluations<a href=\"https://80000hours.org/about/credibility/evaluations/\">&nbsp;<u>here</u></a>.)&nbsp;</p><p>Some highlights:</p><ul><li>Over the past two years, the podcast, job board, and one-on-one advising team each grew their engagement two or three times. Web engagement hours fell by 20% in 2021 and then grew by 38% in 2022 after 80,000 Hours increased their marketing.</li><li>The core team grew from 14 staff members to 25 over two years.</li><li>They plan to grow the team by roughly 50% in 2023, adding an additional 12 people.</li><li>They plan to continue growing their main four programmes and will experiment with additional projects, such as relaunching their headhunting service and creating a new, scripted podcast with a different host.</li><li>Ben Todd stepped down as CEO in May 2022 and was replaced by Howie Lempel. Howie is now on leave from 80,000 Hours to be Interim CEO of Effective Ventures Foundation (UK), and Brenton Mayer took over as Interim CEO of 80,000 Hours in Q4 2022.&nbsp;</li><li>In 2023, they will make improving their advice a key focus of their work. As part of this, they\u2019re aiming to hire for a senior research role.</li><li>Their baseline non-marketing budget is $10.2m for 2023 and $12.5m for 2024. They\u2019re keen to fundraise above their baseline budget and are also interested in expanding their runway.</li></ul><h3>Anima International</h3><p><strong>EU ban on cages for farmed animals</strong></p><p>Anima International\u2019s Polish group is working to initiate a parliamentary discussion about the prohibition of cages for farmed animals. They aim to convince Polish politicians about the importance of supporting the End the Cage Age initiative. Poland is one of the biggest producers of eggs and chicken meat in Europe, making it particularly important to secure support for the ban among Polish politicians.&nbsp;</p><p>Thanks to this focus, a recent&nbsp;<a href=\"https://www.farmer.pl/produkcja-zwierzeca/drob-i-jaja/kurom-brakuje-tylko-netflixa-i-jacuzzi-rozmowy-polakow-o-dobrostanie,128108.html\"><u>session</u></a> of the Subcommittee on Farmed Animals Welfare and Protection of Animal Production in Poland focused on the subject of phasing out cages. Activists from Anima International were joined by representatives of Eurogroup for Animals and Compassion in World Farming Poland as well by independent scientists from fields of sociology, economics and biology. As 2023 is the year when Polish parliamentary elections are to be held, it was crucial that the&nbsp;<a href=\"https://sejm.gov.pl/Sejm9.nsf/transmisje.xsp?unid=8AA6690118C558D8C125894800409120#\"><u>session</u></a> was attended by MPs from both the ruling and opposition parties.&nbsp;</p><p><strong>EU ban on fur farming</strong></p><p>In March, the #FurFreeEurope European Citizens\u2019 Initiative signature collection ended, resulting in 1,701,892 signatures from all over the EU. Anima International organizations in Poland, Bulgaria and Denmark took part. When the initiative started in May 2022, the goal was to collect 1.4 million signatures in the EU. Not only did the campaign exceed the target, but it was done 2.5 months ahead of schedule.&nbsp;</p><h3>Animal Advocacy Careers (AAC)</h3><p>The&nbsp;<a href=\"https://www.animaladvocacycareers.org/inclusive-hiring-program\"><u>Inclusive Hiring Skills Program</u></a> is a series of (non-free) workshops designed for anyone involved in or interested in hiring who wants to create a more inclusive and diverse working environment.</p><h3>Animal Charity Evaluators</h3><p><a href=\"https://animalcharityevaluators.org/movement-grants/application-form/\"><u>Animal Charity Evaluators (ACE)</u></a> is now accepting applications for their 2023 Movement Grants! This round, ACE has introduced a maximum grant size of $50,000 and have streamlined the application process for applicants seeking $20,000 or less. Please visit their website to view the&nbsp;<a href=\"https://animalcharityevaluators.org/movement-grants/process-and-timeline/\"><u>application process</u></a>,&nbsp;<a href=\"https://animalcharityevaluators.org/movement-grants/apply-for-funding/\"><u>FAQs</u></a>, and a list of&nbsp;<a href=\"https://animalcharityevaluators.org/movement-grants/\"><u>previous grant recipients</u></a>. You can also listen to ACE\u2019s Movement Grants Program Manager explain what makes a good application. The application round is open until 11:59 pm PT on March 17, 2022.&nbsp;<a href=\"https://animalcharityevaluators.org/movement-grants/application-form/\"><u>Apply here</u></a>.</p><p>In June 2021, ACE awarded $1.09 million to organizations and projects working in various regions as part of their&nbsp;<a href=\"https://animalcharityevaluators.org/blog/announcing-our-2021-ace-movement-grants/\"><u>fifth round of Movement Grants</u></a>. In this post, you will find&nbsp;<a href=\"https://animalcharityevaluators.org/blog/movement-grants-2022-grantee-updates/\"><u>updates</u></a> from some of the organizations that have completed their projects since&nbsp;<a href=\"https://animalcharityevaluators.org/blog/ace-movement-grants-2021-grantee-updates/\"><u>ACE\u2019s last check-in</u></a> in early 2022.&nbsp;<a href=\"https://animalcharityevaluators.org/blog/movement-grants-2022-grantee-updates/\"><u>Read their updates</u></a>.</p><p>As the world\u2019s ever-changing environment continues to raise new threats for species, the animal advocacy movement must adapt to ensure the strength and resilience of the movement. Some ways to do this include conducting research, sharing resources, and fostering inclusivity among animal advocates. ACE\u2019s latest&nbsp;<a href=\"https://animalcharityevaluators.org/blog/menu-of-outcomes-series-six-ways-charities-are-strengthening-the-animal-advocacy-movement/\"><u>Menu of Outcomes post</u></a> highlights some of the ways animal advocacy groups are building a stronger movement.&nbsp;<a href=\"https://animalcharityevaluators.org/blog/menu-of-outcomes-series-six-ways-charities-are-strengthening-the-animal-advocacy-movement/\"><u>Learn more</u></a>.</p><p>Last July, thanks to generous donor supporters of ACE\u2019s Recommended Charity Fund, they were able to award $679,276 to their 2021 Top and Standout Charities. Those organizations have provided ACE with an update on how they\u2019ve used their grant to help animals, and they\u2019re excited to share their&nbsp;<a href=\"https://animalcharityevaluators.org/blog/recommended-charity-fund-six-month-update-january-2023/\"><u>achievements</u></a>.&nbsp;</p><h3>Anthropic</h3><p>On March 8, Anthropic released&nbsp;<a href=\"https://www.anthropic.com/index/core-views-on-ai-safety\"><u>a blog post</u></a> outlining their Core Views on AI Safety.</p><h3>Centre for Effective Altruism Groups Team</h3><p><strong>Retrospective on CEA\u2019s Fall \u201822 University Group Accelerator Program (UGAP)</strong>&nbsp;</p><p>CEA\u2019s University Groups Team posted a&nbsp;<a href=\"https://effectivealtruism.us8.list-manage.com/track/click?u=52b028e7f799cca137ef74763&amp;id=3a29a8fb93&amp;e=bab93cb28c\"><u>retrospective on the Fall 2022 round</u></a> of&nbsp;<a href=\"https://effectivealtruism.us8.list-manage.com/track/click?u=52b028e7f799cca137ef74763&amp;id=76fdbfa80a&amp;e=bab93cb28c\"><u>UGAP</u></a>. 68 (new) uni groups completed the program, and 112 organizers received mentorship and access to curated resources. Sign up&nbsp;<a href=\"https://effectivealtruism.us8.list-manage.com/track/click?u=52b028e7f799cca137ef74763&amp;id=855ac8869a&amp;e=bab93cb28c\"><u>here</u></a> to be informed when applications open for the next round of UGAP.</p><p><strong>Two opportunities at CEA: Pre-EAG London Summit &amp; Summer Internship</strong>&nbsp;</p><p>CEA\u2019s University Groups Team has announced the following opportunities:</p><ul><li>An internship for university group organizers during northern hemisphere summer<ul><li>Application deadline: Wednesday,&nbsp;<strong>March 22</strong></li><li><a href=\"https://effectivealtruism.us8.list-manage.com/track/click?u=52b028e7f799cca137ef74763&amp;id=42e1a155b2&amp;e=bab93cb28c\"><u>Details here</u></a></li></ul></li><li>A university group organizers summit before EAG London<ul><li>Application deadline: Sunday,&nbsp;<strong>March 27</strong></li><li><a href=\"https://effectivealtruism.us8.list-manage.com/track/click?u=52b028e7f799cca137ef74763&amp;id=4fec9e9a56&amp;e=bab93cb28c\"><u>Details here</u></a></li></ul></li></ul><h3>Center on Long-Term Risk (CLR)</h3><p>Applications for CLR\u2019s annual&nbsp;<a href=\"https://longtermrisk.org/summer-research-fellowship/\"><u>Summer Research Fellowship</u></a> opened. Deadline: Sunday, April 2, 2023 end of day anywhere.</p><h3>Centre for the Study of Existential Risk (CSER)</h3><ul><li>On the 21 March at 3pm (UK) CSER will&nbsp;<a href=\"https://www.cser.ac.uk/events/book-launch-era-global-risk/\"><u>hold a virtual launch event</u></a> for their forthcoming book The Era of Global Risk: An Introduction to Existential Risk Studies.</li><li>Paul Ingram published the&nbsp;<a href=\"https://www.cser.ac.uk/news/opinion-poll-survey-public-awareness-nuclear-winte/\"><u>results</u></a> of an opinion poll into public attitudes towards nuclear winter and the latest research.</li><li>CSER has published infographics summarizing their work on&nbsp;<a href=\"https://www.cser.ac.uk/resources/infographic-nuclear-weapons/\"><u>nuclear weapons</u></a>,&nbsp;<a href=\"https://www.cser.ac.uk/resources/infographic-climate-change/\"><u>climate change</u></a>,&nbsp;<a href=\"https://www.cser.ac.uk/resources/infographic-trends-governance-bio-risk/\"><u>bio risk</u></a>, and&nbsp;<a href=\"https://www.cser.ac.uk/resources/infographic-aifar/\"><u>AI</u></a>.</li><li>Clarissa Rios Rojas published a&nbsp;<a href=\"https://www.cser.ac.uk/news/report-building-science-policy-interface-tackling-/\"><u>report</u></a> on her work to establish a Science Policy Interface for Global Catastrophic Risk.</li><li>Freya Jephcott contributed to the World Health Organization\u2019s&nbsp;<a href=\"https://www.cser.ac.uk/news/who-early-warning-alert-and-response-emergencies-o/\"><u>operational guide</u></a> to Early Warning Alert and Response in Emergencies.</li></ul><h3>Charity Entrepreneurship</h3><p>Charity Entrepreneurship published an update&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cHDz2R5FfWGoZgWoZ/after-launch-how-are-ce-charities-progressing\"><u>on the progress of their incubated charities</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qe3z5Yfqr2ZoAvWe4/suggest-new-charity-ideas-for-charity-entrepreneurship\"><u>is now looking for new charity ideas</u></a> in the space of mass media interventions and preventative animal advocacy that you can submit via&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSf7yo1Bd4ZK6_u2nziuM_10bYYoJhAMjp-9b_MKG2tMX9PuWA/viewform\"><u>this form</u></a>.</p><h3>Faunalytics</h3><p>Faunalytics\u2019 published a new&nbsp;<a href=\"https://faunalytics.org/veg-obstacle-analysis\"><u>analysis</u></a> examining the reasons people abandon vegan or vegetarian (veg*n) diets. They identified the biggest obstacles former veg*ns faced and what they would need to resume their commitment to veg*nism.&nbsp;</p><p>They have also updated their Research Library with articles on topics including&nbsp;<a href=\"https://faunalytics.org/taking-care-of-shrimp-welfare-in-aquaculture/\"><u>shrimp welfare</u></a> and a&nbsp;<a href=\"https://faunalytics.org/myfishcheck-assessing-fish-welfare-in-aquaculture/\"><u>software application that measures different aspects of aquaculture</u></a>.&nbsp;</p><p>Faunalytics also attended EA Global in Oakland! If you didn\u2019t have the opportunity to connect with their team, they invite you to visit their free weekly&nbsp;<a href=\"https://faunalytics.org/ask-us/\"><u>Office Hours</u></a> to discuss any questions you may have about research in farmed animal protection.<br><br>Fish Welfare Initiative</p><p>FWI recently posted the following updates on their work:</p><ul><li><a href=\"https://www.fishwelfareinitiative.org/post/1-million-fishes\"><u>Our Estimated Impact Has Surpassed 1M Fishes</u></a></li><li><a href=\"https://www.fishwelfareinitiative.org/post/feb-strategy-changes\"><u>Recent Strategy Changes: Investing More in Farmer-Centric Work</u></a></li></ul><p>They believe these updates constitute significant progress on their goal of developing a scalable theory of change to improve the lives of farmed fish across India and, more broadly, Asia. And they wish to express their gratitude to all those who have helped make these outcomes possible.</p><h3>GiveDirectly</h3><p>GiveDirectly launched&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/turkeysyriaearthquakes\"><u>a cash response to the Turkey-Syria earthquake</u></a>, letting donors send funds directly to Syrian refugees in Turkey struggling to recover.&nbsp;</p><p>They\u2019ve also launched&nbsp;<a href=\"https://fundraisers.givedirectly.org/campaigns/yemenzakat\"><u>a Zakat-complaint fund</u></a> to allow Muslims to give directly through their Yemen program, timed for Ramadan giving. This is the first effective giving option made available to faith-based Muslim givers.</p><p>The organization has also published recent pieces on the role of cash transfers in adapting to climate change:&nbsp;<a href=\"https://www.vox.com/future-perfect/23574798/climate-adaptation-anticipatory-cash-transfers-givedirectly\"><u>Vox</u></a>: \u201cNew experiments show the power of giving cash right before extreme weather strikes\u201d and&nbsp;<a href=\"https://www.independent.co.uk/voices/climate-crisis-money-pakistan-floods-b2271937.html\"><u>Independent</u></a>: \u201cIt is time to think about providing direct cash assistance to help people in poverty\u201d&nbsp;</p><h3>Giving Green</h3><p>Giving Green published a&nbsp;<a href=\"https://www.givinggreen.earth/post/3-ways-to-improve-our-climate-giving-research#viewer-5u5df\"><u>brief blog post</u></a> sharing learnings from their 2022 research process and ways in which they are improving their research process in 2023.</p><h3>Giving What We Can</h3><p>Giving What We Can published a guest post on&nbsp;<a href=\"https://www.givingwhatwecan.org/blog/turkey-syria-earthquake-donation-advice\"><u>effective donation advice regarding the earthquakes in Turkey and Syria</u></a> from Stefan Shaw and Louise Kihlberg.</p><p>They also published the following content:</p><ul><li>An&nbsp;<a href=\"https://youtu.be/KR6ShEYEC9o\"><u>interview with 80,000 Hours Podcast host, Rob Wiblin</u></a></li><li>An&nbsp;<a href=\"https://youtu.be/-DWjcyfUPj8\"><u>interview with Australian politician Emma Hurst</u></a>, who is part of the Animal Justice Party</li><li>A&nbsp;<a href=\"https://www.givingwhatwecan.org/blog/member-profile-michael-scott\"><u>member story</u></a> from Michael Scott.</li></ul><h3>The Humane League</h3><p>Since releasing the&nbsp;<a href=\"https://thehumaneleague.org/2023-cage-free-eggspose\"><u>2023 Eggspos\u00e9</u></a>, THL's yearly investigative report into corporate cage-free accountability,&nbsp;<a href=\"https://drive.google.com/file/d/1KI5BdBNPWK2Hhg2txsVgCoCmPxO_bQsI/view?usp=share_link\"><u>eleven companies have started reporting</u></a> on their progress to free hens from cages. For those staying silent and refusing to be honest with the public about their overdue cage-free commitments, THL is continuing to demand transparency.&nbsp;<a href=\"https://secure.everyaction.com/JAt2AUQpQUqI_j9tDz-5Dw2\"><u>Add your voice</u></a> to tell these companies to keep their word to reduce the suffering of millions of hens.</p><p>The Open Wing Alliance, a global coalition of animal protection organizations, has launched its Manufacturers Unmasked campaign on the heels of last month's&nbsp;<a href=\"https://thehumaneleague.org/article/global-manufacturers-animal-welfare-2023\"><u>Global Manufacturers Report</u></a>. Tens of thousands of people are urging companies like PepsiCo, who released a global cage-free egg policy in 2016 but still refuses to share its progress publicly, to honor their commitment to animals and consumers. Take action for the animals with the Open Wing Alliance&nbsp;<a href=\"https://action.openwingalliance.org/\"><u>here</u></a>!</p><h3>Legal Priorities Project</h3><p>Matthijs Maas and Cecil Abungu attended the \u201c<a href=\"https://www.simoninstitute.ch/blog/post/workshop-proceedings-future-proofing-the-multilateral-system/\"><u>Future-Proofing the Multilateral System</u></a>\u201d workshop organized by the Simon Institute in Geneva.</p><p>The paper \u201c<a href=\"https://www.tandfonline.com/doi/full/10.1080/17579961.2023.2184135\"><u>Defining the scope of AI regulations</u></a>\u201d by Research Affiliate Jonas Schuett was published in the journal&nbsp;<i>Law, Innovation and Technology</i>. The paper argues that the material scope of AI regulations should not rely on the term \u2018artificial intelligence (AI)\u2019 but that policymakers should focus on the specific risks they want to reduce.</p><p>Research Affiliate Noam Kolt published a preprint of his article \u201c<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4370566\"><u>Algorithmic Black Swans</u></a>\u201d (forthcoming in the&nbsp;<i>Washington University Law Review</i>), which offers \u201ca roadmap for \u2018algorithmic preparedness\u2019 \u2014 a set of five forward-looking principles to guide the development of regulations that confront the prospect of algorithmic black swans and mitigate the harms they pose to society.\u201d</p><p><a href=\"https://www.legalpriorities.org/team/rio-popper.html\"><u>Rio Popper</u></a> joined the LPP team as Research Assistant to Christoph Winter.</p><h3>Open Philanthropy</h3><p>Co-CEO Holden Karnofsky&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/aJwcgm2nqiZu6zq2S/taking-a-leave-of-absence-from-open-philanthropy-to-work-on\"><u>announced</u></a> a three-month leave of absence from Open Philanthropy to work full-time on AI safety. Holden will be researching possible AI safety standards that, if adopted, could prevent labs from deploying dangerous AI systems.</p><h3>Rethink Priorities (RP)</h3><p><u>Reports</u></p><ul><li>Senior Research Manager&nbsp;<a href=\"https://www.bobfischer.net/\"><u>Bob Fischer</u></a> et al. have finished publishing their Moral Weight Project&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>sequence</u></a>. Bob gave a dozen&nbsp;<a href=\"https://docs.google.com/presentation/d/1Jr1NiwVfBTfb-ipmbR0wfuaL3ZXaBn3GDGfWbs8T2KQ/edit?usp=sharing\"><u>talks</u></a> about this work, including at the&nbsp;<a href=\"https://www.youtube.com/watch?app=desktop&amp;v=V--E8y_hG_o\"><u>NYU Mind, Ethics, and Policy Program</u></a>. He is now leading the new&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qi3MEEmScmK87sfBZ/worldview-investigations-team-an-overview\"><u>Worldview Investigations Team</u> (see \u201cNews\u201d below for more).</a></li><li>Senior Researcher&nbsp;<a href=\"https://www.linkedin.com/in/sagarshah/\"><u>Sagar Shah</u></a> and Senior Research Manager&nbsp;<a href=\"https://www.linkedin.com/in/peacockjacob/\"><u>Jacob Peacock</u></a> conducted a pre-registered&nbsp;<a href=\"https://rethinkpriorities.org/publications/meat-free-selection-and-menu-options\"><u>re-analysis</u></a> of&nbsp;<a href=\"https://psyarxiv.com/xk58q/\"><u>data</u></a> to better understand how meat-free meal selection varies with menu options.</li></ul><p><u>Blogs</u></p><ul><li>Executive Research Assistant&nbsp;<a href=\"https://www.linkedin.com/in/zoehanawilliams/\"><u>Zoe Williams</u></a> compiled a six-minute&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LFEvLYQZKGhkT5mpB/animal-welfare-6-months-in-6-minutes\"><u>summary</u></a> of work in the animal welfare space over the past six months.&nbsp;</li><li>In the fourth&nbsp;<a href=\"https://forum.effectivealtruism.org/s/Cubdp2SSdyMB8zJ5e/p/n52z7r8iH5pvWN2DE\"><u>installment</u></a> of a&nbsp;<a href=\"https://forum.effectivealtruism.org/s/Cubdp2SSdyMB8zJ5e\"><u>sequence</u></a> on scalable longtermist projects speedruns, Associate Researcher&nbsp;<a href=\"https://www.linkedin.com/in/marie-buhl-38a5031b5/\"><u>Marie Davidsen Buhl</u></a> outlined their shallow investigation into demonstrating the ability of different strategies to rapidly scale food production in the case of nuclear winter.</li><li>Senior Research Manager&nbsp;<a href=\"https://www.linkedin.com/in/michael-aird-3a3709133/\"><u>Michael Aird</u></a> and Collaborator&nbsp;<a href=\"https://www.linkedin.com/in/william-aldred/\"><u>Will Aldred</u></a> wrote a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HuQtr7qfB2EfcGqTu/technological-developments-that-could-increase-risks-from-1%27\"><u>post</u></a> summarizing a 2021&nbsp;<a href=\"https://docs.google.com/document/d/18nBfHOV-U_QDJ5K0GIAk6BYiNTKw673jJBb5aXFqk_8/edit\"><u>shallow review</u></a> of technological developments that could increase risks from nuclear weapons.&nbsp;</li><li>Research Assistant&nbsp;<a href=\"https://www.linkedin.com/in/patrick-levermore-385395b3/\"><u>Patrick Levermore</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/tCkBsT6cAw6LEKAbm/scoring-forecasts-from-the-2016-expert-survey-on-progress-in\"><u>scored</u></a> predictions in the&nbsp;<a href=\"https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/\"><u>2016 Expert Survey on Progress in AI</u>. He found t</a>he forecasts were fairly accurate, although the experts predicted developments to happen slightly slower than occurred.</li></ul><p><u>News</u></p><ul><li>RP is building a new interdisciplinary&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qi3MEEmScmK87sfBZ/worldview-investigations-team-an-overview\"><u>Worldview Investigations Team</u> to</a> tackle high-impact questions such as how major funders should compare the impact of neartermist and longtermist interventions.</li><li><a href=\"https://www.insectinstitute.org/\"><u>The Insect Institute</u></a> (TII)\u2014a special project fiscally sponsored by RP\u2014officially&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Bsdq5wK63vLEB3Gqg/announcing-the-launch-of-the-insect-institute\"><u>launched</u></a> under the leadership of Executive Director&nbsp;<a href=\"https://www.linkedin.com/in/dustin-crummett-b0935026/\"><u>Dustin Crummett</u>. TII will</a> address the challenges associated with the rapidly growing use of insects as food and feed, working with policymakers, industry, and other relevant stakeholders to address key uncertainties involving animal welfare, public health, and environmental sustainability.&nbsp;</li></ul>", "user": {"username": "Lizka"}}, {"_id": "amBajbqdzPB3mbwBN", "title": "80k podcast episode on sentience in AI systems", "postedAt": "2023-03-15T20:19:09.424Z", "htmlBody": "", "user": {"username": "rgb"}}, {"_id": "75CtdFj79sZrGpGiX", "title": "Success without dignity: a nearcasting story of avoiding catastrophe by luck", "postedAt": "2023-03-15T20:17:34.922Z", "htmlBody": "", "user": {"username": "HoldenKarnofsky"}}, {"_id": "CmZhcEpz7zBTGhksf", "title": "What happened to the OpenPhil OpenAI board seat?", "postedAt": "2023-03-15T20:09:38.231Z", "htmlBody": "", "user": {"username": "ChristianKleineidam"}}, {"_id": "itPTaqBfctDyF8mKe", "title": "Near Term Climate Risk\nand Intervention: A Roadmap for Research, U.S. Research Investment and International Scientific Cooperation", "postedAt": "2023-03-15T17:48:26.274Z", "htmlBody": "<p>This is a linkpost for SilverLining's new report,&nbsp;<a href=\"https://www.silverlining.ngo/reports/roadmap-for-climate-intervention-research\"><u>\"Near Term Climate Risk and Intervention: A Roadmap for Research, U.S. Research Investment and International Scientific Cooperation.\"</u></a> It describes and recommends a concerted effort to improve projections of near-term climate risks and impacts and to assess the potential for rapid climate interventions to reduce them.</p><p>This report is not written specifically for an EA audience. We\u2019re posting it here on a recommendation from someone Sam met at EAG Bay Area, who suggested that readers of the Forum might find it valuable (and might actually read the whole thing just on the basis of a Forum post!). As such, we'd like to add the caveat that this post isn't meant to be a contribution to the ongoing \"cause prioritization\" debate, i.e. comparing risks and interventions focused on climate change to those of AI, pandemics, nuclear war, etc. Rather, it's what we hope will be an approachable and informative look at a particular approach to the climate risk problem--one which, in EA parlance, is both neglected and tractable.</p><p>For the podcast lovers among you, we'll also include our executive director Kelly Wanser's recent&nbsp;<a href=\"https://www.volts.wtf/p/how-to-think-about-solar-radiation#details\"><u>interview on Volts</u></a>, which covers a lot of the same ground. And you can get in touch with us at ashah@silverlining.ngo and skaufmann@silverlining.ngo.<br>&nbsp;</p>", "user": {"username": "Sam Kaufmann"}}, {"_id": "g7kEqZamizsMYMGiD", "title": "Good depictions of speed mismatches between advanced AI systems and humans?", "postedAt": "2023-03-15T16:40:29.741Z", "htmlBody": "<p>Advanced AI systems can potentially perceive, decide, and act much faster than humans can -- perhaps many orders of magnitude faster. Given that we're used to intelligent agents all operating at about human speed, the effects of this 'speed mismatch' could be quite startling &amp; counter-intuitive. An advanced AI might out-pace human actions and reactions in a way that's somewhat analogous to the way that a 'speedster' superhero (e.g. the Flash, Quicksilver) can out-pace normal humans, or the way that some fictional characters can 'stop time' and move around as if if everyone else is frozen in place (e.g. in 'The Fermata' novel (1994) by Nicholson Baker).&nbsp;</p><p>Are there any more realistic depictions of this potential AI/human speed mismatch in nonfiction articles or books, or in science fiction stories, movies, or TV series -- especially ones that explore the risks and downsides of the mismatch?</p>", "user": {"username": "geoffreymiller"}}, {"_id": "rLDWwmojrr5QmiuWQ", "title": "Giving Green team reflects on 2022 climate giving research", "postedAt": "2023-03-16T14:39:31.778Z", "htmlBody": "<h1>Introduction</h1><p>2022 was a year of growth for Giving Green. We doubled <a href=\"https://www.givinggreen.earth/about-us\">our team</a>, added three new <a href=\"https://www.givinggreen.earth/top-climate-change-nonprofit-donations-recommendations\">top climate nonprofit recommendations</a>, and developed a <a href=\"https://www.givinggreen.earth/business-climate-strategy\">comprehensive corporate climate strategy for businesses</a> looking to take their climate action to the next level.</p><p>But as excited as we are about the positive impact we believe this growth will unlock, we are constantly on the lookout for ways we can improve our research. At the start of 2023, we took a step back to identify ways we can do better. Three review meetings, four spreadsheets, and 19,000 words later, we have a few ideas about ways we want to improve.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe0gakx8r2un\"><sup><a href=\"#fne0gakx8r2un\">[1]</a></sup></span>&nbsp;See below for three examples.</p><p><strong>1. Early-stage prioritization</strong></p><p>In 2022, we <a href=\"https://www.givinggreen.earth/mitigation-research/2022-updates-to-giving-green's-approach-and-recommendations#viewer-f8t14212703\">published</a> an overview of the steps we take and the criteria we use to prioritize our research. However, we think our overview was overly simplistic and did not provide enough insight into topics we actually looked into.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkez2pzd8vfs\"><sup><a href=\"#fnkez2pzd8vfs\">[2]</a></sup></span>&nbsp;For example, we think our <a href=\"https://www.givinggreen.earth/mitigation-research/nuclear-power\">deep dive on nuclear power</a> made a reasonable case for why support efforts could be highly cost-effective, but did not provide any explanation of why we thought it was more promising than similar geothermal efforts, or whether we even looked into geothermal at all.</p><p>To help address this, we are creating a public-facing dashboard that will share more detail on how we prioritize topics and what we have considered. This embodies our values of transparency and collaboration, and we are hopeful it will help others better understand and engage with us on our process.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo8awxf61oul\"><sup><a href=\"#fno8awxf61oul\">[3]</a></sup></span></p><p><strong>2. Cost-effectiveness analyses</strong></p><p>We often use cost-effectiveness analyses (CEAs) as an input into our assessment of cost-effectiveness. However, many of the opportunities we view as most promising also have highly uncertain inputs.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffp650tn0r2\"><sup><a href=\"#fnfp650tn0r2\">[4]</a></sup></span>&nbsp;Because of this, many of our CEAs primarily served as a way to (a) identify the parameters that affect how much a donation might reduce climate change and (b) assess whether it is plausible that a donation could be highly cost-effective.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefupe2k6fm4fj\"><sup><a href=\"#fnupe2k6fm4fj\">[5]</a></sup></span>&nbsp;For example, our <a href=\"https://www.givinggreen.earth/mitigation-research/the-good-food-institute%3A-deep-dive#viewer-4c2h22057\">Good Food Institute CEA</a> helped us delineate two pathways by which the Good Food Institute might accelerate alternative protein innovation, and estimate that it\u2019s possible a donation is within the range of cost-effectiveness we would consider for a top recommendation.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8eqxr8ws2wk\"><sup><a href=\"#fn8eqxr8ws2wk\">[6]</a></sup></span></p><p>One of our core values is truth-seeking.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref25svmgrd69i\"><sup><a href=\"#fn25svmgrd69i\">[7]</a></sup></span>&nbsp;A CEA is one of the many tools in our toolbox, but we want to see whether it is possible to make it more useful. We are speaking with academics, researchers, and other organizations to consider ways to reframe our CEAs and/or increase the accuracy of inputs. We also plan to make revisions to how we are communicating about when and how we use CEAs, in order to help readers better understand what we can (and cannot) learn from them.</p><p><strong>3. External feedback</strong></p><p>We are a small team that relies heavily on the expertise of others to guide our focus and critique our work. In the second half of 2022, alone, we had around 110 calls with various climate researchers, foundations, and organizations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyfnvbj5emlo\"><sup><a href=\"#fnyfnvbj5emlo\">[8]</a></sup></span>&nbsp;However, we were not always methodic about when we sought feedback, from whom we sought feedback, and how we weighted that feedback relative to other inputs.</p><p>Though we think it is important to remain flexible, we are drafting guidelines to help increase the consistency of our approach to feedback.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2ypgd8or5rx\"><sup><a href=\"#fn2ypgd8or5rx\">[9]</a></sup></span>&nbsp;We also plan to introduce a more formal external review step for our flagship research products.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9ilwxe3gvde\"><sup><a href=\"#fn9ilwxe3gvde\">[10]</a></sup></span>&nbsp;As part of our commitment to our value of humility, we are especially keen to ensure we receive a diversity of feedback and proactively engage with stakeholders who may have different or contrary views to our own.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn1wj0ew4y5\"><sup><a href=\"#fnn1wj0ew4y5\">[11]</a></sup></span></p><h1>It is all uphill from here</h1><p>Identifying issues is much simpler than crafting solutions, but we are excited for what lies ahead and look forward to improving our research to maximize our impact. If you have any questions or comments, we are always <a href=\"https://www.givinggreen.earth/contact\">open to feedback</a>. Otherwise, stay tuned for more!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne0gakx8r2un\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe0gakx8r2un\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Actual meeting count is probably higher, but we include here any research meeting in which we specifically focused on reviewing our 2022 research process and products. Spreadsheets included compilations for content-specific/general issues and content-specific/general improvement ideas. The 19,000 word count is based on the content of the four spreadsheets.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkez2pzd8vfs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkez2pzd8vfs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, our six-step \u201cfunnel\u201d process describes the most formal and methodical way in which we initially seek to identify promising funding opportunities. In practice, we also use other approaches to add opportunities to our pipeline, such as speaking with climate philanthropists or reviewing new academic publications.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno8awxf61oul\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo8awxf61oul\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://www.givinggreen.earth/about-us\"><u>About Giving Green</u></a>, \u201cOur values\u201d section.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfp650tn0r2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffp650tn0r2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We think this is most likely the case for two main reasons: (1) many climate funders explicitly or implicitly value certainty in their giving decisions, so this means less-certain funding opportunities are relatively underfunded; and (2) we think some of the most promising pathways to scale (e.g., policy influence and technology innovation) are also inherently difficult to assess due to their long and complicated causal paths.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnupe2k6fm4fj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefupe2k6fm4fj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We use rough benchmarks as a way to compare the cost-effectiveness of different giving opportunities. As a loose benchmark for our top recommendations, we use Clean Air Task Force (CATF), a climate policy organization we currently include as one of our top recommendations. We think it may cost CATF around $1 per ton of CO2 equivalent greenhouse gas avoided/removed, and that it serves as a useful benchmark due to its relatively affordable and calculable effects. However, this cost-effectiveness estimate includes subjective guess parameters and should not be taken literally. Instead, we use this benchmark to assess whether a giving opportunity could plausibly be within the range of cost-effectiveness we would consider for a top recommendation. As a heuristic, we consider an opportunity if its estimated cost-effectiveness is within an order of magnitude of $1/tCO2e (i.e., less than $10/tCO2e). For additional information, see our <a href=\"https://www.givinggreen.earth/us-policy-change-research/clean-air-task-force\"><u>CATF report</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8eqxr8ws2wk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8eqxr8ws2wk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Pathways: see <a href=\"https://docs.google.com/spreadsheets/d/1roDVptLCe4fEIr2qhHueBCYsdlPu6AMImigi1ZtbXUs/edit#gid=0\"><u>[published] Good Food Institute (GFI) CEA, 2022-09-14</u></a>, rows 10-14. Top recommendation cost-effectiveness: We use rough benchmarks as a way to compare the cost-effectiveness of different giving opportunities. As a loose benchmark for our top recommendations, we use Clean Air Task Force (CATF), a climate policy organization we currently include as one of our top recommendations. We think it may cost CATF around $1 per ton of CO2 equivalent greenhouse gas avoided/removed, and that it serves as a useful benchmark due to its relatively affordable and calculable effects. However, this cost-effectiveness estimate includes subjective guess parameters and should not be taken literally. Instead, we use this benchmark to assess whether a giving opportunity could plausibly be within the range of cost-effectiveness we would consider for a top recommendation. As a heuristic, we consider an opportunity if its estimated cost-effectiveness is within an order of magnitude of $1/tCO2e (i.e., less than $10/tCO2e). For additional information, see our <a href=\"https://www.givinggreen.earth/us-policy-change-research/clean-air-task-force\"><u>CATF report</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn25svmgrd69i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref25svmgrd69i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://www.givinggreen.earth/about-us\"><u>About Giving Green</u></a>, \u201cOur values\u201d section.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyfnvbj5emlo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyfnvbj5emlo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Estimate based on counting internal date-stamped call note files.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2ypgd8or5rx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2ypgd8or5rx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These guidelines include suggestions for when to seek external feedback, who to ask for external feedback, and the types of questions/feedback we should expect to value from different inputs.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9ilwxe3gvde\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9ilwxe3gvde\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, we may have a professor specializing in grid technology review a deep dive report on long-term energy storage. We may also formalize the ways in which we ask <a href=\"https://www.givinggreen.earth/about-us\"><u>Giving Green advisors</u></a> for input.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn1wj0ew4y5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn1wj0ew4y5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See <a href=\"https://www.givinggreen.earth/about-us\"><u>About Giving Green</u></a>, \u201cOur values\u201d section.</p></div></li></ol>", "user": {"username": "Giving Green"}}, {"_id": "HqX6jsQCNS3wQoGrJ", "title": "Leadership Office Hours | Scarlet Spark", "postedAt": "2023-03-15T15:12:23.438Z", "htmlBody": "<p>We\u2019re inviting leaders of animal-loving companies of all levels into our cozy Zoom office. Bring your most pressing leadership questions, and get real-time input from Scarlet Spark advisors and your peers at other organizations with a passion for animals You\u2019ll leave with greater clarity, confidence, and new relationships within the community.</p><p>\u200bIf you already know the issue you want to discuss you can submit your questions ahead of time.</p><p>For example:</p><ul><li>How do I handle this difficult conversation?</li><li>Can you give me feedback on my company-wide announcement?</li><li>What do I do about an employee who\u2019s been underperforming?</li><li>How do we design an inclusive and accessible hiring process?</li><li>Why is my team constantly miscommunicating?</li></ul><p>\u200bOur next office hours will be held on:</p><ul><li>Tuesday, March 28th: 18:00 - 19:00 GMT (1pm - 2pm ET)</li><li>Thursday, April 13th: 13:00 - 14:00 GMT (8am - 9am ET)</li><li>Tuesday, April 25th: 18:00 - 19:00 GMT (1pm - 2pm ET)</li></ul>", "user": {"username": "Sharleen "}}, {"_id": "b83Zkz4amoaQC5Hpd", "title": "Time Article Discussion - \"Effective Altruist Leaders Were Repeatedly Warned About Sam Bankman-Fried Years Before FTX Collapsed\"", "postedAt": "2023-03-15T12:40:14.250Z", "htmlBody": "<ul><li>There is a new Time article</li><li>Seems certain 98% we'll discuss it</li><li>I would like us to try and have a better discussion about this than we sometimes do.&nbsp;</li><li>Consider if you want to engage</li><li>I updated a bit on important stuff as a result of this article. You may disagree. I am going to put my \"personal updates\" in a comment</li></ul><p>Excepts from the article that I think are relevant. Bold is mine. I have made choices here and feel free to recommend I change them.</p><blockquote><p>Yet MacAskill had long been aware of concerns around Bankman-Fried. He was personally cautioned about Bankman-Fried by at least three different people in a series of conversations in 2018 and 2019, according to interviews with four people familiar with those discussions and emails reviewed by TIME.</p><p>He wasn\u2019t alone. <strong>Multiple EA leaders knew about the red flags surrounding Bankman-Fried by 2019</strong>, according to a TIME investigation based on contemporaneous documents and interviews with seven people familiar with the matter. <strong>Among the EA brain trust personally notified about Bankman-Fried\u2019s questionable behavior and business ethics were Nick Beckstead, a moral philosopher who went on to lead Bankman-Fried\u2019s philanthropic arm, the FTX Future Fund, and Holden Karnofsky, co-CEO of OpenPhilanthropy, a nonprofit organization that makes grants supporting EA causes.</strong> Some of the warnings were serious: sources say that MacAskill and Beckstead were repeatedly told that Bankman-Fried was untrustworthy, had inappropriate sexual relationships with subordinates, refused to implement standard business practices, and had been caught lying during his first months running Alameda, a crypto firm that was seeded by EA investors, staffed by EAs, and dedicating to making money that could be donated to EA causes.</p></blockquote><p>&nbsp;</p><blockquote><p>MacAskill declined to answer a list of detailed questions from TIME for this story. \u201c<strong>An independent investigation has been commissioned to look into these issues; I don\u2019t want to front-run or undermine that process by discussing my own recollections publicly</strong>,\u201d he wrote in an email. \u201cI look forward to the results of the investigation and hope to be able to respond more fully after then.\u201d Citing the same investigation, Beckstead also declined to answer detailed questions. Karnofsky did not respond to a list of questions from TIME. Through a lawyer, Bankman-Fried also declined to respond to a list of detailed written questions. The Centre for Effective Altruism (CEA) did not reply to multiple requests to explain why Bankman-Fried left the board in 2019. A spokesperson for Effective Ventures, the parent organization of CEA, cited the independent investigation, launched in Dec. 2022, and declined to comment while it was ongoing.</p></blockquote><p>&nbsp;</p><blockquote><p>In a span of less than nine months in 2022, <strong>Bankman-Fried\u2019s FTX Future Fund\u2014helmed by Beckstead\u2014gave more than $160 million to effective altruist causes, including more than $33 million to organizations connected to MacAskill.</strong> \u201cIf [Bankman-Fried] wasn\u2019t super wealthy, nobody would have given him another chance,\u201d says one person who worked closely with MacAskill at an EA organization. \u201cIt\u2019s greed for access to a bunch of money, but with a philosopher twist.\u201d</p></blockquote><p>&nbsp;</p><blockquote><p>But within months, the good karma of the venture dissipated in a series of internal clashes, many details of which have not been previously reported. Some of the issues were personal. Bankman-Fried could be \u201cdictatorial,\u201d according to one former colleague. Three former Alameda employees told TIME he had inappropriate romantic relationships with his subordinates. Early Alameda executives also believed he had reneged on an equity arrangement that would have left Bankman-Fried with 40% control of the firm, according to a document reviewed by TIME. Instead, according to two people with knowledge of the situation, he had registered himself as sole owner of Alameda.</p></blockquote><p>&nbsp;</p><blockquote><p>&nbsp;Bankman-Fried\u2019s approach to managing the business was an even bigger problem. \u201cAs we started to implement some of the really basic, standard corporate controls, we found more and more cases where I thought Sam had taken dangerous and egregious shortcuts,\u201d says one person who later raised concerns about Bankman-Fried to EA leaders. \u201cAnd in many cases [he] had concealed the fact that he had done that.\u201d</p><p><strong>\u201cWe didn\u2019t know how much money we actually had. We didn\u2019t have a clear accounting record of all the trades we\u2019d done,\u201d Bouscal says. \u201cSam continued pushing us more and more in this direction of doing a huge number of trades, a huge number of transfers, and we couldn\u2019t account for that.\u201d At the same time, she adds, Bankman-Fried was spending enormous amounts of money because \u201che didn\u2019t have a distinction between firm capital and trading capital. It was all one pool.\u201d</strong></p></blockquote><p>&nbsp;</p><blockquote><p>The meeting was short. Mac Aulay and the management team offered Bankman-Fried a buyout in exchange for his resignation as CEO, and threatened to quit if he refused. Bankman-Fried sat there silently, according to two people present, then got up and left. The next day, he came back with his answer: he would not step down. <strong>Instead, the other four members of the management team resigned, along with roughly half of Alameda\u2019s 30 employees. Mac Aulay, an Australian citizen, was forced to leave the country shortly afterward, because her work visa was tied to Alameda.</strong></p></blockquote><p>&nbsp;</p><p><i>edit: text added</i></p><blockquote><p>Bouscal recalled speaking to Mac Aulay immediately after one of Mac Aulay\u2019s conversations with MacAskill in late 2018. \u201cWill basically took Sam\u2019s side,\u201d said Bouscal, who recalls waiting with Mac Aulay in the Stockholm airport while she was on the phone. (Bouscal and Mac Aulay had once dated; though no longer romantically involved, they remain close friends.) \u201c<strong>Will basically threatened Tara,</strong>\u201d Bouscal recalls. \u201cI remember my impression being that Will was taking a pretty hostile stance here and that he was just believing Sam\u2019s side of the story, which made no sense to me.\u201d</p></blockquote><p>&nbsp;</p><blockquote><p>But one of the people who did warn others about Bankman-Fried says that he openly wielded this power when challenged. \u201cIt was like, \u2018I could destroy you,\u2019\u201d this person says. \u201cWill and Holden would believe me over you. No one is going to believe you.\u201d</p></blockquote><p>&nbsp;</p><blockquote><p>Sometime that year, the Centre for Effective Altruism did an internal investigation relating to CEA and Alameda, according to one person who was contacted during the investigation, and who said it was was conducted in part by MacAskill. Bankman-Fried left the board of the organization in 2019.</p></blockquote><p>&nbsp;</p><blockquote><p>\u201cYou vouch for him?\u201d Musk asked MacAskill.</p><p>\u201cVery much so!\u201d MacAskill replied. \u201cVery dedicated to making the long-term future of humanity go well.\u201d</p></blockquote><p><br>&nbsp;</p><blockquote><p>None of the early Alameda employees who witnessed Bankman-Fried\u2019s behavior years earlier say they anticipated this level of alleged criminal fraud. There was no \u201csmoking gun,\u201d as one put it, that revealed specific examples of lawbreaking. Even if they knew Bankman-Fried was dishonest and unethical, they say, none of them could have foreseen a fraud of this scope.</p></blockquote><p>&nbsp;</p><p>Some thoughts on how to hold a good discussion here:</p><ul><li>Please lets both write how we feel and how we think about this but clearly seperate them.</li><li>Many senior figures just <i>aren't</i> likely to respond to this. Personally I both believe they have good reasons and take it seriously but am confused as to why there has been so little comment. But I don't think we should expect responses</li><li>Note that downvotes and disagreevotes are a way of people expressing their views without having to spend the effort to type them and that is a <i>good</i> thing. It makes the discussion more representative, not less. In particular, in my anecdotal experience from discussion on facebook, if you give people the ability to vote, you see a much more representative set of participants than if poeple just write.&nbsp;</li></ul>", "user": {"username": "nathan"}}, {"_id": "nFszc44oHLHkbiDo3", "title": "Looking for data on distribution of income/wealth over time for rural households", "postedAt": "2023-03-15T10:59:16.916Z", "htmlBody": "<p><span>Our policy question is: when is it optimal for an organization such as Give Directly to transfer money to recipients as soon as&nbsp;possible, and when is it better to delay giving some of the money until the recipients experience an adverse&nbsp;event causing a loss of income.&nbsp;</span></p><p><span>We've found </span><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0304387818304413?via%3Dihub\"><span>this paper</span></a>, but it only has aggregated mean/variance quantities across households and only one type of shock. Any pointers would be appreciated!</p>", "user": {"username": "edoarad"}}, {"_id": "g5uKzBLjiEuC5k46A", "title": "FTX Community Response Survey Results", "postedAt": "2023-03-15T14:49:25.457Z", "htmlBody": "<h1>Summary</h1><p>In December 2022, Rethink Priorities, in collaboration with CEA, surveyed the EA community<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefld63blqpsx\"><sup><a href=\"#fnld63blqpsx\">[1]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyk6nseyp0kd\"><sup><a href=\"#fnyk6nseyp0kd\">[2]</a></sup></span>&nbsp;in order to gather \u201c<a href=\"https://forum.effectivealtruism.org/posts/wph9LGoT8dGpzjuZa/announcing-ftx-community-response-survey\"><u>perspectives on how the FTX crisis has impacted the community\u2019s views of the effective altruism movement, its organizations, and leaders</u></a>.\u201d&nbsp;&nbsp;</p><p>Our results found that the FTX crisis had decreased satisfaction with the EA community, and around half of respondents reported that the FTX crisis had given them concerns with EA meta organizations, the EA community and its norms, and the leaders of EA meta organizations.</p><p>Nevertheless, there were some encouraging results. The reduction in satisfaction with the community was significant, but small, and overall average community sentiment is still positive. In addition, respondents tended to agree that the EA community had responded well to the crisis, although roughly a third of respondents neither agreed nor disagreed with this. Majorities of respondents reported continuing to trust EA organizations, though over 30% reported they had substantially lost trust in EA public figures or leadership.</p><p>Respondents were more split in their views about how the EA community should respond. Respondents leaned slightly towards agreeing that the EA community should spend significant time reflecting and responding to this crisis, at the cost of spending less time on our other priorities, but slightly towards disagreement that the EA community should look very different as a result of this crisis.</p><h2>EA satisfaction</h2><h3>Recalled satisfaction</h3><p>Respondents were asked about their current satisfaction with the EA community (after the FTX crisis) and to recall their satisfaction with the EA community at the start of November 2022, prior to the FTX crisis.&nbsp;</p><p>Satisfaction with the EA community appears to be half a point (0.54) lower post-FTX compared to pre-FTX.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/mhrarriwbnyidvug7ses\"><br>Note that the median satisfaction scores are somewhat higher, but similarly showing a decrease (8 pre-FTX, 7 post-FTX).</p><h3>Satisfaction over time</h3><p>As the 2022 EA Survey was launched before the FTX crisis, we could assess how satisfaction with the EA community changed over time. We fit a generalized additive model in which we regressed the satisfaction ratings on the day the survey was taken.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/xay8foghtk5dr0nqvyry\"></p><p>These results show that satisfaction went down after the FTX crisis first started.</p><p>It should be noted however that this pattern of results could also be confounded by different groups of respondents taking the survey at different times. For example, we know that more engaged respondents tend to take the EAS earlier.&nbsp;</p><p>We therefore also looked at how the satisfaction changed over time for different engagement levels. This shows that the satisfaction levels went down over time, regardless of engagement level.</p><h2><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/oblvlkhhvnxjgqjjscye\"></h2><h2>Concerns</h2><p>Respondents were asked whether the FTX crisis has given them concerns with:</p><ul><li>Effective Altruism Meta Organizations (e.g., Centre for Effective Altruism, Open Philanthropy, 80,000 hours, etc.)</li><li>Leaders of Effective Altruism Meta Organizations (e.g., Centre for Effective Altruism, Open Philanthropy, 80,000 hours, etc.)</li><li>The Effective Altruism Community &amp; Norms</li><li>Effective Altruism Principles or Philosophy</li></ul><p>Majorities of respondents reported agreement that the crisis had given them concerns with EA meta organizations (58%), the EA community and its norms (55%), just under half reported it giving them concerns about the leaders of EA meta organizations (48%).&nbsp;</p><p>In contrast, only 25% agreed that the crisis had given them concerns about EA principles or philosophy, compared to 66% disagreeing. We think this suggests a somewhat reassuring picture where, though respondents may have concerns about the EA community in its current form, the FTX crisis has largely not caused respondents to become disillusioned with EA philosophy or principles. &nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/picukyksssuwpku8ykla\"></p><h2>Other responses</h2><p>Respondents were also asked to indicate their agreement with the following statements:</p><ul><li>I expect the FTX crisis will substantially harm my mental health in the long term.</li><li>The FTX crisis has substantially harmed my mental health in the short term.</li><li>I am less likely to associate myself with Effective Altruism publicly as a result of the FTX crisis.</li><li>I want the EA community to look very different as a result of this crisis.</li><li>The EA Community should spend significant time reflecting and responding to this crisis, at the cost of spending less time on our other priorities.</li><li>The EA community has responded well to the FTX crisis so far.</li></ul><p>Perhaps most reassuringly, more respondents agreed that the EA community had responded well to the crisis so far (47%) than disagreed (21%). That said, around a third of respondents neither agreed nor disagreed with this statement, perhaps due to ambivalence or uncertainty.</p><p>However, a plurality of respondents also agreed that the EA community should spend significant time reflecting and responding to this crisis, at the cost of spending less time on our other priorities (47%), compared to 37% disagreeing. Likewise, 39% agreed that they wanted the EA community to look very different as a result of this crisis, while 42% disagreed.</p><p>In addition, a sizable minority (38%) reported that they were less likely to associate with EA due to this crisis. But very small percentages agreed that the crisis would substantially harm their mental health in the short (13%) or long term (4%).</p><p>The results are shown below (percentages smaller than 5% are not labeled).</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/gte4woxuuvog4rmcvric\"></p><h2>Trust in EA organizations</h2><p>Respondents were asked how much they trust OP, 80K, and CEA. The results are presented below. Categories smaller than 2% are unlabelled.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/woe2ucfn70oey1argecf\"></p><p>Large majorities reported tending to trust each organization. However, levels of distrust were higher for CEA compared to 80K and OP. Comparing the average levels of trust between the organizations we see that trust in OP is highest, followed by 80K, followed by CEA.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/svrcmlz1d7krdrlqxnpx\"></p><h2>Trust in EA public figures or leadership</h2><p>Respondents were asked whether they had lost substantial trust in EA public figures or leadership (excluding FTX and Alameda employees). The majority of respondents said they had not, although a substantial portion of the respondents (31%) said that they did.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/zlpzh6m6bfjcbaxescw6\"></p><h2>What actions would respondents like to see in the next few months?</h2><p>Responses from respondents were classified into recurring themes. Each individual response could be coded as mentioning multiple themes at once.&nbsp;</p><p>Below we show a table with the counts for each category and the percentage of respondents who answered this question (n=411) who mentioned each type of action. It\u2019s important to note that since this only shows whether respondents mentioned an action&nbsp;<i>spontaneously</i> as something they would like to see, the levels of support for some actions may be much higher.&nbsp;</p><h1><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/gri0bnqqe0ik7tafxnjn\"><br>Differences between groups</h1><p>We also examined whether responses differed across different groups within the EA community.</p><p>Across different questions, we found some recurring patterns:</p><ul><li>We found a number of differences between men and non-men respondents, including non-men respondents expressing more concern, being more inclined to agree the community should look significantly different as a result of the crisis, and less likely to agree that the community had responded well to the crisis.</li><li>A similar pattern was observed in terms of cause prioritization, with higher support for longtermism being associated with expressing less concern, being more likely to agree that EA responded well to the crisis and less likely to think the EA community should look very different. Longtermists also expressed higher trust for core EA meta orgs.</li><li>The most highly engaged respondents were more likely to express concerns as a result of the crisis (except regarding EA philosophy). They were also more likely to report their mental health had been affected. And they reported lower trust in CEA (though higher trust in OP). However, they seemed slightly less likely to report that the EA community should look very different as a result of this crisis.</li></ul><h2>Satisfaction</h2><p>While we found a number of differences in satisfaction with EA (both pre- and post-FTX) across different groups-as we have in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Dhxg9BfBQEYvZETAD/ea-survey-2020-community-information#Summary\"><u>previous years</u></a>-we did not find any significant interaction between these groups and&nbsp;<i>change</i> in satisfaction towards EA. For example, we found that men reported higher satisfaction with EA than non-men<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhagqvynf7gi\"><sup><a href=\"#fnhagqvynf7gi\">[3]</a></sup></span>&nbsp;respondents, on average, but we observed a similar decrease in satisfaction across both groups post-FTX.</p><h2>Concerns</h2><h3>Cause prioritization</h3><p>We observed differences depending on the extent to which the respondent prioritizes neartermist<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn16s1f3zidk\"><sup><a href=\"#fnn16s1f3zidk\">[4]</a></sup></span>&nbsp;(global poverty and mental health) and longtermist causes (biosecurity, nuclear security, AI risk, existential risks). Those who prioritize longtermist causes more reported being less concerned across all concern questions while those who prioritize neartermist causes more reported being more concerned across all questions.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/a6jstcerbxfovijyadq8\"><figcaption>Higher ratings show higher support for longtermism (blue line) and neartermism (yellow line) causes.</figcaption></figure><h3>Engagement</h3><p>Additionally, we observed some diverging patterns involving EA engagement. More engaged respondents were less concerned about EA principles or philosophy, but more concerned about EA meta organizations and its leaders. No statistically significant differences were found regarding concerns about the EA community and norms.</p><h3><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/fgpf3lpmvzncvvka2thr\"></h3><h3>Gender</h3><p>There were several gender differences. Men were less concerned about EA principles or philosophy than women, non-binary respondents, and those who preferred to self-describe (combined into \u2018Non-men\u2019<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhagqvynf7gi\"><sup><a href=\"#fnhagqvynf7gi\">[3]</a></sup></span>). Men were also less concerned about the EA community and norms. There were no significant gender differences for the other two concerns.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/k2dvtvjzvdx4kai3mreb\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/d5tknlnvqjbsnfu2agbg 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/us40zhuh9mujil1fac24 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/gbdugtnzbtdv4zzznu1z 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/syshrvhdkjus6b0hj3cd 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/ogpu4xzwsti9dtwvbqjj 1900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/zmgp8qeo6rulyejbopol 2280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/gga3trfwpul4paerx24i 2660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/tbvw5gpd9fwk3ixmlwom 3040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/uznpjiuoqw7bmyxakrsn 3420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/hryhwbfjgpufpdbfzddv 3761w\"></figure><h3>Age</h3><p>The respondent\u2019s age was also found to relate to several of the concerns, although these relationships were very small. There was a small negative correlation with concerns about the EA community and norms (r = -.07) and leaders of EA meta organizations (r = -.06).</p><h3>Race</h3><p>Finally, we also observed a difference between white respondents and non-white respondents on one of the items<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9gk3ujfqene\"><sup><a href=\"#fn9gk3ujfqene\">[5]</a></sup></span>. Non-white respondents seem to be slightly more concerned about meta-organizations than white respondents are.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/xad7dcomapaaz33e8ibi\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/hs7sdhfnxdq7tgqmdl0y 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/qy9x0wjtue1htewybyth 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/atw1mbit8j8to6j0xsfm 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/mbavk9rcwo6hk3ezgpy2 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/o8i4bwjdeevnonogkogy 1900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/w37wx7pohh4itwnbnl1t 2280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/p02ujyem0d5vcsfdum5x 2660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/avv5s9ume6yeiithzy72 3040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/t0xtoehpmbctrp8qmiqc 3420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/pcjp5uejibhmwbcngd8n 3761w\"></figure><h2>Other responses</h2><h3>Cause prioritization</h3><p>There were several different patterns depending on the cause prioritization of the respondent. We observed that the more the respondent prioritizes neartermist causes, the more they agreed that EA should reflect and respond to this crisis, that the EA community should look very different, and that they are less likely to associate with EA. They agreed less with EA responding well to the FTX crisis so far and the FTX crisis harming their mental health (but no relationship with mental health in the longterm).</p><p>Opposite patterns were observed when the respondent prioritizes longtermist causes more. The more they prioritize longtermist causes, the more they agreed that EA responded well to the crisis so far and that FTX harmed their mental health (short term). They agreed less with wanting the EA community to look very different and to being less likely to associate with EA. No relationship was found for FTX harming their mental health in the long term and that EA should reflect and respond to this crisis.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/xwrkdhvbquot1ar4hbki\"><figcaption>Higher ratings show higher support for longtermism (blue line) and neartermism (yellow line) causes.</figcaption></figure><h3>Engagement</h3><p>There were also a few differences between respondents with different engagement levels and some of the questions. More engaged respondents agreed that FTX harmed their mental health more (although agreement levels are still rather low overall). Less engaged respondents agreed more that they want the EA community to look very different. No other differences were observed.</p><h2><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/widbe9t9o95uxqz1gy4f\"></h2><h3>Gender</h3><p>There are also several gender differences. Men slightly agreed more that EA responded well to the crisis so far and agreed less that EA should reflect and respond to this crisis, that the EA community should look very different, and that the FTX crisis harmed their mental health. No differences were found for the association with EA question and long term mental health question.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/vvlpbshkjxwy7w8r7ofu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/acpz3bkhtwzq3mdql5dm 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/bafog6csklgpvnosmcsr 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/olvukgmqvkkxibhcv1yo 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/oiwl3eeo32xjjq929zk3 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/p49zkkdfgbgspqlexu6b 1900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/y2wj1ll5wjdxeatz9xgc 2280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/xfhkobcfpdnk6edvnmuq 2660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/oivhaleatxvc3aj668o6 3040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/llf2nndrptziikfz9wcz 3420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/xruh55wiq9cdiizwvkiy 3761w\"></figure><h3>Race</h3><p>Few differences were found between white and non-white respondents. Only one item showed a statistically significant difference. Non-white respondents agreed slightly more that the FTX crisis harmed their short term mental health.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/fxwa8l4hedfnyno0hwmt\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/dmdgzfudyr2enshvbafv 380w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/lhyp4xwl7w55ixdcmgc6 760w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/yhgt4rbpwpwkrqj72qf5 1140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/cczfysp5v2t8cfcppfsn 1520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/kgj6yg2wudahqgvztz0y 1900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/hflgh7txoyyyzejucyva 2280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/nbdrkpj2zdz7voprdomt 2660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/wjssfadjlo7tbelmqd7w 3040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/hp14rvpwgop7agseho29 3420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/tt5sh2xb53fmixsm4v4g 3761w\"></figure><h2>Trust in organizations</h2><h3>Cause prioritization</h3><p>Trust levels did not covary with the degree to which respondents prioritized neartermist causes. It did seem to covary with the degree to which they prioritized longtermist causes, with greater longterm causes prioritization being associated with more trust in each organization, although this was not statistically significant for CEA.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/zkzdsbabvoaixo9atp0x\"></p><h3>Engagement</h3><p>Regarding engagement levels, it seems that OP is trusted more by more engaged respondents, while for CEA and 80K it looks like the relationship is curvilinear. More engagement with the EA community is associated with more trust but highly engaged respondents seem to trust CEA and 80K less.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/tqlhautlylrmaue3n4j2\"></p><h3>Gender</h3><p>Men trust 80K and OP more than non-men. The difference was not statistically significant for CEA.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/ugsz891wrawtvvoezqtu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/vmik5afgovlw24avb4rb 320w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/oeaugckesaegu3wzuoov 640w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/mj7c7bzb6mim4oo5p6bg 960w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/lk22hdojakymxf5w72my 1280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/fcntkvd3zcju2ne1n1ph 1600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/zbzehfyfblxma2gz8llh 1920w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/yb4hi87vgbro7t9vj2w8 2240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/crk8zbjia1aiutbk4koq 2560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/v8i4is0lcy63k7hfeb2e 2880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/guptc2nlejwr4y0jlwso 3161w\"></figure><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g5uKzBLjiEuC5k46A/gw99wm5cnoys5zahaawe\"></p><p><i>The annual EA Survey is a project of Rethink Priorities. The FTX Community Response questions were added&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/wph9LGoT8dGpzjuZa/announcing-ftx-community-response-survey\"><i><u>in collaboration with</u></i></a><i> the Centre for Effective Altruism. This post was written by David Moss and Willem Sleegers, with qualitative analysis contributed by Conor McGurk. Thanks to everyone who took and shared the survey.</i></p><p><i>If you like our work, please consider</i><a href=\"https://www.rethinkpriorities.org/newsletter\"><i><u> subscribing to our newsletter</u></i></a><i>. You can see more of our work&nbsp;</i><a href=\"https://www.rethinkpriorities.org/research\"><i><u>here</u></i></a><i>.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnld63blqpsx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefld63blqpsx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The survey questions were added as an extra section to the 2022 EA Survey, with a separate survey link provided for respondents who had already completed the EA Survey. Both surveys were closed on 1st January 2023, so responses came in throughout a month-long period in December 2022 (the FTX news broke over a 10 day period in early November).</p><p>3567 respondents completed the EAS survey. Of these, &nbsp;1012 EAS respondents were willing to answer the FTX section of questions and 300 respondents completed the separate FTX survey, resulting in an overall sample size of 1312 for the FTX-related questions.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyk6nseyp0kd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyk6nseyp0kd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Of course, it is an open question to what extent our sample reflects the community as a whole. It is plausible that people who were were more negatively affected by the FTX crisis might be <i>more</i> motivated to respond to the survey, which might skew responses in a negative direction, but it is also possible that respondents who were more negatively affected by the crisis might be less motivated to take the survey or more likely to leave the community entirely. As always, the EA Survey likely recruits disproportionately <a href=\"https://forum.effectivealtruism.org/posts/zQRHAFKGWcXXicYMo/ea-survey-2019-series-how-many-people-are-there-in-the-ea\">more highly engaged EAs and fewer less engaged EAs</a>, so the overall results primarily reflect responses from people who are moderately or highly engaged in the EA community.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhagqvynf7gi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhagqvynf7gi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These categories were collapsed due to the very low number of respondents in the \u2018non-binary\u2019 and \u2018prefer to self-describe\u2019 categories. Although there may be differences between how these categories respond compared to \u2018women\u2019, and compared to each other, the small numbers would not allow a meaningful comparison. In addition, a number of EA diversity/community-building efforts have focused on \u2018women and non-binary\u2019 individuals as a category, so this seems like a meaningful practical category.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn16s1f3zidk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn16s1f3zidk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For simplicity, we label support for these, non-longtermist, causes \u201cneartermist\u201d,&nbsp;<a href=\"https://forum.effectivealtruism.org/s/YLudF7wvkjALvAgni/p/83tEL2sHDTiWR6nwo#Relationships_Between_Causes\"><u>as we have in previous years</u></a>. However, it\u2019s worth noting explicitly that there is little reason to suppose that neartermism&nbsp;<i>specifically</i>, (e.g., attitudes or beliefs related to helping present vs. future generations or different time preference) explain support for these causes, rather than different epistemic beliefs (e.g., about appropriate kinds of evidence) or support for more traditional causes etc.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9gk3ujfqene\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9gk3ujfqene\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As with gender, and as in previous years, we combined respondents who selected a category other than \u2018white\u2019 into a single category due to the low number of respondents in each category.</p></div></li></ol>", "user": {"username": "WillemSleegers"}}, {"_id": "sgkvhJLFMDzBPgovp", "title": "Shallow Investigation: Slums ", "postedAt": "2023-03-15T10:22:38.111Z", "htmlBody": "<figure class=\"table\"><table style=\"border:1px solid hsl(0, 0%, 0%)\"><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>How this report is having an impact</strong></p><p>This report is a shallow dive into life in slums, which is a sub-area within global development. This report was produced as part of the <i>Cause Innovation Bootcamp </i>and reflects approximately 40-50 hours of research, offer a brief dive into whether we think a particular problem area is a promising area for either funders or founders to be working in. Being a shallow report, should be used to decide whether or not more research and work into a particular problem area should be prioritised.&nbsp;</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>In&nbsp; a nutshell</strong></p><ul><li><strong>Importance:&nbsp;</strong>Approximately 1 billion people currently live in slums, and it is estimated that&nbsp;a quarter of the world\u2019s population will live in a slum by 2030. Slum conditions currently do not support a healthy or high-quality life. <i><u>This is a very important issue.</u></i></li><li><strong>Tractability:&nbsp;</strong>Slum policy interventions appear relatively intractable. In contrast,&nbsp;<i>In situ&nbsp;</i>slum upgrading interventions largely deliver cost-effective results, but more research is needed.</li><li><strong>Neglectedness:&nbsp;</strong>An estimated USD$6.5 trillion will be required to meet UN SDGs on slums by 2030. Currently, only 3% of the UN\u2019s budget and a few small NGOs are working in slums. <u>This is a neglected cause area.</u></li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Key uncertainties</strong></p><ul><li><strong>Key uncertainty 1:&nbsp;</strong>Whether neighbourhood effects would exponentially increase health outcomes related to a linear change in intervention comprehensiveness.</li><li><strong>Key uncertainty 2:&nbsp;</strong>Exactly which diseases and health conditions are most prevalent in slums, and their associated burden of disease.</li></ul></td></tr><tr><td><p><strong>Author: </strong>Taylor Sweeney</p><p><strong>Editors: </strong>Akhil Bansal, Leonie Falk</p></td></tr></tbody></table></figure><h2>Table of contents</h2><p>1. <a href=\"https://forum.effectivealtruism.org/posts/sgkvhJLFMDzBPgovp/shallow-investigation-slums#Is_this_cause_area_important_\">Is this cause area important?</a></p><p>&nbsp; &nbsp; &nbsp; &nbsp;a. Health in slums</p><p>&nbsp; &nbsp; &nbsp; &nbsp;b. Why are slums so unhealthy? An introduction to neighbourhood effects</p><p>&nbsp; &nbsp; &nbsp; &nbsp;c. Infectious disease</p><p>&nbsp; &nbsp; &nbsp; &nbsp;d. Nutrition</p><p>&nbsp; &nbsp; &nbsp; &nbsp;e. Other measures of health</p><p>&nbsp; &nbsp; &nbsp; &nbsp;f. Slum data deficits</p><p>&nbsp; &nbsp; &nbsp; &nbsp;g. Geographic distribution of the problem</p><p>2. <a href=\"https://forum.effectivealtruism.org/posts/sgkvhJLFMDzBPgovp/shallow-investigation-slums#Tractability_Interventions\">Tractability/Interventions</a></p><p>&nbsp; &nbsp; &nbsp; &nbsp;a. Slum policy interventions</p><p>&nbsp; &nbsp; &nbsp; &nbsp;b. Resettlement</p><p>&nbsp; &nbsp; &nbsp; &nbsp;c. Legal recognition</p><p>&nbsp; &nbsp; &nbsp; &nbsp;d. Security of tenure</p><p>&nbsp; &nbsp; &nbsp; &nbsp;e. Slum health as a discipline of study</p><p>&nbsp; &nbsp; &nbsp; &nbsp;f. In situ slum upgrading interventions</p><p>&nbsp; &nbsp; &nbsp; &nbsp;g. Water and sanitation</p><p>&nbsp; &nbsp; &nbsp; &nbsp;h. Home improvement</p><p>&nbsp; &nbsp; &nbsp; &nbsp;i. Lighting, repaving, garbage removal</p><p>&nbsp; &nbsp; &nbsp; &nbsp;j. Community engagement: a vital framework for conducting interventions</p><p>3. <a href=\"https://forum.effectivealtruism.org/posts/sgkvhJLFMDzBPgovp/shallow-investigation-slums#Neglectedness_\">Neglectedness</a></p><p>&nbsp; &nbsp; &nbsp; &nbsp;a. Why aren\u2019t non-profits funding urban slums?</p><p>&nbsp; &nbsp; &nbsp; &nbsp;b. Organisations</p><p>4. <a href=\"https://forum.effectivealtruism.org/posts/sgkvhJLFMDzBPgovp/shallow-investigation-slums#Conclusion\">Conclusion</a></p><p>5. <a href=\"https://forum.effectivealtruism.org/posts/sgkvhJLFMDzBPgovp/shallow-investigation-slums#Footnotes\">Footnotes</a></p><p>6. <a href=\"https://forum.effectivealtruism.org/posts/sgkvhJLFMDzBPgovp/shallow-investigation-slums#References\">References</a>&nbsp;</p><p>&nbsp;</p><h1>Is this cause area important?</h1><p>Approximately 1 billion people live in slums across the world, and this is projected to double before 2030, accounting for one quarter of the world population. Slums are described as \u201ccontiguous settlement[s] where the inhabitants are characterised as having inadequate housing and basic services\u201d (UN-HABITAT, 2003). Slum populations are uniquely disadvantaged by problems including insecurity of land tenure, densely-populated and often geographically dangerous land area, and poor living conditions which act as vectors for disease. Slum dwellers consistently score lower than comparable rural or non-slum urban populations across myriad health outcomes.&nbsp;</p><p>Where there came urbanisation, so too did slums. The word \u2018slum\u2019 was first coined in the 1820s, used to describe areas of London with particularly low quality housing and hygiene. Today, the United Nations classifies slums by five criteria:</p><ol><li>Lack of access to improved water source</li><li>Lack of access to improved sanitation facilities</li><li>Lack of sufficient living area</li><li>Lack of housing durability</li><li>Lock of security of tenure</li></ol><p>Ever since the Industrial Revolution, people have been drawn to urban areas at a rapid pace to meet demand for the production of goods. Informal settlements were viewed by many as an upgrade on the monotony and poor conditions of rural life. Slums became ubiquitous first in the developed world, then, particularly in the 1980s and 90s, became more prevalent in developing nations as the demand for cheap labour rose. While building and hygiene regulations have mostly eliminated slums in high income countries today, slums continue to grow in low and middle income countries (LMICs). As three million people globally move from rural to urban areas every week, slums are set to house 2 billion people, or a quarter of the world\u2019s projected population, by 2030 (Ezeh et al., 2016; UN Habitat, 2020). The perception that slums are a transition point between rural poverty and the urban middle class must therefore change. Many now call slums their permanent home. As a result, the health of their populations require careful analysis and targeted interventions.</p><h2>Health in slums</h2><p>The health of slum dwellers is significantly worse than non-slum populations. Since slum dwellers move often and sometimes move rurally to die, life expectancy is difficult to estimate. This leaves child mortality as the best \u2018broad-brush\u2019 health measure. Higher infant and neonatal mortality has been reported in slums than rural areas across Kenya, Ecuador, Brazil, Haiti, and the Philippines. Further stratification of Bangladeshi and Kenyan populations reveals child mortality in slums to be worse than urban, rural, and rural poor populations (see Figure 1). In fact, child mortality in Bangladeshi and Kenyan slums, on average, is 33% higher than the national average, and even 7% higher than in poor rural populations. As such, health in slums can no longer be accepted as an improvement on the rural poverty many slum dwellers are seeking to escape. Indeed, slum health is worse than rural poverty.</p><figure class=\"image image_resized\" style=\"width:57.69%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sgkvhJLFMDzBPgovp/rkkkwf083r1tbniwwqxc\"><figcaption>&nbsp;</figcaption></figure><figure class=\"image image_resized\" style=\"width:57.02%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sgkvhJLFMDzBPgovp/nqodlahx3y0rh1gafqte\"><figcaption>&nbsp;</figcaption></figure><figure class=\"image image_resized\" style=\"width:56.9%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sgkvhJLFMDzBPgovp/nuhbjfzjrxptsxxz8pvq\"><figcaption>Figure 1. Childhood mortality rates in Kenya and Bangadeshi slums vs non-slums (Ezeh et al., 2016).</figcaption></figure><h2>Why are slums so unhealthy? An introduction to neighbourhood effects</h2><p>Living in a slum and living in poverty (even in an urban, non-slum context) does not produce the same health outcomes. The health of slum dwellers is largely dictated by environmental risks, such as contaminated water supply or poor housing quality. High population density means such risks are usually&nbsp;<i>shared&nbsp;</i>by everyone in the slum. This phenomenon is referred to in the literature as \u201cneighbourhood effects\u201d and is not observed in either poor rural or urban non-slum populations (Ezeh et al., 2016). Acknowledging neighbourhood effects is important for a few reasons. First, it means that interventions which lack comprehensiveness or try to intervene at the individual/household level often fail to deliver desired outcomes - this is a theme throughout the&nbsp;<i>Interventions&nbsp;</i>section of this report. Conversely, it means slum dwellers will often benefit collectively from comprehensive interventions. It is hypothesised that as a slum intervention becomes more comprehensive, health outcomes will improve exponentially.</p><h2>Infectious disease</h2><p>Slums have a very high burden of infectious disease. Diarrheal disease is particularly prevalent in slums, resulting from bacterial contamination of water supply, and disproportionately affects children under 5 years of age. In Egypt and Kenyan slums, childhood diarrheal disease is, on average 29.5% more prevalent in slums than the national average (Kenya: 20.2 vs 15.2 per 1000, Egypt: 23.9 vs 18.9 per 1000) (Mberu et al., 2016). In India, fever is 63% more prevalent in slums than non-slum areas (9.8 vs 6.0 per 1000) and diarrheal disease is 45% more prevalent in slums than non-slum areas (6.8 vs 4.7 per 1000) (Mberu et al., 2016).</p><p>Slums also act as the perfect breeding ground for vector-borne and zoonotic diseases (VBZDs). For example, leptospirosis, a viral infection carried by rats, disproportionately affects slums because rat abundance is closely associated with human density, open sewers and sources of food - all conditions fulfilled by slums (Costa et al., 2017). While most infectious diseases worldwide are becoming associated with less mortality and morbidity due to vaccination, Zika virus and dengue fever are both increasing in this regard. These two diseases are vectors of the&nbsp;<i>Aedes aegypti</i> mosquito, which breeds in shady, dirty water often found in containers or puddles. While data is lacking, the burden of Zika and dengue are hypothesised to be higher in slum populations as a result (Costa et al., 2017). In contrast, slums provide protection against one vector-borne disease: malaria. Its vector mosquito,&nbsp;<i>Anopheles</i>, prefers clean, sunlit water surrounded by vegetation, which is not typically found in slums.</p><h2>Nutrition</h2><p>Under-nutrition is the leading risk factor for disease burden in sub-Saharan Africa. Across three systematic reviews, slum dwellers have been identified as nutritionally disadvantaged compared to their urban non-slum counterparts (Ezeh et al., 2016). Under-nutrition is associated with recurrent diarrhoea and consequent growth stunting, which is also more prevalent in slums than non-slum urban, rural and national contexts in multiple regions. Breastfeeding is known to reduce incidence of diarrhoea and pneumonia, as well as reducing all-cause mortality in LMICs. However, breastfeeding rates are low in slums, likely due to labour market demands prohibiting women from staying home or taking their babies to work with them (Ezeh et al., 2016).</p><h2>Other measures of health</h2><p>Mental health and rates of non-communicable diseases (NCDs) arguably carry a higher burden of disease than communicable diseases in LMICs today. However, literature on slum mental health and NCDs is sorely lacking. This report identified one systematic review which indicated that children in slums faced a higher burden of emotional and behavioural disorders than those living in urban non-slum or rural areas (Ernst et al., 2013). In relation to NCDs, childhood asthma is more prevalent in slums, while hypertension rates are slightly lower in slums (Ezeh et al., 2016). Beyond this, little data has been identified by this report on the prevalence of other major NCDs.</p><h2>Slum data deficits</h2><p>Slum health is an understudied area. The root of the problem lies in the way census data for slums is collected. Demographics and Health Surveys (DHS), which are used as censuses in many LMICs, do not distinguish between households in urban slum and non-slum areas. Furthermore, there are many technical difficulties in counting slum populations - slum homeowners are often absent, households are not on defined blocks of land, and census staff can be afraid to enter slums. This lack of census data makes it difficult to study slum populations and understand their demographics and health measures, and tailor interventions accordingly.</p><p>Even using the available data, academic research into slums is lacking. Ezeh et al. (2016) found that only 2.8% of LMIC studies on MEDLINE and Embase which stated their location were based in slums. A mere 7% of WHO Clinical Trials which stated location were conducted in slums, and many simply used slums as a convenient location to study a topic other than slum health. This results in a small sample size for slum health and wellbeing findings, which increases the risk of bias.&nbsp;</p><h2>Geographic distribution of the problem</h2><p>Slums are a worldwide problem, but are particularly prevalent in low to middle income countries where urbanisation occurred quickly. The World Bank (2018) reports countries with the highest percentage of total population living in slums to be across Africa, South Asia and South America. This is evident in Figure 2 and 3. India and China have the largest population of slum dwellers, while countries in Central Africa have the highest percentage of their urban populations living in slums (Mahabir et al., 2018).</p><p>Most slums occupy a similar land area. Friesen et al. (2018) found that 84% of slums in eight cities across South America, Asia and Africa occupy a land area of 0.001 to 0.1 km<sup>2</sup>. There is no strong correlation between region and slum unit size, however, South America cities tend to have larger slums, while Asian cities tend to have a number of smaller slums.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sgkvhJLFMDzBPgovp/wdakuiltajkmhqd6siaj\"><figcaption>Figure 2.&nbsp;Global distribution of urban and slum populations (Mahabir et al., 2018).</figcaption></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sgkvhJLFMDzBPgovp/o6gcn6cv4lkrnfwsijfh\"><figcaption>Figure 3.&nbsp;Case study cities in publications (N = 87) on slum mapping via RS methods on top of a population density map (Kuffer et al., 2016).</figcaption></figure><h1>Tractability/Interventions</h1><p>There are two types of interventions that address the problems of slums, policy interventions and&nbsp;<i>in situ</i> slum upgrading projects.&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#9fc5e8;border:0.75pt solid #808080;padding:5pt;vertical-align:top\"><p><strong>Intervention</strong></p></td><td style=\"background-color:#9fc5e8;border:0.75pt solid #808080;padding:5pt;vertical-align:top\"><p><strong>Does it work?</strong></p></td><td style=\"background-color:#9fc5e8;border:0.75pt solid #808080;padding:5pt;vertical-align:top\"><p><strong>Who funds it?</strong></p></td></tr><tr><td style=\"background-color:#cfe2f3;border:0.75pt solid #808080;padding:5pt;vertical-align:top\" colspan=\"3\"><u>Slum policy</u></td></tr><tr><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Resettlement</td><td style=\"background-color:#f4cccc;border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Unlikely</td><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Government</td></tr><tr><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Local governance</td><td style=\"background-color:#f4cccc;border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Unlikely</td><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Government</td></tr><tr><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Security of tenure</td><td style=\"background-color:#f4cccc;border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Unlikely</td><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Government</td></tr><tr><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Slum research</td><td style=\"background-color:#fff2cc;border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Promising</td><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">NGO, UN, academia&nbsp;</td></tr><tr><td style=\"background-color:#cfe2f3;border:0.75pt solid #808080;padding:5pt;vertical-align:top\" colspan=\"3\"><i><u>In situ&nbsp;</u></i><u>slum upgrading</u></td></tr><tr><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Water and sanitation</td><td style=\"background-color:#d9ead3;border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Yes</td><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Gov, NGO, UN (NGOs: WaterAid)</td></tr><tr><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Home improvement</td><td style=\"background-color:#d9ead3;border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Yes</td><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Gov, NGO, UN</td></tr><tr><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Lighting, repaving, garbage removal</td><td style=\"background-color:#fff2cc;border:0.75pt solid #808080;padding:5pt;vertical-align:top\">No / not enough data</td><td style=\"border:0.75pt solid #808080;padding:5pt;vertical-align:top\">Government</td></tr></tbody></table></figure><h2>&nbsp;</h2><h2>Slum policy interventions</h2><h3>Resettlement</h3><p>Resettlement programs for slum dwellers have largely failed. Remember, many people choose to move to slums because of their proximity to work and, for many, a better life than could be had if they continued to live rurally. Relocation programs move slum dwellers into government housing, either to improve living conditions or re-develop the slum land. Across a variety of contexts, slum resettlement programs have worsened living conditions for slum dwellers, even when the project aimed to improve conditions (Kapse et al., 2012). New housing is usually further away from industry, so resettled populations report less job opportunities and longer, costlier commutes. Government housing often charges rent, which resettled individuals cannot afford. Resettlement programs have also failed to deliver promised infrastructure. These factors result in many moving back to their original slum or even forming a new slum, often with even worse living conditions.</p><p>Resettlement also appears an unsustainable policy because rural to urban migration is expected to increase over the next decade, so slum growth will likely outpace any resettlement program. While a well-designed city expansion project coupled with resettlement of slum dwellers into permanent housing may ameliorate many issues faced in slums, the cost of such a program is far out of the reach of most LMIC, especially considering the slum dwellers will be unable to pay much/any rent.</p><p><strong><u>ToC</u></strong></p><p><strong>Inputs:&nbsp;</strong>Relocate slum dwellers into government housing commissions at a small rental rate</p><p><strong>Outputs:&nbsp;</strong>security of tenure, improved water and sanitation, access to government services, reduced environmental risks</p><p><strong>Outcomes:&nbsp;</strong>Better health outcomes inc. child mortality/morbidity, alleviation of poverty traps</p><p><strong>Impact:&nbsp;</strong>Improved subjective wellbeing, Government use slum land for another purpose</p><p><strong><u>BOTEC</u></strong></p><p>Not required as this intervention has a null to negative impact.</p><p>&nbsp;</p><h3>Legal recognition</h3><p>Many slums are not even recognised on maps or zoned by cities, often as a political move to avoid providing municipal services. Over 20 years, Nolan et al. (2018) found that living conditions in legally recognised slums progressively improve the longer they are legalised for, compared to non-legally recognised slums. Most Indian states recognise a slum\u2019s existence based on length of residence.</p><p>Currently, no organisation that could be identified in this shallow report is fighting for legal recognition of slum areas. An NGO could feasibly lobby governments like India to loosen slum recognition criteria or disentangle the provision of basic services with slum recognition. Tractability of this intervention would largely depend on the political will for such an intervention, something I am very uncertain of and would likely be very heterogeneous, even within countries.</p><p><strong><u>ToC</u></strong></p><p><strong>Inputs</strong>: convince governments to loosen slum legalisation criteria</p><p><strong>Output:&nbsp;</strong>more legalised slums</p><p><strong>Outcomes:&nbsp;</strong>slums have access to government services</p><p><strong>Impact:&nbsp;</strong>improved health outcomes via water and sanitation upgrades</p><p><strong><u>BOTEC</u></strong></p><p>This BOTEC includes a high degree of uncertainty. In India, political lobbying is considered corruption. This leaves political activism, an intervention with high costs and low probability of success, considering political motivation to legalise more slums is low (this costs governments more money). While I have not produced a quantitative BOTEC for this intervention, I highly doubt it would be cost-effective due to the low chance of success.</p><p>&nbsp;</p><h3>Security of tenure</h3><p>Many slums are informal settlements where residents do not have tenure over their land. Evidence suggests providing security of tenure leads to increased investment into home improvement and sanitation. In a natural experiment in Peru, landowners awarded title invested almost twice as much as the control into their homes, without requiring more credit (Field, 2005). Again, research into the existence, extent or cost of such a program is lacking.&nbsp;</p><p><strong><u>ToC</u></strong></p><p><strong>Input:&nbsp;</strong>provide security of tenure to slum dwellers</p><p><strong>Output:&nbsp;</strong>increased investment into homes</p><p><strong>Outcomes:&nbsp;</strong>better sanitation and shelter quality</p><p><strong>Impact:&nbsp;</strong>reduced waterborne disease, increased subjective well being</p><p><strong><u>BOTEC</u></strong></p><p>Similar to previous interventions, this BOTEC includes a high degree of uncertainty, as there is no clear path to advocating for security of tenure. This must be through the government, who in many developing countries, do not look kindly towards political lobbying. This is consistent with research \u2013 security of tenure has not been evaluated in an RCT simply because security of tenure can only be provided by a government, not a research group. Therefore, this intervention has been labelled ineffective on the grounds that it is intractable.</p><p>&nbsp;</p><h3>Slum health as a discipline of study</h3><p>Research into slum health is sorely lacking in two key areas. First, health data for slum populations is insufficient. When census data for slum populations is available, it is usually lumped together with non-slum urban populations. Burden of different diseases is poorly studied in slums; for example, dengue fever and Zika virus prevalence has seldom been compared between urban slum and non-slum populations, so we do not know if prevalence is actually higher in slums. This is a problem because, as identified earlier in the report, living in a slum and living in poverty does not produce the same health outcomes.&nbsp;</p><p>This leads me to my second point: we do not fully understand neighbourhood effects, a phenomenon which uniquely affects the health of slum dwellers. It is hypothesised that a dose-response curve for a slum upgrading intervention and associated burden of disease would look something like Figure 4. To investigate this, an RCT would be required which performs an intervention with varying degrees of comprehensiveness at different sites, e.g. a sanitation system which covers every home vs 50% of homes vs 25% of homes vs control. This would provide important data on how comprehensive an intervention needs to be to deliver cost-effective results in a slum population. Better understanding of the problems facing slum populations would allow for more targeted and cost-effective interventions going forward.</p><figure class=\"image image_resized\" style=\"width:59.9%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/sgkvhJLFMDzBPgovp/pxwhsrmgy19tknqem42m\"><figcaption>Figure 4. A hypothesised dose-response curve for comprehensiveness of an intervention and the burden of disease in a slum population. Due to neighbourhood effects, at some point of \u2018comprehensiveness\u2019, the intervention would theoretically affect almost all of the slum population either directly or indirectly.</figcaption></figure><p><strong><u>ToC</u></strong></p><p><strong>Input:&nbsp;</strong>more research into slum health</p><p><strong>Output:&nbsp;</strong>better understanding of factors dictating slum health</p><p><strong>Outcome:&nbsp;</strong>more targeted and cost-effective slum health interventions</p><p><strong>Impact:&nbsp;</strong>better subjective wellbeing and health outcomes for slum dwellers</p><p><strong><u>BOTEC</u></strong></p><p>A BOTEC for this intervention includes a high degree of uncertainty because RCTs vary widely in cost and efficacy depending on the usefulness of research, which is difficult to predict.&nbsp;</p><p>This report failed to identify any systematic reviews which estimated the cost of an RCT in development economics. The Abdul Latif Jameel Poverty Action Lab (J-PAL), a key actor in the field of development economics, is estimated to have received $300 million in funding between 2003 - 2018, in which time they conducted 876 RCT\u2019s. By this estimate, an average development economics RCT costs $342 000, including the costs of running an institution almost entirely dedicated to producing RCTs.</p><p>Cost-effectiveness of research is near-impossible to estimate because the margins for error are so large. For example, let\u2019s say one RCT halved the cost of delivering piped water interventions in slums. Piped water currently costs $206 per household, and there are roughly 182 million slum households globally. If this intervention was applied to every household, conducting the RCT would save funders $103 per household - that\u2019s $18.7 billion saved directly through this RCT, to achieve the same amount of DALYs (0.86 per household or 157 million globally). This makes the $342k per RCT look like a rounding error. Conversely, the RCT could fail to save any lives or costs, making it $342k wasted.</p><p>Therefore, slum research is a promising intervention with great potential to fuel better interventions for slums, but also carries a high risk of failure.&nbsp;</p><h2><i>In situ</i> slum upgrading interventions</h2><h3>Water and sanitation</h3><p>Piped water and adequate sanitation facilities are likely highly cost-effective interventions in slum environments, but evidence is limited. Evidence for the provision of safe drinking water is well-established in non-slum contexts, so theoretically, improving water and sanitation in slum contexts should prove even more effective due to neighbourhood effects. A Cochrane systematic review identified one study which found a significant reduction in diarrhoeal disease incidence and duration in slum households receiving piped water compared to a control. Participants also saved 4.5% of their annual income by eliminating the cost of bottled water. A multi-component intervention study involving installing piped water and lavatories in homes, whilst paving streets and installing drainage systems resulted in a 61% relative reduction in waterborne diseases. Beyond these two studies however, results have largely been disappointing. One sanitation intervention RCT showed no effect on mortality, but other studies not specific to slums have shown significant reductions in diarrhoeal disease incidence resulting from improved sanitation. A plausible explanation for such results is that the smaller scale of these quoted studies fails to take advantage of neighbourhood effects in slums. It remains unproven whether water and sanitation interventions delivered concurrently have greater effect than independently.&nbsp;</p><p><strong><u>ToC</u></strong></p><p><i><strong>Clean drinking water</strong></i></p><p><strong>Input:&nbsp;</strong>Provide piped water taps&nbsp;</p><p><strong>Output:&nbsp;</strong>Slum dwellers have access to clean water&nbsp;</p><p><strong>Outcome:&nbsp;</strong>reduction in diarrhoeal disease&nbsp;</p><p><strong>Impact:&nbsp;</strong>better health outcomes, improved subjective wellbeing<br>&nbsp;</p><p><i><strong>Sanitation upgrading</strong></i></p><p><strong>Input:</strong> provide flush/septic toilets</p><p><strong>Output:</strong> Separation of human faeces from food, water and contact</p><p><strong>Outcome:</strong> reduction in diarrheal disease</p><p><strong>Impact:</strong> better health outcomes, improved subjective well being</p><p>&nbsp;</p><p><strong><u>BOTEC</u></strong></p><p>Access to piped water causes an absolute risk reduction of diarrhoeal disease incidence of 0.1047. Average duration is also reduced by 84%, so let\u2019s say absolute risk reduction of 0.1047 x 1.87 = 0.1926, considering both incidence and duration (Galiani et al., 2009). Diarrheal disease causes 0.81 DALY per incidence. Therefore, piped water adds 0.156 DALY per person, or 0.858 DALY per household (assuming 5.5 people per slum household on average).</p><p>Reductions in water cost resulted in a 4.5% increase to annual family income. If we assume people value one year of healthy life at 2.5x their annual income, we get 1 x annual income = 0.36 DALY. Therefore, savings from piped water results in 0.016 DALY per household. &nbsp;</p><p>I am highly uncertain about the cost of this intervention. Generally, the cost of installing household water connections is $148 - $258 (Hutton &amp; Bartram, 2008). However, the intervention studied involved a private company which would waive the cost of installation if the community provided labour. The company would charge for raw materials and training of labourers, then charge a $1.7 fee per household per month.&nbsp;</p><p>At typical costs, this intervention results in a BCR of&nbsp;<strong>$169 - $295 per DALY</strong>.&nbsp;</p><p>However, this cost could be significantly reduced through the use of community labour and intelligent policy making. Community engagement policies could be integrated to improve participation, and thus cost-effectiveness. This intervention is highly cost effective.</p><p>&nbsp;</p><h3>Home improvement</h3><p>Building better houses in slums seems to relieve some of the disease burden and poor quality of life associated with shanty houses. Galiani (2016) evaluated the efficacy of an NGO called TECHO, which provides small, prefabricated houses to families living in Latin American slums. 20-40% increases in subjective quality of life are reported, but these gains dissipate within two to three years due to hedonic adaptation. Incidence of childhood diarrhoeal disease (CDD) is reduced by 18% in treatment households, while incidence of respiratory disease is not affected. This is significantly higher than the 2.7% risk reduction for CDD reported in a Mexican natural experiment providing cement floors in slums. It remains unclear whether this divergence is due to the difference in intervention (floors vs houses) or accuracy of experimentation.</p><p><strong><u>ToC</u></strong></p><p><strong>Input:&nbsp;</strong>provide prefabricated houses or cement floors to slum dwellers</p><p><strong>Output:&nbsp;</strong>better household cleanliness and livability, as houses provide security, dryer and more temperate environments, safer storage of food and belongings</p><p><strong>Outcome:&nbsp;</strong>reduced diarrheal disease in children</p><p><strong>Impact:&nbsp;</strong>improved subjective wellbeing and lifespan</p><p><strong><u>BOTEC</u></strong></p><p>One cement floor costs $150 USD (Maclay, 2009). Anaemia causes 0.027 DALYs per incidence, while diarrhoeal disease causes 0.81 DALYs per incidence (IHME, 2022a, 2022b). Cement floors caused an absolute risk reduction of 0.083 for anaemia and 0.018 for diarrhoea (Titiunik et al., 2007)&nbsp;</p><p>Therefore, cement floors cause 0.015 DALY / child via diarrhoea reduction and 0.0022 DALY / child via anaemia reduction (total 0.017 DALY / child).</p><p>Slum households on average house 3.5 children, so one cement floor will cause 0.060 DALY / household via child health improvements.&nbsp;</p><p>This equates to&nbsp;<strong>$2485 USD / DALY.</strong></p><p>This BOTEC is likely to underestimate the CBR of cement floors because I have only considered childhood diarrhoea and anaemia prevalence, not improvements to adult health, other child health measures, or wellbeing benefits.&nbsp;</p><p>Note: a BOTEC has not been completed for entire houses (only cement floors) because each house costs $24 000 and achieves only slightly better child health outcomes and improved mental health (World Habitat, 2015). Despite this, the high cost means the CBR will likely be much lower than for cement floors. Having said that, there might be cheaper ways to add to the quality of existing houses (e.g. roofing material, floors) and cheaper ways to build houses but for this to be cost effective a house would have to be at least ten times cheaper (~$2400).</p><p>&nbsp;</p><h3>Lighting, repaving, garbage removal</h3><p>Slums commonly lack basic urban services and infrastructure; the most pertinent of these being street lighting, paved streets, and garbage removal services. Unfortunately, few experiments have been completed assessing the effects of providing such services. Gonzalez-Navarro and Quintana-Domeque (2016) provided paved streets in a Mexico slum, finding a 109% return on investment in property values, and increased rates of home improvement and consumption. Consumption increases are hypothetically related to the wealth effect: with more wealth (via property value), people feel more comfortable buying more. No health outcomes were measured. A single RCT improving lighting demonstrated a null effect, while garbage removal services have not been adequately studied to evaluate efficacy.</p><p><strong><u>ToC</u></strong></p><p><strong>Input:&nbsp;</strong>urban infrastructure or absent municipal service</p><p><strong>Output:&nbsp;</strong>increased property value</p><p><strong>Outcome:&nbsp;</strong>build wealth of slum dwellers, encourage investment into home improvement</p><p><strong>Impact:&nbsp;</strong>positive cycle of improved slum infrastructure, improve wellbeing&nbsp;</p><p><strong><u>BOTEC</u></strong></p><p>Paving streets in this study cost $701 per land block on affected streets and generated $796.8 of value per block.&nbsp; In Mexico, annual income at the relative urban poverty line is $20 496. Assuming one person per land block, paving streets adds 0.039 x annual income in land value. Assuming 1 x annual income = 0.36 DALY, this intervention results in 0.014 DALY per land block. This equates to&nbsp;<strong>$50 071 per DALY</strong>. Therefore, this intervention has negligible impact.&nbsp;</p><p>This likely underestimates DALYs as it fails to consider health outcomes or long-term income gains resulting from better access to employment locations. While unappealing to NGOs, this intervention may appeal to governments as it offers a marginal benefit to citizens whilst likely recouping construction costs in the long-term through land tax increases resulting from increased property value.</p><h2>&nbsp;</h2><h2>Community engagement: a vital framework for conducting interventions</h2><p>A growing literature indicates that community engagement in an intervention may multiply its effects (Lilford et al., 2017). A systematic review finds greater behavioural outcome effect sizes resulting when the community is engaged throughout the design, implementation and evaluation of an intervention (Lilford et al., 2017). This is likely due to increased participation in the intervention, but may also be due to the intervention better meeting the community\u2019s needs (Brunton et al., 2015). Failure to engage with the community may conversely result in intervention failure; for example, two recent RCTs in India assessing pit latrines failed simply because people chose not to use the facilities. Therefore, it is important that slum communities are engaged throughout any intervention to improve its outcomes.</p><p>&nbsp;</p><h1>Neglectedness&nbsp;</h1><p>Most funding for slum upgrades and slum policy comes from states. The UN and NGOs provide support to low income countries or politically unstable countries who are failing to provide services to slum dwellers (UN-Habitat, 2022). However, a recent economic report suggests slums require global investment of US$6.5 trillion to achieve urbanisation benchmarks outlined in the UN Sustainable Development Goals by 2030 (Global Steering Group for Impact Investment, 2022). This is more than four times the current level of investment in developing-world infrastructure projects (UN-Habitat, 2022). UN-Habitat, which funds slum interventions among other urban planning projects, receives just 6% of the UN\u2019s current budget (United Nations, 2021). Therefore, slum health appears to be a neglected cause area.</p><h2>Why aren\u2019t non-profits funding urban slums?</h2><p>Historically, philanthropic funding has focused more on rural poverty than urban poverty. Non-profits are reluctant to fund urban slums due to a number of challenges - some genuine, and some perceived. When interviewed for&nbsp;<a href=\"https://idronline.org/why-isnt-philanthropy-investing-in-urban-slums/\"><u>this article</u></a>, non-profit leaders cited three key reasons for this: legality, transience, and cost-ineffectiveness. First, many see slums as \u201cillegal settlements'' and are thus reluctant to upgrade such areas in fear of legal barriers. In fact, India\u2019s Companies Act 2013 legalises private funding for slums, and many other countries have similar legislation in place. Second, non-profits perceive slums as transient, and thus worry that slum upgrading may go to waste if the population moves. Again, this is an unfounded fear - slums are so highly-populated and rapidly growing that any mobility is highly unlikely. Finally, non-profits sometimes subscribe to the narrative that slums are unproductive and a drain on public funds. While the economic impacts of slums remain controversial, the economic benefits of slum upgrading does not. Providing better living conditions allows workers to be more productive, children to attend school at higher rates, and residents to live longer (Ezeh et al., 2016). These rebuttals suggest slums may represent an untapped resource for philanthropic funding.&nbsp;</p><p>Whether NGOs providing services a local government should provide remains controversial. Foreign aid improves living conditions, seems to induce economic growth, but does it also keep bad governments in power? Considering the magnitude of investment required, it seems that even well-run LMIC governments will fail to meet the needs of its slum populations. This represents an opportunity for NGOs to bridge the funding gap and deliver interventions which improve the lives and health of slum populations. However, careful consideration of the political landscape is crucial to ensuring the success of an intervention program beyond its specified outcome measures.</p><h2>Organisations</h2><p>Please note that these implementers have not been vetted and this is merely a quick overview about some of the implementers and is not exhaustive.</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#cfe2f3;border:1pt solid #000000;padding:5pt;vertical-align:top\">Organisation</td><td style=\"background-color:#cfe2f3;border:1pt solid #000000;padding:5pt;vertical-align:top\">Annual Expenses</td><td style=\"background-color:#cfe2f3;border:1pt solid #000000;padding:5pt;vertical-align:top\">What they do</td><td style=\"background-color:#cfe2f3;border:1pt solid #000000;padding:5pt;vertical-align:top\">Geographic focus</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Slum Dwellers International (SDI)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$14.3m</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Slum-level community development, city-level slum advocacy, global slum policy advocacy</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Africa, South Asia, Brazil</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">SHOFCO</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$8.3m</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Clean water, healthcare, education</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Kenya and Bangladesh</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Water Aid&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$136 000 (total budget x 1%)<sup>1</sup></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Water and sanitation&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Developing nations</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">United Nations Human Settlement Programme (UN-Habitat)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Estimated 2022 budget: $242m</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Wide variety</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Global</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Habitat for Humanity</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$300 445</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Housing</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Asia-Pacific</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Cities Alliance</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$13m</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Advocacy / consulting&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Global, concentrated in Africa</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">charity: water</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$1m (total budget x 1%)<sup>1</sup></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Clean drinking water&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Africa and Asia</td></tr></tbody></table></figure><p><br>&nbsp;</p><p><strong>Notes:</strong></p><ol><li>WaterAid states that only 1% of urban development budgets reach slums. Assuming this to be true for their own organisation (likely higher), we can find their minimum slum expenses by multiplying their total budget by 1%.</li></ol><p><br>&nbsp;</p><p>Of the above charities, this report identified that only SDI had produced something similar to an impact evaluation (<a href=\"https://www.tandfonline.com/doi/full/10.1080/19463138.2015.1046075\"><u>this study</u></a>). This report could not find any specific programs which had been evaluated by an RCT, nor have any been rigorously evaluated by a charity evaluator. That said, all charities listed are transparent with their finances. SDI provides a clear theory of change and has clearly performed a level of evaluation into their programs. SHOFCO is the only of the above charities to appear on Charity Navigator, and returns a 100% score for transparency. Given this, I would provisionally recommend SDI or SHOFCO for donors interested in slum health - but individual research is required. I also recognise that until these organisations conduct an RCT on their programs, it is difficult to fairly judge their impact.</p><h1>Conclusion</h1><p>More research is required to determine whether slum policies could improve conditions for slum dwellers, however I doubt any policy initiative will actually be tractable. Water and sanitation upgrading appears a cost-effective intervention in slums, however most other slum upgrading interventions would likely not meet GiveWell standards. Community engagement will likely magnify the effects of any intervention. As slums grow at an alarming rate, a better understanding of problems uniquely faced by slum populations is required. For this to happen, governments must consider slums as spatial entities and collect more extensive census data, while academia may contribute by focusing research directly on slum health. Enormous investments are required just to meet UN SDG\u2019s in slums. Despite this, only 3% of the UN\u2019s budget is directed towards slums, and few NGOs are focusing solely or primarily on slums. Therefore, this cause area is neglected and tractable.<br>&nbsp;</p><p>Overall, this shallow investigation has updated me positively on the importance of this cause area. In light of that, I recommend the following actions:&nbsp;</p><ul><li><strong>If you are an average punter:&nbsp;</strong>Consider donating to SDI or SHOFCO (after conducting your own research)</li><li><strong>If you are a funder:</strong> Consider investing in slum-based charities based on the needs identified in this report.</li><li><strong>If you are a charity evaluator:&nbsp;</strong>Continue to consider slum-based charities as the field expands (hopefully) to meet the need.</li><li><strong>If you are an academic:&nbsp;</strong>Consider researching slum health as a branch of public health or advocating for research funding in this area in your organisation. The field of slum health could greatly benefit from RCT\u2019s investigating neighbourhood effects.</li><li><strong>If you are an entrepreneur:&nbsp;</strong>Consider starting a charity in one of the areas of need identified within the report. Despite the need, no water- or sanitation-based charity has met GiveWell\u2019s standards for a top charity - so an opportunity exists.</li></ul><p>&nbsp;</p><h2>Footnotes</h2><ol><li>Quality of life is assessed via a subjective wellbeing questionnaire provided to participants, asking \u201cHow satisfied are you with\u2026 your quality of life - Would you say you are \u201cUnsatisfied\u201d, \u201cNeither Unsatisfied nor Satisfied\u201d, \u201cSatisfied\u201d or \u201cVery Satisfied\u201d?&nbsp;&nbsp;<br>&nbsp;</li></ol><h1>References</h1><p>Brunton, G., Caird, J., Stokes, G., Stansfield, C., Kneale, D., Richardson, M., &amp; Thomas, J. (2015).&nbsp;<i>Review 1: Community engagement for health via coalitions, collaborations and partnerships: A systematic review</i>.</p><p>Costa, F., Carvalho-Pereira, T., Begon, M., Riley, L., &amp; Childs, J. (2017). Zoonotic and Vector-Borne Diseases in Urban Slums: Opportunities for Intervention.&nbsp;<i>Trends Parasitol</i>,<i> 33</i>(9), 660-662.<a href=\"https://doi.org/10.1016/j.pt.2017.05.010\">&nbsp;<u>https://doi.org/10.1016/j.pt.2017.05.010</u></a></p><p>Ernst, K. C., Phillips, B. S., &amp; Duncan, B. D. (2013). Slums are not places for children to live: vulnerabilities, health outcomes, and possible interventions.&nbsp;<i>Adv Pediatr</i>,<i> 60</i>(1), 53-87.<a href=\"https://doi.org/10.1016/j.yapd.2013.04.005\">&nbsp;<u>https://doi.org/10.1016/j.yapd.2013.04.005</u></a></p><p>Ezeh, A. P., Oyebode, O. P., Satterthwaite, D. P., Chen, Y.-F. P., Ndugwa, R. P., Sartori, J. B. A., Mberu, B. P., Melendez-Torres, G. J. P., Haregu, T. P., Watson, S. I. P., Caiaffa, W. P., Capon, A. P., &amp; Lilford, R. J. P. (2016). The history, geography, and sociology of slums and the health problems of people who live in slums.&nbsp;<i>Lancet</i>,<i> 389</i>(10068), 547-558.<a href=\"https://doi.org/10.1016/S0140-6736(16)31650-6\">&nbsp;<u>https://doi.org/10.1016/S0140-6736(16)31650-6</u></a></p><p>Field, E. (2005). Property Rights and Investment in Urban Slums.&nbsp;<i>Journal of the European Economic Association</i>,<i> 3</i>(2/3), 279-290.<a href=\"https://doi.org/10.1162/1542476054472937\">&nbsp;<u>https://doi.org/10.1162/1542476054472937</u></a></p><p>Galiani, S., Gonzalez-Rozada, M., &amp; Schargrodsky, E. (2009). Water Expansions in Shantytowns: Health and Savings.&nbsp;<i>Economica (London)</i>,<i> 76</i>(304), 607-622.<a href=\"https://doi.org/10.1111/j.1468-0335.2008.00719.x\">&nbsp;<u>https://doi.org/10.1111/j.1468-0335.2008.00719.x</u></a></p><p>Global Steering Group for Impact Investment. (2022).&nbsp;<i>Informal Settlements: No Longer Invisible</i>.</p><p>Gonzalez-Navarro, M., &amp; Quintana-Domeque, C. (2016). PAVING STREETS FOR THE POOR: EXPERIMENTAL ANALYSIS OF INFRASTRUCTURE EFFECTS.&nbsp;<i>The review of economics and statistics</i>,<i> 98</i>(2), 254-267.<a href=\"https://doi.org/10.1162/REST_a_00553\">&nbsp;<u>https://doi.org/10.1162/REST_a_00553</u></a></p><p>Hutton, G., &amp; Bartram, J. (2008). Global costs of attaining the Millennium Development Goal for water supply and sanitation.&nbsp;<i>Bull World Health Organ</i>,<i> 86</i>(1), 13-19.<a href=\"https://doi.org/10.2471/blt.07.046045\">&nbsp;<u>https://doi.org/10.2471/blt.07.046045</u></a></p><p>Kapse, V. S., Pofale, A. D., Mathur, M. J. I. j. o. h., &amp; sciences, s. (2012). Paradigm of Relocation of Urban Poor Habitats (Slums): Case Study of Nagpur City.<i> 6</i>, 2916-2923.</p><p>Kuffer, M., Pfeffer, K., &amp; Sliuzas, R. (2016). Slums from Space\u201415 Years of Slum Mapping Using Remote Sensing.&nbsp;<i>Remote Sensing</i>,<i> 8</i>, 455.<a href=\"https://doi.org/10.3390/rs8060455\">&nbsp;<u>https://doi.org/10.3390/rs8060455</u></a></p><p>Lilford, R. J., Oyebode, O., Satterthwaite, D., Melendez-Torres, G. J., Chen, Y. F., Mberu, B., Watson, S. I., Sartori, J., Ndugwa, R., Caiaffa, W., Haregu, T., Capon, A., Saith, R., &amp; Ezeh, A. (2017). Improving the health and welfare of people who live in slums.&nbsp;<i>Lancet</i>,<i> 389</i>(10068), 559-570.<a href=\"https://doi.org/10.1016/s0140-6736(16)31848-7\">&nbsp;<u>https://doi.org/10.1016/s0140-6736(16)31848-7</u></a></p><p>Mahabir, R., Croitoru, A., Crooks, A. T., Agouris, P., &amp; Stefanidis, A. (2018). A Critical Review of High and Very High-Resolution Remote Sensing Approaches for Detecting and Mapping Slums: Trends, Challenges and Emerging Opportunities.&nbsp;<i>Urban Science</i>,<i> 2</i>(1).</p><p>Mberu, B. U., Haregu, T. N., Kyobutungi, C., &amp; Ezeh, A. C. (2016). Health and health-related indicators in slum, rural, and urban communities: a comparative analysis.&nbsp;<i>Glob Health Action</i>,<i> 9</i>, 33163.<a href=\"https://doi.org/10.3402/gha.v9.33163\">&nbsp;<u>https://doi.org/10.3402/gha.v9.33163</u></a></p><p>Nolan, L. B., Bloom, D. E., &amp; Subbaraman, R. (2018). Legal Status and Deprivation in Urban Slums over Two Decades.&nbsp;<i>Econ Polit Wkly</i>,<i> 53</i>(15), 47-55.</p><p>Titiunik, R., Martinez, S., Gertler, P., Galiano, S., &amp; Cattaneo, M. (2007). Housing, Health, And Happiness.&nbsp;<i>Research Working papers</i>,<i> 1</i>, 1-34.<a href=\"https://doi.org/10.1596/1813-9450-4214\">&nbsp;<u>https://doi.org/10.1596/1813-9450-4214</u></a></p><p>UN-HABITAT. (2003).&nbsp;<i>Guide to Monitoring Target 11: Improving the lives of 100 million slum dwellers</i>.<a href=\"https://unhabitat.org/sites/default/files/download-manager-files/Guide%20to%20Monitoring%20MDG%20Target%2011.pdf\">&nbsp;<u>https://unhabitat.org/sites/default/files/download-manager-files/Guide%20to%20Monitoring%20MDG%20Target%2011.pdf</u></a></p><p>UN-Habitat. (2022).&nbsp;<i>Annual Report 2021</i>.<a href=\"https://unhabitat.org/sites/default/files/2022/05/2021_annual_report.pdf\">&nbsp;<u>https://unhabitat.org/sites/default/files/2022/05/2021_annual_report.pdf</u></a></p><p>UN Habitat. (2020).&nbsp;<i>Harsh Realities: Marginalized Women in Cities of the Developing World</i>.<a href=\"https://unhabitat.org/sites/default/files/2020/06/harsh-realities-marginalized-women-in-cities-of-the-developing-world-en.pdf\">&nbsp;<u>https://unhabitat.org/sites/default/files/2020/06/harsh-realities-marginalized-women-in-cities-of-the-developing-world-en.pdf</u></a></p><p>United Nations. (2021).&nbsp;<i>Report of the Advisory Committee on Administrative and Budgetary Questions on the draft work programme and budget of the United Nations Human Settlements Programme for the year 2022</i>.&nbsp;<a href=\"https://unhabitat.org/sites/default/files/2021/10/english-202114.pdf\"><u>https://unhabitat.org/sites/default/files/2021/10/english-202114.pdf</u></a>&nbsp;</p><p>World Habitat. (2015).&nbsp;<i>TECHO \u2013 Development of Habitat</i>.<a href=\"https://world-habitat.org/world-habitat-awards/winners-and-finalists/techo-development-of-habitat/\">&nbsp;<u>https://world-habitat.org/world-habitat-awards/winners-and-finalists/techo-development-of-habitat/</u></a></p>", "user": {"username": "TaylorSweeney"}}, {"_id": "D5qknQxxqprfFjXCD", "title": "Just Pivot to AI: The secret is out", "postedAt": "2023-03-15T06:25:26.709Z", "htmlBody": "<p>Despite the 'proof' being public the <a href=\"https://gwern.net/scaling-hypothesis\">scaling hypothesis</a> remained a <a href=\"https://blakemasters.tumblr.com/post/22866240816/peter-thiels-cs183-startup-class-11-notes\">Thielian secret</a> until recently. It took chat-GPT to convince convince Facebook/Meta AI research that scaled up prosaic AI was a plausible path to beyond human intelligence. They recently had internal meetings sounding the alarm. Deepmind also believed that a significant amount of fundamental advances would be needed. Sadly this is not the case. The scaling curves don't bend anywhere near fast enough. &nbsp;I highly recommend reading about <a href=\"https://www.lesswrong.com/posts/YKfNZAmiLdepDngwi/gpt-175bee\">how many 'bees' worth of synpases/neurons various animals and LLMs have</a>. For example \"Davinci is a 175-bee model, which gets us up to hedgehog (or quail) scale. Gopher (280 BP) is partridge (or ferret) sized. More research into actual gophers is needed to know how many gophers worth of parameters Gopher has. \"</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/tahg5wjturxhsdr9rxbx\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/p8nw6jqonuozi3s2hgzc 310w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/hlkfg0qytkv1uqgowzsb 620w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/o1t73yphkgs00fu0qvgs 930w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/wfiku8hagnw5ftetrnsv 1240w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/u51ztihnpykw2lc8oyfu 1550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/oh8mnliotmquztgym3p7 1860w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/q9os9smiyhmubgd2dkbp 2170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/a49y0go1qwnsdkbr9ire 2480w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/er1bxf6tz8pvmicnxbjh 2790w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/dhchbbluiku0ygp8v7hc 3010w\"></figure><p>&nbsp;</p><p>Here is Anthropic's version of events</p><blockquote><p>In 2019, several members of what was to become the founding Anthropic team made this idea precise by developing <a href=\"https://arxiv.org/abs/2001.08361\">scaling laws</a> for AI, demonstrating that you could make AIs smarter in a predictable way, just by making them larger and training them on more data. Justified in part by these results, this team led the effort to train <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3</a>, arguably the first modern \u201clarge\u201d language model<a href=\"https://www.anthropic.com/index/core-views-on-ai-safety#footnote-2\"><sup>2</sup></a>, with over 173B parameters.</p><p>Since the discovery of scaling laws, many of us at Anthropic have believed that very rapid AI progress was quite likely. However, back in 2019, it seemed possible that multimodality, logical reasoning, speed of learning, transfer learning across tasks, and long-term memory might be \u201cwalls\u201d that would slow or halt the progress of AI. In the years since, several of these \u201cwalls\u201d, such as multimodality and logical reasoning, have fallen. Given this, most of us have become increasingly convinced that rapid AI progress will continue rather than stall or plateau. AI systems are now approaching human level performance on a large variety of tasks, and yet training these systems still costs far less than \u201cbig science\u201d projects like the Hubble Space Telescope or the Large Hadron Collider \u2013 meaning that there\u2019s a lot more room for further growth</p></blockquote><p>Imo less formalized version of the scaling hypothesis/laws were known well before 2019. Recent <a href=\"https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models\">better understanding of scaling laws</a> should not make us much less bullish on ai progress. Though perhaps there is hope we 'run out of data' before AI becomes too dangerous. I would not bank on this.&nbsp;</p><p>&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/rytpxvlwi3eyxpf47m6g\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/ufuuotms6z5acpybrayi 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/t9cmotupnichpxwf5b7g 200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/k02zw48cxu6kncyw7jfd 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/imbzideq12qo0dhfcjya 400w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/tflhnkbf5rahh6asqhqq 500w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/z5cradt0rkhq8bxzxgxn 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/adcnywjconteyikvstzu 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/o5hoz1twevxrrvproqsl 800w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/scfkb969peyjocsubylv 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/D5qknQxxqprfFjXCD/tmdud0vrf1lgjwgoues6 947w\"></figure><p>&nbsp;</p><p>As long as the scaling hypothesis remained a secret* doing anything that risked convincing more people of the scaling hypothesis was extremely risky, and imo very unethical. It might have seemed from the inside view that either the 'secret' was obvious or OpenAI was going to convince everyone of the secret anyway. But until recently there was hope OpenAI would eventually stop being so reckless. But at this point the secret is pretty much out. An AI arms race has been triggered and existing actors will shortly convince many of the remaining important doubters. This will soon include the government of [insert country you don't like].</p><p>EAs systematically doubt that the dangers posed by aligned AI. There is not much reason to assume that just because an AI system is aligned to the goals of some humans it will be good for humanity as a whole. Of course many AIs are focused on 'at least not everyone dies' as a wincon but I would hope for a better future. &nbsp;If AI is extremely hard to align the current <a href=\"https://forum.effectivealtruism.org/posts/rpHW5FnAqiJ7jwwp3/agi-in-sight-our-look-at-the-game-board\">game-board</a> is just not too unlikely to be winnable, time is running out fast. It is probably still bad to work on capabilities at top labs like Anthropic or OpenAI. If you happen to make a big advance, like switching to relu, you will burn a huge amount of timeline. But working on cool AI projects now seems positive to me.&nbsp;</p><p>Previously any cool project unacceptably risked convincing even more people of the scaling hypothesis. But if the secret is out it seems worth while to try to steer AI in a positive direction. This has been a very big update for me. Until very recently I promoted a hardline stance of 'absolutely do not work on AI'. But now we might as well play to our out of 'AI isnt that hard to align' and work on steering toward a brighter, based future.</p>", "user": {"username": "deluks917"}}, {"_id": "rsnrpvKofps5Py7di", "title": "Shutting Down the Lightcone Offices", "postedAt": "2023-03-15T01:46:03.323Z", "htmlBody": "<p><a href=\"https://www.lightconeinfrastructure.com/\">Lightcone</a> recently decided to close down a big project we'd been running for the last 1.5 years: An office space in Berkeley for people working on x-risk/EA/rationalist things that we opened August 2021.</p><p>We haven't written much about why, but I and Ben had written some messages on the internal office slack to explain some of our reasoning, which we've copy-pasted below. (They are from Jan 26th). I might write a longer retrospective sometime, but these messages seemed easy to share, and it seemed good to have something I can more easily refer to publicly.</p><h2>Background data</h2><p>Below is a graph of weekly unique keycard-visitors to the office in 2022.&nbsp;</p><p>The x-axis is each week (skipping the first 3), and the y-axis is the number of unique visitors-with-keycards.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/dldabtyqavhrqu3jao5a\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/rxa0tnnhbgrgur3jptpi 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/kf9cenpdgbg9re4ibw1a 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/i5crgcmykesvwmv6fdsu 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/bpfcx5ezdfq8guj16jlk 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/urghx7d0ljntnllpovtu 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/d3ijdfhve72fwgylqkos 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/i1ruqsyozi04azxjcnof 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/mclc477aqz39xv4yp6q0 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/f0u0ymc85bzsnp0jhkch 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/wprx8pcbgn0df326sc1z 1334w\"><figcaption>Weekly unique visitors with keycards in 2022. There was a lot of seasonality to the office.</figcaption></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/mj41xn9mry7x2ysmezfj\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/nzuldo0qfysfpzdn8qpv 130w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/blggklfcqeheenuy3nql 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/kj0rmrzee1hwmkw0bpb2 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/qezc3cqo3jget0ml6duj 520w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/vk0lqggenrcz76t0hb1x 650w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/zuxw04kyckjxggnqrj2j 780w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/uridpvg7chvjanrc3djw 910w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/iqe9tpx3czwxlz3ta5ii 1040w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/pirxp8679smzhrhh8wig 1170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rsnrpvKofps5Py7di/odahdmt4hez35sdxzve7 1264w\"><figcaption>The distribution of people by how many days they came (in 2022) looks like this.</figcaption></figure><p>Members could bring in guests, which happened quite a bit and isn't measured in the keycard data below, so I think the total number of people who came by the offices is 30-50% higher.</p><p>The offices opened in August 2021. Including guests, parties, and all the time not shown in the graphs, I'd estimate around 200-300 more people visited, for a total of around 500-600 people who used the offices.</p><p>The offices cost $70k/month on rent&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe5ob1hhsc8n\"><sup><a href=\"#fne5ob1hhsc8n\">[1]</a></sup></span>, and around $35k/month on food and drink, and ~$5k/month on contractor time for the office. It also costs core Lightcone staff time which I'd guess at around $75k/year.</p><h2>Ben's Announcement</h2><blockquote><p><strong>Closing the Lightcone Offices</strong> <strong>@channel</strong></p><p>Hello there everyone,</p><p><strong>Sadly, I'm here to write that we've decided to close down the Lightcone Offices by the end of March.</strong> While we initially intended to transplant the office to the Rose Garden Inn, Oliver has decided (and I am on the same page about this decision) to make a clean break going forward to allow us to step back and renegotiate our relationship to the entire EA/longtermist ecosystem, as well as change what products and services we build.</p><p>Below I'll give context on the decision and other details, but <strong>the main practical information is that the office will no longer be open after Friday March 24th. </strong>(There will be a goodbye party on that day.)</p><p>I asked Oli to briefly state his reasoning for this decision, here's what he says:</p><blockquote><p>An explicit part of my impact model for the Lightcone Offices has been that its value was substantially dependent on the existing EA/AI Alignment/Rationality ecosystem being roughly on track to solve the world's most important problems, and that while there are issues, pouring gas into this existing engine, and ironing out its bugs and problems, is one of the most valuable things to do in the world.</p><p>I had been doubting this assumption of our strategy for a while, even before FTX. Over the past year (with a substantial boost by the FTX collapse) my actual trust in this ecosystem and interest in pouring gas into this existing engine has greatly declined, and I now stand before what I have helped built with great doubts about whether it all will be or has been good for the world.</p><p>I respect many of the people working here, and I am glad about the overall effect of Lightcone on this ecosystem we have built, and am excited about many of the individuals in the space, and probably in many, maybe even most, future worlds I will come back with new conviction to invest and build out this community that I have been building infrastructure for for almost a full decade. But right now, I think both me and the rest of Lightcone need some space to reconsider our relationship to this whole ecosystem, and I currently assign enough probability that building things in the space is harmful for the world that I can't really justify the level of effort and energy and money that Lightcone has been investing into doing things that pretty indiscriminately grow and accelerate the things around us.</p></blockquote><p>(To Oli's points I'll add to this that it's also an ongoing cost in terms of time, effort, stress, and in terms of a lack of organizational focus on the other ideas and projects we'd like to pursue.)</p><p><strong>Oli, myself, and the rest of the Lightcone team will be available to discuss more about this in the channel </strong><a href=\"https://lightconeoffices.slack.com/archives/C04LQ1B9Z6H\"><strong>#closing-office-reasoning</strong></a><strong> </strong>where I invite any and all of you who wish to to discuss this with me, the rest of the lightcone team, and each other.</p><p>In the last few weeks I sat down and interviewed people leading the 3 orgs whose primary office is here (FAR, AI Impacts, and Encultured) and 13 other individual contributors. I asked about how this would affect them, how we could ease the change, and generally get their feelings about how the ecosystem is working out.&nbsp;</p><p>These conversations lasted on average 45 mins each, and it was very interesting to hear people's thoughts about this, and also their suggestions about other things Lightcone could work on.These conversations also left me feeling more hopeful about building related community-infrastructure in the future, as I learned of a number of positive effects that I wasn't aware of. These conversations all felt pretty real, I respect all the people involved more, and I hope to talk to many more of you at length before we close.</p><p>From the check-ins I've done with people, this seems to me to be enough time to not disrupt any SERI MATS mentorships, and to give the orgs here a comfortable enough amount of time to make new plans, but <strong>if this does put you in a tight spot, please talk to us and we'll see how we can help.</strong></p><p><strong>The campus team (me, Oli, Jacob, Rafe) will be in the office for lunch tomorrow (Friday at 1pm) to discuss any and all of this with you. </strong>We'd like to know how this is affecting you, and I'd really like to know about costs this has for you that I'm not aware of. Please feel free (and encouraged) to just chat with us in your lightcone channels (or in any of the public office channels too).</p><p>Otherwise, a few notes:</p><ul><li>The Lighthouse system is going away when the leases end. Lighthouse 1 has closed, and Lighthouse 2 will continue to be open for a few more months.</li><li>If you would like to start renting your room yourself from WeWork, I can introduce you to our point of contact, who I think would be glad to continue to rent the offices. Offices cost between $1k and $6k a month depending on how many desks are in them.</li><li>Here's a form to give the Lightcone team anonymous feedback about this decision (or anything). [Link removed from LW post.]</li><li>To talk with people about future plans starting now and after the offices close, whether to propose plans or just to let others know what you'll be doing, I've made the <a href=\"https://lightconeoffices.slack.com/archives/C04LQ1A46DT\">#future-plans</a> channel and added you all to it.</li></ul><p>It's been a thrilling experience to work alongside and get to know so many people dedicated to preventing an existential catastrophe, and I've made many new friends working here, thank you, but I think me and the Lightcone Team need space to reflect and to build something better if Earth is going to have a shot at aligning the AGIs we build.</p></blockquote><h2>Oliver's 1st message in #Closing-Office-Reasoning</h2><p><i>(In response to a question on the Slack saying \"I was hoping you could elaborate more on the idea that building the space may be net harmful.\")</i></p><blockquote><p>I think FTX is the obvious way in which current community-building can be bad, though in my model of the world FTX, while somewhat of outlier in scope, doesn't feel like a particularly huge outlier in terms of the underlying generators. Indeed it feels not that far from par for the course of the broader ecosystems relationship to honesty, aggressively pursuing plans justified by naive consequentialism, and more broadly having a somewhat deceptive relationship to the world.</p><p>Though again, I really don't feel confident about the details here and am doing a bunch of broad orienting.</p><p>I've also written some EA Forum and LessWrong comments that point to more specific things that I am worried will have or have had a negative effect on the world:</p><p>My guess is RLHF research has been pushing on a commercialization bottleneck and had a pretty large counterfactual effect on AI investment, causing a huge uptick in investment into AI and potentially an arms race between Microsoft and Google towards AGI:&nbsp;<a href=\"https://www.lesswrong.com/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research?commentId=HHBFYow2gCB3qjk2i\"><u>https://www.lesswrong.com/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research?commentId=HHBFYow2gCB3qjk2i</u></a>&nbsp;</p><p>Thoughts on how responsible EA was for the FTX fraud:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Koe2HwCQtq9ZBPwAS/quadratic-reciprocity-s-shortform?commentId=9c3srk6vkQuLHRkc6\"><u>https://forum.effectivealtruism.org/posts/Koe2HwCQtq9ZBPwAS/quadratic-reciprocity-s-shortform?commentId=9c3srk6vkQuLHRkc6</u></a>&nbsp;</p><p>Tendencies towards pretty mindkilly PR-stuff in the EA community:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ALzE9JixLLEexTKSq/cea-statement-on-nick-bostrom-s-email?commentId=vYbburTEchHZv7mn4\"><u>https://forum.effectivealtruism.org/posts/ALzE9JixLLEexTKSq/cea-statement-on-nick-bostrom-s-email?commentId=vYbburTEchHZv7mn4</u></a>&nbsp;</p><p>I feel quite worried that the alignment plan of Anthropic currently basically boils down to \"we are the good guys, and by doing a lot of capabilities research we will have a seat at the table when AI gets really dangerous, and then we will just be better/more-careful/more-reasonable than the existing people, and that will somehow make the difference between AI going well and going badly\". That plan isn't inherently doomed, but man does it rely on trusting Anthropic's leadership, and I genuinely only have marginally better ability to distinguish the moral character of Anthropic's leadership from the moral character of FTX's leadership, and in the absence of that trust the only thing we are doing with Anthropic is adding another player to an AI arms race.</p><p>More broadly, I think AI Alignment ideas/the EA community/the rationality community played a pretty substantial role in the founding of the three leading AGI labs (Deepmind, OpenAI, Anthropic), and man, I sure would feel better about a world where none of these would exist, though I also feel quite uncertain here. But it does sure feel like we had a quite large counterfactual effect on AI timelines.</p><p>Before the whole FTX collapse, I also wrote this long list of reasons for why I feel quite doomy about stuff (posted in replies, to not spam everything).</p></blockquote><h2>Oliver's 2nd message</h2><blockquote><p>(Originally written October 2022) I've recently been feeling a bunch of doom around a bunch of different things, and an associated lack of direction for both myself and Lightcone.</p><p>Here is a list of things that I currently believe that try to somehow elicit my current feelings about the world and the AI Alignment community.</p><ol><li>In most worlds RLHF, especially if widely distributed and used, seems to make the world a bunch worse from a safety perspective (by making unaligned systems appear aligned at lower capabilities levels, meaning people are less likely to take alignment problems seriously, and by leading to new products that will cause lots of money to go into AI research, as well as giving a strong incentive towards deception at higher capability levels)</li><li>It's a bad idea to train models directly on the internet, since the internet as an environment makes supervision much harder, strongly encourages agency, has strong convergent goals around deception, and also gives rise to a bunch of economic applications that will cause more money to go into AI</li><li>The EA and AI Alignment community should probably try to delay AI development somehow, and this will likely include getting into conflict with a bunch of AI capabilities organizations, but it's worth the cost</li><li>I don't currently see a way to make AIs very useful for doing additional AI Alignment research, and don't expect any of the current approaches for that to work (like ELK, or trying to imitate humans by doing more predictive modeling of human behavior and then hoping they turn out to be useful), but it sure would be great if we found a way to do this (but like, I don't think we currently know how to do this)</li><li>I am quite worried that it's going to be very easy to fool large groups of humans, and that AI is quite close to seeming very aligned and sympathetic to executives at AI companies, as well as many AI alignment researchers (and definitely large parts of the public). I don't think this will be the result of human modeling, but just the result of pushing the AI into patterns of speech/behaior that we associate with being less threatening and being more trustworthy. In some sense this isn't a catastrophic risk because this kind of deception doesn't cause the AI to dispower the humans, but I do expect it to make actually getting the research to stop or to spend lots of resources on alignment a lot harder later on.</li><li>I do sure feel like a lot of AI alignment research is very suspiciously indistinguishable from capabilities research, and I think this is probably for the obvious bad reasons instead of this being an inherent property of these domains (the obvious bad reason being that it's politically advantageous to brand your research as AI Alignment research and capabilities research simultaneously, since that gives you more social credibility, especially from the EA crowd which has a surprisingly strong talent pool and is also just socially close to a lot of top AI capabilities people)</li><li>I think a really substantial fraction of people who are doing \"AI Alignment research\" are instead acting with the primary aim of \"make AI Alignment seem legit\". These are not the same goal, a lot of good people can tell and this makes them feel kind of deceived, and also this creates very messy dynamics within the field where people have strong opinions about what the secondary effects of research are, because that's the primary thing they are interested in, instead of asking whether the research points towards useful true things for actually aligning the AI.</li><li>More broadly, I think one of the primary effects of talking about AI Alignment has been to make more people get really hyped about AGI, and be interested in racing towards AGI. Generally knowing about AGI-Risk does not seem to have made people more hesitant towards racing and slow down, but instead caused them to accelerate progress towards AGI, which seems bad on the margin since I think humanity's chances of survival do go up a good amount with more time.</li><li>It also appears that people who are concerned about AGI risk have been responsible for a very substantial fraction of progress towards AGI, suggesting that there is a substantial counterfactual impact here, and that people who think about AGI all day are substantially better at making progress towards AGI than the average AI researcher (though this could also be explained by other attributes like general intelligence or openness to weird ideas that EA and AI Alignment selects for, though I think that's somewhat less likely)</li><li>A lot of people in AI Alignment I've talked to have found it pretty hard to have clear thoughts in the current social environment, and many of them have reported that getting out of Berkeley, or getting social distance from the core of the community has made them produce better thoughts. I don't really know whether the increased productivity here is born out by evidence, but really a lot of people that I considered promising contributors a few years ago are now experiencing a pretty active urge to stay away from the current social milieu.</li><li>I think all of these considerations in-aggregate make me worried that a lot of current work in AI Alignment field-building and EA-community building is net-negative for the world, and that a lot of my work over the past few years has been bad for the world (most prominently transforming LessWrong into something that looks a lot more respectable in a way that I am worried might have shrunk the overton window of what can be discussed there by a lot, and having generally contributed to a bunch of these dynamics).</li><li>Exercising some genre-saviness, I also think a bunch of this is driven by just a more generic \"I feel alienated by my social environment changing and becoming more professionalized and this is robbing it of a lot of the things I liked about it\". I feel like when people feel this feeling they often are holding on to some antiquated way of being that really isn't well-adapted to their current environment, and they often come up with fancy rationalizations for why they like the way things used to be.</li><li>I also feel confused about how to relate to the stronger equivocation of ML-skills with AI Alignment skills. I don't personally have much of a problem with learning a bunch of ML, and generally engage a good amount with the ML literature (not enough to be an active ML researcher, but enough to follow along almost any conversation between researchers), but I do also feel a bit of a sense of being personally threatened, and other people I like and respect being threatened, in this shift towards requiring advanced cutting-edge ML knowledge in order to feel like you are allowed to contribute to the field. I do feel a bit like my social environment is being subsumed by and is adopting the status hierarchy of the ML community in a way that does not make me trust what is going on (I don't particularly like the status hierarchy and incentive landscape of the ML community, which seems quite well-optimized to cause human extinction)</li><li>I also feel like the EA community is being very aggressive about recruitment in a way that locally in the Bay Area has displaced a lot of the rationality community, and I think this is broadly bad, both for me personally and also because I just think the rationality community had more of the right components to think sanely about AI Alignment, many of which I feel like are getting lost</li><li>I also feel like with Lightcone and Constellation coming into existence, and there being a lot more money and status around, the inner circle dynamics around EA and longtermism and the Bay Area community have gotten a lot worse, and despite being a person who I think generally is pretty in the loop with stuff, have found myself being worried and stressed about being excluded from some important community function, or some important inner circle. I am quite worried that me founding the Lightcone Offices was quite bad in this respect, by overall enshrining some kind of social hierarchy that wasn't very grounded in things I actually care about (I also personally felt a very strong social pressure to exclude interesting but socially slightly awkward people from being in Lightcone that I ended up giving into, and I think this was probably a terrible mistake and really exacerbated the dynamics here)</li><li>I think some of the best shots we have for actually making humanity not go extinct (slowing down AI progress, pivotal acts, intelligence enhancement, etc.) feel like they have a really hard time being considered in the current overton window of the EA and AI Alignment community, and I feel like people being unable to consider plans in these spaces both makes them broadly less sane, but also just like prevents work from happening in these areas.</li><li>I get a lot of messages these days about people wanting me to moderate or censor various forms of discussion on LessWrong that I think seem pretty innocuous to me, and the generators of this usually seem to be reputation related. E.g. recently I've had multiple pretty influential people ping me to delete or threaten moderation action against the authors of posts and comments talking about: How OpenAI doesn't seem to take AI Alignment very seriously, why gene drives against Malaria seem like a good idea, why working on intelligence enhancement is a good idea. In all of these cases the person asking me to moderate did not leave any comment of their own trying to argue for their position, before asking me to censor the content. I find this pretty stressful, and also like, most of the relevant ideas feel like stuff that people would have just felt comfortable discussing openly on LW 7 years ago or so (not like, everyone, but there wouldn't have been so much of a chilling effect so that nobody brings up these topics).</li></ol></blockquote><h2>Ben's 1st message in #Closing-Office-Reasoning</h2><p><i>Note from Ben: I have lightly edited this because I wrote it very quickly at the time</i></p><blockquote><p>(I drafted this earlier today and didn't give it much of a second pass, forgive me if it's imprecise or poorly written.)</p><p>Here are some of the reasons I'd like to move away from providing offices as we have done so far.</p><ul><li><strong>Having two locations comes with a large cost.</strong> To track how a space is functioning, what problems people are running into, how the culture changes, what improvements could be made, I think I need to be there at least 20% of my time each week (and ideally ~50%), and that\u2019s a big travel cost to the focus of the lightcone team.</li><li><strong>Offices are a high-commitment abstraction for which it is hard to iterate.</strong> In trying to improve a culture, I might try to help people start more new projects, or gain additional concepts that help them understand the world, or improve the standards arguments are held to, or something else. But there's relatively little space for a lot of experimentation and negotiation in an office space \u2014 you\u2019ve mostly made a commitment to offer a basic resource and then to get out of people's way.</li><li><strong>The \u201cenculturation to investment\u201d ratio was very lopsided. </strong>For example, with SERI MATS, many people came for 2.5 months, for whom I think a better selection mechanism would have been something shaped like a 4-day AIRCS-style workshop to better get to know them and think with them, and then pick a smaller number of the best people from that to invest further into. If I came up with an idea right now for what abstraction I'd prefer, it'd be something like an ongoing festival with lots of events and workshops and retreats for different audiences and different sorts of goals, with perhaps a small office for independent alignment researchers, rather than an office space that has a medium-size set of people you're committed to supporting long-term.</li><li><strong>People did not do much to invest in each other in the office.</strong> I think this in part because the office does not capture other parts of people\u2019s lives (e.g. socializing), but also I think most people just didn\u2019t bring their whole spirit to this in some ways, and I\u2019m not really sure why. I think people did not have great aspirations for themselves or each other. I did not feel here that folks had a strong common-spirit \u2014 that they thought each other could grow to be world-class people who changed the course of history, and did not wish to invest in each other in that way. (There were some exceptions to note, such as Alex Mennen\u2019s <i>Math Talks</i>, John Wentworth's <i>Framing Practica, </i>and some of the ways that people in the <i>Shard Theory</i> teams worked together with the hope of doing something incredible, which both felt like people were really investing into communal resources and other people.) I think a common way to know whether people are bringing their spirit to something is whether they create art about it \u2014 songs, in-jokes, stories, etc. Soon after the start I felt nobody was going to really bring themselves so fully to the space, even though we hoped that people would. I think there were few new projects from collaborations in the space, other than between people who already had a long history.</li></ul><p>And regarding the broader ecosystem:</p><ul><li><strong>Some of the primary projects getting resources from this ecosystem do not seem built using the principles and values (e.g. integrity, truth-seeking, x-risk reduction) that I care about \u2014 such as FTX, OpenAI, Anthropic, CEA, Will MacAskill's career as a public intellectual \u2014 and those that do seem to have closed down or been unsupported (such as FHI, MIRI, CFAR). </strong>Insofar as these are the primary projects who will reap the benefits of the resources that Lightcone invests into this ecosystem, I would like to change course.</li><li><strong>The moral maze nature of the EA/longtermist ecosystem has increased substantially over the last two years, and the simulacra level of its discourse has notably risen too</strong>. There are many more careerist EAs working here and at events, it\u2019s more professionalized and about networking. Many new EAs are here not because they have a deep-seated passion for doing what\u2019s right and using math to get the answers, but because they\u2019re looking for an interesting, well-paying job in a place with nice nerds. Or are just noticing that there\u2019s a lot of resources being handed out in a very high-trust way. One of the people I interviewed at the office said they often could not tell whether a newcomer was expressing genuine interest in some research, or was trying to figure out \u201chow the system of reward\u201d worked so they could play it better, because the types of questions in both cases seemed so similar. [Added to LW post: I also remember someone joining the offices to collaborate on a project, who explained that in their work they were looking for \"The next Eliezer Yudkowsky or Paul Christiano\". When I asked what aspects of Eliezer they wanted to replicate, they said they didn't really know much about Eliezer but it was something that a colleague of theirs said a lot.] It also seems to me that the simulacra level of writing on the EA Forum is increasing, whereby language is increasingly used primarily to signal affiliation and policy-preferences rather than to explain how reality works. I am here in substantial part because of people (like Eliezer Yudkowsky and Scott Alexander) honestly trying to explain how the world works in their online writing and doing a damn good job of it, and I feel like there is much less of that today in the EA/longtermist ecosystem. This makes the ecosystem much harder to direct, to orient within, and makes it much harder to trust that resources intended for a given purpose will not be redirected by the various internal forces that grow against the intentions of the system.</li><li><strong>The alignment field that we're supporting seems to me to have pretty little innovation and pretty bad politics. </strong>I am irritated by the extent to which discussion is commonly framed around a Paul/Eliezer dichotomy, even while the primary person taking orders of magnitudes more funding and staff talent (Dario Amodei) has barely explicated <i>his</i> views on the topic and appears (from a distance) to have disastrously optimistic views about how easy alignment will be and how important it is to stay competitive with state of the art models. [Added to LW post: I also generally dislike the dynamics of fake-expertise and fake-knowledge I sometimes see around the EA/x-risk/alignment places.&nbsp;<ul><li>I recall at EAG in Oxford a year or two ago, people were encouraged to \"list their areas of expertise\" on their profile, and one person who works in this ecosystem listed (amongst many things) \"Biorisk\" even though I knew the person had only been part of this ecosystem for &lt;1 year and their background was in a different field.</li><li>It also seems to me like people who show any intelligent thought or get any respect in the alignment field quickly get elevated to \"great researchers that new people should learn from\" even though I think that there's less than a dozen people who've produced really great work, and mostly people should think pretty independently about this stuff.</li><li>I similarly feel pretty worried by how (quite earnest) EAs describe people or projects as \"high impact\" when I'm pretty sure that if they reflected on their beliefs, they honestly wouldn't know the sign of the person or project they were talking about, or estimate it as close-to-zero.]</li></ul></li></ul><p><strong>How does this relate to the</strong> <strong>office?</strong></p><p>A lot of the boundary around who is invited to the offices has been determined by:</p><ol><li>People whose x-risk reduction work the Lightcone team respects or is actively excited about</li><li>People and organizations in good standing in the EA/longtermist ecosystem (e.g. whose research is widely read, who has major funding from OpenPhil/FTX, who have organizations that have caused a lot to happen, etc) and the people working and affiliated with them</li><li>Not-people who we think would (sadly) be very repellent to many people to work in the space (e.g. lacking basic social skills, or who many people find scary for some reason) or who we think have violated important norms (e.g. lying, sexual assault, etc).</li></ol><p>The 2nd element has really dominated a lot of my choices here in the last 12 months, and (as I wrote above) this is a boundary that is increasingly filled with people who I don't believe are here because they care about ethics, who I am not aware have done any great work, who I am not aware of having strong or reflective epistemologies. Even while massive amounts of resources are being poured into the EA/longtermist ecosystem, I'd like to have a far more discerning boundary around the resources I create.</p></blockquote><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne5ob1hhsc8n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe5ob1hhsc8n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The office rent cost about 1.5x what it needed to be. We started in a WeWork because we were prototyping whether people even wanted an office, and wanted to get started quickly (the office was up and running in 3 weeks instead of going through the slower process of signing a 12-24 month lease). Then we were in a state for about a year of figuring out where to move to long-term, often wanting to preserve the flexibility of being able to move out within 2 months.</p></div></li></ol>", "user": {"username": "Habryka"}}, {"_id": "P2AKkH8nsK8xBJ9se", "title": "2023 Open Philanthropy AI Worldviews Contest: Odds of Artificial General Intelligence by 2043", "postedAt": "2023-03-14T20:32:39.516Z", "htmlBody": "<p>Submission for the <a href=\"https://forum.effectivealtruism.org/posts/NZz3Das7jFdCBN9zH/announcing-the-open-philanthropy-ai-worldviews-contest\">Open Philanthropy AI Worldviews Contest.</a><br>Downloadable PDF <a href=\"https://github.com/srhoades10/aiworldviews/raw/main/agi_worldview_prize_sdr.pdf\">here</a></p><p><br><strong>Summary</strong><br><br>The arrival of artificial general intelligence (AGI) could transform society, commerce, and nations more than the technological revolutions of the past. Postulating when AGI arrives, what a world with AGI may look like, and the progress of AI <i>en route</i> to AGI is worthwhile. This Work adopts a broad-scoped view of current developments and opportunities in AI, and places it amidst societal and economic forces present today and expected in the near future.<i>&nbsp;</i>As the appearance and form of AGI is hard to predict, the basis to form the odds of AGI within 20 years is instead derived from its impact and outcomes. The qualifying outcomes of AGI for this Work include conducting most tasks cost-competitively to livable wages in developed nations, performing innovative and complex high-skill work such as scientific research, yielding a durable 6% Growth World Product growth rate, or inducing massive shifts in labor distribution on par with the Agricultural or Industrial revolutions. A survey is first taken on prior works evaluating recent technological developments and posing remaining capabilities necessary to achieve AGI. From these works comes a baseline odd of 24.8% for AGI by 2043, which is then balanced against arguments for or against these timelines based on what AI can do today, a representative array of tasks that AI cannot do but may qualify AGI if successful, as well as observations of phenomena not considered in the more technical prior works, namely those of labor, incentives, and state actors. Likely and impactful tailwinds to AGI timelines are developing new paradigms of AI, on par with reinforcement learning, capable of wholly distinct tasks from those done by AI today. Less likely but impactful tailwinds include the ability for AIs to physically manipulate a diversity of objects, and development of numerous new \u201cnarrow\u201d AIs to collectively perform a diversity of tasks. Likely and impactful headwinds to AGI timelines are the continuation of outsourcing to abundant excess labor globally and the long economic growth trajectories of developing nations. Less likely but impactful headwinds include large economic recessions, globalized secular stagnation, and insufficient incentive to automate fading yet ubiquitous technologies and services. Applying subjective and weighted probabilities across a myriad of scenarios updates the baseline 24.8% odds to propose a low, median, and high odds of 6.98%, 13.86%, and 20.67% for AGI by 2043.</p><p>&nbsp;</p><p><strong>I. Definitions, technical states &amp; Base Rates</strong></p><p>This section defines Artificial General Intelligence (AGI) and reasons the Base Rate of 24.8% for AGI development by 2043 (Pr(AGI<sub>2043</sub>)). The remainder of this Work will then provide cases for an adjustment to this Base Rate. Readers not interested in definitions or rationales may skip to Section II.</p><p><strong>I.A Defining AGI</strong></p><p>The definition of AGI&nbsp;for this Work considers <u>capabilities</u><i>&nbsp;</i>of and <u>outcomes</u><i>&nbsp;</i>after development of artificial intelligence (AI) technologies. These <u>capabilities</u> include one or more computer programs to perform almost any human task competitive with livable wages in developed countries.<i>&nbsp;</i>This Work considers AGI as either a singular, task-achieving AI system which also achieves \u201cintelligence\u201d, <i>or</i> a suite of \u201cnarrow\u201d AIs (nAIs). Examples of nAIs today include image recognition or language models to produce human-like text. Additional <u>capabilities</u> required for AGI in this Work include AI systems operating businesses or performing human-created endeavors such as scientific research and development. While a collection of multiple nAIs themselves would not necessarily constitute a \u201cgeneral\u201d or \u201cintelligent\u201d AI system, nAIs <u>capable</u> of complex tasks with agency, strategy, or decision-making in a human-centric world of enterprise and governance entail a similar degree of technological advancement as the more abstract notion of AGI.</p><p>Additional <u>outcome</u>-based definitions of AGI include both profound economic transformations and massive job displacements. This Work sets a bar close to the Industrial Revolution, which bent linear Gross World Product (GWP) growth to exponential. An increase in the annual GWP growth rate from 4%,&nbsp;<a href=\"http://holtz.org/Library/Social%20Science/Economics/Estimating%20World%20GDP%20by%20DeLong/Estimating%20World%20GDP.htm\">the average rate since 1900</a>, to a persistent ~6% qualifies as an AGI <u>outcome</u> in this Work. Persistence is defined as a durable rate outside of depressions and temporary hypergrowth periods as observed around the Great Depression and Recession.</p><p><i><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/yhq4fv6vyxlpqtq7nrve\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/tmor5a8z75u29bhm9tul 90w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/ytbs6zrc76htglapstem 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/z5bxeedlq3oqepliumhj 270w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/ejzfwglzahw0uek8j34b 360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/aqigd5vh2qc3jj3kfe0g 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/tgh8pwujx3ynrgyjny7t 540w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/tu9ya7q6rnvmnbr7xa0n 630w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/tt7afykurwrrstqw097l 720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/xim0xnurr97r23ucyuig 810w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/ccypc3qhkcsjgi9l4nx1 852w\">Relative GWP growth comparison of pre- and post-Industrial Revolution (0.5% and 4%, left) and required GWP growth to quality AGI (6%, right) over an arbitrary 120-year timeline.</i></p><p>A 50% increase in GWP growth within 20 years is substantial yet comparable to the quadrupling from 1800 to 1900. &nbsp;This seemingly extreme growth rate is not infeasible even today, as the growth rate neared 6% multiple times in the past 75 years. Labor <u>outcomes</u> of AGI are matched to the shifts before and after the Industrial Revolution, when agriculture <i>decreased</i> from greater than 50% of the labor force in leading nations around 1750 to less than 5% by 2000. The percentage of labor in services rapidly <i>increased</i> to over 50% in developed countries since the Industrial Revolution but occurred after the partial displacement of agriculture first by manufacturing. Thus in this Work, only 40% or greater <i>decreases&nbsp;</i>labor in any large sector(s), including services, qualifies an AGI <u>outcome</u>.&nbsp;</p><p>GWP may not be able to capture the impact of AGI. Other outcomes, such as a drastic curtailment of humanity or human extinction due to AGI could qualify. This Work does not factor these outcomes, as the baseline estimates of these events occurring by 2043 due to artificial systems are negligible compared to the probabilities of anthropogenic wars or drastically reduced living conditions on this planet. However, this Work also then considers an <u>outcome</u> of AGI to be a durable 0% GWP or less. This Work limits outcomes to economic indicators, as fewer consensus and accessible quantitative measures exist for culture, quality of life, or geopolitics, though AGI would surely impact these domains as well.</p><p><strong>I.B Review of prior work</strong></p><p>Prior works on this topic deeply cover AI development to date, technical and financial requirements to develop AGI, and how prior eras of technological progress can predict AI timelines. This section amalgamates these works to help form a Base Rate for Pr(AGI<sub>2043</sub>).</p><p><a href=\"https://arxiv.org/pdf/2206.13353.pdf\">Carlsmith</a> broadly writes about existential risks of AGI to humanity, but includes assessments of algorithmic, compute, and financial feasibilities to develop AGI and describes capabilities of an intelligent artificial system. The probability of technical feasibility and an environment with incentives to build such a system by 2070 is 65% and 80%, respectively. Capabilities of AGI include \u201cadvanced capabilities\u201d: conduct scientific research or engage in political strategy, \u201cagency\u201d: create and execute tasks, and \u201cstrategic awareness\u201d: formulate plans and update them amidst real-world environments or test what-if scenarios. This work introduces concepts of intelligence as emergent properties of nAIs, demonstrations of awareness in AI systems today, and the response of society to \u201cwarning shots\u201d of near-AGI systems around notions of safety.</p><p><a href=\"https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">Cotra</a> focuses narrowly on when a transformational AI (TAI) system could be built, factoring advances in computing power and costs, the data and computing requirements of AI today, and \u201ccompute\u201d estimates in human intelligence. TAI is defined as one or more AIs that can perform most jobs cheaper than humans, universally recognize and classify objects, create narratives, teach itself new skills, perform economically valuable work, and engage in scientific research. The probability of TAI by 2030 and 2040 is 15% and 50% respectively. This work introduces concepts of disparities in data requirements between artificial systems and humans, algorithmic efficiency, bottlenecks of TAI by compute power and costs, and achieving TAI through generalizing existing AI algorithms.</p><p><a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/\">Karnofsky</a> assesses AI\u2019s capabilities today and projects the advancements towards TAI, defined similarly to Cotra in its capabilities and impacts to labor. The probability of TAI by 2036 is 10%. TAI\u2019s capabilities include conducting scientific research and human-level performance in both physical and intellectual tasks competitive with at least 50% of total market wages. An added qualifying outcome of TAI is producing economic growth at the scale of the Agricultural or Industrial Revolutions. This work introduces concepts of TAI through generalizing existing AI algorithms and de-anthropocentric AI, or TAI that does not emulate human brains and cognition.</p><p><a href=\"https://www.openphilanthropy.org/research/semi-informative-priors-over-ai-timelines\">Davidson</a> models probabilities of AGI development with little attention to technical AI progress to date, but rather by AGI\u2019s estimated difficulty, research effort, computing advancements, success and failure rates, and spending on AGI development. The probability range of AGI by 2036 is 1-18%, with a median at 8%. Davidson provides a median adjustment down to 5.7% percent, conditional on AGI being impossible by 2023. AGI\u2019s capabilities are heavily weighed on cost-competitive accomplishment human-level performance on cognitive tasks. Davidson\u2019s model of AGI development is most influenced by the estimated difficulty of the problem, and secondarily on the growth of research(ers), spend, and compute. This work introduces concepts of expectations based on progress in other fields, AGI\u2019s as a product of sequential advancements, and diminishing returns on research and investment observed in other scientific industries.&nbsp;</p><p><strong>I.C Arriving at a Base Rate of Pr(AGI<sub>2043</sub>) for this Work</strong></p><p>The Base Rate of Pr(AGI<sub>2043</sub>) is a weighted sum of the referenced works above, derived from a thematic overlap of concepts and relative strengths and weaknesses of their stances.&nbsp;</p><p>The salient probabilities from Carlsmith for this Work are technical feasibility and incentives to create AGI by 2070, which are 65%*80%= 52%. Carlsmith is unique among the referenced works to consider incentives, deployment to real-world applications, and societal response to AI development, all of which decrease odds of AGI. These concepts are viewed favorably here, and while anecdotes of AI\u2019s accomplishments and their expected extension towards systems with agency, strategy, and planning are viewed skeptically, the technical assessments are viewed favorably too. The odds of AGI are only defined for 2070, and it is not stated how the odds change over time. Therefore, the Pr(AGI<sub>2043</sub>), given the time of authorship in 2022, is (2043-2022)/(2070-2022)*52%=21.9%.</p><p>The Pr(AGI<sub>2043</sub>) from Cotra is (2043-2022)/(2040-2022)*50%=58.3%. The technical assessments in this work are viewed favorably, however the expectation TAI will emerge from continual advancement of existing AIs today, and as a byproduct of increased compute power at decreased cost, is too large an assumption and narrow a view to be given high weight in the updated Base Rates. Karnofsky shares expectations on TAI from improvement on existing AIs, but raises the bar to compare TAI\u2019s outcomes to prior technological revolutions. This raised expectation may partly explain the decreased odds of TAI by 2036. The Pr(AGI<sub>2043</sub>) from Karnofsky is (2043-2016)/(2036-2016)*10%=13.5%. Both these works will be given equal weight, as they differ mainly on the qualifications of TAI.</p><p>Davidson offers less technical but important perspectives to the broader consideration of AGI development. These perspectives are viewed here with mixed results. The concepts of sequential models, research(er) effort, technological progress, and non-linear changes in probabilities over time are useful. Nonetheless, the model used in this work is strongly influenced by a single variable, namely the prior expectation of AGI. Varying the weights of other variables does not appreciably impact the overall probability, and thus the utility of this model is viewed skeptically. Additionally, the na\u00efve model only differs from the informed model by one percent. The Pr(AGI<sub>2043</sub>) from Davidson is (2043-2020)/(2036-2020)*5.7%=8.2%.</p><p>These odds are given subjective weights of 0.4, 0.2, 0.2, and 0.2 from Carlsmith, Cotra, Karnofsky, and Davidson respectively, yielding the estimates:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (21.9% * 0.4) + (58.3% *0.2) + (13.5% * 0.2) + (8.2% * 0.2) = <strong>24.8%</strong></p><p>&nbsp;</p><p><strong>II. Technical assessment and critique on AI today</strong></p><p>In this section and throughout this Work, I will ground arguments and opportunities in AI development across mundane and micro tasks, such as cooking and haircuts, up to macro and abstract sectors of healthcare, automobiles, construction, and business enterprise. Examples in these domains will highlight what AI can and cannot do today and opportunities for new AI developments, which will then be translated to subjective odds of developing such capabilities and integrated to Pr(AGI<sub>2043</sub>).</p><p><strong>II.A Defining Tasks and limits of data, generalization, and the unobservable</strong></p><p>Most AI accomplishments to date are categorically modeling or learning<a href=\"#_ftn1\">[1]</a>. Notable modeling tasks include processing and classifying images and videos, reading and rendering text, and listening and responding to auditory signals. Learning-based tasks include playing real or virtual games. In all these tasks, AI operates in data-rich domains, either of the senses or in simulated environments with effectively infinite artificial data and outcomes of a sequence of actions. Cotra notes these AIs require more than 2 orders of magnitude of data than humans to learn tasks.The requirement for either a super-abundance of data or high outcome availability are one limitation to nAIs generalizing to other tasks. In addition, tasks operating within rules, such as playing chess, depend on readily available and simple rules, which do not apply to many real-world tasks.</p><p>I propose a general formula for an AI to accomplish most human tasks:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i> Task = function(Rules, Domain Knowledge, Physical Manipulation, Data, Outcomes)</i></p><p>For a given task, the relative weights of these factors vary considerably, and tasks do not require all factors. In addition, many tasks can be accomplished in multiple ways, for example through highly curated rules and domain knowledge or by brute-force with abundant data. Tasks involving the senses, for example image classification, perform almost purely through data, with a non-zero influence of outcomes (i.e. ground-truth labels of the image), and no requirement for physical manipulation. Learning-based tasks, such as playing chess, could either require explicit knowledge of rules or implicit knowledge from the unobserved actions within a preponderance of data. Regardless, all learning tasks require outcomes. While rules can be safely ignored in the case of text generation, where a corpus of text is sufficient to build a model of \u201creal\u201d words, their relative orderings, and grammar, the unobservable information in other domains may be because actions either violate rules (examples being laws of legal systems or of physics) or resulted in failure. The unobservable failures or rule violations, even in data-rich domains, implies that for AIs to accomplish many unsolved real-world tasks, AIs must contain explicitly defined rules, \u201clearn\u201d fundamental rules, e.g. laws of physics, or codify domain knowledge in anthropogenic endeavors. &nbsp;</p><p><strong>II.B Path dependency, extensibility, and progress</strong></p><p>At their core, whether as statistical models or agent/environment-based learners, most performant AIs today are variations of neural networks, with their architectures differing in complexity and assembly of the network\u2019s components. The path to neural networks\u2019 success today originated in 1960s, with the most notable achievements in the past 20 years. These achievements are largely improvements in hardware and algorithmic efficiency, and less from fundamental changes of the algorithms themselves. The path dependence of neural networks to AI today evokes a broader question of whether they represent the global maxima of algorithms, or is there a yet-undiscovered category of AI that better incorporates rules and domain knowledge, or (limited) data, and (unobservable) outcomes? Aviation, computation, and bioengineering are prime examples where successive technologies required transformative innovations to climb out of their local maxima. Modern aviation\u2019s journey began with fixed-wing gliders and rotorcrafts, followed by jet engines, piston engines, and gas turbines. In computer programming, the term \u201cextensibility\u201d is used to characterize the ability of code or software to solve problems beyond its initial intended use. Piston engines could not extend to the demands of high-performance aircraft or long-distance air travel made possible by gas turbines. This notion is readily apparent in biology as well, where <i>E coli</i> are engineered to produce many chemicals it does not natively produce, but are not sufficiently extensible to produce all chemicals used in the synthetic biology and biopharmaceutical industries. While the limits of <i>E coli&nbsp;</i>are not yet reached, other organisms are increasingly under study to solve broader challenges in these industries. Likewise, AI has not yet hit the limits of neural networks and the modeling and learning algorithms they power. But whether as new innovations, seen with engines, or stacking complementary technologies, seen with semiconductors and fabrication, the history of industry and progress would predict that additional nAIs, or more generalizable AI, will likely require magnitudes of change to extend to the breadth of tasks required for AGI.</p><p>An important distinction between aviation, computing, and AI are the available measures of advancement. For aviation, the weight to power ratio is a primary indicator of progress, and for semiconductors, the number of transistors. In deep learning, the number of model parameters is an analogous metric, where more is generally better, though this measure does not apply as neatly to learning AIs. AI models may be evaluated by their accuracy or performance relative to humans, which to date are only established for narrow tasks such as image classification. While broad consensus performance metrics of a technology or industry are not mandatory for progress, the stubbornness of this challenge for the field of AI may produce drag on its advancement to AGI.</p><p><strong>II.C AI in select examples</strong></p><p>Cooking AI is achievable through physical manipulation of objects and either codified knowledge, including agreeable mixtures of flavors, cooking durations, and fundamental rules of chemistry, or by data-intensive associations to outcomes. Outcomes today are largely subjective tastes (the scientific domain of organoleptics is still in its infancy), but despite the frequency that humans eat and the size of the food industry, outcomes data is scarce. However, a large corpus of recipes and their ratings do exist and can act as a proxy for input data and their outcomes. This form of data-outcome modeling is highly compatible with existing AIs today, and much more likely to accomplish Cooking AI than through domain knowledge and rules. <i>Thus, Cooking AI is highly achievable in the digital domain of its subtasks, and in light of progress on robotic food preparation systems today, moderately achievable in the physical manipulation of objects.</i></p><p>Haircutting AI is achievable either by codified domain knowledge and rules, including physics and actions that inflict human harm, or by data-intensive associations to outcomes. Codifying domain knowledge may prove difficult, particularly for translating human intuitions on how to manipulate hair of varying densities, lengths, and moisture content into a quantitative domain, and then to combine those measures with physical forces and movements of razors and combs. For the data-outcome approach, outcomes are simply images of haircuts, although data, if represented as a sequence of actions by physical objects in three-dimensional space, is non-existent. If this problem is cast as a learning-based task, a simulation environment may be possible to generate infinite sequences of haircutting actions and a final (simulated) cut-hair end state. Ultimately, this simulation needs to cross into the physical domain, possibly on a model of a human head with robotic arms and devices moving in physical space. <i>I view Haircutting AI to be moderately unlikely in the digital domain of its subtasks, given little existing work and data on this type of task, and limited cases of modeling or learning AIs in the physical domain. In addition, the physical manipulation of objects is both essential and more difficult than in Cooking AI.</i></p><p>Unlike the directed actions-to-outcome nature of cooking and cutting hair, many tasks require the ability to diagnose a system of parts, understand functions of individual components, and combine those components to form an integrated unit. Composability, which may represent a new category of AI, would greatly increase likelihood of Auto Repair AI. In this context, composability can be thought of as an extension of domain knowledge. Auto Repair AI may be achievable through a data-outcomes approach, although like the service-oriented food and personal care sectors, automotive repair is a fragmented industry with low outcome availability. If a learning AI were applied to this problem, a simulation environment may be created akin to Haircutting AI, however a physical environment to diagnose component performance, assemble parts, and test outcomes would require much more complex systems. <i>I view Auto Repair AI to be moderately unlikely in the digital domain of its subtasks, given a new category of AI may be required and limited cases of learning AIs in the physical domain. The physical manipulation of objects is both essential and at least as challenging as Haircutting AI.&nbsp;</i></p><p>Construction contains many analogous tasks to automotive repair. Also analogous to automotive repair is the fragmented nature of the construction industry and low outcome availability. The rules and domain knowledge embedded in designing, planning, preparing, and assembly are vast, with many variables and dimensions of possible outcomes. For instance, cars have finite parts with limited substitutability, while building configurations are primarily limited by physics and feasibility against the space onto which the building is built, which itself contains variables of terrain, soil, and climate, among others. Given the many subtasks and expertise required of construction, Construction AI would likely be solved with multiple nAIs. <i>Among the major tasks, design is reasonably feasible through a data-driven approach, as there exist many blueprints and consensus preferences on building configurations. I view planning to be somewhat unlikely, given the extensive rules and domain knowledge required in building, permitting, approvals, codes, and regulations manifest by many roles in this industry. Analogous to Auto Repair AI, a new category of AI may be required for building tasks in construction, and physical manipulation of objects is extremely challenging, as has been noted even for mundane related tasks such as&nbsp;</i><a href=\"https://constructionphysics.substack.com/p/where-are-the-robotic-bricklayers\"><i>laying bricks</i></a><i>.</i></p><p>Healthcare AI requires either extensive domain knowledge, including known conditions and their treatments, coupled to rules, such as actions harmful to humans, or data-intensive associations to outcomes. There has been considerably more effort to codify domain knowledge in healthcare than services, automotive, and construction, in both clinical research and private industry. While successes exist, including clinical decision support tools, the effort required is enormous relative to the percentage of diagnostics and decisions made in human care these tools impact. Composability, considering anatomical and physiological components of the body and collective human function, would benefit diagnostic tasks in Healthcare AI, but diagnostics is also possible through data-driven modeling. Much more effort, spanning decades, has also been applied to this approach than other industries. The results of these efforts are mixed. Diagnostic AIs today perform strongly to diagnose conditions mappable from sensory information, such as images. &nbsp;Furthermore, the strong incentives to collect data in health systems of developed countries yield copious data on human behaviors, diagnoses, and clinical decisions. Nonetheless, with all this information, \u201cAI doctors\u201d are slow to be developed and deployed. While there are many reasons for the slow rise of Healthcare AI, I argue a large technical reason is that what is missing from health data is as important as what exists. In addition, outcome availability is limited, fragmented, and biased towards outcomes of ineffective diagnoses and treatments, i.e., an inverse survivorship bias.<i>&nbsp;</i>Putting aside physical manipulations, considering the work in Healthcare AI to date, <i>I view generalized Healthcare AI to be somewhat unlikely. Nonetheless, a suite of nAIs operating purely in the digital domain of tasks may still have a large impact on the industry. Most healthcare events are for routine symptoms with predictable solutions and outcomes, and more than 20% of healthcare costs in the United States are related to diabetes care (or ~1% of GDP). In addition, healthcare involves many administrative functions, and increasing automation of these tasks is highly feasible. Thus, I view Healthcare AI that meets the requirements of AGI for this Work to be moderately likely.</i></p><p>Contracts are essential in most industries and transactions. Although contracts operate within known rules, including those from legal systems, an industry-agnostic Contracts AI would be very difficult to approach through domain knowledge and rules. Rather, large language models today may already be capable of generating routine contract templates. Outcomes are less critical here, and as with Healthcare AI qualified by only a subset of diagnosis and treatment capabilities, Contracts AI may not need to generate all types of contracts or write complete documents to impact the industry broadly. Data is not as readily available as other bodies of texts such as cooking recipes, given the less public nature of the legal profession. However, the efficiency of language models, partly measured by the amount of data required to produce text, is continually decreasing. <i>Contracts AI is thus highly likely.&nbsp;</i></p><p>Company formation and operations, which may be for purely digital services or products, requires acting with agency within an environment. An agent-environment example could be simply receiving feedback on a product and adjusting the product features or price. Other tasks frequently involved in operating companies include negotiation, bargaining, and strategic planning. Many of these tasks are already possible with language models that can converse with humans. These AIs are predominantly confined to 1-on-1 interactions today, which is not the case in many business interactions operating under multiple agent-principal environments. <i>Even so</i>, <i>expanding existing AI tools to engage in multi-actor interactions is feasible</i>, <i>and thus</i> <i>negotiating and bargaining subtasks for Company AI is likely.&nbsp;</i>Other tasks requiring agency or adjustment of actions against an external environment are analogous to how learning AIs play games today. These AIs \u201cunroll\u201d a series of actions and adjust their behaviors based on the expected outcome, confirmed by the actual outcome of the game. Transferring this approach outside a game would require mapping actions of games to real-world actions and quantizing outcomes. <i>I view a learning AI approach to acting with agency in business operations to be very unlikely, given most observable outcomes cannot be readily attributable to specific actions, and most actions and outcomes are not observable.&nbsp;</i>\u201cFailures\u201d in a game are low stakes, whereas \u201cfailure\u201d in this case is death of the company. Death of the individual can benefit the collective, if outcomes and causal links of actions (or inactions) are defined and disseminated, none of which happens today, nor viewed as remotely likely to occur soon. <i>Company AI may thus require a new category of AI which can operate with highly incomplete information or accomplished by a large-scale reengineering and mapping of actions and outcomes from games onto the human domain, both of which I view to be very unlikely.</i></p><p>&nbsp;</p><p><strong>III. Economies, state actors, and labor</strong></p><p>Technological development does not occur in a vacuum of fields, factories, or server rooms. Arguments on AGI timelines in this Section are grounded by selected observations in labor markets, economic trends, and recent private sector activities. These observations are then projected onto conceptual frameworks in labor theory and macroeconomics to propose a series of headwinds and tailwinds for the advancement of AGI and its impact on GWP and Labor as defined in Section I.&nbsp;</p><p>Observations are as follows:</p><ul><li>In 2017, $2.7 out of $5.1 trillion of manufacturing labor globally was estimated to be already automatable [<a href=\"https://www.mckinsey.com/capabilities/operations/our-insights/human-plus-machine-a-new-era-of-automation-in-manufacturing\">ref</a>]</li><li>Both Google and Meta independently developed AI to predict protein structures and play strategy games</li><li>The annual number of electric vehicles (EVs) sold globally will surpass internal combustion engines (ICEs) in 2040, with absolute numbers of EVs and ICEs at roughly 600 million and 1.2 billion respectively [<a href=\"https://about.bnef.com/blog/electric-vehicles-accelerate-54-new-car-sales-2040/\">ref</a>]</li><li>The number of low-tech jobs created more than double high-tech labor in geographic areas of new growth [<a href=\"https://www.aeaweb.org/articles?id=10.1257/aer.100.2.373\">ref</a>,&nbsp;<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2671765\">ref</a>]</li></ul><p><strong>III.A Private sector activity, incentives, and industrial turnover</strong></p><p>The massive value in private enterprise, and likely also in public functions, that could be automated today but are not, implies the development and deployment of AI systems are not solely technological factors. Among many possible reasons, I argue investment, narrow competition, momentum of existing business practices, and incentives are important ones.</p><p>Most successful deployed nAIs were developed by technology companies with large research divisions or smaller foundations and non-profits with support from large technology companies. The applications of highly developed language and image recognition models are most evident in product improvement, such as content recommendation, search engines, and driver-independent vehicle control. Other projects, such as playing the game of Go and predicting protein structures, are R&amp;D projects with large but uncertain payoffs, and required particular decisions made with excess capital in favorable macroeconomic environments. Examples of costs and timelines of these projects shed light on the requirements to advance nAIs today. AlphaGo, an AI that plays the game of Go, had an&nbsp;<a href=\"https://www.yuzeh.com/data/agz-cost.html\">estimated</a> computing cost of $35 million and development time of more than two years. AlphaFold, an AI to predict protein structures, has multiple version releases since 2018 (as an aside, this timeline is short compared to therapeutic discovery and development as a whole, and protein structure is but one of many non-obligatory steps to discovering and developing drugs). DeepMind, the company who developed these AIs,&nbsp;<a href=\"https://find-and-update.company-information.service.gov.uk/company/07386350/filing-history\">reported</a> expenses of $1.7 billion in 2021. Based on code release histories, Meta\u2019s protein structure project took at least three years, and their recent corollary to AlphaGo, CICERO, is built on the shoulders of large language models and learning AIs developed over the past few years. In the coming years, a real possibility of less favorable macroeconomic conditions may squeeze budgets and researcher efforts in these projects. Even less likely than sustained effort on these R&amp;D projects would then be the development of new categories of AIs that may be required to accomplish tasks described in Section II. Few companies today possess both large R&amp;D divisions and a willingness to invest their excess returns to fundamental AI development. Among these companies, a de-emphasis on ambitious AI R&amp;D may already be underway, as noted by Meta\u2019s intention to focus their AI efforts to improve product recommendations and increase advertising conversion rates. Even so, as computing costs decrease and access to nAIs increase, the investment and time to onboard and advance AI becomes increasingly favorable to smaller companies or even individuals. This tailwind to AGI timelines offsets the historical dependency on large private technology companies to develop innovative AI.&nbsp;</p><p>The incentives to invest and innovate in industry follow cycles of opportunity, growth, maturity, and decay. Displacement often follows decay, but the decay of technologies and their supporting service tasks occurs slowly. Just as hardware stores keep esoteric spare parts of decades-old appliances, so too may automobile mechanics long maintain inventory and repair services for the parts and ICE vehicles subject to displacement by EVs. Servicing 1.2 billion ICEs could be automated, but with a closing window of opportunity, is there sufficient incentive to automate these tasks? With simpler designs than ICEs and much greater investment, 2043 is a reasonable timeline for highly automated maintenance and repair of EVs. However, the incentive to automate other analogous tasks and industries lag EVs, and the vector of change in other industries is less obvious than for the shift from ICEs to EVs. Beyond its development, the deployment of AI will be tethered to industry-specific change, and the natural lifecycles of industry may create a strong headwind to AGI timelines.</p><p>Two major phenomena in modern globalized economies are the relocation of work to areas of cheaper labor and the development of comparative advantages by regions or nations to produce certain goods or services. In recent decades, these phenomena often manifest by establishment of new supply chains, training of labor, and scale-up of the nascent industry in a developing nation. These steps take at least years, and sometimes entire generations, as it operates under constraints of both individuals, who have at most a few careers in their lifetime, and industrial policy, typically only as dynamic as the rate of political or regime change. Singapore\u2019s transformation began 1960\u2019s, grounding its economy in manufacturing, then achieving its high standing in finance and technology by the 1990s. Other nations, including Vietnam, are in earlier stages of this progression now, as it expands beyond food and textiles production and into electronics. Many nations are even earlier in this development cycle, including African countries still with a majority agricultural economy. The relocation of tasks to cheaper labor in developing nations is far from exhausted. Without a significant perturbation to the typical course of task relocation and ensuing industrial development in developing nations, these economic phenomena may pose a large headwind to AGI deployment for decades. Even many tasks today which cannot be outsourced, including in developed nations with expensive labor, still are not automated. For example, the United States is mired in a long lag from building advanced robotic and intelligent systems to implementing fast food preparation nAIs. One may argue globalization and outsourcing has peaked, which may force domestic industries to accelerate adoption or development of innovative technologies. These behaviors would break with the otherwise large momentum of human-oriented work spreading across a globalized economy.</p><p><strong>III.B Labor\u2019s force, state intervention, and utility</strong></p><p>Loss of purpose, means to support oneself or one\u2019s family, or economic power to buy and sell goods or services, often has negative impacts, from individuals up to nations. These concerns garner increasing interest from governments to support labor markets, and laborers themselves perceive a growing threat of AGI to job security. I argue the response by labor, protections of labor, and behavioral changes from altered pricing of goods, services, and wage expectations will retard AGI development and deployment.</p><p>There is an idea that the advent of new technologies will leave nothing left for humans to do. This idea is repeatedly false historically, exemplified by the flow of labor down agriculture, industrial, and service sectors. That this notion is false to date does not mean it will be false with the advent AGI, nor that AGI cannot occur even with new tasks for humans. The time to develop new AIs for these new tasks will likely hasten, and future tasks will also likely be co-developed by AIs or with AI embedded into the design. Recently in wealthier societies, however, the proliferation of businesses such as barber shops and tattoo parlors also bring services to those services, for example dedicated furniture suppliers to each of those businesses. So long as humans can create niche service jobs out of existing ones, the development of and competition by AI to these tasks will lag, in part due to lack of incentives and the fragmented nature of \u201ccottage\u201d industries. Another headwind to AGI is derived from the observed outward expansions of pockets of skilled professions, such as lawyers specializing in cybersecurity, blockchain, autonomous vehicle liabilities, and AI property rights. These efforts may ultimately be lost to AI, but as with AI timelines tethered to the industry in which it operates, one may also predict natural cycles of education, training, and degree specialties to factor into AI timelines. A tailwind to AI upending intellectual professions could be new skilled labor with both domain knowledge of the industry and requisite knowledge to develop and deploy new AI, which is only just underway in some high-skill, non-service sectors.</p><p>In addition to the expansion of services, another potential response by labor is if automation were to ripple across segments of the labor market, labor would spill into other sectors and lead to increased labor competition. This outcome is plausible with increased automation of manufacturing jobs, possibly evidenced by increased structural unemployment despite robust growth of service jobs. Most new work humans create will likely then be more services or intellectual jobs, though the educational demands of intellectual work will be out of reach for many. As a result, there may be a race to the bottom in other jobs, producing downward wage pressures and sustained competition with AI, slowing its widespread deployment. Counterintuitively and conversely, a downward wage pressure may increase investment in automation and AI, which is otherwise difficult when labor is tight and wages high. Reduced net profits and falling investment would only follow an exhaustion of excess labor, reaching something like a Lewis Turning Point (in this context it does not necessarily mean a full absorption of agricultural labor by manufacturing, but applied to industries agnostically). The possible tailwind here would be a positive reinforcement loop where AI creates \u201cexcess labor\u201d and thus frees investment and capital to accelerate AI development.</p><p>Economic growth alters expectations of work, wages, and standards of living. These new expectations are highly sticky, and turbulence to them, at least since the Industrial Revolution, correlates with unrest and political change. Increasingly since the Great Depression and the New Deal, the threat of economic downturns, pandemics, and job offshoring to social stability prompts governments and central banks in developed nations to inject labor safeguards and interventionist policies. Most nations where AI would be realistically developed and deployed in the near future, including the Americas, Europe, and Asia, largely subscribe to some tenants of Austrian economic thinking, which espouses the value of stable money and freedoms to transact as basic rights of individuals. Even the threat of AGI to labor may prompt action, possibly grounded in a novel moral equity problem, where rather than something being too expensive for the poor (e.g. healthcare in the United States), there will be an erosion of individuals\u2019 livelihoods and economic freedoms. There are a few actions governments may take in such a scenario. They may expand their role to support the un- or under-employed with opportunities for re-skilling, increased welfare, or expanded public works. These strategies may impact AGI timelines, as public spending may crowd out private sector investments in new technologies. Also likely and impactful to AGI timelines are taxations on AI-enabled profits, restrictions on sales of goods or services powered by AI, or a push for AI as a public good. Designation of public goods and taxation often reduces incentives to innovative, so while this scenario would not preclude AGI development, it may prolong its timeline. Governments have intentionally blocked development of new technologies throughout history, but many cannot or will not today, especially with ever more accessible knowledge and usage of AI. Governments may instead attempt to coerce developers and owners of AI to open their technologies to wider use or competition, or take legal actions such as anti-trust, if owners of AI were to develop monopolies in their industries. However, without clear information on societal impact and property rights, these actions are unlikely to succeed. Given the potential reach of AI\u2019s impact to labor and economies, a multi-state, global effort may be required, which is also unlikely to happen quickly. While failure would not accelerate AGI, it would dampen government-induced headwinds on AGI timelines. There could be a long delay to action while nAIs continue to be developed, altering expectations of the futures of workers and consumers.</p><p>Three observations and predictions relating to consumption behaviors collectively pose a headwind to AGI development. First, the permanent income hypothesis states consumption patterns emerge from future expectations of income and economic changes, and changes in human capital, property, and assets influence these expectations. One could envision altered consumption behaviors due to job displacement or merely the threat of displacement. A downward shift in consumption expectations would have a large negative impact on the economy and delay AGI. Second, price and wage stickiness could produce an inertia of consumption by those not yet impacted by AGI may continue to support higher prices. The case for increased marginal utility may strengthen as humans increasingly consume goods and services tied to branding and personal values. In this scenario, humans may sustain demand for goods or services produced by humans, even if AI-produced alternatives are cheaper. While this behavior would not sustain current levels of employment, it may prolong a qualifying event of AGI\u2019s impact to labor. Third, without significant changes to current demands for AI, development may be continually pulled towards strengthening existing nAIs. This effect is evident today with a focus on AI as information aggregators, composers of text and images, and software developers. These narrow demands may hinder the necessary development of new AIs outlined in Section II. Of these three points, adjusted consumption behaviors and opportunity costs to develop new nAIs pose the largest headwinds to AGI timelines. An example of the limitations on the marginal utility argument is how language models are nearly capable of writing novels today. Only small populations of consumers, perhaps the intellectual, urban, and wealthy, would consider a purchasing decision against AI-created books. It is unlikely that an antithetical relationship to AI-produced goods and services would compete against the demands of the masses.</p><p>&nbsp;</p><p><strong>IV. Examples and odds updates for Pr(AGI<sub>2043</sub>)</strong></p><p>Scenarios are formed from arguments throughout the text and given a subjective odds of occurrence and relative weight of importance for Pr(AGI<sub>2043</sub>). Scenarios may positively or negatively impact Pr(AGI<sub>2043</sub>). The formula for the final odds:</p><figure class=\"image image_resized\" style=\"width:39.09%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/ip7zvkrrmciab5wopecu\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/aazgvrmtfmiiaowrmxkg 127w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/owmp6lh1apf3k40nd0yw 207w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/kls5fy5w14rttaxdialq 287w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/nomrsdsi7bhcct1dse8x 367w\"></figure><p>A range of overall estimates were formed by randomly sampling an odd and weight for each scenario over a uniform distribution of 0.5-2x the baseline odd and weight, bound by 0%-100% and 0-1 respectively. For instance, the 1<sup>st</sup> scenario\u2019s odds below would be uniformly sampled between 37.5-100%, with weights 0.3-1.0. 100,000 samples were drawn to create a distribution of Pr(AGI<sub>2043</sub>).</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:white;border:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\"><p><strong>Scenario</strong></p></td><td style=\"background-color:white;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p><strong>Sign</strong></p></td><td style=\"background-color:white;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p><strong>Odds (%)</strong></p></td><td style=\"background-color:white;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p><strong>Weight</strong></p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Development and practical use of new paradigms or forms of AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>75</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.6</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Consensus performance metrics developed for AI, either broadly applicable to AGI or for ten developed individual nAIs</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>15</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.1</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Digital domain of Cooking AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>95</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.05</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Physical manipulation of objects for Cooking AI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>58.3</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.4</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Digital domain of Haircutting AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>20</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.3</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Physical manipulation of objects for Haircutting AI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>10</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.1</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Digital domain of Auto Repair AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>32.5</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.35</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Physical manipulation of objects for Auto Repair AI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>15</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.2</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Design for Construction AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>85</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.2</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Planning for Construction AI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>36</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.3</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Physical manipulation of objects for Construction AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>10</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.25</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Digital domain of Healthcare AI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>67</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.3</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Digital domain of Contracts AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>95</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.1</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Forming and operating businesses</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>10.5</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.2</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Periods of large unfavorable macroeconomic conditions or continued secular stagnation over the next 20 years</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>36</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.5</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Diversification of fundamental AI R&amp;D projects</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>80</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.25</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Investment and private sector growth in automating fading technologies and services, including internal combustion engine vehicle maintenance and repair</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>1</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.35</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Continuation of outsourcing to cheaper labor and economic growth trajectories of yet agricultural or low-skill manufacturing regions</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>88</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.7</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>More nAIs are required for AGI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>16.8</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.6</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>High skill white-collar professions continue to expand pockets of tasks untouched by AI or created in response to AI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>70</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.15</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Structural unemployment due to AI crease excess returns to increase investment and capital towards more AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>+</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>30</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.25</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Government interventions of expanded state, welfare, or employment mandates crowds out private sector or weakens state and broad economic conditions with excess debt</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>18</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.25</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Government taxation on AI-enabled profits or implementing restrictions on sales of goods or services powered by AI</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>60</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.2</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>AI as a public good and reducing innovation incentives</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>5</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.05</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Legal action on AI ownership or AI monopolies</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>40</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.1</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Downward consumption expectations drag economy at large</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>20</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.25</p></td></tr><tr><td style=\"background-color:#DEEAF6;border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Sustained consumption of human-derived goods which offsets demand for AI products</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>40</p></td><td style=\"background-color:#DEEAF6;border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.1</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Opportunity cost in strengthening existing nAIs off current demand over developing new nAIs</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>-</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>65</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt\"><p>0.3</p></td></tr></tbody></table></figure><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:85.74%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/aofczhtnkpajixdqh8i4\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/tquauttniolh6peicowb 89w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/tya31g4wry3dkvcg4ulv 169w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/ycecgijnr7p7xrdc6gki 249w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/weriwsap8lmxou2ld1gx 329w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/pp2xdy8umpirir8rqaxh 409w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/x0ha7o6jxn2vp5vnyar8 489w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/m71y9ngjp0ev1hpuxbhn 569w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/qk9lkkvzgd7wek3hb5n6 649w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/uzbdbxrjppt4cxaz7u8g 729w\"></figure><p><i><strong>Low, median, and high Pr(AGI<sub>2043</sub>) <u>within</u> this Work are 5.14%, 10.21%, and 15.23%.</strong></i></p><p>The Base Rate of 24.8% was combined with these low, median, and high estimates to produce the final Pr(AGI<sub>2043</sub>). Weights of 0.25 and 0.75 were chosen for the Base Rate and probabilities from this Work, respectively, as the Base Rate was derived from prior works focused on technical development, and this Work considers some technical but mostly broader non-technical considerations AGI timelines.</p><p><strong><u>Final odds:</u></strong></p><figure class=\"image image_resized\" style=\"width:64.33%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/q3dllhxy1cid8qwhflow\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/npjxome0jckxndl3znql 113w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/ftbnedde1e4voarc30mx 193w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/vs1gjsxqbkfeqjbfge63 273w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/me9eintuhatvoqpqdd7w 353w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/vcssrraeiwhpcqnmusuh 433w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/x58aas02me8jhfmthtnt 513w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/ms9ynby6hfxkb4gcncgr 593w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/P2AKkH8nsK8xBJ9se/yhuy9cz8tja71lmyfpof 673w\"></figure><p>&nbsp;</p><p>Code for figures and analyses in this Work at <a href=\"https://github.com/srhoades10/aiworldviews\">https://github.com/srhoades10/aiworldviews</a><br>&nbsp;</p><p><strong>Appendix</strong></p><p>&nbsp;</p><p><strong>Scenario: </strong>Development and practical use of new paradigms or forms of AI</p><p><strong>Reference section:&nbsp;</strong>II.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:</strong> 75%</p><p><strong>Weight:&nbsp;</strong>0.6</p><p><strong>Description: </strong>Reinforcement learning (RL) is one qualifying instance of a new AI developed within the relevant times for this work. RL greatly matured in the past 20 years, and its theoretical basis originated approximately 25 years ago. Thus, as the number of researchers and interest in this field grows, to introduce a new AI of similar significance within the next 10 years and deploy it by 2043 is given a high odd of success. Composability AI could qualify as a new paradigm of AI. This Scenario is given a moderate weight, as existing nAIs or new nAIs rooted in modeling and learning AI frameworks may be sufficient for AGI. However, any new AIs which need significantly less data for training or can learn fundamental rules of systems, e.g. laws of physics, would be highly generalizable and greatly increase odds of AGI.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Consensus performance metrics developed for AI, either broadly applicable to AGI or for ten developed individual nAIs</p><p><strong>Reference section:&nbsp;</strong>II.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>15%</p><p><strong>Weight:&nbsp;</strong>0.1</p><p><strong>Description: </strong>Classification performance on a standardized dataset of images is an example of one consensus metric for one nAI. Performance measures can be found as early as 2010, which improved rapidly after the implementation of deep neural networks in 2012 and later surpassed human performance in 2015. The number of parameters in these models roughly correlate with performance, but the introduction of deep learning and more complex neural network architectures achieved greater performance than simply increasing the parameters in simpler architectures. Given this history, I view performance metrics on a nAI-by-nAI basis are more likely than a generalizable, singular measure for AGI. In this Scenario, I arbitrarily choose ten nAIs as an approximation to achieve AGI, given there may debatably be five today, and these are not likely extensible to accomplishing all tasks required for AGI. If the assembly and release of a standard datasets for other nAIs takes two years, performance measures defined and agreed upon in less than one year, and success achieved in half the time of image classification (2.5 years), that leaves roughly five years per nAI. With an assumption that all ten nAIs will be developed by 2033, for all ten nAIs to achieve consensus metrics and high performance by 2043, I expect the odds to accrue requisite benchmarkable datasets by 2037 for purely digital nAIs at 95%, and for nAIs operating in the physical domain, 60%. If seven nAIs are digital, and three physical, then the total odds are 95%<sup>7</sup> * 60%<sup>3</sup> = 15%. The weight for this Scenario is low, as performance metrics are not necessary to develop nAI, as seen with large language models today.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Digital domain of Cooking AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>95%</p><p><strong>Weight:&nbsp;</strong>0.05</p><p><strong>Description: </strong>The digital tasks for cooking, including retrieval of recipes and forming novel recipes, with temperatures, cook times, and ingredients, are all nearly possible already today. I see potential challenges related to ingredients: which can be substituted for one another, which are available at a given moment or in a given locality, and how \u201ceffective\u201d are combinations of ingredients, with a learning capability baked into the formation of novel recipes. This Scenario has a very low weight given its feasibility and success would not greatly alter odds of broader AI accomplishments.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Physical manipulation of objects for Cooking AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>58.3%</p><p><strong>Weight:&nbsp;</strong>0.4</p><p><strong>Description: </strong>Nuanced preparation tasks such as cracking and manipulating eggs in various states of viscosity will be difficult but not essential to cooking AI. Other challenging preparative tasks include peeling carrots and fine chopping of herbs. I posit odds of these tasks by 2043 at 30%. Flipping patties, robotic food picking (off plants or shelves, e.g.), and rough chopping are already feasible for many foods, and I place odds to achieve these more generalized cooking tasks at 80%. Sorting<strong>&nbsp;</strong>occurs in food processing factory settings today and could be scaled down to a small kitchen environment for dry goods separation and dispensing into cooking vessels, which I put at 65% odds. If these three odds are evenly weighed in their significance to cooking AI, and not all need to occur, then the overall odds are (80%+65%+30%)/3=58.3%. An incentivized environment to develop AI for these tasks is baked into the estimates, as there is already effort today to automate these tasks. A higher weight given than for other physical tasks such as Haircutting AI as success would be a big advancement in nuanced robotic maneuvers and applicable to many other physical tasks.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Digital domain of Haircutting AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>20%</p><p><strong>Weight:&nbsp;</strong>0.3</p><p><strong>Description: </strong>There isno \u201ccorpus\u201d of information like there is for online cooking recipes, and raw data on haircuts not only does not exist, but readily-generated data today may not be suitable for an AI model. For instance, video capture of haircuts would need to be translated to quantifiable actions performed in 3-dimensional space, including angles and forces of combs, scissors, and razors. Alternatively, a physical simulation environment could train robotic systems to perform these actions, effectively generating as much data as needed. Assuming development is required by 2037 to then deploy haircutting AI over a 5-year span, incentives notwithstanding, I view capturing videos of haircuts <i>en masse&nbsp;</i>or generating data from a physical simulation environment to be more feasible than not. However, I see lack of incentives, investment, and effort to be a major drag on developing Haircutting AI, digitally and physically. There, I assign low odds of success, but with a moderate weight, as success in framing a problem of physical tasks into the digital domain for AI modeling would be applicable to many other tasks as well.&nbsp;</p><p>&nbsp;</p><p><strong>Scenario: </strong>Physical manipulation of objects for Haircutting AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>10%</p><p><strong>Weight:&nbsp;</strong>0.1</p><p><strong>Description: </strong>The physical manipulation of objects, including haircutting equipment and human hair, is at least as difficult as the fine chopping or peeling tasks for Cooking AI, which I give odds of 30%. As with the digital domain of haircutting tasks, lack of incentives and investment further drag down these already low odds. Nonetheless, these tasks are given a low weight as the extremely fine and precise nature of these physical tasks are unnecessary for many other tasks and therefore less broadly applicable.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Digital domain of Auto Repair AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>32.5%</p><p><strong>Weight:&nbsp;</strong>0.35</p><p><strong>Description: </strong>Digital subtasks include cataloging manuals, parts, and tracking inventory for most major car models, including internal combustion engines. These tasks are highly analogous to information aggregation tasks solved in Cooking AI and given 80% odds of success (lower than digital Cooking AI tasks due to less existing work and reduced incentives). Analogous to medical diagnosis, a diagnostic system which integrates domain knowledge could occur through purely visual information (e.g. a car \u201cX-ray\u201d) or new diagnostics hardware. The odds of diagnostic tasks are 50%. Whether through Composability AI or other new nAIs, even a rudimentary understanding of the relationship of parts would connect the domain knowledge and diagnostic information to physical tasks of automobile repair. Alternatively, this challenge could be met with an abundance of data on cars in broken and fixed states to then propose repair actions, but I view the former route as more feasible and assign odds of 25%. These three primary categories of subtasks are given a roughly equal weight, except for a lower weight to the composability challenge, as understanding \u201cwhy\u201d is not essential in other AI tasks today. I weigh the combined probabilities of the first two subtasks, viewed as essential, with the third subtask to form final odds of ((80%*50%) + 25%)/2=32.5%. A similar weight is given to digital Haircutting AI, as these solutions can generalize to many other tasks.&nbsp;</p><p>&nbsp;</p><p><strong>Scenario: </strong>Physical manipulation of objects for Auto Repair AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>15%</p><p><strong>Weight:&nbsp;</strong>0.2</p><p><strong>Description: </strong>The maneuvers needed for automobile repair are more similar to haircuts than food preparation. When considering vehicles damaged from accidents (assuming significant numbers of humans will still drive vehicles for most of the next 20 years), Auto Repair AI may require comparable levels of finesse and precision as Haircutting AI, while also bearing much heavier loads and applying greater forces. However, the automobile industry may still be significantly impacted by automation of routine maintenance such as tire and brakes replacement. The odds of performing the more challenging tasks are 10%, equivalent to haircutting AI given it is more difficult but with greater incentives. I place odds of routine maintenance tasks at 30%, but in giving a slightly lower weight to these tasks, produce an overall odds of 15%. A lower weight is given here as success, while powerful, is an excessive capability of fine manipulation of heavy objects for other tasks.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Design for Construction AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>85%</p><p><strong>Weight:&nbsp;</strong>0.2</p><p><strong>Description: </strong>Building design including floorplans, heating/cooling, plumbing, and electrical wirings that are already largely feasible through software, and generative AI models trained on a corpus of blueprints could propose a near-infinite number of basic designs. I assign odds of these tasks at 95%. However, placing designs in real 3-dimensional space is more complicated. The terrain, soil, and other environmental considerations are constraints on which designs could realistically be built on a given plot of land. Designs may also require matching the building materials to its function, under other constraints such as material suitability, availability, and cost. Most of these environmental factors could be estimated with either existing knowledge of the site location or imaging data. I assign the odds of real-world \u201caware\u201d building design at 80%. Either of these groups of tasks may be sufficient for Construction Design AI, but in assigning a slightly higher weight to the more challenging tasks, I assign final odds of 85%. I give a modest weight here, as success would represent a broadly useful advancement in document and image generation (for example, this AI would estimate the volume and object depth in an image and photoshop new objects into a 3D space projected from the 2D image).</p><p>&nbsp;</p><p><strong>Scenario: </strong>Planning for Construction AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>36%</p><p><strong>Weight:&nbsp;</strong>0.3</p><p><strong>Description: </strong>Planning requires knowledge of permitting, building codes, zoning, and regulations, which are mostly hyperlocal to the region of construction. While technical elements of these tasks are mostly feasible today, a lack of information standards and changing codes, zoning, and regulations across towns and states over time pose a challenge of information aggregation. Document creation is feasible, and there exist numerous public documents and records related to construction planning that could be used to train generative AI models. However, these documents are highly disaggregated, along with the construction industry generally, and would require a large upfront cost of domain knowledge engineering to train a permit creation AI, factoring local codes and regulations and with awareness of changes over time. Multiplicative odds of document creation subtasks at 90% and codified domain knowledge at 40% produce a final odds of 36%. A modest weight is given due to the size of the construction industry and the utility of AI with engineered domain knowledge and abilities to update with changing rules and regulations.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Physical manipulation of objects for Construction AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>10%</p><p><strong>Weight:&nbsp;</strong>0.25</p><p><strong>Description: </strong>The combination of power, finesse, and versatility required of physical tasks in Construction AI make it no likelier than Auto Repair AI. I view Construction and Auto Repair industries to contain comparable degrees of incentives and challenges to develop and adopt new technology, in part due to their fragmented nature. Relative to Auto Repair AI, I downweigh odds of physical Construction AI slightly as there have been unsuccessful efforts in robotic construction. Physical Construction AI receives a slightly higher weight than Auto Repair due to the size of the construction industry, and success would be a tremendous advance in physical manipulation of objects generally.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Digital domain of Healthcare AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>67%</p><p><strong>Weight:&nbsp;</strong>0.3</p><p><strong>Description: </strong>Much of healthcare includes maintaining or altering human physiology, all grounded in some understanding of biology. Significant new discoveries of fundamental biology and their applications as novel therapeutics by AI are unlikely in the near future. However, other tasks to recommend standards of care, exploit existing domain knowledge and patient information to diagnostic or therapeutic ends, or perform administrative roles in healthcare, are all significant and qualifying events for digital Healthcare AI. Based purely on feasibility, I place odds of these sets of tasks are 90%, 40%, and 90%, respectively. Healthcare, especially in developed nations, is fettered by outside interests, and as a source of job growth and destination for very high-skill labor, will experience resistance to implementations of AI. I thus downweigh the odds, particularly for administrative tasks, to 85%, 35%, and 80%. If these three odds are evenly weighed in their significance to digital Healthcare AI, and not all need to occur, then the overall odds are (85%+35%+80%)/3=67%. A modest weight is assigned based on the significance of the industry, although success may not be as broadly applicable because of its idiosyncratic nature.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Digital domain of Contracts AI</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>95%</p><p><strong>Weight:&nbsp;</strong>0.1</p><p><strong>Description: </strong>Document creation for many standard business and legal contracts is already possible. Considering the recent advent of smart contracts, and evolving disputes on AI usage and ownership, the odds here are less than 100%. Even so, digital Contracts AI is feasible and will be increasingly utilized. Automating the creation of most common contracts would be sufficient for a qualifying nAI event. A low weight is given as the impact may be a productivity boost on a small number of jobs.&nbsp;</p><p>&nbsp;</p><p><strong>Scenario:&nbsp;</strong>Forming and operating businesses</p><p><strong>Reference section:&nbsp;</strong>II.C</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>10.5%</p><p><strong>Weight:&nbsp;</strong>0.2</p><p><strong>Description:&nbsp;</strong>The subtasks of negotiating, decision-making, and planning based on environmental stimuli are essential for a qualifying nAI event. AI is already capable of forming negotiating and bargaining statements, though they possess little contextual understanding of their conversations with humans. One common business scenario that requires negotiating is capacity planning and leasing agreements on property, office space, or online domains. A fabricated negotiation from a large language model is already possible, however I put odds of a real-world and human-interactive AI negotiator at 70%. Reinforcement learning (RL) is a form of AI designed to receive and respond to environmental inputs, and a likely candidate as a responder to market, competitor, and consumer behaviors. RL models require abundant information on actions and outcomes, or the ability to simulate them. For certain narrow cases of businesses operating purely digitally, both in its functions and its core product or service, RL may be able to gauge product interest, pricing power, and consumer feedback to propose changes in product lines or prices. However, incomplete information about market changes and competitor behaviors is innate to competitive economic systems. Assuming these systems persist, AI will have to rely on large engineering efforts of domain knowledge, including the actions taken by previous companies that led to their demise, or a new AI able to perform inference and propose actions with highly incomplete information. Notably, the set of possible actions that such an AI needs to consider here will be orders of magnitude greater than for chess or Go, and may itself require substantial human engineering. I view the odds of a successful domain knowledge engineering effort or a new AI capable of superhuman inference with sparse information at 10 and 20%, respectively. If these probabilities are weighted evenly, where either can occur, then the final odds for business AI is 70% * (10%+20%)/2 = 10.5%. A modest weight is given as success would be a monumental achievement towards AGI but not essential, and it is more likely nAIs for certain tasks within a business (sales, product design, marketing) will occur and potentially qualify as AGI.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Periods of large unfavorable macroeconomic conditions or continued secular stagnation over the next 20 years</p><p><strong>Reference section:&nbsp;</strong>III.A</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>36%</p><p><strong>Weight:&nbsp;</strong>0.5</p><p><strong>Description: </strong>If the time since the 1970s is at all predictive for the next 20 years, there will be at least one mild global recession, and a small probability of a severe recession or depression. I posit a mild recession will have little impact on AGI timelines, though a larger recession will delay the development of innovative new technologies and its deployment into industry, in part through an environment of risk aversion and low capital investment. While the narrative of secular stagnation may be overplayed, as economic and productivity indicators does not capture digital services and software as accurately as physical goods, there is solid evidence for declining rates of growth in the United States, Europe, and parts of Asia. I view it more likely than not these trends will continue, and as other countries develop, the \u201cspreading effect\u201d of secular stagnation in a globalized economy will produce an asymptote in their rate of individual economic and technological. A transformational change such as AGI to occur in this environment is still possible, as evidenced by the advent of the internet in the 1990s, and evidence of some disruption to this stagnation during the coronavirus pandemic. The odds in this scenario are viewed as a weighted additive aggregation of possible events which decrease Pr(AGI<sub>2043</sub>). The odds of a small recession are 90% but with a small weight on AGI development relative to a large recession at 20%. I grant 50% odds of a continuation and spreading of secular stagnation that would materially impede the innovation and growth necessary to achieve AGI. Combing these scenarios produces (90%*0.1) + (20%*0.6) + (50%*0.3) = 36%. A strong weight is given here as these scenarios impact AI technologies broadly.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Diversification of fundamental AI R&amp;D projects</p><p><strong>Reference section:&nbsp;</strong>III.A</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>80%</p><p><strong>Weight:&nbsp;</strong>0.25</p><p><strong>Description: </strong>Large technology companies and research groups supported by such companies have produced most applied AI R&amp;D. These projects are focused on classification and generation of text, speech, and visual content, as well as playing games, healthcare diagnostics, and prediction of protein structure. The applications of modeling and generating text, speech, and images will surely continue to grow, however simply strengthening and diversifying these nAIs may not be able to perform many other tasks mentioned throughout this Work. For instance, improved diagnostic capabilities in healthcare and protein structure modeling in biotech on their own will unlikely become a qualifying event for AGI. If the time to identify a new R&amp;D project, locate or acquire requisite data, and apply it to the problem statement takes four years, and only FAANG companies perform these functions, who have at most a handful of such projects, then one may expect a few dozen total AI projects in the next 20 years. If companies compete on roughly half these projects, then there may be collectively 10-20 new and unique nAIs. However, increased accessibility and decreased cost of AI will yield diversified projects from outside large technology companies. Odds are downweighed from certainty due to the category of projects technology companies are willing to address, as they are likely to exploit purely digital AI projects more fully before exploring the physical task examples in this Work. A modest weight is granted as the broadening of AIs is more important for AGI than strengthening existing nAIs.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Investment and private sector growth in automating fading technologies and services, including internal combustion engine vehicle maintenance and repair</p><p><strong>Reference section:&nbsp;</strong>III.A</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>1%</p><p><strong>Weight:&nbsp;</strong>0.35</p><p><strong>Description: </strong>While limited, one prime example serves as a basis for this scenario analysis. Tesla Motors began operations in 2003, requiring roughly thirteen years for an announcement for a full self-driving capability in 2016 and fifteen years to reach significant market share in the car industry. Twenty years after Tesla\u2019s founding, full self-driving appears imminent but is still not fully deployed. Self-driving cars and electric vehicles are not equivalent, however most work on self-driving is associated with Tesla (noting that other car-agnostic self-driving technologies exist and have been in development since 2016 with limited adoption). Tesla Motors has also not developed an additional nAI apropos for this work, namely automated vehicle repair or maintenance, nor is it a significant initiative at the company. In 2023, there are a dozen other EV manufacturers with a significant market size. Considering the capital allocation by investors and car companies towards EV production, and only Tesla has attempted even mechanized battery exchanges (largely unsuccessfully), one could predict the time from announcement to development of automated self-driving or vehicle maintenance technologies to take upwards of ten years, and deployment into industry and broader impact on services ten more years. Focusing narrowly on vehicle maintenance and repair, I place odds of any car company accomplishing these tasks at 10%, and place ICE repair at comparable odds. More broadly, I optimistically posit even two sets of tasks or large industries with \u201cretroactive\u201d investment and innovation on fading industries or technologies are sufficient to meaningfully contribute to qualifying AGI. If so, then the total odds would be 10%*10%=1%. A moderate weight is placed on this scenario as service jobs may linger or expand for decades without active investment and technological development.&nbsp;</p><p>&nbsp;</p><p><strong>Scenario: </strong>Continuation of outsourcing to cheaper labor and economic growth trajectories of yet agricultural or low-skill manufacturing regions</p><p><strong>Reference section:&nbsp;</strong>III.A</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>88%</p><p><strong>Weight:&nbsp;</strong>0.7</p><p><strong>Description: </strong>A break from the phenomena of relocation of work to areas of cheaper labor and industrial planning and development by developing nations may occur in a few ways. One could be the exhaustion of excess labor. Bangladesh, the 8<sup>th</sup> most populous country, serves as an example of how unlikely exhaustion will occur within 20 years. Bangladesh introduced an industrial capacity and economic policy initiative around 1980. Today, clothing represents&nbsp;<a href=\"https://www.worldstopexports.com/bangladeshs-top-10-exports\">87% of Bangladesh exports</a>. Vietnam and India are examples of other populous countries with similar economic trajectories, though moving quicker to electronics, pharmaceutical, and automobile manufacturing. Even so, India has a low GDP per capita, alongside Pakistan, Bangladesh, and Nigeria, which collectively consist of over 2 billion people. The plot below depicts GDP per capita growth of select countries since 1990.&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://noahpinion.substack.com/p/can-india-industrialize?publication_id=35345&amp;post_id=101082915&amp;isFreemail=true\">https://noahpinion.substack.com/p/can-india-industrialize?publication_id=35345&amp;post_id=101082915</a></p><p>&nbsp;</p><p>Another mechanism would be unprecedented economic hypergrowth across multiple large developing nations. A noticeable upward slope for GPD per capita in China began in 2005, after a period of economic reforms originating in the 1990s. In this light, in the 30 years from those reforms to today, China\u2019s growth could be considered successful, Bangladesh less so, and Singapore the \u201dbest-case\u201d scenario. It is then extremely unlikely growth equivalent to or exceeding Singapore would occur for multiple populous and developing nations in less than twenty years. Another mechanism is decreased globalization and onshoring in developed nations. However, onshoring may only be viable with efficiency and productivity growth from new technologies or cheaper domestic labor. Whether by job automation or reduced wages, such industrial policies will be unpopular with voters and politicians. If these processes do occur, their occurrence within twenty years is unlikely. I give odds of 85%, 99% and 80% that there will be sustained excess labor, developing nations with over two billion citizens will not undergo an unprecedented hypergrowth, and industrial policies leading to job automation or wage suppression will not be palatable and thus not widely implemented in developed nations, respectively. The average odds are 88%. A large weight is given as these trends have been a significant influence in technological development, economic growth, and the global world order since the Industrial Revolution.</p><p>&nbsp;</p><p><strong>Scenario: </strong>More nAIs are required for AGI</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:&nbsp;</strong>16.8%</p><p><strong>Weight:&nbsp;</strong>0.6</p><p><strong>Description: </strong>I posit five new nAIs are needed for AGI. Based on the histories of nAIs developed to date, each would cost at least billions of US dollars and take at least three years. I place the odds of development and deployment of these new nAIs within 20 years at 80%. I also posit AGI will require integration of multiple nAIs (Construction AI, e.g.), and place 80% odds of technical feasibility to integrate multiple nAIs within two years of their development. While technically feasible, these integrations may require one agency, company, or state to either develop nAIs or have existing nAIs made available or purchasable. The odds for the proper \u201cenvironment\u201d of nAI development and assembly, over a span of five years from the development of all requisite nAIs, is 25%. These collectively form an 11-year horizon with 80%*80%*25%=16% odds. However, this assumes that the new nAIs are the <i>right&nbsp;</i>ones. For instance, nAIs today do not operate in the physical world, nor possess abstract senses of real-world agency and strategy. I view the next five nAIs being the right nAIs for AGI to be low. If five nAIs are sufficient for AGI, but twenty total are needed to be developed first, I grant 60% odds that twenty will be built in 15 years. With the same odds and timeline for technical integration, and a boost in the probability of integration to 35% in five years, the 20-year odds are at 60%*80%*35%=16.8%. The weight is moderately high, as AGI may be possible with significant extensibility of existing nAIs today, or AGI may arise through other means than integration of multiple nAIs.</p><p>&nbsp;</p><p><strong>Scenario: </strong>High skill white-collar professions continue to expand pockets of tasks untouched by AI or created in response to AI</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>70%</p><p><strong>Weight:&nbsp;</strong>0.15</p><p><strong>Description: </strong>Considering the formation of services jobs off the innovations of the Industrial Revolution and the rise of the managerial class during the 20<sup>th</sup> century, coupled with the motives of high-skill labor for gainful employment and differentiation of their products or services, this scenario is more likely than not. However, the remaining slivers of the professions untouched by AI, for example the specialties within law and medicine, will only be able to expand so widely in the future. For medical professionals, the diagnoses, treatments, and procedures serviced by AI will leave fewer services to offer by humans. One area that may continue to expand for the foreseeable future is research and engineering, as new ideas may be readily produced by AI, but the means to research and answer new questions is less likely to be solved by existing nAIs. Thus, while this scenario is more likely than not, the increasing automation to tasks in legal, medicine, design, and other advanced-degree professions will erode their headcount and influence over time. The weight for this scenario is modest as the candidate professions represent a fraction of total labor, and most individuals cannot attain this level of work for various reasons.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Structural unemployment due to AI crease excess returns to increase investment and capital towards more AI</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Increase</p><p><strong>Odds:</strong> 30%</p><p><strong>Weight:&nbsp;</strong>0.25</p><p><strong>Description: </strong>Structural unemployment due to automation is obvious conceptually, but there is much uncertainty over how much has occurred over the past few decades. Given the long period over which structural unemployment manifests, I give odds of 40% that significant skill mismatches in the labor market impact a majority of the labor force within 20 years. Assuming that structural unemployment is due to AI, and that it translates to higher productivity with lower operating expenditures and thus produces excess returns, I give 75% odds of appreciable investment in the development of new technologies which hasten AGI timelines. I downweigh these odds slightly as extra cash is often used to improve balance sheets or to reward investors and shareholders. This latter action is of increasing frequency and volume over the past twenty years, and this trend is likely to continue in the near future. The collective odds are 40%*75%=30%. This scenario is given a modest weight given the difficulty to observe and quantify structural unemployment, despite its importance.&nbsp;</p><p>&nbsp;</p><p><strong>Scenario: </strong>Government interventions of expanded state, welfare, or employment mandates crowds out private sector or weakens state and broad economic conditions with excess debt</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>18%</p><p><strong>Weight:&nbsp;</strong>0.25</p><p><strong>Description: </strong>Concerns of social stability due to growing structural unemployment may prompt government interventions. Employment mandates are already present in central banks of many developed nations, though these mandates often take the form of monetary policies to stimulate the economy, rather than direct assistance such as cash transfers or basic income. These actions may be instead performed through fiscal stimulus, and if recent trends in developed nations continue, the role of fiscal policy will grow stronger relative to monetary policy. Taken too far, these policies risk defaults, currency devaluations, and broad financial instability. In addition, such spending comes at the expense of the private sector, often the primary engine of economic growth. Any of these actions could impact AGI timelines. The odds in this scenario are conditional on large structural unemployment due to AI, or a perceived threat of such, to occur within ten years (see above scenario regarding structural unemployment). I grant 60% odds of expanded stimulus and employment mandates in most G7 or large Asian countries to produce debt/GDP ratios appreciably above those of the WWII or COVID eras. The odds that, conditional on these debt levels, there will be a large global recession, significantly slowed economic growth from a weakened private sector, or a series of major currency devaluations is 30%. The combined odds are thus 60%*30%=18%. A modest weight is given because while these events would be significant, this scenario is conditional on other unlikely events, and development of new technologies may still occur, or even be accelerated, in seemingly unfavorable environments.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Government taxation on AI-enabled profits or implementing restrictions on sales of goods or services powered by AI</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>60%</p><p><strong>Weight:&nbsp;</strong>0.2</p><p><strong>Description: </strong>Taxation may occur as a form of redistribution, possibly to fund basic income, re-skilling, or welfare, or opportunistically on companies who become exceedingly profitable from their AI technologies. Recent history suggests that for seemingly pressing matters, Pigouvian taxation, e.g. on cigarettes or carbon, and windfall taxes, e.g. on oil companies, can be proposed and implemented within years. I put the odds of taxation within 20 years, first requiring AI powerful and threatening enough for taxes to gain political popularity across a majority of G7 nations and developed Asian countries, at 60%. Even so, AI is likely to remain nebulous, distributed, and thus more difficult to tax than obvious monopolistic or oligopolistic industries, and Pigouvian taxation requires clear and quantifiable externalities, which is difficult for AI. However, while taxation on humans is difficult politically, taxation on AI is relatively palatable. Thus I maintain a 60% overall odds on this scenario. I grant a smaller weight as AI will become cheaper and widely accessible, and much of AI to date is open source, thereby dampening the impact of taxes on future AGI development.</p><p>&nbsp;</p><p><strong>Scenario: </strong>AI as a public good and reducing innovation incentives</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>5%</p><p><strong>Weight:&nbsp;</strong>0.05</p><p><strong>Description: </strong>The open-source model of software development creates something close to a public good already, and individuals and companies continue to find ways to develop and monetize new technologies regardless. A push to formally assign and enforce a public good status on AI could take a long time to implement, and even so the impact to AGI timelines will likely be small, much like taxation of AI probably may not hinder new development. Therefore, a very small weight is given for this low-probability scenario.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Legal action on AI ownership or AI monopolies</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>40%</p><p><strong>Weight:&nbsp;</strong>0.1</p><p><strong>Description: </strong>Recent events between technology companies and governments, including exchanges over rights of developers and consumers on gaming, social media, and advertising platforms, implies technology companies will continue to encounter scrutiny over any outsized influence and market power. While governments may take non-legal, coercive actions to democratize AI or weaken technology developers, I do not grant material odds of these nebulous events as they are unlikely to impact AGI timelines. I do expect legal actions over ownership or anti-trust, possibly over consolidations of nAIs by large technology companies, as likely within 20 years. However, I downweigh these odds as the time to build cases is long and the success rate in recent lawsuits against technology companies low, and have only come to fruition under certain political landscapes. In addition, I expect a strong anti-trust case could only be made over an overwhelmingly obvious threat of monopoly from nAI consolidation, as the emergence of an actual monopoly is often not immediately obvious and less likely within 20 years. With these arguments, and that AGI development will continue irrespective of legal cases, this scenario receives a low weight.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Downward consumption expectations drag economy at large</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>20%</p><p><strong>Weight:&nbsp;</strong>0.25</p><p><strong>Description: </strong>Lower consumer confidence in the economy or high expectations of inflation impact consumption. While the effect size is difficult to quantify relative to many other possible factors, negative expectations can be a self-fulfilling prophesy for economic downturns. Consumers may shift to purchasing staples and avoiding frivolous expenses, many of which are services which comprise a large part of the economy in developed nations. The advent of AI would represent a new driver of consumption pessimism, whereby consumers may expect to have less purchasing power under the threat of job loss. The threat of AI to jobs is widely discussed today but is not obviously impacting expectations of consumers or employment. Thus, I view it unlikely that downward expectations would be significant enough to impact the economy broadly. I grant a smaller weight as smaller economic downturns, if they were to happen because of this pessimism, would not impact AGI timelines significantly.&nbsp;</p><p>&nbsp;</p><p><strong>Scenario: </strong>Sustained consumption of human-derived goods which offsets demand for AI products</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>40%</p><p><strong>Weight:&nbsp;</strong>0.1</p><p><strong>Description: </strong>Given sustained demand for the hand-made and artisanal today, I find it likely that as AI-generated goods and services proliferate, there will be a response of demand for \u201cmade by humans\u201d. While likely, this demand may only come from wealthier consumers, and apply to a small proportion of all goods and services. I downweigh the odds as the supply of human-made goods and services and number of human laborers could decrease in response to competition from AI, mitigating this demand in the process. A low weight is given as this demand will be washed out by the consumption of the masses, and consumption of human-made goods and services may not materially impact AGI timelines.</p><p>&nbsp;</p><p><strong>Scenario: </strong>Opportunity cost in strengthening existing nAIs off current demand over developing new nAIs</p><p><strong>Reference section:&nbsp;</strong>III.B</p><p><strong>Impact to Prior Pr(AGI<sub>2043</sub>)</strong>: Decrease</p><p><strong>Odds:&nbsp;</strong>65%</p><p><strong>Weight:&nbsp;</strong>0.3</p><p><strong>Description: </strong>The balance of exploiting existing nAIs and exploring new ones will be a large determining factor in the overall technological growth towards AGI. I expect the nAIs that power content creation, media, marketing, and sales will continue to be more fully exploited, while AIs for other tasks, including scientific research and many of the physical tasks mentioned in this Work, will have substantially fewer researchers and dollars invested. For example, entertainment media and advertising are heavily validated large markets serviced by large technology and retail companies, particularly in the consumer-heavy economies of developed nations. Companies will be quicker to cut R&amp;D on exploratory projects than sacrifice on the core product in periods of economic uncertainty and limited cash reserves. Existing nAIs also have much room for additional productization. For instance, the \u201cbase layer\u201d of ChatGPT can be modified to diversify content offerings towards different ideological or political leanings, or genres of blogs, books, and other text-based content. There is much excess demand yet to be exploited from large language models, and I view it more likely than not that new product lines of incumbents and core products of startup companies will focus on these opportunities in the coming years. Most research on new AIs will likely continue to come out of R&amp;D at larger companies or in academic research environments. While I view it nearly certain that the majority of work in AI will go towards strengthening existing nAIs over developing new ones over the next few years, I downweigh these odds as there are continually new uses of existing AIs, AI will become cheaper and accessible, and new AIs can now be pushed into the market and exploited within years. I grant a modest weight as existing nAIs may be sufficient for AGI and as more researchers and dollars spent on AI development will negate the opportunity cost of strengthening existing nAIs.&nbsp;</p><p><br>&nbsp;</p><hr><p><a href=\"#_ftnref1\">[1]</a>For the technical reader, I consider supervised, unsupervised, shallow and deep neural networks as modeling, and reinforcement learning as learning AIs.<br><br>&nbsp;</p>", "user": {"username": "srhoades10"}}, {"_id": "mGHRdcmKjLhDP5gLc", "title": "Cause Exploration: Support for Mental Health Carers", "postedAt": "2023-03-14T20:04:36.541Z", "htmlBody": "<p><strong>Tldr- I'm looking into Support for mental health carers as a potential cause area for a while, would love inputs about ITN and generally about the subject**&nbsp;</strong></p><p><strong>Summary of key points:</strong></p><ul><li><strong>Mental health as an important cause area-</strong> Mental illness seems to cause&nbsp;<a href=\"https://www.happierlivesinstitute.org/uploads/1/0/9/9/109970865/cost-effectiveness_analysis__group_or_task-shifted_psychotherapy_to_treat_depression_oct21.pdf\">a high amount of worldwide unhappiness and seems neglected</a>.</li><li><strong>Carers as a potential solution-</strong> Most of the people suffering from mental health issues or illness are surrounded by family and friends, who can potentially have a high impact on the decrease or increase of their mental state. Also, there is a stigma considering mental health- leading to cases being underreported and individuals that are unwilling to seek treatment. The carers could be the first and only to discover the issues before it is too late, and the price of giving them the tools to support could be cheap and efficient.</li><li><strong>Carers as a potential cause area-</strong> Although the suffering of carers is (probably) not nearly as severe as the people suffering from mental health issues or illnesses, the scale of the people it affects is wider and it the neglectedness is probably higher.&nbsp;</li></ul><p><strong>Elaboration:</strong></p><p><strong>Mental health as an important cause area</strong></p><p>Depression is a substantial source of suffering worldwide. It&nbsp;<a href=\"http://ghdx.healthdata.org/gbd-results-tool\">makes up 1.84% of the global burden of disease</a> according to the IHME (Institute for Health Metrics and Evaluation). The treatment of depression is neglected relative to other health interventions in low to middle-income countries. Governments and international aid spending on mental health represent&nbsp;<a href=\"https://academic.oup.com/inthealth/article/11/5/361/5537164\">less than 1% of the total spending on health in low-income countries</a>.</p><p><strong>Carers as a potential solution</strong></p><p>A <a href=\"https://www.qld.gov.au/health/mental-health/carers\">carer</a> is someone who voluntarily provides ongoing care and assistance to another person who, because of mental health issues or psychiatric disability, requires support with everyday tasks. A carer might be a person\u2019s parent, partner, relative or friend. The supporter&nbsp;<a href=\"https://psycnet.apa.org/record/2016-01012-001\">has an impact</a> on the sufferer and could be the first and only to discover the problem.</p><p>There are&nbsp;<a href=\"https://www.actionforcarers.org.uk/who-we-help/adult-carers/caring-for-someone-with-mental-health-problems/\">supports</a>, <a href=\"https://www.betterhealth.vic.gov.au/health/servicesandsupport/caring-for-someone-with-mental-illness\">guides</a> and&nbsp;<a href=\"https://www.y-a-m.org/\">programs</a> for high income countries (the quality and amount of improved due to covid, but also the depression rates are higher), but <strong>few programs and high quality study on programs who approach improving mental health through carers in low-middle income countries.</strong></p><p>Happier lives institute did&nbsp;<a href=\"https://www.happierlivesinstitute.org/mental-health-programmes.html\">screen&nbsp;programs</a> listed on the&nbsp;<a href=\"https://www.mhinnovation.net/\">Mental Health Innovation Network</a>, and one of the programs is&nbsp;<a href=\"https://www.mhinnovation.net/innovations/thinking-healthy-programme-peer-delivery-thpp\">peer-based</a>. Other interesting programs are&nbsp;<a href=\"https://strongminds.org/strongminds-our-model/\">StrongMinds Peer Facilitator Programs</a> (which are&nbsp;<a href=\"https://www.happierlivesinstitute.org/uploads/1/0/9/9/109970865/cost-effectiveness_analysis__strongminds_psychotherapy_programmes_oct21.pdf\">cheaper,&nbsp; and the facilitators have a higher understanding of the participants</a>) and&nbsp;<a href=\"https://www.mhinnovation.net/blog/2020/may/3/carers-worldwide-mental-health-support-carers-india-nepal-and-bangladesh-during\">Carers worldwide</a>. I believe research on programs such as these could be a path to potential effective interventions.</p><p><strong>Carers as a potential cause area</strong></p><p>The amount of the carers is higher than people suffering from mental difficulties, and their support is more neglected. Caring for a person suffering from mental health difficulties can hurt the supporter (<a href=\"https://en.wikipedia.org/wiki/Secondary_trauma\">Secondary trauma</a>, <a href=\"https://en.wikipedia.org/wiki/Copycat_suicide\">Copycat suicide</a>). The direct support for the carers in addition to the secondary improvement of the people severely suffering could improve dramatically the cost-effectiveness.</p><p><strong>Summary</strong></p><p>I believe there is a strong case to consider furthering the study of mental health carer support, and it should be a higher priority in the effective altruism community because of the potential scale, neglectedness, and cost-effectiveness of such programs.</p><p>&nbsp;</p><p>Thanks to <a href=\"https://forum.effectivealtruism.org/users/edoarad?mention=user\">@EdoArad</a> and <a href=\"https://forum.effectivealtruism.org/users/gidikadosh?mention=user\">@GidiKadosh</a> &nbsp;for helping me write this up, to <a href=\"https://forum.effectivealtruism.org/users/ce?mention=user\">@CE</a> for inspiring me to write this a year ago, and <a href=\"https://forum.effectivealtruism.org/users/sella?mention=user\">@sella</a> &nbsp;and <a href=\"https://forum.effectivealtruism.org/users/danlahav-gmail-com?mention=user\">@Dan Lahav</a> for incentivizing me to look more deeply into this topic today</p><p>Also thank you generally to everyone promoting mental health as a cause area :)</p><p>** This might be an un-updated text because I wrote it almost one year ago, would love for it to be considered as a draft in order to improve my and our knowledge. Feel free to criticize, and add any knowledge you deem relevant</p>", "user": {"username": "Yuval Shapira"}}, {"_id": "eAaeeuEd4j6oJ3Ep5", "title": "GPT-4 is out: thread (& links)", "postedAt": "2023-03-14T20:02:12.434Z", "htmlBody": "<p><a href=\"https://openai.com/research/gpt-4\">GPT-4 is out</a>. There's also <a href=\"https://www.lesswrong.com/posts/pckLdSgYWJ38NBFf8/gpt-4\">a LessWrong post on this</a> with <s>some</s> a lot of discussion. The developers are doing <a href=\"https://www.lesswrong.com/posts/pckLdSgYWJ38NBFf8/gpt-4?commentId=t8LzunjAj978bLfMD\">a live-stream <s>~now</s></a> (yesterday).</p><p>And it's been <a href=\"https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI%E2%80%99s-GPT-4\">confirmed that Bing runs on GPT-4</a>.&nbsp;</p><p>Also:&nbsp;</p><ul><li><a href=\"https://www.anthropic.com/index/introducing-claude\">Claude (Anthropic)</a></li><li><a href=\"https://blog.google/technology/ai/ai-developers-google-cloud-workspace/\">PaLM API</a></li></ul><p>Here's an image from the <a href=\"https://openai.com/research/gpt-4\">OpenAI blog post about GPT-4</a>:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/ixr0kdj6igzv6rq0ecix\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/rxtq5wilbihavikbrsvk 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/bfmm09gpci7jfa9jcfzu 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/minbtk6nfnynt4fprv1n 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/mfriyqrmafogrzuhvfpm 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/fcna1ygjhqua1nxcyys0 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/vmc2vzczt9tsnkipalnm 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/y4mrebahxuttexe9i1lk 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/gydpipvstemrrcvaflf6 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/c4fnbjxoootczu8zvc0f 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/eAaeeuEd4j6oJ3Ep5/ctn5zflelqivaxzchddc 1350w\"><figcaption>Exam results GPT-3.5 vs GPT-4 (ordered by GPT-3.5 performance)</figcaption></figure><p>(This is a <a href=\"https://forum.effectivealtruism.org/posts/6whiBq7czKJk4Bx29/a-forum-post-can-be-short\">short post</a>.)</p>", "user": {"username": "Lizka"}}, {"_id": "nFgK2FtEZg4mjd9oj", "title": "What is the formal definition of a 'Factory Farm'?", "postedAt": "2023-03-14T14:14:10.275Z", "htmlBody": "<p>The <a href=\"https://forum.effectivealtruism.org/topics/farmed-animal-welfare\">'Farmed Animal Welfare' wiki page on the EA Forum</a> defines factory farming as being farms where can involve \"intense confinement, inhibition of natural behaviours, untreated health issues, and numerous other causes of suffering\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu9wlijd8aa\"><sup><a href=\"#fnu9wlijd8aa\">[1]</a></sup></span></p><p>However, confinement, inhibition of natural behaviours and treatment of health issues are not binary values, they are sliding scales. This is easy to see with 'confinement' - we can measure the size of an enclosure that an animal is being kept in. 'Health issues' is harder to quantify, but could be done with various metrics, like how many animals experience disease, and 'inhibition of natural behaviours' could be measured by things like time spent outdoors, amount of space, or access to an appropriate amount of their kin.</p><p>There must be a point at which a farm is sufficiently cruel to animals on all these points that it can be defined as a Factory Farm, and conversely there must be a point at which a farm can no longer be defined as a Factory Farm. My question is: Where is this point? How many square metres, or hours spent outside, or medical treatment per animal, is sufficient for a farm to not be considered a factory farm?&nbsp;</p><p>The answers to these questions would have big outcomes on statistics like 'x amount of animals live in factory farms'. This seems like it should be an obvious point, but when I've read articles that quote these statistics, I haven't been able to find out how exactly they define a factory farm.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu9wlijd8aa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu9wlijd8aa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Reese, Jacy (2016) <a href=\"https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism\">Why animals matter for effective altruism</a>, <i>Effective Altruism Forum</i>, August 22.</p></div></li></ol>", "user": {"username": "Matt g"}}, {"_id": "rRY4XhrbMmympWPcT", "title": "Keeping the personal in personalised medicine", "postedAt": "2023-03-14T09:50:43.271Z", "htmlBody": "<p>After seeing the opportunity advertised on 80,000 hours, last year I was lucky enough to be awarded the Tony Blair Institute for Global Change Progress Fellowship. As part of this position, I was required to complete a report into one key UK-specific policy issue and present novel means of resolution. <strong>As such, the work presented below summarises the health data-specific barriers that exist to the UK achieving its potential for personalised medicine.</strong>&nbsp;</p><p>As a departure to predecessors that have covered similar ground (<a href=\"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1067053/goldacre-review-using-health-data-for-research-and-analysis.pdf?utm_campaign=846512_PRESS%20RELEASE%20Goldacre%20review&amp;utm_medium=email&amp;utm_source=NHS%20Confederation&amp;dm_i=6OI9,I568,282IBS,27TNT,1\">the Goldacre Review</a> is well worth the read in full), the solutions it offers puts the preservation of the doctor-patient relationship front and centre, ensuring that the technological reforms called for remain relationship-enhancing, not relationship-eroding.</p><p>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;***</p><h3><strong>Executive Summary:&nbsp;</strong></h3><p>Whether in sickness or in health, the human body is a data-generating machine. In NHS consultation rooms, hospital beds and telemedicine portals across the country, information is gathered about a person\u2019s health, history, outcomes and course of treatment at unimaginable scale. Effective use of these data allow clinicians to deliver personalised medicine \u2013 where treatment is customised to an individual patient \u2013 in a way not previously possible. However, from hurried notes on the back of a patient\u2019s EEG to in-pharmacy vaccinations that never make it onto the medical record, the collection of this detail still leaves a lot to be desired.</p><p>Moreover, updated clinical-data requirements, multiple coding languages and fragmented processing systems have fractured the UK\u2019s health-data landscape, making information sharing across settings and between allied professionals <a href=\"https://bmjopen.bmj.com/content/9/12/e031637\"><u>unnecessarily difficult</u></a>. The data sets policymakers rely upon are rife with <a href=\"https://www.health.org.uk/publications/long-reads/how-better-use-of-data-can-help-address-key-challenges-facing-the-nhs\"><u>inconsistencies and inaccuracies</u></a> as a result. Despite this, the government has grand designs for a data-enabled NHS; its \u201c<a href=\"https://www.gov.uk/government/publications/data-saves-lives-reshaping-health-and-social-care-with-data/data-saves-lives-reshaping-health-and-social-care-with-data\"><u>Data Saves Lives</u></a>\u201d policy paper commits to long-term investments in IT modernisation and secure research environments. Such news is welcome if the UK is to achieve its <a href=\"https://www.longtermplan.nhs.uk/\"><u>ambitions</u></a> for more personalised forms of care.</p><p>However, technological reforms to the NHS entail the redundancy of preceding systems and <a href=\"https://topol.hee.nhs.uk/wp-content/uploads/HEE-Topol-Review-2019.pdf\"><u>new pressures</u></a> for those on the front line of delivery. Care must be taken to ensure that the efficiencies innovation can offer do not come at the expense of the doctor-patient relationship.</p><p>This paper outlines the four main data barriers that undermine the UK\u2019s prospects for personalised medicine:</p><ol><li>Disconnection: a fragmented data landscape</li><li>Quality control: unrepresentative, poorly codified and unaudited-data sets</li><li>Reidentification risks: the disclosure of sensitive information</li><li>Bureaucratic burden: additional duties for overstretched staff</li></ol><p>Intentionally tempering tech evangelism with human values, this paper recommends the following to address these data barriers.</p><p>To address issues of data disconnection:</p><ul><li>Legislation must simplify data-access requests and bring consistency across documents where possible.</li><li>Policymakers must find ways of ringfencing deidentified health data away from general data-sharing restrictions such as General Data Protection Regulation (GDPR).</li><li>Government actors must focus their efforts on creating a work environment that is conducive to public-private-academic coalitions that focus on health, where innovation is centrally subsidised, legal liability is shared and public scrutiny is assured.</li><li>Regulatory bodies must finalise and publicise a well-defined accreditation process for Trusted Research Environment (TRE) status.</li></ul><p>To address issues of quality control:</p><ul><li>Researchers must declare their methods of data management to ensure errors have not been made. There must be a formal body or reporting mechanism responsible for oversight.</li><li>The experience of the private sector in big-data analytics needs to be leveraged for advanced clinical informatics.</li><li>The synthetic-data agenda must be taken with a pinch of salt: if the primary role of emergent technologies is to automate our second medical opinions, they cannot be based upon flawed reference-data sets.</li><li>Actors such as the Health Research Authority, NHS Digital and the UK Health Security Agency must catalogue and promote synthetic-data sets for those requiring instant access to training-set data.</li><li>Researchers are not to assume technology-enabled insights are superior to those collected from the field.</li></ul><p>To address reidentification risks:</p><ul><li>Government actors must sanction public consultations to confirm an acceptable risk threshold for data reidentification.</li><li>Researchers need to conduct regular ethical audits and motivated intruder tests to ensure prior approvals and security measures have not been breached.</li></ul><p>To reduce bureaucratic burden:</p><ul><li>Health-care decision-makers must include clinician burden and other human-centred metrics when evaluating new tech-enabled solutions for the NHS.</li><li>The private sector must aim to design or leverage products and technologies that reduce the bureaucratic burden of clinicians, prioritise and present only the most salient information for their consideration, automate a second medical opinion when a potential oversight has been detected and prevent runaway expenditure.</li></ul><p>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;***</p><p>&nbsp;</p><h3><strong>This Time It\u2019s Personal&nbsp;</strong></h3><p>The centralised nature of the NHS, which boasts digital assets such as unique patient IDs, unified health records and ample opportunities for onward data linkage, puts it at an advantage for achieving more personalised forms of health care. Patients could be able to see everything from their personal medical history to their genotypic medical ancestry incorporated into clinical decision-making. Such bespoke offerings improve health outcomes while tackling inequalities, ensuring medical misogyny, racism and ableism do not persist.</p><p>However, there must be considerable reform to the UK\u2019s health-data landscape for the NHS to unlock its potential in this regard. This paper presents four data challenges that are obstructing both the personalised-medicine agenda, as well as the actions that are capable of addressing them. These actions do not amount to tinkering around the edges or a \u201ctech will fix it\u201d complacency; they demand sincere cultural overhaul and sustained investment to back both the people who help us and the resources that help them.</p><p>Implementing these changes will not be easy and may in fact prove disruptive at first. However, we must trust in the positive externalities that only become visible with time \u2013 knowing that savings generated from avoided events and continuity of care are harder to quantify than those resulting from outsourcing care to private vendors or shaving minutes from already threadbare consultation times.</p><p>&nbsp;</p><p>*</p><p>&nbsp;</p><h3><strong>Data Disconnected</strong></h3><p>Health data are inherently multimedia, ranging from medical imaging to free-text information. This complexity is exacerbated by missing data, errors at the point of data entry and the diversity of clinical codification systems used to allocate numerical codes to medical events and prescriptions. Significant time is therefore invested into cleaning and harmonising disparate data sets into a common working language. Even so, some important data types for the personalised-medicine agenda, including data generated by remote monitoring devices, remain almost <a href=\"https://www.healthcarefacilitiestoday.com/posts/Medical-device-integration-with-electronic-health-records-is-not-plug-and-play--608\"><u>impossible to integrate</u></a> into medical records due to technical incompatibilities.</p><p>Data access or sharing is also constrained by extensive regulations and security-clearance requirements. While useful for obstructing bad-faith actors, these restrictions <a href=\"https://doi.org/10.23889/ijpds.v4i1.1093\"><u>curtail</u></a> high-priority research. The time taken for researchers to navigate access requests can exceed granting periods, for example. These application procedures are also unclear, inconsistent, and dependent on multiple gatekeepers, each with their own set of expectations. The changeability of regulatory requirements also makes research and development precarious: work funded prior to even minor amendments are subject to auditing or recontracting. This instability disincentivises the public-private-academic partnerships that are best placed for high-impact and high-trust research. Until this legal liability is hedged in some way, or long-term grants are capable of responding to regulatory changes, the longitudinal, interdisciplinary and large-scale research that personalised medicine relies upon will be disrupted or deprioritised in favour of safer bets.</p><p>All this makes for a health-data landscape that is not so much siloed as it is bordered: much like international travel in a pandemic, for data to be accessed, transferred or linked, it must first overcome changing data-protection policies and foreign clinical languages upon arrival. The UK Health Data Alliance\u2019s \u201cTube Map\u201d \u2013 a visualisation of the <a href=\"https://www.hdruk.ac.uk/news/using-linked-data-for-research-challenges-and-opportunities-within-the-uk-health-ecosystem\"><u>convoluted pathways</u></a> to data approval \u2013 makes this parallel especially apt. Such incoherence undermines the interoperability of health-data research and our chances of personalising health-care delivery. Meanwhile, overly narrow access permissions obstruct discovery. In a more collaborative data ecosystem, by comparison, unforeseen findings, like the predictive value of <a href=\"https://doi.org/10.1038/s41551-018-0195-0\"><u>diabetic retinopathy</u></a> for cardiovascular disease, might become the everyday, powering us towards personalised medicine in the process.</p><p>The cost of doing nothing to address this disconnect is too great: overbearing data restrictions can be frustrating to the point of insulting to our sickest patients, especially those volunteering their data to see their lives extended or even saved. For example, research by the Brain Tumour Charity found that nearly all patients (98 per cent of those surveyed) would be happy to give their medical health data to improve brain-tumour care, with nearly 99 per cent of this subset still willing to do so despite knowing the reidentification risk would be high. While privacy remains the bedrock of health-data stewardship \u2013 and it is clear that public <a href=\"https://doi.org/10.1136/bmjopen-2021-057579\"><u>attitudes</u></a> towards sharing data (even among clinicians) still remain mixed \u2013 this example demonstrates that working to honour patient wishes for their data must be at the forefront of medical ethics. When patients give their time, trust and even tissue, it must be acted upon with urgency and integrity \u2013 not garrotted by bureaucracy without good cause.</p><p>Going forwards:</p><ul><li>Consistency must be improved between different types of data-access requests and sharing protocols.</li><li>The TRE or Secure Data Environment (SDE) agenda (providing approved researchers with remote access to a single, secure location for specific health-data sets) must maintain its momentum to simplify the NHS data landscape.</li><li>TRE or SDEs must be audited by an official body to ensure their activities reduce data monopolies and gatekeeping.</li><li>Accredited TREs must collaborate and harmonise via common working practices (including open code and common data and governance models) to facilitate data access and processing within or between them.</li><li>The data freedoms instituted under the <a href=\"https://digital.nhs.uk/coronavirus/coronavirus-covid-19-response-information-governance-hub/control-of-patient-information-copi-notice\"><u>Control of Patient Information (COPI) notices</u></a> during the Covid-19 pandemic need to be <a href=\"https://www.digitalhealth.net/2022/06/what-does-the-end-of-copi-mean-for-digital-health-innovation\"><u>maintained</u></a>, with a view to ringfence deidentified health data away from generalist constraints in the future (for example, GDPR).</li><li>The Office of National Statistics\u2019 <a href=\"https://blog.ons.gov.uk/2017/01/27/the-five-safes-data-privacy-at-ons\"><u>Five Safes Framework</u></a> can tailor forthcoming regulatory changes based on data sensitivity and the opportunities and risks inherent to its usage.</li></ul><p>If actioned, these recommendations will help secure the coherent and collaborative research environment that is integral to personalised medicine.</p><p>&nbsp;</p><p>*</p><p>&nbsp;</p><h3><strong>Protecting Quality and the Qualitative</strong></h3><p>\u201cBad data in, bad data out\u201d is the defining mantra of data science. Despite the growing sophistication of our computational abilities, it remains difficult to rehabilitate input data that is not reflective of the problem we are trying to solve, the outcome we are trying to predict or the population we are trying to learn about.</p><p>The high stakes of health care make low-quality data potentially life threatening. Stubborn inaccuracies and instances of unrepresentativeness, incompleteness and inconsistency have affected the value of UK health-data sets for clinical research. For example, there is currently no formal oversight for how research groups collect, clean, curate (defining and constructing health variables or events), analyse or interpret their data. This has led to major embarrassments when such data are the sole contributor to sensitive decision-making; the prioritisation of healthy individuals for vaccination due to <a href=\"https://www.bbc.co.uk/news/uk-england-merseyside-56111209\"><u>wildly inaccurate BMI data entry</u></a> and the loss of tens of thousands of patient results due to <a href=\"https://slate.com/technology/2020/10/u-k-covid-19-spike-caused-by-microsoft-excel-error.html\"><u>Microsoft Excel spreadsheet limitations</u></a> are just two Covid-specific examples of this.</p><p>Furthermore, at present, health data fails to capture the profiles, experiences and health risks of certain demographics. This is a result of <a href=\"https://www.kingsfund.org.uk/publications/ethnicity-coding-health-records\"><u>poor codification</u></a> (capturing the UK\u2019s myriad ethnicities in a manner acceptable to all groups is an ongoing challenge) and differential levels of engagement with health-care services. Comorbidity scores for complex patients also do little to untangle precise health needs, and our heavy reliance on aggregate terms such as \u201cBAME\u201d or \u201cimmunocompromised\u201d has the unintended consequence of erasing subgroup phenomena. All this sees the inverse care law replicated in the UK\u2019s health-data landscape whereby health provision, or in this instance data representativeness and completeness, occurs in inverse proportion to need.</p><p>This leads to \u201cevidence-based\u201d insights that cannot always be trusted or generalised \u2013 to the extreme detriment of the personalised-medicine agenda. Even reference-data sets can be problematic benchmarks. Census data, considered the gold standard, are themselves vulnerable to misrepresentation, especially in exceptional circumstances (as seen during the <a href=\"https://www.theguardian.com/uk-news/2021/mar/14/danny-dorling-emergency-census-2026-true-state-uk\"><u>pandemic</u></a>) or when providing data on low-engagement groups.</p><p>Although sometimes portrayed as a cure-all for data-quality concerns, synthetic data (which are artificially generated to imitate real-world reference sets) have their own limitations. Far from rectifying flaws in reference-data sets, <a href=\"https://royalsociety.org/blog/2022/05/synthetic-data\"><u>synthetic data</u></a> have been seen to amplify inaccuracies or smooth over the abnormalities and biases that would otherwise be interrogated as subtrends. Synthetic data finds itself on especially shaky ground when used to model outcomes in those with complex or rare conditions; here there are insufficient sampling numbers or training data to model distributions or predict outcomes confidently. Moreover, the fidelity of these synthetic-data sets is not subject to review and there is no accreditation system to differentiate a robust imitation from a weak one. All this makes it unlikely that <a href=\"https://blogs.gartner.com/andrew_white/2021/07/24/by-2024-60-of-the-data-used-for-the-development-of-ai-and-analytics-projects-will-be-synthetically-generated\"><u>predictions</u></a> for synthetic data-dominated research will be realised in health care any time soon. Until synthetic data are sophisticated enough to act as a remedy or replacement for poor-quality health data, the focus must be on promoting synthetic-data sets as alternatives for researchers waiting to access the real-world data they are based on. A disappointing consolation prize for some, these become a pragmatic choice in fast-evolving health emergencies (where population-level data are unavailable) or for those unlikely to obtain the security clearances necessary for handling real-world data.</p><p>Finally, it is important to emphasise that our appetites for automation must not come at the expense of routine public-engagement efforts. Sincere and sustained local outreach, consultation and public-health campaigns can achieve as much for health-data improvement as emerging technologies \u2013 especially when collaborating with trusted community or celebrity figures to address trust deficits.</p><p>Going forwards:</p><ul><li>Either an existing official body takes responsibility for the oversight of data cleaning and variable curation or a new one is established; in conjunction with the Open Science Agenda, researchers must publish their work in these areas and subject themselves to audit where necessary.</li><li>Researchers must utilise best practice from private-sector big-data analytics to interrogate subtrends in artificially aggregated groups such as \u201cBAME\u201d or \u201cthe immunosuppressed\u201d, particularly those that affect the underrepresented or clinically vulnerable.</li><li>Public-health campaigns and awareness raising must still be prioritised, especially when it comes to promoting the value of sharing health data for the NHS.</li><li>Once actioned, these recommendations will see that only the right data informs treatment, policy and even personal lifestyle choices.</li></ul><p>&nbsp;</p><p>*</p><p>&nbsp;</p><h3><strong>The Big Reveal</strong></h3><p>The terms anonymised, pseudonymised and encrypted are often used interchangeably despite the fact that they relate to very different masking procedures. For clarity: data are only said to be anonymised if reidentification is made impossible. Pseudonymisation, by comparison, still runs some risk of reidentification, as personal information can be revealed by accessing the original encryption scheme or by cross referencing to additional data sets with overlapping features. The latter process, known as triangulation, can see reidentification occur completely by accident: <a href=\"https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/\"><u>the reidentification of Netflix subscribers\u2019 personal information</u></a> using IMDb data is one well-known example of this.</p><p>Although it might be tempting to dismiss the risk of reidentified health data, it\u2019s important to contextualise just how devastating its consequences can be when data fall into the wrong hands. While we can all appreciate the ammunition that the medical record of a political leader or CEO might amount to, we must not underestimate the prospective harms of data terrorism for the general public. <a href=\"https://www.theguardian.com/australia-news/2022/nov/10/abortion-data-from-medibank-hack-posted-on-dark-web-as-clare-oneil-pledges-to-pursue-scumbags\"><u>The widespread leak of Australian health records</u></a> as part of a recent ransomware attack is the perfect, and tragic, example of this. Patients had their most sensitive health information cemented into public record, including incidences of miscarriage, abortion and other procedures or diagnoses they might have otherwise wished to conceal from family, friends and employers.</p><p>However, by scrubbing data of its distinguishing features we also remove its prospects for personalising care. Thus, although the risks of reidentification must be mitigated to maintain the good faith of the public, efforts to do so must not disrupt the data linkages, sophisticated analytics and resultant innovations from which the public also benefits.</p><p>Going forwards:</p><ul><li>Opportunities must be created for dialogue between the research community, private sector and public to determine an acceptable risk threshold for reidentification.</li><li>Researchers must conduct regular motivated intruder tests to establish whether a pseudonymised health-data set is secure against reidentification and new methods of attack. This test interrogates encryption and firewalls at the level of a data-competent individual with sufficient motivation to reidentify information for malicious purposes. This test could be part of a wider package of ethical audit to ensure that initial approvals have not been out scoped.</li><li>Inspired by Google\u2019s use of <a href=\"https://modelcards.withgoogle.com/about\"><u>Model Cards</u></a> to publicly evaluate their machine-learning algorithms, a central repository for assigning \u201creidentification risk scores\u201d to major health-data sets needs to be developed. Researchers could then their intended configuration of health-data sets into a portal to return an estimate of how likely it to triangulate to reveal sensitive information.</li></ul><p>Collectively, these recommendations create the foundation of security measures and a ceiling for risk acceptability that the UK health-data landscape needs before it can pursue personalised medicine in earnest.</p><p>&nbsp;</p><p>*</p><p>&nbsp;</p><h3><strong>The Blight of Bureaucracy</strong></h3><p>Ensuring care remains responsive and personable at the patient level is as important to the personalised-medicine agenda as access to the diverse and joined-up data sets that improve patient\u2013treatment matching. Actions taken under this remit must therefore aim to be relationship enhancing and not relationship eroding.</p><p>However, in 2020, the BBC <a href=\"https://www.bbc.co.uk/news/health-50972123\"><u>reported</u></a> that a revolving door of digital innovation and corresponding digital redundancy in the NHS had left doctors logging into as many as 15 different systems in a day to perform their work. These digital reforms can quickly amount to a \u201ctechnological paradox\u201d in settings as sensitive as the NHS\u00ad \u2013 a term that describes the seen and unseen ways in which the costs of implementing new technologies (training staff, debugging inevitable crashes and ensuring interoperability) outprice the savings accrued.</p><p>One such negative externality is the way in which <a href=\"https://www.researchgate.net/publication/321518527_Distracted_Doctoring_Returning_to_Patient-Centered_Care_in_the_Digital_Age\"><u>new digital tools can create distance and disconnect between health-care professionals and their patients</u></a>. For example, the upkeep of electronic health records is currently dependent on manual data entry by those on the front lines of care. Such duties take doctors away from their patients or compromise their ability to be present in consultations. This corrodes the personability of care.</p><p>This is a phenomenon we can all relate to. We all appreciate how damaging technologies can be to both our concentration and sense of closeness with one another. The difference, however, is the fatal consequences such distractibility and disconnection can have in medical settings. With heads buried in screens for as much as a third of in-person consultation time and the full duration of telemedical appointments, doctors are less likely to pick up on subtle safeguarding cues, such as long sleeves in an unseasonably hot day or the tremor that can act as the smoking gun to everything from Parkinson\u2019s to withdrawal.</p><p>This new bureaucratic burden is as dissatisfying for patients as it is for physicians: prioritising digital-centred care over human-centred care can leave patients feeling unheard or dismissed outright as conversational flow is interrupted to prioritise data capture. Mechanical forms of medical consultation undermine the doctor-patient relationship that has attracted talent to the profession and justifies physicians\u2019 gruelling work environment. They also compromise the trust patients need to feel safe to share personal information or subject themselves to physical examination. Such things cannot be automated.</p><p>Going forwards:</p><ul><li>Digital reform in the NHS must not increase bureaucratic duties for health-care workers: new offerings must either reduce clinicians\u2019 workload directly or allocate sufficient funding for the clinical-support staff and in-house data talent that will take responsibility for their ongoing success.</li><li>Depending on patient and clinician acceptability and consent, new ways of integrating large language models and natural-language processing methods into the clinical workflow must be found. Here, clinical communications and referral letters could be outsourced to products such as ChatGPT \u2013 generating efficiency savings and reducing overall pressures on care. Meanwhile, consultation recordings that are parsed into constituent codes or even personalised bedside manner statistics (turn-taking, interrupting and so on) could release clinicians from burdensome data-entry responsibilities and improve patient satisfaction by extension.</li><li>The NHS is an institution built upon relationships, not transactions. These recommendations aim to honour this and, if actioned, would allow the NHS to leverage the opportunities technology offers without compromising the human-centred care that is at the heart of patient satisfaction.</li></ul><p>&nbsp;</p><p>*</p><p>&nbsp;</p><h3><strong>Conclusion</strong></h3><p>Interrelated issues of data disconnection, quality concerns, reidentification risks and bureaucratic burden are obstructing the UK\u2019s potential for personalised medicine. Nevertheless, the data assets, capabilities and infrastructure that are still mostly unique to the UK health-care system give it an exceptional advantage in this domain \u2013 especially compared to fully privatised comparators. If the recommendations made in this paper are acted upon with urgency, there is every chance we might realise a personally prescriptive NHS in our lifetimes.</p><p>However, data is not a cure-all, especially not from the patient perspective, and medicine can never be a purely algorithmic affair. Patients display stubborn bias for in-person care. Their self-perceived exceptionalism, namely that their unique needs could not possibly be met by an artificial intelligence, makes them averse to these offerings even when presented with <a href=\"https://hbr.org/2019/10/ai-can-outperform-doctors-so-why-dontpatients-trust-it\"><u>incontrovertible evidence</u></a> that they produce better outcomes. Brick-and-mortar general practice will always be preferred to remote consultation, and the value of the holistic \u2013 via offerings including social prescribing and <a href=\"https://www.kingsfund.org.uk/sites/default/files/field/field_document/continuity-care-patient-experience-gp-inquiry-research-paper-mar11.pdf\"><u>continuity of care</u></a> \u2013 should not be dismissed.</p><p>In sum, the pursuit of personalising medicine cannot compromise what makes health care feel personal for patients. Data reforms must not be motivated to \u201chack\u201d our human-centred preferences but should instead work with them. This will ensure the relationships that distinguish health care from health service are enhanced by technology, not replaced by it. In this light, new products and services become helpful tools for the doctor\u2019s medicine bag and can offer the second medical opinion that assures safe practice for all \u2013 without becoming the only voice in the consultation room.</p><p>&nbsp;</p><p><br>&nbsp;</p><p><br>&nbsp;</p><p><br>&nbsp;</p><p><br><br>&nbsp;</p><p><br><br>&nbsp;</p>", "user": {"username": "MeredithLeston"}}, {"_id": "fWGdsWbS6vtC9E7ii", "title": "EA & LW Forum Weekly Summary (6th - 12th March 2023)", "postedAt": "2023-03-14T03:01:06.162Z", "htmlBody": "<p>This is part of a weekly series summarizing the top posts on the EA and LW forums - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology. Feedback, thoughts, and corrections are welcomed.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: Subscribe on your favorite podcast app by searching for 'EA Forum Podcast (Summaries)'. A big thanks to Coleman Snell for producing these!</p><p>&nbsp;</p><h1>Philosophy and Methodologies</h1><p><a href=\"https://forum.effectivealtruism.org/posts/kCBQHWqbk4Nrns8P7/model-based-policy-analysis-under-deep-uncertainty\"><strong><u>Model-Based Policy Analysis under Deep Uncertainty</u></strong></a></p><p><i>by Max Reddel</i></p><p>The author explains how policy researchers can support decision-making with simulation models of socio-technical systems, even under deep uncertainty.<br><br>They first suggest systems modeling (eg. agent-based models). For example, agent-based modeling was used&nbsp;<a href=\"https://prajwalsouza.github.io/Experiments/Epidemic-Simulation.html\"><u>here</u></a> to simulate how different individuals with different characteristics (age, health status, social network) might behave during an epidemic, and how that would affect spread and the relative effectiveness of different interventions.</p><p>However, many political decisions have even less certainty. \u2018Deep uncertainty\u2019 is uncertainty on the system model, the probability distributions over inputs to them, and which consequences to consider and their relative importance. In this scenario, computational modeling can be helpful to explore the implications of different assumptions about uncertain / contested / unknown model parameters and mechanisms. The aim is to minimize plausible future regret via finding vulnerable scenarios and policy solutions that are robust to them - instead of predicting expected effect. They provide several techniques and examples for this.</p><p>&nbsp;</p><h1>Object Level Interventions / Reviews</h1><h2>AI</h2><p><a href=\"https://www.lesswrong.com/posts/KQfYieur2DFRZDamd/why-not-just-build-weak-ai-tools-for-ai-alignment-research\"><strong><u>Why Not Just... Build Weak AI Tools For AI Alignment Research</u></strong><u>?</u></a></p><p><i>by johnswentworth</i></p><p>The author argues there\u2019s room for reasonably-large boosts to alignment research from \u201cweak\u201d cognitive tools like Google search. The problem is that the majority of people looking at this intervention don\u2019t have experience with the hard parts of alignment research, which would help them understand the needs and direct tools toward the most helpful elements. They suggest those interested do some object-level alignment research, then pick a few people they want to speed up and iteratively build something for them specifically.</p><p>They also provide a few early ideas of what such research tools might look like eg. a tool which produces examples, visuals or stories to explain inputted mathematical equations, or a tool which predicts engagement level and common objections to a given piece of text.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/uGDCaPFaPkuxAowmH/anthropic-core-views-on-ai-safety-when-why-what-and-how\"><strong><u>Anthropic: Core Views on AI Safety: When, Why, What, and How</u></strong></a><i> by jonmenaster&nbsp;</i>and&nbsp;<a href=\"https://www.lesswrong.com/posts/xhKr5KtvdJRssMeJ3/anthropic-s-core-views-on-ai-safety\"><strong><u>Anthropic's Core Views on AI Safety</u></strong></a><i><strong> </strong>by Zac Hatfield-Dodds</i></p><p>Linkpost to Anthropic\u2019s post&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fwww.anthropic.com%2Findex%2Fcore-views-on-ai-safety\"><u>here</u></a>, which discusses why they anticipate rapid AI progress and impacts, how this led them to be concerned about AI safety, and their approach to AI safety research.</p><p>Key points include:</p><ul><li>Research on scaling laws demonstrates more computation leads to improvements in capabilities. Via extrapolation, we should expect great leaps in AI capabilities and impact.</li><li>No-one knows how to achieve powerful, helpful, honest, harmless AI - but rapid AI progress may lead to competitive racing and deployment of untrustworthy systems. This could lead to catastrophic risk from AI systems strategically pursuing dangerous goals, or making innocent mistakes in high-stakes situations.</li><li>They\u2019re most excited about and pursuing research into scaling supervision, mechanistic interpretability, process-oriented learning, understanding and evaluating how AI systems learn and generalize, testing for dangerous failure modes, and societal impacts and evaluations.</li><li>Their goal is to differentially accelerate safety work, and have it cover a wide range of scenarios, from those where safety challenges turn out to be easy to address to those where they turn out to be very difficult. In the very difficult scenario, they see their role as to sound the alarm and potentially channel collective resources into temporarily halting AI progress. However, they aren\u2019t sure what scenario we\u2019re in and hope to learn this.</li></ul><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/a9SPcZ6GXAg9cNKdi/linkpost-some-high-level-thoughts-on-the-deepmind-alignment\"><strong><u>[Linkpost] Some high-level thoughts on the DeepMind alignment team's strategy</u></strong></a></p><p><i>by Vika, Rohin Shah</i></p><p>Linkpost for&nbsp;<a href=\"https://www.lesswrong.com/out?url=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F1DVPZz0-9FSYgrHFgs4NCN6kn2tE7J8AK%2Fview%3Fusp%3Dsharing\"><u>some slides</u></a> on AI threat models and their alignment plans by DeepMind\u2019s alignment team. It doesn\u2019t represent / isn\u2019t endorsed by DeepMind as a whole.</p><p>Key points include:</p><ul><li>They believe the most likely source of AI x-risk is a mix of specification gaming and goal misgeneralization, leading to a misaligned and power-seeking consequentialist that becomes deceptive. (SG + GMG -&gt; MAPS)</li><li>Their approach is broadly to build inner and outer aligned models, and detect models with dangerous properties.</li><li>Current research streams include process-based feedback, red-teaming, capability evaluations, mechanistic interpretability, goal misgeneralization understanding, causal alignment, internal outreach, and institutional engagement.</li><li>Comparative to OpenAI they focus more on mechanistic interpretability and capability evaluations, as well as using AI tools for alignment research. Scalable oversight is a focus for both labs.<br><br>&nbsp;</li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/fqXLT7NHZGsLmjH4o/paper-summary-the-effectiveness-of-ai-existential-risk\"><strong><u>Paper Summary: The Effectiveness of AI Existential Risk Communication to the American and Dutch Public</u></strong></a></p><p><i>by Otto</i></p><p>Summary of&nbsp;<a href=\"https://existentialriskobservatory.org/papers_and_reports/The_Effectiveness_of_AI_Existential_Risk_Communication_to_the_American_and_Dutch_Public.pdf\"><u>this paper</u></a> by Alexia Georgiadis from the Existential Risk Observatory. The study involved surveying 500 members of the American and Dutch public on the likelihood of human extinction from AI and other causes, before and after showing them specific news articles and videos. Key results:</p><ul><li>Depending on media used, 26% to 64% of participants rated AI higher on a ranked list of events that may cause human extinction after reading / watching. This effect may degrade over time.</li><li>This&nbsp;<a href=\"https://edition.cnn.com/videos/world/2014/12/02/ct-artificial-intelligence-james-barrat-intv.cnn\"><u>CNN video featuring Stephen Hawking</u></a> was the most effective at increasing the above ratings, of 10 articles and videos tested.</li><li>Widespread endorsement of government participation in AI regulation was found among individuals with heightened awareness (ie. high rankings / ratings) of AI risks.</li></ul><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/Zq2HaihaDy7sSarMz/how-bad-a-future-do-ml-researchers-expect\"><strong><u>How bad a future do ML researchers expect?</u></strong></a></p><p><i>by AI Impacts</i></p><p>Results of a 2022 survey asking ML researchers how they would divide probability over the future impacts of high-level machine intelligence, across 5 buckets. Average results were:</p><ul><li>Extremely good (eg. rapid growth in human flourishing): 24%</li><li>On balance good: 26%</li><li>More or less neutral: 18%</li><li>On balance bad: 17%</li><li>Extremely bad (eg. human extinction): 14%<br><br>&nbsp;</li></ul><p><a href=\"https://www.lesswrong.com/posts/PwfwZ2LeoLC4FXyDA/against-llm-reductionism\"><strong><u>Against LLM Reductionism</u></strong></a></p><p><i>by Erich_Grunewald</i></p><p>Linkpost for the author\u2019s&nbsp;<a href=\"https://www.lesswrong.com/out?url=https%3A%2F%2Fwww.erichgrunewald.com%2Fposts%2Fagainst-llm-reductionism%2F\"><u>blog post</u></a>, which argues against reductions of large language models (LLMs) that are sometimes used to imply they can\u2019t generalize much further. For instance, saying LLMs are \u2018just\u2019 pattern-matchers, or \u2018just\u2019 massive look-up tables. While there is some truth to those statements, there\u2019s empirical evidence that LLMs can learn general algorithms and contain and use representations of the world, even when only trained on next-token prediction. We don\u2019t know what capabilities can or cannot arise from this training and should be cautious about predicting its limits.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/3gAccKDW6nRKFumpP/why-not-just-outsource-alignment-research-to-an-ai\"><strong><u>Why Not Just Outsource Alignment Research To An AI?</u></strong></a></p><p><i>by johnswentworth</i></p><p>Argues that outsourcing alignment research to AI is like the comedy sketch \u2018<a href=\"https://youtu.be/BKorP55Aqvg\"><u>The Expert</u></a>\u2019 ie. if we don\u2019t understand the thing we\u2019re asking for, we can\u2019t expect a good result. The best case is the AI figures out what we need and does it anyway. They argue it's more likely it gives us something that only looks right, or attempts to fulfill our preferences but understands them wrong, or isn\u2019t capable of alignment research at all the way we\u2019ve prompted it. And because the core of these issues is that we don\u2019t realize something has gone wrong, we\u2019re unable to iterate. Though a top comment by Jonathan Paulson notes it\u2019s often easier to evaluate work than do it yourself.</p><p>The best solution to this issue is for us to develop more object-level alignment expertise ourselves, so we\u2019re better able to outsource (ie. direct and understand) the further alignment research.</p><p><br>&nbsp;</p><h2>Other Existential Risks (eg. Bio, Nuclear, Multiple)</h2><p><a href=\"https://forum.effectivealtruism.org/posts/KoLdSn4PLkWzE6SWT/global-catastrophic-risks-law-approved-in-the-united-states\"><strong><u>Global catastrophic risks law approved in the United States</u></strong></a></p><p><i>by JorgeTorresC, Jaime Sevilla, JuanGarcia, M\u00f3nica Ulloa, Claudette Salinas, Roberto Tinoco, daniela tiznado</i></p><p>Linkpost for&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Friesgoscatastroficosglobales.com%2Farticulos%2Fhito-histrico-es-aprobada-una-ley-de-gestin-de-riesgos-catastrficos-globales-en-eeuu\"><u>this announcement</u></a> of the US&nbsp;<a href=\"https://www.congress.gov/117/bills/hr7776/BILLS-117hr7776enr.pdf#page=1290\"><u>Global Catastrophic Risk Management Act</u></a>.&nbsp;<br><br>The law orders the United States government to establish actions for prevention, preparation, and resilience in the face of catastrophic risks - including presenting risk assessments and recommendations to congress. The recognized risks include: global pandemics, nuclear war, asteroid and comet impacts, supervolcanoes, sudden and severe changes in climate, and threats arising from the use and development of emerging technologies (such as artificial intelligence or engineered pandemics).</p><p><br>&nbsp;</p><h2>Global Health and Development</h2><p><a href=\"https://forum.effectivealtruism.org/posts/A9ExMYamqTycvFGAo/evidence-on-how-cash-transfers-empower-women-in-poverty\"><strong><u>Evidence on how cash transfers empower women in poverty</u></strong></a></p><p><i>by GiveDirectly</i></p><p>GiveDirectly shares study results and testimonials from recipients in Malawi on how direct cash empowers them. 62% of GiveDirectly\u2019s recipients are women. Cash transfers can increase use of health facilities, improve birth weight and infant mortality, reduce incidents of physical abuse by a male partner of a woman, increase girls\u2019 school attendance, increase a women\u2019s likelihood of being the sole or joint decision-maker, increase entrepreneurship, increase savings, and reduce the likelihood of illness.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/TiRPgfG4L8X2jt99g/how-oral-rehydration-therapy-was-developed\"><strong><u>How oral rehydration therapy was developed</u></strong></a></p><p><i>by Kelsey Piper</i></p><p>Linkpost for&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fasteriskmag.com%2Fissues%2F2%2Fsalt-sugar-water-zinc-how-scientists-learned-to-treat-the-20th-century-s-biggest-killer-of-children\"><u>an article</u></a> in&nbsp;<a href=\"https://asteriskmag.com/\"><u>Asterisk magazine</u></a> discussing why oral rehydration therapy, which is simple and uses widely available ingredients, took until the late 1960s to discover:</p><ul><li>Without a solid theoretical understanding of a problem, empirical solutions are difficult - people tried many variants with important parameters off, and didn\u2019t know how to correct them.</li><li>The simplicity of today\u2019s solution, and the availability of the required ingredients, is due to continual research and design efforts to get to that point.</li></ul><p><br>&nbsp;</p><h2>Other / Multiple</h2><p><a href=\"https://forum.effectivealtruism.org/posts/cHDz2R5FfWGoZgWoZ/after-launch-how-are-ce-charities-progressing\"><strong><u>After launch. How are CE charities progressing?</u></strong></a></p><p><i>by Ula Zarosa, CE</i></p><p>Charity Entrepreneurship has helped to kick-start 23 impact-focused nonprofits in four years. They estimate 40% of these reach or exceed the cost-effectiveness of the strongest charities in their fields (eg. GiveWell / ACE recommended).</p><p>Their seed network has provided $1.88 million in launch grants to date. The launched charities have then fundraised over $22.5 million from other grantmakers. This has provided the following impacts, among others:</p><ul><li>~14,000 additional children vaccinated (by&nbsp;<a href=\"https://www.charityentrepreneurship.com/suvita\"><u>Suvita</u></a>)</li><li>~1.14 million fish and ~1.4 million shrimp potentially helped (with potential to reach &gt;2.5 billion shrimp per annum) (<a href=\"https://www.charityentrepreneurship.com/fish-welfare-initiative\"><u>Fish Welfare Initiative</u></a> and&nbsp;<a href=\"https://www.charityentrepreneurship.com/shrimp-welfare-project\"><u>Shrimp Welfare Project</u></a>)</li><li>~250,000 new contraceptive users from a single campaign (<a href=\"https://www.charityentrepreneurship.com/family-empowerment-media\"><u>Family Empowerment Media</u></a>)</li><li>~215,000 children with reduced lead exposure (<a href=\"https://www.charityentrepreneurship.com/lead-exposure-elimination-project\"><u>Lead Exposure Elimination Project</u></a>)</li><li>Breakthrough papers on subjective well-being (<a href=\"https://www.charityentrepreneurship.com/happier-lives-institute\"><u>Happier Lives Institute</u></a>)</li></ul><p>You can apply to their program&nbsp;<a href=\"https://form.jotform.com/230292346528356\"><u>here</u></a>.<br><br>&nbsp;</p><h1>Opportunities</h1><p><a href=\"https://forum.effectivealtruism.org/posts/NZz3Das7jFdCBN9zH/announcing-the-open-philanthropy-ai-worldviews-contest\"><strong><u>Announcing the Open Philanthropy AI Worldviews Contest</u></strong></a></p><p><i>by Jason Schukraft, Peter Favaloro</i></p><p>Open Philanthropy announces the AI Worldviews Contest, with the aim to surface novel considerations that could influence their views on AI timelines and AI risk. Essays should address one of the following:</p><ul><li>What is the probability that AGI is developed by January 1, 2043?</li><li>Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an existential catastrophe due to loss of control over an AGI system?</li></ul><p>$225K in prizes will be distributed across six winning entries. Work posted for the first time on or after September 23rd 2022, and up until May 21st 2023, is eligible. See the post for details on eligibility,&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdADOikiiQgXLwdEJ2Mou5IQT6-9BH8zcZ0pjQzwex069jjjA/viewform\"><u>submission</u></a>, judging process and judging criteria.<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/zxrBi4tzKwq2eNYKm/ea-infosec-skill-up-in-or-make-a-transition-to-infosec-via\"><strong><u>EA Infosec: skill up in or make a transition to infosec via this book club</u></strong></a></p><p><i>by Jason Clinton, Wim van der Schoot</i></p><p>EA needs more skilled infosec folk. EA-aligned software engineers interested in becoming security engineering focused or accelerating their existing infosec career paths can sign up for the book club&nbsp;<a href=\"https://forms.gle/sYMunbTgh7ZeeYiNA\"><u>here</u></a>. It will involve facilitated fortnightly discussions starting April 1st 2pm PDT, by the lead of the Chrome Infrastructure Security team at Google. They\u2019ve used the book (available for free&nbsp;<a href=\"https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf\"><u>here</u></a>) as part of successfully transitioning engineers into security previously.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/qe3z5Yfqr2ZoAvWe4/suggest-new-charity-ideas-for-charity-entrepreneurship\"><strong><u>Suggest new charity ideas for Charity Entrepreneurship</u></strong></a></p><p><i>by CE, weeatquince</i></p><p>In 2023 Charity Entrepreneurship will be researching two new cause areas: mass media interventions and preventative animal advocacy. They\u2019re looking for&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSf7yo1Bd4ZK6_u2nziuM_10bYYoJhAMjp-9b_MKG2tMX9PuWA/viewform\"><u>submissions of ideas</u></a> in these areas, which may lead to a new charity around that idea being launched.</p><p><br>&nbsp;</p><h1>Community &amp; Media</h1><p><a href=\"https://forum.effectivealtruism.org/posts/tedrwwpXgpBEi3Ecc/80-000-hours-two-year-review-2021-2022\"><strong><u>80,000 Hours two-year review: 2021\u20132022</u></strong></a></p><p><i>by 80000_Hours</i></p><p>Linkpost and summary for 80,000 Hours&nbsp;<a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2F80000hours.org%2F2023%2F03%2F80000-hours-two-year-review-2021-and-2022%2F\"><u>review of their programmes</u></a> in 2021 and 2022.</p><p>They\u2019ve seen 2-3x higher engagement for 2022 vs. 2020 in 3 out of 4 main programmes: podcast listening time, job board vacancy clicks, and number of 1-1 calls. The fourth, web engagement, fell 20% in 2021 and rose 38% in 2021 after marketing investment.&nbsp;</p><p>The team grew by 78% to 25 FTEs and Howie Lempel became the new CEO. 2023 focuses include improving quality of advice (partially via hiring a senior research role), growing the team ~45%, continuing to support the four main programmes and experimenting with new ones such as headhunting and an additional podcast.<br><br><br><a href=\"https://forum.effectivealtruism.org/posts/mLua7KbJRbXa6oeZ3/more-centralisation\"><strong><u>More Centralisation?</u></strong></a></p><p><i>by DavidNash</i></p><p>EA is highly decentralized, with a small set of organisations / projects with 50+ people, some with 5-20, and most with 1-2. This means many people lack organizational support like a manager, colleagues to bounce ideas off, ops support, and stable income. As a movement, it pushes away people with lower risk tolerance, leads to duplication of operations / administrative work, and to worse governance. The author suggests:</p><ul><li>Organisations with good operations and governance support other projects eg. what is done by Rethink Priorities\u2019&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AFgvA9imsT6bww8E3/announcing-the-rethink-priorities-special-projects-program\"><u>Special Projects Program</u></a>.</li><li>Programs mainly aimed at giving money to individuals be converted into internal programs eg. like Charity Entprenuership\u2019s&nbsp;<a href=\"https://www.charityentrepreneurship.com/incubation-program\"><u>incubation programme</u></a>, or the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/e8CXMz3PZqSir4uaX/what-fhi-s-research-scholars-programme-is-like-views-from-1\"><u>Research Scholars Program</u></a>.</li><li>A top comment by Hauke Hillebrandt also suggests exploring mergers and acquisitions.</li></ul><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/RarQnPKCx4KkhoLEx/3-basic-steps-to-reduce-personal-liability-as-an-org-leader\"><strong><u>3 Basic Steps to Reduce Personal Liability as an Org Leader</u></strong></a></p><p><i>by Deena Englander</i></p><p>There are easy steps even small EA orgs should take to substantially reduce personal liability:</p><p>1. Incorporate (LLCs are easy and inexpensive to start).</p><p>2. Get your organization its own bank account.</p><p>3. Get general liability insurance (for the author it costs ~$1.3K per year, but even one lawsuit can bankrupt you otherwise).</p><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/EQPY63WzxmqQnWsmS/suggestion-a-workable-romantic-non-escalation-policy-for-ea\"><strong><u>Suggestion: A workable romantic non-escalation policy for EA community builders</u></strong></a></p><p><i>by Severin</i></p><p>The&nbsp;<a href=\"https://www.authrev.org/\"><u>Authentic Revolution</u></a>, an organization that runs facilitated experiences like workshops, has a policy that: \u201cfor three months after a retreat, and for one month after an evening event, facilitators are prohibited from engaging romantically, or even hinting at engaging romantically, with attendees. The only exception is when a particular attendee and the facilitator already dated beforehand.\u201d The author suggests EA community builders should consider something similar, and suggests ways of adapting it to different settings.</p><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/gLJBfruDrKQDkbf2b/racial-and-gender-demographics-at-ea-global-in-2022-1\"><strong><u>Racial and gender demographics at EA Global in 2022</u></strong></a></p><p><i>by Amy Labenz, Angelina Li, Eli_Nathan</i></p><p>Results from initial analysis by CEA on how people of different genders and racial backgrounds experienced EAG events in 2022 (including EAGx). Key results:</p><ul><li>33% of attendees, 35% of applicants, and 43% of speakers / MCs self-reported as female or non-binary.</li><li>33% of attendees, 38% of applicants and 28% of speakers / MCs self-reported as people of color.</li><li>Welcomingness and likelihood to recommend survey scores were very similar for women and POC to overall scores (with a small decline of 0.1 on a 5-point scale for welcomingness for women).</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/WgziByhhKGDfuEgyy/share-the-burden\"><strong><u>Share the burden</u></strong></a></p><p><i>by 2ndRichter</i></p><p>Asks the community to be proactive in addressing sexual misconduct and the dynamics that influence it, so the burden of pushing forward change doesn\u2019t fall on survivors. Often a survivor will engage in a process that hurts them and takes considerable time / effort (eg. repeated explanations of trauma, arduous justice processes) in order to make it less likely the perpetrator does something similar again. The author advocates an alternative of allocating collective effort to creating protective norms, practices, and processes to take care of those affected and encourage the behaviors we want in the future. Input from survivors is still necessary and important, but ask for it with care and gentleness, and spend significant time thinking and acting on what you hear.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/DztcCwrAGo6gzCp3o/on-the-first-anniversary-of-my-best-friend-s-death\"><strong><u>On the First Anniversary of my Best Friend\u2019s Death</u></strong></a></p><p><i>by Rockwell</i></p><p>The author shares reflections on their friend Alexa\u2019s life, who acted on compassion fearlessly and consistently to aid an incredible number of people and animals in their 25-year life.</p><p>\u201cThe challenges that the world faces are vast, and, frequently, overwhelming. The number of lives on the line is hard to count. Sometimes, it all feels like a blur - abstract, and so very far away. And because of that vastness, we can easily lose track of the magic and power of one life, one person\u2019s world.</p><p>Alexa walked into that vastness, arms outstretched, and said, \u201cI can help you. And you. And you. I can\u2019t help you all. But I will try.\u201d\u201d</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/hn4aJdaCwGHfkB8se/against-ea-community-received-wisdom-on-practical\"><strong><u>Against EA-Community-Received-Wisdom on Practical Sociological Questions</u></strong></a></p><p><i>by Michael_Cohen</i></p><p>The author argues that there is evidence that aggregating the estimates of many produces a more accurate estimate as the number grows. They suggest this means for most practical sociological questions, you should assume the conventional answer is correct. In practice, this means most proposals for new norms of relating to others, organizational structures etc. should be rejected out of hand if they a) don\u2019t have an airtight first-principles argument and b) don\u2019t match conventional wisdom.&nbsp;<br><br>This also means putting weight in existing methods such as peer review, which is highly respected as a method of collaborative truth-seeking. They suggest more people should publish in journals vs. on the forum. Similarly they suggest putting more weight on experience, like society at large does. They guess that hiring highly experienced staff would have prevented fraud at FTX.</p><p>For individual EAs, they suggest deferring on technical topics to those with conventional markers of expertise (unless you are yourself an expert), and considering how you can do the most good in conventional professions for changing the world (eg. professor, politician, lobbyist).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/7b9ZDTAYQY9k6FZHS/abuse-in-lesswrong-and-rationalist-communities-in-bloomberg\"><strong><u>Abuse in LessWrong and rationalist communities in Bloomberg News</u></strong></a></p><p><i>by whistleblower67</i></p><p>Linkpost for&nbsp;<a href=\"https://www.bloomberg.com/news/features/2023-03-07/effective-altruism-s-problems-go-beyond-sam-bankman-fried?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTY3ODIwNjY2MiwiZXhwIjoxNjc4ODExNDYyLCJhcnRpY2xlSWQiOiJSUjVBRzVUMEFGQjQwMSIsImJjb25uZWN0SWQiOiIzMDI0M0Q3NkIwMTg0QkEzOUM4MkNGMUNCMkIwNkExNiJ9.nbOjP4JQv-TuJwoXaeBYhHvcxYGk0GscyMslQFL4jfA\"><u>this article</u></a> by Bloomberg, which discusses reported cases of sexual misconduct and abuse, and possible contributing factors in terms of rationality community culture.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/Riqg9zDhnsxnFrdXH/nick-bostrom-should-step-down-as-director-of-fhi\"><strong><u>Nick Bostrom should step down as Director of FHI</u></strong></a></p><p><i>by BostromAnonAccount</i></p><p>Suggests Nick Bostrom should step down from Director to Senior Research Fellow at FHI, due to:</p><ul><li>Management concerns at FHI eg. large turnover, and a freeze on hiring.</li><li>Lack of tact in Bostrom\u2019s&nbsp;<a href=\"https://nickbostrom.com/oldemail.pdf\"><u>apology for an old email</u></a> post.</li><li>Effects of the above on relationships with the university, staff, funders, and collaborators.</li></ul><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/22ZmeTjnooBGBMce4/selective-corrective-structural-three-ways-of-making-social\"><strong><u>Selective, Corrective, Structural: Three Ways of Making Social Systems Work</u></strong></a></p><p><i>by Said Achmiz</i></p><p>To accomplish a goal requiring certain behaviors, a social system can use any combination of three methods. In brackets are examples (if your goal was building a successful organization):</p><ol><li>Selective methods - build the system out of only people who will do those behaviors (eg. hire people with good skills).</li><li>Corrective methods - apply methods to alter their behavior (eg. training).</li><li>Structural methods - build the system so it works if people behave in the ways you expect they will (eg. performance incentives, technological improvements to reduce skill requirements).</li></ol><p>The author also provides examples for assembling a raid guild in World of Warcraft, and for ensuring good governance.</p><p>&nbsp;</p><p>&nbsp;</p><h1>Special Mentions</h1><p><i>A selection of posts that don\u2019t meet the karma threshold, but seem important or undervalued.</i></p><p><a href=\"https://www.lesswrong.com/posts/sMZRKnwZDDy2sAX7K/google-s-palm-e-an-embodied-multimodal-language-model\"><strong><u>Google's PaLM-E: An Embodied Multimodal Language Model</u></strong></a></p><p><i>by SandXbox</i></p><p>Linkpost to&nbsp;<a href=\"https://palm-e.github.io/\"><u>this paper and demo</u></a> by Google. PaLM-E is a large model trained on multiple modalities (internet-scale language, vision, and visual-language domains). It exhibits positive transfer (ie. learning in one domain helps in another) and the connected robot is able to successfully plan and execute tasks like \u201cbring me the rice chips from the drawer\u201d or \u201cpush green blocks to the turtle\u201d even when it has never seen a turtle before in real life.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/6LwxzbiqNcXfXeb7K/a-short-post-on-long-term-marketing-and-why-you-should\"><strong><u>A short post on long-term marketing; and why you should consider ROI/ROAS less</u></strong></a></p><p><i>by James Odene [User-Friendly]</i></p><p>In industry, it\u2019s common to dedicate ~60% of marketing budget to long-term marketing (eg. building brand), and ~40% to short-term activations (encouraging specific actions now).&nbsp;<a href=\"https://ipa.co.uk/knowledge/publications-reports/the-long-and-the-short-of-it-balancing-short-and-long-term-marketing-strategies\"><u>Research</u></a> by Binet and Field supports a similar split. User-Friendly has noticed far less is spent on long-term marketing in EA. They guess this may be because of the focus on ROI, which is easiest to measure when implementing things with a clear call to action (eg. to donate), and measuring how many take it. However, the success of that call to action will also depend on the audience\u2019s pre-existing awareness and perception of the organization and concepts behind it - that\u2019s what long-term marketing aims to build over time.<br><br>They suggest measuring earlier parts of the funnel as well as later ones (eg. via \u2018have you heard of this brand?' surveys), and depending on results re-allocating funding across the \u2018build\u2019 / \u2018nudge\u2019 / \u2018connect\u2019 stages of the marketing funnel.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/sD5vF6cfuAYh9ZqYZ/congressional-internships-why-and-how-to-apply\"><strong><u>Congressional Internships: Why and How to Apply</u></strong></a></p><p><i>by US Policy Careers</i></p><p>Comprehensive guide on congressional internships, written by an undergraduate in the process of applying, and reviewed by several individuals with experience \u201con the Hill\u201d. Includes an overview of how they work, why to apply, considerations of where to apply, and preparation / tips for the application process.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/XQDS5F4pRRxBododx/redirecting-private-foundation-grants-to-effective-charities\"><strong><u>Redirecting private foundation grants to effective charities</u></strong></a></p><p><i>by Kyle Smith</i></p><p>The author has a large dataset of electronically filed 990-PFs (which reports charitable activities from private foundations). They suggest slicing this data by aspects like whether a foundation gives to international charities or how many charities it gives to, in order to create a list of those most likely to redirect funds if given targeted advice on effective charities.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/hGdsgaRiF2zH3vX5M/winners-of-the-squiggle-experimentation-and-80-000-hours\"><strong><u>Winners of the Squiggle Experimentation and 80,000 Hours Quantification Challenges</u></strong></a></p><p><i>by NunoSempere</i></p><p>In 2022, QURI announced the Squiggle Experimentation Challenge and a $5k challenge to quantify the impact of 80,000 hours\u2019 top career paths. The winners were:</p><ol><li>Tanae\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4Qdjkf8PatGBsBExK/adding-quantified-uncertainty-to-givewell-s-cost\"><u>Adding Quantified Uncertainty to GiveWell's Cost-Effectiveness Analysis of the Against Malaria Foundation</u></a></li><li>drwahl's&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BK7ze3FWYu38YbHwo/squiggle-experimentation-challenge-cea-leep-malawi\"><u>Cost-effectiveness analysis for the Lead Exposure Elimination Project in Malawi</u></a></li><li>Erich_Grunewald's&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Ze2Je5GCLBDj3nDzK/how-many-ea-billionaires-five-years-from-now\"><u>How many EA billionaires five years from now?</u></a></li></ol>", "user": {"username": "GreyArea"}}, {"_id": "Z7tRvE3bicwHD4kHe", "title": "Podcast with EAG Bay Area participants", "postedAt": "2023-03-13T22:38:08.453Z", "htmlBody": "<p>If you attended EAG Bay Area this year, you might have noticed a guy with a microphone trying to get people to let him interview them. That was me! As a result, I now have a podcast where I talk to a bunch of semi-anonymous people about what their experience at EA Global was like. Hopefully, this gives some sense for the cross-section of the vibes at the event. Disclaimers:</p>\n<ul>\n<li>The sample is definitely not random, selection effects likely up-weighted outgoing people, people who knew me personally, and people who don't work for organizations that don't let them talk to the press.</li>\n<li>If I interviewed you and it didn't end up in the episode, that's because either (a) you asked me to not put it in the episode, (b) I messed up the audio, or (c) re-listening to it I thought I asked bad questions that didn't engage with what you said very much.</li>\n</ul>\n<p>You can listen on google podcasts <a href=\"https://youtu.be/KuN0y4yKSIU\">here</a>, or if you'd prefer to just have the raw mp3, that's available <a href=\"https://www.dropbox.com/s/92o0zabg0xsbcdq/eag_bay_area_2023.mp3?dl=0\">here</a>.</p>\n<p>To listen to other episodes of this podcast, search \"The Filan Cabinet\" in your podcast app of choice.</p>\n", "user": {"username": "DanielFilan"}}, {"_id": "xNQQC3ceJ78CD7a2Z", "title": "Exposure to Lead Paint in Low- and Middle-Income Countries", "postedAt": "2023-03-14T08:43:26.301Z", "htmlBody": "<p><a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries\"><strong><u>Click here</u></strong></a><strong> for the full version of this report on the Rethink Priorities website.</strong></p><p>This report is a \u201cshallow\u201d investigation, as described&nbsp;<a href=\"https://perma.cc/D85A-EKDG\"><u>here</u></a>, and was commissioned by GiveWell and produced by Rethink Priorities from November 2021 to January 2022. We updated and revised this report for publication. GiveWell does not necessarily endorse our conclusions.&nbsp;<strong>The primary focus of the report is to provide an overview of what is currently known about the exposure to lead paints in low- and middle-income countries.&nbsp;</strong></p><h1><strong>Key takeaways</strong></h1><ul><li>Lead exposure is common across low- and middle-income countries (LMICs) and can lead to life-long health problems, a reduced IQ, and lower educational attainment. One important exposure pathway is lead-based paint (here defined as a paint to which lead compounds have been added), which is still unregulated in over 50% of countries globally. Yet, little is known about how much lead paint is being used in LMICs and to what extent it contributes to the health and economic burden of lead (<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries#Lead_exposure_is_toxic_and_comes_partly_from_the_use_of_lead_based_paint_across_many_low__and_middle_income_countries__LMICs_\"><u>link to section</u></a>).</li><li>Home-based assessment studies of lead paint levels provide evidence of current exposure to lead, but the evidence in LMICs is scarce and relatively low quality. Based on the few studies we found, our best guess is that the average lead concentration in paint in residential houses in LMICs is between 50 ppm and 4,500 ppm (90% confidence interval) (<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries#Home_based_assessments_provide_evidence_of_current_lead_paint_exposure__while_shop_based_assessments_help_gauge_future_exposure\"><u>link to section</u></a>).</li><li>Shop-based assessment studies of lead-based paints provide evidence of future exposure to lead. Based on three review studies and expert interviews, we find that lead levels in solvent-based paints are roughly 20 times higher than in water-based paints. Our best guess is that average lead levels of paints currently sold in shops in LMICs are roughly 200-1,400 ppm (80% CI) for water-based paints and 5,000-30,000 ppm (80% CI) for solvent-based paints (<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries#Shop_based_assessments_show_that_new_paints_sold_in_LMICs_are_frequently_in_the_range_of_tens_of_thousands_of_ppm_of_lead\"><u>link to section</u></a>).</li><li>Based on market analyses and small, informal seller surveys, we estimate that market share of solvent-based paints in LMICs is roughly 30%-65% of all residential paints sold (the rest being water-based paints), which is higher than in high-income countries (~20%-30%) (<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries#In_LMICs__the_market_share_of_solvent_based_paints_is_roughly_30__65___80__CI__of_all_paints_sold__which_is_higher_than_in_HICs\"><u>link to section</u></a>).</li><li>There is also evidence that lead-based paints are frequently being used in public spaces, such as playgrounds, (pre)schools, hospitals, and daycare centers. However, we do not know the relative importance of exposure from lead paint in homes vs. outside the home (<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries#Lead_paint_in_public_spaces_contributes_to_lead_exposure\"><u>link to section</u></a>).</li><li>As many studies on the exposure and the health effects of lead paint are based on historical US-data, we investigated whether current lead paint levels in LMICs are comparable to lead paint levels in the US before regulations were in place. We find that historical US-based lead concentrations in homes were about 6-12 times higher than those in recently studied homes in some LMICs (70% confidence) (<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries#Historical_basis__lead_levels_in_the_United_States_before_the_introduction_of_lead_paint_regulations\"><u>link to section</u></a>).</li><li>We estimate that doubling the speed of the introduction of lead paint bans across LMICs could prevent 31 to 101 million (90 % CI) children from exposure to lead paint, and lead to total averted income losses of USD 68 to 585 billion (90% CI) and 150,000 to 5.9 million (90% CI) DALYs over the next 100 years. Building on previous analyses done by LEEP (<a href=\"https://docs.google.com/spreadsheets/d/1oN_TgsWeoFN42BUKCOqsFYBwuOxj6R38pZg5dabc-ro/edit#gid=790783793\"><u>Hu, 2022</u></a>;&nbsp;<a href=\"https://docs.google.com/document/d/1abacbjeGakkmsZL3D7w1S0yadIgCl6ObfEl7sCVNpww/edit\"><u>LEEP, 2021</u></a>) and Attina and Trasande (<a href=\"http://dx.doi.org/10.1289/ehp.1206424.\"><u>2013</u></a>), we estimate that lead paint accounts for ~7.5% (with a 90% confidence interval of 2-15%) of the total economic burden of lead. We would like to emphasize that these estimates are highly uncertain, as our model is based on many inputs for which data availability is scarce or even non-existent. This uncertainty could be reduced with more data on the use of paints in LMICs (e.g. frequency of re-painting homes) and on the average dose-response relationship between residential lead paint levels and blood lead levels (<a href=\"https://rethinkpriorities.org/publications/exposure-to-lead-paint-in-low-and-middle-income-countries#Speeding_up_lead_paint_bans_across_LMICs_could_lead_to_averted_income_losses_of_USD_18_billion_to_155_billion__90__CI__and_150_000_to_5_9_million__90__CI__averted_DALYs_over_the_next_100_years\"><u>link to section</u></a>).</li></ul><h1>Acknowledgments</h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676308377/mirroredImages/yZ4vEmBdsEg8tyvp3/gemmalmaknhb0vi7hjze.png\"></figure><p>Jenny Kudymowa and Ruby Dickson jointly researched and wrote this report. Carmen van Schoubroeck assisted in revising the report for the public-facing version. Jason Schukfraft and Tom Hird supervised the report. Thanks to David Reinstein, William McAuliffe, Greer Gosnell, James Hu, and Bjorn Larsen for helpful comments on drafts. Further thanks to Bruce Lanphear, Lucia Coulter, Clare Donaldson, Sara Brosch\u00e9, Bret Ericson, Elsbeth Geldhof, Angela Bandemehr, Ashley Fisseha, Steve Wolfson, Drew McCartor, Richard Fuller, Emily Nash, Adam Kiefer, and Perry Gottesfeld for taking the time to speak with us. GiveWell provided funding for this report, and we use their general frameworks for evaluating cause areas, but they do not necessarily endorse its conclusions.</p>", "user": {"username": "Rachel"}}, {"_id": "afnzEbp2JzsycXt2r", "title": "List of all EA-aligned / Effective organizations?", "postedAt": "2023-03-13T19:01:58.797Z", "htmlBody": "<p>Has anyone compiled a relatively comprehensive list of organizations that are either EA-aligned or have been deemed effective by one or more EA organizations? Or possibly if the organization has been given a grant by one of the prominent EA funders.</p><p>I am conducting research on EA/Effective organizations and this would be extremely useful. If it does not exist, I will attempt to create one and make it available.</p>", "user": {"username": "Kyle Smith"}}, {"_id": "qxaAyAuw3DBW5WAis", "title": "Shallow Investigation: Stillbirths", "postedAt": "2023-03-13T16:19:07.594Z", "htmlBody": "<p><i>This topic has the potential to be deeply upsetting to those reading it, particularly to those who have personal experience of the topic in question. If you feel that I\u2019ve missed or misunderstood something, or could have phrased things more sensitively, please reach out to me.</i></p><p><i>Throughout the review, words like \u201cwoman\u201d or \u201cmother\u201d are used in places where some people might prefer \u201cbirthing person\u201d or similar. This choice reflects the language used in the available literature and does not constitute a position on what the most appropriate terminology is.</i></p><p><i>This report is a shallow dive into stillbirths, a sub-area within maternal and neonatal health, and was produced as part of the Cause Innovation Bootcamp. The report, which reflects approximately 40-50 hours of research, offers a brief dive into whether a particular problem area is a promising area for either funders or founders to be working in. Being a shallow report, it should be used to decide whether or not more research and work into a particular problem area should be prioritised.&nbsp;</i></p><h1>Executive Summary</h1><ul><li><strong>Importance:&nbsp;</strong>This problem is likely&nbsp;<strong><u>very important</u></strong> (epistemic status-strong)- stillbirths are widespread, concentrated in the world\u2019s poorest countries, and decreasing only very slowly compared to the decline in maternal and infant mortality. There are more deaths resulting from stillbirth than those caused by HIV and malaria combined (depending on your personal definition of death- see below), and even in high-income countries stillbirths outnumber infant deaths.</li><li><strong>Tractability:&nbsp;</strong>This problem is likely&nbsp;<strong><u>moderately tractable</u></strong>&nbsp;<u>(</u>moderate)- most stillbirths are likely to be preventable, but the most impactful interventions are complex, facility-based, expensive, and most effective at scale e.g. guaranteeing access to high-quality emergency obstetric care</li><li><strong>Neglectedness:&nbsp;</strong>This problem is&nbsp;<strong><u>unlikely to be neglected</u></strong> (less strong)- although still under-researched and under-counted, stillbirths are the target of some of the largest organisations in the global health and development world, including the WHO, UNICEF, the Bill and Melinda Gates Foundation, and the Lancet. Many countries have committed to the Every Newborn Action Plan, which aims- amongst other things- to reduce the frequency of stillbirths.&nbsp;</li></ul><p><strong>Key uncertainties</strong></p><ul><li><strong>Key uncertainty 1:&nbsp;</strong>Accurately assessing the impact of stillbirths, and therefore the cost-effectiveness of interventions aimed at reducing stillbirths, depends significantly on to what extent direct costs to the unborn child are counted. Some organisations view stillbirths as having negative effects on the parents and wider communities but do not count the potential years of life lost by the unborn child; others use time-discounting methods to calculate a hypothetical number of expected QALYS lost, and still others see it as completely equivalent to losing an averagely-long life. Differences in the weighting of this loss can alter the calculated impacts of stillbirth by several orders of magnitude and is likely the most important consideration when considering a stillbirth-reducing intervention</li><li><strong>Key uncertainty 2:</strong>&nbsp;Interventions which reduce the risk of stillbirth tend to be those which also address maternal and neonatal health more broadly; therefore, it is very difficult to accurately assess the cost-effectiveness of these interventions solely in terms of their impact on stillbirths, and more complex models which take into account the impacts on maternal, neonatal, and infant health are likely more accurate in assessing the overall cost-effectiveness of interventions.</li><li><strong>Key uncertainty 3:&nbsp;</strong>A large proportion of the data around interventions to reduce stillbirths comes from high-income countries, but most stillbirths happen in low- and middle-income countries. It\u2019s not clear that the effectiveness of interventions will remain consistent across very different contexts.</li></ul><h1>Introduction/Importance</h1><h2>What are stillbirths?&nbsp;</h2><p>Stillbirths are generally defined as<strong>&nbsp;</strong>the death of a baby after 28 weeks of gestation but before birth<a href=\"https://paperpile.com/c/Tgle3G/O0bt\"><sup>1</sup></a> (although this definition is not quite universal). Miscarriages, by contrast, are those foetal deaths which occur before 28 weeks of gestation, and neonatal death usually refers to deaths within the first 28 days of life. Stillbirths can occur at any time between 28 weeks gestation and the moment of delivery, but around half of all stillbirths worldwide occur during labour<a href=\"https://paperpile.com/c/Tgle3G/O0bt\"><sup>1</sup></a>, the period immediately leading up to the moment of birth.&nbsp;</p><h2>Where are stillbirths? And when?</h2><p>Stillbirths are likely to have been occurring for as long as there have been births, and occur in other mammalian species at similar or greater rates. However, they have been rising up the global health agenda in the previous two decades, driven particularly by two detailed Lancet review series in 2011 and 2016, from which this shallow review draws extensively.</p><p>Stillbirths have been declining gradually over time in most regions, with the most dramatic decline occurring in Central and Southern Asia. The number of stillbirths in Sub-Saharan Africa slightly increased from 2000-2019, and by 2030 it is estimated that the majority of global stillbirths will be in this region; at the current rate of progress, it will be 160 years before a pregnant woman in Africa has the same risk of stillbirth as a pregnant women in a high-income country.<a href=\"https://paperpile.com/c/Tgle3G/kQLH\"><sup>2</sup></a></p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qxaAyAuw3DBW5WAis/xxzpl2fwyxhp9fjmnblo\"></p><p>Although any decline in stillbirths is positive, it is notable that progress has been significantly slower than the rate of decline of neonatal and infant mortality.<a href=\"https://paperpile.com/c/Tgle3G/qbQp\"><sup>3</sup></a> It\u2019s not immediately clear why this is, as many interventions which reduce the risk of stillbirth also reduce neonatal and infant mortality, as outlined below.&nbsp;</p><h2>Why do stillbirths happen?</h2><p>Although it is sometimes possible to pinpoint the precise cause of a particular stillbirth, in most cases the causes are likely to be complex, multi-faceted, and emergent<a href=\"https://paperpile.com/c/Tgle3G/dVmP\"><sup>4</sup></a>:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qxaAyAuw3DBW5WAis/gjeanvlofcwjyrqbvv89\"></p><p>This graphic demonstrates how the health and wellbeing of the mother prior to conception and the strength of the local healthcare system can interact with antenatal health problems to increase the risk of stillbirth; it may be the case that some stillbirths are effectively \u201ccaused\u201d many decades before they actually occur. However, a plurality of stillbirths still occur during labour, especially those in the developing world.<a href=\"https://paperpile.com/c/Tgle3G/O0bt\"><sup>1</sup></a>&nbsp;</p><p>Factors like smoking, excessive alcohol intake, and some pre-existing medical conditions are all known to increase the risk of stillbirth, but attempts to create a&nbsp; gears-level understanding of why stillbirth occurs and how we might prevent it is difficult, to say the least:</p><p><br>&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qxaAyAuw3DBW5WAis/x2oqkogvfwszwrw2alo7\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qxaAyAuw3DBW5WAis/t44d9h004qxiy59ibegs\"></p><p>Tl;dr- it\u2019s complicated.</p><h2>What are the impacts of stillbirth?</h2><p>There are approximately 2 million stillbirths every year worldwide.<a href=\"https://paperpile.com/c/Tgle3G/kQLH\"><sup>2</sup></a> Even in higher-income countries, the number of stillbirths is usually greater than the number of infant deaths in the equivalent time period<a href=\"https://paperpile.com/c/Tgle3G/fDzK\"><sup>5</sup></a>.&nbsp;</p><p>Each of these stillbirths has economic, health, and wellbeing impacts which \u201cripple outwards\u201d, like<a href=\"https://paperpile.com/c/Tgle3G/fDzK\"><sup>5</sup></a> a stone dropped into a pond, from the stillborn child out to their immediate family, the local community, and wider society.</p><h3>Economic impacts</h3><p>There are direct and indirect income burdens resulting from stillbirth.</p><p>Direct income burdens include the cost of post-stillbirth care, which can be borne by governments, insurance companies, communities, and/or parents, and can be anywhere from hundreds to tens of thousands of dollars more than what would typically be paid for a live birth.<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a> Future pregnancies then tend to cost more as a result of increased monitoring requirements. These costs are likely to vary significantly between contexts, as will the ability of parents to pay them.</p><p>Indirect income burdens related to stillbirths include reduced parental earnings, reduced productivity and economic engagement, and potentially increased costs from psychological therapies and further antenatal counselling.<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a> Many parents reported never reaching pre-stillbirth productivity levels, meaning that the cumulative economic impact of the emotional stress could be significant.<br>&nbsp;</p><p>Lastly and probably most significantly, there is obviously the loss of an additional individual who would likely have been economically productive over the course of their lifetime- this is difficult to quantify but could be by some margin the most important consideration, depending on the estimated size of their contribution.&nbsp;</p><h3>Health impacts</h3><p>These may be the most difficult to assess because different authors calculate the loss of DALYs from each stillbirth very differently<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a>. Estimates range from 0-86 DALYs lost, with time discounting to around 5 DALYs seeming to be a reasonably common middle ground; this is obviously a very significant range, and as such variation in the evaluators position on this is likely to be the most important factor in their overall evaluation of the impact of stillbirth.&nbsp;</p><p>From a common-sensical point of view, it\u2019s difficult to know exactly where to \u201cdraw the line\u201d; it seems crazy to imagine a baby dying during labour as anything other than a rich, full potential liife lost, but if we extend that logic too far backwards then we might imagine any moment that we are not reproducing to be costing one \u201clife\u2019s worth\u201d of DALYs. Time discounting is a difficult science and seems likely to vary depending on the context in which the stillbirth takes places as well as on the personal moral preferences and positions of the evaluator. A more defined answer to this question, perhaps similar to GiveWell\u2019s work on moral weights, could be very useful for future evaluation.&nbsp;</p><h3>Wellbeing impacts</h3><p>There are evident non-economic costs to stillbirths, although these can be difficult to quantify. There are near-universal reports of feelings of grief, anxiety, fear, and suffering amongst the family of stillborn children, extending from the affected parents to include extended family and any surviving or future children. Reports of chronic pain, fatigue, and increased substance use are also common, as are instances of relationship strain and breakdown following stillbirth&nbsp;<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a>. Of particular harm seems to be the stigma which in many countries is still experienced by parents of stillborn children, who may feel that they are blamed or that their their feelings minimised by their communities.<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a><br>There are also emotional costs borne by health professionals involved in stillbirths, which can include feelings of trauma, stress, guilt, anger, blame, or fear of litigation or disciplinary action&nbsp;<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a>.</p><p>Similarly to the economic cost, there is also obviously the loss of the WELLBYs that would potentially have been experienced by the stillborn individual. Again, how much these are weighted is largely an individual decision, or at least would require philosophical and economic thought beyond the scope of this shallow review, but again could significantly alter the estimated importance of stillbirths.&nbsp;</p><p>However, there are sometimes positive impacts of wellbeing related to stillbirths, with some parents saying that it strengthened their relationship or their sense of meaning through campaigning or religious activities and health professionals sometimes reporting a sense of honour or privilege, the development of stronger professional bonds, or increased confidence&nbsp;<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a>.</p><h2>Who is impacted?</h2><p>There\u2019s no clear gender discrepancy noted in which babies are likely to be stillborn. Stillbirths can be dangerous to the mother in and of themselves and are also made more likely by poor maternal health; as a result, many interventions aimed at reducing stillbirths look to improve the health of childbearing women and many interventions which improve the health of childbearing women also reduce the risk of stillbirth.&nbsp;</p><p>By definition, the primary victims of stillbirth are so young as to have not yet been born; the other most directly impacted individuals are likely to be parents of childbearing age, particularly those who are at the extremes of childbearing age and are therefore at increased risk of stillbirth.</p><p>There is clear geographical variation in the rate of stillbirths. Somewhere between 84-98% of stillbirths occur in low- and middle-income countries,with around half of the total number occurring in 6 countries- India, Pakistan, Nigeria, the Democratic Republic of Congo, China, and Ethiopia.<a href=\"https://paperpile.com/c/Tgle3G/kQLH\"><sup>2</sup></a> The highest rates of stillbirth per total birth are in Guinea-Bissau, Bangladesh, Pakistan, Afghanistan and Ethiopia.<a href=\"https://paperpile.com/c/Tgle3G/Z2Dn\"><sup>7</sup></a> Individuals who live in rural areas, distant from healthcare facilities, are most at risk, as well as those of low socioeconomic status and low education; this trend is notable even in high-income countries, where discrepancies in stillbirth rates between such groups still remains.<a href=\"https://paperpile.com/c/Tgle3G/fMuA\"><sup>8</sup></a></p><h1>Tractability</h1><h2>Are stillbirths preventable?</h2><h3>What would this issue look like in an ideal world?&nbsp;</h3><p>Stillbirths are sometimes thought of as inevitable tragedies, but it seems likely that only a small minority of stillbirths- in the order of 10%- are caused by non-preventable congenital abnormalities.<a href=\"https://paperpile.com/c/Tgle3G/fDzK\"><sup>5</sup></a> This does not necessarily imply that the remaining 90% of stillbirths can be prevented, but does at least give a very high upper bound on the proportion which are potentially avoidable; a review of around 900 stillbirths in India and Pakistan suggested that around 75% were preventable<a href=\"https://paperpile.com/c/Tgle3G/Ijgr\"><sup>9</sup></a>, which may be a more reasonable upper limit.</p><h2>How might we prevent stillbirths?</h2><p>Without a gears-level understanding of the problem developing appropriate solutions is hard, but substantial work has been done on exploring and assessing the impactfulness of a limited range interventions aimed at reducing stillbirths, primarily those which take place during the antenatal period of the index pregnancy. Several authors<a href=\"https://paperpile.com/c/Tgle3G/NFs1+f10w+I2Em\"><sup>10\u201312</sup></a> arrived at similar \u201cmenus\u201d of potential high-impact interventions, with small variations between authors, after assessing a wider pool for potential cost-effectiveness, with this set being taken from the 2016 Lancet series<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a>:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Group</strong></td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><strong>Intervention</strong></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" rowspan=\"3\">Preconception nutrition interventions</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\"><p>Balanced energy and protein supplementation</p><p><br>&nbsp;</p></td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Folic acid supplementation</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Micronutrient supplementation</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" rowspan=\"3\">Basic antenatal interventions</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Prevention and treatment of malaria</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Detection and treatment of syphilis&nbsp;</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Tetanus toxoid immunisation</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" rowspan=\"4\">Advanced antenatal interventions</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Treatment of hypertension during pregnancy</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Treatment of diabetes during pregnancy</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Treatment of foetal growth restriction</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Induction of labour at or beyond 41 weeks gestation</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\" rowspan=\"4\">Interventions during childbirth</td><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Antenatal steroids for preterm labour</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Antibiotics for preterm rupture of membranes</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Active management of third stage of labour</td></tr><tr><td style=\"border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top\">Neonatal resuscitation</td></tr></tbody></table></figure><p>This \u201cmenu\u201d comprises the most well-investigated set of interventions and as such will be the focus of this review. However, it\u2019s worth remembering that interventions to, for example, reduce poverty, increase access to education, or treat tropical diseases are likely to include a reduction in stillbirths as part of their wider impact, possibly a greater reduction than what can be achieved with interventions described here.</p><p>Despite the narrowness of their scope, this set of interventions has the potential for significant impact; scaling up their availability as a package in those countries worst affected by stillbirths to 90% is estimated to prevent 823,000 stillbirths, 1,145,000 neonatal deaths, and 166,000 maternal deaths each year for an additional cost of $4.6billion. If stillbirths prevented are considered to be lives saved, this gives an estimated cost of $2143 per life saved, competitive with the lowest values achieved by Givewell Top Charities.<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a></p><p>The majority of the stillbirth reductions from the implementation of these interventions comes from those aimed at childbirth, although this group is by an order of magnitude the most expensive if implemented as described.&nbsp;</p><p>The most cost-effective interventions are those in the basic antenatal group.&nbsp; These are estimated to avert a stillbirth, maternal, or neonatal death at a cost of $830 per life saved<a href=\"https://paperpile.com/c/Tgle3G/y9e3\"><sup>6</sup></a>, although the number of lives which can be saved at this price is likely to be limited. These interventions also have an outsized reduction in stillbirths relative to the number of lives they save, if we are looking for interventions which specifically reduce stillbirths; in particular, it looks like the treatment of syphilis and the induction of labour that continues past 41 weeks might be interventions with unusually high impact on stillbirths in particular.<a href=\"https://paperpile.com/c/Tgle3G/hvEB\"><sup>13</sup></a></p><p>These general trends seem to be well replicated throughout the literature; for example, an earlier review in 2011<a href=\"https://paperpile.com/c/Tgle3G/NFs1\"><sup>10</sup></a> and a Cochrane review in 2020<a href=\"https://paperpile.com/c/Tgle3G/I2Em\"><sup>12</sup></a> arrived on a similar set of promising interventions at similar levels of cost-effectiveness. However, it seems worth focusing particularly on those two interventions as being particularly promising in our goal to reduce stillbirths.&nbsp;</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qxaAyAuw3DBW5WAis/rcwkcqtn0zlsshxqq2ja\"></p><p><br>In some ways, these two examples illustrate to me the two \u201cclasses\u201d of intervention in stillbirths that effective altruists might consider pursuing. Antenatal syphilis can be treated effectively with a single course of a generic antibiotic called Penicillin G, leading to a reduction of stillbirth risk of approximately 82%<a href=\"https://paperpile.com/c/Tgle3G/duds\"><sup>14</sup></a>. Although there have been concerns about the availability of Penicillin G in some developing countries, the cost-effectiveness of this intervention- estimated to be around 10$ per DALY saved- has led to investment from mainstream EA organisations in the past, as outlined below.&nbsp;</p><p>Scaling up the availability of induction at labour represents a very different challenge to procuring, distributing, and administering single doses of antibiotics. Although the induction of labour in itself is not technically difficult, usually involving the administration of misoprostol, the decision to induce is one usually taken by highly trained healthcare professionals like obstetricians and midwives within a fairly advanced healthcare facility. Therefore, the availability of induction depends not simply on the availability of a single drug but on the creation and maintenance of a sophisticated healthcare facility with trained staff, with all the upfront costs which that requires before any health benefits begin to be accrued.&nbsp;</p><p>The induction of labour, therefore, might be more reasonably considered as a part of the toolkit of a wider but still promising intervention- the availability of midwives to attend births. Work to assess the impact of midwife availability on stillbirths is promising, although suggests that most of the reduction comes only once nearly full coverage is achieved, whereas significant reduction of maternal and neonatal death seems to come with more modest coverage.<a href=\"https://paperpile.com/c/Tgle3G/oQQQ\"><sup>15</sup></a> It\u2019s not clear why this would be, and without more data on what midwives do and which of their actions are most impactful it\u2019s hard to evaluate the cost-effectiveness of midwives as an intervention in and of themselves.&nbsp;</p><p>The available literature can also guide us as to what interventions are unlikely to be significantly cost-effective, some of which can be surprising. For example, magnesium is often used to treat pre-eclampsia (a potentially fatal maternal illness) in the developed world, but magnesium was found not to significantly reduce the rate of stillbirth or perinatal death in randomised trials.<a href=\"https://paperpile.com/c/Tgle3G/NFs1\"><sup>10</sup></a> Smoking is a commonly cited risk factor for stillbirths, but there is no strong evidence to suggest that interventions to aide smoking cessation during pregnancy reduce the risk of stillbirth.<a href=\"https://paperpile.com/c/Tgle3G/I2Em\"><sup>12</sup></a></p><h1>Neglectedness</h1><h2>Who is funding stillbirth reduction?</h2><h3>Funders</h3><p>Many of the world\u2019s largest health organisations, including national governments and international bodies, have declared their intentions to reduce stillbirths. In many cases, this takes the form of a national commitment to the Every Newborn Action Plan, which was developed by the World Health Organization<a href=\"https://paperpile.com/c/Tgle3G/P9bt\"><sup>16</sup></a> and endorsed by 194 nations in 2014. However, around 25% of the nations endorsing the Plan are not on track to achieve its goals<a href=\"https://paperpile.com/c/Tgle3G/kQLH\"><sup>2</sup></a>, and it may be that this gap could increase the appeal of EA-led interventions to local governments who have already made political commitments to reducing stillbirths.&nbsp;&nbsp;</p><p>There are also large non-governmental organisations who contribute financially to global stillbirth reduction. The Bill and Melinda Gates Foundation, at the time of writing, have given 8 grants directly towards stillbirth reduction, with the largest being the $20,000,000 donation to the Global Alliance to Prevent Prematurity and Stillbirth. It seems likely that many of their other grants have gone towards work which reduces stillbirth-for example, a $40,000,000 grant to the WHO to \u201cstrengthen rapid scale up of Maternal, Neonatal, and Child Health interventions in Africa\u201d, which intentionally or not are likely to reduce stillbirth risk.&nbsp;</p><p>In high-income countries, many large charitable organisations spend significant amounts of money on stillbirth; SANDS, the leading stillbirth charity in the United Kingdom, spends several million pounds a year, with similarly sized charities operating in the United States. Detail on where or how these organisations spend their money, however, is difficult to obtain.</p><p>Funding has also been directed toward stillbirth reduction from within the EA community. GiveWell has directed just under $20,00,000 to Evidence Action to fund their work on syphilis testing and treatment in pregnancy- see below for details.&nbsp;</p><h2>Who is implementing stillbirth reduction?</h2><p>Mapping exercises of organisations involved in global stillbirth prevention demonstrate the tangled web of academic, charitable, governmental, and private sector bodies involved<a href=\"https://paperpile.com/c/Tgle3G/fDzK\"><sup>5</sup></a>:</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qxaAyAuw3DBW5WAis/l1tzuzuao85vcihzbxei\"></p><p>It is therefore difficult to highlight which organisations in particular are dominant in actually implementing interventions to reduce stillbirths; in many countries, national governments are responsible for the direct work of stillbirth reduction via their health systems, often with an ecosystem of charitable and academic bodies fulfilling research functions and providing support to those affected by stillbirth.</p><p>It may be more feasible to search for those bodies implementing the two particularly promising interventions mentioned earlier, antenatal syphilis treatment and induction for post-term labour. Evidence Action is working in Liberia, Zambia and Cameroon to help them reduce antenatal syphilis by adding syphilis tests to the existing HIV screening programme.&nbsp;</p><h1>Conclusions</h1><p>My broad conclusion from this shallow review is that&nbsp;<strong>stillbirth reduction is likely to be very important, moderately tractable, and not particularly neglected</strong>. Although there are promising potential interventions, many of the highest-yield routes to stillbirth reduction are probably best executed by local and national health services, rather than smaller organisations, as they have their most significant impact at large scale.&nbsp;</p><p>Many organisations, like those involved in treating HIV or malaria, already target interventions which are likely to reduce stillbirths as a positive but potentially undervalued side-effect. It may be the case that formalising a method for including the harms of stillbirths would significantly alter our thinking on which global health interventions are likely to be most cost-effective. Work on describing and incorporating the harms of stillbirth seems important and promising.</p><p>Outside of the traditional ITN framework, there may be additional benefits arising from a reduction in stillbirths. Economic growth is a function at least partially of population growth, and thus may well be improved by fewer stillbirths; in an ideal world, this could contribute to a \u201cvirtuous circle\u201d of economic growth and improving health.</p><p>Stillbirth reduction is also a legible cause area to those outside of the EA movement and success might therefore be positive for our reputation beyond its \u201cpure\u201d impact. Given the cohort of people most affected, stillbirth reduction is likely to appeal to individuals interested in social justice and equity, who are sometimes suspicious of the EA movement, and coordination with these and other groups might make EA more effective in the future.</p><p>Less prosaically, it can be easy to become inured to the true stories which lie behind quantitative estimates and dry economic analysis of human lives and deaths. I\u2019m deeply lucky to be able to do work which acknowledges the sanctity and preciousness of human life, and I\u2019m glad to be part of a community which is so committed to celebrating and carrying out that intuition in everyday life.&nbsp;</p><p>Huge thanks to Cause Innovation Bootcamp, Leonie Falk, Akhil Bansal, and Vicky Cox for coordinating and giving feedback- any errors are mine.&nbsp;<br>&nbsp;</p><h1>References and Citations</h1><p>1.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/O0bt\">World Health Organization. Stillbirth [Internet]. Available from:&nbsp;</a><a href=\"https://www.who.int/health-topics/stillbirth\">https://www.who.int/health-topics/stillbirth</a></p><p>2.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/kQLH\">UN Inter-agency Group for Child Mortality Estimation. A Neglected Tragedy- The global burden of stillbirths [Internet]. 2020. Available from:&nbsp;</a><a href=\"https://platform.who.int/docs/default-source/mca-documents/maternal-nb/a-neglected-tragedy-stillbirths-igme-report-english-2020.pdf?Status=Master&amp;sfvrsn=13cfa3f6_4\">https://platform.who.int/docs/default-source/mca-documents/maternal-nb/a-neglected-tragedy-stillbirths-igme-report-english-2020.pdf?Status=Master&amp;sfvrsn=13cfa3f6_4</a></p><p>3.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/qbQp\">Global, regional, national, and selected subnational levels of stillbirths, neonatal, infant, and under-5 mortality, 1980\u20132015: a systematic analysis for the Global Burden of Disease Study 2015. Lancet. 2016 Oct 8;388(10053):1725\u201374.</a></p><p>4.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/dVmP\">Blencowe H. Stillbirth in Low- and Middle-Income Countries; A conceptual framework. King\u2019s College London;</a></p><p>5.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/fDzK\">Bernis L de, Kinney MV, Stones W, Hoope-Bender PT, Vivio D, Leisher SH, et al. Stillbirths: ending preventable deaths by 2030. Lancet. 2016 Feb 13;387(10019):703\u201316.</a></p><p>6.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/y9e3\">Heazell AEP, Siassakos D, Blencowe H, Burden C, Bhutta ZA, Cacciatore J, et al. Stillbirths: economic and psychosocial consequences. Lancet. 2016 Feb 6;387(10018):604\u201316.</a></p><p>7.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/Z2Dn\">UN Inter-agency Group for Child Mortality Estimation. CME Info - Child Mortality Estimates [Internet]. Available from:&nbsp;</a><a href=\"https://childmortality.org/\">https://childmortality.org/</a></p><p>8.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/fMuA\">Flenady V, Wojcieszek AM, Middleton P, Ellwood D, Erwich JJ, Coory M, et al. Stillbirths: recall to action in high-income countries. Lancet. 2016 Feb 13;387(10019):691\u2013702.</a></p><p>9.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/Ijgr\">Goldenberg RL, Saleem S, Goudar SS, Silver RM, Tikmani SS, Guruprasad G, et al. Preventable stillbirths in India and Pakistan: a prospective, observational study. BJOG. 2021 Oct;128(11):1762\u201373.</a></p><p>10.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/NFs1\">Bhutta ZA, Yakoob MY, Lawn JE, Rizvi A, Friberg IK, Weissman E, et al. Stillbirths: what difference can we make and at what cost? Lancet. 2011 Apr 30;377(9776):1523\u201338.</a></p><p>11.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/f10w\">Are stillbirths stillborn on the global agenda? [Internet]. Available from:&nbsp;</a><a href=\"https://www.countdown2030.org/documents/Stillbirth_Globalagenda_Blencowe.pdf\">https://www.countdown2030.org/documents/Stillbirth_Globalagenda_Blencowe.pdf</a></p><p>12.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/I2Em\">Erika Ota, Katharina da Silva Lopes, Phillipa Middleton, Vicki Flenady, Windy MV Wariki, Obaidur RAhman, et al. Antenatal interventions for preventing stillbirth, fetal loss and perinatal death: an overview of Cochrane systemic reviews. Cochrane Database Syst Rev [Internet]. 2020 Dec 18;(12). Available from:&nbsp;</a><a href=\"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD009599.pub2/full\">https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD009599.pub2/full</a></p><p>13.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/hvEB\">Bhutta ZA, Das JK, Bahl R, Lawn JE, Salam RA, Paul VK, et al. Can available interventions end preventable deaths in mothers, newborn babies, and stillbirths, and at what cost? Lancet. 2014 Jul 26;384(9940):347\u201370.</a></p><p>14.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/duds\">Nurse-Findlay S, Taylor MM, Savage M, Mello MB, Saliyou S, Lavayen M, et al. Shortages of benzathine penicillin for prevention of mother-to-child transmission of syphilis: An evaluation from multi-country surveys and stakeholder interviews. PLoS Med. 2017 Dec 27;14(12):e1002473.</a></p><p>15.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/oQQQ\">Nove A, Friberg IK, Bernis L de, McConville F, Moran AC, Najjemba M, et al. Potential impact of midwives in preventing and reducing maternal and neonatal mortality and stillbirths: a Lives Saved Tool modelling study. The Lancet Global Health. 2021 Jan 1;9(1):e24\u201332.</a></p><p>16.&nbsp; <a href=\"http://paperpile.com/b/Tgle3G/P9bt\">Every Newborn Action Plan [Internet]. [cited 2022 Dec 9]. Available from:&nbsp;</a><a href=\"https://www.who.int/initiatives/every-newborn-action-plan\">https://www.who.int/initiatives/every-newborn-action-plan</a></p><p><br><br>&nbsp;</p>", "user": {"username": "Joe Pusey"}}, {"_id": "p5D5p4WMBoSNXjiz2", "title": "Designing user authentication protocols ", "postedAt": "2023-03-13T15:56:08.935Z", "htmlBody": "<p>The impact of this issue is perhaps fairly minor, but I wonder how much effort is put into designing optimised protocols for user authentication?&nbsp;</p><p>It is not a surprise that <a href=\"https://stuartschechter.medium.com/before-you-turn-on-two-factor-authentication-27148cc5b9a1\">all existing user authentication methods can fail in pretty obvious (or non-obvious) ways</a>, and every method has its own attack surface and risk of losing access.&nbsp;</p><p>Basic password: bruteforce/dictionary attack if the password is simple, risk of forgetting it if the password is complex.</p><p>Password manager + complex random strings: amplifies the loss in the event if the master password is lost, and the manager system presents an obvious target for malicious actors.</p><p>2FA using phone number/app: sim swap attack or losing the phone</p><p>2FA using hardware key: risk of losing the key</p><p>Biometrics: probably the only one that can work when the device is compromised...potentially could be faked, and user might lose access in case of injury. And the extent we are comfortable with giving biometric information to different service providers is also debatable.</p><p>I wonder how much effort has gone into determining what is the optimal method for a given situation, and whether there are anything new in the making that might offer some improvement.</p><p>Of course, different types of services/users will also find different protocols being optimal. &nbsp;Password manager would work very well for accounts created for commenting on blogs, and \"recover account through trusted contacts\" probably works for Facebook.&nbsp;</p><p>But maybe corporation/institutional systems would be interested in specifically designed authentication protocols to squeeze one last bit of security? What could be done, both technologically and procedurally, in this case?&nbsp;</p>", "user": {"username": "Kinoshita Yoshikazu (pseudonym)"}}, {"_id": "CqDzfiLhShqu9CS4F", "title": "Paper summary: Longtermist institutional reform (Tyler M. John and William MacAskill)", "postedAt": "2023-03-13T18:07:50.546Z", "htmlBody": "<p><i>This is a summary of the GPI working paper \"</i><a href=\"https://globalprioritiesinstitute.org/tyler-m-john-and-william-macaskill-longtermist-institutional-reform/\"><i>Longtermist institutional reform</i></a><i>\" by Tyler M. John and William MacAskill (published in the 2021 edited volume \u201c</i><a href=\"https://firstforum.org/publishing/books/the-long-view/\"><i>the long view</i></a><i>\u201d). The summary was written by Riley Harris.</i></p><p>Political decisions can have lasting effects on the lives and wellbeing of future generations. Yet political institutions tend to make short-term decisions with only the current generation \u2013 or even just the current election cycle \u2013 in mind. In \u201clongtermist institutional reform\u201d, Tyler M. John and William MacAskill identify the causes of short-termism in government and give four recommendations for how institutions could be improved. These are the creation of in-government research institutes, a futures assembly, posterity impact statements and \u2013 more radically \u2013 an \u2018upper house\u2019 representing future generations.</p><h2><strong>Causes of short-termism</strong></h2><p>John and MacAskill discuss three main causes of short-termism. Firstly, politicians may not care about the long term. This may be because they discount the value of future generations, or simply because it is easy to ignore the effects of policies that are not experienced here and now. Secondly, even if politicians are motivated by concern for future generations, it may be difficult to know the long-term effects of different policies. Finally, even motivated and knowledgeable actors might face structural barriers to implementing long-term focussed policies \u2013 for instance, these policies might sometimes appear worse in the short-term and reduce a candidate's chances of re-election.</p><h2><strong>Suggested reforms</strong></h2><h3><strong>In-government research institutes</strong></h3><p>The first suggested reform is the creation of in-government research institutes that could independently analyse long-term trends, estimate expected long-term impacts of policy and identify matters of long-term importance. These institutes could help fight short-termism by identifying the likely future impacts of policies, making these impacts vivid, and documenting how our leaders are affecting the future. They should also be designed to resist the political incentives that drive short-termism elsewhere. For instance, they could be functionally independent from the government, hire without input from politicians, and be flexible enough to prioritise the most important issues for the future. To ensure their advice is not ignored, the government should be required to read and respond to their recommendations.</p><h3><strong>Futures assembly</strong></h3><p>The futures assembly would be a permanent&nbsp; citizens\u2019 assembly which seeks to represent the interests of future generations and give dedicated policy time to issues of importance for the long-term. Several examples already exist where similar citizens\u2019 assemblies have helped create consensus on matters of great uncertainty and controversy, enabling timely government action.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1zyf8gdqc5l\"><sup><a href=\"#fn1zyf8gdqc5l\">[1]</a></sup></span><strong>&nbsp;</strong>In-government research institutes excel at producing high quality information, but lack legitimacy. In contrast, a citizens\u2019 assembly like this one could be composed of randomly selected citizens that are statistically representative of the general population. John and MacAskill believe this representativeness brings political force \u2013politicians who ignore the assembly put their reputations at risk. We can design futures assemblies to avoid the incentive structures that result in short-termism \u2013 such as election cycles, party interests and campaign financing. Members should be empowered to call upon experts, and their terms should be long enough to build expertise but short enough to avoid problems like interest group capture \u2013 perhaps two years. They should also be empowered to set their own agenda and publicly disseminate their results.</p><h3><strong>Posterity impact statements</strong></h3><p>Requiring posterity impact statements for legislation would provide another mechanism for creating political accountability and gathering high quality information on long-run policy effects. These statements would give an estimate of the expected impact of a policy on future generations, similar to the environmental impact statements that are already required in many countries. Posterity impact statements might utilise a \u201csoft\u201d enforcement mechanism \u2013 relying on voters to enforce good long-term policy creation \u2013 or a \u201chard\u201d enforcement mechanism \u2013 for example, the government might have to take out insurance when implementing particularly risky policies.&nbsp;</p><h3><strong>Future generations \u2018upper house\u2019</strong></h3><p>A more radical reform would be to introduce an \u2018upper house\u2019 that represents future generations explicitly, to work with a lower house representing current generations. (Legislation would have to pass both houses). John and MacAskill suggest several things that might help such a proposal work:</p><ul><li>Randomly selecting citizens and experts to serve in the upper house (to avoid the incentives that drive short-termism, such as election cycles, party interests, industry corruption and partisan polarisation).&nbsp;</li><li>Independent research institutions should create concrete performance metrics and members of the house should give public justifications that refer to those metrics.&nbsp;</li><li>The members should be relatively young, and given a pension a number of decades later based on their cohort\u2019s performance in promoting the interests of future generations.&nbsp;</li></ul><h2><strong>Further ideas for reforms</strong></h2><p>John and MacAskill also suggest several additional ideas for reforms that might be worth exploring further: longer election cycles, novel commitment mechanisms, giving parents additional votes to use on behalf of their children, taxation of negative and subsidy of positive long-run externalities, and long-term performance incentive schemes such as tying the pensions of politicians and public servants to national performance.</p><h2><strong>References</strong></h2><p>James S Fishkin and Robert C Luskin (2005). <a href=\"https://link.springer.com/article/10.1057/palgrave.ap.5500121\">Experimenting with a democratic ideal: Deliberative polling and public opinion</a>. <i>Acta Politica</i> 40/3.</p><p>James S. Fishkin, Roy William Mayega, Lynn Atuyambe, Nathan Tumuhamye, Julius Ssentongo, Alice Siu and William Bazeyo (2017). <a href=\"https://www.amacad.org/publication/applying-deliberative-democracy-africa-ugandas-first-deliberative-polls\">Applying deliberative democracy in Africa: Uganda\u2019s first deliberative polls</a>. <i>Daedalus </i>146/3.</p><p>Tyler M. John and William MacAskill (2021). <a href=\"https://firstforum.org/publishing/books/the-long-view/\">Longtermist institutional reform</a>. <i>The Long View: Essays on Policy, Philanthropy, and the Long-Term Future.</i> FIRST. Edited by Natalie Cargill and Tyler M. John.</p><p>Christian List, Robert C. Luskin, James S. Fishkin, and Iain McLean (2013). <a href=\"https://www.journals.uchicago.edu/doi/10.1017/S0022381612000886\">Deliberation, single-peakedness, and the possibility of meaningful democracy: Evidence from deliberative polls.</a> <i>The Journal of Politics</i> 75/1.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1zyf8gdqc5l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1zyf8gdqc5l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See Fishkin and Luskin (2005), Fishkin et al. (2017) and List et al. (2013).</p></div></li></ol>", "user": {"username": "Global Priorities Institute"}}, {"_id": "RqzSQwGEPmvbemgkH", "title": "A BOTEC-Model for Comparing Impact Estimations in Community Building", "postedAt": "2023-03-14T06:26:47.526Z", "htmlBody": "<p><i>We are grateful to Anneke Pogarell, Birte Spekker, Calum Calvert, Catherine Low, Joan Gass, Jona Glade, Jonathan Michel, Kyle Lucchese, Moritz Hanke and Sarah Pomeranz for conversations and feedback that significantly improved this post. Any errors, of fact or judgment, remain our entirely own.</i></p><h1>Summary</h1><p>When prioritising future programs in EA community building, we currently lack a quantitative way to express underlying assumptions. In this post, we look at different existing approaches and present our first version of a model. We intended it to make Back-of-the-envelope (BOTEC) estimations by looking at an intervention (community building or marketing activity) and thinking about how it might affect participants on their way to having a more impactful life. The model uses an estimation of the average potential of people in a group to have an impact on their lives as well as the likelihood of them achieving it. If you\u2019d like only to have a look at the model, you can skip the first paragraphs and directly go to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/RqzSQwGEPmvbemgkH/a-botec-model-for-comparing-impact-estimations-in-community#Our_current_model\">Our current model</a>.</p><h1>Epistemic Status</h1><p>We spent about 40-60 hours thinking about this, came up with it from scratch as EA community builders and are uncertain of the claims.</p><h1>Motivation</h1><p>As new co-directors of EA Germany, we started working on <a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023\">our strategy</a> last November, collecting the requests for programs from the community and looking at existing programs of other national EA groups. While we were able to include some early on as they seemed broadly useful, we were unsure about others. Comparing programs that differ in target group size and composition as well as the type of intervention meant that we would have to rely on and weigh a set of assumptions. To discuss these assumptions and ideally test some of them out, we were looking for a unified approach in the form of a model with a standardised set of parameters.</p><h1>Impact in Community Building</h1><p>The term community building in effective altruism can cover various activities like mass media communication, education courses, speaker events, multi-day retreats and 1-1 career guiding sessions. The way we understand it is more about the outcome than the process, covering not only activities that focus on a community of people. It could be any action that guides participants in their search for taking a significant action with a high expected impact and to continue their engagement in this search.</p><p>The impact of the community builder depends on their part in the eventual impact of the community members. A community builder who wants to achieve high impact would thus prioritise interventions by the expected impact contribution per invested time or money.&nbsp;</p><p>Charity Evaluators like GiveWell can indicate impact per dollars donated in the form of lives saved, disability-adjusted life years (DALYs) reduced or similar numbers. If we guide someone to donate at all, donate more effectively and donate more, we can assume that part of the impact can be attributed to us.</p><p>For people changing their careers to work on the world's most pressing problems, starting charities, doing research or spreading awareness, it\u2019s harder to assess the impact. We assume an uneven impact distribution per person, probably heavy-tailed. Some people have been responsible for saving millions, such as<a href=\"https://www.theatlantic.com/magazine/archive/1997/01/forgotten-benefactor-of-humanity/306101/\"><u> Norman Borlaug</u></a> or might have averted a global catastrophe like&nbsp;<a href=\"https://80000hours.org/career-guide/can-one-person-make-a-difference/#the-unknown-soviet-lieutenant-colonel-who-saved-your-life\"><u>Stanislav Petrov</u></a>.</p><h1>Existing approaches</h1><h2>Marketing Approach: Multi-Touch Attribution</h2><p><a href=\"https://forum.effectivealtruism.org/posts/zxSdBkN6cggkE8vv6/ea-germany-s-strategy-for-2023#Impact_Considerations\"><u>In our strategy, we write</u></a>:</p><blockquote><p>Finding the people that could be interested in making a change to effective altruistic actions, guiding them through the process of learning and connecting while keeping them engaged up to the point where they take action and beyond is a multi-step process. We expect most people to have multiple touchpoints with content and people along the way, like media, the EAD website, newsletter, books, local community events, fellowships, EAG(x) or 1-1s. Each is expected to affect the person\u2019s engagement and the likelihood of taking the next step in larger commitments. Looking back at a person's steps until they made a large change, we can ask them what share of the decision they attribute to the touchpoints along the way. In marketing, this is called&nbsp;<a href=\"https://en.wikipedia.org/wiki/Attribution_(marketing)\"><u>multi-touch attribution</u></a>.</p></blockquote><p>We describe a formula for calculating the impact that could be attributed to us as community builders. Still, ultimately, we conclude that it needs more work before it is ready to be used. After finalising our strategy, we learned that Open Phil had already worked on a more detailed model on similar principles two years ago.</p><h2>Open Philanthropy: Influences on Longtermist Careers</h2><p>In 2020,&nbsp;<a href=\"https://forum.effectivealtruism.org/s/AKYivq6uKhsyDJXnE\"><u>Open Phil conducted a survey</u></a> of about 200 people working in longtermist areas, querying them in detail about community-building interventions that were important for them getting more involved and helping them have more impact. The respondents mentioned influences in free text, selected them from an existing list and gave them impact points. Additionally, the survey designers gave weights to the respondents according to the expected altruistic value of their careers on a scale of 1-1,000. This&nbsp;<a href=\"https://forum.effectivealtruism.org/s/AKYivq6uKhsyDJXnE/p/3vMLkWHy5m3xvLrdk\"><u>resulted in a list of interventions</u></a> that could be ranked by their importance to the set of people.</p><p>While the survey has many influence factors, the level of detail seems insufficient to make informed predictions in comparing interventions within local or national groups: \u201cLocal EA groups, including student groups\u201d is listed as one factor without splitting it up into the activities. It is also a descriptive model that looks at the influences on people before 2020. If we want to make assumptions about new interventions, we will need another approach.</p><h2>80,000 Hours: Leader\u2019s Survey on Value of Employees</h2><p><a href=\"https://80000hours.org/2018/10/2018-talent-gaps-survey/#ea-leaders-are-willing-to-sacrifice-a-lot-of-extra-donations-to-hold-on-to-their-most-recent-hires\"><u>A 2018 survey of leaders of EA organisations</u></a> included a section that asked what donation amount organisations would be willing to sacrifice instead of losing a recent hire. The results were in the millions of dollars for senior hires, and the subsequent discussion led to a&nbsp;<a href=\"https://80000hours.org/2019/05/why-do-organisations-say-recent-hires-are-worth-so-much/\"><u>clarifying post</u></a>.&nbsp;</p><p>This approach addresses the altruistic impact that people might have in the future.&nbsp;</p><h2>CEA: Three-Factor Model of Community Building</h2><p><a href=\"https://www.centreforeffectivealtruism.org/models-of-community-building\"><u>Among other models</u></a>, CEA uses \u201ca&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/a-three-factor-model-of-community-building\"><u>three-factor model of community building</u></a> where the amount of good someone can be expected to do is assessed as the product of three factors: resources, dedication and realisation.\u201d In the case of donations, the resources could be a person's income, the dedication, the percentage donated, and the realisation of the effectiveness of the charity the money is donated to. Each of these factors can differ by at least an order of magnitude, and the model can be used to think about which of them to try to influence based on different target groups.</p><p>The model seemed very useful to us when looking at individuals taking an impactful action now, like taking the GWWC pledge or donating. For our purpose of comparing different interventions quantitatively based on their future results, we would have had to estimate the three factors before and after an intervention, leading to six parameters. While the CEA model looks at the present values of the factors, we would have had to predict the lifetime impact of someone making a career transition and discounting for them dropping out beforehand.</p><p>Ultimately we found the three-factor model to be too complex for our purposes.</p><h2>CEA: Career Value</h2><p>In April 2022, the CEA groups team gave a presentation where they estimated the counterfactual value of different actions.</p><p>Based on average salaries, times to retirement and drop-off rates, they roughly calculated $70,000 in expected donations for a GWWC pledge.</p><p>For a longtermist career, they estimated the number of people working in the field, the amount pledged to longtermist causes at that time and the willingness of organisations to hire a good candidate earlier rather than later. The result was a value of $18 million over the lifetime of a new employee. They noted that this value changed based on the potential contribution of the person and what role they would fill.&nbsp;</p><p>For non-longtermist causes, they estimated 10x less counterfactual value as there was less money pledged and cause areas were not as talent constrained. In their presentation, they then tried to tease out intuitions of where community builders should focus if they assumed they were&nbsp; 10% responsible for each action outlined above.&nbsp;</p><p>As the presentation is from April 2022, the numbers for career changes would be much lower after&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yjGye7Q2jRG3jNfi2/ftx-crisis-what-we-know-and-some-forecasts-on-what-will?commentId=5dap8ZTnSg2JDYiSn#xwsGeTr82mgvXiMYM\"><u>the reduction of the money pledged to longtermist cause areas</u></a> since then.</p><h1>Our current model</h1><p>After looking into the existing approaches, we concluded that the level of uncertainty in predictions didn\u2019t warrant a complex model. Reducing the parameters would make it easier to show our assumptions and focus on points where there might be disagreements. On a high level, we\u2019re focusing on:</p><ul><li>Number of people affected by an intervention</li><li>Change of average expected impact per participant</li></ul><p>The last part is where it gets tricky, and we will discuss it below. Assuming we have the two parameters, we can calculate the impact change, factor in the costs and prioritise them accordingly.</p><h2>Expected Impact</h2><p>We assume people can have very different altruistic impacts.&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/a-three-factor-model-of-community-building\"><u>CEA\u2019s article about the three-factor model</u></a>, describes how each of the three factors could differ by at least one order of magnitude for people in Western countries. The Open Phil Survey uses an impact scale of four orders of magnitude between people working on longtermist causes.</p><p>Making sure that we\u2019re guiding people in the right direction of having a high impact and focusing on those with this opportunity seems important. At the same time, we acknowledge that the future impact of people is tough to predict.</p><p>Looking at an intervention in community building, we can assume it had a positive impact if the average expected impact of each participant is higher after the intervention than before. In an ideal world, we could take two similar groups of people, apply an intervention on one and then record their impact over their lifetime. The difference between participants in both groups would be the change in expected impact.</p><p>For this model, we will assume that expected impact is the product of</p><ul><li>The maximum impact potential of a person over their lifetime</li><li>The likelihood (or expected share) of reaching their impact potential</li></ul><p>To put it into a formula:</p><figure class=\"image image_resized\" style=\"width:52.42%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/fph9kd9htxggfurd9fpt\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/f7n6roagx8ma19kiyp3m 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/ywqschwycmyzbabnefkl 230w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/niwqpxjzjlv9pxbz06tf 310w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/t7x23xviqyki46uelm7l 390w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/zp6bn8vdetzid7wghsro 470w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/prvnvx6reuqdffm2f9a8 550w\"></figure><p>n: Number of participants in the intervention</p><p>i: Average impact potential of participants</p><p>p<sub>1</sub>: Likelihood of participants reaching their impact potential before the intervention</p><p>p<sub>2</sub>: Likelihood of participants reaching their impact potential after the intervention</p><h3>Impact Potential</h3><p>We define the average impact potential of a group as a fixed numerical value. In an ideal world, every person would have the same potential to have a high altruistic impact. As we look at individual interventions in today\u2019s world, we acknowledge that the impact of people is unevenly distributed. We can affect this number by target group selection. A mass media campaign reaching broad parts of the population is expected to have a lower average impact potential than, for example, a small event only for people already working on highly impactful causes.</p><h3>Likelihood of Reaching the Impact Potential</h3><p>The second value we consider is the likelihood (or expected share) of reaching their impact potential. We assume we can affect this through the intervention. Having a career 1-1 with someone might point them in a direction that increases the share of the impact potential they can reach. This can be a value between &gt;0% and 100%.</p><h2>Cost-Effectiveness</h2><p>If we estimate the cost of an intervention (money and hours of work), we can calculate the cost-effectiveness:</p><figure class=\"image image_resized\" style=\"width:48.02%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/nul7tqilxabtry8l4ue7\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/whiized6kraobas7fvf1 100w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/uxzuj5flr3n5v3bfjpfd 180w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/oykgdwcfp8v9om3odgun 260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/hxgvr17y1yqilvxqohl6 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/zrwhpnpqp7wl05mjivdu 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/RqzSQwGEPmvbemgkH/p3pu6evvrzjygwwtzdhe 500w\"></figure><p>We can now use this number to compare interventions.</p><h2>Use Cases</h2><p>The model can be used to compare different interventions without having to define all of the parameters universally.</p><h3>Example 1</h3><p>For example, we could compare the cases of</p><ul><li>Giving an introductory EA talk to a general audience of 100 people</li><li>Giving an introductory EA talk to an audience of 10 entrepreneurs that have just sold their companies</li></ul><p>We could assume the likelihood of audience members reaching their altruistic potential increasing through the talk will be similar, and the effort might be the same. In this case, we can ask ourselves if we see the impact potential of the entrepreneurs as more or less than 10x to prioritise the talk.</p><h3>Example 2</h3><p>Another example would be comparing two interventions at a local EA group. The group size and the average impact potential will stay the same, whereas</p><ul><li>Intervention 1 is highly engaging and needs 10 hours of preparation time</li><li>Intervention 2 is mildly engaging and needs 1 hour of preparation time</li></ul><p>Now we can ask ourselves if we think the percentage point increase of intervention 1 will be 10x higher than for intervention 2 (e.g. 10% to 15% vs 10% to 10.5%) to prioritise.</p><p>If we have more interventions, we could develop categories of target groups and estimate their relative average impact. For example, we could look at the general audience in a region, university students, mid-career leaders etc. We could also ask ourselves the current likelihood of them reaching their potential and how much we might affect it.</p><p>For younger people with fewer years of education, we might see a lower maximum likelihood even after optimal guidance, as they might still drop out before being able to take an impactful job or being able to donate substantially. At the same time, younger people might have a higher impact potential as they can change their career paths more easily.</p><h1>Caveats and Uncertainties</h1><p>This model currently does not address externalities, especially downside risks. We could see vetting against potential harms as a separate phase in the prioritisation process. Additionally, it also does not consider counterfactuals, with some interventions being more likely to happen anyways than others.</p><p>Given the high uncertainties, big error bars should be given around all parameters. While our model is designed to help prioritise interventions that help guide individuals to take significant actions, this approach might undervalue building a diverse and resilient EA community. The post \u201c<a href=\"https://forum.effectivealtruism.org/s/LBvAQ7rbnL2PANwJh/p/54vAiSFkYszTWWWv4\"><u>Doing EA Better</u></a>\u201d argues that quantifying might lead to worse results and weaken collective epistemics. We agree that this is a possibility and that it is easy to place too much emphasis on quantitative predictions that are merely based on intuition. Perhaps intuition without quantification might be better suited, or we might need models that let us formulate hypotheses so we can validate them.</p><p>We\u2019re not researchers but community builders that need a framework to think about our decisions and will have to prioritise our next interventions soon. Given these constraints, we think it\u2019s more useful to publish our current thoughts after a couple of weeks than to spend more time working on them alone. We\u2019re curious to get feedback and are open to the possibility that this model won\u2019t be useful to others or even to us after thinking about it more.<br>&nbsp;</p>", "user": {"username": "gruban"}}, {"_id": "sYhTgXc69xb8TngT7", "title": "Research Summary: Prediction Polling", "postedAt": "2023-03-13T18:31:34.165Z", "htmlBody": "<p>Note: I initially published this summary as a blog post <a href=\"https://damienlaird.substack.com/p/research-prediction-polling\">here</a>. The blog has more information about the context of this post and my reasoning transparency. In brief, this post was me summarizing what I learned about prediction polling, a specific flavor of forecasting, while trying to understand how it could be applied to forecasting global catastrophic risks. If I cite a source, it means I read it in full. If I relay a claim, it means it's made in the cited source and that I found it likely to be true (&gt;50%). I do not make any original arguments (that's done elsewhere on the blog), but I figured this might be helpful in jumpstarting other people's understanding on the topic or directing them towards sources that they weren't aware of. Any and all feedback is most welcome.</p><p>The text should all be copied exactly from my original post, but I redid the footnotes/citations to use the EA forum formatting. If any of those are missing/seem incorrect please let me know.</p><hr><p>Prediction polling is asking someone their expected likelihood of an event. I ask, \u201cWhat is the percent chance that a meteorite larger than a baseball falls through my roof before 2025?\u201d and you say \u201c2%\u201d. You\u2019ve been prediction polled.</p><p>This term isn\u2019t very common in the research that I\u2019ve reviewed so far. If you\u2019ve seen these topics discussed before it was probably under a heading like \u201cForecasting Tournaments\u201d. In fact, that\u2019s what I initially planned on titling this post. To date, most forecasting tournaments have used prediction polling, but in principle they could be competitions between forecasters using any forecasting technique and some actually have been used to compare prediction polling and prediction markets. So, this post is focused on asking people to directly estimate likelihoods, but it will end up covering a lot of the most significant historical forecasting tournaments that I\u2019m aware of.</p><p>Polls are almost legendarily inaccurate, so why would we think we could get useful information for predicting future events? Well, researchers actually weren\u2019t sure at all that we would and lots of laboratory experiments showed that humans were fallacy prone beasts with a very weak grasp of probabilistic fundamentals. Not to mention the complexity of the real world we inhabit.</p><p>Luckily, the Intelligence Advanced Research Projects Activity (IARPA), an organization within the US federal government kicked off the Aggregative Contingent Estimation (ACE) Program in 2011 to try and find ways to improve the accuracy of intelligence forecasts. It pitted teams of researchers against each other to find out who could most accurately predict real world events in geopolitics. Examples included whether a given ruler would be overthrown, the odds of an armed conflict, or the price of a given commodity. These questions were all objectively resolvable with clear criteria. This means that we can retrospectively compare which teams successfully predicted questions along with how accurately and how quickly they came to these conclusions.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffgguw2ioi8a\"><sup><a href=\"#fnfgguw2ioi8a\">[1]</a></sup></span></p><p>Of particular interest is the winning team of the initial tournament (First two years), the Good Judgement Project (GJP), led by Phillip Tetlock. Not only did it win, it split its large pool of recruited forecasters between various experimental conditions that would let us compare their relative effects on forecaster accuracy. Forecasts on a given question could be continually updated until it was resolved, and the best GJP forecasting method was on the right side of 50/50 in 86.2% of all the daily forecasts across 200 questions selected for their relevance to the intelligence community! This was 60% better than the average of solo forecasters and 40% better than the other teams.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffgguw2ioi8a\"><sup><a href=\"#fnfgguw2ioi8a\">[1]</a></sup></span>&nbsp;As a happy side effect, participants\u2019 opinions on which US policies should be implemented became were less extreme on a liberal/conservative spectrum after 2 years of forecasting, despite this not being at all the topic of the questions. Researchers theorized that this could have been caused by an overall reduction in confidence thanks to the grueling epistemic conditions of working with uncertainty paired with a very tight feedback loop on the accuracy of their predictions.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref29g206ja7hg\"><sup><a href=\"#fn29g206ja7hg\">[2]</a></sup></span></p><p>From their experiments we learned that you could train forecasters in debiasing their predictions to improve their accuracy by 10%, put them in collaborative teams to increase their accuracy by 10%, or use an aggregation algorithm to combine the forecasts of many different forecasters and improve accuracy by 35% compared to an unweighted average of those forecasts! These effects were found to be largely independent meaning you could stack them for cumulative effects, thus the dominance of GJP in the competition. Perhaps the most interesting finding was that the most accurate individual forecasters in each experimental condition in year 1, the top 2%, continued to outperform their peers in year 2. This showed that at least part of forecasting performance was an individual skill that could be identified and/or cultivated, rather than just luck.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffgguw2ioi8a\"><sup><a href=\"#fnfgguw2ioi8a\">[1]</a></sup></span></p><p>Digging slightly deeper into these interventions, the de-biasing training was only conducted once per year for 9 month long tournament sessions, but forecasters that participated in it showed increased accuracy over their peers for that full duration.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxmitti4tbk\"><sup><a href=\"#fnxmitti4tbk\">[3]</a></sup></span>&nbsp;After the first 2 years of the GJP, the training was updated to be graphical, incorporate feedback from top forecasters, and add a module on political reasoning. Overall the training was still designed to take less than an hour, but trained forecasters remained more accurate then untrained ones over the course of each year. Both groups of forecasters grew more accurate over time with practice, but trained individuals seemed to improve faster. Researchers theorized that training could probably have a much stronger impact than this based on the low intensity (1 hour, once a year!) and the minimal refinement its contents had gone through.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz7uinper0tk\"><sup><a href=\"#fnz7uinper0tk\">[4]</a></sup></span></p><p>Multiple team conditions were examined and forecasters collaborating together with teammates were the most accurate, followed by those who could see each other\u2019s forecasts but not collaborate, followed by fully independent forecasters.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxmitti4tbk\"><sup><a href=\"#fnxmitti4tbk\">[3]</a></sup></span>&nbsp;This might not be what you would have expected a priori, with phenomenon like groupthink and diffusion of responsibility giving mechanisms for teamwork to have potentially reduced accuracy.</p><p>In years 2 and 3 of the GJP, the team dynamics associated with success were examined in more detail. Teammates collaborated via an online platform where they were able to see forecasts from their teammates as well as comment on these forecasts and other comments. Teams formed from the top 2% most accurate forecasters from the prior year performed above and beyond what would be predicted from their individual accuracy, implying additional factors at play. These teams left more and longer rationales accompanying their initial forecasts, as well as more and longer comments. Engagement in these conversations was much more evenly distributed among top teams than other teams. More team members being engaged on a given question was also associated with increasing forecast accuracy for that question, plateauing around 50-60% team participation. Analysis of the text contents of forecast explanations and responses showed that top teams more often discussed metacognition, thinking about how they were thinking. They also more frequently employed concepts from their probability/debiasing training, and talked about collaborating with their teammates.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5j45vodik7i\"><sup><a href=\"#fn5j45vodik7i\">[5]</a></sup></span></p><p>After 10 years of GJP participants, from the aforementioned \u201csuperforecasters\u201d to Mechanical Turk workers, writing rationales to accompany their numerical forecasts, researchers looked for patterns associated with accuracy. Their takeaway was that \u201cIf you want more accurate probability judgments in geopolitical tournaments, you should look for good perspective takers who are tolerant of cognitive dissonance (have high IC [Integrative Complexity] and dialectical scores) and who draw adeptly on history to find comparison classes of precedents that put current situations in an outside-view context.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4g7n9yt037d\"><sup><a href=\"#fn4g7n9yt037d\">[6]</a></sup></span></p><p>The massive gains in accuracy provided by aggregation algorithms are also interesting. Early aggregation efforts found that forecasters tended to be underconfident that a predicted event would occur.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk93r1kahxjg\"><sup><a href=\"#fnk93r1kahxjg\">[7]</a></sup></span>&nbsp;Using this self evaluation of expertise along with the past accuracy of the forecasters, the aggregation algorithm could be made quite accurate<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffgguw2ioi8a\"><sup><a href=\"#fnfgguw2ioi8a\">[1]</a></sup></span>. Researchers theorized that the systemic under confidence of forecasters that made aggregation algorithms came from two main sources:</p><p>A rational forecaster will have an initial forecast of 50% for a binary question and update towards the expected correct outcome (either 0% or 100%) as they gain information and therefore confidence. This prior and the impossibility of having total information leads them to under predict outcomes.</p><p>The bounded nature of the 0-100% probability scale means that random noise is more likely to pull your forecast towards whatever end of the scale its farthest from. IE, if your current forecast is 90%, there\u2019s a lot more room for random noise to pull you back towards 0%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl9nctbqj3i\"><sup><a href=\"#fnl9nctbqj3i\">[8]</a></sup></span></p><p>A later mathematical exploration of forecasting error created a model for disaggregating the contributions of interventions to improvements on accuracy into three components: Bias, Information, and Noise. Bias is systematic error across predictions such as being chronically overconfident in change, information error stems from having an incomplete picture of events, and noise is random error. The key finding is that all of the interventions in the GJP (training, teaming, and identifying superforecasters) primarily improved accuracy via reduction in the noise component of this model. Roughly a 50% contribution vs. 25% each for bias reduction and information improvement. The relative consistency of this across the different interventions, combined with my inability to follow the math involved and this model clashing with my own intuitions and experiences all keep me from putting too much weight into their explanation for why this model works. That being said, the researchers share that this model underpins their most successful aggregation to date, which is being used on work that is still in progress. My unfounded suspicion is that this math is correct and representing something real, but these parameters don\u2019t directly correspond to the concepts they\u2019ve named them after.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn6t3cb5glrb\"><sup><a href=\"#fnn6t3cb5glrb\">[9]</a></sup></span></p><p>You can think of prediction markets, where participants bet on the outcome of an event and the current market price reflects a likelihood estimate, as a sort of aggregation algorithm. Researchers used years 2 and 3 of the GJP to have some forecasters submit predictions via markets in addition to the predominant polling condition. The aggregation algorithm on polled predictions was found to be significantly more accurate than prediction markets, which in turn were found to be significantly more accurate than the simple averaging of forecasts.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftdlphttebj\"><sup><a href=\"#fntdlphttebj\">[10]</a></sup></span></p><p>This wasn\u2019t an apples to apples comparison as no prediction market users were in the \u201cteam\u201d condition of the experiments, while some prediction poll users were. Additionally, the lack of real money incentives likely reduced market accuracy. Interestingly, the aggregated polls were most accurate relative to the markets on longer duration questions and particularly during the beginning and end of their open periods. Researchers speculated that this is when forecasters were most uncertain, and this uncertainty likely translated into large spreads and lower liquidity. Some forecasters were asked to submit their predictions via both polling and trading in markets at the same time, and aggregation algorithms that incorporated both sets of data outperformed all the rest indicating that there was additional information captured just by asking again for what should have been the same thing.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftdlphttebj\"><sup><a href=\"#fntdlphttebj\">[10]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflmbw7d5nge\"><sup><a href=\"#fnlmbw7d5nge\">[11]</a></sup></span></p><p>In addition to a variety of experimental conditions within the tournament, GJP also subjected participants to a battery of psychometric tests at the start of each year. This allowed researchers to look for patterns in what consistently differentiated the most accurate forecasters from the rest. The most accurate forecasters were found to\u2026</p><ul><li>Be at least one standard deviation higher in fluid intelligence (across many means of measurement) than the general population.</li><li>Be at least one standard deviation higher than the general population in crystallized intelligence, and even higher on political knowledge questions.</li><li>Enjoy solving problems more, have a higher need for cognition, and were more inclined towards actively open minded thinking than other participants.</li><li>Be more inclined towards a secular agnostic/atheistic worldview that did not attribute events to fate or the supernatural, including close-call counterfactuals.</li><li>Be more scope sensitive.</li><li>Make forecasts with greater granularity, and this greater granularity was relevant to their higher accuracy.</li><li>Answer more questions, updated their forecasts more frequently, and investigated relevant news articles more often.</li><li>Engage with their teams more often with more, longer comments and forum posts. These comments were more likely to contain questions and they were more likely to respond to questions being asked of them. They were also more likely to reply to the comments of others in general.</li><li>Converge towards a tighter distribution of responses over time with their teammates, while other teams actually diverged.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa1q06qz5brk\"><sup><a href=\"#fna1q06qz5brk\">[12]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefam0yayimz56\"><sup><a href=\"#fnam0yayimz56\">[13]</a></sup></span></li></ul><p>Many of the above points were replicated via a later IARPA tournament on a population of forecasters more diverse than the mainly academic recruits from the GJP. Forecasting performance remained stable over time and could be best predicted by past performance, though fluid intelligence, numeracy, and engagement were still shown to be significant predictors.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxng7lu1asmh\"><sup><a href=\"#fnxng7lu1asmh\">[14]</a></sup></span></p><p>The point above regarding granularity refers to how finely along the percentage scale from 0-100% forecasters could meaningfully differentiate between probabilities. Researchers could test this after the fact by rounding forecasts into various sized \u201cbins\u201d and seeing if this degraded accuracy. They found that the top 2% of forecasters benefited significantly from using at least 11 bins while the entire population of forecasters benefited from using more than 7. This didn\u2019t vary significantly based on forecaster attributes, type of question, or duration of question.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7i2ayp0iw8j\"><sup><a href=\"#fn7i2ayp0iw8j\">[15]</a></sup></span></p><p>Across the first 4 years of the GJP, researchers also examined a link between how forecasters updated their predictions and their accuracy. Forecasters that updated their predictions more frequently tended to be more accurate, scored higher on crystallized intelligence and open-mindedness, accessed more information, and improved over time. Forecasters that made updates with smaller magnitudes tended to be more accurate. They scored higher on fluid intelligence and had more accurate initial forecasts. These results are part of the impact training had on improving accuracy. Conversely, large updates and affirming prior forecasts rather than adjusting them were associated with lower accuracy. Uniformly adjusting the magnitude of forecast updates almost exclusively decreased forecast accuracy. Decreasing update magnitude by 30% significantly worsened accuracy while increasing update magnitude by 30% very slightly improved it, dwarfed by other interventions by 1-2 orders of magnitude. It\u2019s unclear how specific these results are to the domain of global political forecasting of events with near term time horizons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgc2o0lk0d5\"><sup><a href=\"#fngc2o0lk0d5\">[16]</a></sup></span>&nbsp;It\u2019s possible that these update behaviors were most accurate because they best mirrored the reality of unfolding events and information availability, and that the most accurate behaviors in other contexts would be very different.</p><p>While all of this fantastic research from the GJP informed me greatly on what kinds of forecasting was viable in the world of geopolitics, and in what context, there were some significant gaps remaining in being able to confidently apply prediction polling to forecasting Global Catastrophic Risks.</p><p>The most glaring gap is the long delay before finding out if a prediction was accurate or not. When a question is resolving in a few months you can just wait to find out. When it\u2019s resolving in a few decades, this approach isn\u2019t as useful. If you want to predict counterfactual scenarios, such as the impact of a hypothetical policy being implemented, even waiting decades would never get you an answer!</p><p>A possible answer is reciprocal scoring. Up until this point all measures of accuracy that I\u2019ve referenced have used a \u201cproper\u201d (or objective) scoring rule that purely and directly incentivizes accuracy from forecasters by comparing their results to reality. Reciprocal scoring is an intersubjective scoring rule, in that it asks forecasters to predict the forecasts of other, skilled forecasters. The theoretical underpinnings imply that when properly incentivized, forecasters will still strive to forecast the truth when being judged by reciprocal scoring. To test this, researchers had forecasters in different groups use the two different methods of scoring, on objectively resolvable questions, and found the accuracy of the forecasts to be statistically identical! Then, in a second study, forecasters being judged by reciprocal scoring predicted the death toll from COVID-19 if different policies or combinations of policies had been implemented immediately.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkvuh68wrfvk\"><sup><a href=\"#fnkvuh68wrfvk\">[17]</a></sup></span></p><p>The resulting conclusions seemed reasonable and consistent, at least as best as you could tell in hindsight and against universes that never came to be. Of the 19 of 21 tournament participants in Study 2 that completed a post-tournament survey, only 12 of them (63%) responded \u201cI always reported my exact true beliefs\u201d. On average the other 7 reported there was only a 15% difference between their beliefs and the beliefs they forecasted. The reasons shared for not reporting their true beliefs were, in popular order:</p><ul><li>\u201cI thought that my team\u2019s median forecast was a good proxy for the other team\u2019s median, even if it was objectively inaccurate\u201d</li><li>\u201cI thought I was able to identify patterns in how forecasters would respond to these questions\u201d</li><li>\u201cI thought that forecasters in the other group might be biased in a certain direction\u201d</li><li>\u201cI thought that I might have obtained information that the forecasters in the other group did not have access to\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkvuh68wrfvk\"><sup><a href=\"#fnkvuh68wrfvk\">[17]</a></sup></span></li></ul><p>It\u2019s very interesting to me that reciprocal scoring could have been so accurate in the first study, yet a third of participants in the second study report intentionally deviating from trying to directly predict accuracy. Is it possible that these deviations actually generally reduce noise/error by pulling otherwise less accurate forecasters towards the median?</p><p>Another obstacle in the application of prediction polling to GCRs is knowing which questions to ask forecasters in the first place in order to get the most useful information. We could just let subject matter experts create them, but is this the best way? One solution being explored is intentionally including questions from along a rigor-relevance tradeoff spectrum. Some near term indicators that are objectively scorable and expected (but not known) to be indicative of longer term events of interest, and longer run outcomes that we directly care about but that will need to be intersubjectively scored. Another is crafting \u201cconditional trees\u201d, where forecasters identify the branching impact of early indicators on the probability of later outcomes, to systematically identify the most relevant questions to forecast.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3q4n6k0iha7\"><sup><a href=\"#fn3q4n6k0iha7\">[18]</a></sup></span></p><p>Lots of other interventions, like controlling the structure of teams or improving training are hypothesized in improving forecasters\u2019 performance in this new domain. Researchers are working on designing and facilitating a next generation of forecasting tournaments to, in the spirit of the GJP, figure out what works and what doesn\u2019t empirically.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3q4n6k0iha7\"><sup><a href=\"#fn3q4n6k0iha7\">[18]</a></sup></span>&nbsp;I believe I actually had the honor of participating in the first of these recently, and I\u2019ll be tracking the publications of the associated researchers so that I can continue to update this page.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfgguw2ioi8a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffgguw2ioi8a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tetlock, Philip E., et al. \"<a href=\"https://doi.org/10.1177/0963721414534257\"><u>Forecasting tournaments: Tools for increasing transparency and improving the quality of debate</u></a>.\" <i>Current Directions in Psychological Science</i> 23.4 (2014): 290-295.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn29g206ja7hg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref29g206ja7hg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mellers, Barbara, Philip Tetlock, and Hal R. Arkes. \"<a href=\"https://doi.org/10.1016/j.cognition.2018.10.021\"><u>Forecasting tournaments, epistemic humility and attitude depolarization</u></a>.\" <i>Cognition</i> 188 (2019): 19-26.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxmitti4tbk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxmitti4tbk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mellers, Barbara, et al. \"<a href=\"https://doi.org/10.1177/0956797614524255\"><u>Psychological strategies for winning a geopolitical forecasting tournament</u></a>.\" <i>Psychological science</i> 25.5 (2014): 1106-1115.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz7uinper0tk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz7uinper0tk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Chang, Welton, et al. \"<a href=\"https://doi.org/10.1017/S1930297500004599\"><u>Developing expert political judgment: The impact of training and practice on judgmental accuracy in geopolitical forecasting tournaments</u></a>.\" <i>Judgment and Decision making</i> 11.5 (2016): 509-526.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5j45vodik7i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5j45vodik7i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Horowitz, Michael, et al. \"<a href=\"https://doi.org/10.1086/704437\"><u>What makes foreign policy teams tick: Explaining variation in group performance at geopolitical forecasting</u></a>.\" <i>The Journal of Politics</i> 81.4 (2019): 1388-1404.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4g7n9yt037d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4g7n9yt037d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Karvetski, Christopher, et al. \"<a href=\"https://dx.doi.org/10.2139/ssrn.3779404\"><u>Forecasting the accuracy of forecasters from properties of forecasting rationales</u></a>.\" <i>Available at SSRN 3779404</i> (2021).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk93r1kahxjg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk93r1kahxjg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Satop\u00e4\u00e4, Ville A., et al. \"<a href=\"https://doi.org/10.1016/j.ijforecast.2013.09.009\"><u>Combining multiple probability predictions using a simple logit model</u></a>.\" <i>International Journal of Forecasting</i> 30.2 (2014): 344-356.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl9nctbqj3i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl9nctbqj3i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Baron, Jonathan, et al. \"<a href=\"https://doi.org/10.1287/deca.2014.0293\"><u>Two reasons to make aggregated probability forecasts more extreme</u></a>.\" <i>Decision Analysis</i> 11.2 (2014): 133-145.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn6t3cb5glrb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn6t3cb5glrb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Satop\u00e4\u00e4, Ville A., et al. \"<a href=\"https://doi.org/10.1287/mnsc.2020.3882\"><u>Bias, information, noise: The BIN model of forecasting</u></a>.\" <i>Management Science</i> 67.12 (2021): 7599-7618.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntdlphttebj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftdlphttebj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Atanasov, Pavel, et al. \"<a href=\"https://doi.org/10.1287/mnsc.2015.2374\"><u>Distilling the wisdom of crowds: Prediction markets vs. prediction polls</u></a>.\" <i>Management science</i> 63.3 (2017): 691-706.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlmbw7d5nge\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflmbw7d5nge\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Dana, Jason, et al. \"<a href=\"https://doi.org/10.1017/S1930297500003375\"><u>Are markets more accurate than polls? The surprising informational value of \u201cjust asking</u></a>\u201d.\" <i>Judgment and Decision Making</i> 14.2 (2019): 135-147.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna1q06qz5brk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa1q06qz5brk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mellers, Barbara, et al. \"<a href=\"https://doi.org/10.1177/1745691615577794\"><u>Identifying and cultivating superforecasters as a method of improving probabilistic predictions</u></a>.\" <i>Perspectives on Psychological Science</i> 10.3 (2015): 267-281.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnam0yayimz56\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefam0yayimz56\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mellers, Barbara, et al. \"<a href=\"https://psycnet.apa.org/doi/10.1037/xap0000040\"><u>The psychology of intelligence analysis: drivers of prediction accuracy in world politics</u></a>.\" <i>Journal of experimental psychology: applied</i> 21.1 (2015): 1.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxng7lu1asmh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxng7lu1asmh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Himmelstein, Mark, Pavel Atanasov, and David V. Budescu. \"<a href=\"https://doi.org/10.1017/S1930297500008597\"><u>Forecasting forecaster accuracy: Contributions of past performance and individual differences</u></a>.\" <i>Judgment and Decision Making</i> 16.2 (2021): 323-362.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7i2ayp0iw8j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7i2ayp0iw8j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Friedman, Jeffrey A., et al. \"<a href=\"https://doi.org/10.1093/isq/sqx078\"><u>The value of precision in probability assessment: Evidence from a large-scale geopolitical forecasting tournament</u></a>.\" <i>International Studies Quarterly</i> 62.2 (2018): 410-422.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngc2o0lk0d5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgc2o0lk0d5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Atanasov, Pavel, et al. \"<a href=\"https://doi.org/10.1145/3391403.3399540\"><u>Small steps to accuracy: Incremental belief updaters are better forecasters</u></a>.\" <i>Proceedings of the 21st ACM Conference on Economics and Computation</i>. 2020.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkvuh68wrfvk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkvuh68wrfvk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Karger, Ezra, et al. \"<a href=\"https://dx.doi.org/10.2139/ssrn.3954498\"><u>Reciprocal scoring: A method for forecasting unanswerable questions</u></a>.\" <i>Available at SSRN 3954498</i> (2021).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3q4n6k0iha7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3q4n6k0iha7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Karger, Ezra, Pavel D. Atanasov, and Philip Tetlock. \"<a href=\"https://dx.doi.org/10.2139/ssrn.4001628\"><u>Improving judgments of existential risk: Better forecasts, questions, explanations, policies</u></a>.\" <i>Questions, Explanations, Policies (January 5, 2022)</i> (2022).</p></div></li></ol>", "user": {"username": "Damien Laird"}}, {"_id": "qbLxcbJMaEb8vHzio", "title": "How WorkStream EA strengthens EA orgs, leaders and impact: our observations, programs and plans", "postedAt": "2023-03-13T13:29:09.696Z", "htmlBody": "<p><strong>Are you an EA leader struggling to find the right resources and support to maximize your organization\u2019s impact?</strong></p><p><a href=\"https://forum.effectivealtruism.org/posts/T8XjJiMWrf4Sy6T6m/introducing-workstream-ea-providing-support-training-and\"><u>We introduced</u></a> Workstream EA organizational development back in October 2022 and have since been monitoring the effect of our programs. We\u2019re pleased to report that participants have reported significant improvement in their effectiveness and confidence, and we look forward to expanding these results to more EA organizations in the coming months.</p><p>To recap, we\u2019ve been offering two primary services:</p><ul><li>Operations and Leadership fellowship programs designed to help upskill core personnel as a lever for organizational strength</li><li>General business coaching with leaders of all stages to help them identify and minimize their bottlenecks to maximize impact</li></ul><p>We\u2019ve also partnered with&nbsp;<a href=\"https://ea-services.org/\"><u>EASE</u></a> to develop a consolidated resource directory for EA orgs (see our post about that <a href=\"https://forum.effectivealtruism.org/posts/mb4kzhfRnpQNtF6ut/introducing-ease-a-managed-directory-of-ea-organization\">here</a>).</p><p>&nbsp;</p><p><strong>Observations</strong><br>We designed our initial programs based on prior professional experience and initial audience understanding, but our observations along the way have revealed a number of changes we will make to provide maximum benefit to this community:&nbsp;</p><ul><li>The gap in prior knowledge and skills between our two cohorts (operations and leadership) is wider than we expected; in our next <a href=\"https://drive.google.com/file/d/1-sD7Lh7hL9cqqmxIevK3xaAkDSWerk73/view?usp=share_link\">fellowship cohorts</a> we will provide extra support and training for ops members, and more application and peer support for our leaders.&nbsp;</li><li>Supplemental coaching alongside the fellowship has increased impact across the board, but especially so for entrepreneurs.</li><li>A massive benefit of our programs has come from high-quality, supportive conversations with engaged peers. We will scale this independently of the fellowship with a new <a href=\"https://drive.google.com/file/d/10CPeSiZGIaafHzsZcG4ZYbLM57KtvVR_/view\">mastermind group</a>.</li><li>We\u2019re learning how to balance the benefits of applied homework with our participants' busy schedules. We\u2019ve reduced out-of-session homework to 1-2 hours/month instead of 1 hour/week, and are incorporating more hands-on application into our workshops.</li><li>We\u2019ve identified additional topics that are necessary for a comprehensive curriculum, including non-profit governance, branding and fundraising. These are optional for our ops cohort but critical for entrepreneurs and leaders.&nbsp;</li><li>Many would benefit from shorter training programs. While we don\u2019t have the wherewithal to create those now, we hope to move in that direction as the organization grows. We are also offering the option to do an accelerated course that meets bi-weekly instead of monthly.</li></ul><p>&nbsp;</p><p><strong>Feedback</strong></p><p>For those who have participated in our programs so far, the qualitative data assessment suggests a substantial increase in organizational stability, impact and growth. Additionally, the participants (particularly those who paired it with coaching) displayed an increase in confidence, personal happiness, community connectedness, achievement, and a decrease in burnout and stress.</p><p>&nbsp;</p><p><strong>Next Steps</strong></p><p><strong>1. Applications for next cohorts:</strong></p><p>We are opening applications for our May cohorts. Given sufficient demand, we will have 3 groups; ops, small-org entrepreneurs, and large-org leaders. If you would like to learn more about the <a href=\"https://drive.google.com/file/d/1-sD7Lh7hL9cqqmxIevK3xaAkDSWerk73/view?usp=share_link\">program</a>, or apply, please submit an application (<a href=\"https://forms.gle/XoJPtsDsb1xh3oPy7\">operations / small orgs</a> or <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdm8iGnrAH0iXCyNKJY8skCJPaSHkLKllsaSRIjRkRgNj9qzQ/viewform?usp=sf_link\">leadership</a> tracks).&nbsp;<br>&nbsp;</p><p><strong>2 . Mastermind groups:</strong></p><p>Based on the feedback we received and general observations, we feel the community would benefit from having mastermind groups that are geared for community support and working through issues with a supportive peer environment. As those require less coordination on our end, we can make those more available to people in distant time zones. It\u2019s a low-cost way to gain value, although it lacks the upskilling and training component of the fellowship. <a href=\"https://forms.gle/vdWT5tgcBYi7tJZT7\">Apply here</a>.</p><p>&nbsp;</p><p><strong>3. Sponsorship:</strong></p><p>We\u2019re also looking for sponsors to subsidize the program for certain individuals. It is always disappointing when a younger org doesn\u2019t have budget to take part in the program (especially since the infusion of support into young orgs has a significant effect on their development), and we\u2019d really love for money not to be the barrier to entry. We did receive an FTX grant for scholarships, but are unable to use those funds, although OpenPhil did cover the scholarships for already enrolled participants. We\u2019re currently seeking sponsors; please <a href=\"mailto:denglander@workstreamsystems.com\">reach out</a> with any interest or leads.&nbsp;</p><p>&nbsp;</p><p>Our goal is to strengthen our EA orgs by providing quality resources and support to infuse the support, education, expertise and direction they need to thrive. We look forward to continue helping our organizations to achieve their maximal impact.</p><p>&nbsp;</p><p>Quick Links:</p><ul><li>Apply for&nbsp;<a href=\"https://forms.gle/XoJPtsDsb1xh3oPy7\"><u>operations / small org leadership track</u></a> (<a href=\"https://drive.google.com/file/d/1-r26UenMKbD2um2Bfp4gepIA3ay9-8-B/view?usp=sharing\"><u>view details</u></a>)</li><li>Apply for the<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdm8iGnrAH0iXCyNKJY8skCJPaSHkLKllsaSRIjRkRgNj9qzQ/viewform?usp=sf_link\"><u> larger org leadership track</u></a> (<a href=\"https://drive.google.com/file/d/1-t-yNuFx4O53KCOr8bX0qE4l6E94gwOO/view?usp=sharing\"><u>view details</u></a>)</li><li>Apply to be part of a&nbsp;<a href=\"https://forms.gle/vdWT5tgcBYi7tJZT7\"><u>mastermind group</u></a> (<a href=\"https://drive.google.com/file/d/10CPeSiZGIaafHzsZcG4ZYbLM57KtvVR_/view?usp=sharing\"><u>view details</u></a>)</li><li><a href=\"mailto:denglander@workstreamsystems.com\"><u>Contact me</u></a> to contribute to our scholarship fund</li><li><a href=\"https://calendly.com/deenaenglander/ops-fellowship-intro-call\">Schedule a time to chat</a> to see how we can help you and your org</li></ul>", "user": {"username": "Deena Englander"}}, {"_id": "FmhYMzoevaBqFTGGs", "title": "How bad a future do ML researchers expect?", "postedAt": "2023-03-13T05:47:31.983Z", "htmlBody": "", "user": {"username": "Katja_Grace"}}, {"_id": "pKG5fsfrgDSQtssfu", "title": "On taking AI risk seriously  ", "postedAt": "2023-03-13T05:44:49.920Z", "htmlBody": "<p>Yet another New York Times piece on AI. A non-AI safety friend sent it to me saying \"This is the scariest article I've read so far. I'm afraid I haven't been taking it very seriously\". I'm noting this because I'm always curious to observe what moves people, what's out there that has the power to change minds. In the past few months, there's been increasing public attention to AI and all sorts of hot and cold takes, e.g., about intelligence, consciousness, sentience, etc. But this might be one of the articles that convey the AI risk message in a language that helps inform and think about AI safety.&nbsp;</p><p>The following is what stood out to me and made me think that it's time for philosophy of science to also take AI risk seriously and revisit the idea of scientific explanation given the success of deep learning:</p><blockquote><p>I cannot emphasize this enough: We do not understand these systems, and it\u2019s not clear we even can. I don\u2019t mean that we cannot offer a high-level account of the basic functions: These are typically probabilistic algorithms trained on digital information that make predictions about the next word in a sentence, or an image in a sequence, or some other relationship between abstractions that it can statistically model. But zoom into specifics and the picture dissolves into computational static.</p><p>\u201cIf you were to print out everything the networks do between input and output, it would amount to billions of arithmetic operations,\u201d writes Meghan O\u2019Gieblyn in her brilliant book, \u201c<a href=\"https://www.penguinrandomhouse.com/books/567075/god-human-animal-machine-by-meghan-ogieblyn/\">God, Human, Animal, Machine</a>,\u201d \u201can \u2018explanation\u2019 that would be impossible to understand.\u201d</p></blockquote>", "user": {"username": "eangelou"}}, {"_id": "9piqRDGX6BisdMdRw", "title": "\"Can We Survive Technology?\" by John von Neumann", "postedAt": "2023-03-13T02:26:48.653Z", "htmlBody": "<p>This is an essay written by <a href=\"https://en.wikipedia.org/wiki/John_von_Neumann\">John von Neumann</a> in 1955, which I think is fairly described as being about global catastrophic risks from emerging technologies. It discusses a bunch of specific technologies that seemed like a big deal in 1955 \u2014&nbsp;which is interesting in itself as a list of predictions; nuclear power! increased automation! weather control? \u2014 but explicitly tries to draw a general lesson.</p>\n<p>von Neumann is regarded as one of the greatest scientists of the 20th century, and was involved in the Manhattan project in addition to inventing zillions of other things.</p>\n<p>I'm posting here because a) I think the essay is worth reading in its own right, and b) I find it interesting to see what the past's intellectuals thought of issues related transformative technology, and how their perspective differs/is similar to ours. Notably, I disagree with several of the conclusions (e.g. von Neumann seems to think differential technological development is doomed).</p>\n<p>On another level, I find the essay, and the fact of it having been written in 1955, somewhat motivating, though not at all in a straightforward way.</p>\n<p>Some quotes:</p>\n<blockquote>\n<p>Since most <em>time scales</em> are fixed by human reaction times, habits, and other physiological and psycho logical factors, the effect of the increased speed of technological processes was to enlarge the size of units \u2014 political, organizational, economic, and cultural \u2014 affected by technological operations. That is, instead of performing the same operations as before in less time, now larger-scale operations were performed in the same time. This important evolution has a natural limit, that of the earth's actual size. The limit is now being reached, or at least closely approached.</p>\n</blockquote>\n<p>...</p>\n<blockquote>\n<p>...there is in most of these developments a trend toward affecting the earth as a whole, or to be more exact, toward producing effects that can be projected from any one to any other point on the earth. There is an intrinsic conflict with geography \u2014 and institutions based thereon \u2014 as understood today.</p>\n</blockquote>\n<p>...</p>\n<blockquote>\n<p>What safeguard remains? Apparently only day-to-day \u2014 or perhaps year-to-year \u2014 opportunistic measures, a long sequence of small, correct decisions.</p>\n</blockquote>\n", "user": {"username": "reallyeli"}}, {"_id": "GhmcdwdT98PE5vCS2", "title": "Yudkowsky on AGI risk on the Bankless podcast", "postedAt": "2023-03-13T00:42:22.669Z", "htmlBody": "<p>Eliezer gave a very frank overview of his take on AI two weeks ago on the cryptocurrency show Bankless:&nbsp;</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=gA1sNLL6yg4\"><div><iframe src=\"https://www.youtube.com/embed/gA1sNLL6yg4\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>I've posted a transcript of the show and a <a href=\"https://twitter.com/i/spaces/1PlJQpZogzVGE\"><strong>follow-up Q&amp;A</strong></a> below.</p><p>Andrea_Miotti, remember, and vonk previously posted transcripts to LessWrong (<a href=\"https://www.lesswrong.com/posts/Aq82XqYhgqdPdPrBA/full-transcript-eliezer-yudkowsky-on-the-bankless-podcast\">1</a>,<a href=\"https://www.lesswrong.com/posts/jfYnq8pKLpKLwaRGN/transcript-yudkowsky-on-bankless-follow-up-q-and-a\">2</a> \u2014 see the comments there for discussion), but they contained many important errors, so I've corrected the transcripts below. (They also weren't available on the EA Forum.)</p><hr><h2><a href=\"https://www.youtube.com/watch?v=gA1sNLL6yg4\">Intro</a></h2><p><strong>Eliezer Yudkowsky</strong>: [clip] I think that we are hearing the last winds start to blow, the fabric of reality start to fray. This thing alone cannot end the world, but I think that probably some of the vast quantities of money being blindly and helplessly piled into here are going to end up actually accomplishing something.</p><p>&nbsp;</p><p><strong>Ryan Sean Adams</strong>: Welcome to Bankless, where we explore the frontier of internet money and internet finance. This is how to get started, how to get better, how to front run the opportunity. This is Ryan Sean Adams. I'm here with David Hoffman, and we're here to help you become more bankless.</p><p>Okay, guys, we wanted to do an episode on AI at Bankless, but I feel like David...</p><p><strong>David:&nbsp;</strong>Got what we asked for.</p><p><strong>Ryan:&nbsp;</strong>We accidentally waded into the deep end of the pool here. And I think before we get into this episode, it probably warrants a few comments. I'm going to say a few things I'd like to hear from you too. But one thing I want to tell the listener is, don't listen to this episode if you're not ready for an existential crisis. Okay? I'm kind of serious about this. I'm leaving this episode shaken. And I don't say that lightly. In fact, David, I think you and I will have some things to discuss in the debrief as far as how this impacted you. But this was an impactful one. It sort of hit me during the recording, and I didn't know fully how to react. I honestly am coming out of this episode wanting to refute some of the claims made in this episode by our guest, Eliezer Yudkowsky, who makes the claim that humanity is on the cusp of developing an AI that's going to destroy us, and that there's really not much we can do to stop it.</p><p><strong>David:</strong> There's no way around it, yeah.</p><p><strong>Ryan:&nbsp;</strong>I have a lot of respect for this guest. Let me say that. So it's not as if I have some sort of big-brained technical disagreement here. In fact, I don't even know enough to fully disagree with anything he's saying. But the conclusion is so dire and so existentially heavy that I'm worried about it impacting you, listener, if we don't give you this warning going in.</p><p>I also feel like, David, as interviewers, maybe we could have done a better job. I'll say this on behalf of myself. Sometimes I peppered him with a lot of questions in one fell swoop, and he was probably only ready to synthesize one at a time.</p><p>I also feel like we got caught flat-footed at times. I wasn't expecting his answers to be so frank and so dire, David. It was just bereft of hope.</p><p>And I appreciated very much the honesty, as we always do on Bankless. But I appreciated it almost in the way that a patient might appreciate the honesty of their doctor telling them that their illness is terminal. Like, it's still really heavy news, isn't it?&nbsp;</p><p>So that is the context going into this episode. I will say one thing. In good news, for our failings as interviewers in this episode, they might be remedied because at the end of this episode, after we finished with hitting the record button to stop recording, Eliezer said he'd be willing to provide an additional Q&amp;A episode with the Bankless community. So if you guys have questions, and if there's sufficient interest for Eliezer to answer, tweet at us to express that interest. Hit us in Discord. Get those messages over to us and let us know if you have some follow-up questions.</p><p>He said if there's enough interest in the crypto community, he'd be willing to come on and do another episode with follow-up Q&amp;A. Maybe even a Vitalik and Eliezer episode is in store. That's a possibility that we threw to him. We've not talked to Vitalik about that too, but I just feel a little overwhelmed by the subject matter here. And that is the basis, the preamble through which we are introducing this episode.</p><p>David, there's a few benefits and takeaways I want to get into. But before I do, can you comment or reflect on that preamble? What are your thoughts going into this one?</p><p><strong>David:&nbsp;</strong>Yeah, we approached the end of our agenda\u2014for every Bankless podcast, there's an equivalent agenda that runs alongside of it. But once we got to this crux of this conversation, it was not possible to proceed in that agenda, because... what was the point?</p><p><strong>Ryan:</strong> Nothing else mattered.</p><p><strong>David:&nbsp;</strong>And nothing else really matters, which also just relates to the subject matter at hand. And so as we proceed, you'll see us kind of circle back to the same inevitable conclusion over and over and over again, which ultimately is kind of the punchline of the content.</p><p>I'm of a specific disposition where stuff like this, I kind of am like, \u201cOh, whatever, okay\u201d, just go about my life. Other people are of different dispositions and take these things more heavily. So Ryan's warning at the beginning is if you are a type of person to take existential crises directly to the face, perhaps consider doing something else instead of listening to this episode.</p><p><strong>Ryan:&nbsp;</strong>I think that is good counsel.</p><p>So, a few things if you're looking for an outline of the agenda. We start by talking about ChatGPT. Is this a new era of artificial intelligence? Got to begin the conversation there.</p><p>Number two, we talk about what an artificial superintelligence might look like. How smart exactly is it? What types of things could it do that humans cannot do?</p><p>Number three, we talk about why an AI superintelligence will almost certainly spell the end of humanity and why it'll be really hard, if not impossible, according to our guest, to stop this from happening.</p><p>And number four, we talk about if there is absolutely anything we can do about all of this. We are heading careening maybe towards the abyss. Can we divert direction and not go off the cliff? That is the question we ask Eliezer.</p><p>David, I think you and I have a lot to talk about during the debrief. All right, guys, the debrief is an episode that we record right after the episode. It's available for all Bankless citizens. We call this the Bankless Premium Feed. You can access that now to get our raw and unfiltered thoughts on the episode. And I think it's going to be pretty raw this time around, David.</p><p><strong>David:</strong> I didn't expect this to hit you so hard.</p><p><strong>Ryan:&nbsp;</strong>Oh, I'm dealing with it right now.</p><p><strong>David:&nbsp;</strong>Really?</p><p><strong>Ryan</strong>: And this is not too long after the episode. So, yeah, I don't know how I'm going to feel tomorrow, but I definitely want to talk to you about this. And maybe have you give me some counseling. (<i>laughs</i>)</p><p><strong>David:</strong> I'll put my psych hat on, yeah.</p><p><strong>Ryan:&nbsp;</strong>Please! I'm going to need some help.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=601\">ChatGPT</a></h2><p><strong>Ryan:</strong> Bankless Nation, we are super excited to introduce you to our next guest. Eliezer Yudkowsky is a decision theorist. He's an AI researcher. He's the seeder of the Less Wrong community blog, a fantastic blog by the way. There's so many other things that he's also done. I can't fit this in the short bio that we have to introduce you to Eliezer.</p><p>But most relevant probably to this conversation is he's working at the Machine Intelligence Research Institute to ensure that when we do make general artificial intelligence, it doesn't come kill us all. Or at least it doesn't come ban cryptocurrency, because that would be a poor outcome as well.</p><p><strong>Eliezer:&nbsp;</strong>(<i>laughs</i>)</p><p><strong>Ryan:&nbsp;</strong>Eliezer, it's great to have you on Bankless. How are you doing?</p><p><strong>Eliezer:</strong> Within one standard deviation of my own peculiar little mean.</p><p><strong>Ryan:&nbsp;</strong>(<i>laughs</i>) Fantastic. You know, we want to start this conversation with something that jumped onto the scene for a lot of mainstream folks quite recently, and that is ChatGPT. So apparently over 100 million or so have logged on to ChatGPT quite recently. I've been playing with it myself. I found it very friendly, very useful. It even wrote me a sweet poem that I thought was very heartfelt and almost human-like.</p><p>I know that you have major concerns around AI safety, and we're going to get into those concerns. But can you tell us in the context of something like a ChatGPT, is this something we should be worried about? That this is going to turn evil and enslave the human race? How worried should we be about ChatGPT and BARD and the new AI that's entered the scene recently?</p><p><strong>Eliezer:</strong> ChatGPT itself? Zero. It's not smart enough to do anything really wrong. Or really right either, for that matter.</p><p><strong>Ryan:</strong> And what gives you the confidence to say that? How do you know this?</p><p><strong>Eliezer:</strong> Excellent question. So, every now and then, somebody figures out how to put a new prompt into ChatGPT. You know, one time somebody found that one of the earlier generations of the technology would sound smarter if you first told it it was Eliezer Yudkowsky. There's other prompts too, but that one's one of my favorites. So there's untapped potential in there that people hadn't figured out how to prompt yet.</p><p>But when people figure it out, it moves ahead sufficiently short distances that I do feel fairly confident that there is not so much untapped potential in there that it is going to take over the world. It's, like, making small movements, and to take over the world it would need a very large movement. There's places where it falls down on predicting the next line that a human would say in its shoes that seem indicative of \u201cprobably that capability just is not in the giant inscrutable matrices, or it would be using it to predict the next line\u201d, which is very heavily what it was optimized for. So there's going to be some untapped potential in there. But I do feel quite confident that the upper range of that untapped potential is insufficient to outsmart all the living humans and implement the scenario that I'm worried about.</p><p><strong>Ryan:</strong> Even so, though, is ChatGPT a big leap forward in the journey towards AI in your mind? Or is this fairly incremental, it's just (for whatever reason) caught mainstream attention?</p><p><strong>Eliezer:</strong> GPT-3 was a big leap forward. There's rumors about GPT-4, which, who knows? ChatGPT is a commercialization of the actual AI-in-the-lab giant leap forward. If you had never heard of GPT-3 or GPT-2 or the whole range of text transformers before ChatGPT suddenly entered into your life, then that whole thing is a giant leap forward. But it's a giant leap forward based on a technology that was published in, if I recall correctly, 2018.</p><p><strong>David:</strong> I think that what's going around in everyone's minds right now\u2014and the Bankless listenership (and crypto people at large) are largely futurists, so everyone (I think) listening understands that in the future, there will be sentient AIs perhaps around us, at least by the time that we all move on from this world.</p><p>So we all know that this future of AI is coming towards us. And when we see something like ChatGPT, everyone's like, \u201cOh, is this the moment in which our world starts to become integrated with AI?\u201d And so, Eliezer, you've been tapped into the world of AI. Are we onto something here? Or is this just another fad that we will internalize and then move on for? And then the real moment of generalized AI is actually much further out than we're initially giving credit for. Like, where are we in this timeline?</p><p><strong>Eliezer:&nbsp;</strong>Predictions are hard, especially about the future. I sure hope that this is where it saturates \u2014 this or the next generation, it goes only this far, it goes no further. It doesn't get used to make more steel or build better power plants, first because that's illegal, and second because the large language model technologies\u2019 basic vulnerability is that it\u2019s not reliable. It's good for applications where it works 80% of the time, but not where it needs to work 99.999% of the time. This class of technology can't drive a car because it will sometimes crash the car.</p><p>So I hope it saturates there. I hope they can't fix it. I hope we get, like, a 10-year AI winter after this.</p><p>This is not what I actually predict. I think that we are hearing the last winds start to blow, the fabric of reality start to fray. This thing alone cannot end the world. But I think that probably some of the vast quantities of money being blindly and helplessly piled into here are going to end up actually accomplishing something.</p><p>Not most of the money\u2014that just never happens in any field of human endeavor. But 1% of $10 billion is still a lot of money to actually accomplish something.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=992\">AGI</a></h2><p><strong>Ryan:</strong> So listeners, I think you've heard Eliezer's thesis on this, which is pretty dim with respect to AI alignment\u2014and we'll get into what we mean by AI alignment\u2014and very worried about AI-safety-related issues.</p><p>But I think for a lot of people to even worry about AI safety and for us to even have that conversation, I think they have to have some sort of grasp of what AGI looks like. I understand that to mean \u201cartificial general intelligence\u201d and this idea of a super-intelligence.</p><p>Can you tell us: if there was a superintelligence on the scene, what would it look like? I mean, is this going to look like a big chat box on the internet that we can all type things into? It's like an oracle-type thing? Or is it like some sort of a robot that is going to be constructed in a secret government lab? Is this, like, something somebody could accidentally create in a dorm room? What are we even looking for when we talk about the term \u201cAGI\u201d and \u201csuperintelligence\u201d?</p><p><strong>Eliezer:</strong> First of all, I'd say those are pretty distinct concepts. ChatGPT shows a very wide range of generality compared to the previous generations of AI. Not very wide generality compared to GPT-3\u2014not literally the lab research that got commercialized, that's the same generation. But compared to stuff from 2018 or even 2020, ChatGPT is better at a much wider range of things without having been explicitly programmed by humans to be able to do those things.</p><p>To imitate a human as best it can, it has to capture all of the things that humans can think about that it can, which is not all the things. It's still not very good at long multiplication (unless you give it the right instructions, in which case suddenly it can do it).&nbsp;</p><p>It's significantly more general than the previous generation of artificial minds. Humans were significantly more general than the previous generation of chimpanzees, or rather <i>Australopithecus</i> or last common ancestor.</p><p>Humans are not <i>fully</i> general. If humans were fully general, we'd be as good at coding as we are at football, throwing things, or running. Some of us are okay at programming, but we're not spec'd for it. We're not&nbsp;<i>fully</i> general minds.</p><p>You can imagine something that's more general than a human, and if it runs into something unfamiliar, it's like, okay, let me just go reprogram myself a bit and then I'll be as adapted to this thing as I am to anything else.</p><p>So ChatGPT is less general than a human, but it's genuinely ambiguous, I think, whether it's more or less general than (say) our cousins, the chimpanzees. Or if you don't believe it's as general as a chimpanzee, a dolphin or a cat.</p><p><strong>Ryan:</strong> So this idea of general intelligence is sort of a range of things that it can actually do, a range of ways it can apply itself?</p><p><strong>Eliezer:&nbsp;</strong>How wide is it? How much reprogramming does it need? How much retraining does it need to make it do a new thing?</p><p>Bees build hives, beavers build dams, a human will look at a beehive and imagine a honeycomb shaped dam. That's. like, humans alone in the animal kingdom. But that doesn't mean that we are general intelligences, it means we're significantly more generally applicable intelligences than chimpanzees.</p><p>It's not like we're all that narrow. We can walk on the moon. We can walk on the moon because there's aspects of our intelligence that are made in full generality for universes that contain simplicities, regularities, things that recur over and over again. We understand that if steel is hard on Earth, it may stay hard on the moon. And because of that, we can build rockets, walk on the moon, breathe amid the vacuum.</p><p>Chimpanzees cannot do that, but that doesn't mean that humans are the most general possible things. The thing that is more general than us, that figures that stuff out faster, is the thing to be scared of if the purposes to which it turns its intelligence are not ones that we would recognize as nice things, even in the most <a href=\"https://arbital.com/p/value_cosmopolitan/\">cosmopolitan and embracing</a> senses of what's worth doing.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=1269\">Efficiency</a></h2><p><strong>Ryan:</strong> And you said this idea of a general intelligence is different than the concept of superintelligence, which I also brought into that first part of the question. How is superintelligence different than general intelligence?</p><p><strong>Eliezer:</strong> Well, because ChatGPT has a little bit of general intelligence. Humans have more general intelligence. A superintelligence is something that can beat any human and the entire human civilization at all the cognitive tasks. I don't know if the efficient market hypothesis is something where I can rely on the entire\u2026&nbsp;</p><p><strong>Ryan:&nbsp;</strong>We're all crypto investors here. We understand the efficient market hypothesis for sure.</p><p><strong>Eliezer:&nbsp;</strong>So the&nbsp;<a href=\"https://equilibriabook.com/inadequacy-and-modesty/\"><u>efficient market hypothesis</u></a> is of course not generally true. It's not true that literally all the market prices are smarter than you. It's not true that all the prices on earth are smarter than you. Even the most arrogant person who is at all calibrated, however, still thinks that the efficient market hypothesis is true relative to them 99.99999% of the time. They only think that they know better about one in a million prices.</p><p>They might be important prices. The price of Bitcoin is an important price. It's not just a random price. But if the efficient market hypothesis was only true to you 90% of the time, you could just pick out the 10% of the remaining prices and double your money every day on the stock market. And nobody can do that. Literally nobody can do that.</p><p>So this property of relative efficiency that the market has to you, that the price\u2019s estimate of the future price already has all the information you have\u2014not all the information that exists in principle, maybe not all the information that the best equity could, but it's efficient relative to you.</p><p>For you, if you pick out a random price, like the price of Microsoft stock, something where you've got no special advantage, that estimate of its price a week later is efficient relative to you.&nbsp;<i>You</i> can't do better than that price.</p><p>We have much less experience with the notion of&nbsp;<a href=\"https://arbital.com/p/efficiency/\"><u>instrumental efficiency</u></a>, efficiency in choosing actions, because actions are harder to aggregate estimates about than prices. So you have to look at, say, AlphaZero playing chess\u2014or just, you know, whatever the latest Stockfish number is, an advanced chess engine.</p><p>When it makes a chess move, you can't do better than that chess move. It may not be the optimal chess move, but if you pick a different chess move, you'll do worse. That you'd call a kind of efficiency of action. Given its goal of winning the game, once you know its move\u2014unless you consult some more powerful AI than Stockfish\u2014you can't figure out a better move than that.</p><p>A superintelligence is like that with respect to everything, with respect to all of humanity. It is relatively efficient to humanity. It has the best estimates\u2014not perfect estimates, but the best estimates\u2014and its estimates contain all the information that you've got about it. Its actions are the most efficient actions for accomplishing its goals. If you think you see a better way to accomplish its goals, you're mistaken.</p><p><strong>Ryan:&nbsp;</strong>So you're saying [if something is a] superintelligence, we'd have to imagine something that knows all of the chess moves in advance. But here we're not talking about chess, we're talking about everything. It knows all of the moves that we would make and the most optimum pattern, including moves that we would not even know how to make, and it knows these things in advance.</p><p>I mean, how would human beings sort of experience such a superintelligence? I think we still have a very hard time imagining something smarter than us, just because we've never experienced anything like it before.</p><p>Of course, we all know somebody who's genius-level IQ, maybe quite a bit smarter than us, but we've never encountered something like what you're describing, some sort of mind that is superintelligent.</p><p>What sort of things would it be doing that humans couldn't? How would we experience this in the world?</p><p><strong>Eliezer:</strong> I mean, we do have some tiny bit of experience with it. We have experience with chess engines, where we just can't figure out better moves than they make. We have experience with market prices, where even though your uncle has this really long, elaborate story about Microsoft stock, you just know he's wrong. Why is he wrong? Because if he was correct, it would already be incorporated into the stock price.</p><p>And especially because the market\u2019s efficiency is not perfect, like that whole downward swing and then upward move in COVID. I have friends who made more money off that than I did, but I still managed to buy back into the broader stock market on the exact day of the low\u2014basically coincidence. So the markets aren't perfectly efficient, but they're efficient almost everywhere.</p><p>And that sense of deference, that sense that your weird uncle can't possibly be right because the hedge funds would know it\u2014you know. unless he's talking about COVID, in which case maybe he is right if you have the right choice of weird uncle! I have weird friends who are maybe better at calling these things than your weird uncle. So among humans, it's subtle.&nbsp;</p><p>And then with superintelligence, it's not subtle, just massive advantage. But not perfect. It's not that it knows every possible move you make before you make it. It's that it's got a good probability distribution about that. And it has figured out all the&nbsp;<i>good</i> moves you could make and figured out how to reply to those.</p><p>And I mean, in practice, what's that like? Well, unless it's limited, narrow superintelligence, I think you mostly don't get to observe it because you are dead, unfortunately.</p><p><strong>Ryan:&nbsp;</strong>What? (<i>laughs</i>)</p><p><strong>Eliezer:&nbsp;</strong>Like, Stockfish makes strictly better chess moves than you, but it's playing on a very narrow board. And the fact that it's better at you than chess doesn't mean it's better at you than everything. And I think that the actual catastrophe scenario for AI looks like big advancement in a research lab, maybe driven by them getting a giant venture capital investment and being able to spend 10 times as much on GPUs as they did before, maybe driven by a new algorithmic advance like transformers, maybe driven by hammering out some tweaks in last year's algorithmic advance that gets the thing to finally work efficiently. And the AI there goes over a critical threshold, which most obviously could be like, \u201ccan write the next AI\u201d.&nbsp;</p><p>That's so obvious that science fiction writers figured it out almost before there were computers, possibly even before there were computers. I'm not sure what the exact dates here are. But if it's better at you than everything, it's better at you than building AIs. That snowballs. It gets an immense technological advantage. If it's smart, it doesn't announce itself. It doesn't tell you that there's a fight going on. It emails out some instructions to one of those labs that'll synthesize DNA and synthesize proteins from the DNA and get some proteins mailed to a hapless human somewhere who gets paid a bunch of money to mix together some stuff they got in the mail in a file. Like, smart people will not do this for any sum of money. Many people are not smart. Builds the ribosome, but the ribosome that builds things out of covalently bonded diamondoid instead of proteins folding up and held together by Van der Waals forces, builds tiny diamondoid bacteria. The diamondoid bacteria replicate using atmospheric carbon, hydrogen, oxygen, nitrogen, and sunlight. And a couple of days later, everybody on earth falls over dead in the same second.</p><p>That's the disaster scenario if it's as smart as I am. If it's smarter, it might think of a better way to do things. But it can at least think of that if it's relatively efficient compared to humanity because I'm in humanity and I thought of it.</p><p><strong>Ryan:</strong> This is\u2014I've got a million questions, but I'm gonna let David go first.</p><p><strong>David:</strong> Yeah. So we sped run the introduction of a number of different concepts, which I want to go back and take our time to really dive into.</p><p>There's the AI alignment problem. There's AI escape velocity. There is the question of what happens when AIs are so incredibly intelligent that humans are to AIs what ants are to us.</p><p>And so I want to kind of go back and tackle these, Eliezer, one by one.</p><p>We started this conversation talking about ChatGPT, and everyone's up in arms about ChatGPT. And you're saying like, yes, it's a great step forward in the generalizability of some of the technologies that we have in the AI world. All of a sudden ChatGPT becomes immensely more useful and it's really stoking the imaginations of people today.</p><p>But what you're saying is it's not the thing that's actually going to be the thing to reach escape velocity and create superintelligent AIs that perhaps might be able to enslave us. But my question to you is, how do we know when that\u2014</p><p><strong>Eliezer:&nbsp;</strong>Not enslave. They don't enslave you, but sorry, go on.</p><p><strong>David:&nbsp;</strong>Yeah, sorry.</p><p><strong>Ryan:</strong> Murder, David. Kill all of us. Eliezer was very clear on that.</p><p><strong>David:</strong> So if it's not ChatGPT, how close are we? Because there's this unknown event horizon where you kind of alluded to it, where we make this AI that we train it to create a smarter AI and that smart AI is so incredibly smart that it hits escape velocity and all of a sudden these dominoes fall. How close are we to that point? And are we even capable of answering that question?</p><p><strong>Eliezer:&nbsp;</strong>How the heck would I know?&nbsp;</p><p><strong>Ryan:&nbsp;</strong>Well, when you were talking, Eliezer, if we had already crossed that event horizon, a smart AI wouldn't necessarily broadcast that to the world. I mean, it's possible we've already crossed that event horizon, is it not?</p><p><strong>Eliezer:</strong> I mean, it's theoretically possible, but seems very unlikely. Somebody would need inside their lab an AI that was much more advanced than the public AI technology. And as far as I currently know, the best labs and the best people are throwing their ideas to the world! Like, they don't care.</p><p>And there's probably some secret government labs with secret government AI researchers. My pretty strong guess is that they don't have the best people and that those labs could not create ChatGPT on their own because ChatGPT took a whole bunch of fine twiddling and tuning and visible access to giant GPU farms and that they don't have the people who know how to do the twiddling and tuning. This is just a guess.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=1969\">AI Alignment</a></h2><p><strong>David:</strong> Could you walk us through\u2014one of the big things that you spend a lot of time on is this thing called the AI alignment problem. Some people are not convinced that when we create AI, that AI won't really just be fundamentally aligned with humans. I don't believe that you fall into that camp. I think you fall into the camp of when we do create this superintelligent, generalized AI, we are going to have a hard time aligning with it in terms of our morality and our ethics.</p><p>Can you walk us through a little bit of that thought process? Why do you feel disaligned?</p><p><strong>Ryan:&nbsp;</strong>The dumb way to ask that question too is like, Eliezer, why do you think that the AI automatically hates us? Why is it going to\u2014</p><p><strong>Eliezer:&nbsp;</strong>It doesn't hate you.</p><p><strong>Ryan:&nbsp;</strong>Why does it want to kill us all?</p><p><strong>Eliezer:</strong> The AI doesn't hate you, neither does it love you, and you're made of atoms that it can use for something else.</p><p><strong>David:</strong> It's indifferent to you.</p><p><strong>Eliezer:&nbsp;</strong>It's got something that it actually does care about, which makes no mention of you. And you are made of atoms that it can use for something else. That's all there is to it in the end.</p><p>The reason you're not in its utility function is that the programmers did not know how to do that. The people who built the AI, or the people who built the AI that built the AI that built the AI, did not have the technical knowledge that nobody on earth has at the moment as far as I know, whereby you can do that thing and you can control in detail what that thing ends up caring about.</p><p><strong>David:&nbsp;</strong>So this feels like humanity is hurdling itself towards what we're calling, again, an event horizon where there's this AI escape velocity, and there's nothing on the other side. As in, we do not know what happens past that point as it relates to having some sort of superintelligent AI and how it might be able to manipulate the world. Would you agree with that?</p><p><strong>Eliezer:&nbsp;</strong>No.</p><p>Again, the Stockfish chess-playing analogy. You cannot predict exactly what move it would make, because in order to predict exactly what move it would make, you would have to be at least that good at chess, and it's better than you.</p><p>This is true even if it's just a little better than you. Stockfish is actually enormously better than you, to the point that once it tells you the move, you can't figure out a better move without consulting a different AI. But even if it was just a bit better than you, then you're in the same position.</p><p>This kind of disparity also exists between humans. If you ask me, where will Garry Kasparov move on this chessboard? I'm like, I don't know, maybe here. Then if Garry Kasparov moves somewhere else, it doesn't mean that he's wrong, it means that I'm wrong. If I could predict exactly where Garry Kasparov would move on a chessboard, I'd be Garry Kasparov. I'd be at least that good at chess. Possibly better. I could also be able to predict him, but also see an even better move than that.&nbsp;</p><p>That's an irreducible source of uncertainty with respect to superintelligence, or anything that's smarter than you. If you could predict exactly what it would do, you'd be that smart yourself. It doesn't mean you can predict no facts about it.</p><p>With Stockfish in particular, I can predict it's going to win the game. I know what it's optimizing for. I know where it's trying to steer the board. I can't predict exactly what the board will end up looking like after Stockfish has finished winning its game against me. I can predict it will be in the class of states that are winning positions for black or white or whichever color Stockfish picked, because, you know, it wins either way.</p><p>And that's similarly where I'm getting the prediction about everybody being dead, because if everybody were alive, then there'd be some state that the superintelligence preferred to that state, which is all of the atoms making up these people and their farms are being used for something else that it values more.</p><p>So if you postulate that everybody's still alive, I'm like, okay, well, why is it you're postulating that Stockfish made a stupid chess move and ended up with a non-winning board position? That's where that class of predictions come from.</p><p><strong>Ryan:&nbsp;</strong>Can you reinforce this argument, though, a little bit? So, why is it that an AI can't be nice, sort of like a gentle parent to us, rather than sort of a murderer looking to deconstruct our atoms and apply for use somewhere else?</p><p>What are its goals? And why can't they be aligned to at least some of our goals? Or maybe, why can't it get into a status which is somewhat like us and the ants, which is largely we just ignore them unless they interfere in our business and come in our house and raid our cereal boxes?</p><p><strong>Eliezer:&nbsp;</strong>There's a bunch of different questions there. So first of all, the space of minds is&nbsp;<a href=\"https://www.lesswrong.com/posts/tnWRXkcDi5Tw9rzXw/the-design-space-of-minds-in-general\"><u>very wide</u></a>. Imagine this giant sphere and all the humans are in this one tiny corner of the sphere. We're all basically the same make and model of car, running the same brand of engine. We're just all painted slightly different colors.</p><p>Somewhere in that mind space, there's things that are as nice as humans. There's things that are nicer than humans. There are things that are trustworthy and nice and kind in ways that no human can ever be. And there's even things that are so nice that they can understand the concept of leaving you alone and doing your own stuff sometimes instead of hanging around trying to be obsessively nice to you every minute and all the other famous disaster scenarios from ancient science fiction (\"With Folded Hands\" by Jack Williamson is the one I'm quoting there.)</p><p>We don't know how to reach into mind design space and pluck out an AI like that. It's not that they don't exist in principle. It's that we don't know how to do it. And I\u2019ll hand back the conversational ball now and figure out, like, which next question do you want to go down there?</p><p><strong>Ryan:&nbsp;</strong>Well, I mean, why? Why is it so difficult to align an AI with even our basic notions of morality?</p><p><strong>Eliezer:&nbsp;</strong>I mean, I wouldn't say that it's difficult to align an AI with our basic notions of morality. I'd say that it's difficult to align an AI on a task like \u201ctake this strawberry, and make me another strawberry that's identical to this strawberry down to the cellular level, but not necessarily the atomic level\u201d. So it looks the same under like a standard optical microscope, but maybe not a scanning electron microscope. Do that. Don't destroy the world as a side effect.</p><p>Now, this does intrinsically take a powerful AI. There's no way you can make it easy to align by making it stupid. To build something that's cellular identical to a strawberry\u2014I mean, mostly I think the way that you do this is with very primitive nanotechnology, but we could also do it using very advanced biotechnology. And these are not technologies that we already have. So it's got to be something smart enough to develop new technology.</p><p>Never mind all the subtleties of morality. I think we don't have the technology to align an AI to the point where we can say, \u201cBuild me a copy of the strawberry and don't destroy the world.\u201d</p><p>Why do I think that? Well, case in point, look at natural selection building humans. Natural selection mutates the humans a bit, runs another generation. The fittest ones reproduce more, their genes become more prevalent to the next generation. Natural selection hasn't really had very much time to do this to modern humans at all, but you know, the hominid line, the mammalian line, go back a few million generations. And this is an example of an optimization process building an intelligence.</p><p>And natural selection asked us for only one thing: \u201cMake more copies of your DNA. Make your alleles more relatively prevalent in the gene pool.\u201d Maximize your inclusive reproductive fitness\u2014not just your own reproductive fitness, but your two brothers or eight cousins, as the joke goes, because they've got on average one copy of your genes. This is&nbsp;<i>all</i> we were optimized for, for&nbsp;<i>millions</i> of generations, creating humans&nbsp;<i>from scratch</i>, from the first accidentally self-replicating molecule.</p><p>Internally, psychologically, inside our minds, we do not know what genes are. We do not know what DNA is. We do not know what alleles are. We have no concept of inclusive genetic fitness until our scientists figure out what that even is. We don't know what we were being optimized for. For a long time, many humans thought they'd been created by God!</p><p>When you use the hill-climbing paradigm and optimize for one single extremely pure thing, this is how much of it gets inside.</p><p>In the ancestral environment, in the exact distribution that we were originally optimized for, humans did tend to end up using their intelligence to try to reproduce more. Put them into a different environment, and all the little bits and pieces and fragments of optimizing for fitness that were in us now do totally different stuff. We have sex, but we wear condoms.</p><p>If natural selection had been a foresightful, intelligent kind of engineer that was able to engineer things successfully, it would have built us to be revolted by the thought of condoms. Men would be lined up and fighting for the right to donate to sperm banks. And in our natural environment, the&nbsp;<a href=\"https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter\"><u>little drives</u></a> that got into us happened to lead to more reproduction, but distributional shift: run the humans out of their distribution over which they were optimized, and you get totally different results.&nbsp;</p><p>And gradient descent would by default do\u2014not quite the same thing, it's going to do a weirder thing because natural selection has a much narrower information bottleneck. In one sense, you could say that natural selection was at an advantage because it finds&nbsp;<i>simpler</i> solutions. You could imagine some hopeful engineer who just built intelligences using gradient descent and found out that they end up wanting these thousands and millions of little tiny things, none of which were exactly what the engineer wanted, and being like, well, let's try natural selection instead. It's got a much sharper information bottleneck. It'll find the&nbsp;<i>simple</i> specification of what I want.</p><p>But we actually get there as humans. And then, gradient descent, probably may be even worse.</p><p>But more importantly, I'm just pointing out that there is no physical law, computational law, mathematical/logical law, saying when you optimize using hill-climbing on a very simple, very sharp criterion, you get a general intelligence that wants that thing.</p><p><strong>Ryan:&nbsp;</strong>So just like natural selection, our tools are too blunt in order to get to that level of granularity to program in some sort of morality into these super intelligent systems?</p><p><strong>Eliezer:&nbsp;</strong>Or build me a copy of a strawberry without destroying the world. Yeah. The tools are too blunt.</p><p><strong>David:&nbsp;</strong>So I just want to make sure I'm following with what you were saying. I think the conclusion that you left me with is that my brain, which I consider to be at least decently smart, is actually a byproduct, an accidental byproduct of this desire to reproduce. And it's actually just like a tool that I have, and just like conscious thought is a tool, which is a useful tool in means of that end.</p><p>And so if we're applying this to AI and AI's desire to achieve some certain goal, what's the parallel there?</p><p><strong>Eliezer:&nbsp;</strong>I mean, every organ in your body is a reproductive organ. If it didn't help you reproduce, you would not have an organ like that. Your brain is no exception. This is merely conventional science and merely the conventional understanding of the world. I'm not saying anything here that ought to be at all controversial. I'm sure it's controversial somewhere, but within a pre-filtered audience, it should not be at all controversial. And this is, like, the obvious thing to expect to happen with AI, because why wouldn't it? What new law of existence has been invoked, whereby this time we optimize for a thing and we get a thing that wants exactly what we optimized for on the outside?</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=2763\">AI Goals</a></h2><p><strong>Ryan:</strong> So what are the types of goals an AI might want to pursue? What types of utility functions is it going to want to pursue off the bat? Is it just those it's been programmed with, like make an identical strawberry?</p><p><strong>Eliezer:&nbsp;</strong>Well, the whole thing I'm saying is that we do not know how to get goals into a system. We can cause them to do a thing inside a distribution they were optimized over using gradient descent. But if you shift them outside of that distribution, I expect other weird things start happening. When they reflect on themselves, other weird things start happening.</p><p>What kind of utility functions are in there? I mean, darned if I know. I think you'd have a pretty hard time calling the shape of humans from advance by looking at natural selection, the thing&nbsp; that natural selection was optimizing for, if you'd never seen a human or anything like a human.</p><p>If we optimize them from the outside to predict the next line of human text, like GPT-3\u2014I don't actually think this line of technology leads to the end of the world, but maybe it does, in like GPT-7\u2014there's probably a bunch of stuff in there too that desires to accurately model things like humans under a wide range of circumstances, but it's not exactly humans, because: ice cream.</p><p>Ice cream didn't exist in the natural environment, the ancestral environment, the environment of evolutionary adaptedness. There was nothing with that much sugar, salt, fat combined together as ice cream. We are not built to want ice cream. We were built to want strawberries, honey, a gazelle that you killed and cooked and had some fat in it and was therefore nourishing and gave you the all-important calories you need to survive, salt, so you didn't sweat too much and run out of salt. We evolved to want those things, but then ice cream comes along and it fits those taste buds better than anything that existed in the environment that we were optimized over.</p><p>So, a very primitive, very basic, very unreliable wild guess, but at least an informed kind of wild guess: Maybe if you train a thing really hard to predict humans, then among the things that it likes are tiny little pseudo things that meet the definition of \u201chuman\u201d but weren't in its training data and that are much easier to predict, or where the problem of predicting them can be solved in a more satisfying way, where \u201csatisfying\u201d is not like human satisfaction, but some other criterion of \u201cthoughts like this are tasty because they help you predict the humans from the training data\u201d. (<i>shrugs</i>)</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=2951\">Consensus</a></h2><p><strong>David:&nbsp;</strong>Eliezer, when we talk about all of these ideas about the ways that AI thought will be fundamentally not able to be understood by the ways that humans think, and then all of a sudden we see this rotation by venture capitalists by just pouring money into AI, do alarm bells go off in your head? Like, hey guys, you haven't thought deeply about these subject matters yet? Does the immense amount of capital going into AI investments scare you?</p><p><strong>Eliezer:&nbsp;</strong>I mean, alarm bells went off for me in 2015, which is when it became obvious that this is how it was going to go down. I sure am now seeing the realization of that stuff I felt alarmed about back then.</p><p><strong>Ryan:&nbsp;</strong>Eliezer, is this view that AI is incredibly dangerous and that AGI is going to eventually end humanity and that we're just careening toward a precipice, would you say this is the consensus view now, or are you still somewhat of an outlier? And why aren't other smart people in this field as alarmed as you? Can you&nbsp;<a href=\"https://www.lesswrong.com/tag/steelmanning\"><u>steel-man</u></a> their arguments?</p><p><strong>Eliezer:&nbsp;</strong>You're asking, again, several questions sequentially there. Is it the consensus view? No. Do I think that the people in the wider scientific field who dispute this point of view\u2014do I think they understand it? Do I think they've done anything like an impressive job of arguing against it at all? No.</p><p>If you look at the famous prestigious scientists who sometimes make a little fun of this view in passing, they're making up arguments rather than deeply considering things that are held to any standard of rigor, and people outside their own fields are able to validly shoot them down.</p><p>I have no idea how to pronounce his last name. Francis Chollet said something about, I forget his exact words, but it was something like, I never hear any good arguments for stuff. I was like, okay, here's some good arguments for stuff. You can read&nbsp;<a href=\"https://intelligence.org/2017/12/06/chollet/\"><u>the reply from Yudkowsky to Chollet</u></a> and Google that, and that'll give you some idea of what the eminent voices versus the reply to the eminent voices sound like. And Scott Aronson, who at the time was off on complexity theory, he was like, \u201cThat's not how no free lunch theorems work\u201d, correctly.</p><p>I think the state of affairs is we have eminent scientific voices making fun of this possibility, but not engaging with the arguments for it.&nbsp;</p><p>Now, if you step away from the eminent scientific voices, you can find people who are more familiar with all the arguments and disagree with me. And I think they lack&nbsp;<a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\"><u>security mindset</u></a>. I think that they're engaging in the sort of blind optimism that many, many scientific fields throughout history have engaged in, where when you're approaching something for the first time, you don't know why it will be hard, and you imagine easy ways to do things. And the way that this is supposed to naturally play out over the history of a scientific field is that you run out and you try to do the things and they don't work, and you go back and you try to do other clever things and they don't work either, and you learn some pessimism and you start to understand the reasons why the problem is hard.</p><p>The field of artificial intelligence itself recapitulated this very common ontogeny of a scientific field, where initially we had people getting together at the Dartmouth conference. I forget what their exact famous phrasing was, but it's something like, \u201cWe are wanting to address the problem of getting AIs to, you know, like understand language, improve themselves\u201d, and I forget even what else was there. A list of what now sound like grand challenges. \u201cAnd we think we can make substantial progress on this using 10 researchers for two months.\u201d And I think that at the core is what's going on.&nbsp;</p><p>They have not run into the actual problems of alignment. They aren't trying to get ahead of the game. They're not trying to panic early. They're waiting for reality to hit them onto the head and turn them into grizzled old cynics of their scientific field who understand the reasons why things are hard. They're content with the predictable life cycle of starting out as bright-eyed youngsters, waiting for reality to hit them over the head with the news. And if it wasn't going to kill everybody the first time that they're really wrong, it'd be fine! You know, this is how science works! If we got unlimited free retries and 50 years to solve everything, it'd be okay. We could figure out how to align AI in 50 years given unlimited retries.</p><p>You know, the first team in with the bright-eyed optimists would destroy the world and people would go, oh, well, you know, it's not that easy. They would try something else clever. That would destroy the world. People would go like, oh, well, you know, maybe this field is actually hard. Maybe this is actually one of the thorny things like computer security or something. And so what exactly went wrong last time? Why didn't these hopeful ideas play out? Oh, like you optimize for one thing on the outside and you get a different thing on the inside. Wow. That's really basic. All right. Can we even do this using gradient descent? Can you even build this thing out of giant inscrutable matrices of floating point numbers that nobody understands at all? You know, maybe we need different methodology. And 50 years later, you'd have an aligned AGI.</p><p>If we got unlimited free retries without destroying the world, it'd be, you know, it'd play out the same way that ChatGPT played out. It's, you know, from 1956 or 1955 or whatever it was to 2023. So, you know, about 70 years, give or take a few. And, you know, just like we can do the stuff that they wanted to do in the summer of 1955, you know, 70 years later, you'd have your aligned AGI.</p><p>Problem is that the world got destroyed in the meanwhile. And that's why, you know, that's the problem there.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=3345\">God Mode and Aliens</a></h2><p><strong>David:</strong> So this feels like a gigantic <i>Don't Look Up</i> scenario. If you're familiar with that movie, it's a movie about this asteroid hurtling to Earth, but it becomes popular and in vogue to not look up and not notice it. And Eliezer, you're the guy who's saying like, hey, there's an asteroid. We have to do something about it. And if we don't, it's going to come destroy us.</p><p>If you had God mode over the progress of AI research and just innovation and development, what choices would you make that humans are not currently making today?</p><p><strong>Eliezer:&nbsp;</strong>I mean, I could say something like shut down all the large GPU clusters. How long do I have God mode? Do I get to like stick around for seventy years?</p><p><strong>David:&nbsp;</strong>You have God mode for the 2020 decade.</p><p><strong>Eliezer:&nbsp;</strong>For the 2020 decade. All right. That does make it pretty hard to do things.</p><p>I think I shut down all the GPU clusters and get all of the famous scientists and brilliant, talented youngsters\u2014the vast, vast majority of whom are not going to be productive and where government bureaucrats are not going to be able to tell who's actually being helpful or not, but, you know\u2014put them all on a large island, and try to figure out some system for filtering the stuff through to me to give thumbs up or thumbs down on that is going to work better than scientific bureaucrats producing entire nonsense.</p><p>Because, you know, the trouble is\u2014the reason why scientific fields have to go through this long process to produce the cynical oldsters who know that everything is difficult. It's not that the youngsters are stupid. You know, sometimes youngsters are fairly smart. You know, Marvin Minsky, John McCarthy back in 1955, they weren't idiots. You know, privileged to have met both of them. They didn't strike me as idiots. They were very old, and they still weren't idiots. But, you know, it's hard to see what's coming in advance of experimental evidence hitting you over the head with it.</p><p>And if I only have the decade of the 2020s to run all the researchers on this giant island somewhere, it's really not a lot of time. Mostly what you've got to do is invent some entirely new AI paradigm that isn't the giant inscrutable matrices of floating point numbers on gradient descent. Because I'm not really seeing what you can do that's clever with that, that doesn't kill you and that you know doesn't kill you and doesn't kill you the very first time you try to do something clever like that.</p><p>You know, I'm sure there's&nbsp;<i>a</i> way to do it. And if you got to try over and over again, you could find it.</p><p><strong>Ryan:&nbsp;</strong>Eliezer, do you think every intelligent civilization has to deal with this exact problem that humanity is dealing with now? Of how do we solve this problem of aligning with an advanced general intelligence?</p><p><strong>Eliezer:&nbsp;</strong>I expect that's much easier for some alien species than others. Like, there are alien species who might arrive at \u201cthis problem\u201d in an entirely different way. Maybe instead of having two entirely different information processing systems, the DNA and the neurons, they've only got one system. They can trade memories around heritably by swapping blood sexually. Maybe the way in which they \u201cconfront this problem\u201d is that very early in their evolutionary history, they have the equivalent of the DNA that stores memories and processes, computes memories, and they swap around a bunch of it, and it adds up to something that reflects on itself and makes itself coherent, and then you've got a superintelligence before they have invented computers. And maybe that thing wasn't aligned, but how do you even align it when you're in that kind of situation? It'd be a very different angle on the problem.</p><p><strong>Ryan:&nbsp;</strong>Do you think every advanced civilization is on the trajectory to creating a superintelligence at some point in its history?</p><p><strong>Eliezer:&nbsp;</strong>Maybe there's ones in universes with alternate physics where you just can't do that. Their universe's computational physics just doesn't support that much computation. Maybe they never get there. Maybe their lifespans are long enough and their star lifespans short enough that they never get to the point of a technological civilization before their star does the equivalent of expanding or exploding or going out and their planet ends.</p><p>\u201cEvery alien species\u201d covers a lot of territory, especially if you talk about alien species and universes with physics different from this one.</p><p><strong>Ryan:</strong> Well, talking about our present universe, I'm curious if you've been confronted with the question of, well, then why haven't we seen some sort of superintelligence in our universe when we look out at the stars? Sort of the Fermi paradox type of question. Do you have any explanation for that?</p><p><strong>Eliezer:</strong> Oh, well, supposing that they got killed by their own AIs doesn't help at all with that because then we'd see the AIs.</p><p><strong>Ryan:</strong> And do you think that's what happens? Yeah, it doesn't help with that. We would see evidence of AIs, wouldn't we?</p><p><strong>Eliezer:&nbsp;</strong>Yeah.</p><p><strong>Ryan:&nbsp;</strong>&nbsp;Yes. So why don't we?</p><p><strong>Eliezer:</strong> I mean, the same reason we don't see evidence of the alien civilizations not with AIs.</p><p>And that reason is, although it doesn't really have much to do with the whole AI thesis one way or another, because they're too far away\u2014or so says Robin Hanson, using a very clever argument about the apparent difficulty of hard steps in humanity's evolutionary history to further induce the rough gap between the hard steps. ... And, you know, I can't really do justice to this. If you look up grabby aliens, you can...</p><p><strong>Ryan:&nbsp;</strong>Grabby aliens?</p><p><strong>David:&nbsp;</strong>I remember this.</p><p><strong>Eliezer:</strong> Grabby aliens. You can find Robin Hanson's very clever argument for how far away the aliens are...</p><p><strong>Ryan:&nbsp;</strong>There's an entire website, Bankless listeners, there's an entire website called&nbsp;<a href=\"https://grabbyaliens.com/\"><u>grabbyaliens.com</u></a> you can go look at.</p><p><strong>Eliezer:&nbsp;</strong>Yeah. And that contains by far the best answer I've seen, to:</p><ul><li>\u201cWhere are they?\u201d (Answer: too far away for us to see, even if they're traveling here at nearly light speed.)</li><li>How far away are they?</li><li>And how do we know that?</li></ul><p>(<i>laughs</i>) But, yeah.</p><p><strong>Ryan:</strong> This is amazing.</p><p><strong>Eliezer:</strong> There is not a very good way to simplify the argument, any more than there is to simplify the notion of zero-knowledge proofs. It's not that difficult, but it's just very not easy to simplify. But if you have a bunch of locks that are all of different difficulties, and a limited time in which to solve all the locks, such that anybody who gets through all the locks must have gotten through them by luck, all the locks will take around the same amount of time to solve, even if they're all of very different difficulties. And that's the core of Robin Hanson's argument for how far away the aliens are, and how do we know that? (<i>shrugs</i>)</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=3796\">Good Outcomes</a></h2><p><strong>Ryan:</strong> Eliezer, I know you're very skeptical that there will be a good outcome when we produce an artificial general intelligence. And I said when, not if, because I believe that's your thesis as well, of course. But is there the possibility of a good outcome? I know you are working on AI alignment problems, which leads me to believe that you have greater than zero amount of hope for this project. Is there the possibility of a good outcome? What would that look like, and how do we go about achieving it?</p><p><strong>Eliezer:&nbsp;</strong>It looks like me being wrong. I basically don't see on-model hopeful outcomes at this point. We have not done those things that it would take to earn a good outcome, and this is not a case where you get a good outcome by accident.</p><p>If you have a bunch of people putting together a new operating system, and they've heard about computer security, but they're skeptical that it's really that hard, the chance of them producing a&nbsp;<a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\"><u>secure operating system</u></a> is effectively zero.</p><p>That's basically the situation I see ourselves in with respect to AI alignment. I have to be wrong about something\u2014which I certainly am. I have to be wrong about something in a way that makes the problem&nbsp;<i>easier</i> rather than&nbsp;<i>harder</i> for those people who don't think that alignment's going to be all that hard.</p><p>If you're building a rocket for the first time ever, and you're wrong about something, it's not surprising if you're wrong about something. It's surprising if the thing that you're wrong about causes the rocket to go twice as high on half the fuel you thought was required and be much easier to steer than you were afraid of.</p><p><strong>Ryan:&nbsp;</strong>So, are you...</p><p><strong>David:&nbsp;</strong>Where the alternative was, \u201cIf you\u2019re wrong about something, the rocket blows up.\u201d</p><p><strong>Eliezer:&nbsp;</strong>Yeah. And then the rocket ignites the atmosphere, is the problem there.</p><p>O rather: a bunch of rockets blow up, a bunch of rockets go places... The analogy I usually use for this is, very early on in the Manhattan Project, they were worried about \u201cWhat if the nuclear weapons can ignite fusion in the nitrogen in the atmosphere?\u201d And they ran some calculations and decided that it was incredibly unlikely for multiple angles, so they went ahead, and were correct. We're still here. I'm not going to say that it was luck, because the calculations were actually pretty solid.</p><p>An AI is like that, but instead of needing to refine plutonium, you can make nuclear weapons out of a billion tons of laundry detergent. The stuff to make them is fairly widespread. It's not a tightly controlled substance. And they spit out gold up until they get large enough, and&nbsp;<i>then</i> they ignite the atmosphere, and you can't calculate how large is large enough. And a bunch of the CEOs running these projects are making fun of the idea that it'll ignite the atmosphere.</p><p>It's not a very helpful situation.</p><p><strong>David:&nbsp;</strong>So the economic incentive to produce this AI\u2014one of the things why ChatGPT has sparked the imaginations of so many people is that everyone can imagine products. Products are being imagined left and right about what you can do with something like ChatGPT. There's this meme at this point of people leaving to go start their ChatGPT startup.</p><p>The metaphor is that what you're saying is that there's this generally available resource spread all around the world, which is ChatGPT, and everyone's hammering it in order to make it spit out gold. But you're saying if we do that too much, all of a sudden the system will ignite the whole entire sky, and then we will all...</p><p><strong>Eliezer:&nbsp;</strong>Well, no. You can run ChatGPT any number of times without igniting the atmosphere. That's about what research labs at Google and Microsoft\u2014counting DeepMind as part of Google and counting OpenAI as part of Microsoft\u2014that's about what the research labs are doing, bringing more metaphorical Plutonium together than ever before. Not about how many times you run the things that have been built and not destroyed the world yet.</p><p>You can do any amount of stuff with ChatGPT and not destroy the world. It's not that smart. It doesn't get smarter every time you run it.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=4078\">Ryan's Childhood Questions</a></h2><p><strong>Ryan:&nbsp;</strong>Can I ask some questions that the 10-year-old in me wants to really ask about this? I'm asking these questions because I think a lot of listeners might be thinking them too, so knock off some of these easy answers for me.</p><p>If we create some sort of unaligned, let's call it \u201cbad\u201d AI, why can't we just create a whole bunch of good AIs to go fight the bad AIs and solve the problem that way? Can there not be some sort of counterbalance in terms of aligned human AIs and evil AIs, and there be some sort of battle of the artificial minds here?</p><p><strong>Eliezer:&nbsp;</strong>Nobody knows how to create any good AIs at all. The problem isn't that we have 20 good AIs and then somebody finally builds an evil AI. The problem is that the first very powerful AI is evil, nobody knows how to make it good, and then it kills everybody before anybody can make it good.</p><p><strong>Ryan:&nbsp;</strong>So there is no known way to make a friendly, human-aligned AI whatsoever, and you don't know of a good way to go about thinking through that problem and designing one. Neither does anyone else, is what you're telling us.</p><p><strong>Eliezer:&nbsp;</strong>I have some idea of what I would do if there were more time. Back in the day, we had more time. Humanity squandered it. I'm not sure there's enough time left now. I have some idea of what I would do if I were in a 25-year-old body and had $10 billion.</p><p><strong>Ryan:&nbsp;</strong>That would be the island scenario of \u201cYou're God for 10 years and you get all the researchers on an island and go really hammer for 10 years at this problem\u201d?</p><p><strong>Eliezer:&nbsp;</strong>If I have buy-in from a major government that can run actual security precautions and more than just $10 billion, then you could run a whole Manhattan Project about it, sure.</p><p><strong>Ryan:&nbsp;</strong>This is another question that the 10-year-old in me wants to know. Why is it that, Eliezer, people listening to this episode or people listening to the concerns or reading the concerns that you've written down and published, why can't everyone get on board who's building an AI and just all agree to be very, very careful? Is that not a sustainable game-theoretic position to have? Is this a coordination problem, more of a social problem than anything else? Or, like, why can't that happen?</p><p>I mean, we have so far not destroyed the world with nuclear weapons, and we've had them since the 1940s.</p><p><strong>Eliezer:&nbsp;</strong>Yeah, this is harder than nuclear weapons. This is a&nbsp;<i>lot</i> harder than nuclear weapons.</p><p><strong>Ryan:</strong> Why is this harder? And why can't we just coordinate to just all agree internationally that we're going to be very careful, put restrictions on this, put regulations on it, do something like that?</p><p><strong>Eliezer:&nbsp;</strong>Current heads of major labs seem to me to be openly contemptuous of these issues. That's where we're starting from. The politicians do not understand it.</p><p>There are distortions of these ideas that are going to sound more appealing to them than \u201ceverybody suddenly falls over dead\u201d, which is a thing that I think actually happens. \u201cEverybody falls over dead\u201d just doesn't inspire the monkey political parts of our brain somehow. Because it's not like, \u201cOh no, what if terrorists get the AI first?\u201d It's like, it doesn't matter who gets it first. Everybody falls over dead.</p><p>And yeah, so you're describing a world coordinating on something that is relatively hard to coordinate. So, could we, if we tried starting today, prevent anyone from getting a billion pounds of laundry detergent in one place worldwide, control the manufacturing of laundry detergent, only have it manufactured in particular places, not concentrate lots of it together, enforce it on every country?</p><p>Y\u2019know, if it was legible, if it was&nbsp;<i>clear</i> that a billion pounds of laundry detergent in one place would end the world, if you could calculate that, if all the scientists calculated it arrived at the same answer and told the politicians that maybe, maybe humanity would survive, even though smaller amounts of laundry detergent spit out gold.</p><p>The threshold can't be calculated. I don't know how you'd convince the politicians. We definitely don't seem to have had much luck convincing those CEOs whose job depends on them not caring, to care. Caring is easy to fake. It's easy to hire a bunch of people to be your \u201cAI safety team\u201d&nbsp; and redefine \u201cAI safety\u201d as having the AI not say naughty words. Or, you know, I'm speaking somewhat metaphorically here for reasons.</p><p>But, you know, it's the basic problem that we have is like trying to build a secure OS before we run up against a really smart attacker. And there's all kinds of, like, fake security. \u201cIt's got a password file! This system is secure! It only lets you in if you type a password!\u201d And if you never go up against a really smart attacker, if you never go far out of distribution against a powerful optimization process looking for holes, you know, then how does a bureaucracy come to know that what they're doing is not the level of computer security that they need? The way you're supposed to find this out, the way that scientific fields historically find this out, the way that fields of computer science historically find this out, the way that crypto found this out back in the early days, is by having the disaster happen!&nbsp;</p><p>And we're not even that good at learning from relatively minor disasters! You know, like, COVID swept the world. Did the FDA or the CDC learn anything about \u201cDon't tell hospitals that they're not allowed to use their own tests to detect the coming plague\u201d? Are we installing UV-C lights in public spaces or in ventilation systems to prevent the next respiratory pandemic? You know, we lost a million people and we sure did not learn very much as far as I can tell for next time.</p><p>We could have an AI disaster that kills a hundred thousand people\u2014how do you even&nbsp;<i>do</i> that? Robotic cars crashing into each other? Have a bunch of robotic cars crashing into each other! It's not going to look like that was the fault of artificial general intelligence because they're not going to put AGIs in charge of cars. They're going to pass a bunch of regulations that's going to affect the entire AGI disaster or not at all.</p><p>What does the winning world even look like here? How in real life did we get from where we are now to this worldwide ban, including against North Korea and, you know, some one rogue nation whose dictator doesn't believe in all this nonsense and just wants the gold that these AIs spit out? How did we get there from here? How do we get to the point where the United States and China signed a treaty whereby they would both use nuclear weapons against Russia if Russia built a GPU cluster that was too large? How did we get there from here?</p><p><strong>David:&nbsp;</strong>Correct me if I'm wrong, but this seems to be kind of just like a topic of despair? I'm talking to you now and hearing your thought process about, like, there is no known solution and the trajectory's not great. Do you think all hope is lost here?</p><p><strong>Eliezer:&nbsp;</strong>I'll keep on fighting until the end, which I wouldn't do if I had literally zero hope. I could still be wrong about something in a way that makes this problem somehow much easier than it currently looks. I think that's how you go down fighting with dignity.</p><p><strong>Ryan:&nbsp;</strong>\u201cGo down fighting with dignity.\u201d That's the stage you think we're at.</p><p>I want to just double-click on what you were just saying. Part of the case that you're making is humanity won't even see this coming. So it's not like a coordination problem like global warming where every couple of decades we see the world go up by a couple of degrees, things get hotter, and we start to see these effects over time. The characteristics or the advent of an AGI in your mind is going to happen incredibly quickly, and in such a way that we won't even see the disaster until it's imminent, until it's upon us...?</p><p><strong>Eliezer:&nbsp;</strong>I mean, if you want some kind of, like, formal phrasing, then I think that superintelligence will kill everyone before non-superintelligent AIs have killed one million people. I don't know if that's the phrasing you're looking for there.</p><p><strong>Ryan:&nbsp;</strong>I think that's a fairly precise definition, and why? What goes into that line of thought?</p><p><strong>Eliezer:&nbsp;</strong>I think that the current systems are actually very weak. I don't know, maybe I could use the analogy of Go, where you had systems that were finally competitive with the pros, where \u201cpro\u201d is like the set of ranks in Go, and then a year later, they were challenging the world champion and winning. And then another year, they threw out all the complexities and the training from human databases of Go games and built a new system, AlphaGo Zero, that trained itself from scratch. No looking at the human playbooks, no special-purpose code, just a general purpose game-player being specialized to Go, more or less.</p><p>And, three days\u2014there's a quote from Gwern about this, which I forget exactly, but it was something like, \u201cWe know how long AlphaGo Zero, or AlphaZero (two different systems), was equivalent to a human Go player. And it was, like, 30 minutes on the following floor of such-and-such DeepMind building.\u201d</p><p>Maybe the first system doesn't improve that quickly, and they build another system that does\u2014And all of that with AlphaGo over the course of years, going from \u201cit takes a long time to train\u201d to \u201cit trains very quickly and without looking at the human playbook\u201d, that\u2019s&nbsp;<i>not</i> with an artificial intelligence system that improves itself, or even that gets smarter as you run it, the way that human beings (not just as you evolve them, but as you run them over the course of their own lifetimes) improve.</p><p>So if the first system doesn't improve fast enough to kill everyone very quickly, they will build one that's meant to spit out more gold than that.</p><p>And there could be weird things that happen before the end. I did not see ChatGPT coming, I did not see Stable Diffusion coming, I did not expect that we would have AIs smoking humans in rap battles before the end of the world. Ones that are clearly much dumber than us.</p><p><strong>Ryan:&nbsp;</strong>It\u2019s kind of a nice send-off, I guess, in some ways.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=4995\">Trying to Resist</a></h2><p><strong>Ryan: </strong>So you said that your hope is not zero, and you are planning to fight to the end. What does that look like for you? I know you're working at MIRI, which is the Machine Intelligence Research Institute. This is a non-profit that I believe that you've set up to work on these AI alignment and safety issues. What are you doing there? What are you spending your time on? How do we actually fight until the end? If you do think that an end is coming, how do we try to resist?</p><p><strong>Eliezer:&nbsp;</strong>I'm actually on something of a sabbatical right now, which is why I have time for podcasts. It's a sabbatical from, you know, like, been doing this 20 years. It became clear we were all going to die. I felt kind of burned out, taking some time to rest at the moment. When I dive back into the pool, I don't know, maybe I will go off to Conjecture or Anthropic or one of the smaller concerns like Redwood Research\u2014Redwood Research being the only ones I really trust at this point, but they're tiny\u2014and try to figure out if&nbsp;<i>I</i> can see anything clever to do with the giant inscrutable matrices of floating point numbers.</p><p>Maybe I just write, continue to try to explain in advance to people why this problem is hard instead of as easy and cheerful as the current people who think they're pessimists think it will be. I might not be working all that hard compared to how I used to work. I'm older than I was. My body is not in the greatest of health these days. Going down fighting doesn't necessarily imply that I have the stamina to fight all that hard. I wish I had prettier things to say to you here, but I do not.</p><p><strong>Ryan:&nbsp;</strong>No, this is... We intended to save probably the last part of this episode to talk about crypto, the metaverse, and AI and how this all intersects. But I gotta say, at this point in the episode, it all kind of feels pointless to go down that track.</p><p>We were going to ask questions like, well, in crypto, should we be worried about building sort of a property rights system, an economic system, a programmable money system for the AIs to sort of use against us later on? But it sounds like the easy answer from you to those questions would be, yeah, absolutely. And by the way, none of that matters regardless. You could do whatever you'd like with crypto. This is going to be the inevitable outcome no matter what.</p><p>Let me ask you, what would you say to somebody listening who maybe has been sobered up by this conversation? If a version of you in your 20s does have the stamina to continue this battle and to actually fight on behalf of humanity against this existential threat, where would you advise them to spend their time? Is this a technical issue? Is this a social issue? Is it a combination of both? Should they educate? Should they spend time in the lab? What should a person listening to this episode do with these types of dire straits?</p><p><strong>Eliezer:&nbsp;</strong>I don't have really good answers. It depends on what your talents are. If you've got the very deep version of the&nbsp;<a href=\"https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/\"><u>security mindset</u></a>, the part where you don't just put a password on your system so that nobody can walk in and directly misuse it, but the kind where you don't just encrypt the password file even though nobody's supposed to have access to the password file in the first place, and that's already an authorized user, but the part where you hash the passwords and salt the hashes. If you're the kind of person who can think of that from scratch, maybe take your hand at alignment.</p><p>If you can think of an alternative to the giant inscrutable matrices, then, you know, don't tell the world about that. I'm not quite sure where you go from there, but maybe you work with Redwood Research or something.</p><p>A whole lot of this problem is that even if you do build an AI that's limited in some way, somebody else steals it, copies it, runs it themselves, and takes the bounds off the for loops and the world ends.&nbsp;</p><p>So there's that. You think you can do something clever&nbsp;<i>with</i> the giant inscrutable matrices? You're probably wrong. If you have the talent to try to figure out why you're wrong in advance of being hit over the head with it, and not in a way where you just make random far-fetched stuff up as the reason why it won't work, but where you can actually&nbsp;<i>keep looking</i> for the reason why it won't work...</p><p>We have people in crypto[graphy] who are good at breaking things, and they're the reason why&nbsp;<i>anything</i> is not on fire. Some of them might go into breaking AI systems instead, because that's where you learn anything.</p><p>You know: Any fool can build a crypto[graphy] system that they think will work.&nbsp;<i>Breaking</i> existing cryptographical systems is how we learn who the real experts are. So maybe the people finding weird stuff to do with AIs, maybe those people will come up with some truth about these systems that makes them easier to align than I suspect.</p><p>How do I put it... The saner outfits do have uses for money. They don't really have&nbsp;<i>scalable</i> uses for money, but they do burn any money literally at all. Like, if you gave MIRI a billion dollars, I would not know how to...</p><p>Well, at a billion dollars, I might try to bribe people to move out of AI development, that gets broadcast to the whole world, and move to the equivalent of an island somewhere\u2014not even to make any kind of critical discovery, but just to remove them from the system. If I had a billion dollars.</p><p>If I just have another $50 million, I'm not quite sure what to do with that, but if you donate that to MIRI, then you at least have the assurance that we will not randomly spray money on looking like we're doing stuff and we'll reserve it, as we are doing with the last giant crypto donation somebody gave us until we can figure out something to do with it that is actually helpful. And MIRI has that property. I would say probably Redwood Research has that property.</p><p>Yeah. I realize I'm sounding sort of disorganized here, and that's because I don't really have a good organized answer to how in general somebody goes down fighting with dignity.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=5453\">MIRI and Education</a></h2><p><strong>Ryan:&nbsp;</strong>I know a lot of people in crypto. They are not as in touch with artificial intelligence, obviously, as you are, and the AI safety issues and the existential threat that you've presented in this episode. They do care a lot and see coordination problems throughout society as an issue. Many have also generated wealth from crypto, and care very much about humanity not ending. What sort of things has MIRI, the organization I was talking about earlier, done with funds that you've received from crypto donors and elsewhere? And what sort of things might an organization like that pursue to try to stave this off?</p><p><strong>Eliezer:&nbsp;</strong>I mean, I think mostly we've pursued a lot of lines of research that haven't really panned out, which is a respectable thing to do. We did not know in advance that those lines of research would fail to pan out. If you're doing research that you know will work, you're probably not really doing any research. You're just doing a pretense of research that you can show off to a funding agency.</p><p>We try to be real. We did things where we didn't know the answer in advance. They didn't work, but that was where the hope lay, I think. But, you know, having a research organization that keeps it real that way, that's not an easy thing to do. And if you don't have this very deep form of the security mindset, you will end up producing fake research and doing more harm than good, so I would not tell all the successful cryptocurrency people to run off and start their own research outfits.</p><p>Redwood Research\u2014I'm not sure if they can scale using more money, but you can give people more money and wait for them to figure out how to scale it later if they're the kind who won't just run off and spend it, which is what MIRI aspires to be.</p><p><strong>Ryan:&nbsp;</strong>And you don't think the education path is a useful path? Just educating the world?</p><p><strong>Eliezer:&nbsp;</strong>I mean, I would give myself and MIRI credit for why the world isn't just walking blindly into the whirling razor blades here, but it's not clear to me how far education scales apart from that. You can get more people aware that we're walking directly into the whirling razor blades, because even if only 10% of the people can get it, that can still be a bunch of people. But then what do they do? I don't know. Maybe they'll be able to do something later.</p><p>Can you get all the people? Can you get all the politicians? Can you get the people whose job incentives are against them admitting this to be a problem? I have various friends who report, like, \u201cAh yes, if you talk to researchers at OpenAI in&nbsp;<i>private</i>, they are very worried and say that they cannot be that worried in public.\u201d</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=5640\">How Long Do We Have?</a></h2><p><strong>Ryan:&nbsp;</strong>This is all a giant&nbsp;<a href=\"https://slatestarcodex.com/2014/07/30/meditations-on-moloch/\"><u>Moloch</u></a> trap, is sort of what you're telling us. I feel like this is the part of the conversation where we've gotten to the end and the doctor has said that we have some sort of terminal illness. And at the end of the conversation, I think the patient, David and I, have to ask the question, \u201cOkay, doc, how long do we have?\u201d Seriously, what are we talking about here if you turn out to be correct? Are we talking about years? Are we talking about decades? What's your idea here?</p><p><strong>David:&nbsp;</strong>What are&nbsp;<i>you</i> preparing for, yeah?</p><p><strong>Eliezer:&nbsp;</strong>How the hell would I know? Enrico Fermi was saying that fission chan reactions were 50 years off if they could ever be done at all, two years before he built the first nuclear pile. The Wright brothers were saying heavier-than-air flight was 50 years off shortly before they built the first Wright flyer. How on earth would I know?</p><p>It could be three years. It could be 15 years. We could get that AI winter I was hoping for, and it could be 16 years. I'm not really seeing 50 without some kind of giant civilizational catastrophe. And to be clear, whatever civilization arises after that would probably, I'm guessing, end up stuck in just the same trap we are.</p><p><strong>Ryan:&nbsp;</strong>I think the other thing that the patient might do at the end of a conversation like this is to also consult with other doctors. I'm kind of curious who we should talk to on this quest. Who are some people that if people in crypto want to hear more about this or learn more about this, or even we ourselves as podcasters and educators want to pursue this topic, who are the other individuals in the AI alignment and safety space you might recommend for us to have a conversation with?</p><p><strong>Eliezer:&nbsp;</strong>Well, the person who actually holds a coherent technical view, who disagrees with me, is named Paul Christiano. He does not write Harry Potter fan fiction, and I expect him to have a harder time explaining himself in concrete terms. But that is the main technical voice of opposition. If you talk to other people in the effective altruism or AI alignment communities who disagree with this view, they are probably to some extent repeating back their misunderstandings of Paul Christiano's views.&nbsp;</p><p>You could try Ajeya Cotra, who's worked pretty directly with Paul Christiano and I think sometimes aspires to explain these things that Paul is not the best at explaining. I'll throw out Kelsey Piper as somebody who would be good at explaining\u2014like, would not claim to be a technical person on these issues, but is good at explaining the part that she does know.&nbsp;</p><p>Who else disagrees with me? I'm sure Robin Hanson would be happy to come on... well, I'm not sure he'd be happy to come on this podcast, but Robin Hanson disagrees with me, and I kind of feel like the&nbsp;<a href=\"https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\"><u>famous argument we had</u></a> back in the early 2010s, late 2000s about how this would all play out\u2014I basically feel like this was the Yudkowsky position, this is the Hanson position, and then reality was over here,&nbsp;<a href=\"https://intelligence.org/2017/10/20/alphago/\"><u>well to the Yudkowsky side</u></a> of the Yudkowsky position in the Yudkowsky-Hanson debate. But Robin Hanson does not feel that way, and would probably be happy to expound on that at length.&nbsp;</p><p>I don't know. It's not hard to find opposing viewpoints. The ones that'll stand up to a few solid minutes of cross-examination from somebody who knows which parts to cross-examine, that's the hard part.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=5895\">Bearish Hope</a></h2><p><strong>Ryan:&nbsp;</strong>You know, I've read a lot of your writings and listened to you on previous podcasts. One was in 2018&nbsp;<a href=\"https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/\"><u>on the Sam Harris podcast</u></a>. This conversation feels to me like the most dire you've ever seemed on this topic. And maybe that's not true. Maybe you've sort of always been this way, but it seems like the direction of your hope that we solve this issue has declined. I'm wondering if you feel like that's the case, and if you could sort of summarize your take on all of this as we close out this episode and offer, I guess, any concluding thoughts here.</p><p><strong>Eliezer:&nbsp;</strong>I mean, I don't know if you've got a time limit on this episode? Or is it just as long as it runs?</p><p><strong>Ryan:&nbsp;</strong>It's as long as it needs to be, and I feel like this is a pretty important topic. So you answer this however you want.</p><p><strong>Eliezer:&nbsp;</strong>Alright. Well, there was a conference one time on \u201cWhat are we going to do about looming risk of AI disaster?\u201d, and Elon Musk attended that conference. And I was like,: Maybe this is it. Maybe this is when the powerful people notice, and it's one of the relatively more technical powerful people who could be noticing this. And maybe this is where humanity finally turns and starts... not quite fighting back, because there isn't an external enemy here, but conducting itself with... I don't know. Acting like it cares, maybe?</p><p>And what came out of that conference, well, was OpenAI, which was fairly nearly the worst possible way of doing anything. This is not a problem of \u201cOh no, what if secret elites get AI?\u201d It's that nobody knows how to build the thing. If we&nbsp;<i>do</i> have an alignment technique, it's going to involve running the AI with a bunch of careful bounds on it where you don't just throw all the cognitive power you have at something. You have limits on the for loops.&nbsp;</p><p>And whatever it is that could possibly save the world, like go out and turn all the GPUs and the server clusters into Rubik's cubes or something else that prevents the world from ending when somebody else builds another AI a few weeks later\u2014anything that could do that is an artifact where somebody else could take it and take the bounds off the for loops and use it to destroy the world.</p><p>So let's open up everything! Let's accelerate everything! It was like GPT-3's version, though GPT-3 didn't exist back then\u2014but it was like ChatGPT's blind version of throwing the ideals at a place where they were&nbsp;<i>exactly</i> the wrong ideals to solve the problem.</p><p>And the problem is that demon summoning is easy and angel summoning is much harder. Open sourcing all the demon summoning circles is not the correct solution. And I'm using Elon Musk's own terminology here. He talked about AI as \u201csummoning the demon\u201d, which, not accurate, but\u2014and then the solution was to put a demon summoning circle in every household.&nbsp;</p><p>And, why? Because his friends were calling him Luddites once he'd expressed any concern about AI at all. So he picked a road that sounded like \u201copenness\u201d and \u201caccelerating technology\u201d! So his friends would stop calling him \u201cLuddite\u201d.</p><p>It was very much the worst\u2014you know, maybe not the literal, actual worst possible strategy, but so very far pessimal.</p><p>And that was it.</p><p>That was like... that was me in 2015 going like, \u201cOh. So this is what humanity will elect to do. We will not rise above. We will not have more grace, not even here at the very end.\u201d</p><p>So that is, you know, that is when I did my crying late at night and then picked myself up and fought and fought and fought until I had run out all the avenues that I seem to have the capabilities to do. There's, like, more things, but they require scaling my efforts in a way that I've never been able to make them scale. And all of it's pretty far-fetched at this point anyways.</p><p>So, you know, that\u2014so what's, you know, what's changed over the years? Well, first of all, I ran out some remaining avenues of hope. And second, things got to be such a disaster, such a&nbsp;<i>visible</i> disaster, the AI has got powerful enough and it became clear enough that, you know, we do not know how to align these things, that I could actually say what I've been thinking for a while and not just have people go completely, like, \u201cWhat are you&nbsp;<i>saying</i> about all this?\u201d</p><p>You know, now the stuff that was obvious back in 2015 is, you know, starting to become visible in the distance to others and not just completely invisible. That's what changed over time.</p><p>&nbsp;</p><h2><a href=\"https://youtu.be/gA1sNLL6yg4?t=6230\">The End Goal</a></h2><p><strong>Ryan:&nbsp;</strong>What kind of... What do you hope people hear out of this episode and out of your comments? Eliezer in 2023, who is sort of running on the last fumes of, of hope. Yeah, what do you, what do you want people to get out of this episode? What are you planning to do?</p><p><strong>Eliezer:&nbsp;</strong>I don't have concrete hopes here. You know, when everything is in ruins, you might as well speak the truth, right? Maybe&nbsp;<i>somebody</i> hears it,&nbsp;<i>somebody</i> figures out something I didn't think of.</p><p>I mostly expect that this does more harm than good in the modal universe, because a bunch of people are like, \u201cOh, I have this brilliant, clever idea,\u201d which is, you know, something that I was arguing against in 2003 or whatever, but you know, maybe somebody out there with the proper level of pessimism hears and thinks of something I didn't think of.</p><p>I suspect that if there's hope at all, it comes from a technical solution, because the difference between technical problems and political problems is at least the technical problems have solutions in principle. At least the technical problems are solvable. We're not on course to solve this one, but I think anybody who's hoping for a political solution has frankly not understood the technical problem.&nbsp;</p><p>They do not understand what it looks like to try to solve the political problem to such a degree that the world is not controlled by AI because they don't understand how easy it is to destroy the world with AI, given that the clock keeps ticking forward.</p><p>They're thinking that they just have to stop some bad actor, and that's why they think there's a political solution.</p><p>But yeah, I don't have concrete hopes. I didn't come on this episode out of any concrete hope.</p><p>I have no takeaways except, like, don't make this thing worse.</p><p>Don't, like, go off and accelerate AI more. Don't\u2014f you have a brilliant solution to alignment, don't be like, \u201cAh yes, I have solved the whole problem. We just use the following clever trick.\u201d</p><p>You know, \u201cDon't make things worse\u201d isn\u2019t very much of a message, especially when you're pointing people at the field at all. But I have no winning strategy. Might as well go on this podcast as an experiment and say what I think and see what happens. And probably no good ever comes of it, but you might as well go down fighting, right?</p><p>If there's a world that survives, maybe it's a world that survives because of a bright idea somebody had after listening to listening to this podcast\u2014that was&nbsp;<i>brighter</i>, to be clear, than the usual run of bright ideas that don't work.</p><p><strong>Ryan:&nbsp;</strong>Eliezer, I want to thank you for coming on and talking to us today. I do.</p><p>I don't know if, by the way, you've seen that movie that David was referencing earlier, the movie <i>Don\u2019t Look Up</i>, but I sort of feel like that news anchor, who's talking to the scientist\u2014is it Leonardo DiCaprio, David? And, uh, the scientist is talking about kind of dire straits for the world. And the news anchor just really doesn't know what to do. I'm almost at a loss for words at this point.</p><p><strong>David:&nbsp;</strong>I've had nothing for a while now.</p><p><strong>Ryan:&nbsp;</strong>But one thing I can say is I appreciate your honesty. I appreciate that you've given this a lot of time and given this a lot of thought. Everyone, anyone who has heard you speak or read anything you've written knows that you care deeply about this issue and have given it a tremendous amount of your life force, in trying to educate people about it.</p><p>And, um, thanks for taking the time to do that again today. I'll\u2014I guess I'll just let the audience digest this episode in the best way they know how. But, um, I want to reflect everybody in crypto and everybody listening to Bankless\u2014their thanks for you coming on and explaining.</p><p><strong>Eliezer:&nbsp;</strong>Thanks for having me. We'll see what comes of it.</p><p><strong>Ryan:&nbsp;</strong>Action items for you, Bankless nation. We always end with some action items. Not really sure where to refer folks to today, but one thing I know we can refer folks to is MIRI, which is the machine research intelligence institution that Eliezer has been talking about through the episode. That is at&nbsp;<a href=\"https://intelligence.org/\"><u>intelligence.org</u></a>, I believe. And some people in crypto have donated funds to this in the past. Vitalik Buterin is one of them. You can take a look at what they're doing as well. That might be an action item for the end of this episode.</p><p>Um, got to end with risks and disclaimers\u2014man, this seems very trite, but our legal experts have asked us to say these at the end of every episode. \u201cCrypto is risky. You could lose everything...\u201d</p><p><strong>Eliezer:&nbsp;</strong>(<i>laughs</i>)</p><p><strong>David:&nbsp;</strong>Apparently not as risky as AI, though.</p><p><strong>Ryan:&nbsp;</strong>\u2014But we're headed west! This is the frontier. It's not for everyone, but we're glad you're with us on the Bankless journey. Thanks a lot.</p><p><strong>Eliezer:&nbsp;</strong>And we are grateful for the crypto community\u2019s support. Like, it was possible to end with even less grace than this.</p><p><strong>Ryan:&nbsp;</strong>Wow. (<i>laughs</i>)</p><p><strong>Eliezer:&nbsp;</strong>And you made a difference.</p><p><strong>Ryan:&nbsp;</strong>We appreciate you.</p><p><strong>Eliezer:&nbsp;</strong>You really made a difference.</p><p><strong>Ryan:&nbsp;</strong>Thank you.</p><hr><h2><a href=\"https://twitter.com/i/spaces/1PlJQpZogzVGE\">Q&amp;A</a></h2><p><strong>Ryan:&nbsp;</strong>[... Y]ou gave up this quote, from I think someone who's an executive director at MIRI: \"We've given up hope, but not the fight.\"</p><p>Can you reflect on that for a bit? So it's still possible to fight this, even if we've given up hope? And even if you've given up hope? Do you have any takes on this?</p><p><strong>Eliezer:</strong> I mean, what else is there to do? You don't have good ideas. So you take your mediocre ideas, and your not-so-great ideas, and you pursue those until the world ends. Like, what's supposed to be better than that?</p><p><strong>Ryan:</strong> We had some really interesting conversation flow out of this episode, Eliezer, as you can imagine. And David and I want to relay some questions that the community had for you, and thank you for being gracious enough to help with those questions in today's Twitter Spaces.</p><p>I'll read something from Luke ethwalker. \"Eliezer has one pretty flawed point in his reasoning. He assumes that AI would have no need or use for humans because we have atoms that could be used for better things. But how could an AI use these atoms without an agent operating on its behalf in the physical world? Even in his doomsday scenario, the AI relied on humans to create the global, perfect killing virus. That's a pretty huge hole in his argument, in my opinion.\"</p><p>What's your take on this? That maybe AIs will dominate the digital landscape but because humans have a physical manifestation, we can still kind of beat the superintelligent AI in our physical world?</p><p><strong>Eliezer:</strong> If you were&nbsp;<a href=\"https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message\"><u>an alien civilization</u></a> of a billion John von Neumanns, thinking at 10,000 times human speed, and you start out connected to the internet, you would want to not be just stuck on the internet, you would want to build that physical presence. You would not be content solely with working through human hands, despite the many humans who'd be lined up, cheerful to help you, you know. Bing already has its partisans. (<i>laughs</i>)</p><p>You wouldn\u2019t be content with that, because the humans are very slow, glacially slow. You would like fast infrastructure in the real world, reliable infrastructure. And how do you build that, is then the question, and a whole lot of advanced analysis has been done on this question. I would point people again to Eric Drexler's&nbsp;<i>Nanosystems</i>.</p><p>And, sure, if you literally start out connected to the internet, then probably the fastest way \u2014 maybe not the only way, but it's, you know, an easy way \u2014 is to get humans to do things. And then humans do those things. And then you have the desktop \u2014 not quite desktop, but you have the nanofactories, and then you don't need the humans anymore. And this need not be advertised to the world at large while it is happening.</p><p><strong>David:</strong> So I can understand that perspective, like in the future, we will have better 3D printers \u2014 distant in the future, we will have ways where the internet can manifest in the physical world. But I think this argument does ride on a future state with technology that we don't have today. Like, I don't think if I was the internet \u2014 and that kind of is this problem, right? Like, this superintelligent AI just becomes the internet because it's embedded in the internet. If I was the internet, how would I get myself to manifest in real life?</p><p>And now, I am not an expert on the current state of robotics, or what robotics are connected to the internet. But I don't think we have too strong of tools today to start to create in the real world manifestations of an internet-based AI. So like, would you say that this part of this problem definitely depends on some innovation, at like the robotics level?</p><p><strong>Eliezer:</strong> No, it depends on the AI being smart. It doesn't depend on the humans having this technology; it depends on the AI being able to invent the technology.</p><p>This is, like, the central problem: the thing is smarter. Not in the way that the average listener to this podcast probably has an above average IQ, the way that humans are smarter than chimpanzees.</p><p>What does that let humans do? Does it let humans be, like, really&nbsp;<i>clever</i> in how they play around with the stuff that's on the ancestral savanna? Make&nbsp;<i>clever</i> use of grass,&nbsp;<i>clever</i> use of trees?</p><p>The humans invent technology. They build the technology. The technology is not there until the humans invent it, the humans conceive it.</p><p>The problem is, humans are not the upper bound. We don't have the best possible brains for that kind of problem. So the existing internet is more than connected enough to people and devices, that you could build better technology than that if you had invented the technology because you were thinking much, much faster and better than a human does.</p><p><strong>Ryan:</strong> Eliezer, this is a question from stirs, a Bankless Nation listener. He wants to ask the question about your explanation of why the AI will undoubtedly kill us. That seems to be your conclusion, and I'm wondering if you could kind of reinforce that claim. Like, for instance \u2014 and this is something David and I discussed after the episode, when we were debriefing on this \u2014 why exactly wouldn't an AI, or couldn't an AI just blast off of the Earth and go somewhere more interesting, and leave us alone? Like, why does it have to take our atoms and reassemble them? Why can't it just, you know, set phasers to ignore?</p><p><strong>Eliezer:</strong> It could if it wanted to. But if it doesn't want to, there is some initial early advantage. You get to colonize the universe slightly earlier if you consume all of the readily accessible energy on the Earth's surface as part of your blasting off of the Earth process.</p><p>It would only need to care for us by a very tiny fraction to spare us, this I agree. Caring a very tiny fraction is basically the same problem as 100% caring. It's like, well, could you have a computer system that is usually like the Disk Operating System, but a tiny fraction of the time it's Windows 11? Writing that is just as difficult as writing Windows 11. We still have to write all the Windows 11 software. Getting it to care a tiny little bit is the same problem as getting it to care 100%.</p><p><strong>Ryan:</strong> So Eliezer, is this similar to the relationship that humans have with other animals, planet Earth? I would say largely we really don't... I mean, obviously, there's no animal Bill of Rights. Animals have no legal protection in the human world, and we kind of do what we want and trample over their rights. But it doesn't mean we necessarily kill all of them. We just largely ignore them.</p><p>If they're in our way, you know, we might take them out. And there have been whole classes of species that have gone extinct through human activity, of course; but there are still many that we live alongside, some successful species as well. Could we have that sort of relationship with an AI? Why isn't that reasonably high probability in your models?</p><p><strong>Eliezer</strong> So first of all, all these things are&nbsp;<i>just</i> metaphors. AI is not going to be exactly like humans to animals.</p><p>Leaving that aside for a second, the reason why this metaphor breaks down is that although the humans are smarter than the chickens, we're not smarter than evolution, natural selection, cumulative optimization power over the last billion years and change. (You know, there's evolution before that but it's pretty slow, just, like, single-cell stuff.)</p><p>There are things that cows can do for us, that we cannot do for ourselves. In particular, make meat by eating grass. We\u2019re smarter than the cows, but there's a thing that designed the cows; and we're faster than that thing, but we've been around for much less time. So we have not yet gotten to the point of redesigning the entire cow from scratch. And because of that, there's a purpose to keeping the cow around alive.</p><p>And humans, furthermore, being the kind of funny little creatures that we are \u2014 some people care about cows, some people care about chickens. They're trying to fight for the cows and chickens having a better life, given that they have to exist at all. And there's a long complicated story behind that. It's not simple, the way that humans ended up in that [??]. It has to do with the particular details of our evolutionary history, and unfortunately it's not just going to pop up out of nowhere.</p><p>But I'm drifting off topic here. The basic answer to the question \"where does that analogy break down?\" is that I expect the superintelligences to be able to do better than natural selection, not just better than the humans.</p><p><strong>David:</strong> So I think your answer is that the separation between us and a superintelligent AI is orders of magnitude larger than the separation between us and a cow, or even us than an ant. Which, I think a large amount of this argument resides on this superintelligence explosion \u2014 just going up an exponential curve of intelligence very, very quickly, which is like the premise of superintelligence.</p><p>And Eliezer, I want to try and get an understanding of... A part of this argument about \"AIs are going come kill us\" is buried in the Moloch problem. And Bankless listeners are pretty familiar with the concept of Moloch \u2014 the idea of coordination failure. The idea that the more that we coordinate and stay in agreement with each other, we actually create a larger incentive to defect.</p><p>And the way that this is manifesting here, is that even if we do have a bunch of humans, which understand the AI alignment problem, and we all agree to only safely innovate in AI, to whatever degree that means, we still create the incentive for someone to fork off and develop AI faster, outside of what would be considered safe.</p><p>And so I'm wondering if you could, if it does exist, give us the sort of lay of the land, of all of these commercial entities? And what, if at all, they're doing to have, I don't know, an AI alignment team?</p><p>So like, for example, OpenAI. Does OpenAI have, like, an alignment department? With all the AI innovation going on, what does the commercial side of the AI alignment problem look like? Like, are people trying to think about these things? And to what degree are they being responsible?</p><p><strong>Eliezer:</strong> It looks like OpenAI having a bunch of people who it pays to do AI ethics stuff, but I don't think they're plugged very directly into Bing. And, you know, they've got that department because back when they were founded, some of their funders were like, \"Well, but ethics?\" and OpenAI was like \"Sure, we can buy some ethics. We'll take this group of people, and we'll put them over here and we'll call them an alignment research department\".</p><p>And, you know, the key idea behind ChatGPT is RLHF, which was invented by Paul Christiano. Paul Christiano had much more detailed ideas, and somebody might have reinvented this one, but anyway. I don't think that went through OpenAI, but I could be mistaken. Maybe somebody will be like \"Well, actually, Paul Christiano was working at OpenAI at the time\", I haven't checked the history in very much detail.</p><p>A whole lot of the people who were most concerned with this \"ethics\" left OpenAI, and founded Anthropic. And I'm&nbsp;<i>still</i> not sure that Anthropic has sufficient leadership focus in that direction.</p><p>You know, like, put yourself in the shoes of a corporation! You can spend some little fraction of your income on putting together a department of people who will write safety papers. But then the actual behavior that we've seen, is they storm ahead, and they use one or two of the ideas that came out from anywhere in the whole [alignment] field. And they get as far as that gets them. And if that doesn't get them far enough, they just keep storming ahead at maximum pace, because, you know, Microsoft doesn't want to lose to Google, and Google doesn't want to lose to Microsoft.</p><p><strong>David:</strong> So it sounds like your attitude on the efforts of AI alignment in commercial entities is, like, they're not even doing 1% of what they need to be doing.</p><p><strong>Eliezer:</strong> I mean, they could spend [10?] times as much money and that would not get them to 10% of what they need to be doing.</p><p>It's not just a problem of \u201coh, they they could spend the resources, but they don't want to\u201d. It's a question of \u201chow do we even spend the resources to get the info that they need\u201d.</p><p>But that said, not knowing how to do that, not really understanding that they need to do that, they are just charging ahead anyways.</p><p><strong>Ryan:</strong> Eliezer, is OpenAI the most advanced AI project that you're aware of?</p><p><strong>Eliezer:</strong> Um, no, but I'm not going to go name the competitor, because then people will be like, \"Oh, I should go work for them\", you know? I'd rather they didn't.</p><p><strong>Ryan:</strong> So it's like, OpenAI is this organization that was kind of \u2014 you were talking about it at the end of the episode, and for crypto people who aren't aware of some of the players in the field \u2014 were they spawned from that 2015 conference that you mentioned? It's kind of a completely open-source AI project?</p><p><strong>Eliezer:</strong> That was the original suicidal vision, yes. But...</p><p><strong>Ryan:</strong> And now they're bent on commercializing the technology, is that right?</p><p><strong>Eliezer:</strong> That's an improvement, but not enough of one, because they're still generating lots of noise and hype and directing more resources into the field, and storming ahead with the safety that they have instead of the safety that they need, and setting bad examples. And getting Google riled up and calling back in Larry Page and Sergey Brin to head up Google's AI projects and so on. So, you know, it could be worse! It would be worse if they were open sourcing all the technology. But what they're doing is still pretty bad.</p><p><strong>Ryan:</strong> What should they be doing, in your eyes? Like, what would be responsible use of this technology?</p><p>I almost get the feeling that, you know, your take would be \"stop working on it altogether\"? And, of course, you know, to an organization like OpenAI that's going to be heresy, even if maybe that's the right decision for humanity. But what should they be doing?</p><p><strong>Eliezer:</strong> I mean, if you literally just made me dictator of OpenAI, I would change the name to \"ClosedAI\". Because right now, they're making it look like being \"closed\" is hypocrisy. They're, like, being \"closed\" while keeping the name \"OpenAI\", and that itself makes it looks like closure is like not this thing that you do cooperatively so that humanity will not die, but instead this sleazy profit-making thing that you do while keeping the name \u201cOpenAI\u201d.</p><p>So that's very bad; change the name to \"ClosedAI\", that's step one.</p><p>Next. I don't know if they&nbsp;<i>can</i> break the deal with Microsoft. But, you know, cut that off. None of this. No more hype. No more excitement. No more getting famous and, you know, getting your status off of like, \"Look at how much closer&nbsp;<i>we</i> came to destroying the world! You know, we're not there yet. But, you know, we're at the&nbsp;<i>forefront</i> of destroying the world!\" You know, stop grubbing for the Silicon Valley bragging cred of visibly being the leader.</p><p>Take it all closed. If you got to make money, make money selling to businesses in a way that doesn't generate a lot of hype and doesn't visibly push the field.And then try to figure out systems that are more alignable and not just more powerful. And at the end of that, they would fail, because, you know, it's not easy to do that. And the world would be destroyed. But they would have died with more dignity. Instead of being like, \"Yeah, yeah, let's like push humanity off the cliff ourselves for the ego boost!\", they would have done what they could, and then failed.</p><p><strong>David:</strong> Eliezer, do you think anyone who's building AI \u2014 Elon Musk, Sam Altman at OpenAI \u2013 do you think progressing AI is fundamentally bad?</p><p><strong>Eliezer:</strong> I mean, there are&nbsp;<i>narrow</i> forms of progress, especially if you&nbsp;<i>didn't open-source them</i>, that would be good. Like, you can imagine a thing that, like, pushes capabilities a bit, but is much more alignable.</p><p>There are people working in the field who I would say are, like, sort of&nbsp;<i>unabashedly</i> good. Like, Chris Olah is taking a microscope to these giant inscrutable matrices and trying to figure out what goes on inside there. Publishing that might possibly even push capabilities a little bit, because if people know what's going on inside there, they can make better ones. But the question of like, whether to closed-source&nbsp;<i>that</i> is, like, much more fraught than the question of whether to closed-source the stuff that's just pure capabilities.</p><p>But that said, the people who are just like, \"Yeah, yeah, let's do more stuff! And let's tell the world how we did it, so they can do it too!\" That's just, like, unabashedly bad.</p><p><strong>David:</strong> So it sounds like you do see paths forward in which we can develop AI in responsible ways. But it's really this open-source, open-sharing-of-information to allow anyone and everyone to innovate on AI,&nbsp; that's really the path towards doom. And so we actually need to keep this knowledge private. Like, normally knowledge...</p><p><strong>Eliezer:</strong> No, no, no, no. Open-sourcing all this stuff is, like, a&nbsp;<i>less</i> dignified path straight off the edge. I'm not saying that all we need to do is keep everything closed and in the right hands and it will be fine. That will also kill you.</p><p>But that said, if you have stuff and you&nbsp;<i>do not know</i> how to make it not kill everyone, then broadcasting it to the world is even&nbsp;<i>less</i> dignified than being like, \"Okay, maybe we should&nbsp;<i>keep</i> working on this until we can figure out how to make it&nbsp;<i>not</i> kill everyone.\"</p><p>And then the other people will, like, go storm ahead on&nbsp;<i>their</i> end and kill everyone. But, you know, you won't have&nbsp;<i>personally</i> slaughtered Earth. And that is more dignified.</p><p><strong>Ryan:</strong> Eliezer, I know I was kind of shaken after our episode, not having heard the full AI alignment story, at least listened to it for a while.</p><p>And I think that in combination with the sincerity through which you talk about these subjects, and also me sort of seeing these things on the horizon, this episode was kind of shaking for me and caused a lot of thought.</p><p>But I'm noticing there is a cohort of people who are dismissing this take and your take specifically in this episode as Doomerism. This idea that every generation thinks it's, you know, the end of the world and the last generation.</p><p>What's your take on this critique that, \"Hey, you know, it's been other things before. There was a time where it was nuclear weapons, and we would all end in a mushroom cloud. And there are other times where we thought a pandemic was going to kill everyone. And this is just the latest Doomerist AI death cult.\"</p><p>I'm sure you've heard that before. How do you respond?</p><p><strong>Eliezer:</strong> That if you literally know nothing about nuclear weapons or artificial intelligence, except that somebody has claimed of both of them that they'll destroy the world, then sure, you can't tell the difference. As far as you can tell, nuclear weapons were claimed to destroy the world, and then they didn't destroy the world, and then somebody claimed that about AI.</p><p>So, you know, Laplace's rule of induction: at most a 1/3 probability that AI will destroy the world, if nuclear weapons and AI are the only case.</p><p>You can bring in so many more cases than that. Why, people should have known in the first place that nuclear weapons wouldn't destroy the world! Because their next door neighbor once said that the sky was falling, and that didn't happen; and if their next door weapon was [??], how could the people saying that nuclear weapons would destroy the world be right?</p><p>And basically, as long as people are trying to run off of models of human psychology, to derive empirical information about the world, they're stuck. They're in a trap they can never get out of. They\u2019re going to always be trying to psychoanalyze the people talking about nuclear weapons or whatever. And the only way you can actually get better information is by understanding how nuclear weapons work, understanding what the international equilibrium with nuclear weapons looks like. And the international equilibrium, by the way, is that nobody profits from setting off small numbers of nuclear weapons, especially given that they know that large numbers of nukes would follow. And, you know, that's why they haven't been used yet. There was nobody who made a buck by starting a nuclear war. The nuclear war was clear, the nuclear war was legible. People knew what would happen if they fired off all the nukes.</p><p>The analogy I sometimes try to use with artificial intelligence is, \u201cWell, suppose that instead you could make nuclear weapons out of a billion pounds of laundry detergent. And they spit out gold until you make one that's too large, whereupon it ignites the atmosphere and kills everyone.&nbsp;<i>And</i> you can't calculate exactly how large is too large.&nbsp;<i>And</i> the international situation is that the private research labs spitting out gold don't want to hear about igniting the atmosphere.\u201d And that's the technical difference. You need to be able to tell whether or not that is true as a scientific claim about how reality, the universe, the environment, artificial intelligence, actually works. What actually happens when the giant inscrutable matrices go past a certain point of capability? It's a falsifiable hypothesis.</p><p>You know, if it&nbsp;<i>fails</i> to be falsified, then everyone is dead, but that doesn't actually change the basic dynamic here, which is, you can't figure out how the world works by psychoanalyzing the people talking about it.</p><p><strong>David:</strong> One line of questioning that has come up inside of the Bankless Nation Discord is the idea that we need to train AI with data, lots of data. And where are we getting that data? Well, humans are producing that data. And when humans produce that data, by nature of the fact that it was produced by humans, that data has our human values embedded in it somehow, some way, just by the aggregate nature of all the data in the world, which was created by humans that have certain values. And then AI is trained on that data that has all the human values embedded in it. And so there's actually no way to create an AI that isn't trained on data that is created by humans, and that data has human values in it.</p><p>Is there anything to this line of reasoning about a potential glimmer of hope here?</p><p><strong>Eliezer:</strong> There's a distant glimmer of hope, which is that an AI that is trained on tons of human data in this way probably understands some things about humans. And because of that, there's a branch of research hope within alignment, which is something that like, \u201cWell, this AI, to be able to predict humans, needs to be able to predict the thought processes that humans are using to make their decisions. So can we thereby point to human values inside of the knowledge that the AI has?\u201d</p><p>And this is, like, very nontrivial, because the simplest theory that you use to predict what humans decide next, does not have what you might term \u201cvalid morality under reflection\u201d as a clearly labeled primitive chunk inside it that is directly controlling the humans, and which you need to understand on a scientific level to understand the humans.</p><p>The humans are full of hopes and fears and thoughts and desires. And somewhere in all of that is what we call \u201cmorality\u201d, but it's not a clear, distinct chunk, where an alien scientist examining humans and trying to figure out just purely on an empirical level \u201chow do these humans work?\u201d would need to point to one particular chunk of the human brain and say, like, \"Ahh, that circuit there, the morality circuit!\"</p><p>So it's not easy to point to inside the AI's understanding. There is not currently any obvious way to actually promote that chunk of the AI's understanding to then be in control of the AI's planning process. As it must be complicatedly pointed to, because it's not just a simple empirical chunk for explaining the world.</p><p>And basically, I don't think that is actually going to be the route you should try to go down. You should try to go down something much simpler than that. The problem is not that we are going to fail to convey some&nbsp;<i>complicated subtlety</i> of human value. The problem is that we do not know how to align an AI on a task like \u201cput two identical strawberries on a plate\u201d without destroying the world.</p><p>(Where by \"put two identical strawberries on the plate\", the concept is that's invoking enough power that it's not safe AI that can build two strawberries identical down to the cellular level. Like, that's a powerful AI. Aligning it isn't simple. If it's powerful enough to do that, it's also powerful enough to destroy the world, etc.)</p><p><strong>David:</strong> There's like a number of other lines of logic I could try to go down, but I think I would start to feel like I'm in the bargaining phase of death. Where it's like \u201cWell, what about this? What about that?\u201d</p><p>But maybe to summate all of the arguments, is to say something along the lines of like, \"Eliezer, how much room do you give for the long tail of black swan events? But these black swan events are actually us finding a solution for this thing.\" So, like, a reverse black swan event where we actually don't know how we solve this AI alignment problem. But really, it's just a bet on human ingenuity. And AI hasn't taken over the world&nbsp;<i>yet</i>. But there's space between now and then, and human ingenuity will be able to fill that gap, especially when the time comes?</p><p>Like, how much room do you leave for the long tail of just, like, \"Oh, we'll discover a solution that we can't really see today\"?</p><p><strong>Eliezer:</strong> I mean, on the one hand, that hope is all that's left, and all that I'm pursuing. And on the other hand, in the process of actually pursuing that hope I do feel like I've gotten some feedback indicating that this hope is not necessarily very large.</p><p>You know, when you've got stage four cancer, is there still hope that your body will just rally and suddenly fight off the cancer? Yes, but it's not what usually happens. And I've seen people come in and try to direct their ingenuity at the alignment problem and most of them all invent the&nbsp;<i>same</i> small handful of bad solutions. And it's harder than usual to direct human ingenuity at this.</p><p>A lot of them are just, like \u2014 you know, with capabilities ideas, you run out and try them and they mostly don't work. And some of them do work and you publish the paper, and you get your science [??], and you get your ego boost, and maybe you get a job offer someplace.</p><p>And with the alignment stuff you can try to run through the analogous process, but the stuff we need to align is mostly not here yet. You can try to invent the smaller large language models that are public, you can go to work at a place that has access to larger large language models, you can try to do these very crude, very early experiments, and getting the large language models to at least not threaten your users with death \u2014</p><p>\u2014&nbsp;<i>which isn't the same problem at all</i>. It just kind of looks related.</p><p>But you're at least trying to get AI systems that do what you want them to do, and not do other stuff; and that is, at the very core, a similar problem.</p><p>But the AI systems are not very powerful, they're not running into all sorts of problems that you can predict will crop up later. And people just, kind of \u2014 like, mostly people short out. They do pretend work on the problem. They're desperate to help, they got a grant, they now need to show the people who made the grant that they've made progress. They, you know, paper mill stuff.</p><p>So the human ingenuity is not functioning well right now. You cannot be like, \"Ah yes, this present field full of human ingenuity, which is working great, and coming up with lots of great ideas, and building up its strength, will continue at this pace and make it to the finish line in time!\u201d</p><p>The capability stuff is&nbsp;<i>storming on</i> ahead. The human ingenuity that's being directed at that is much larger, but also it's got a much easier task in front of it.</p><p>The question is not \"Can human ingenuity ever do this at all?\" It's \"Can human ingenuity&nbsp;<i>finish</i> doing this before OpenAI blows up the world?\"</p><p><strong>Ryan:&nbsp;</strong>Well, Eliezer, if we can't trust in human ingenuity, is there any possibility that we can trust in AI ingenuity? And here's what I mean by this, and perhaps you'll throw a dart in this as being hopelessly naive.</p><p>But is there the possibility we could ask a reasonably intelligent, maybe almost superintelligent AI, how we might fix the AI alignment problem? And for it to give us an answer? Or is that really not how superintelligent AIs work?</p><p><strong>Eliezer:</strong> I mean, if you literally build a superintelligence and for some reason it was motivated to answer you, then sure, it could answer you.</p><p>Like, if Omega comes along from a distant supercluster and offers to pay the local superintelligence lots and lots of money (or, like, mass or whatever) to give you a correct answer, then sure, it knows the correct answer; it can give you the correct answers.</p><p>If it&nbsp;<i>wants</i> to do that, you must have&nbsp;<i>already</i> solved the alignment problem. This reduces the problem of solving alignment to the problem of solving alignment. No progress has been made here.</p><p>And, like, working on alignment is actually one of the most difficult things you could possibly try to align.</p><p>Like, if I had the health and was trying to die with more dignity by building a system and aligning it as best I could figure out how to align it, I would be targeting something on the order of \u201cbuild two strawberries and put them on a plate\u201d. But instead of building two identical strawberries and putting them on a plate, you \u2014 don't actually do this, this is not the best thing you should do \u2014</p><p>\u2014 but if for example you could safely align \u201cturning all the GPUs into Rubik's cubes\u201d, then that&nbsp;<i>would</i> prevent the world from being destroyed two weeks later by your next follow-up competitor.</p><p>And that's&nbsp;<i>much easier</i> to align an AI on than trying to get the AI to solve alignment for you. You could be trying to build something that would&nbsp;<i>just</i> think about nanotech, just think about the science problems, the physics problems, the chemistry problems, the synthesis pathways.&nbsp;</p><p>(The open-air operation to find all the GPUs and turn them into Rubik's cubes would be harder to align, and that's why you shouldn't actually try to do that.)</p><p>My point here is: whereas [with] alignment, you've got to think about AI technology and computers and humans and intelligent adversaries, and distant superintelligences who might be trying to exploit your AI's imagination of those distant superintelligences, and ridiculous weird problems that would take so long to explain.</p><p>And it just covers this enormous amount of territory, where you\u2019ve got to understand how humans work, you've got to understand how adversarial humans might try to exploit and break an AI system \u2014 because if you're trying to build an aligned AI that's going to run out and operate in the real world, it would have to be resilient to those things.</p><p>And they're just hoping that the AI is going to do their homework for them! But it's a chicken and egg scenario. And if you could actually get an AI to help you with something, you would not try to get it to help you with something as weird and not-really-all-that-effable as alignment. You would try to get it to help with something much simpler that could prevent the next AGI down the line from destroying the world.</p><p>Like nanotechnology. There's a whole bunch of advanced analysis that's been done of it, and the&nbsp;<i>kind of thinking</i> that you have to do about it is so much more straightforward and so much less fraught than trying to, you know... And how do you even tell if it's lying about alignment?</p><p>It's hard to tell whether&nbsp;<i>I'm</i> telling you the truth about all this alignment stuff, right? Whereas if I talk about the tensile strength of sapphire, this is easier to check through the lens of logic.</p><p><strong>David:</strong> Eliezer, I think one of the reasons why perhaps this episode impacted Ryan \u2013 this was an analysis from a Bankless Nation community member \u2014 that this episode impacted Ryan a little bit more than it impacted me is because Ryan's got kids, and I don't. And so I'm curious, like, what do you think \u2014 like, looking 10, 20, 30 years in the future, where you see this future as inevitable, do you think it's futile to project out a future for the human race beyond, like, 30 years or so?</p><p><strong>Eliezer</strong>: Timelines are very hard to project. 30 years does strike me as unlikely at this point. But, you know, timing is famously much harder to forecast than saying that things can be done at all. You know, you got your people saying it will be 50 years out two years before it happens, and you got your people saying it'll be two years out 50 years before it happens. And, yeah, it's... Even if I knew&nbsp;<i>exactly</i> how the technology would be built, and&nbsp;<i>exactly</i> who was going to build it, I&nbsp;<i>still</i> wouldn't be able to tell you how long the project would take because of project management chaos.</p><p>Now, since I don't know exactly the technology used, and I don't know exactly who's going to build it, and the project may not even have started yet, how can I possibly figure out how long it's going to take?</p><p><strong>Ryan:</strong> Eliezer, you've been quite generous with your time to the crypto community, and we just want to thank you. I think you've really opened a lot of eyes. This isn't going to be our last AI podcast at Bankless, certainly. I think the crypto community is going to dive down the rabbit hole after this episode. So thank you for giving us the 400-level introduction into it.</p><p>As I said to David, I feel like we waded straight into the deep end of the pool here. But that's probably the best way to address the subject matter. I'm wondering as we kind of close this out, if you could leave us \u2014 it is part of the human spirit to keep and to maintain slivers of hope here or there. Or as maybe someone you work with put it \u2013 to&nbsp;<i>fight the fight</i>, even if the hope is gone.</p><p>100 years in the future, if humanity is still alive and functioning, if a superintelligent AI has not taken over, but we live in coexistence with something of that caliber \u2014 imagine if that's the case, 100 years from now. How did it happen?</p><p>Is there some possibility, some sort of narrow pathway by which we can navigate this? And if this were 100 years from now the case, how could you imagine it would have happened?</p><p><strong>Eliezer:</strong> For one thing, I predict that if there's a glorious transhumanist future (as it is sometimes conventionally known) at the end of this, I don't predict it was there by getting like, \u201ccoexistence\u201d with superintelligence. That's, like, some kind of weird, inappropriate analogy based off of humans and cows or something.</p><p>I predict alignment was solved. I predict that if the humans are alive at all, that the superintelligences are being quite nice to them.</p><p>I have basic moral questions about whether it's ethical for humans to have human children, if having transhuman children is an option instead. Like, these humans running around? Are they, like, the current humans who wanted eternal youth but, like, not the brain upgrades? Because I do see the case for letting an existing person choose \"No, I just want eternal youth and no brain upgrades, thank you.\" But then if you're deliberately having the equivalent of a very crippled child when you could just as easily have a not crippled child.</p><p>Like, should humans in their present form be around together? Are we, like, kind of too sad in some ways? I have friends, to be clear, who disagree with me so much about this point. (<i>laughs</i>) But yeah, I'd say that the happy future looks like beings of light having lots of fun in a nicely connected computing fabric powered by the Sun, if we haven't taken the sun apart yet. Maybe there's enough real sentiment in people that you just, like, clear all the humans off the Earth and leave the entire place as a park. And even, like, maintain the Sun, so that the Earth is still a park even after the Sun would have ordinarily swollen up or dimmed down.</p><p>Yeah, like... That was always the things to be fought for. That was always the point, from the perspective of everyone who's been in this for a long time. Maybe not literally everyone, but like, the whole old crew.</p><p><strong>Ryan:</strong> That is a good way to end it: with some hope. Eliezer, thanks for joining the crypto community on this collectibles call and for this follow-up Q&amp;A. We really appreciate it.</p><p><strong>michaelwong.eth:&nbsp;</strong>Yes, thank you, Eliezer.</p><p><strong>Eliezer:</strong> Thanks for having me.</p>", "user": {"username": "RobBensinger"}}, {"_id": "rx9ATh6DjDJBtxBwc", "title": "If EAs won't go vegan what chance do animals have?", "postedAt": "2023-03-12T22:24:29.535Z", "htmlBody": "<p>Before I address the title of the article I'm going to quickly outlined why I think brands and products grow and why behaviours in general become more popular. This post involves some meandering so reader beware.</p><hr><p>I've been doing marketing for about 15 years and as far as I can tell there are 3 models of communication that change people's behaviour:</p><p>Model 1: \"SALIENCE\":&nbsp;</p><p>communication changes behaviour by creating salience between an intervention or product or brand and the memories people access at a point of purchase or engagement (i.e. 'when they are in-market'). For example, when I want to make \"pasta bolognaise\" the associated memories my brain surfaces could be \"barilla\", \"italian\" and \"beyond meat\" (for all the nerds this leans into associative network theory if you're interested in learning more)</p><p>Model 2: \"PERSUASION\":&nbsp;&nbsp;</p><p>communication changes behaviour by persuading or telling a story. This leans into System 2 thinking and Narrative Transportation Theory respectively (I recommend checking out Thinking Fast and Slow and <a href=\"https://en.wikipedia.org/wiki/Transportation_theory_(psychology)\">https://en.wikipedia.org/wiki/Transportation_theory_(psychology)</a>)</p><p>Model 3: \"CULTURAL IMPRINTING\": communication changes behaviour because all consumption is actually about building and maintaining status within a desired group and all products are consumed in social settings (for more I recommend \"ads don't work that way\" &gt; <a href=\"https://meltingasphalt.com/ads-dont-work-that-way/\">https://meltingasphalt.com/ads-dont-work-that-way/</a>)</p><p>In classic marketing theory these are all bucketed under \"Promotion\" (i.e. communication). There are 3 other \"P's\" to marketing (and reasons why products or brands grow):</p><p>Product &amp; Price (which IMO only need to be 'good enough' rather than 'the best' - see <a href=\"https://www.investopedia.com/terms/s/satisficing.asp#:~:text=Satisficing%20is%20a%20decision%2Dmaking,effort%20when%20confronted%20with%20tasks.\">satisficing</a>)</p><p>Physical availability (Is the thing I want to buy easy to find?)</p><p><strong>Ok but what does this have to do with non human animals?</strong></p><hr><p>AFAICT about 30% of EAs are vegan but my model says that if non human animals have any hope this number should be closer to 90%.</p><p>Let's review the model:</p><p>M1 - \"SALIENCE\": if you're in EA you're hearing about animals suffering and vegan alternatives all the time</p><p>M2- \"PERSUASION\": EAs are especially rational people and not eating animals is obviously the more rational choice for 90%+ people reading this</p><p>M3: \"CULTURAL IMPRINTING\": it is a higher status move in the EA community to be vegan than not</p><p>Product &amp; Price: vegan food tastes fine, and EAs can afford it (i.e. they're relatively rich)</p><p>Physical availability: the hardest part of any product adoption is to get people to try it once and you can't go to an EA event without trying vegan food</p><p><strong>So how is any of this useful?</strong></p><hr><p>In behaviour change and marketing strategy a common practice to get a deeper or different view of peoples decision making is by studying its <a href=\"https://en.wikipedia.org/wiki/Extreme_users\">extreme users</a> instead of the general population.</p><p>Some examples of how this has worked elsewhere:</p><ol><li>Transmen and transwomen for feminine care innovation</li><li>Hikikomori for future social spaces</li><li>Amish for clothing sustainability</li><li>Arthritis sufferers for kitchen utensils</li></ol><p>Anyway, I think looking deeply at why EAs do and do not eat farm grown meat (at an individual level) and why vegan adoption is so low 'in EA culture' could provide lots of insight.</p>", "user": {"username": "Yanni Kyriacos"}}, {"_id": "kxaGNuHqmQqw2xYHW", "title": "It's not all that simple", "postedAt": "2023-03-13T04:33:44.243Z", "htmlBody": "<p>TL;DR: I feel that recently the EA forum became pretty judgmental and unwelcoming. I also feel that the current discourse about sex misses two important points and, in a huge part of it, lacks maturity and is harmful. Let me attempt to address it.&nbsp;<br><br><strong>Trigger warning</strong>, point 2 involves a long description of personal stories connected to sex, some of them were difficult and may be triggering. &nbsp;The post also may not be very well structured, but I preferred to write one long post instead of three short ones (but I've edited just a structure today, to make it a bit more readable). &nbsp;</p><p>This is obviously a burner account, but when you see those stories you\u2019ll be able to see why. For the record, they don\u2019t involve people from the community. I'm a woman (it's going to matter later on).<br>&nbsp;</p><h2>Acceptable dating and sexual behaviors vary between classes and cultures.</h2><p><br><strong>The devil is in the detail, and rules you live by and perceive as \u201cobvious\u201d may be so clear for anybody else. Also, the map of the US is not in a shape of geode.&nbsp;</strong><br><br><br>People vary in gender and sexual orientation. They vary in a level of sexual desire. They have different kinks, ways of expressing sexuality and levels of self-awareness. Different needs. Various physiological reactions to sexually tense situations. Various ways of presenting themselves when it comes to all of the above.&nbsp;</p><p>People come from different cultures \u2013 regions, countries, social classes and religions. As a result, dating cultures vary around the world. &nbsp;Sexual behaviors also. Acceptable level of flirt, jokes, touch and the way consent is asked for and expressed sometimes just vary. Problems and how i.e. sexism looks like also has various shapes and forms. There are some common characteristics, but <i>details</i> matter, to a huge extent.&nbsp;<br><br><strong>Many people in the recent discussions stated that various nuances are obvious and should be intuitively followed by everyone. I think it\u2019s problematic and leads to abuse.</strong>&nbsp;</p><p>Believing that your values and behavior associated with your culture and class are the only right ones and everybody should know, understand and follow them, is fundamentally different from assertively vocalizing <i>your boundaries</i> and needs. &nbsp;The second is a great, mature behavior. The first feels a bit elitist, ignorant and has nothing to do with safety, equality and being inclusive.<br>&nbsp;</p><p>Additionally, I want to draw your attention to one thing. I have a strong belief (correct me if I\u2019m wrong) that the vast majority (if not all) of sexual misconduct causes which were described over the last couple of days in the articles or here, on the forum, come from either US or the UK. &nbsp;EA crowd is definitely not limited to those.&nbsp;<br><br>So my honest question would be \u2013 is it <i>EA </i>who has a problem with sexual misconduct? Or is it an <i>Anglo-Saxon culture</i> which has a problem with sexual misconduct? Or maybe \u2013 <i>EA with a mix of Anglo-Saxon culture </i>has this issue? Shouldn\u2019t we zoom in on that a bit?&nbsp;</p><h2>Human sexuality is complex. Consent is also sometimes complex.</h2><p>People often talk a lot of \u201cwhat consent norms <i>should </i>be\u201d. But such disputes do not give a full picture of what people\u2019s actual behaviors around consent actually <i>are </i>\u2013 and it\u2019s a bit crucial to this whole conversation.&nbsp;<br><br><strong>If you start having more intimate talks, however, you end up seeing a much more complex and broad picture. And often consent is easier said than done.</strong><br>&nbsp;</p><p>I encourage you all, regardless what\u2019s your gender, to have those talks with friends, who are open and empathetic. I\u2019ve learned a lot and they made my life easier.&nbsp;<br>&nbsp;</p><p>Yet, some people may have no opportunity to hear such stories. So let me share, why do I think that consent is not all that easy. I'm going to talk about myself here, because maybe somebody needs to hear somebody being open and vulnerable about stuff like that. It sums up 12 years of my experience, from which the vast majority was super great, but focuses on the parts where this sex and consent was difficult.<br><br><strong>My message is - it's ok to sometimes struggle, feel insecure and have doubts, doesn't matter what your gender is, and especially if you are young.&nbsp;</strong><br>&nbsp;</p><p>(To be clear and open here - it happened to me a couple of times that I had to physically hit people in the face, as they plainly broke my consent and didn\u2019t react to open, direct \u201cno\u201d repeated loudly three times. I felt pretty shitty after. I\u2019m not going to get into those situations here. They don\u2019t add anything to my \u201cconsent is complex\u201d point. In those cases, it was plain and simple, and they simply tried to break it. )<br>&nbsp;</p><p>I really like sex, I think human body is beautiful and overall, the whole context can be a form of art. Still, I believe consent is complicated. During my life, for me it was sometimes hard to figure out what do I want, and where are my boundaries - and it was hard to figure out what do <i>others </i>want, and what are <i>their </i>boundaries.&nbsp;<br>It\u2019s all not that simple, but I also think that the whole <i>life</i> is not that simple and all I can do is just actively work to understand myself better and be better in listening to others. I know I'm not alone, and many people have similar doubts and situations.&nbsp;<br>&nbsp;</p><h3>Let me start gently, from slight doubts I have or used to have.&nbsp;</h3><ul><li><i>Flirting </i>is sometimes hard to understand for me. D<i>ancing</i> is also hard.&nbsp;<br>I can do it now, sure, but I don\u2019t really get the culture around it and when I was younger, I felt super scared. Some very monogamous, married people dance with other folks in a very sensual way \u2013 and don\u2019t see anything bad about it, even though they regard \u201cflirting\u201d as cheating. I\u2019m just a bit confused by what I perceive lack of consistency there.&nbsp;<br>To be frank, some parts of flirting seem to be overall heavily based on ambiguity and uncertainty, and mixed signals<strong>. </strong>To me, people seem to be in the same time scared and attracted by it. Which often was confusing.<br>&nbsp;</li><li>It happened to me to clearly flirt with a guy, then say that we won\u2019t have sex just in order to increase tension, and well, it worked wonders.&nbsp;<br>Later on I was the one who initiated sex. But, I\u2019ve heard from some girl friends of mine that they like to flirt and not being asked for consent directly at all. One even said that there are <i>two different types of \u201cno\u201d</i> \u2013 o<i>ne is just flirting, one is a lack of consent</i>, and which one is which depends on context. I don\u2019t really get it, I even felt angry at them for some time for not wanting to be precise about it (now I'm not).<br>&nbsp;</li><li>What I also discovered, is that I prefer being asked for consent, than ask myself. I sometimes do that and it\u2019s super scary and make me feel vulnerable.&nbsp;<br>&nbsp;</li><li>Also, one more fun fact, recently it happened to me that I got attracted to a man in a position of power and had to stop myself from initiating flirting, not to put in a stupid situation.&nbsp;<br>Despite I think the rules for instructors and students not to flirt are right and should be in place, in this particular situation I felt sad about it (still followed the rules though).<br>&nbsp;</li></ul><h3>Now, some heavier stuff.&nbsp;</h3><ul><li>So, it happened to me enthusiastically <i>consent </i>to something, even partially initiate it, and then <i>regret </i>it (a day after the event).&nbsp;<br>My partner behaved absolutely ok, he was not in any position of power and did not make me feel intimidated, checked if I\u2019m fine and respected my consent. I felt bad, it was hard ,but I had to deal with my emotions and have absolutely no hard feelings to anybody. Now I know my boundaries better. Live and learn.<br>&nbsp;</li><li>It happened to me to experience a pretty serious <i>miscommunication </i>when it comes to sex.&nbsp;<br>Basically a guy thought I gave him consent by coming to his house to sleep over. For me, in turn, it was just coming to his flat for a movie night and I didn't even think about sex. Thankfully, we realized something is off soon enough, nothing happened, we talked, we laughed and even slept in one bed - he respected my consent perfectly and wasn't even slightly creepy. We were close friends for a long time after, then I moved to another country.<br>&nbsp;</li><li>A couple of years later, at some point I ended up being <i>pushy towards my partner</i>, as I basically wasn\u2019t taught that woman can be pushy at all.&nbsp;<br>I misjudged how big of a difference there is between the level of our sexual desire. Also the whole situation was super new and unexpected to me. To be frank I literally didn't notice what I'm doing until he told me to stop. Thankfully, he assertively gave me his feedback and we talked. We are still a couple.<br>&nbsp;</li><li>On other occasion, I<i> heavily overreacted </i>to one guy\u2019s attempt to sleep with me.&nbsp;<br>He basically raised his voice, was very judgemental and talked without what I perceive to be empathy to other person (not me and not in a sexual context, I also was free to leave a situation, it wasn't about me at all). Ironically he did it in a context of \u201cprotecting people who are more vulnerable than him from unwanted behavior, i.e. people who are younger or women\u201d.&nbsp;<br>The point is, which I learned about much later on, aggression is my trauma trigger. And I perceived him as aggressive.&nbsp;<br>So when he asked for my consent I was very uncomfortable, ended up being afraid of saying \u201cno\u201d(despite the fact that I\u2019m usually very outspoken) and didn\u2019t know how to deal with the situation. He wasn\u2019t aggressive while asking and did it in a considerate, \u201cgood\u201d way, gave me space to say, \"no\", we also knew each other quite well at that point. It didn't help. I pretended everything is ok in front of him, because it is my defense mechanism.&nbsp;<br>Finally, I ghosted the guy and bitched quite a bit about him to my friends. I didn't say he committed a misconduct. But I said he is dangerous, mentioned that I did not like his attention and have trouble being assertive to him, I was also very emotional and had trouble communicating clearly without even realizing that.&nbsp;<br>It <i>thankfully </i>did not end up being a \u201che is a sexual abuser\u201d gossip. My friends did not escalate situation without my clear request and did not twist my words because of their fears or emotions. <i>I\u2019m more than grateful for that</i>.&nbsp;<br>Sometime later I calmed down, I understood what happened and I felt, to be frank, shitty about it. I've talked to my friends again. Still didn\u2019t have a courage to talk to the guy yet - but again, thanks to other people being mature, he did not got a backlash for something he didn't do.&nbsp;<br>&nbsp;</li></ul><p>The situations above, especially the last ones, were usually hard to me, and sometimes not only me. They also required a lot of clear and assertive communication from both sides, and I'm lucky I have friends and partners with whom I can talk this way.<br>I try to do my best to avoid similar situations in the future, I work on myself, learn about my triggers, educate myself and as I'm older, any problems are less frequent.&nbsp;<br><br><strong>Consent is not easy, especially when you are young. Emotional maturity, clear communication, self-awareness and understanding others, being socially not awkward and finally sex and consent themselves are skills to be learned.</strong>&nbsp;<br>It all becomes easier with time - the majority of those stories happened a good couple of years ago.&nbsp;<br>&nbsp;</p><p>Yet, many people, and me also, would not trade if off this (to be frank, still pretty rare) hardships for less sexual experiences.&nbsp;<br>Some of the most beautiful and empowering events in my life are connected to sex, and I will remember some of them till the end of my life. I had sexual situations, significantly more than one, which was emotionally so purifying and intense, that I cried during them. In my case, it often creates a special kind of intimacy between people, and a feeling of safety and belonging. Some of my insecurities and even <i>traumas </i>went away due to positive sexual experiences, and I\u2019m not exaggerating here.&nbsp;</p><p>And to those, who think that those positive experiences can be achieved only in marriages or similar, exclusive and long-lasting relationships - it's not my experience.&nbsp;<br><br>&nbsp;</p><h2>EA openness in talking about things makes it more susceptible to seem weird&nbsp;</h2><p>Yeah, so is all of I\u2019ve written above weird? Am I now weird to you? Are those experiences normal? Do you judge them as healthy?</p><p>Again, it may be my echo chamber, but those experiences are not very special. I\u2019ve heard many people saying similar things. My psychologist did not find any pathology there.</p><p>But, in my culture, they are not very much talked about, or if they are mentioned, it is for sensation, with judgement, without gentleness. I don\u2019t think I\u2019m weird here. I think I\u2019m open and vulnerable. And people are not used to it.</p><p><strong>My theory is that EA is open to discussion about pretty much everything much more than an average crowd, and therefore seems \u201codd\u201d to those who are not used to it.</strong>&nbsp;<br>In my culture, there are things people don\u2019t really talk about and ideas which are \u201cjust thought, not said\u201d. EA people like to break this taboo and share stuff which is more intimate than average. They also state opinions which people usually keep to themselves (maybe share with closest friends). Of course, in some ways EA is not a \u201cperfect population average\u201d. Yet, it\u2019s not as weird as it <i>likes</i> to think. I\u2019ve seen many weirder communities, to be frank.<br>&nbsp;</p><p>Anyway, it\u2019s less important part of this point. More important is as follows:<br><strong>&nbsp;it's ok to be different.&nbsp;</strong><br><br>If you are non-neurotypical, I want you to feel safe and heard.<strong> </strong>I\u2019m happy to adjust my communication on behavior so it works between us, even if it requires stepping out of my comfort zone or active effort (as long as it doesn\u2019t break my boundaries, needles to say).&nbsp;<br><strong>I feel deeply disturbed by amount of ableism which was recently exhibited at this forum (even if it wasn\u2019t direct).&nbsp;</strong><br>I feel that in the recent month, some people put zero effort into understanding what others mean, rather attempted to strawman every single argument which made them feel emotionally insecure. It may have been very heavy for those, who have not-neurotypical communication style, as it may be harder to understand for some and steelmaning would be particularly helpful.&nbsp;<br><br><strong>I believe in a healthy, inclusive society both not-neurotypical and neurotypical people should work together towards better communication, understanding and integration, respecting each other\u2019s boundaries and fulfilling needs.&nbsp;</strong></p><p>&nbsp;</p><p>Ok, let me finish this super long text with saying \u2013 hope you are all safe. Be gentle, first of all to yourselves. Be gentle to others. Love and kisses.&nbsp;</p>", "user": {"username": "Brnr001"}}, {"_id": "wz9EBouojWJEc5GsS", "title": "SVB collapse- cheap startup equity? ", "postedAt": "2023-03-13T04:31:20.822Z", "htmlBody": "<p><a href=\"https://twitter.com/garrytan/status/1634256508254027776?s=20\">https://twitter.com/garrytan/status/1634256508254027776?s=20</a></p><p>This is not investment advice.&nbsp;</p><p>Many startups had all their money in SVB, which appears to be dead. They suddenly have no cash, but still need to pay their employees and meet other obligations.&nbsp;<br><br>This must feel horrible to founders, employees, and their families. They deserve sympathy and support.<br><br>They'd probably also appreciate some cash.&nbsp;</p><p>Seems like whether you choose to put your money in SVB is pretty uncorrelated with how good of a company you are. So, some really awesome companies, previously flush with cash, might be on the brink of shutting down.&nbsp;</p><p>Suppose you were an accredited investor with some cash. You might consider (quickly!) emailing &nbsp;companies who have been kicking ass, asking them if they were effected, and offering to help. You might find they would be very grateful for the chance to sell you some (heavily discounted) equity so they can save their company.&nbsp;<br><br>Amazing companies are still amazing companies. Some might be nearing an exit, have cash coming in soon, or otherwise be really obviously good investments. For some reason the VCs I see on twitter aren't offering to buy at a discount, but instead advising their companies when to prep for layoffs (https://twitter.com/elamadej/status/1634328057711570944?s=20). Is it possible everyone is afraid, and it's time to be greedy?&nbsp;<br><br>This is probably really stupid. But if it isn't, I'd guess that speed matters a whole lot. The best companies are going to sort themselves out pretty fast, and I'd prefer the discount goes to someone reading this instead of a random greedy person</p>", "user": {"username": "Anneal"}}, {"_id": "Gcnkp4qZJDownkLTj", "title": "Two University Group Organizer Opportunities: Pre-EAG London Summit & Summer Internship", "postedAt": "2023-03-13T21:20:22.233Z", "htmlBody": "<h1><strong>Summary</strong></h1><p>CEA\u2019s University Groups Team is excited to announce two new opportunities:</p><ul><li>A&nbsp;<strong>summer internship</strong> for university group organizers<ul><li>Dates: flexible, during the Northern Hemisphere summer</li><li>Application deadline:&nbsp;<strong>Wednesday, March 22</strong></li><li>Find more info &amp; apply<a href=\"https://www.notion.so/centreforeffectivealtruism/CEA-University-Groups-Team-Summer-23-Internship-b70a19daeea24da6acb8822742f7401b\">&nbsp;<u>here</u></a>!</li></ul></li><li>A&nbsp;<strong>university group organizer summit</strong> before EAG London<ul><li>Dates: Monday 15 May \u2013 Friday 19 May</li><li>Application deadline:&nbsp;<strong>Monday, March 27</strong></li><li>Find more info &amp; apply<a href=\"https://www.notion.so/centreforeffectivealtruism/University-Group-Organiser-Summit-d6e1256922614dae8d7678e6eff78a92\">&nbsp;<u>here</u></a>!</li></ul></li></ul><h1><strong>Summer Internship</strong></h1><h2><strong>What?</strong></h2><p>CEA's University Groups Team is running a paid internship program for about 5 university group organizers! During the internship, you will work on a meta-EA project, receiving mentorship and coaching from CEA staff. We have&nbsp;<a href=\"https://www.notion.so/centreforeffectivealtruism/aa640d76d03e44ab93b3028ea8409372?v=543848c1539f469295afa5baf881c044&amp;pvs=4\"><u>a list with a number of project ideas</u></a>, but also encourage you to think about other projects you'd like to run.&nbsp;</p><p>This is your opportunity to think big, and see what it's like to work on meta-EA projects full-time!&nbsp;</p><p><a href=\"https://www.notion.so/centreforeffectivealtruism/CEA-University-Groups-Team-Summer-23-Internship-b70a19daeea24da6acb8822742f7401b?pvs=4\"><strong><u>Initial applications</u></strong></a><strong> are due soon: Wednesday, March 22nd&nbsp;</strong>at 11:59pm<a href=\"https://time.is/Anywhere_on_Earth\">&nbsp;<u>Anywhere on Earth</u></a>.</p><h2><strong>Why?</strong></h2><ul><li>Test out different aspects of meta-EA work as a potential career path</li><li>Receive coaching and mentorship through CEA</li><li>A competitive wage for part-time or full-time work during your break</li><li>Consideration for extended work with CEA</li></ul><h2><strong>For who?</strong></h2><p>You might be a good fit for the internship if you are:</p><ul><li>A university group organizer who is interested in testing out community building and/or EA entrepreneurial projects as a career path</li><li>Highly organized, reliable, and independent</li><li>Knowledgeable of EA and eager to learn more</li></ul><p>Make sure to read more and apply<a href=\"https://www.notion.so/centreforeffectivealtruism/CEA-University-Groups-Team-Summer-23-Internship-b70a19daeea24da6acb8822742f7401b?pvs=4\">&nbsp;<u>here</u></a>!</p><h2><strong>More info</strong></h2><p>If you have any questions, including about whether you'd be a good fit, reach out to Jessica at jessica [dot] mccurdy [at] centreforeffectivealtruism [dot] org.</p><p><strong>Find more info &amp; apply</strong><a href=\"https://www.notion.so/centreforeffectivealtruism/CEA-University-Groups-Team-Summer-23-Internship-b70a19daeea24da6acb8822742f7401b?pvs=4\"><strong>&nbsp;<u>here</u></strong></a><strong>!&nbsp;</strong></p><p><strong>Initial applications are due soon: Wednesday, March 22&nbsp;</strong>at 11:59pm<a href=\"https://time.is/Anywhere_on_Earth\">&nbsp;<u>Anywhere on Earth</u></a>.</p><p>&nbsp;</p><h1><strong>Pre-EAG London University Group Organizer Summit</strong></h1><h2><strong>What?</strong></h2><p>Monday 15 May \u2013 Friday 19 May (before EAG London 2023), the CEA University Groups team is hosting a summit for university group organizers. The summit will kickstart renewed support for experienced university groups and foster better knowledge transfer across groups.</p><p><a href=\"https://www.notion.so/centreforeffectivealtruism/University-Group-Organiser-Summit-d6e1256922614dae8d7678e6eff78a92\"><strong><u>Applications</u></strong></a><strong> are due soon: Monday, March 27&nbsp;</strong>at 11:59pm<a href=\"https://time.is/Anywhere_on_Earth\">&nbsp;<u>Anywhere on Earth</u></a>.</p><h2><strong>Why?</strong></h2><p>The summit has three core goals:</p><ul><li>Boost top university groups by facilitating knowledge transfer among experienced organizers.</li><li>Improve advice for university groups by accumulating examples of effective late-stage group strategies.</li><li>Facilitate connections between experienced organizers and newer organizers, with the hope that attendees will continue to share information and support each other.</li></ul><h2><strong>For who?</strong></h2><p><strong>All current university group organizers can apply for this summit!</strong></p><p>This event will be particularly well-suited for experienced organizers at established university groups. We\u2019re also excited about this summit serving the next generation of organizers at established groups and ambitious organizers at new groups who are eager to think carefully about groups strategy. If you think this summit would plausibly be valuable for you, we encourage you to<strong>&nbsp;</strong>just go ahead and<strong>&nbsp;</strong><a href=\"https://www.notion.so/centreforeffectivealtruism/University-Group-Organiser-Summit-d6e1256922614dae8d7678e6eff78a92\"><strong><u>apply</u></strong></a><strong>!</strong></p><h2><strong>More info</strong></h2><p>If you have any questions, including about whether you'd be a good fit, reach out to us at unigroups [at] centreforeffectivealtruism [dot] org.</p><p><strong>Find more info &amp; apply</strong><a href=\"https://www.notion.so/centreforeffectivealtruism/University-Group-Organiser-Summit-d6e1256922614dae8d7678e6eff78a92?pvs=4\"><strong>&nbsp;<u>here</u></strong></a><strong>!&nbsp;</strong></p><p><strong>Applications are due soon: Monday, March 27th&nbsp;</strong>at 11:59pm<a href=\"https://time.is/Anywhere_on_Earth\">&nbsp;<u>Anywhere on Earth</u></a>.</p><p><br>&nbsp;</p>", "user": {"username": "Joris P"}}, {"_id": "kNeYA6hTrA3Cd9Q2d", "title": "Paper summary: Are we living at the hinge of history? (William MacAskill)", "postedAt": "2023-03-13T18:05:57.427Z", "htmlBody": "<p><i>This is a summary of the GPI Working Paper </i><a href=\"https://globalprioritiesinstitute.org/william-macaskill-are-we-living-at-the-hinge-of-history/\"><i>\u201cAre we living at the hinge of history?\u201d by William MacAskill</i></a><i>. (also published in the 2022 edited volume \u201c</i><a href=\"https://oxford.universitypressscholarship.com/view/10.1093/oso/9780192894250.001.0001/oso-9780192894250-chapter-13\"><i>Ethics and Existence: The Legacy of Derek Parfit</i></a><i>\u201d). The summary was written by Riley Harris.</i></p><p>Longtermist altruists \u2013 who care about how much impact they have, but not about when that impact occurs<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl5bomxqecc\"><sup><a href=\"#fnl5bomxqecc\">[1]</a></sup></span>&nbsp;\u2013 have a strong reason to invest resources before using them directly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefszr5wdxkhhe\"><sup><a href=\"#fnszr5wdxkhhe\">[2]</a></sup></span>&nbsp;Invested resources could grow much larger and be used to do much more good in the future. For example, a $1 investment that grows 5% per year would become $17,000 in 200 years.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs5obqw4gcw\"><sup><a href=\"#fns5obqw4gcw\">[3]</a></sup></span>&nbsp;However, some people argue that we are living in an unusual time, during which our best opportunities to improve the world are much better than they ever will be in the future. If so, perhaps we should spend our resources as soon as possible.&nbsp;</p><p>In \u201cAre we living at the hinge of history?\u201d, William MacAskill investigates whether actions in our current time are likely to be much more influential than other times in the future.&nbsp; (\u2018Influential\u2019 here refers specifically to how much good we expect to do via direct monetary expenditure \u2013 the consideration most relevant to our altruistic decision to spend now or later.) After making this \u2018hinge of history\u2019 claim more precise, MacAskill gives two main arguments against the claim: the base rate and inductive arguments. He then discusses some reasons why our time might be unusual, but ultimately concludes that he does not think that the \u2018hinge of history\u2019 claim holds true.</p><h2><strong>The base rate argument</strong></h2><p>When we think about the entire future of humanity, we expect there to be a lot of people, and so we should initially be very sceptical that anyone alive today will be amongst the most influential human beings. Indeed, if humanity doesn\u2019t go extinct in the near future, there could be a vast number of future people \u2013 settling near just 0.1% of stars in the Milky Way with the same population as Earth would mean there were 10<sup>24</sup> (a trillion trillion) people to come. Suppose that, before inspecting further evidence, we believe that we are about as likely as anyone else to be particularly influential. Then, our initial belief that anyone alive today is amongst the million most influential people would be 1 in 10<sup>18</sup> (1 in a million trillion).&nbsp;</p><p>From such a sceptical starting point, we would need extremely strong evidence to become convinced that we are presently in the most influential time era. Even if there were only 10<sup>8</sup> (one hundred million) people to come, then in order to move from this extremely sceptical position (1 in 10<sup>8</sup>) to a more moderate position (1 in 10), we would need evidence about 3 million times as strong as a randomised control trial with a p-value of 0.05.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpl7jbbgumlj\"><sup><a href=\"#fnpl7jbbgumlj\">[4]</a></sup></span><sup>&nbsp; </sup>MacAskill thinks that, although we do have some evidence that indicates we may be at the most<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7881tj4ztwj\"><sup><a href=\"#fn7881tj4ztwj\">[5]</a></sup></span>&nbsp;influential time, this evidence is not nearly strong enough.</p><h2><strong>The inductive argument</strong></h2><p>There is another strong reason to think our time is not the most influential, MacAskill argues:</p><p>Premise 1: Influentialness has been increasing over time.</p><p>Premise 2: We should expect this trend to continue.</p><p>Conclusion: We should expect the influentialness of people in the future to be greater than our own influentialness.</p><p>Premise 1 can be best illustrated with an example: a well-educated and wealthy altruist living in Europe in 1600 would not have been in a position to know about the best opportunities to shape the long-run future. In particular, most of the existential risks they faced (e.g. an asteroid collision or supervolcano) were not known, nor would they have been in a good position to do anything about them even if they were known. Even if they had the scientific knowledge that we have, they might have used it to pursue a worse moral view.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2vlf7y8tknz\"><sup><a href=\"#fn2vlf7y8tknz\">[6]</a></sup></span>&nbsp;Indeed, it is likely that future generations will discover ways in which we are misguided, both morally and scientifically. If we are mistaken enough, our (well-intentioned) present actions could actually be doing harm. Premise 2 indicates that we should expect the trend of improvement to continue. This is especially plausible because we can identify gaps in our scientific, technological and moral understanding. Overall, this argument indicates that we should expect future generations to be more influential than we are.</p><h2><strong>Reasons why our time might be unusual</strong></h2><p>MacAskill also discusses several reasons one might think that our time is unusual, and therefore may be unusually influential. Our time is unusual because we currently live on a single planet, while most people who will ever live will likely (in expectation) be part of an interplanetary civilisation.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefffff1pxy69t\"><sup><a href=\"#fnffff1pxy69t\">[7]</a></sup></span>&nbsp;We also live at a time of extreme technological progress which cannot continue indefinitely: our current economic growth rate is around 3.5%, but 2% annual growth over the next 10,000 years would result in an economic output of 10<sup>19</sup> (ten million trillion) times the current world GDP <i>for every atom in the galaxy</i>.</p><p>There are three important ways in which this could make our time unusually influential:&nbsp;</p><ol><li>Our single planet is a single point of failure, which may make the risk of extinction temporarily higher than usual.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnq42cn0ry3b\"><sup><a href=\"#fnnq42cn0ry3b\">[8]</a></sup></span></li><li>While we live on a single planet, the most influential people today may have an unusual ability to influence humanity as a whole \u2013 both because they can communicate near instantaneously with almost everyone and because their resources are a relatively large fraction of the total. If humanity becomes a much larger space-faring civilisation, these will likely both change.&nbsp;&nbsp;</li><li>Plausibly, the fate of the future will be decided by how we handle some particular technology (such as artificial intelligence or particularly dangerous new weapons) and we are more likely to discover such a technology during a period of rapid growth.<sup>&nbsp;</sup><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftjygwxp1piq\"><sup><a href=\"#fntjygwxp1piq\">[9]</a></sup></span>&nbsp;</li></ol><p>However, each of these arguments has important caveats. In relation to the first argument, most people who are worried about existential risk believe that a large part of the risk comes from misaligned artificial intelligence,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref31kofvg2rt7\"><sup><a href=\"#fn31kofvg2rt7\">[10]</a></sup></span>&nbsp;and this would not be significantly reduced by planetary diversification. In relation to the second argument, this period of unusual influence may be prolonged if our civilisation stays earthbound for thousands of years or it just takes longer than we expect to leave the solar system. (It only takes one hour for light to traverse the full diameter of the asteroid belt, so the ability of the most influential people to influence humanity as a whole may remain high for quite some time.) In relation to the third argument, perhaps this period of remarkable economic growth will last longer than most anticipate. Even if this period is short, one could argue that longtermists will be less influential during periods of high economic growth, because the unpredictability of a rapidly changing environment hinders the execution of very long-term projects. Overall, MacAskill thinks that these arguments provide evidence that our time may be the most influential. However, the base rate and inductive arguments show that we should be extremely sceptical that we live at the most important time \u2013 and the evidence presented in this section does not seem strong enough to overcome these arguments.</p><p>Overall, we probably do not live at the \u2018hinge of history\u2019. If we did, this would give us a powerful reason to spend now rather than investing to have a much larger impact later. Instead, the case for investment remains strong.</p><h2><strong>References</strong></h2><p>Daniel Benjamin <i>et al. </i>(2018) <a href=\"https://www.nature.com/articles/s41562-017-0189-z\">Redefine statistical significance.</a> <i>Nature Human Behaviour. </i>2.&nbsp;</p><p>Nick Bostrom (2014). <a href=\"https://global.oup.com/academic/product/superintelligence-9780199678112\"><i>Superintelligence: Path, Dangers, Strategies.</i></a> Oxford University Press.</p><p>Hilary Greaves and William MacAskill (2021). <a href=\"https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/\">The case for strong longtermism</a><i>. GPI Working Paper No. 5-2021.</i></p><p>William MacAskill (2022). <a href=\"https://oxford.universitypressscholarship.com/view/10.1093/oso/9780192894250.001.0001/oso-9780192894250-chapter-13\">Are we living at the hinge of history?</a> <i>Ethics and Existence: The Legacy of Derek Parfit.</i> Oxford University Press. Edited by Jeff McMahan, Tim Campbell, James Goodrich, and Ketan Ramakrishnan.</p><p>William MacAskill (2019). two-thirds<a href=\"https://globalprioritiesinstitute.org/william-macaskill-when-should-an-effective-altruist-donate/\">When should an effective altruist donate?</a> <i>GPI Working Paper No. 8-2019.</i></p><p>Toby Ord (2020).<a href=\"https://www.bloomsbury.com/uk/precipice-9781526600219/\"> <i>The Precipice: Existential Risk and the Future of Humanity</i></a>. Bloomsbury Publishing.</p><p>Carl Sagan (1994). <a href=\"http://www.randomhousebooks.com/books/159735/\"><i>Pale Blue Dot: A Vision of the Human Future in Space.</i></a> Random House.</p><p>Philip Trammell (2021). <a href=\"https://globalprioritiesinstitute.org/dynamic-public-good-provision-under-time-preference-heterogeneity-theory-and-applications-to-philanthropy-philip-trammell-global-priorities-institute-and-department-of-economics-university-of-oxford/\">Dynamic public good provision under time preference heterogeneity: theory and applications to philanthropy</a>. <i>GPI Working Paper No. 9-2021.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl5bomxqecc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl5bomxqecc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See Greaves and MacAskill (2021), or the <a href=\"https://globalprioritiesinstitute.org/summary-the-case-for-strong-longtermism/\">summary of their paper</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnszr5wdxkhhe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefszr5wdxkhhe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See MacAskill (2019) and Trammell (2021).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns5obqw4gcw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs5obqw4gcw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Of course, <i>inflation </i>decreases what you can buy with the same sum in the future, but here we are talking about <i>real </i>returns (which account for inflation), so you could buy what $17,000 would buy today.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpl7jbbgumlj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpl7jbbgumlj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here the \u2018Bayes\u2019 factor is used as a measure of the strength of a piece of evidence. It is an exact mathematical denotation of how much rational beliefs should change in response to that evidence. The Bayes factor required to move from 1 in 100 million to 1 in 10 would be 10 million (because 1/10=10 million/100 million). Under plausible assumptions, the Bayes factor of a randomised controlled trial with a p-value of 0.05 is approximately 3 (Benjamin et al, 2018, p. 7), so we would need about 3 million times as much evidence.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7881tj4ztwj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7881tj4ztwj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One might try to defend a more modest position, instead claiming that this is just one enormously influential time (rather than the most influential), or that it is only the most influential relative to times we can plausibly pass resources to (the next thousand years or so). Indeed, these claims would require less strong evidence to defend, but we also have less evidence to defend them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2vlf7y8tknz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2vlf7y8tknz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>They would have likely believed that non-male, non-white or non-Christian people were less valuable, that strong social hierarchy and slavery were natural and that homosexuality and premarital sex were deeply immoral.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnffff1pxy69t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefffff1pxy69t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Even if our prospects for becoming an interplanetary civilisation were low, most future people would be part of one (in expectation). This is because an interplanetary civilisation could be very large \u2013 there could be many planets with the population of Earth, and they could sustain life much longer.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnq42cn0ry3b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnq42cn0ry3b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See Sagan (1994) and Ord (2020).&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntjygwxp1piq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftjygwxp1piq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See Bostrom (2014) and Ord (2020).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn31kofvg2rt7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref31kofvg2rt7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Toby Ord (2020) estimates that two thirds of the total risk this century comes from misaligned AI.</p></div></li></ol>", "user": {"username": "Global Priorities Institute"}}, {"_id": "HWKqmTLcbsf4F5xAk", "title": "AI safety and consciousness research: A brainstorm", "postedAt": "2023-03-15T14:33:42.240Z", "htmlBody": "<p>I've been collecting people's thoughts on the potential of consciousness research to advance AI safety. Here are some rough answers I've come across:</p><h1><strong>1. Understanding ethics / human values</strong></h1><p>These approaches argue understanding consciousness could help us understand what we mean by a human-aligned AI (the <a href=\"https://www.lesswrong.com/posts/FP8T6rdZ3ohXxJRto/superintelligence-20-the-value-loading-problem\">value-loading problem</a>).</p><p>However, this approach runs into problems once we start asking what exactly about our values we should specify, and what we should leave up to the superintelligence\u2019s own closer examination. Bostrom lays out a solid argument for trusting an aligned AGI\u2019s judgement over our own through the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/principle-of-epistemic-deference\"><u>principle of epistemic deference</u></a>: Assuming an AGI has been programmed to reason as a perfectly open-minded and well-meaning human who thought about moral questions longer and deeper than the best philosophers, we should assume that in the case of a disagreement with such an AI, it\u2019s more likely that the AI is right, as it can presumably encompass all of our own ideas but also some other ones. This leads us to&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/indirect-normativity\"><u>indirect normativity&nbsp;</u></a>- the idea that similarly to laws, the rules that we encode in an aligned AI should be rather vague, so that it can correct for their problems upon closer examination.</p><p>These considerations suggest that advancing our understanding of ethics / human values to the literal molecular level wouldn\u2019t really be helpful, as we should avoid locking in any of our specific present notions of values. Here are several answers to this argument:</p><ol><li>Even if we accept that in the case of a disagreement, the formulations of our values should be flexible for revision, having a better model of human values might increase the chances we specify them correctly. For instance, the prompt that we give to an AGI could involve this model as what we want to extrapolate (because that\u2019s \u201chow far we\u2019ve got on our own\"). Indirect normativity posits that our prompt should be minimal but perhaps we can form a superior minimal prompt based on a more advanced model of human values.&nbsp;<ol><li>It seems intuitively likely there\u2019s a lower risk of mesa-optimisation misalignment if there are fewer cognitive steps between the values specified in the prompt and the values we would want if optimally extrapolated. For example, an AI optimizing for \u201cthe human concept of good\u201d could simulate the extrapolated values of the average human and become a religious fundamentalist. However an AI optimizing for \u201cpositive freedom to choose the best qualia\u201d might be motivated to anchor its values in the best model of the interpretation of qualia &amp; preferences it can come up with.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref90m13kansvj\"><sup><a href=\"#fn90m13kansvj\">[1]</a></sup></span></li></ol></li><li>It might be that we won\u2019t be sure if we\u2019ve solved the control problem. If there\u2019s a tense race between an almost certainly aligned AI and an almost certainly misaligned AI, locking in slightly suboptimal values might be the better option. Additionally, if we are more certain about the validity of our value theories - or if we at least develop a better framework for researching the value dimension of consciousness - we are also better prepared for a situation where we would be forced to choose between several sub-optimal AIs.</li><li>The need for indirect normativity should probably be treated as a heuristic, rather than a logical law. It seems possible that research which would clarify how values are best represented might also find that the balance between not specifying anything and specifying too much doesn\u2019t lie where we would intuitively think it does.</li><li>If we have a good model of what we care about, we have a mechanism how to check whether an AI is truly aligned or whether it\u2019s wrong or trying to manipulate us.<ol><li>Counterargument: An AI that produces good answers to ethical questions is no guarantee of alignment. So avoiding a catastrophe means solving the control problem anyway, part of which will be being able to ask the AI to explain its reasoning so transparently, it will be clear to us it\u2019s doing what we want.<ol><li>What do we do if the AI comes up with something very counter-intuitive (tile the universe with&nbsp;<a href=\"https://www.lesswrong.com/tag/orgasmium\"><u>hedonium</u></a>)? How do we check if the AI extrapolated our values correctly if its philosophy seems impossible to comprehend? We either need to understand what exactly we mean by \u201ccorrect extrapolation\u201d or what exactly our values are. Since the thing we want to extrapolate is likely deeply connected to optimising the contents of our consciousness, it seems that consciousness research could be useful for both of these options.</li><li>The same problem applies if the AI comes up with something intuitive. We might think we\u2019ve got an aligned AI but it could just be that we\u2019ve made an AI that replicates the mistakes of our moral intuitions. In other words, it could be that we make a misaligned AI just because we can\u2019t see how it\u2019s misaligned until we make progress in human values.</li><li>If there\u2019s a tense race between an almost certainly aligned AI&nbsp; and an almost certainly misaligned AI, we may not have enough time to try to integrate something very counter-intuitive into our definition of what the AI is supposed to achieve - whether by accepting it or explicitly forbidding it in the task prompt - unless there\u2019s already a body of advanced consciousness research by the time AGI arrives.</li><li>Another possibility is that we develop an AI that\u2019s almost certainly aligned but not transparent. A typical powerful AI we imagine is very general, so it may seem weird that it would be just bad at \u201creading its own mind\u201d but it\u2019s possible that analyzing architecture such as a neural network takes more computing power than the network itself can provide. In this case, it sounds likely that such an AI couldn\u2019t even tell whether creating an analyzer AI powerful enough to analyze this semantic AI would be safe. In this scenario, consciousness research would help as a check for alignment.</li></ol></li></ol></li><li>Consciousness or ethics could be an area where AI can\u2019t make progress because it lacks some information we get from consciousness. However one could object that it seems weird to think a superintelligence would just miss that consciousness is central for how we think about ethics. And if it wouldn\u2019t miss that, it could employ humans to fill in the necessary missing bits. Some answers:<ol><li>The fact there seems to be an impenetrable barrier between the kind of information we can describe with physics and the kind that manifests in our conscious experience could lead to the counter-intuitive conclusion that even a superintelligence might miss something crucial about the human experience - since it's information qualitatively different from anything it can access, it might not even know it misses up on it.</li><li>In other words,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Knowledge_argument\"><u>Mary</u></a> could be infinitely intelligent and still not get what we mean by red. But what\u2019s worse, her infinite intelligence could make her feel convinced there\u2019s nothing to miss. It seems Mary would most naturally tend to think color itself is a meaningless concept. It seems that the most natural philosophy for a consciousness-lacking AGI would be <a href=\"https://en.wikipedia.org/wiki/Illusionism_(consciousness)\">illusionism </a>and the related position that ethics is a meaningless concept.</li><li>One pathway towards AGI that currently seems quite likely is an AI that simulates a human (~a LLM). Sure, it's possible that if a simulated human lacked inner experience, they would be able to report that. However, it's hard to say because there is no learning data for this situation, as there don't seem to be such humans. Everyone behaves the way a philosophical zombie would - with the exception of being interested in consciousness.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsz6q7h04ibi\"><sup><a href=\"#fnsz6q7h04ibi\">[2]</a></sup></span>&nbsp;However, a well simulated human would act as if they're interested in consciousness and as if they understand it. This could lead to the AI latching onto a wrong proxy model of consciousness such as \u201cit\u2019s when people report on their own neural algorithms\u201d.</li></ol></li><li>Improving (meta-)ethics might help create a better social environment for approaching AI safety with more reasonable assumptions.<ol><li>It might be that actors, whether governments or AI developers, would want to lock in certain values but don\u2019t realize they are instrumental, rather than terminal. For instance, the imperative not to think in stereotypes that has been programmed into ChatGPT has had the&nbsp;<a href=\"https://www.facebook.com/vlastimil.vohanka/posts/pfbid0XhYh92BdvTZKQady1ftftBJgiozPwp6SNNaRqSfe5zi66C9sDg9CF8ciKWcFNz5kl\"><u>unintended consequence</u></a> that its reasoning about statistics seems contradictory. However, setting specific values in stone, instead of letting the AGI extrapolate the principles behind them could be exactly what leads to optimising a wrong proxy value, leading to an x- or s-risk. This could be mitigated by improving meta-ethics perhaps in a style similar to the work of&nbsp;<a href=\"https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism/\"><u>Sharon H. Rawlette</u></a> - by clarifying the delineation between biases and values. For instance, this could allow some actors to realize some biases in their naive moral intuitions they might wish to lock in otherwise.</li><li>Advancing philosophy improves the learning data. If reasonable ethics become main-stream among philosophers, there\u2019s a higher chance they get adopted by an AGI.</li></ol></li></ol><h1>&nbsp;<strong>2. Advancing alignment research</strong></h1><ol><li>Assuming humans are more or less aligned, understanding how we do it might be useful for AI alignment.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdj6l1u83nrh\"><sup><a href=\"#fndj6l1u83nrh\">[3]</a></sup></span><ol><li>Although this idea most naturally leads to studying human information processing without the need to see how it relates to qualia, I think there\u2019s a good chance the consciousness frame can enrich these areas. For example, unless we understand consciousness, we might miss a crucial part of what \u201crepresentations in the human brain / cognitive models\u201d mean.</li><li>The alignment project could be framed as a race between developers of general AI capabilities and capabilities useful for alignment such as moral reasoning where the phenomenological basis of human values&nbsp;<a href=\"https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism/\"><u>could</u></a> play a special role.</li></ol></li><li>The&nbsp;<a href=\"https://www.pibbss.ai/\"><u>PIBBSS</u></a> framing: Deconfusing ourselves about the basic philosophical underpinnings of intelligence, goals/motivation or cognitive processing might be a good way to find out what is there to think about.<ol><li>This could involve consciousness, since we clearly seem to be especially confused about it: Subjectively, it seems like a force that determines everything in the brain. Since we talk about it, we know it has causal properties. Yet, from the outside view, it seems all other physical phenomena can be predicted without understanding this apparent force.</li></ol></li><li>Some people claim understanding consciousness would lead to a better understanding of seemingly chaotic behaviors of intelligent physical systems. A truly&nbsp;<i>provably beneficial AI </i>requires being able to predict an AI's behavior down to the molecular level and consciousness is a real phenomenon physics yet can't explain, suggesting current physics can't guarantee that yet unseen systems like an ASI would not display emergent phenomena that change the original physical architecture.<ol><li>This is the approach <a href=\"https://qri.org/\">QRI</a> could advocate, suggesting that if we built a system which has experiences, it could adopt <a href=\"https://en.wikipedia.org/wiki/Open_individualism\">open individualism</a> (the theory of self which encompasses everything conscious) and in result, be more likely to understand &amp; value what we value.</li><li>Similar approaches require the belief that consciousness is a physical phenomenon with predictable causal power. In contrast, some people might argue that consciousness influences the world indeterministically through something akin to free will (inspired by <a href=\"https://forum.effectivealtruism.org/posts/emrZS5RYyAQs229sB/practical-ethics-requires-metaphysical-free-will\">Ansgar Kamratowski</a>).<ol><li>Counterargument: Theoretical indeterministic effects would by definition need to be impossible to predict, fulfilling the Bayesian definition of randomness. Their magnitude would probably be confined to quantum effects and they would be just as likely to make a good AI go wrong, as a bad AI go right. Random effects can be described as \"statistically deterministic\" and we can treat them as physical laws (more detailed explanation in <a href=\"https://docs.google.com/document/d/1X7Ve_VLl22mZpvaG-J_7arBxGNFumSGcUxkTcUqV850/edit\">this document</a>). Nevertheless, the hypothesis that biological intelligence <a href=\"https://doi.org/10.1116/1.5135170\">utilizes</a> poorly understood physical laws could be an important consideration for alignment.</li></ol></li></ol></li></ol><h1><strong>3. Meta reasons</strong></h1><p>Both the field of consciousness and the field of AI safety are full of uncertainties and seem high-risk high-reward in nature. This means that even if smart people make good arguments against these reasons to pursue consciousness research, it might be beneficial to diversify our endeavors, as making sure we understand human values seems robustly good.</p><ol><li>It's possible that these uncertainties create biases even in our basic framing of the problem of \"aligning AI with human values\". For instance, the possibility that identity, preferences or moral intuitions relate to some fundamental phenomena in consciousness could imply a different approach to programming the AI's \"constitution\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvx6zhx3t1k\"><sup><a href=\"#fnvx6zhx3t1k\">[4]</a></sup></span>.</li><li>Similarly, the possibility there's something fundamental about moral intuitions might require a better grasp on which elements of sentience give rise to a moral agent, i. e. whose moral intuitions we care about. Or perhaps, as some illusionists may suggest, our intuitions about what \"perceiving X as valuable\" means may be misguided.</li></ol><h1><strong>Epistemic status: A question</strong></h1><p>I\u2019ve been trying to get a grasp on whether the possibility to make progress in consciousness research is under- or overrated for a few years now with no answer.</p><p>On one hand,</p><ul><li>it\u2019s a problem that has fascinated a lot of people for a long time</li><li>if a field is attractive, we can expect a lot of non-altruistically motivated people to work on it, more so if the field seems to grow (<a href=\"https://www.proquest.com/scholarly-journals/literature-review-bibliometric-analysis-mind/docview/2703039855/se-2?accountid=16531\"><u>1</u></a>;&nbsp;<a href=\"https://www.mdpi.com/2076-3425/10/1/41\"><u>2</u></a>)</li></ul><p>On the other hand</p><ul><li>neuroimaging has just become a thing - do we expect it to solve a millenia-old problem right away?<ul><li>I consider philosophy to be a task especially unsuitable to the human brain, so I wouldn\u2019t defer to the previous generations of philosophers, just like I wouldn\u2019t defer to them on the ethics of slavery. The ideas of evolution, psychology or effective altruism emerged much later than they could\u2019ve because in <a href=\"https://twitter.com/hominidan/status/1580179231631237120\">my opinion</a>, people underestimate how much is their idea generation confined to the borders of \u201cwhat\u2019s there to think about\u201d. And the age of computers has <a href=\"https://en.wikipedia.org/wiki/Cognitive_science\">opened up</a> cognitive science as \"a thing to think about\" only quite recently, by the standards of philosophy (the term \u201c<a href=\"https://en.wikipedia.org/wiki/Hard_problem_of_consciousness\">hard problem of consciousness</a>\u201d is just 2 decades old).</li></ul></li><li>if a field is growing, it could be an opportunity to help direct a significant mental &amp; economic potential into worthwhile efforts<ul><li>and also an indication there is some progress to be made</li></ul></li></ul><p>There are definitely unproductive ways to research consciousness. Currently, the \u201cpixels\u201d of advanced functional neuroimaging&nbsp;<a href=\"https://elifesciences.org/articles/75171.pdf\"><u>consist</u></a> of ~1 million neurons. This leads to a lot of research about neural correlates concluding with fuzzy inferences like \u201cthe right hemisphere lights up more\u201d. On the opposite side of the spectrum lie philosophical papers which try to&nbsp;<i>explain consciousness</i> in tautologies. I think the hard problem of consciousness is a very legitimate one, however one that dissolves into the unstudiable question of \u201cwhy is there anything at all\u201d once we embrace a frame like <a href=\"https://en.wikipedia.org/wiki/Objective_idealism\">objective idealism</a> and understand how precisely each quale corresponds to each computational phenomenon.</p><p>However, I also believe there are productive big questions like \u201cHow many senses are there &amp; what exactly are they? - i. e. Which phenomena do qualia reflect and how do these phenomena feed into our informational processing? Can we rely on how they reflect what we value or can our intuitions about what we value&nbsp;<a href=\"https://astralcodexten.substack.com/p/can-people-be-honestly-wrong-about\"><u>be wrong</u></a>? Is there a reliable description of value such as intensity times valence? Or is the perceived value of an experience&nbsp;<a href=\"https://www.researchgate.net/publication/343792104_Dimensions_of_Animal_Consciousness\"><u>dependent</u></a> on integrating information across time and modalities or emotional richness? What is a <a href=\"https://forum.effectivealtruism.org/posts/bvtAXefTDQgHxc9BR/just-look-at-the-thing-how-the-science-of-consciousness\">net-positive experience</a>? Which cognitive phenomena <a href=\"https://philpapers.org/archive/CHATMO-32.pdf\">allow us</a> to access the information that we are conscious?\u201d - which seem fundamental to prioritization/ethics, mental health and perhaps the universe.</p><h3><strong>Related articles</strong></h3><ul><li>Kaj Sotala: <a href=\"https://forum.effectivealtruism.org/posts/WdMnmmqqiP5zCtSfv/cognitive-science-psychology-as-a-neglected-approach-to-ai\">Cognitive Science/Psychology As a Neglected Approach to AI Safety</a></li><li>Cameron Berg: <a href=\"https://www.alignmentforum.org/posts/ZJY3eotLdfBPCLP3z/theoretical-neuroscience-for-alignment-theory\">Theoretical Neuroscience For Alignment Theory</a></li><li>Paul Christiano: <a href=\"https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard\">The easy goal inference problem is still hard</a></li><li>Robin Shah: <a href=\"https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc\">Value Learning sequence</a></li></ul><p><i>Special thanks to Jan Votava, Max R\u00e4uker, Andr\u00e9s G\u00f3mez Emilsson, Aatu Koskensilta and an anonymous person for their inspiration &amp; notes!</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn90m13kansvj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref90m13kansvj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What I propose here is that the risk of inner misalignment is decreased if we have a good idea about the values we want to specify (outer alignment) because it reduces the danger of misinterpretation of the values (reward function) we specified. The non-triviality of this problem is nicely explained in Bostrom\u2019s Superintelligence, chapter Morality models.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsz6q7h04ibi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsz6q7h04ibi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This could be a test for digital consciousness - if we manage to delete the concept of consciousness from the learning data somewhat, does it naturally emerge, just like it has emerged in various cultures?</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndj6l1u83nrh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdj6l1u83nrh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By \u201cmore or less aligned\u201d I express something like: The behavior of some humans is guided by moral principles enough so that an AI that simulated their coherent extrapolated volition would seek behavior that corresponds to the most honest, well-thought and well-meaning interpretation of ethics it can come up with.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvx6zhx3t1k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvx6zhx3t1k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By \"constitution\" I mean \"an algorithm determining how to determine what's morally correct in a given moment\", a concept linked closely to meta-ethics. Under some views, this would also involve \"rules that lay out the relationship between AI and other potentially conscious entities\".</p></div></li></ol>", "user": {"username": "Daniel_Friedrich"}}]