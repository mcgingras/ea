[{"_id": "9EY5iQoQNBhesspuE", "postedAt": "2017-07-20T21:54:22.363Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>This was great and I really enjoyed reading it. It's a pleasure to see one EA disagreeing with another with such eloquence, kindness and depth.</p>\n<p>What I would say is that, even as someone doing a PhD in Philosophy, I found a bunch of this hard to follow (I don't really do any work on consciousness), particularly objection 7 and when you introduced QRI's own approach. I'll entirely understand if you think making this more accessible is more trouble that it's worth, I just thought I'd let you know.</p>\n", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "WtErgKDea3zSHM7v8", "postedAt": "2017-07-20T22:17:07.848Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Re: 2, I don't see how we should expect functionalism to resolve disputes over which agents are conscious. Panpsychism does not such thing, nor does physicalism or dualism or any other theory of mind. Any of these theories can inform inquiry about which agents are conscious, in tandem with empirical work, but the connection is tenuous and it seems to me that at least 70% of the work is empirical. Theory of mind mostly gives a theoretical basis for empirical work.</p>\n<p>The problem lies more with the specific anti-realist account of sentience that some people at FRI have, which basically boils down to &quot;it's morally relevant suffering if I think it's morally relevant suffering.&quot; I suspect that a good functionalist framework need not involve this.</p>\n<blockquote>\n<p>&quot;But it seems a stretch to say that the alleged tension is problematic when talking about tables. So why would it be problematic when talking about suffering?&quot;</p>\n</blockquote>\n<p>Actually I think the tension would be problematic if we had philosophical debates about tables and edge cases which may or may not be tables.</p>\n", "parentCommentId": null, "user": {"username": "kbog"}}, {"_id": "QGttXdGqBpaGj3GPg", "postedAt": "2017-07-20T22:22:11.280Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Thanks Michael!</p>\n<p>Re: Objection 7, I think Aaronson's point is that, if we actually take seriously the idea that a computer / Turing machine could generate consciousness simply by running the right computer code, we should be prepared for a lot of very, very weird implications.</p>\n<p>Re: QRI's approach, yeah I was trying to balance bringing up my work, vs not derailing the focus of the critique. I probably should have spent more words on that (I may go back and edit it).</p>\n", "parentCommentId": "9EY5iQoQNBhesspuE", "user": {"username": "MikeJohnson"}}, {"_id": "wG7zyuk8CrtitCmxT", "postedAt": "2017-07-20T22:31:38.767Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>Re: 2, I don't see how we should expect functionalism to resolve disputes over which agents are conscious. </p>\n</blockquote>\n<p>I think analytic functionalism is internally consistent on whether agents are conscious, as is the realist panpsychism approach, and so on. The problem comes in, as you note, when we want to be anti-realist about consciousness yet also care about suffering.</p>\n<blockquote>\n<p>it seems to me that at least 70% of the work is empirical. Theory of mind mostly gives a theoretical basis for empirical work.</p>\n</blockquote>\n<p>In practice, it may be difficult to cleanly distinguish between theoretical work on consciousness, and empirical work on consciousness. At least, we may need to be very careful in how we're defining &quot;consciousness&quot;, &quot;empirical&quot;, etc.</p>\n<blockquote>\n<p>The problem lies more with the specific anti-realist account of sentience that some people at FRI have, which basically boils down to &quot;it's morally relevant suffering if I think it's morally relevant suffering.&quot; I suspect that a good functionalist framework need not involve this.</p>\n</blockquote>\n<p>It's an open question whether this is possible under functionalism-- my argument is that it's not possible to find a functionalist framework which has a clear or privileged definition of what morally relevant suffering is.</p>\n", "parentCommentId": "WtErgKDea3zSHM7v8", "user": {"username": "MikeJohnson"}}, {"_id": "BpFEaCb3vNh9wkjKW", "postedAt": "2017-07-20T23:10:53.756Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>This looks sensible to me. I'd just quickly note that I'm not sure if it's quite accurate to describe this as &quot;FRI's metaphysics&quot;, exactly - I work for FRI, but haven't been sold on the metaphysics that you're criticizing. In particular, I find myself skeptical of the premise &quot;suffering is impossible to define objectively&quot;, which you largely focus on. (Though part of this may be simply because I haven't yet properly read/considered Brian's argument for it, so it's <em>possible</em> that I would change my mind about that.)</p>\n<p>But in any case, I've currently got three papers in various stages of review, submission or preparation (that other FRI people have helped me with), and none of those papers presuppose this specific brand of metaphysics. There's a bunch of other work being done, too, which I know of and which I don't think presupposes it. So it doesn't feel quite accurate to me to suggest that the metaphysics would be holding back our progress, though of course there can be <em>some</em> research being carried out that's explicitly committed to this particular metaphysics.</p>\n<p>(opinions in this comment purely mine, not an official FRI statement etc.)</p>\n", "parentCommentId": null, "user": {"username": "Kaj_Sotala"}}, {"_id": "P2npBe5f9dKejtamd", "postedAt": "2017-07-21T08:41:12.364Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>that precisely mapping between physical processes and (Turing-level) computational processes is inherently impossible</p>\n</blockquote>\n<p>Curious for your take on the premise that ontologies always have tacit telos.</p>\n<p>Also, we desire to expand the domains of our perception with scientific instrumentation and abstractions. This expansion always generates some mapping (ontology) from the new data to our existing sensory modalities. </p>\n<p>I think this is relevant for the dissonance model of suffering, though I can't fully articulate how yet.</p>\n", "parentCommentId": null, "user": {"username": "RomeoStevens"}}, {"_id": "zj7QBhYSwmDwD92xv", "postedAt": "2017-07-21T08:50:04.806Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Aside:</p>\n<blockquote>\n<p>Essentially, the STV is an argument that much of the apparent complexity of emotional valence is evolutionarily contingent, and if we consider a mathematical object isomorphic to a phenomenological experience, the mathematical property which corresponds to how pleasant it is to be that experience is the object\u2019s symmetry.</p>\n</blockquote>\n<p>I don't see how this can work given (I think) isomorphism is transitive and there are lots of isomorphisms between sets of mathematical objects which will not preserve symmetry. </p>\n<p>Toy example. Say we can map the set of all phenomenological states (P) onto 2D shapes (S), and we hypothesize their valence corresponds to their symmetry along the y=0 plane. Now suppose an arbitrary shear transformation applied to every member of S, giving S!. P (we grant) is isomorphic to S. Yet S! is isomorphic to S, and therefore also isomorphic to P; and the members of S and S! which are symmetrical differ. So which set of shapes should we use?</p>\n", "parentCommentId": null, "user": {"username": "Gregory_Lewis"}}, {"_id": "H3q6iLGJRnffWX8af", "postedAt": "2017-07-21T08:57:37.469Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Brian's view is maybe best described as eliminativism about consciousness (which may already seem counterintuitive to many) <em>plus</em> a counterintuitive way to draw boundaries in concept space. \nLuke Muehlhauser said about Brian's way of assigning non-zero moral relevance to any process that remotely resembles aspects of our concept of consciousness:</p>\n<p> &quot;Mr. Tomasik\u2019s view [...] amounts to pansychism about consciousness as an uninformative special case of \u201cpan-everythingism about everything.&quot; </p>\n<p>See this <a href=\"http://www.openphilanthropy.org/brian-tomasik-research-lead-foundational-research-institute-october-6-2016\">conversation</a>.</p>\n<p>So the disagreement there does not appear to be about questions such as &quot;What produces people's impression of there being a hard problem of consciousness?,&quot; but rather whether anything that is &quot;non-infinitely separated in multi-dimensional concept space&quot; still deserves <em>some</em>  (tiny) recognition as fitting into the definition. As Luke says <a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#HowToRead\">here</a>, the concept &quot;consciousness&quot; works more like &quot;life&quot; (= fuzzy) and less like &quot;water&quot; (= H2O), and so if one shares this view, it becomes non-trivial to come up with an all-encompassing definition. </p>\n<p>While most (? my impression anyway as someone who works there) researchers at FRI place highest credence on functionalism and eliminativism, there is more skepticism about Brian's inclination to never draw hard boundaries in concept space. </p>\n", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "2SKaiBLfNkbQqRL5n", "postedAt": "2017-07-21T11:55:26.092Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Trivial objection, but the y=0 axis also gets transformed so the symmetries are preserved. In maths, symmetries aren't usually thought of as depending on some specific axis. E.g. the symmetry group of a cube is the same as the symmetry group of a rotated version of the cube.</p>\n", "parentCommentId": "zj7QBhYSwmDwD92xv", "user": null}, {"_id": "4ekmBpymqcdWP46tu", "postedAt": "2017-07-21T12:11:10.928Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>What's the problem if a group of people explores the implications of a well-respected position in philosophy and are (I think) fully aware of the implications? Exploring a different position should be a task for people who actually place more than a tiny bit of credence in it, it seems to me - especially when it comes to a new and speculative hypothesis like principle qualia.</p>\n<p>This post mostly reads like a contribution to a long-standing philosophical debate to me and would be more appropriately presented as arguing against a philosophical assumption rather than against a research group working under that assumption.</p>\n<p>In the cog-sci / neuroscience institute where I currently work, productive work is being done under similar, though less explicit, assumptions as Brian's / FRI's. Including <a href=\"http://www.sciencedirect.com/science/article/pii/S0893608002000527\">relevant work</a> on modelling valence in animals in the reinforcement learning framework.</p>\n<p>I know you disagree with these assumptions but a post like this can make it seem to outsiders as if you're criticizing a somewhat crazy position and by extension cast a bad light on FRI. </p>\n", "parentCommentId": null, "user": null}, {"_id": "tg2FAKnopkLiMj8w8", "postedAt": "2017-07-21T12:28:45.075Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Mea culpa. I was naively thinking of super-imposing the 'previous' axes. I hope the underlying worry still stands given the arbitrarily many sets of mathematical objects which could be reversibly mapped onto phenomenological states, but perhaps this betrays a deeper misunderstanding.</p>\n", "parentCommentId": "2SKaiBLfNkbQqRL5n", "user": {"username": "Gregory_Lewis"}}, {"_id": "vnSs4MZJvXZ4PD58K", "postedAt": "2017-07-21T13:00:53.252Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>Including <a href=\"http://www.sciencedirect.com/science/article/pii/S0893608002000527\">relevant work</a> on modelling valence</p>\n</blockquote>\n<p>Cool. :) I found that article enlightening and discussed it on pp. 20-21 of <a href=\"https://arxiv.org/abs/1410.8233\">my RL paper</a>.</p>\n", "parentCommentId": "4ekmBpymqcdWP46tu", "user": {"username": "Brian_Tomasik"}}, {"_id": "4ehmwjYcmj2PXupXv", "postedAt": "2017-07-21T14:15:01.392Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>One of the authors (Peter Dayan) is my supervisor, let me know if you'd like me to ask him anything, he does a lot of RL-style modelling :)</p>\n", "parentCommentId": "vnSs4MZJvXZ4PD58K", "user": null}, {"_id": "JHKXNBtnK9CtGcgbk", "postedAt": "2017-07-21T14:23:16.501Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I'll assume you meant isomorphically mapped rather than reversibly mapped, otherwise there's indeed a lot of random things you can map anything.</p>\n<p>I tend to think of isomorphic objects as equivalent in every way that can be mathematically described (and that includes every way I could think of). However, objects can be made of different elements so the equivalence is only after stripping away all information about the elements and seeing them as abstract entities that relate to each other in some way. So you could get {Paris, Rome, London} == {1,2,3}. What Mike is getting at though I think is that the elements also have to be isomorphic all the way down - then I can't think of a reason to not see such completely isomorphic objects as the same.</p>\n", "parentCommentId": "tg2FAKnopkLiMj8w8", "user": null}, {"_id": "sWTSrqDqxGxZpK7yZ", "postedAt": "2017-07-21T15:35:07.988Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>What's the problem if a group of people explores the implications of a well-respected position in philosophy and are (I think) fully aware of the implications? </p>\n</blockquote>\n<p>If the position is wrong then their work is of little use, or possibly harmful. FRI is a nonprofit organization affiliated with EA which uses nontrivial amounts of human and financial capital, of course it's a problem if the work isn't high value. </p>\n<p>I wouldn't be so quick to assume that the idea that moral status boils down to asking 'which computations do I care about' is a well-respected position in philosophy. It probably exists but not in substantial measure.</p>\n", "parentCommentId": "4ekmBpymqcdWP46tu", "user": {"username": "kbog"}}, {"_id": "kEwFbaCzjJfRZ5bfe", "postedAt": "2017-07-21T15:39:04.734Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>What would you say are the philosophical or other premises that FRI does accept (or tends to assume in its work), which distinguishes it from other people/organizations working in a similar space such as MIRI, OpenAI, and QRI? Is it just something like &quot;preventing suffering is the most important thing to work on (and the disjunction of assumptions that can lead to this conclusion)&quot;?</p>\n<p>It seems to me that a belief in anti-realism about consciousness explains a lot of Brian's (near) certainty about his values and hence his focus on suffering. People who are not so sure about consciousness anti-realism tend to be less certain about their values as a result, and hence don't focus on suffering as much. Does this seem right, and if so, can you explain what premises led you to work for FRI?</p>\n", "parentCommentId": "BpFEaCb3vNh9wkjKW", "user": {"username": "Wei_Dai"}}, {"_id": "oCqTLfgoYQjmyeqWR", "postedAt": "2017-07-21T15:42:47.518Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Great! It's not super important, but I'd be curious to know his own thoughts on the question of why pleasure and pain feel different and aren't just a single dimension of motivation, given that you can shift all rewards up or down uniformly while keeping behavior unchanged. <a href=\"http://reducing-suffering.org/why-organisms-feel-both-suffering-and-happiness/#Reward_and_punishment_as_separate_brain_systems\">Here</a> is one possible explanation, which mentions Daw et al. (2002).</p>\n<p>I'd also be curious to know at what level of complexity / ability of artificial RL systems he would start to grant them ethical consideration.</p>\n", "parentCommentId": "4ehmwjYcmj2PXupXv", "user": {"username": "Brian_Tomasik"}}, {"_id": "MFHGSuEaDAadEbWvm", "postedAt": "2017-07-21T16:03:08.478Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>As far as I can see that's just functionalism / physicalism plus moral anti-realism which are both well-respected. But as philosophy of mind and moral philosophy are separate fields you won't see much discussion of the intersection of these views. Completely agreed if you do assume the position is wrong.</p>\n", "parentCommentId": "sWTSrqDqxGxZpK7yZ", "user": null}, {"_id": "QzeavGidcdBJEBN7b", "postedAt": "2017-07-21T16:20:14.962Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I think the choice of a metaethical view is less important than you think. Anti-realism is frequently a much richer view than just talking about preferences. It says that our moral statements aren't truth-apt, but just because our statements aren't truth-apt doesn't mean they're merely about preferences. Anti-realists can give accounts of why a rigorous moral theory is justified and is the right one to follow, not much different from how realists can. Conversely, you could even be a moral realist who believes that moral status boils down to which computations you happen to care about. Anyway, the point is that anti-realists can take pretty much any view in normative ethics, and justify those views in mostly the same ways that realists tend to justify their views (i.e. reasons other than personal preference). Just because we're not talking about whether a moral principle is true or not doesn't mean that we can no longer use the same basic reasons and arguments in favor of or against that principle. Those reasons will just have a different meaning.</p>\n<p>Plus, physicalism is a weaker assertion than the view that consciousness is merely a matter of computation or information processing. Consciousness could be reducible to physical phenomena but without being reducible to computational steps. (eta: this is probably what most physicalists think.)</p>\n", "parentCommentId": "MFHGSuEaDAadEbWvm", "user": {"username": "kbog"}}, {"_id": "qPmgNMKNdRfJCviyL", "postedAt": "2017-07-21T18:57:35.806Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Hi S\u00f6ren- your general point (am I critiquing FRI, or functionalism?) is reasonable. I do note in the piece <em>why</em> I focus on FRI:</p>\n<blockquote>\n<p>Note: FRI is not the only EA organization which holds functionalist views on consciousness; much of the following critique would also apply to e.g. MIRI, FHI, and OpenPhil. I focus on FRI because (1) Brian\u2019s writings on consciousness &amp; functionalism have been hugely influential in the community, and are clear enough <em>to</em> criticize; (2) the fact that FRI is particularly clear about what it cares about- suffering- allows a particularly clear critique about what problems it will run into with functionalism; (3) I believe FRI is at the forefront of an important cause area which has not crystallized yet, and I think it\u2019s critically important to get these objections bouncing around this subcommunity.</p>\n</blockquote>\n<p>I should say too that the purpose of bringing up QRI's work is not to suggest FRI should be focusing on this, but instead that effort developing alternatives helps calibrate the field:</p>\n<blockquote>\n<p>I mention all this because I think analytic functionalism- which is to say radical skepticism/eliminativism, the metaphysics of last resort- only looks as good as it does because nobody\u2019s been building out any alternatives.</p>\n</blockquote>\n", "parentCommentId": "4ekmBpymqcdWP46tu", "user": {"username": "MikeJohnson"}}, {"_id": "9Pdb2ecDWCCt6DDEw", "postedAt": "2017-07-21T19:37:03.104Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>Curious for your take on the premise that ontologies always have tacit telos.</p>\n</blockquote>\n<p>Some ontologies seem to have <em>more</em> of a telos 'baked in'-- e.g., Christianity might be a good example-- whereas other ontologies have zero explicit telos-- e.g., pure mathematics.</p>\n<p>But I think you're right that there's <em>always</em> a tacit telos, perhaps based on elegance. When I argue that &quot;consciousness is a physics problem&quot;, I'm arguing that it inherits physics' tacit telos, which seems to be elegance-as-operationalized-by-symmetry.</p>\n<p>I wonder if &quot;elegance&quot; always captures telos? This would indicate a certain theory-of-effective-social/personal-change...</p>\n<blockquote>\n<p>Also, we desire to expand the domains of our perception with scientific instrumentation and abstractions. This expansion always generates some mapping (ontology) from the new data to our existing sensory modalities.</p>\n</blockquote>\n<p>Yeah, it doesn't seem technology can ever truly be &quot;teleologically neutral&quot;.</p>\n", "parentCommentId": "P2npBe5f9dKejtamd", "user": {"username": "MikeJohnson"}}, {"_id": "fwmKSeLdx8ro6Br2N", "postedAt": "2017-07-21T19:54:03.937Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Rather than put words in the mouths of other people at FRI, I'd rather let them personally answer which philosophical premises they accept and what motivates them, if they wish.</p>\n<p>For me personally, I've just had, for a long time, the intuition that preventing extreme suffering is the most important priority. To the best that I can tell, much of this intuition can be traced to having suffered from depression and general feelings of crushing hopelessness for large parts of my life, and wanting to save anyone else from experiencing a similar (or worse!) magnitude of suffering. I seem to recall that I was less suffering-focused before I started getting depressed for the first time.</p>\n<p>Since then, that intuition has been reinforced by reading up on other suffering-focused works; something like <a href=\"https://foundational-research.org/tranquilism/\">tranquilism</a> feels like a sensible theory to me, especially given some of my <a href=\"http://kajsotala.fi/2017/07/meditation-insights-suffering-and-pleasure-are-intrinsically-bound-together/\">own experiences with meditation</a> which are generally compatible with the kind of theory of mind implied by tranquilism. That's something that has come later, though.</p>\n<p>To clarify, none of this means that I would <em>only</em> value suffering prevention: I'd much rather see a universe-wide flourishing civilization full of <a href=\"https://www.hedweb.com/\">minds in various states of bliss</a>, than a dead and barren universe. My position is more of a <a href=\"https://en.wikipedia.org/wiki/Prioritarianism\">prioritarian</a> one: let's <em>first</em> take care of everyone who's experiencing enormous suffering, and make sure none of our descendants are going to be subject to that fate, before we start thinking about colonizing the rest of the universe and filling it with entirely new minds.</p>\n", "parentCommentId": "kEwFbaCzjJfRZ5bfe", "user": {"username": "Kaj_Sotala"}}, {"_id": "rrNfkzKgPyc9XEdK9", "postedAt": "2017-07-21T21:07:49.467Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I'm a bit surprised to find that Brian Tomasik <a href=\"http://reducing-suffering.org/dissolving-confusion-about-consciousness/\">attributes</a> his current views on consciousness to his conversations with Carl Shulman, since in my experience Carl is a very careful thinker and the case for accepting anti-realism as the answer to the problem of consciousness seems pretty weak, at least as explained by Brian. I'm very curious to read Carl's own explanation of his views, if he has written one down. I scanned <a href=\"https://timelines.issarice.com/wiki/Timeline_of_Carl_Shulman_publications\">Carl Shulman's list of writings</a> but was unable to find anything that addressed this.</p>\n", "parentCommentId": null, "user": {"username": "Wei_Dai"}}, {"_id": "aKiX3h3xWfrMpJ6Yi", "postedAt": "2017-07-21T23:08:37.136Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>Is it just something like &quot;preventing suffering is the most important thing to work on (and the disjunction of assumptions that can lead to this conclusion)&quot;?</p>\n</blockquote>\n<p>I also don't want to speak for FRI as a whole, but yeah, I think it's safe to say that a main thing that makes FRI unique is its suffering focus.</p>\n<p>My high confidence in suffering-focused values results from moral anti-realism generally (or, if moral realism is true, then my unconcern for the moral truth). I don't think consciousness anti-realism plays a big role because I would still be suffering-focused even if qualia were &quot;real&quot;. My suffering focus is ultimately driven by the visceral feeling that extreme suffering is so severe that nothing else compares in importance. Theoretical arguments take a back seat to this conviction.</p>\n", "parentCommentId": "kEwFbaCzjJfRZ5bfe", "user": {"username": "Brian_Tomasik"}}, {"_id": "MnycTa6s7pyGvnLAZ", "postedAt": "2017-07-21T23:16:58.135Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>Is it just something like &quot;preventing suffering is the most important thing to work on (and the disjunction of assumptions that can lead to this conclusion)&quot;?</p>\n</blockquote>\n<p>This sounds right. Before 2016, I would have said that rough value alignment (normatively &quot;suffering-focused&quot;) is very-close-to necessary, but we updated away from this condition and for quite some time now hold the view that it is not essential if people are otherwise a good fit. We still have an expectation that researchers think about research-relevant background assumptions in ways that are not completely different from ours on every issue, but single disagreements are practically never a dealbreaker. We've had qualia realists both on the team (part-time) and as interns, and some team members now don't hold strong views on the issue one way or the other. Brian especially is a really strong advocate of epistemic diversity and goes much further with it than I feel most people would go. </p>\n<blockquote>\n<p>People who are not so sure about consciousness anti-realism tend to be less certain about their values as a result, and hence don't focus on suffering as much. </p>\n</blockquote>\n<p>Hm, this does not fit my observations. We had and still have people on our team who don't have strong confidence in either view, and there exists also a sizeable cluster of people who seem highly confident in both qualia realism and morality being about reducing suffering, the most notable example being David Pearce. </p>\n<p>The one view that seems unusually prevalent within FRI, apart from people self-identifying with suffering-focused values, is a particular anti-realist perspective on morality and moral reasoning where valuing open-ended moral reflection is not always regarded as the by default &quot;prudent&quot; thing to do. This is far from a consensus and many team members value moral reflection a great deal, but many of us expect less \u201cwork\u201d to be done by value-reflection procedures than others in the EA movement seemingly expect. Perhaps this is due to different ways of thinking about extrapolation procedures, or perhaps it\u2019s due to us having made stronger lock-ins to certain aspects of our moral self image. </p>\n<p>Paul Christiano\u2019s <a href=\"https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\">indirect normativity write-up</a> for instance deals with the &quot;Is \u201cPassing the Buck\u201d Problematic?\u201d objection in an in my view unsatisfying way. Working towards a situation where everyone has much more time to think about their values is more promising the more likely it is that there is \u201cmuch to be gained,\u201d normatively. But this somewhat begs the question. If one finds suffering-focused views very appealing, other interventions become more promising. There seems to be high value of information on narrowing down one\u2019s moral uncertainty in this domain (much more so, arguably, than with questions of consciousness or which computations to morally care about). One way to attempt to reduce one\u2019s moral uncertainty and capitalize on the value of information is by thinking more about the object-level arguments in population ethics; another way to do it is by thinking more about the value of moral reflection, how much it depends on intuition or self-image-based &quot;lock ins&quot; vs. how much it (either in general or in one's personal case) is based on other things that are more receptive to information gains or intelligence gains. </p>\n<p>Personally, I would be totally eager to place the fate of \u201cWhich computations count as suffering?\u201d into the hands of some in-advance specified reflection process, even when I feel like I don\u2019t understand the way moral reflection will work out in the details of this complex algorithm. I\u2019d be less confident in my current understanding of consciousness than I\u2019d be confident in being able to pick a reassuring-seeming way of delegating the decision-making to smarter advisors. However, I get the opposite feeling when it comes to questions of population ethics. There, I feel like I have thought about the issue a lot, experience it as easier and more straightforward to think about than consciousness and whether I care about insects or electrons or Jupiter brains, and I have some strong intuitions and aspects of my self-identity about the matter and am unsure in which legitimate ways (as opposed to failures of goal preservation) I could gain evidence that would strongly change my mind. It would feel wrong to me to place the fate of my values into some in-advance specified, open-ended deliberation algorithm where I won\u2019t really understand how it will play out and what initial settings make which kind of difference to the end result (and why). I'd be fine with quite &quot;conservative&quot; reflection procedures where I could be confident that it would likely output something that does not seem too far away from my current thinking, but would be gradually more worried about more open-ended ones.</p>\n", "parentCommentId": "kEwFbaCzjJfRZ5bfe", "user": {"username": "Lukas_Gloor"}}, {"_id": "udfe2t4HggA7Wc88J", "postedAt": "2017-07-21T23:18:45.730Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I don't want to put words in Carl's mouth, and certainly Carl doesn't necessarily endorse anything I write. Perhaps he'll chime in. :)</p>\n<p>For more defenses of anti-realism (i.e., type-A physicalism), <a href=\"http://reducing-suffering.org/hard-problem-consciousness/#Examples_of_type-A_and_type-B_physicalists\">here</a> are some other authors. Dennett is the most famous, though some complain that he doesn't use rigorous philosophical arguments/jargon.</p>\n", "parentCommentId": "rrNfkzKgPyc9XEdK9", "user": {"username": "Brian_Tomasik"}}, {"_id": "fM742tqZvP3v7z3YD", "postedAt": "2017-07-21T23:58:20.609Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Interesting that you mention the &quot;waterfall&quot;/&quot;bag of popcorn&quot; argument against computationalism in the same article as citing Scott Aaronson, since he actually gives some arguments against it (see section 6 of <a href=\"https://arxiv.org/abs/1108.1791)\">https://arxiv.org/abs/1108.1791)</a>. In particular, he suggests that we can argue that a process P isn't contributing any computation when having a P-oracle doesn't let you solve the problem faster.</p>\n<p>I don't think this fully lays to rest the question of what things are performing computations, but I think we can distinguish them in <em>some</em> ways, which makes me hopeful that there's an underlying distinction.</p>\n<p>There's always going to be a huge epistemic problem, of course. The homomorphic encryption shows that there will always be computations that we can't distinguish from noise (I <em>just</em> wrote a blog post about this - curse Scott and his beating me to the punch by years). But I think we can reasonably expect such things to be rare in nature.</p>\n", "parentCommentId": null, "user": {"username": "Michael_PJ"}}, {"_id": "MrqP2uQR2wuLCjMDv", "postedAt": "2017-07-22T00:03:09.631Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>If they're isomorphic, then they really are the same for mathematical purposes. Possibly if you view STV as having a metaphysical component then you incur some dependence on philosophy of mathematics to say what a mathematical structure is, whether isomorphic structures are distinct, etc.</p>\n", "parentCommentId": "tg2FAKnopkLiMj8w8", "user": {"username": "Michael_PJ"}}, {"_id": "xLrkDJnNWZHYh4qwD", "postedAt": "2017-07-22T00:33:58.055Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I agree with this. </p>\n", "parentCommentId": "QzeavGidcdBJEBN7b", "user": {"username": "Lukas_Gloor"}}, {"_id": "GfT7gJdvtomW6w94t", "postedAt": "2017-07-22T00:57:54.392Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Hi Kaj- that makes a lot of sense. I would say FRI currently looks very eliminativism-heavy from the outside (see e.g., <a href=\"https://foundational-research.org/research/#consciousness)\">https://foundational-research.org/research/#consciousness)</a>, but it sounds like the inside view is indeed different.</p>\n<p>As I noted on FB, I'll look forward to seeing where FRI goes with its research.</p>\n", "parentCommentId": "BpFEaCb3vNh9wkjKW", "user": {"username": "MikeJohnson"}}, {"_id": "mfnMscvEWhWKiD7Mw", "postedAt": "2017-07-22T01:00:19.737Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>While most (? my impression anyway as someone who works there) researchers at FRI place highest credence on functionalism and eliminativism, there is more skepticism about Brian's inclination to never draw hard boundaries in concept space.</p>\n</blockquote>\n<p>It would be interesting to see FRI develop what 'suffering-focused ethics, as informed by functionalism/eliminativism, but with hard boundaries in concept space' might look like.</p>\n", "parentCommentId": "H3q6iLGJRnffWX8af", "user": {"username": "MikeJohnson"}}, {"_id": "dNX52csy3PGaq9jt7", "postedAt": "2017-07-22T01:04:42.304Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>This may or may not be relevant, but I would definitely say that Brian's views are not 'fringe views' in the philosophy of mind; they're quite widely held in philosophy and elsewhere. I believe Brian sticks out because his writing is so clear, and because he doesn't avoid thinking about and admitting strange implications of his views.</p>\n<p>That said I don't know Carl's specific views on the topic.</p>\n", "parentCommentId": "udfe2t4HggA7Wc88J", "user": {"username": "MikeJohnson"}}, {"_id": "6rie9PhrL9yBso35c", "postedAt": "2017-07-22T10:06:39.448Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>The one view that seems unusually prevalent within FRI, apart from people self-identifying with suffering-focused values, is a particular anti-realist perspective on morality and moral reasoning where valuing open-ended moral reflection is not always regarded as the by default &quot;prudent&quot; thing to do.</p>\n</blockquote>\n<p>Thanks for pointing this out. I've noticed this myself in some of FRI's writings, and I'd say this, along with the high amount of certainty on various object-level philosophical questions that presumably cause the disvaluing of reflection about them, are what most &quot;turns me off&quot; about FRI. I worry a lot about potential failures of goal preservation (i.e., value drift) too, but because I'm highly uncertain about just about every <a href=\"http://lesswrong.com/lw/khf/six_plausible_metaethical_alternatives/\">meta-ethical</a> and <a href=\"http://lesswrong.com/lw/1r9/shut_up_and_divide/\">normative</a> <a href=\"http://lesswrong.com/lw/fyb/ontological_crisis_in_humans/\">question</a>, I see no choice but to try to design some sort of reflection procedure that I can trust enough to hand off control to. In other words, I have nothing I'd want to &quot;lock in&quot; at this point and since I'm by default constantly handing off control to my future self with few safeguards against value drift, doing something better than that default is one of my highest priorities. If other people are also uncertain and place high value on (safe/correct) reflection as a result, that helps with my goal (because we can then pool resources together to work out what safe/correct reflection is), so it's regrettable to see FRI people sometimes argue for more certainty than I think is warranted and especially to see them argue against reflection.</p>\n", "parentCommentId": "MnycTa6s7pyGvnLAZ", "user": {"username": "Wei_Dai"}}, {"_id": "XLkt75eXdtJnxL6hn", "postedAt": "2017-07-22T10:49:58.300Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>That makes sense. I do think as a general policy, valuing reflection is more positive-sum, and if one does not feel like much is &quot;locked in&quot; yet then it becomes very natural too. I'm not saying that people who value reflection more than I do are doing it wrong; I think I would even argue for reflection being very important and recommend it to new people, if I felt more comfortable that they'd end up pursuing things that are beneficial from all/most plausible perspectives. Though what I find regrettable is that the &quot;default&quot; interventions that are said to be good from as many perspectives as possible oftentimes do not seem great from a suffering-focused perspective.</p>\n", "parentCommentId": "6rie9PhrL9yBso35c", "user": {"username": "Lukas_Gloor"}}, {"_id": "NmxEYDkD9xQ7gdPsW", "postedAt": "2017-07-22T10:59:39.625Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Thanks, for the clarification, I can't comment much as I don't know much about the different flavors or anti realism. </p>\n<p>One thing I'd like to point out, and I'm happy to be corrected on that, is that when an anti realist argues they will often (always?) base themselves on principles such as consistency. It seems hard to argue anything without referring to any principle. But someone who who doesn't support the application of a principle won't be convinced and that's up to preferences too. (I certainly know people who reject the drowning child argument because they explicitly don't care about consistency). So you could see debate about ethics because people are exploring the implications of principles they happen to share.</p>\n<p>Agree on physicalism being a fairly general set of views.</p>\n", "parentCommentId": "QzeavGidcdBJEBN7b", "user": null}, {"_id": "NxRZH2pbjySq4JrCR", "postedAt": "2017-07-22T11:07:08.085Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Makes sense :)</p>\n", "parentCommentId": "qPmgNMKNdRfJCviyL", "user": null}, {"_id": "Bh4nieg87EuTRdg4W", "postedAt": "2017-07-22T14:42:39.569Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I really enjoyed your linked piece on meta-ethics. Short but insightful. I believe I'd fall into the second bucket.</p>\n<p>If you're looking for what (2) might look like in practice, and how we might try to relate it to the human brain's architecture/drives, you might enjoy this: <a href=\"http://opentheory.net/2017/05/why-we-seek-out-pleasure-the-symmetry-theory-of-homeostatic-regulation/\">http://opentheory.net/2017/05/why-we-seek-out-pleasure-the-symmetry-theory-of-homeostatic-regulation/</a></p>\n<p>I'd also agree that designing trustworthy reflection procedures is important. My intuitions here are:\n(1) value-drift is a big potential problem with FRI's work (even if they &quot;lock in&quot; caring about suffering, if their definition of 'suffering' drifts, their tacit values do too);\n(2) value-drift will be a problem for any system of ethics that doesn't cleanly 'compile to physics'. (This is a big claim, centering around my Objection 6, above.)</p>\n<p>Perhaps we could generalize this latter point as &quot;if <a href=\"http://www.scottaaronson.com/blog/?p=3327\">information is physical</a>, and value is informational, then value is physical too.&quot;</p>\n", "parentCommentId": "6rie9PhrL9yBso35c", "user": {"username": "MikeJohnson"}}, {"_id": "7F2d67WDoaCax4p8Q", "postedAt": "2017-07-22T15:39:42.498Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Elegance is probably worth exploring in the same way that moral descriptivism as a field turned up some interesting things. My naive take is something like 'efficient compression of signaling future abundance.'</p>\n<p>Another frame for the problem: what is mathematical and scientific taste and how does it work?</p>\n<p>Also, more efficient objection to religion: 'your compression scheme is lossy bro.' :D</p>\n", "parentCommentId": "9Pdb2ecDWCCt6DDEw", "user": {"username": "RomeoStevens"}}, {"_id": "vcjYgCh5GJvEyubSA", "postedAt": "2017-07-22T17:42:48.749Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>he suggests that we can argue that a process P isn't contributing any computation when having a P-oracle doesn't let you solve the problem faster.</p>\n</blockquote>\n<p>Interesting idea. :) Aaronson says (p. 23):</p>\n<blockquote>\n<p>I conjecture that, given any chess-playing algorithm A that accesses a \u201cwaterfall oracle\u201d W, there is an equally-good chess-playing algorithm A\u2032, with similar time and space requirements, that does not access W.</p>\n</blockquote>\n<p>I'm not so sure this is true. There might be clever ways to use the implicit computations of falling water to save computational cost. For example, <a href=\"https://doi.org/10.1007/978-3-540-39432-7_63\">Fernando and Sojakka (2003)</a> used water waves to help process inputs:</p>\n<blockquote>\n<p>This paper demonstrates that the waves produced on the surface of water can be used as the medium for a \u201cLiquid State Machine\u201d that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made.</p>\n</blockquote>\n<p>That said, I agree that the computational-complexity test seems like one helpful consideration for identifying which computations a system is performing.</p>\n", "parentCommentId": "fM742tqZvP3v7z3YD", "user": {"username": "Brian_Tomasik"}}, {"_id": "GwTXELcqxxzud4uLP", "postedAt": "2017-07-22T21:44:44.147Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Interesting. I'm a moral anti-realist who also focuses on suffering, but not to the extent that you do (e.g. not worrying that much about suffering at the level of fundamental physics.) I would have predicted that theoretical arguments were what convinced you to care about fundamental physics suffering, not any sort of visceral feeling.</p>\n", "parentCommentId": "aKiX3h3xWfrMpJ6Yi", "user": {"username": "kokotajlod"}}, {"_id": "9tx67M4SoZdFotGaq", "postedAt": "2017-07-22T21:47:29.152Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>SoerenMind: It's wayyy more than just functionalism/physicalism plus moral anti-realism. There are tons of people who hold both views, and only a tiny fraction of them are negative utilitarians or anything close. In fact I'd bet it's somewhat unusual for any sort of moral anti-realist to be any sort of utilitarian.</p>\n", "parentCommentId": "MFHGSuEaDAadEbWvm", "user": {"username": "kokotajlod"}}, {"_id": "pkxL65MaE7xApPRTz", "postedAt": "2017-07-22T23:27:20.458Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Sorry, I meant that emotion is what makes me care about (extreme) suffering in the first place. With that foundation, one should use arguments to clarify what reducing suffering looks like in practice and what &quot;suffering&quot; even means. Also, there's some blending of rational arguments and emotion. I now care a bit about suffering in fundamental physics on an emotional level because my conception of suffering has been changed by learning more about the world and philosophy of mind. (That said, I <a href=\"http://reducing-suffering.org/is-there-suffering-in-fundamental-physics/#How_much_do_I_care\">still care</a> a lot about animals.)</p>\n", "parentCommentId": "GwTXELcqxxzud4uLP", "user": {"username": "Brian_Tomasik"}}, {"_id": "pzzeXcbdm9WiSQxPt", "postedAt": "2017-07-23T19:07:12.745Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>much of the following critique would also apply to e.g. MIRI, FHI, and OpenPhil. </p>\n</blockquote>\n<p>I'm a little confused here. Where does MIRI or FHI say anything about consciousness, much less assume any particular view? </p>\n", "parentCommentId": null, "user": {"username": "kbog"}}, {"_id": "azSYFoMrw5Gznxn96", "postedAt": "2017-07-23T20:57:34.226Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>My sense that MIRI and FHI are fairly strong believers in functionalism, based on reading various pieces on LessWrong, personal conversation with people who work there, and 'revealed preference' research directions. OpenPhil may be more of a stretch to categorize in this way; I'm going off what I recall of Holden's debate on AI risk, some limited personal interactions with people that work there, and Luke Muehlhauser's report (he was up-front about his assumptions on this).</p>\n<p>Of course it's harder to pin down what people at these organizations believe than it is in Brian's case, since Brian writes a great deal about his views.</p>\n<p>So to my knowledge, this statement is essentially correct, although there may be definitional &amp; epistemological quibbles.</p>\n", "parentCommentId": "pzzeXcbdm9WiSQxPt", "user": {"username": "MikeJohnson"}}, {"_id": "wKDgWy4Y8C5h458k6", "postedAt": "2017-07-25T11:01:35.955Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Wait, are you equating &quot;functionalism&quot; with &quot;doesn't believe suffering can be meaningfully defined&quot;? I thought your criticism was mostly about the latter; I don't think it's automatically implied by the former. If you had a precise enough theory about the functional role and source of suffering, then this would be a functionalist theory that specified objective criteria for the presence of suffering.</p>\n<p>(You could reasonably argue that it doesn't look likely that functionalism <em>will</em> provide such a theory, but then I've always assumed that anyone who has thought seriously about philosophy of mind has acknowledged that functionalism has major deficiencies and is at best our &quot;least wrong&quot; placeholder theory until somebody comes up with something better.)</p>\n", "parentCommentId": "azSYFoMrw5Gznxn96", "user": {"username": "Kaj_Sotala"}}, {"_id": "b9bXbeGpaXAaXEPZy", "postedAt": "2017-07-25T17:36:44.107Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Functionalism seems internally consistent (although perhaps too radically skeptical). However, in my view it also seems to lead to some flavor of moral nihilism; consciousness anti-realism makes suffering realism difficult/complicated.</p>\n<blockquote>\n<p>If you had a precise enough theory about the functional role and source of suffering, then this would be a functionalist theory that specified objective criteria for the presence of suffering.</p>\n</blockquote>\n<p>I think whether suffering is a 'natural kind' is prior to this analysis: e.g., to precisely/objectively explain the functional role and source of something, it needs to have a precise/crisp/objective existence.</p>\n<blockquote>\n<p>I've always assumed that anyone who has thought seriously about philosophy of mind has acknowledged that functionalism has major deficiencies and is at best our &quot;least wrong&quot; placeholder theory until somebody comes up with something better.)</p>\n</blockquote>\n<p>Part of my reason for writing this critique is to argue that functionalism isn't a useful theory of mind, because it doesn't do what we need theories of mind to do (adjudicate disagreements in a principled way, especially in novel contexts). </p>\n<p>If it <em>is</em> a placeholder, then I think the question becomes, &quot;what would 'something better' look like, and what would count as evidence that something <em>is</em> better? I'd love to get your (and FRI's) input here.</p>\n", "parentCommentId": "wKDgWy4Y8C5h458k6", "user": {"username": "MikeJohnson"}}, {"_id": "CQ4MBCtFWLeoon46f", "postedAt": "2017-07-25T23:17:19.852Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>I think whether suffering is a 'natural kind' is prior to this analysis: e.g., to precisely/objectively explain the functional role and source of something, it needs to have a precise/crisp/objective existence.</p>\n</blockquote>\n<p>I take this as meaning that you agree that accepting functionalism is orthogonal to the question of whether suffering is &quot;real&quot; or not?</p>\n<blockquote>\n<p>If it is a placeholder, then I think the question becomes, &quot;what would 'something better' look like, and what would count as evidence that something is better?</p>\n</blockquote>\n<p>What something better would look like - if I knew <em>that</em>, I'd be busy writing a paper about it. :-) That seems to be a part of the problem - everyone (that I know of) agrees that functionalism is deeply unsatisfactory, but very few people seem to have any clue of what a better theory might look like. Off the top of my head, I'd like such a theory to at least be able to offer some insight into what exactly <em>is</em> conscious, and not have the issue where you can hypothesize all kinds of weird computations (like Aaronson did in your quote) and be left confused about which of them are conscious and which are not, and why. (roughly, my desiderata are similar to <a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#AppendixB\">Luke Muehlhauser's</a>)</p>\n", "parentCommentId": "b9bXbeGpaXAaXEPZy", "user": {"username": "Kaj_Sotala"}}, {"_id": "XFu7PDF2gHeGikHHL", "postedAt": "2017-07-26T00:05:44.424Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Well I think there is a big difference between FRI, where the point of view is at the forefront of their work and explicitly stated in research, and MIRI/FHI, where it's secondary to their main work and is only something which is inferred on the basis of what their researchers happen to believe. Plus as Kaj said you can be a functionalist without being all subjectivist about it.</p>\n<p>But Open Phil does seem to have this view now to at least the same extent as FRI does (cf. Muelhauser's consciousness document).</p>\n", "parentCommentId": "azSYFoMrw5Gznxn96", "user": {"username": "kbog"}}, {"_id": "cArCRy4YaXfQcjWAA", "postedAt": "2017-07-26T18:33:54.752Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>I take this as meaning that you agree that accepting functionalism is orthogonal to the question of whether suffering is &quot;real&quot; or not?</p>\n</blockquote>\n<p>Ah, the opposite actually- my expectation is that if 'consciousness' isn't real, 'suffering' can't be real either.</p>\n<blockquote>\n<p>What something better would look like - if I knew that, I'd be busy writing a paper about it. :-) That seems to be a part of the problem - everyone (that I know of) agrees that functionalism is deeply unsatisfactory, but very few people seem to have any clue of what a better theory might look like. Off the top of my head, I'd like such a theory to at least be able to offer some insight into what exactly is conscious, and not have the issue where you can hypothesize all kinds of weird computations (like Aaronson did in your quote) and be left confused about which of them are conscious and which are not, and why. (roughly, my desiderata are similar to Luke Muehlhauser's)</p>\n</blockquote>\n<p>Thanks, this is helpful. :)</p>\n<p>The following is tangential, but I thought you'd enjoy this Yuri Harari quote on abstraction and suffering:</p>\n<blockquote>\n<p>In terms of power, it\u2019s obvious that this ability [to create abstractions] made Homo sapiens the most powerful animal in the world, and now gives us control of the entire planet. From an ethical perspective, whether it was good or bad, that\u2019s a far more complicated question. The key issue is that because our power depends on collective fictions, we are not good in distinguishing between fiction and reality. Humans find it very difficult to know what is real and what is just a fictional story in their own minds, and this causes a lot of disasters, wars and problems.</p>\n</blockquote>\n<blockquote>\n<p>The best test to know whether an entity is real or fictional is the test of suffering. A nation cannot suffer, it cannot feel pain, it cannot feel fear, it has no consciousness. Even if it loses a war, the soldier suffers, the civilians suffer, but the nation cannot suffer. Similarly, a corporation cannot suffer, the pound sterling, when it loses its value, it doesn\u2019t suffer. All these things, they\u2019re fictions. If people bear in mind this distinction, it could improve the way we treat one another and the other animals. It\u2019s not such a good idea to cause suffering to real entities in the service of fictional stories.</p>\n</blockquote>\n", "parentCommentId": "CQ4MBCtFWLeoon46f", "user": {"username": "MikeJohnson"}}, {"_id": "jwJemP2grPGEpAthu", "postedAt": "2017-07-27T02:47:28.996Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>everyone (that I know of) agrees that functionalism is deeply unsatisfactory</p>\n</blockquote>\n<p>I don't. :) I see lots of free parameters for what flavor of functionalism to hold and how to rule on the Aaronson-type cases. But functionalism (perhaps combined with some other random criteria I might reserve the right to apply) perfectly captures my preferred way to think about consciousness.</p>\n<p>I think what is unsatisfactory is that we still know so little about neuroscience and, among other things, what it looks like in the brain when we feel ourselves to have qualia.</p>\n", "parentCommentId": "CQ4MBCtFWLeoon46f", "user": {"username": "Brian_Tomasik"}}, {"_id": "LPmyXWnYWQRNRSnhH", "postedAt": "2017-07-27T02:54:58.837Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I think a default assumption should be that works by individual authors don't necessarily reflect the views of the organization they're part of. :) Indeed, Luke's report says this explicitly:</p>\n<blockquote>\n<p>the rest of this report does not necessarily reflect the intuitions and judgments of the Open Philanthropy Project in general. I explain my views in this report merely so they can serve as one input among many as the Open Philanthropy Project considers how to clarify its values and make its grantmaking choices.</p>\n</blockquote>\n<p>Of course, there is nonzero Bayesian evidence in the sense that an organization is unlikely to publish a viewpoint that it finds completely misguided.</p>\n<p>When FRI put my consciousness pieces on its site, we were planning to add a counterpart article (I think defending type-F monism or something) to have more balance, but that latter article never got written.</p>\n", "parentCommentId": "XFu7PDF2gHeGikHHL", "user": {"username": "Brian_Tomasik"}}, {"_id": "LJCjTC6k9oBMKkCS9", "postedAt": "2017-07-27T21:37:33.495Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>An additional note on this:</p>\n<blockquote>\n<p>What something better would look like - if I knew that, I'd be busy writing a paper about it. :-) That seems to be a part of the problem - everyone (that I know of) agrees that functionalism is deeply unsatisfactory, but very few people seem to have any clue of what a better theory might look like. </p>\n</blockquote>\n<p>I'd propose that if we split the problem of building a theory of consciousness up into subproblems, the task gets a lot easier. This does depend on elegant problem decompositon. Here are the subproblems I propose:\n<a href=\"http://opentheory.net/wp-content/uploads/2016/11/Eight-Problems2-1.png\">http://opentheory.net/wp-content/uploads/2016/11/Eight-Problems2-1.png</a></p>\n<p>A quick-and-messy version of my framework:</p>\n<ul>\n<li><p>(1) figure out what sort of ontology you think can map to both phenomenology (what we're trying to explain) and physics (the world we live in);</p>\n</li>\n<li><p>(2) figure out what subset of that ontology actively contributes to phenomenology;</p>\n</li>\n<li><p>(3) figure out how to determine the boundary of where minds stop, in terms of that-stuff-that-contributes-to-phenomenology;</p>\n</li>\n<li><p>(4) figure out how to turn the information inside that boundary into a mathematical object isomorphic to phenomenology (and what the state space of the object is);</p>\n</li>\n<li><p>(5) figure out how to interpret how properties of this mathematical object map to properties of phenomenology.</p>\n</li>\n</ul>\n<p>The QRI approach is:</p>\n<ul>\n<li><p>(1) Choice of core ontology -&gt; physics (since it maps to physical reality cleanly, or some future version like string theory will);</p>\n</li>\n<li><p>(2) Choice of subset of core ontology that actively contributes to phenomenology -&gt; Andres suspects quantum coherence; I'm more agnostic (I think <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3912322/\">Barrett 2014</a> makes some good points);</p>\n</li>\n<li><p>(3) Identification of boundary condition -&gt; highly dependent on (2);</p>\n</li>\n<li><p>(4) Translation of information in partition into a structured mathematical object isomorphic to phenomenology -&gt; I like how IIT does this;</p>\n</li>\n<li><p>(5) Interpretation of what the mathematical output means -&gt; Probably, following IIT, the dimensional magnitude of the object could correspond with the degree of consciousness of the system. More interestingly, I think the <em>symmetry</em> of this object may plausibly have an identity relationship with the <em>valence</em> of the experience.</p>\n</li>\n</ul>\n<p>Anyway, certain steps in this may be wrong, but that's what the basic QRI &quot;full stack&quot; approach looks like. I think we should be able to iterate as we go, since we can test parts of (5) (like the Symmetry Hypothesis of Valence) without necessarily having the whole 'stack' figured out.</p>\n", "parentCommentId": "CQ4MBCtFWLeoon46f", "user": {"username": "MikeJohnson"}}, {"_id": "KWM9LKr9q27rPPfZu", "postedAt": "2017-07-30T08:29:49.408Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>The quote seems very myopic. Let's say that we have a religion X that has an excellent track record at preventing certain sorts of defections by helping people coordinate on enforcement costs. Suffering in the service of stabilizing this state of affairs may be the best use of resources in a given context.</p>\n", "parentCommentId": "cArCRy4YaXfQcjWAA", "user": {"username": "RomeoStevens"}}, {"_id": "6Lrwqcdx86DJ9sXmw", "postedAt": "2017-07-30T22:17:36.404Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Speaking of the metaphysical correctness of claims about qualia sounds confused, and I think precise definitions of qualia-related terms should be judged by how useful they are for generalizing our preferences about central cases. I expect that any precise definition for qualia-related terms that anyone puts forward before making quite a lot of philosophical progress is going to be very wrong when judged by usefulness for describing preferences, and that the vagueness of the analytic functionalism used by FRI is necessary to avoid going far astray.</p>\n<p>Regarding the objection that shaking a bag of popcorn can be interpreted as carrying out an arbitrary computation, I'm not convinced that this is actually true, and I suspect it isn't. It seems to me that the interpretation would have to be doing essentially all of the computation itself, and it should be possible to make precise the sense in which brains and computers simulating brains carry out a certain computation that waterfalls and bags of popcorn don't. The defense of this objection that you quote from McCabe is weak; the uncontroversial fact that many slightly different physical systems can carry out the same computation does not establish that an arbitrary physical system can be reasonably interpreted as carrying out an arbitrary computation.</p>\n<p>I think the edge cases that you quote Scott Aaronson bringing up are good ones to think about, and I do have a large amount of moral uncertainty about them. But I don't see these as problems specific to analytic functionalism. These are hard problems, and the fact that some more precise theory about qualia may be able to easily answer them is not a point in favor of that theory, since wrong answers are not helpful.</p>\n<p>The Symmetry Theory of Valence sounds wildly implausible. There are tons of claims that people put forward, often contradicting other such claims, that some qualia-related concept is actually some other simple thing. For instance, I've heard claims that goodness is complexity and that what humans value is increasing complexity. Complexity and symmetry aren't quite opposites, but they're certainly anti-correlated, and both theories can't be right. These sorts of theories never end up getting empirical support, although their proponents often claim to have empirical support. For example, proponents of Integrated Information Theory often cite that the cerebrum has a higher Phi value than the cerebellum does as support for the hypothesis that Phi is a good measure of the amount of consciousness a system has, as if comparing two data points was enough to support such a claim, and it turns out that large regular rectangular grids of transistors, and the operation of multiplication by a large Vandermonde matrix, both have arbitrarily high Phi values, and yet the claim that Phi measures consciousness still survives and claims empirical support, despite this damning disconfirmation. And I think the \u201cgoodness is complexity\u201d people also provided examples of good things that they thought they had established are complex and bad things that they thought they had established are not. I know this sounds totally unfair, but I won't be at all surprised if you claim to have found substantial empirical support for your theory, and I still won't take your theory at all seriously if you do, because any evidence you cite will inevitably be highly dubious. The heuristic that claims that a qualia-related concept is some simple other thing are wrong, and that claims of empirical support for such claims never hold up, seems to be pretty well supported. I am almost certain that there are trivial counterexamples to the Symmetry Theory of Valence, even though perhaps you may have developed a theory sophisticated enough to avoid the really obvious failure modes like claiming that a square experiences more pleasure and less suffering than a rectangle because its symmetry group is twice as large.</p>\n", "parentCommentId": null, "user": {"username": "AlexMennen"}}, {"_id": "okFKJRhsfT5jtGgPg", "postedAt": "2017-07-31T05:13:53.099Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>To steelman the popcorn objection, one could say that separating &quot;normal&quot; computations from popcorn shaking requires at least certain sorts of conditions on what counts as a valid interpretation, and such conditions increase the arbitrariness of the theory. Of course, if we adopt a complexity-of-value approach to moral value (as I and probably you think we should), then those conditions on what counts as a computation may be minimal compared with the other forms of arbitrariness we bring to bear.</p>\n<p>I haven't read <em>Principia Qualia</em> and so can't comment competently, but I agree that symmetry seems like not the kind of thing I'm looking for when assessing the moral importance of a physical system, or at least it's not more than one small part of what I'm looking for. Most of what I care about is at the level of ordinary cognitive science, such as mental representations, behaviors, learning, preferences, introspective abilities, etc.</p>\n<p>That said, I do think theories like IIT are at least slightly useful insofar as they expand our vocabulary and provide additional metrics that we might care a little bit about.</p>\n", "parentCommentId": "6Lrwqcdx86DJ9sXmw", "user": {"username": "Brian_Tomasik"}}, {"_id": "d3fgd3729oZHF97ke", "postedAt": "2017-07-31T13:29:48.247Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>That said, I do think theories like IIT are at least slightly useful insofar as they expand our vocabulary and provide additional metrics that we might care a little bit about.</p>\n</blockquote>\n<p>If you expanded on this, I would be interested.</p>\n", "parentCommentId": "okFKJRhsfT5jtGgPg", "user": {"username": "AlexMennen"}}, {"_id": "dBuHvoB7r6i7vyQ3J", "postedAt": "2017-07-31T17:32:40.690Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I think that's fair-- beneficial equilibriums could depend on reifying things like this.</p>\n<p>On the other hand, I'd suggest that with regard to identifying entities that can suffer, false positives are much less harmful than false negatives but they still often incur a cost. E.g., I don't think corporations can suffer, so in many cases it'll be suboptimal to grant them the sorts of protections we grant humans, apes, dogs, and so on. Arguably, a substantial amount of modern ethical and perhaps even political dysfunction is due to not kicking leaky reifications out of our circle of caring. (This last bit is intended to be provocative and I'm not sure how strongly I'd stand behind it...)</p>\n", "parentCommentId": "KWM9LKr9q27rPPfZu", "user": {"username": "MikeJohnson"}}, {"_id": "yxJAaWyYQeSBaSLWd", "postedAt": "2017-07-31T18:34:36.533Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>Speaking of the metaphysical correctness of claims about qualia sounds confused, and I think precise definitions of qualia-related terms should be judged by how useful they are for generalizing our preferences about central cases.</p>\n</blockquote>\n<p>I agree a good theory of qualia should help generalize our preferences about central cases. I disagree that we can get there with the assumption that qualia are intrinsically vague/ineffable. My critique of analytic functionalism is that it is essentially nothing <em>but</em> an assertion of this vagueness.</p>\n<blockquote>\n<p>Regarding the objection that shaking a bag of popcorn can be interpreted as carrying out an arbitrary computation, I'm not convinced that this is actually true, and I suspect it isn't. </p>\n</blockquote>\n<p>Without a bijective mapping between physical states/processes and computational states/processes, I think my point holds. I understand it's counterintuitive, but we should expect that when working in these contexts.</p>\n<blockquote>\n<p>I think the edge cases that you quote Scott Aaronson bringing up are good ones to think about, and I do have a large amount of moral uncertainty about them. But I don't see these as problems specific to analytic functionalism. These are hard problems, and the fact that some more precise theory about qualia may be able to easily answer them is not a point in favor of that theory, since wrong answers are not helpful.</p>\n</blockquote>\n<p>Correct; they're the sorts of things a theory of qualia <em>should</em> be able to address- necessary, not sufficient.</p>\n<p>Re: your comments on the Symmetry Theory of Valence, I feel I have the advantage here since you haven't read the work. Specifically, it feels as though you're pattern-matching me to IIT and channeling Scott Aaronson's critique of Tononi, which is a bit ironic since that forms a significant part of PQ's argument why an IIT-type approach can't work.</p>\n<p>At any rate I'd be happy to address <em>specific</em> criticism of my work. This is obviously a complicated topic and informed external criticism is always helpful. At the same time, I think it's a bit tangential to my critique about FRI's approach: as I noted, </p>\n<blockquote>\n<p>I mention all this because I think analytic functionalism- which is to say radical skepticism/eliminativism, the metaphysics of last resort- only looks as good as it does because nobody\u2019s been building out any alternatives.</p>\n</blockquote>\n", "parentCommentId": "6Lrwqcdx86DJ9sXmw", "user": {"username": "MikeJohnson"}}, {"_id": "rEmLcqASrqH2kJcon", "postedAt": "2017-08-01T09:31:46.111Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I didn't have in mind anything profound. :) The idea is just that &quot;degree of information integration&quot; is one interesting metric along which to compare minds, along with metrics like &quot;number of neurons&quot;, &quot;number of synapses&quot;, &quot;number of ATP molecules consumed per second&quot;, &quot;number of different brain structures&quot;, &quot;number of different high-level behaviors exhibited&quot;, and a thousand other similar things.</p>\n", "parentCommentId": "d3fgd3729oZHF97ke", "user": {"username": "Brian_Tomasik"}}, {"_id": "QthK2e53uu6ZeqKsa", "postedAt": "2017-08-01T20:22:51.097Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>My critique of analytic functionalism is that it is essentially nothing but an assertion of this vagueness.</p>\n</blockquote>\n<p>That's no reason to believe that analytic functionalism is wrong, only that it is not sufficient by itself to answer very many interesting questions.</p>\n<blockquote>\n<p>Without a bijective mapping between physical states/processes and computational states/processes, I think my point holds.</p>\n</blockquote>\n<p>No, it doesn't. I only claim that most physical states/processes have only a very limited collection of computational states/processes that it can reasonably be interpreted as, not that every physical state/process has exactly one computational state/process that it can reasonably be interpreted as, and certainly not that every computational state/process has exactly one physical state/process that can reasonably be interpreted as it. Those are totally different things.</p>\n<blockquote>\n<p>it feels as though you're pattern-matching me to IIT and channeling Scott Aaronson's critique of Tononi</p>\n</blockquote>\n<p>Kind of. But to clarify, I wasn't trying to argue that there will be problems with the Symmetry Theory of Valence that derive from problems with IIT. And when I heard about IIT, I figured that there were probably trivial counterexamples to the claim that Phi measures consciousness and that perhaps I could come up with one if I thought about the formula enough, before Scott Aaronson wrote the blog post where he demonstrated this. So although I used that critique of IIT as an example, I was mainly going off of intuitions I had prior to it. I can see why this kind of very general criticism from someone who hasn't read the details could be frustrating, but I don't expect I'll look into it enough to say anything much more specific.</p>\n<blockquote>\n<p>I mention all this because I think analytic functionalism- which is to say radical skepticism/eliminativism, the metaphysics of last resort- only looks as good as it does because nobody\u2019s been building out any alternatives.</p>\n</blockquote>\n<p>But people have tried developing alternatives to analytic functionalism.</p>\n", "parentCommentId": "yxJAaWyYQeSBaSLWd", "user": {"username": "AlexMennen"}}, {"_id": "4sB3jQNWXYnrP8pH9", "postedAt": "2017-08-01T21:07:05.476Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>That's no reason to believe that analytic functionalism is wrong, only that it is not sufficient by itself to answer very many interesting questions.</p>\n</blockquote>\n<p>I think that's being generous to analytic functionalism. As I suggested in Objection 2,</p>\n<blockquote>\n<p>In short, FRI\u2019s theory of consciousness isn\u2019t actually a theory of consciousness at all, since it doesn\u2019t do the thing we need a theory of consciousness to do: adjudicate disagreements in a principled way. Instead, it gives up any claim on the sorts of objective facts which could in principle adjudicate disagreements.</p>\n</blockquote>\n<p>.</p>\n<blockquote>\n<p>I only claim that most physical states/processes have only a very limited collection of computational states/processes that it can reasonably be interpreted as[.]</p>\n</blockquote>\n<p>I'd like to hear more about this claim; I don't think it's ridiculous on its face (per Brian's and Michael_PJ's comments), but it seems a lot of people have banged their head against this without progress, and my prior is formalizing this is a lot harder than it looks (it may be unformalizable). If you <em>could</em> formalize it, that would have a lot of value for a lot of fields.</p>\n<blockquote>\n<p>So although I used that critique of IIT as an example, I was mainly going off of intuitions I had prior to it. I can see why this kind of very general criticism from someone who hasn't read the details could be frustrating, but I don't expect I'll look into it enough to say anything much more specific.</p>\n</blockquote>\n<p>I don't expect you to either. If you're open to a suggestion about how to approach this in the future, though, I'd offer that if you don't feel like reading something but still want to criticize it, instead of venting your intuitions (which could be valuable, but don't seem calibrated to the actual approach I'm taking), you should press for concrete predictions. </p>\n<p>The following phrases seem highly anti-scientific to me: </p>\n<blockquote>\n<p>sounds wildly implausible | \nThese sorts of theories never end up getting empirical support, although their proponents often claim to have empirical support | \nI won't be at all surprised if you claim to have found substantial empirical support for your theory, and I still won't take your theory at all seriously if you do, because any evidence you cite will inevitably be highly dubious | \nThe heuristic that claims that a qualia-related concept is some simple other thing are wrong, and that claims of empirical support for such claims never hold up | \nI am almost certain that there are trivial counterexamples to the Symmetry Theory of Valence</p>\n</blockquote>\n<p>I.e., these statements seem to lack epistemological rigor, and seem to absolutely prevent you from updating in response to <em>any</em> evidence I might offer, even in principle (i.e., they're actively hostile to your improving your beliefs, regardless of whether I am or am not correct).</p>\n<p>I don't think your intention is to be closed-minded on this topic, and I'm not saying I'm certain STV is correct. Instead, I'm saying you seem to be overreacting to some stereotype you initially pattern-matched me as, and I'd suggest talking about predictions is probably a much healthier way to move forward if you want to spend more time on this. (Thanks!)</p>\n", "parentCommentId": "QthK2e53uu6ZeqKsa", "user": {"username": "MikeJohnson"}}, {"_id": "GJZZmhWMJm4hrmupd", "postedAt": "2017-08-02T09:11:56.638Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>I only claim that most physical states/processes have only a very limited collection of computational states/processes that it can reasonably be interpreted as[.]</p>\n</blockquote>\n<p>I haven't read most of <a href=\"http://www.whitschonbein.com/documents/schonbein-2017-counting-argument.pdf\">this paper</a>, but it seems to argue that.</p>\n", "parentCommentId": "4sB3jQNWXYnrP8pH9", "user": {"username": "Brian_Tomasik"}}, {"_id": "XsA3QQCAjHDYA3PfH", "postedAt": "2017-08-02T18:55:02.167Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>The counterfactual response is typically viewed as inadequate in the face of triviality\narguments. However, when we count the number of automata permitted\nunder that response, we find it succeeds in limiting token physical systems to\nrealizing at most a vanishingly small fraction of the computational systems they\ncould realize if their causal structure could be \u2018repurposed\u2019 as needed. Therefore,\nthe counterfactual response is a prima facie promising reply to triviality\narguments.\nSomeone might object this result nonetheless does not effectively handle\nthe metaphysical issues raised by those arguments. Specifically, an \u2018absolutist\u2019\nregarding the goals of an account of computational realization might hold that\nany satisfactory response to triviality arguments must reduce the number of\npossibly-realized computational systems to one, or to some number close to\none. While the counterfactual response may eliminate the vast majority of\ncomputational systems from consideration, in comparison to any small constant,\nthe number of remaining possibly-realized computational systems is still too high\n(2^n).</p>\n</blockquote>\n<p>That seems like a useful approach- in particular,</p>\n<blockquote>\n<p>On the other hand, the argument suggests at least some computational hypotheses\nregarding cognition are empirically substantive: by identifying types of computation characteristic of cognition (e.g., systematicity, perhaps), we limit potential cognitive devices to those whose causal structure includes these types of computation in the sets of possibilities they support.</p>\n</blockquote>\n<p>This does seem to support the idea that progress can be made on this problem! On the other hand, the author's starting assumption is we can treat a physical system as a computational (digital) automata, which seems like a pretty big assumption.</p>\n<p>I think this assumption may or may not turn out to be <em>ultimately</em> true (Wolfram et al), but given current theory it seems difficult to reduce actual physical systems to computational automata in practice. In particular, it seems difficult to apply this framework to (1) quantum systems (which all physical systems ultimately are), and (2) biological systems which have messy levels of abstraction such as the brain (which we'd want to be able to do for the purposes of functionalism).</p>\n<p>From a physics perspective, I wonder if we could figure out a way to feed in a bounded wavefunction, and get identify some minimum upper bound of reasonable computational interpretations of the system. My instinct is that David Deutsch might be doing relevant work? But I'm not at all sure of this.</p>\n", "parentCommentId": "GJZZmhWMJm4hrmupd", "user": {"username": "MikeJohnson"}}, {"_id": "qsbHF9ycuJhZGcMMg", "postedAt": "2017-08-02T22:30:59.316Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>You may also like <a href=\"http://kybele.psych.cornell.edu/~edelman/Fekete-Edelman-computational-theory-of-conscious-experience-ConCog11.pdf\">Towards a computational theory of experience</a> by Fekete and Edelman- here's their setup:</p>\n<blockquote>\n<p>3.4. Counterfactually stable account of implementation\nTo claim a computational understanding of a system, it is necessary for us to be able to map its instantaneous states and variables to those of a model. Such a mapping is, however, far from sufficient to establish that the system is actually implementing the model: without additional constraints, a large enough conglomerate of objects and events can be mapped so as to realize any arbitrary computation (Chalmers, 1994; Putnam, 1988). A careful analysis of what it means for a physical system to implement an abstract computation (Chalmers, 1994; Maudlin, 1989) suggests that, in addition to specifying a mapping between the respective instantaneous states of the system and the computational model, one needs to spell out the rules that govern the causal transitions between corresponding instantaneous states in a counterfactually resistant manner.</p>\n</blockquote>\n<blockquote>\n<p>In the case of modeling phenomenal experience, the stakes are actually much higher: one expects a model of qualia to be not merely good (in the sense of the goodness of fit between the model and its object), but true and unique. Given that a multitude of distinct but equally good computational models may exist, why is not the system realizing a multitude of different experiences at a given time? Dodging this question amounts to conceding that computation is not nomologically related to qualia.</p>\n</blockquote>\n<blockquote>\n<p>Construing computation in terms of causal interactions between instantaneous states and variables of a system has ramifications that may seem problematic for modeling experience. If computations and their implementations are individuated in terms of causal networks, then any given, specific experience or quale is individuated (in part) by the system\u2019s entire space of possible instantaneous states and their causal interrelationships. In other words, the experience that is unfolding now is defined in part by the entire spectrum of possible experiences available to the system.</p>\n</blockquote>\n<blockquote>\n<p>In subsequent sections, we will show that this explanatory problem is not in fact insurmountable, by outlining a solution for it. Meanwhile, we stress that while computation can be explicated by numbering the instantaneous states of a system and listing rules of transition between these states, it can also be formulated equivalently in dynamical terms, by defining (local) variables and the dynamics that govern their changes over time. For example, in neural-like models computation can be explicated in terms of the instantaneous state of \u2018\u2018representational units\u2019\u2019 and the differential equations that together with present input lead to the unfolding of each unit\u2019s activity over time. Under this description, computational structure results entirely from local physical interactions.</p>\n</blockquote>\n<p>It's a little bit difficult to parse <em>precisely how</em> they believe they solve the multiple realization of computational interpretations of a system, but the key passage seems to be:</p>\n<blockquote>\n<p>Third, because of multiple realizability of computation, one computational process or system can represent another, in that a correspondence can be drawn between certain organizational aspects of one process and those of the other. In the simplest representational scenario, correspondence holds between successive states of the two processes, as well as between their respective timings. In this case, the state-space trajectory of one system unfolds in lockstep with that of the other system, because the dynamics of the two systems are sufficiently close to one another; for example, formal neurons can be wired up into a network whose dynamics would emulate (Grush, 2004) that of the falling rock mentioned above. More interesting are cases in which the correspondence exists on a more abstract level, for instance between a certain similarity structure over some physical variables \u2018\u2018out there\u2019\u2019 in the world (e.g., between objects that fall like a rock and those that drift down like a leaf) and a conceptual structure over certain instances of neural activity, as well as cases in which the system emulates aspects of its own dynamics. Further still, note that once representational mechanisms have been set in place, they can also be used \u2018\u2018offline\u2019\u2019 (Grush, 2004). In all cases, the combinatorics of the world ensures that the correspondence relationship behind instances of representation is highly non-trivial, that is, unlikely to persist purely as a result of a chance configurational alignment between two randomly picked systems (Chalmers, 1994).</p>\n</blockquote>\n<p>My attempt at paraphrasing this: if we can model the evolution of a physical system and the evolution of a computational system with the same phase space for some finite time <em>t</em>, then as <em>t</em> increases we can be increasingly confident the physical system is instantiating this computational system. At the limit (<em>t</em>-&gt;\u221e), this may offer a method for uniquely identifying which computational system a physical system is instantiating.</p>\n<p>My intuition here is that the closer they get to solving the problem of how to 'objectively' determine what computations a physical system is realizing, the further their framework will stray from the Turing paradigm of computation and the closer it will get to a hypercomputation paradigm (which in turn may essentially turn out to be isomorphic to physics). But, I'm sure I'm biased, too. :) Might be worth a look.</p>\n", "parentCommentId": "GJZZmhWMJm4hrmupd", "user": {"username": "MikeJohnson"}}, {"_id": "vgnAYk6tg9BAjc9Jp", "postedAt": "2017-08-03T21:42:16.999Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Yeah, S-risk minimizer being trivially exploitable etc.</p>\n", "parentCommentId": "dBuHvoB7r6i7vyQ3J", "user": {"username": "RomeoStevens"}}, {"_id": "8jQ5RGTfqmLxSRpZi", "postedAt": "2017-08-10T00:57:21.692Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>This is a super interesting article, but...</p>\n<blockquote>\n<p>I worry that FRI\u2019s work leans on the intuition that suffering is real and we can speak coherently about it, to a degree greater than its metaphysics formally allow.</p>\n</blockquote>\n<p>To me, it reads like it was written by someone who has never really encountered suffering.</p>\n<p><a href=\"http://www.mattball.org/2014/11/excerpts-from-letter-to-young-matt.html\">http://www.mattball.org/2014/11/excerpts-from-letter-to-young-matt.html</a></p>\n", "parentCommentId": null, "user": {"username": "MattBall"}}, {"_id": "mdpcYQSWqHjjhNFdh", "postedAt": "2017-08-16T13:12:49.744Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>MIRI/FHI have never published anything which talks about any view of consciousness. There is a huge difference between inferring based on things that people happen to write outside of the organization, and the actual research being published by the organization. In the second case, it's relevant to the research, whether it's an official value of the organization or not. In the first case, it's not obvious why it's relevant at all.</p>\n<p>Luke affirmed elsewhere that Open Phil really heavily leans towards his view on consciousness and moral status. </p>\n", "parentCommentId": "LPmyXWnYWQRNRSnhH", "user": {"username": "kbog"}}, {"_id": "gH9RjgWpvakCY5QdX", "postedAt": "2017-08-21T11:48:23.378Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>I've had a look into Dayan's <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1460-9568.2012.08026.x/epdf\">suggested</a> <a href=\"https://www.princeton.edu/~yael/Publications/NivEtAl2006b.pdf\">papers</a> - they imply an interesting theory. I'll put my thoughts here so the discussion can be public. The theory contradicts the one you link above where the separation between pain and pleasure is a contingency of how our brain works.</p>\n<p>You've <a href=\"https://arxiv.org/abs/1410.8233\">written</a> about another (very intuitive) theory, where the zero-point is where you'd be indifferent between prolonging and ending your life:</p>\n<p>&quot;This explanation may sound plausible due to its analogy to familiar concepts, but it seems to place undue weight on whether an agent\u2019s lifetime is fixed or variable. Yet I would still feel pain and pleasure as being distinct even if I knew exactly when I would die, and a simple RL agent has no concept of death to begin with.&quot;</p>\n<p>Dayan's research suggests that the zero-point will also come up in many circumstances relating to opportunity costs which would deal with that objection. To simplify, let's say the agent expects a fixed average rate of return rho for the foreseeable future. It is faced with a problem where it can either act fast (high energy expenditure) or act slowly (high opportunity costs as it won't get the average return for a while). If rho is negative or zero, there is no need to act quickly at all because there are not opportunity costs. But the higher the opportunity costs get, the fast the agent will want to be at getting its average reward back so it will act quickly despite the immediate cost.</p>\n<p>The speed with which the agent acts is called vigour in Dayan's research. The agent's vigour mathematically implies an average rate of return if the agent is rational. There can be other reasons for low vigour such as a task that requires patience - they have some experiments <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1460-9568.2012.08026.x/epdf\">here</a> in figure 1. In their experiment the optimal vigour (one over tao*) is proportional to the square root of the average return. A <a href=\"https://www.nature.com/articles/srep42287\">recent</a> paper has confirmed the predictions of this model in humans.</p>\n<p>So when is an agent happy according to this model?</p>\n<p>The model would imply that the agent has positive welfare positive welfare when the agent treats it as creating positive opportunity costs while it's doing other things (and vice versa for negative welfare). This would also apply to your example where the agent expends resources to increase or decrease its life-time.</p>\n<p>What I like about this is that the welfare depends on the agent's behaviour and not the way the rewards are internally processed and represented as numbers which is arbitrary.</p>\n<p>I'm still not sure how you would go about calculating the welfare of an agent if you don't have a nice experimental setup like Dayan's. That might be amenable to more thinking. Moreover, all welfare is still relative and it doesn't allow comparisons between agents.</p>\n<p>Edit: I'm not sure though if there's a problem because we now have to assume that the 'inactive' time where the agent doesn't get its average reward is the zero-baseline which is also arbitrary.</p>\n", "parentCommentId": "oCqTLfgoYQjmyeqWR", "user": null}, {"_id": "MPB5t7wsxsFaAfR7q", "postedAt": "2017-08-26T04:23:02.358Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Thanks!! Interesting. I haven't read the linked papers, so let me know if I don't understand properly (as I probably don't).</p>\n<p>I've always thought of simple RL agents as getting a reward at fixed time intervals no matter what they do, in which case they can't act faster or slower. For example, if they skip pressing a lever, they just get a reward of 0 for that time step. Likewise, in an actual animal, the animal's reward neurons don't fire during the time when the lever isn't being pressed, which is equivalent to a reward of 0.</p>\n<p>Of course, animals would prefer to press the lever more often to get a positive reward rather than a reward of 0, but this would be true whether the lever gave positive reward or merely relief from punishment. For example, maybe the time between lever presses is painful, and the pressed lever is merely less painful. This could be the experience of, e.g., a person after a breakup <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/HeartbreakAndIceCream\">consuming ice cream scoops</a> at a higher rate than normal to escape her pain: even with the increased rate of ice cream intake, she may still have negative welfare, just less negative. It seems like vigor just says that what you're doing is better than not doing it?</p>\n<p>For really simple RL agents like those living in Grid World, there is no external clock. Time is sort of defined by when the agent takes its next step. So it's again not clear if a &quot;rate of actions&quot; explanation can help here (but if it helps for more realistic RL agents, that's cool!).</p>\n<p><a href=\"https://ai.stackexchange.com/questions/2226/definition-of-time-step-in-a-mdp/2300#2300\">This answer</a> says that for a Markov Decision Process, &quot;each action taken is done in a time step.&quot; So it seems like a time step is <em>defined</em> as the interval between one action and the next?</p>\n", "parentCommentId": "gH9RjgWpvakCY5QdX", "user": {"username": "Brian_Tomasik"}}, {"_id": "dAhQT4ZR9EWQTvYbK", "postedAt": "2017-08-26T10:11:51.778Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Thanks for the reply. I think I can clarify the issue about discrete time intervals. I'd be curious on your thoughts on the last sentence of my comment above if you have any.</p>\n<p><strong>Discrete time</strong></p>\n<blockquote>\n<p>So it seems like a time step is defined as the interval between one action and the next?</p>\n</blockquote>\n<p>Yes. But in a SEMI or a <a href=\"continuous-time\">https://en.wikipedia.org/wiki/Markov_decision_process#Continuous-time_Markov_Decision_Process</a> Markov Decision Process (SMDP) this is not the case. SMDPs allow temporally extended actions and are commonly used in RL research. Dayan's papers use a continuous SMDP. You can still have RL agents in this formalism and it tracks our situation more closely. But I don't think the formalism matters for our discussion because you can arbitrarily approximate any formalism with a standard MDP - I'll explain below.</p>\n<p>The continuous-time experiment looks roughly like this: Imagine you're in a room and you have to press a lever to get out - and get back to what you would normally be doing and get an average reward rho per second. However, the lever is hard to press. You can press it hard and fast or light and slowly, taking a total time T to complete the press. The total energy cost of pressing is 1/T so ideally you'd press very slowly but that would mean you couldn't be outside the room during that time (opportunity costs).</p>\n<p>In this setting, the 'action' is just the time T that you to press the lever. We can easily approximate this with a standard MDP. E.g. you could take action 1 which completely presses the lever in one time step, costing you 1/1=1 reward in energy. Or you could take action 2, which you would have to take twice to complete the press, costing you only 1/2 reward (so 1/4 for each time you take action 2). And so forth. Does that make sense?</p>\n<p><strong>Zero point</strong></p>\n<p>Of course, if you don't like it outside the room at all, you'll never press the lever - so there is a 'zero point' in terms of how much you like it outside. Below that point you'll never press the lever.</p>\n<blockquote>\n<p>It seems like vigor just says that what you're doing is better than not doing it?</p>\n</blockquote>\n<p>I'm not entirely sure what you mean, but I'll clarify that acting vigorously doesn't say anything about whether the agent is currently happy. It may well act vigorously just to escape punishment. Similarly, an agent that currently works to increase its life-time doesn't necessarily feel good, but its work still implies that it thinks the additional life-time it gets will be good.</p>\n<p>But I think your criticism may be the same as what I said in the edit above - that there is an unwarranted assumption that the agent is at the zero-point before it presses the lever. In the experiments this is assumed because there are no food rewards or shocks during that time. But you could still imaging that a depressed rat would feel bad anyway. </p>\n<p>The theory that assumes nonexistence is the zero-point kind of does the same thing though. Although nonexistence is arguably a definite zero-point, the agent's utility function might still go beyond its life-time...</p>\n<p>Does this clarify the case?</p>\n", "parentCommentId": "MPB5t7wsxsFaAfR7q", "user": null}, {"_id": "fazBQ6WGYdDhcrmrq", "postedAt": "2017-08-26T23:24:02.745Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Your explanation was clear. :)</p>\n<blockquote>\n<p>acting vigorously doesn't say anything about whether the agent is currently happy</p>\n</blockquote>\n<p>Yeah, I guess I meant the trivial observation that you act vigorously if you judge that doing so has higher expected total discounted reward than not doing so. But this doesn't speak to whether, after making that vigorous effort, your experiences will be net positive; they might just be less negative.</p>\n<blockquote>\n<p>Of course, if you don't like it outside the room at all, you'll never press the lever - so there is a 'zero point' in terms of how much you like it outside.</p>\n</blockquote>\n<p>...assuming that sticking around inside the room is neutral. This gets back to the &quot;unwarranted assumption that the agent is at the zero-point before it presses the lever.&quot;</p>\n<blockquote>\n<p>The theory that assumes nonexistence is the zero-point kind of does the same thing though.</p>\n</blockquote>\n<p>Hm. :) I feel like there's a difference between (a) an agent inside the room who hasn't yet pressed the lever to get out and (b) the agent not existing at all. For (a), it seems we ought to be able to give a (qualia and morally nonrealist) answer about whether its experiences are positive or negative or neutral, while for (b), such a question seems misplaced.</p>\n<p>If it were a human in the room, we could ask that person whether her experiences before lever pressing were net positive or negative. I guess such answers could vary a lot between people based on various cultural, psychological, etc. factors unrelated to the activity level of reward networks. If so, perhaps one position could be that the distinction between positive vs. negative welfare is a pretty anthropomorphic concept that doesn't travel well outside of a cognitive system capable of making these kinds of judgments. Intuitively, I feel like there is more to the sign of one's welfare than these high-level, potentially idiosyncratic evaluations, but it's hard to say what.</p>\n<p>I suppose another approach could be to say that the person in the room definitely is at welfare 0 (by fiat) based on lack of reward or punishment signals, regardless of how the person evaluates her welfare verbally.</p>\n", "parentCommentId": "dAhQT4ZR9EWQTvYbK", "user": {"username": "Brian_Tomasik"}}, {"_id": "NmjaaXjwo3TKEhPwn", "postedAt": "2017-08-27T11:54:13.991Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>I feel like there's a difference between (a) an agent inside the room who hasn't yet pressed the lever to get out and (b) the agent not existing at all.</p>\n</blockquote>\n<p>Yes that's probably the right way to think about it. I'm also considering an alternative though: Since we're describing the situation with a simple computational model we shouldn't assume that there's anything going on that isn't captured by the model. E.g. if the agent in the room is depressed, it will be performing 'mental actions' - imagining depressing scenarios etc. But we may have to assume that away, similar to how high school physics would assume no friction etc.</p>\n<p>So we're left with an agent that decides initially that it won't do anything at all (not even updating its beliefs) because it doesn't want to be outside of the room and then remains inactive. The question arises if that's an agent at all and if it's meaningfully different unconsciousness.</p>\n", "parentCommentId": "fazBQ6WGYdDhcrmrq", "user": null}, {"_id": "avjK8XLW8JpMLijxx", "postedAt": "2017-08-27T23:42:29.462Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<blockquote>\n<p>So we're left with an agent that decides initially that it won't do anything at all (not even updating its beliefs) because it doesn't want to be outside of the room and then remains inactive. The question arises if that's an agent at all and if it's meaningfully different unconsciousness.</p>\n</blockquote>\n<p>Hm. :) Well, what if the agent did do stuff inside the room but still decided not to go out? We still wouldn't be able to tell if it was experiencing net positive, negative, or neutral welfare. Examples:</p>\n<ol>\n<li><p>It's winter. The agent is cold indoors and is trying to move to the warm parts of the room. We assume its welfare is net negative. But it doesn't go outside because it's even colder outside.</p>\n</li>\n<li><p>The agent is indoors having a party. We assume it's experiencing net positive welfare. It doesn't want to go outside because the party is inside.</p>\n</li>\n</ol>\n<p>We can reproduce the behavior of these agents with reward/punishment values that are all positive numbers, all negative numbers, or a combination of the two. So if we omit the higher-level thoughts of the agents and just focus on the reward numbers at an abstract level, it doesn't seem like we can meaningfully distinguish positive or negative welfare. Hence, the sign of welfare must come from the richer context that our human-centered knowledge and evaluations bring?</p>\n<p>Of course, qualia nonrealists already knew that the sign and magnitude of an organism's welfare are things we make up. But most people can agree upon, e.g., the sign of the welfare of the person at the party. In contrast, there doesn't seem to be a principled way that most people would agree upon for us to attribute a sign of welfare to a simple RL agent that reproduces the high-level behavior of the person at the party.</p>\n", "parentCommentId": "NmjaaXjwo3TKEhPwn", "user": {"username": "Brian_Tomasik"}}, {"_id": "yCvizLNNHpjboEDcD", "postedAt": "2017-09-20T20:18:50.473Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>After some clarification Dayan thinks that vigour is not the thing I was looking for.</p>\n<p>We discussed this a bit further and he suggested that the temporal difference error does track pretty closely what we mean by happiness/suffering, at least as far as the zero point is concerned. Here's a <a href=\"https://www.researchgate.net/publication/264501328_A_computational_and_neural_model_of_momentary_subjective_well-being\">paper</a> making the case (but it has limited scope IMO).</p>\n<p>If that's true, we wouldn't need e.g. the theory that there's a zero point to keep firing rates close to zero.</p>\n<p>The only problem with TD errors seems to be that they don't account for the difference between wanting and liking. But it's currently just unresolved what the function of liking is. So I came away with the impression that liking vs wanting and not the zero point is the central question.</p>\n<p>I've seen one paper suggesting that liking is basically the consumption of rewards, which would bring us back to the question of the zero point though. But we didn't find that theory satisfying. E.g. food is just a proxy for survival. And as the paper I linked shows, happiness can follow TD errors even when no rewards are consumed.</p>\n<p>Dayan mentioned that liking may even be an epiphenomenon of some things that are going on in the brain when we eat food/have sex etc, similar to how the specific flavour of pleasure we get from listening to music is such an epiphenomenon. I don't know if that would mean that liking has no function.</p>\n<p>Any thoughts?</p>\n", "parentCommentId": "avjK8XLW8JpMLijxx", "user": null}, {"_id": "wD7ogjjwHrSgYzqTp", "postedAt": "2017-09-21T03:37:07.803Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Interesting. :)</p>\n<p><a href=\"https://arxiv.org/abs/1505.04497\">Daswani and Leike (2015)</a> also define (p. 4) happiness as the temporal difference error (in an MDP), and for model-based agents, the definition is, in my interpretation, basically the common Internet slogan that &quot;happiness = reality - expectations&quot;. However, the authors point out (p. 2) that pleasure = reward != happiness. This still leaves open the issue of what <em>pleasure</em> is.</p>\n<p>Personally I think pleasure is more morally relevant. In <a href=\"https://arxiv.org/abs/1410.8233\">Tomasik (2014)</a>, I wrote (p. 11):</p>\n<blockquote>\n<p>After training, dopamine spikes when a cue appears signaling that a reward will arrive, not when the reward itself is consumed [Schultz et al., 1997], but we know subjectively that the main pleasure of a reward comes from consuming it, not predicting it. In other words, in equation (1), the pleasure comes from the actual reward r, not from the amount of dopamine \u03b4.</p>\n</blockquote>\n<p>In <a href=\"https://www.facebook.com/brian.tomasik/posts/726995353362\">this post</a> commenting on Daswani and Leike (2015), I said:</p>\n<blockquote>\n<p>I personally don't think the definition of &quot;happiness&quot; that Daswani and Leike advance is the most morally relevant one, but the authors make an interesting case for their definition. I think their definition corresponds most closely with &quot;being pleased of one's current state in a high-level sense&quot;. In contrast, I think raw pleasure/pain is most morally significant. As a simple test, ask whether you'd rather be in a state where you've been unexpectedly notified that you'll get a cookie in a few minutes or whether you'd rather be in the state where you actually eat the cookie after having been notified a few minutes earlier. Daswani and Leike's definition considers being notified about the cookie to be happiness, while I think eating the cookie has more moral relevance.</p>\n</blockquote>\n<hr />\n<blockquote>\n<p>Dayan mentioned that liking may even be an epiphenomenon of some things that are going on in the brain when we eat food/have sex etc, similar to how the specific flavour of pleasure we get from listening to music is such an epiphenomenon.</p>\n</blockquote>\n<p>I'm not sure I understand, but I wrote a quick thing <a href=\"http://reducing-suffering.org/why-organisms-feel-both-suffering-and-happiness/#Pleasure_vs_pain_red_vs_blue\">here</a> inspired by this comment. Do you think that's what he meant? If so, may I attribute him/you for the idea? It seems fairly plausible. :) Studying what separates red from blue might help shine light on this topic.</p>\n", "parentCommentId": "yCvizLNNHpjboEDcD", "user": {"username": "Brian_Tomasik"}}, {"_id": "5w9mmMEWAwXCQBHcv", "postedAt": "2018-01-17T21:58:50.817Z", "postId": "FfJ4rMTJAB3tnY5De", "htmlBody": "<p>Aaronson's &quot;Is 'information is physical' contentful?&quot; also seems relevant to this discussion (though I'm not sure exactly how to apply his arguments):</p>\n<blockquote>\n<p>But we should\u2019ve learned by now to doubt this sort of argument.  There\u2019s no general principle, in our universe, saying that you can hide as many bits as you want in a physical object, without those bits influencing the object\u2019s observable properties.  On the contrary, in case after case, our laws of physics seem to be intolerant of \u201cwallflower bits,\u201d which hide in a corner without talking to anyone.  If a bit is there, the laws of physics want it to affect other nearby bits and be affected by them in turn.\n...\nIn summary, our laws of physics are structured in such a way that even pure information often has \u201cnowhere to hide\u201d: if the bits are there at all in the abstract machinery of the world, then they\u2019re forced to pipe up and have a measurable effect.  And this is not a tautology, but comes about only because of nontrivial facts about special and general relativity, quantum mechanics, quantum field theory, and thermodynamics.  And this is what I think people should mean when they say \u201cinformation is physical.\u201d</p>\n</blockquote>\n<p><a href=\"https://www.scottaaronson.com/blog/?p=3327\">https://www.scottaaronson.com/blog/?p=3327</a></p>\n", "parentCommentId": "GJZZmhWMJm4hrmupd", "user": {"username": "MikeJohnson"}}]