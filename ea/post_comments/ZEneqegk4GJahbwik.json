[{"_id": "ivStwjKjLjreybaaZ", "postedAt": "2024-03-08T23:12:05.140Z", "postId": "ZEneqegk4GJahbwik", "htmlBody": "<p>Thanks for the interesting post. I'm working on something like what you describe in your \"Going Forward\" section: <a href=\"https://www.lesswrong.com/posts/gz7eSELFcRYJLdnkE/towards-an-ethics-calculator-for-use-by-an-agi\">https://www.lesswrong.com/posts/gz7eSELFcRYJLdnkE/towards-an-ethics-calculator-for-use-by-an-agi</a></p><p>Planning to post an update about my progress on lesswrong soon.</p>", "parentCommentId": null, "user": {"username": "Sean Sweeney"}}, {"_id": "gv5mnYkqbRq2qBrZa", "postedAt": "2024-03-09T13:39:41.769Z", "postId": "ZEneqegk4GJahbwik", "htmlBody": "<blockquote><p>In our work at QURI, this is important because we want to programmatically encode utility functions and use them directly for decision-making</p></blockquote><p>I find this ambition a little concerning, but it could be that I'm reading it wrong. In my mind, the most dangerous possible design for an AI (or an organisation) would be a fixed goal utility function maximiser. For explanations of why, see <a href=\"https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy\">this post</a>, <a href=\"https://titotal.substack.com/p/why-agi-systems-will-not-be-fanatical\">this post</a>, and <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">this post</a>. Such an entity could pursue it's \"number go up\" until the destruction of the universe. Current AI's do not fit this design, so why would you seek to change that?&nbsp;</p><p>Am I misunderstanding this completely?&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "dspnRQanhZhmHdyLX", "postedAt": "2024-03-09T14:37:44.655Z", "postId": "ZEneqegk4GJahbwik", "htmlBody": "<p>Good question! As I tried describing in this post (though I really didn't get in the details of specific QURI plans), there are <i>many</i> sorts of utility functions you can use, and many ways of optimizing over them.</p><p>Using some of my terminology above, I think a lot of people here think of advanced AIs as applying a highly <i>prescriptive, deliberation-extrapolated </i>utility function, with a great deal of optimization power, particularly in situations where there's very little ability to account for utility-function uncertainty. I agree that this is scary and a bad idea, especially in situations where we have little experience in learning to optimize for any sort of explicit utility function.&nbsp;</p><p>But again, utility functions and their optimization can be far more harmless than this.&nbsp;</p><p>Very simple examples of (partial) utility functions include:</p><ul><li>For each animal/being, how much should EA donors value one of their life-years?</li><li>For each of [set of EA projects], how valuable did it seem to be?</li><li>For each of [a long list of potential life hacks], what is the expected value?</li></ul><p>I believe these lists can satisfy the core tenants of Von Neumann\u2013Morgenstern utility functions, but I don't think that many people here would consider \"trying to make these lists using reasonable measures, and generally then taking decisions based on them\" to be particularly scary or controversial.&nbsp;<br><br>In the limit, I could imagine people saying, \"I think that <i>prescriptive, deliberation-extrapolated utility function, with a great deal of optimization power</i> is scary, so we should never optimize or improve anything that's technically any sort of utility function. Therefore, no more cost-benefit analysis, no more rankings of things, etc.\" I think this mistake would be highly unfortunate, and attempted to clarify some aspects of it in this post.</p>", "parentCommentId": "gv5mnYkqbRq2qBrZa", "user": {"username": "oagr"}}, {"_id": "jEbFSrR4bXondidLS", "postedAt": "2024-03-09T15:56:21.116Z", "postId": "ZEneqegk4GJahbwik", "htmlBody": "<p>Reading closer, I would separately note that I think there is some semantic ambiguity in how you and others describe extreme optimizers.&nbsp;</p><p>I think that an agent that's \"intensely maximizing for a goal that can be put into numbers in order to show that it's optimal\" can still be incredibly humble and reserved.&nbsp;</p><p>Holden writes, \"Can we avoid these pitfalls by \u201cjust maximizing correctly?\u201d and basically answers no, but his alternative proposal is to \"apply a broad sense of pluralism and moderation to much of what they do\".</p><p>I think that very arguably, Holden is basically saying, \"<i>The utility we'd get from executing [pluralism and moderation] strategy is greater than we would be executing [naive narrow optimization] strategy, so we should pursue the former</i>\". To me, this can easily be understood as a form of \"utility optimization over utility optimization strategies.\" So Holden's resulting strategy can still be considered utility optimization, in my opinion.&nbsp;</p>", "parentCommentId": "dspnRQanhZhmHdyLX", "user": {"username": "oagr"}}, {"_id": "ymuyEL6ugrBnmpCtY", "postedAt": "2024-03-11T15:41:56.338Z", "postId": "ZEneqegk4GJahbwik", "htmlBody": "<p><strong>Executive summary:</strong> The term \"utility function\" can have many different meanings in different contexts, leading to confusion; this post breaks down and names some key distinctions to help clarify discussions around utility functions.</p><p><strong>Key points:</strong></p><ol><li>Utility functions can be Terminal (describing ultimate values) or Proximal (describing preferences over specific actions/states).</li><li>An agent's Initial terminal utility function may be Modified over time to better optimize the initial function.</li><li>Utility functions can be Descriptive (direct estimate), Empirical (estimated from an algorithm), Prescriptive (extrapolated by an observer), or Operational (a specific executable program).</li><li>Utility functions may be Precomputed all at once or computed On-Demand as needed.</li><li>Utility functions can represent initial intuitions (Deliberation Level 0) or the result of extensive deliberation (Deliberation Levels 1-N).</li><li>When utility functions are made Public, expect some information to be hidden or modified for palatability.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]