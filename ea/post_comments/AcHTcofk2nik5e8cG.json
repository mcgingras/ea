[{"_id": "LcpLba5eZigeqhsQi", "postedAt": "2023-06-05T17:27:50.320Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<blockquote><p>Instead of trying to refute Alice from general principles, I think Bob should instead point to concrete reasons for optimism (for example, Bob could say \u201cfor reasons A, B, and C it is likely that we can coordinate on not building AGI for the next 40 years and solve alignment in the meantime\u201d).</p></blockquote><p>As an aside to the main point of your post, I think Bob arrived at his position by default. I suspect that part of it comes from the fact that the bulk of human experiences deal with natural systems. These natural systems are often robust and could be described as default-success. Take human interaction for instance: we assume that any stranger we meet is not a sociopath, because they rarely are. This system is robust and default-success because anti-social behavior is maladaptive. Because AI is so easy for our brains to place in the category of humans, we might by extension put it in the \"natural system\"-box. With that comes the assumption that it's behavior reverts to default-success. Have you ever been irritated at your computer because it freezes? This irrational response could be traced to us being angry that the computer doesn't follow the rules of behavior that have to be followed when in the (human) box that we erroneously placed it in.</p>", "parentCommentId": null, "user": {"username": "blueberry"}}, {"_id": "pMXfa6ggft6dtiFRR", "postedAt": "2023-06-05T19:30:24.418Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<p>We have a large universe of \"technologies we make for economic benefit\": nearly all of them are \"somewhat fine\" to \"very good\". Famous exceptions of course exist like leaded petrol but are relatively rare. I don't count nuclear bombs in this comparison class given that they were explicitly invented to kill large numbers of people Given the massive commercial incentive to make AI useful, we should plausibly expect it to be safe. This is IMO the base-rate-thinking case. Purely from the outside view, we should expect AI to be fine.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Sol3:2"}}, {"_id": "cHpsC4um64muagCLN", "postedAt": "2023-06-05T20:29:03.158Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<p>blueberry - this is a very good point about humans applying their 'default-success' heuristic (regarding social interactions with mostly-non-psychopathic humans) inappropriately to their potential interactions with AIs.&nbsp;</p>", "parentCommentId": "LcpLba5eZigeqhsQi", "user": {"username": "geoffreymiller"}}, {"_id": "KgCPJCCMPc3RAogSc", "postedAt": "2023-06-05T20:35:11.086Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<p>Lauro - nice post; I especially appreciated your points about default-success versus default-failure mindsets.</p><p>Importantly, I think these defaults apply not just to (1) likelihood of being able to develop AGI and (2) likelihood of AGI imposing doom, but also to (3) likelihood that international regulations/pauses/moratoriums could succeed, and (4) likelihood that an anti-AI moral backlash could succeed, and lots of other related issues.</p><p>For example, some folks seem to think there's a very strong default-failure outcome of trying to coordinate formal global regulation of AI, but the same folks (e.g. me!) may think there's a fairly strong default-success outcome of trying to promote informal global moral stigmatization of AI.&nbsp;</p><p>Of course in each such case, what counts as 'success' or failure' may depend heavily on one's goals. For transhumanist Singularity enthusiasts who actually want humans to be replaced by AIs, a high 'default-fail' rate for AI alignment might be seen as actually a success; for libertarians who want every private citizen to have their own unregulated, unaligned AI, then a default-fail for global AI regulation would be seen as a success. So, we should be careful to be clear about what we're counting as 'success' or 'failure' when we talk about default-success or default-failure mind-sets.</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "YJXXAjSRxEP5ySDRy", "postedAt": "2023-06-06T11:50:58.050Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<p>In general I quite like this post, I think it elucidates some disagreements quite well. However, as an out and proud default-success guy, I\u2019m not sure it represents the default-success argument on uncertainty well. To see why, let\u2019s translate default-failure and default-success into their full beliefs:</p><p>Default-failure is the stance that, by default, highly intelligent AI systems will be developed, at least one of which rebels and successfully defeats and then murders/enslaves all of humanity.</p><p>Default-success is referring to a stance that, by default, that chain of events above won\u2019t happen.&nbsp;</p><p>I think the argument of the default-success people is that uncertainty about the future means that you <i><strong>shouldn\u2019t be default-failure</strong></i>. We\u2019re saying that the uncertainty about the future should translate into uncertainty about:</p><p>1.whether AI systems are developed,&nbsp;</p><p>2. whether they rebel</p><p>3.whether they beat us</p><p>4.whether, having defeated us, they decide to kill/enslave us all.&nbsp;</p><p>In order to get to a 90% chance of doom, you need to estimate a 97% certainty of every single step in that process. Of course, it\u2019s completely fine to have 97% confidence in something if you have a lot of evidence for it. I do not feel that doomers have anywhere close to this standard of evidence. I do agree that discussion is better pointed to discussing this evidence than gesturing to uncertainty, but I don\u2019t think it\u2019s a useless point to make.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "PcvRcpgTJMB9mzKgP", "postedAt": "2023-06-06T12:19:26.702Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<blockquote><p>1.whether AI systems are developed,&nbsp;</p><p>2. whether they rebel</p><p>3.whether they beat us</p><p>4.whether, having defeated us, they decide to kill/enslave us all.&nbsp;</p></blockquote><p>&nbsp;</p><p>I'm curious what 3 (defeat) might look like without 4 happening?</p>", "parentCommentId": "YJXXAjSRxEP5ySDRy", "user": {"username": "blueberry"}}, {"_id": "DW4xghmawY9iWxqAT", "postedAt": "2023-06-06T13:20:18.913Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<blockquote>\n<p>In general I quite like this post, I think it elucidates some disagreements quite well.</p>\n</blockquote>\n<p>Thanks!</p>\n<blockquote>\n<p>I\u2019m not sure it represents the default-success argument on uncertainty well.</p>\n</blockquote>\n<p>I haven't tried to make an object-level argument for either \"AI risk is default-failure\" or \"AI risk is default-success\" (sorry if that was unclear). See Nate's post for the former.</p>\n<p>Re your argument for default-success, you only need to have 97% certainty for 1-4 if every step was independent, which they aren't.</p>\n<blockquote>\n<p>I do agree that discussion is better pointed to discussing this evidence than gesturing to uncertainty</p>\n</blockquote>\n<p>Agreed.</p>\n", "parentCommentId": "YJXXAjSRxEP5ySDRy", "user": {"username": "Lauro Langosco"}}, {"_id": "f8YYKiqt75GKNE5nm", "postedAt": "2023-06-06T13:33:20.814Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<p>Hi Geoffrey! Yeah, good point - I agree that the right way to look at this is finer-grained, separating out prospects for success via different routes (gov regulation, informal coordination, technical alignment, etc).</p>\n", "parentCommentId": "KgCPJCCMPc3RAogSc", "user": {"username": "Lauro Langosco"}}, {"_id": "aBq9wLNvg2rkbGfL6", "postedAt": "2023-06-08T10:58:00.544Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<blockquote><p>Re your argument for default-success, you only need to have 97% certainty for 1-4 if every step was independent, which they aren't.</p></blockquote><p>I'm pretty sure this isn't true. To be clear, I was talking about <i>conditional</i> probabilities, the probability of each occurring, given that the previous steps had already occurred.</p><p>Consider me making an estimate like \"theres a 90% chance you complete this triathlon (without dropping out)\". In order to complete the triathlon as a whole, I need to complete the swimming, cycling and running in turn.&nbsp;</p><p>To get to 90% probability overall, I might estimate that you have a 95% chance of completing the swimming portion, a 96% chance of completing the cycling portion given that you finished the swimming portion, and a 99% chance of you completing the running portion, given that you finished the swimming and running portion. Total probability is 0.95*0.96*0.99=0.90.&nbsp;</p><p>The different events are correlated (a fit person will find all three easier than an unfit person), but that's taken care of in the conditional nature of the calculation. It's also possible that uncertainty is correlated (If I find out you have a broken leg, all three of my estimates will probably go down, even though they are conditional).&nbsp;</p><p>With regards to the doomsday scenario, the point is that there are several possible exit ramps (the AI doesn't get built, it isn't malevolent, it can't kill us all). If you want to be <i>fairly</i> certain that no exit ramps are taken, you have to be <i>very</i> certain that each individual exit ramp won't get taken.&nbsp;</p>", "parentCommentId": "DW4xghmawY9iWxqAT", "user": {"username": "titotal"}}, {"_id": "TPtYi2Bd75dYiamdm", "postedAt": "2023-06-08T11:27:50.926Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<p>That's fair; upon re-reading your comment it's actually pretty obvious you meant the conditional probability, in which case I agree multiplying is fine.</p>\n<p>I think the conditional statements are actually straightforward - e.g. once we've built something far more capable than humanity, and that system \"rebels\" against us, it's pretty certain that we lose, and point (2) is the classic question of how hard alignment is. Your point (1) about whether we build far-superhuman AGI in the next 30 years or so seems like the most uncertain one here.</p>\n", "parentCommentId": "aBq9wLNvg2rkbGfL6", "user": {"username": "Lauro Langosco"}}, {"_id": "LbSZSeitrfCskbp4h", "postedAt": "2023-06-08T14:33:43.823Z", "postId": "AcHTcofk2nik5e8cG", "htmlBody": "<p>Yeah, no worries, I was afraid I'd messed up the math for a second there!</p><p>It's funny, I think my estimates are the opposite of yours, I think 1 is probably the most likely, whereas I view 3 as vastly <i>un</i>likely. None of the proposed takeover scenarios seem within the realm of plausibility, at least in the near future. &nbsp;But I've already stated <a href=\"https://titotal.substack.com/p/the-bullseye-framework-my-case-against\">my case</a> elsewhere.</p>", "parentCommentId": "TPtYi2Bd75dYiamdm", "user": {"username": "titotal"}}]