[{"_id": "GvWiCxwLQJAzF6aK9", "title": "CEEALAR: 2022 Update", "postedAt": "2022-12-13T12:19:50.005Z", "htmlBody": "<p><i>Tldr: we are still going, currently have lots of space, and have potential for further growth. Please&nbsp;</i><a href=\"https://ceealar.org/booking/\"><i><u>apply</u></i></a><i> if you have EA-related learning or research you want to do that requires support.</i></p><p><i><img src=\"http://res.cloudinary.com/cea/image/upload/v1670950062/mirroredImages/GvWiCxwLQJAzF6aK9/nigcndpofc2juskh0wpw.png\"></i></p><h2>Update</h2><p>It\u2019s been a while since our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Trs4FeTN3eYuPs3P9/updates-from-the-centre-for-enabling-ea-learning-and\"><u>last update</u></a>, but, suffice to say, we are still here! During 2021 we gradually increased our numbers again, from a low of 4 grantees mid-pandemic, to 15 by the end of the year (our full capacity, with no room sharing and 2 staff living on site). We lifted all Covid restrictions in March 2022, and things started to again feel like they were pre-pandemic. However, our building and its contents are old, and in mid-May this year we closed to new grantees for building repairs and maintenance. We reopened bookings again at the end of July, by which time we had once again got down to very low numbers - we are now up and running again and starting to fill up with new grantees, but we still have plenty of spare capacity. Please&nbsp;<a href=\"https://ceealar.org/booking\"><u>apply</u></a> if you are interested in doing EA work at the hotel. We are offering (up to full) subsidies on accommodation and board for those wishing to learn or work on research or charitable projects (in fitting with our&nbsp;<a href=\"https://ceealar.org/charitable-objects/\"><u>charitable objects</u></a>). See our&nbsp;<a href=\"https://ceealar.org/grant-making-policy/\"><u>Grant Making Policy</u></a> for more details.</p><p>Along with the ups and downs in numbers, we\u2019ve had ups and downs in other ways. We were delighted to receive our largest grant to date, from the FTX Future Fund, in May ($125k or ~ a year of runway), but this is now bittersweet given&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ftx-collapse\"><u>recent events</u></a>. We condemn the actions of SBF and the FTX/Alameda inner circle, and are ashamed of the association. It\u2019s possible the grant will be subject to clawbacks<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefim3q40nt89l\"><sup><a href=\"#fnim3q40nt89l\">[1]</a></sup></span>&nbsp;as a result of the commenced bankruptcy proceedings. As with many FTX grantees in the EA Community, we are following and discussing the situation as it unfolds. We intend to follow the consensus that emerges around any voluntary returning of unspent funds<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref261l8ig82bp\"><sup><a href=\"#fn261l8ig82bp\">[2]</a></sup></span>.</p><p>Despite the significant funding, with the ongoing energy crisis, inflation in general, and increased spending on building maintenance and salaries, our costs have risen rapidly, and we were recently down to ~4 months of runway again. Enter the Survival &amp; Flourishing Fund (SFF). We are extremely grateful to have been awarded a grant of $224,000(!) by Jaan Tallinn as per their most recent&nbsp;<a href=\"https://survivalandflourishing.fund/sff-2022-h2-recommendations\"><u>announcement</u></a>.</p><p>In order to attract and retain talent, with the last grant we upped our management salaries to ~the UK median salary (\u00a331,286), plus accommodation and food (worth about \u00a36k).&nbsp;</p><p>It\u2019s now 4.5 years since we first opened. Since then we have supported ~100 EAs aspiring to do direct work with their career development, and hosted another ~200 visitors from the EA community participating in events, networking and community building. We\u2019ve established an EA community hub in a relatively low cost location. We believe there is plenty of potential demand for it to scale, but we still need to get the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SdjFiM5dj2zxvLwJF/let-s-advertise-infrastructure-projects\"><u>word out</u></a> (which we are doing in part with this blog post).</p><h2>Our impact</h2><p><strong>Grantee work</strong>&nbsp;</p><p>There are two main aspects to our potential impact: the direct work and career building of our grantees, and the community building and networking we facilitate.</p><p>We are open to people working on all cause areas of EA, with the caveat that the work we facilitate is desk-based and mostly remote. In practice, this has meant that longtermist topics, especially x-risks, and in particular AI Alignment, have been foremost amongst the work of the grantees we have hosted. But we have also had grantees interested in animal welfare, global health, wellbeing, development and progress, and meta topics related to EA community building.&nbsp;</p><p>Since our last update, we have had a number of grantees go on to internships, contracts and jobs at the likes of SERI, CHAI, Alvea, Redwood Research, Conjecture and Open AI (see the Highlights tab on our&nbsp;<a href=\"https://ceealar.org/outputs\"><u>outputs page</u></a>).</p><p><strong>Community building</strong></p><p>People tend to stay at CEEALAR for a few months. We get a lot of international grantees, and the standard visa allows 6 months. The vibe at the hotel varies depending on the interests of the group and the balance of social energy (level of extraversion). Non-work group activities people have taken part in whilst at CEEALAR include learning to play musical instruments, meditation, drawing and painting, (vegan) BBQ's on the beach, (vegan) baking, makeup, photography, swimming in the sea (regardless of the season), creative writing, running (sprinting and jogging), board games, tai chi, qi gong, yoga, sunset watching, liqueur/beer tasting, watching the World Fireworks Championship, going to the Christmas fair, ice skating, EA/fiction reading clubs, improvised gardening, breakfast on the beach, karaoke nights, rollerblading on the promenade, bike rides, contra dancing, fasting, Rubik's cube challenges, walks to the forest (near Stanley Park).</p><p>As members of the EA community, guests have a shared affinity and many values and perspectives in common. As people help each other (both with work, and with more personal concerns), networks strengthen and collaborations naturally emerge. It\u2019s not always plain sailing, of course. Occasionally differing backgrounds and styles of engagement can lead to triggering and conflict, and although we are committed to providing a safe and comfortable space to all people coming from diverse backgrounds, we are constantly learning how to better deal with such issues when they arise.</p><p><strong>The general case for CEEALAR</strong></p><p>The&nbsp;<a href=\"https://ceealar.org/donate/#case\"><u>case for CEEALAR</u></a> remains: we provide people with little or no track record in EA a way of gaining one. People who are promising but may not yet be at the level of getting funded by EA Funds get a chance to upskill and put themselves in a better position career-wise. We give people&nbsp;<a href=\"https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\"><u>time and space</u></a> to think, learn, research and plan the next steps of their (high impact) career, and to make valuable contacts and friendships. Getting accepted into CEEALAR gives our grantees time to study pre-requisite material, and apply for internships and jobs.</p><p>Whilst our costs have increased substantially recently, we are still highly cost effective, able to host people for ~\u00a3800/month including free food, a nightly cooked meal, all logistics taken care of by paid staff, and project guidance. So we can fund 6-10 unproven aspiring EAs for about the cost of 1 established EA worker; our view is that this is an example of&nbsp;<a href=\"https://www.openphilanthropy.org/research/hits-based-giving/\"><u>hits-based giving</u></a>.</p><p>We deliberately want to target people who are early in their EA journey and who don\u2019t yet have a lot of opportunities. If we only hosted people who could already make it professionally in EA, we would probably have lower counterfactual impact.</p><p>Given the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ftx-collapse\"><u>FTX collapse</u></a>, there has been concern recently about the lack of security of grants for EAs. The hotel offers, at least for those able to get visas, longer term security at lower costs, subject to grantees continuing to do meaningful work.</p><blockquote><p><i>\u201cStaying at CEEALAR has provided me with a productive and sustainable working environment to develop projects as well as a wonderful community of friends and connections with whom I have built meaningful relationships, developed my involvement in EA communities, and in some cases leveraged to find future opportunities. Without this experience, it is less likely that I would have chosen to or been able to transition as smoothly towards a career in contributing to AI alignment research.\u201d - Vinay</i></p></blockquote><h2>Challenges</h2><p><strong>Self-reporting progress</strong></p><p>There are methodological/social challenges that come with getting people to self-report progress, and expecting a certain amount of output from them, while allowing for output being hard to quantify (especially for e.g. self-study). We have addressed this by setting up weekly one-on-ones between grantees and staff, and having group meetings to discuss progress and goals for the week. We ask that people report on what they have done, even if it is very little, in order to understand what we can do to help them be more productive.</p><p><strong>Staff turnover</strong></p><p>Our staff can be overloaded with responsibilities, and we want to relieve the pressure on them; living and working in the same building can be difficult in terms of managing work-life balance. The difficulty of getting a significant runway has meant we\u2019ve had to be conservative in our hires, meaning staff were sometimes overworked. Being locked in their workplace during the pandemic didn\u2019t help much either. We also had a false-start with a new hire who left (on good terms) for a high-paying role at Meta not long after they started. Our ideal at the moment would be to have two full-time managers, to provide enough cover for holidays and a decent work-life balance for management while getting the efficiency of a fully occupied building, and perhaps to have scope to hire a third as we expand the project. We are still working toward this. Recently, we hired&nbsp;<a href=\"https://ceealar.org/people/#staff\"><u>Ramika</u></a> as our full time Community &amp; Operations manager; she has thrown herself into the role intends to work long term running and building CEEALAR. We hope to make an additional full-time hire soon, upon filling up further and receiving the SFF grant.</p><p><strong>Finding reliable handypeople</strong></p><p>One thing that we have struggled with throughout our history, especially considering the extensive building maintenance that needs doing, is finding reliable tradespeople to do the work. We still think there is a case for having a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/32dPBBXA2neyLfi3A/what-skills-would-you-like-1-5-eas-to-develop?commentId=ZGCG5GFtdEuF2fehE\"><u>mission-aligned handy-person</u></a>, ideally someone from the EA community. If this is you, let us know!</p><p><strong>FTX crisis</strong></p><p>See above.</p><h2>The Future&nbsp;</h2><p><strong>Next door and the possibility of expansion</strong></p><p>We have scope to expand in the immediate future. The adjoining building next door that&nbsp; Greg Colbourn (the CEEALAR founder) bought in 2019 in a derelict state has been slowly renovated (at Greg\u2019s cost) over the last year, and is nearly ready for occupation. This would expand our capacity by ~70% (from 17 to 29 bedrooms). Greg is happy to donate the building to CEEALAR in the event that there is demand for it. We could then combine the two buildings by knocking through the wall with an archway/double-door, and knocking down the wall between the back yards to create a courtyard, perhaps with a conservatory in between<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrae88rn4vwd\"><sup><a href=\"#fnrae88rn4vwd\">[3]</a></sup></span>. We believe that the network effect of expanding the project locally would lead to an increased value per grantee.</p><p><strong>Potential upgrades enabled with increased funding</strong></p><p>Ideally, with increased funding we would invest in some much needed building and energy system improvements. UK gas and electric bills have recently more than doubled, and our central heating system is old and unreliable, and in need of an overhaul. Ideally we\u2019d like to install a new smart heating system with an air source heat pump, ventilation for en suite rooms; and a 10kW solar panel system with battery storage (and water heating) for electric. This would increase our resilience to further energy shocks and save money in the long run.</p><p><strong>Expanding an EA community in a low-cost location</strong></p><p>There is scope for further roll-out of the EA Community in Blackpool with other projects. As a charity, CEEALAR can\u2019t support earning-to-give, so another project could be an earning-to-give (EtG) EA Hotel that is structured as a for-profit limited company that provides food, accommodation and a stipend to EA-aligned start-up founders either in exchange for equity or as an interest bearing loan. We have hosted events in the past at CEEALAR, and maintain the capacity to do so. However, a difficulty we\u2019ve experienced is that of not having much spare capacity to house outside attendees. To this end, an EA Hotel dedicated to events could be good. It could also host fixed-duration cohort-based internships and bootcamps. There could also be standard shared houses for EAs (that spin out of any of the above) to expand the community..</p><h2>Conclusion</h2><p><strong>Runway</strong></p><p>With current funds, and the SFF grant, we will have ~18-20 months runway at current spend, or 12-15 months runway were we to hire an additional full-time manager, overhaul our energy systems, and expand to hosting 25 grantees within the next 6 months. This means we are not in imminent need of fundraising this giving season, but we will still welcome&nbsp;<a href=\"http://ceealar.org/donate\"><u>donations</u></a> up until we have no need for further building upgrades and 18 months of runway at full capacity.</p><p><strong>We welcome new grantees</strong></p><p>We are still going, currently have lots of space, and have potential for further growth. Please&nbsp;<a href=\"https://ceealar.org/booking/\"><u>apply</u></a> if you want to do EA-related learning or research that requires support, especially if you don\u2019t have runway to do it without otherwise having to earn money in an unrelated job. We welcome applicants from all over the world. Citizens of many countries (especially North America and Europe) can get 6-month&nbsp;<a href=\"https://www.gov.uk/standard-visitor\"><u>Standard Visitor visas</u></a> to the UK upon prior application. But we remind those in the UK especially that they can apply, and we welcome people to stay for up to 2 years for free, provided they maintain a reasonable level of productivity. Please share with anyone you think may be interested or be a good fit!</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnim3q40nt89l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefim3q40nt89l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although as it was &gt;90 days before FTX filed for bankruptcy, clawbacks seem like they might be less likely to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/o8B9kCkwteSqZg9zc/thoughts-on-legal-concerns-surrounding-the-ftx-situation\"><u>apply</u></a>. But it's possible fraudulent conveyance claims may still be brought.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn261l8ig82bp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref261l8ig82bp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We acknowledge a conflict of interest here in that two of our trustees are FTX creditors who lost significant amounts of money when FTX went down.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrae88rn4vwd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrae88rn4vwd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Pending permission being granted by the relevant parties.</p></div></li></ol>", "user": {"username": "EA Hotel"}}, {"_id": "kEd5qWwg8pZjWAeFS", "title": "Announcing the Forecasting Research Institute (we\u2019re hiring)", "postedAt": "2022-12-13T12:11:13.054Z", "htmlBody": "<p>The <a href=\"https://forecastingresearch.org/\">Forecasting Research Institute (FRI)</a> is a new organization focused on advancing the science of forecasting for the public good.&nbsp;</p><p>All decision-making implicitly relies on prediction, so improving prediction accuracy should lead to better decisions. And forecasting has shown early promise in the first-generation research conducted by FRI Chief Scientist&nbsp;<a href=\"https://en.wikipedia.org/wiki/Philip_E._Tetlock\"><u>Phil Tetlock</u></a> and coauthors. But despite burgeoning popular interest in the practice of forecasting (especially among EAs), it has yet to realize its potential as a tool to inform decision-making.</p><p>Early forecasting work focused on establishing a rigorous standard for accuracy, in experimental conditions chosen to provide the cleanest, most precise evidence possible about forecasting itself\u2014a proof of concept, rather than a roadmap for using forecasting in real-world conditions. A great deal of work, both foundational and translational, is still needed to shape forecasting into a tool with practical value.</p><p>That\u2019s why our team is pursuing a two-pronged strategy. One is foundational, aimed at filling in the gaps in the science of forecasting that represent critical barriers to some of the most important uses of forecasting\u2014like how to handle low probability events, long-run and unobservable outcomes, or complex topics that cannot be captured in a single forecast. The other prong is translational, focused on adapting forecasting methods to practical purposes: increasing the decision-relevance of questions, using forecasting to map important disagreements, and identifying the contexts in which forecasting will be most useful.</p><p>Over the next two years we plan to launch multiple research projects aimed at the key outstanding questions for forecasting. We will also analyze and report on our group\u2019s recently completed project, the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Dm5eNgyvEwF9ibvzj/participate-in-the-hybrid-forecasting-persuasion-tournament\"><u>Existential Risk Persuasion Tournament (XPT)</u></a>. This tournament brought together over 200 domain experts and highly skilled forecasters to explore, debate, and forecast potential threats to humanity in the next century, creating a wealth of rich data that our team is mining for forecasting and policy insights.</p><p>In our upcoming projects, we\u2019ll be conducting large, high-powered studies on a new research platform, customized for the demands of forecasting research. We\u2019ll also work closely with selected organizations and policymakers to create forecasting tools informed by practical use-cases. Our planned projects include:</p><ul><li>Developing a forecasting proficiency test for quickly and cheaply identifying accurate forecasters</li><li>Identifying leading indicators of increased risk to humanity from AI by building \u201cAI-risk conditional trees\u201d with the help of domain experts (overview of conditional trees&nbsp;<a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/2022/05/Improving-Judgments-of-Existential-Risk.pdf\"><u>here</u></a>, pg. 13)</li><li>Exploring ways of judging (and incentivizing) answers to unresolvable and far-future questions, such as <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3954498\">reciprocal scoring</a></li><li>Conducting \u201cEpistemic Audits\u201d to help organizations reduce uncertainty, identify action-relevant disagreement, and guide their decision processes.</li></ul><p>(For more on our research priorities, see&nbsp;<a href=\"https://forecastingresearch.org/research\"><u>here</u></a> and&nbsp;<a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/2022/05/Improving-Judgments-of-Existential-Risk.pdf\"><u>here</u></a>.)</p><p>We\u2019re excited to begin FRI\u2019s work at such an auspicious time for the field of forecasting, with the many great projects, people and ideas that currently inhabit it\u2014spanning the gamut from heavyweight organizations like <a href=\"https://www.metaculus.com/\">Metaculus</a> and <a href=\"https://goodjudgment.com/\">GJI</a>, to the numerous innovative projects run by small teams and individuals. This environment presents a wealth of opportunities for collaboration and cooperation, and we\u2019re looking forward to being a part of such a dynamic community.</p><p>Our core&nbsp;<a href=\"https://forecastingresearch.org/team\"><u>team</u></a> consists of Phil Tetlock, Michael Page, Josh Rosenberg, Ezra Karger, Tegan McCaslin, and Zachary Jacobs. We also work with various contractors and external collaborators in the forecasting space. We\u2019re looking to expand our team to include additional&nbsp;<a href=\"https://forecastingresearch.org/roles/research-analyst\"><u>research analysts</u></a>,&nbsp;<a href=\"https://forecastingresearch.org/roles/data-analyst\"><u>data analysts</u></a>,&nbsp;<a href=\"https://forecastingresearch.org/roles/content-editor\"><u>content editors</u></a> and&nbsp;<a href=\"https://forecastingresearch.org/roles/research-assistant\"><u>research assistants</u></a>; more information on the roles and an application form can be found by following those links.</p>", "user": {"username": "Tegan"}}, {"_id": "SyhT4mAD5TsG2ZCcg", "title": "Is there a giving pledge I can sign to give away all of everything I own by 2050?", "postedAt": "2022-12-13T06:27:28.227Z", "htmlBody": "<p>I have made an \"informal pledge\" on this basis but looking to make a formal legally binding version so my signal of virtues becomes even hard-coded in the law.<br><br>I am running quite a few <a href=\"http://Persistventures.com\">impact-focused ventures</a>. My Ai timelines are short, and I believe the currency will be worth nothing by 2050, so I want to pledge away 100% of my wealth. My view on life is that all material things are tools for us to help build towards a world where there is more than enough materials for everyone. I like to identify as a \u2018good karma farmer\u2019. I fully believe that the task of crafting a better world is bigger than any person or group and it is a focus on mine to help spread this positive mindset of valuing good karma as the only scarce resource in the world.&nbsp;<br><br>&nbsp;Sites I have published relative to the spread of these ideas include <a href=\"https://redefinestatus.com/\"><code>Redefine Status</code></a>, <a href=\"https://effectivevibes.com/\"><code>Effective Vibes</code></a>, <a href=\"https://prodemic.org/\"><code>Prodemic</code></a>.</p>", "user": {"username": "jack jay"}}, {"_id": "8bcPkqdLYG78YbnTh", "title": "EA & LW Forums Weekly Summary (5th Dec - 11th Dec 22')", "postedAt": "2022-12-13T02:53:28.627Z", "htmlBody": "<p><i>Supported by Rethink Priorities</i></p><p>This is part of a weekly series summarizing the top posts on the EA and LW forums - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology. Feedback, thoughts, and corrections are welcomed.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: prefer your summaries in podcast form? A big thanks to Coleman Snell for producing these! Subscribe on your favorite podcast app by searching for 'EA Forum Podcast (Summaries)'. More detail <a href=\"https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1\">here</a>.</p><h1><br><br>Top / Curated Readings</h1><p><i>Designed for those without the time to read all the summaries. Everything here is also within the relevant sections later on so feel free to skip if you\u2019re planning to read it all. These are picked by the summaries\u2019 author and don\u2019t reflect the forum \u2018curated\u2019 section.</i></p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/NkPghabDd54nkG3kX/some-observations-from-an-ea-adjacent-charitable-effort\"><u>Some observations from an EA-adjacent (?) charitable effort</u></a></p><p><i>by patio11</i></p><p>The author founded VaccinateCA, which ran USA vaccine location infrastructure for 6 months. They think this saved many thousands of lives, at a cost of ~$1.2M.</p><p>Learnings from this include:<br>1.&nbsp;<strong>Enabling trade as a mechanism for impact:</strong>&nbsp;</p><ul><li>Many actors (eg. Google, the White House, pharmacists) wanted residents vaccinated, but weren\u2019t working together.&nbsp;</li><li>By getting data via calling pharmacies and convincing a few people at Google to show it, they were able to do something very impactful without being hired to.</li></ul><p>2.&nbsp;<strong>Engaging with the system</strong>:</p><ul><li>Many people in policy are skeptical of tech.</li><li>501c3 charity status is extremely useful for reputation and funding.</li><li>Carefully planned comms created initial positive press, which in turn was a key reason Google and government stakeholders engaged with them.</li></ul><p>3.<strong> Factors in the team\u2019s success:</strong></p><ul><li>Money to commit early and boldly.</li><li>Social capital to call in favors in the tech community / get resources.</li><li>Good understanding of the infrastructure of the project (call centers and information flows of IT systems).</li><li>Ability to write good software quickly.</li><li>PR experience and talent.</li><li>A network with lots of high agency people willing to try things.</li><li>People management skills (they didn\u2019t have this, but it would have been very helpful)</li></ul><p>While this was not an \u201cEA Project\u201d the author expects it wouldn\u2019t have happened if EA hadn\u2019t drawn their attention to expected value math, and is grateful for this.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/agKFRa5qBApnfLrSh/why-development-aid-is-a-really-exciting-field\"><u>Why development aid is a really exciting field</u></a></p><p><i>by MathiasKB</i></p><p>Wealthy countries spend a collective $178B on development aid per year - 25% of all giving worldwide. Some aid projects have been cost-effective on a level with Givewell\u2019s top recommendations (eg.&nbsp;<a href=\"https://en.wikipedia.org/wiki/President%27s_Emergency_Plan_for_AIDS_Relief\"><u>PEPFAR</u></a>), while others have caused outright harm.</p><p>Aid is usually distributed via a several step process:</p><ol><li>Decide to spend money on aid. Many countries signed a 1970 UN resolution to spend 0.7% of GNI on official development assistance.</li><li>Government decides a general strategy / principles.</li><li>Government passes a budget, assigning $s to different aid subcategories.</li><li>The country\u2019s aid agency decides on projects. Sometimes this is donating to intermediaries like the UN or WHO, sometimes it\u2019s direct.</li><li>Projects are implemented.</li></ol><p>This area is large scale, tractability is unsure but there are many pathways and some past successes (eg. a grassroots EA campaign in Switzerland redirected funding, and the US aid agency ran a cash-benchmarking experiment with GiveDirectly), and few organisations focus on this area compared to the scale.</p><p>The author and their co-founder have been funded to start an organization in this area. Get in touch if you\u2019re interested in Global Development and Policy.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/bsbMPotiwyL4H3mH6/rethink-priorities-is-hiring-help-support-and-communicate\"><u>Rethink Priorities is hiring: help support and communicate our work</u></a></p><p><i>by Rethink Priorities</i></p><p>Author\u2019s tl;dr:</p><ul><li>The Operations Department is hiring a&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/79560\"><u>Chief of Staff</u></a>,&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/79569\"><u>Development Professional</u></a>, and&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/79573\"><u>Communications Strategy Professional</u></a> (applications close January 8, 2023).&nbsp;</li><li>The Development and Communications team will hold a&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfirChbR5mwmZ-Vp8dzJOd4SfHxJfkWT09zFz44zwUGp7limQ/viewform\"><u>Q&amp;A</u></a>&nbsp;<a href=\"https://us02web.zoom.us/j/85256964239?pwd=eW5KeitkVUYrc2FheUFpTld1ZXVxZz09#success\"><u>webinar</u></a> on its job openings on December 15 at 11 am EST.&nbsp;</li><li>We\u2019re also accepting applications to join our&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/78036\"><u>Board of Directors</u></a> until January 13.&nbsp;</li><li>Visit our&nbsp;<a href=\"https://careers.rethinkpriorities.org/\"><u>Careers Page</u></a> for more information and to apply.</li></ul><p><br>&nbsp;</p><h1>EA Forum</h1><h2>Philosophy and Methodologies</h2><p><a href=\"https://forum.effectivealtruism.org/posts/vbhoFsyQmrntru6Kw/do-brains-contain-many-conscious-subsystems-if-so-should-we\"><u>Do Brains Contain Many Conscious Subsystems? If So, Should We Act Differently?</u></a></p><p><i>by Bob Fischer, Adam Shriver, MichaelStJules</i></p><p>The fifth post in the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>Moral Weight Project Sequence</u></a>.</p><p>The Conscious Subsystems Hypothesis says that brains have subsystems that realize phenomenally conscious states that aren\u2019t accessible to the subjects we typically associate with those brains\u2014namely, the ones who report their experiences to us.</p><p>Some have argued that humans are more likely to have conscious subsystems, so risk-neutral expected utility maximizers should weigh human welfare higher relative to other animals. The authors argue against this on three points:</p><ol><li>If humans have conscious subsystems, other animals probably do too.</li><li>Theories of consciousness that support the conscious subsystems hypothesis also tend to support the hypothesis that many small invertebrates are sentient.</li><li>Risk neutral expected utility maximizers are committed to assumptions, including the assumption that all welfare counts equally.</li></ol><p>The post also discusses claims that support CSH, but assigns low credence to these. Overall, the authors suggest we should not act on the conscious subsystems hypothesis.<br>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/secEczgstteSs2c7r/what-can-we-learn-from-the-empirical-social-science\"><u>What can we learn from the empirical social science literature on the expected contingency of value change?</u></a></p><p><i>by jackva</i></p><p>A short overview of some existing literature on the drivers of value change. A consistent theme is that \u201cthere are good reasons to expect that value change is actually quite predictable and that a certain set of values tend to emerge out of the conditions of modernization.\u201d This makes the author believe a re-run of history reaching today\u2019s level of technological development while maintaining slavery is much more unlikely than stated in WWOTF (What We Owe the Future).</p><p>The author notes historical examples tend to overemphasize contingency, while social science overemphasizes patterns and regularities, so those methodologies can be a good balance to each other.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/F2YfRtMvHfRJibwkj/promoting-compassionate-longtermism\"><u>Promoting compassionate longtermism</u></a></p><p><i>by jonleighton</i></p><p>Some suffering is bad enough that non-existence is preferable. The lock-in of uncompassionate systems (eg. through AI or AI-assisted governments) could cause mass suffering in the future.</p><p><a href=\"https://www.preventsuffering.org/\"><u>OPIS</u></a> (Organisation for the Prevention of Intense Suffering) has until now worked on projects to help ensure that people in severe pain can get access to effective medications. In future, they plan to \u201caddress the very principles of governance, ensure that all significant causes of intense suffering receive adequate attention, and promote strategies to prevent locked-in totalitarianism\u201d. One concrete project within this is a full length film to inspire people with this vision and lay out actionable steps. They\u2019re looking for support in the form of donations and / or time.</p><p>&nbsp;</p><h2>Object Level Interventions / Reviews</h2><p><a href=\"https://forum.effectivealtruism.org/posts/agKFRa5qBApnfLrSh/why-development-aid-is-a-really-exciting-field\"><u>Why development aid is a really exciting field</u></a></p><p><i>by MathiasKB</i></p><p>Wealthy countries spend a collective $178B on development aid per year - 25% of all giving worldwide. Some aid projects have been cost-effective on a level with Givewell\u2019s top recommendations (eg.&nbsp;<a href=\"https://en.wikipedia.org/wiki/President%27s_Emergency_Plan_for_AIDS_Relief\"><u>PEPFAR</u></a>), while others have caused outright harm.</p><p>Aid is usually distributed via a several step process:</p><ol><li>Decide to spend money on aid. Many countries signed a 1970 UN resolution to spend 0.7% of GNI on official development assistance.</li><li>Government decides a general strategy / principles.</li><li>Government passes a budget, assigning $s to different aid subcategories.</li><li>The country\u2019s aid agency decides on projects. Sometimes this is donating to intermediaries like the UN or WHO, sometimes it\u2019s direct.</li><li>Projects are implemented.</li></ol><p>This area is large scale, tractability is unsure but there are many pathways and some past successes (eg. a grassroots EA campaign in Switzerland redirected funding, and the US aid agency ran a cash-benchmarking experiment with GiveDirectly), and few organisations focus on this area compared to the scale.</p><p>The author and their co-founder have been funded to start an organization in this area. Get in touch if you\u2019re interested in Global Development and Policy.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/yRSoqFve3vQ6DWyJh/visualizing-the-development-gap\"><u>Visualizing the development gap</u></a></p><p><i>by Stephen Clare</i></p><p>The US poverty threshold, below which one qualifies for government assistance, is $6625 per person for a family of four. In Malawi, one of the world\u2019s poorest countries, the median income is a twelfth of that (adjusted for purchasing power). Without a change in growth rates, it will take Malawi almost two centuries to catch up to where the US is today.</p><p>This example illustrates the development gap: the difference in living standards between high and low income countries. Working on this is important both for the wellbeing of those alive today, and because it allows more people to participate meaningfully in humanity\u2019s most important century and therefore help those in the future too.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/NkPghabDd54nkG3kX/some-observations-from-an-ea-adjacent-charitable-effort\"><u>Some observations from an EA-adjacent (?) charitable effort</u></a></p><p><i>by patio11</i></p><p>The author founded VaccinateCA, which ran the national shadow vaccine location infrastructure for 6 months. They think this saved many thousands of lives, at a cost of ~$1.2M.</p><p>Learnings from this include:<br>1.&nbsp;<strong>Enabling trade as a mechanism for impact:</strong>&nbsp;</p><ul><li>Many actors (eg. Google, the White House, pharmacists) wanted residents vaccinated, but weren\u2019t working together.&nbsp;</li><li>By convincing a few people at Google to show the data, and getting data via calling pharmacies, they were able to do something very impactful without being hired to do it.</li></ul><p>2.&nbsp;<strong>Engaging with the system</strong>:</p><ul><li>Many people in policy are skeptical of tech.</li><li>501c3 charity status is extremely useful for reputation and funding.</li><li>Carefully planned comms created initial positive press, which in turn was a key reason Google and government stakeholders engaged with them.</li></ul><p>3.<strong> Factors in the team\u2019s success:</strong></p><ul><li>Money to commit early and boldly.</li><li>Social capital to call in favors in the tech community / get resources.</li><li>Good understanding of the infrastructure of the project (call centers and information flows of IT systems).</li><li>Ability to write good software quickly.</li><li>PR experience and talent.</li><li>A network with lots of high agency people willing to try things.</li><li>People management skills (they didn\u2019t have this, but it would have been very helpful)</li></ul><p>While this was not an \u201cEA Project\u201d the author expects it wouldn\u2019t have happened if EA hadn\u2019t drawn their attention to expected value math, and is grateful for this.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/saEXX9Nucz8mh9XgB/race-to-the-top-benchmarks-for-ai-safety\"><u>Race to the Top: Benchmarks for AI Safety</u></a></p><p><i>by isaduan</i></p><p>AI Safety benchmarks make it easier to track the field\u2019s progress, create incentives for researchers (particularly in China) to work on problems relevant to AI safety, and develop auditing and regulations around advanced AI systems.</p><p>The author argues this should be given higher priority than it is, and that we can\u2019t assume good benchmarks will be developed by default. They suggest AI safety researchers help by identifying traits for benchmarks, valuable future benchmarks and their prerequisites, or creating benchmarks directly. AI governance professionals can help by organizing workshops / competitions / prizes around benchmarking, researching how benchmarks could be used, and advising safety researchers on this. It could also be useful to popularize a \u201crace to the top\u201d narrative on AI Safety.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/JFyzCv5YynN665nH8/thoughts-on-agi-organizations-and-capabilities-work\"><u>Thoughts on AGI organizations and capabilities work</u></a></p><p><i>by RobBensinger, So8res</i></p><p>Rob paraphrases Nate\u2019s thoughts on capabilities work and the landscape of AGI organisations. Nate thinks:&nbsp;</p><ol><li>Capabilities work is a bad idea, because it isn\u2019t needed for alignment to progress and it could speed up timelines. We already have many ML systems to study, which our understanding lags behind. Publishing that work is even worse.</li><li>He appreciates OpenAI\u2019s&nbsp;<a href=\"https://openai.com/charter/\"><u>charter</u></a>, openness to talk to EAs / rationalists, clearer alignment effort than FAIR or Google Brain, and transparency about their plans. He considers DeepMind and Anthropic on par and slightly ahead respectively on taking alignment seriously.</li><li>OpenAI, Anthropic, and DeepMind are unusually safety-conscious AI capabilities orgs (e.g., much better than FAIR or Google Brain). But reality doesn't grade on a curve, there's still a lot to improve, and they should still call a halt to mainstream SotA-advancing potentially-AGI-relevant ML work, since the timeline-shortening harms currently outweigh the benefits.</li></ol><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/pt28xhxL2DEej8DyJ/you-should-consider-launching-an-ai-startup\"><u>You should consider launching an AI startup</u></a></p><p><i>by Joshc</i></p><p>AI startups can be big money-makers, particularly as capabilities scale. The author argues that money is key to AI safety, because money:</p><ul><li>Can convert into talent (eg. via funding AI safety industry labs, offering compute to safety researchers, and funding competitions, grants, and fellowships). Doubly so if the bottleneck becomes engineering talent and datasets instead of creative researchers.</li><li>Can convert into influence (eg. lobbying, buying board seats, soft power).</li><li>Is flexible and always useful.</li></ul><p>The author thinks another $10B AI company would be unlikely to counterfactually accelerate timelines by more than a few weeks, and that money / reduced time to AGI tradeoff seems worth it. They also argue that the transformative potential of AI is becoming well-known, and now is the time to act to benefit from our foresight on it. They\u2019re looking for a full-stack developer as a cofounder.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/ydfcCfRAQpneH2wpG/smallpox-eradication\"><u>Smallpox eradication</u></a></p><p><i>by Lizka</i></p><p>Smallpox was confirmed as eradicated on December 9th, 1979. Our World in Data has a&nbsp;<a href=\"https://ourworldindata.org/smallpox\"><u>great explorer</u></a> on its history and how eradication was achieved.</p><p>Smallpox killed ~300 million people in the 20th century alone, and is the only human disease to have been completely eradicated. It also led to the first ever vaccine, after Edward Jenner demonstrated that exposure to cowpox - a related but less severe disease - protected against smallpox. In the 19th and 20th centuries, further improvements were made to the vaccine. In 1959, the WHO launched a global program to eradicate smallpox, including efforts to vaccinate (particularly those in contact with infected individuals - \u2018ring vaccination\u2019), isolate those infected, and monitor spread. They eventually contained the virus primarily to India (86% of cases were there in 1974), and with a final major vaccination campaign, dropped cases there to zero in 1976.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/wPHpdwfu3toRDf6hM/main-paths-to-impact-in-eu-ai-policy\"><u>Main paths to impact in EU AI Policy</u></a></p><p><i>by JOMG_Monnet</i></p><p>An non-exhaustive overview of paths to impact in EU AI Policy, cross-checked with 4 experts:<br><br>1. Working on enforcement of the AI Act, related AI technical standards and adjacent regulation</p><p>2. Working on export controls and using the EU\u2019s soft power</p><p>3. Using career capital from time in EU AI Policy to work in private sector or other policy topics</p><p>Building career capital is a prerequisite for impact in any of these. The author recommends building a personal theory of change before acting.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/XGnbde54nteLGw2RB/binding-fuzzies-and-utilons-together-1\"><u>Binding Fuzzies and Utilons Together</u></a></p><p><i>by eleni, LuisMota</i></p><p>Some interventions are neglected because they have less emotional appeal. EA typically tackles this by redirecting more resources there. The authors suggest we should also tackle the cause, by designing marketing to make them more emotionally appealing. This could generate significant funding, more EA members, and faster engagement.</p><p>As an example, the&nbsp;<a href=\"https://wish.org/\"><u>Make-A-Wish</u></a> website presents specific anecdotes about a sick child, while the&nbsp;<a href=\"https://www.againstmalaria.com/\"><u>Against Malaria Foundation</u></a> website focuses on statistics. Psychology shows the former is more effective at generating charitable behavior.</p><p>Downsides include potential organizational and personal value drift, and reduction in relative funding for Longtermist areas if these are harder to produce emotional content for. They have high uncertainty and suggest a few initial research directions that EAs with a background in psychology could take to develop this further.</p><p><br>&nbsp;</p><h2>Giving Recommendations and Organisation Updates</h2><p><a href=\"https://forum.effectivealtruism.org/posts/J7gdciCXFgqyimAAe/center-on-long-term-risk-2023-fundraiser\"><u>Center on Long-Term Risk: 2023 Fundraiser</u></a><br><i>by stefan.torges</i><br>CLR\u2019s goal is to reduce the worst risks of astronomical suffering, primarily via identifying and advocating for interventions that reliably shape the development and deployment of advanced AI systems in a positive way.</p><p>Current research programs include AI conflict, evidential cooperation in large worlds, and s-risk (suffering risk) macrostrategy. They had originally planned on expanding their s-risk community building, but these plans are on hold due to the funding situation.&nbsp;</p><p>They have a short-term funding shortfall, with only a 6 month runway after implementing cost-saving measures. You can donate&nbsp;<a href=\"https://longtermrisk.org/clr-fundraiser-2022\"><u>here</u></a> (with tax deductible options for Germany, Switzerland, Netherlands, USA and UK).</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/isggu3woGwkpYzqwW/presenting-2022-incubated-charities-charity-entrepreneurship\"><u>Presenting: 2022 Incubated Charities (Charity Entrepreneurship)</u></a></p><p><i>by KarolinaSarek, Joey</i></p><p>5 new charities have launched as a result of the June - August 2022 incubation program:</p><ul><li>Center for Effective Aid Policy- identifying and promoting high-impact development policies and interventions.</li><li>Centre for Exploratory Altruism Research (CEARCH)- conducting cause prioritization research and outreach.</li><li>Maternal Health Initiative- producing transformative benefits to women\u2019s health, agency, and income through increased access to family planning.</li><li>Kaya Guides- reducing depression and anxiety among youth in low-and middle-income countries.</li><li>Vida Plena- building strong mental health in Latin America.</li></ul><p>In 2023, there will be two incubation programs, the first of which applications have closed. If you\u2019re interested in applying for the second (June - August), you can sign up&nbsp;<a href=\"https://www.charityentrepreneurship.com/signup\"><u>here</u></a> to be notified when applications open.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/aW9ANJg7s3Q9BASgu/sogive-grants-a-promising-pilot-our-reflections-and-payout\"><u>SoGive Grants: a promising pilot. Our reflections and payout report.</u></a></p><p><i>by SoGive, Isobel P</i></p><p>SoGive is an EA-aligned research organization and think tank. In 2022, they ran a pilot grants program, granting \u00a3223k to 6 projects (out of 26 initial applicants):</p><ul><li>Founders Pledge - \u00a393,000 - to hire an additional climate researcher.</li><li>Effective Institutions Project - \u00a362,000 - for a regranting program.</li><li>Doebem - \u00a335,000 - a Brazillian effective giving platform, to continue scaling.</li><li>Jack Davies - \u00a330,000 - for research improving methods to scan for neglected X-risks.</li><li>Paul Ingram - \u00a321,000 - poll how nuclear winter info affects nuclear armament support.</li><li>Social Change Lab - \u00a318,400 - 2xFTE for 2 months, researching social movements.</li></ul><p>The funds were sourced from private donors, mainly people earning to give. If you\u2019d like to donate, contact isobel@sogive.org.</p><p>They advise future grant applicants to lay out their theory of change (even if their project is one small part), reflect on how you came to your topic and if you\u2019re the right fit, and consider downside risk.</p><p>They give a detailed review of their evaluation process, which was heavy touch and included a standardized bar to meet, ITN+ framework, delivery risks (eg. is 80% there 80% of the good?), and information value of the project. They tentatively plan to run it again in 2022, with a lighter touch evaluation process (extra time didn\u2019t add much value). They also give reflections and advice for others starting grant programs, and are happy to discuss this with anyone.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/3EWpLid8tkyYJakfm/announcing-bluedot-impact\"><u>Announcing BlueDot Impact</u></a></p><p><i>by Dewi Erwan, Jamie Bernardi, Will Saunter</i></p><p><a href=\"https://bluedotimpact.org/?utm_source=EA+Forum&amp;utm_medium=BDI+announcement+post&amp;utm_campaign=BlueDot+Impact\"><u>BlueDot Impact</u></a> is a non-profit running courses that support participants to develop the knowledge, community and network needed to pursue high-impact careers. The courses focus on literature in high-impact fields, in addition to career opportunities in the space, and bring together interested participants to help build networks and collaborations.&nbsp;</p><p>They\u2019ve made multiple improvements over the past few months including working with pedagogy experts to make discussion sessions more engaging, formalizing the course design process, building systems to improve participant networking, collating downstream opportunities for participants to pursue after the courses, and building their team.</p><p>You can register your interest for future courses&nbsp;<a href=\"https://airtable.com/shr41jcUXUc62n1SM?utm_source=EA+Forum&amp;utm_medium=BDI+announcement+post&amp;utm_campaign=BlueDot+Impact\"><u>here</u></a>.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8CRFTxrRLiizypyBu/r-i-c-e-s-neonatal-lifesaving-partnership-is-funded-by\"><u>r.i.c.e.'s neonatal lifesaving partnership is funded by GiveWell; a description of what we do</u></a></p><p><i>by deanspears</i><br>r.i.c.e collaborates with the Government of Uttar Pradesh and an organization in India to promote&nbsp;<a href=\"https://www.theguardian.com/lifeandstyle/2022/nov/15/kangaroo-mother-care-best-for-early-and-low-weight-babies-says-who\"><u>Kangaroo Mother Care</u></a> (KMC), which is a well-established tool for increasing survival rates of low birth weight babies. They developed a public-private partnership to cause the government\u2019s KMC guidelines to be implemented cost-effectively in a public hospital.</p><p>Their best estimate based on a combination of implementation costs and pre-existing research is that it costs ~$1.8K per life saved. However they are unsure and are planning to compare survival rates in the hospital targeted vs. others in the region next year.</p><p>Both Founders Pledge and GiveWell have made investments this year. They welcome further support - you can donate&nbsp;<a href=\"https://paypal.me/riceinstitute?country.x=US&amp;locale.x=en_US\"><u>here</u></a>. Donations will help maintain the program, scale it up, do better impact evaluation, and potentially expand to other hospitals if they find good implementation partners.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/wDgdRxBmF7GXBsnqr/our-2022-giving\"><u>Our 2022 Giving</u></a></p><p><i>by Jeff Kaufman</i></p><p>For the past several years, Julia and Jeff have donated 50% of their income to charity, divided between GiveWell and the EA Infrastructure Fund. After a decrease in salary to focus on direct work, they\u2019re planning to do the same for 2022 for the sake of simplicity and because of the greater funding needed post the FTX situation. They\u2019ll re-evaluate the percentage in 2023.</p><p>&nbsp;</p><p>&nbsp;</p><h2>Opportunities</h2><p><a href=\"https://forum.effectivealtruism.org/posts/bsbMPotiwyL4H3mH6/rethink-priorities-is-hiring-help-support-and-communicate\"><u>Rethink Priorities is hiring: help support and communicate our work</u></a></p><p><i>by Rethink Priorities</i></p><p>Author\u2019s tl;dr:</p><ul><li>The Operations Department is hiring a&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/79560\"><u>Chief of Staff</u></a>,&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/79569\"><u>Development Professional</u></a>, and&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/79573\"><u>Communications Strategy Professional</u></a> (applications close January 8, 2023).&nbsp;</li><li>The Development and Communications team will hold a&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfirChbR5mwmZ-Vp8dzJOd4SfHxJfkWT09zFz44zwUGp7limQ/viewform\"><u>Q&amp;A</u></a>&nbsp;<a href=\"https://us02web.zoom.us/j/85256964239?pwd=eW5KeitkVUYrc2FheUFpTld1ZXVxZz09#success\"><u>webinar</u></a> on its job openings on December 15 at 11 am EST.&nbsp;</li><li>We\u2019re also accepting applications to join our&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/78036\"><u>Board of Directors</u></a> until January 13.&nbsp;</li><li>Visit our&nbsp;<a href=\"https://careers.rethinkpriorities.org/\"><u>Careers Page</u></a> for more information and to apply.</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/buEazpmcKJhM5KRGx/sff-speculation-grants-as-an-expedited-funding-source\"><u>SFF Speculation Grants as an expedited funding source</u></a>&nbsp;<i>by Andrew Critch</i> &amp;&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CYSXxCsxhBRdxyNPR/sff-is-doubling-speculation-rapid-grant-budgets-ftx-grantees\"><u>SFF is doubling speculation (rapid) grant budgets; FTX grantees should consider applying</u></a>&nbsp;<i>by JueYan</i></p><p>The Survival and Flourishing Fund (SFF) funds many longtermist, x-risk, and meta projects. Its&nbsp;<a href=\"https://survivalandflourishing.fund/speculation-grants\"><u>Speculation Grants</u></a> program can fund charities and projects hosted by organizations with charity status, with some applications able to be improved in days and paid out within a month. In response to the recent extraordinary need, Jaan Tallinn, the main funder of SFF, is doubling speculation budgets. Grantees impacted by recent events should apply.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/gQKozf9QbHpXLm3C7/applications-for-eagx-latam-closing-on-the-20th-of-december\"><u>Applications for EAGx LatAm closing on the 20th of December</u></a></p><p><i>by LGlez</i></p><p>EAGx LatAM will take place in Mexico City Jan 6 - 8. It is primarily targeted at those from / with ties to Latin America, or experienced members of the international community excited to meet them. There is no requirement to speak Spanish. Applications can be made&nbsp;<a href=\"https://www.eaglobal.org/events/eagx-latam/\"><u>here</u></a>, and close on 20th December.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/SHCABNadx8GycWAaJ/climate-research-webinar-by-rethink-priorities-on-tuesday\"><u>Climate research webinar by Rethink Priorities on Tuesday, December 13 at 11 am EST</u></a></p><p><i>by Rethink Priorities</i></p><p>Rethink Priorities resident climate expert, Senior Environmental Economist Greer Gosnell, will give a presentation regarding the research process and findings of a report that evaluates anti-deforestation as a promising climate solution. It will include time for Q&amp;A.</p><p><a href=\"https://forms.gle/v29Q6JxN22pCxhBN6\"><u>Sign up</u></a> here to join, or to be emailed the recording if you can\u2019t make it live.</p><p><br>&nbsp;</p><h2>Community &amp; Media</h2><p><a href=\"https://forum.effectivealtruism.org/posts/K5Snxo5EhgmwJJjR2/announcing-audio-narrations-of-ea-forum-posts-1\"><u>Announcing: Audio narrations of EA Forum posts</u></a></p><p><i>by peterhartree, Sharang Phadke, JP Addison, Lizka, type3audio</i></p><p>You can now subscribe to a podcast of EA forum human-narrated content - including top posts, and these weekly summaries. Post narrations will also appear on the post itself, click the speaker button to listen to them.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\"><u>The nonlinear library</u></a> will continue to generate AI narrations of a larger number of posts on a separate feed, with quality improvements planned.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/CnAhPPsMWAxBm7pii/what-specific-changes-should-we-as-a-community-make-to-the\"><u>What specific changes should we as a community make to the effective altruism community? [Stage 1]</u></a></p><p><i>by Nathan Young</i></p><p>The author is collecting ideas in the comments section, which will then be added to&nbsp;<a href=\"https://pol.is/5kfknjc9mj\"><u>Polis</u></a>. Polis allows people to vote on ideas, and groups similar voters together, to better understand the different clusters in the community. The suggestions with the highest consensus, or high consensus within specific clusters, will be put in a google doc for further research.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8Qdc5mPyrfjttLCZn/learning-from-non-eas-who-seek-to-do-good\"><u>Learning from non-EAs who seek to do good</u></a></p><p><i>by Siobhan_M</i></p><p>The author asks whether EA aims to be a question about doing good effectively, or a community based around ideology. In their experience, it has mainly been the latter, but many&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CnAhPPsMWAxBm7pii/what-specific-changes-should-we-as-a-community-make-to-the\"><u>EAs</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology\"><u>have</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HDAXztEbjJsyHLKP7/outline-of-galef-s-scout-mindset\"><u>expressed</u></a> they\u2019d prefer it be the former.<br><br>They argue the best concrete step toward EA as a question would be to collaborate more with people outside the EA community, without attempting to bring them into the community. This includes policymakers on local and national levels, people with years of expertise in the fields EA works in, and people who are most affected by EA-backed programs.</p><p>Specific ideas include EAG actively recruiting these people, EA groups co-hosting more joint community meetups, EA orgs measuring preferences of those impacted by their programs, applying evidence-based decision-making to all fields (not just top cause areas), engaging with people and critiques outside the EA ecosystem, funding and collaborating with non-EA orgs (eg. via grants), and EA orgs hiring non-EAs.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/XYdLTKZLQwTr337zM/the-spanish-speaking-effective-altruism-community-is-awesome\"><u>The Spanish-Speaking Effective Altruism community is awesome</u></a></p><p><i>by Jaime Sevilla</i></p><p>Since Sandra Malag\u00f3n and Laura Gonz\u00e1lez were funded to work on growing the Spanish-speaking EA community, it\u2019s taken off. There have been 40 introductory fellowships, 2 university groups started, 2 camps, many dedicated community leaders, translation projects, 7-fold activity on Slack vs. 2020, and a community fellowship / new hub in Mexico City. If you\u2019re keen to join in, the slack workspace is&nbsp;<a href=\"https://join.slack.com/t/altruismo-eficaz/shared_invite/zt-9dcv7eki-jrN6GerS0NAI~97RH4dB2A\"><u>here</u></a>, and anyone (English or Spanish speaking) can apply to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/gQKozf9QbHpXLm3C7/applications-for-eagx-latam-closing-on-the-20th-of-december\"><u>EAGxLatAm</u></a>.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/q2M9DDoogZGTJjQbg/revisiting-ea-s-media-policy\"><u>Revisiting EA's media policy</u></a></p><p><i>by Arepo</i></p><p>CEA follows a fidelity model of spreading ideas, which claims because EA ideas are nuanced and the media often isn\u2019t, media communication should only be done by those qualified who are confident the media will report the ideas exactly as stated.</p><p>The author argues against this on four points:</p><ol><li>Sometimes many people doing something \u2018close to\u2019 is better than few doing it \u2018exactly\u2019 eg. few vegans vs. many reductitarians.</li><li>If you don\u2019t actively engage the media, a large portion of coverage will be from detractors, and therefore negative.</li><li>EA\u2019s core ideas are not that nuanced. Most critics have a different emotional response or critique how it\u2019s put into practice, rather than get anything factually wrong.</li><li>The fidelity model contributes to hero worship and concentration of power in EA.</li></ol><p>The author suggests further discussion on this policy, acknowledgement from CEA of the issues with it, experimenting with other approaches in low-risk settings, and historical / statistical research into what approaches have worked for other groups.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/why-did-cea-buy-wytham-abbey\"><u>Why did CEA buy Wytham Abbey?</u></a></p><p><i>by Jeroen_W</i></p><p>Last year, CEA bought Wytham Abbey, a substantial estate in Oxford, to establish as a center to run workshops and meetings. Some media on this suggested it was extremely expensive (~15M pounds), though it seems like CEA bought &lt;1% of the land that figure was based on. The author questions if this was cost-effective and asks CEA to share reasoning and EV calculations, to ensure we\u2019re not rationalizing lavish expenses.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/9qcnrRD3ZHSwibtBC/ea-taskmaster-game\"><u>EA Taskmaster Game</u></a></p><p><i>by Gemma Paterson</i></p><p>Resources, how-tos and reflections for an EA themed game the author ran that was a mix between a scavenger hunt, the TV show Taskmaster and a party quest game. There were various tasks for teams to complete within a time limit, which awarded points not always in relation to difficulty, and had different scoring mechanisms (eg. winner takes all, split between submissions, all get the points). The idea was that prioritization of which tasks to complete would be best done using the scale, neglectedness, and tractability framework - and that doing them would be really fun!</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/uG3s9qDCnntJwci9i/link-post-if-we-don-t-end-factory-farming-soon-it-might-be\"><u>[Link Post] If We Don\u2019t End Factory Farming Soon, It Might Be Here Forever.</u></a></p><p><i>by BrianK</i></p><p><a href=\"https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fwww.forbes.com%2Fsites%2Fbriankateman%2F2022%2F12%2F07%2Fif-we-dont-end-factory-farming-soon-it-might-be-here-forever%2F\"><u>Article from Forbes</u></a>. \u201cOnce rooted, value systems tend to persist for an extremely long time. And when it comes to factory farming, there\u2019s reason to believe we may be at an inflection point.\u201d It references What We Owe the Future and argues that AI may lock in the values of today, including factory farming.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/xdwqbmZv2txYPqFAB/new-interview-with-sbf-on-will-macaskill-earn-to-give-and-ea\"><u>New interview with SBF on Will MacAskill, \"earn to give\" and EA</u></a></p><p><i>by teddyschleifer</i></p><p><a href=\"https://puck.news/sam-bankman-fried-sbf-interview/\"><u>New interview</u></a> with SBF by the post author, focused on \u201cWill MacAskill, effective altruism, and whether \"earn to give\" led him to make the mistakes he made at FTX.\u201d One key section quoted in the comments states he hasn\u2019t talked to Will M since FTX collapsed, and feels \u201cincredibly bad about the impact of this on E.A. and on him\u201d, that SBF has a \u201cduty to sort of spend the rest of my life doing what I can to try and make things right as I can\u201d, and confirmation that a previous reference to a \u201cdumb game that we woke Westerners play\u201d was in regard to \u201ccorporate social responsibility and E.S.G\u201d.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/dDTdviDpm8dAFssqe/ea-london-rebranding-to-ea-uk\"><u>EA London Rebranding to EA UK</u></a></p><p><i>by DavidNash</i></p><p>The EA London website and newsletter have been rebranded to EA UK. The UK has a population of 67 million, only 14% of which live in a place with a paid group organizer (London, Oxford, or Cambridge). Setting up EA UK will help provide virtual support (1-1s, newsletter, directory, project advice) and may encourage more local groups to set up by making it easier to find other EAs in the area.&nbsp;</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/erYvs4tLwnNCopBxg/cea-serious-incident-report\"><u>CEA \u201cserious incident report\u201d?</u></a></p><p><i>by Pagw</i></p><p>CNBC reported that CEA filed a \u2018serious incident report\u2019 with the Charity Commission tied to the collapse of FTX. Commenters note this is standard / expected - the Commission asks for serious incident reports to be filed on anything actual or alleged which risks significant loss of money or reputation.</p><p>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://forum.effectivealtruism.org/posts/4jMsqrwkcXQWauciJ/the-ea-infrastructure-fund-seems-to-have-paused-its\"><u>The EA Infrastructure Fund seems to have paused its grantmaking and approved grant payments. Why?</u></a>&nbsp;<i>by Markus Amalthea Magnuson</i><br><br>&nbsp;</p><h1>LW Forum</h1><h2>AI Related</h2><p><a href=\"https://www.lesswrong.com/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking\"><u>Using GPT-Eliezer against ChatGPT Jailbreaking</u></a></p><p><i>by Stuart_Armstrong, rgorman</i></p><p>The authors propose a separate LLM (large language model) should be used to evaluate safety of prompts to ChatGPT. They tested a hacky version of this by instructing ChatGPT to take on the persona of a suspicious AI safety engineer (Eliezer). They ask that, within that persona, it assesses whether certain prompts are safe to send to ChatGPT.</p><p>In tests to date, this eliminates jailbreaking and effectively filters dangerous prompts, even including the less-straightforwardly-dangerous attempt to get ChatGPT to generate a virtual machine. Commenters have been able to break it, but primarily via SQL-injection style attacks, which could be avoided with a different implementation.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/sbb9bZgojmEa7Yjrc/updating-my-ai-timelines\"><u>Updating my AI timelines</u></a></p><p><i>by Matthew Barnett</i></p><p>The author published a post last year titled&nbsp;<a href=\"https://www.lesswrong.com/posts/Z5gPrKTR2oDmm6fqJ/three-reasons-to-expect-long-ai-timelines\"><u>Three reasons to expect long AI timelines</u></a>. They\u2019ve updated, and now have a median TAI timeline of 2047 and mode 2035. This is because they now:</p><ol><li>Think barriers to language models adopting human-level reasoning were weaker than they\u2019d believed. (eg. reasoning over long sequences, after seeing ChatGPT).</li><li>Built a TAI timelines model, which came out with median 2037.</li><li>Reflected on short-term AI progress accelerating AI progress.</li><li>Noted they\u2019d been underestimating returns to scaling, and the possibility of large companies scaling training budget quickly to the $10B-$100B level.</li><li>Almost everyone else updated to shorter timelines (unless they already had 5-15 year ones).</li><li>They still think regulation will delay development, but not as much as before, given governments have been mostly ignoring recent AI developments.</li></ol><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/v5z6rDuFPKM5dLpz8/probably-good-projects-for-the-ai-safety-ecosystem\"><u>Probably good projects for the AI safety ecosystem</u></a></p><p><i>by Ryan Kidd</i></p><p>Projects the author would be excited to see:</p><ul><li>Variants of the&nbsp;<a href=\"https://serimats.org/\"><u>MATS</u></a> program: eg. one in London, one with rolling admissions, one for AI governance research.</li><li>Support for AI safety university groups: eg. bi-yearly workshops, plug-and-play curriculums, university course templates covering \u2018<a href=\"https://twitter.com/JeffLadish/status/1591904522557063168?t=cNq8YAIk9z_HZ3LKiZSCbQ&amp;s=19\"><u>precipism</u></a>\u2019.</li><li>Talent recruitment organisations: eg. focused on cyber security researchers, established ML talent, or specific gaps like for Sam Bowman\u2019s planned AI safety research group.</li><li>Programs to develop ML safety engineering talent at scale eg. like&nbsp;<a href=\"https://arena.education/\"><u>ARENA</u></a>.</li><li>Contests like&nbsp;<a href=\"https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals\"><u>ELK</u></a> on well-operationalized research problems.</li><li>Hackathons for people with strong ML knowledge to critique AI alignment papers.</li><li>Supplemental work to OP\u2019s worldview investigations team like GCP did for CEA &amp; 80K.<br><br>&nbsp;</li></ul><p><a href=\"https://www.lesswrong.com/posts/7gkXuHEm6CqEGT2mg/ai-safety-seems-hard-to-measure\"><u>AI Safety Seems Hard to Measure</u></a></p><p><i>by HoldenKarnofsky</i></p><p>The author argues it will be hard to tell if AI Safety efforts are successfully reducing risk, because of 4 problems:<br>1. The AI might&nbsp;<i>look</i> like it\u2019s behaving, but actually isn\u2019t.</p><p>2. The AI\u2019s behavior might change once it has power over us.</p><p>3. Current AI systems might be too primitive to deceive or manipulate (but that doesn\u2019t mean future ones won\u2019t).</p><p>4. Far-beyond-human capabilities AI is really hard to predict in general.</p><p><br>&nbsp;</p><h2>Other</h2><p><a href=\"https://www.lesswrong.com/posts/Gh6LWHd9kR5FBmCWx/setting-the-zero-point\"><u>Setting the Zero Point</u></a></p><p><i>by Duncan_Sabien</i></p><p>\u2018Setting the Zero Point\u2019 is a \u201cDark Art\u201d ie. something which causes someone else\u2019s map to unmatch the territory in a way that\u2019s advantageous to you. It involves speaking in a way that takes for granted that the line between \u2018good\u2019 and \u2018bad is at a particular point, without explicitly arguing for that. This makes changes between points below and above that line feel more significant.</p><p>As an example, many people draw a zero point between helping and not helping a child drowning in front of them. One is good, one is bad. The Drowning Child argument argues this point is wrongly set, and should be between helping and not helping&nbsp;<i>any</i> dying child.</p><p>The author describes 14 examples, and suggests that it\u2019s useful to be aware of this dynamic and explicitly name zero points when you notice them.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/pt8uKGHArYH33eD4D/the-story-of-vaccinateca\"><u>The Story Of VaccinateCA</u></a></p><p><i>by hath</i></p><p>Linkpost for Patrick MacKenzie\u2019s&nbsp;<a href=\"https://www.lesswrong.com/out?url=https%3A%2F%2Fwww.worksinprogress.co%2Fissue%2Fthe-story-of-vaccinateca%2F\"><u>writeup of VaccinateCA</u></a>, a non-profit that within 5 days created the best source of vaccine availability data in California, improving from there to on best guess save many thousands of lives.<br>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://www.lesswrong.com/posts/jtMXj24Masrnq3SpS/logical-induction-for-software-engineers\"><u>Logical induction for software engineers</u></a>&nbsp;<i>by Alex Flint</i></p><p><a href=\"https://www.lesswrong.com/posts/PfcQguFpT8CDHcozj/finite-factored-sets-in-pictures-6\"><u>Finite Factored Sets in Pictures</u></a>&nbsp;<i>by Magdalena Wache</i></p><p><a href=\"https://www.lesswrong.com/posts/FAJWEfXxws8pMp8Hk/link-why-i-m-optimistic-about-openai-s-alignment-approach\"><u>[Link] Why I\u2019m optimistic about OpenAI\u2019s alignment approach</u></a>&nbsp;<i>by janleike&nbsp;</i>(linkpost for&nbsp;<a href=\"https://www.lesswrong.com/out?url=https%3A%2F%2Faligned.substack.com%2Fp%2Falignment-optimism\"><u>this</u></a>)</p>", "user": {"username": "GreyArea"}}, {"_id": "CixidC6JCruHue8Hs", "title": "GiveWell\u2019s Moral Weights Underweight the Value of Transfers to the Poor", "postedAt": "2022-12-13T07:23:36.831Z", "htmlBody": "<p>Ethan Ligon and I submitted <a href=\"https://docs.google.com/document/d/1IGsUQuyRvxt2B5pFcekMS5uSl20tqq9v/edit?usp=sharing&amp;ouid=105333888811073179341&amp;rtpof=true&amp;sd=true\">this</a> to GiveWell for their \"Change Our Minds\" contest this year (2022). They will be announcing winners later this week (Dec 15, I think). But before they do, we wanted to share our submission here in case anyone is interested! Ethan has done <a href=\"https://escholarship.org/uc/item/3ts0g5tn\">some amazing work</a> measuring &nbsp;real-world marginal utility by estimating demand systems using consumer purchases data. Seeing this prior work of his and thinking it may be particularly relevant to GiveWell's evaluations (and EA in general), I approached him and we wrote a short paper which we ended up submitting for the challenge.</p><p>Rather than post the entire paper here and risk losing info in the reformatting process, I ask that you <a href=\"https://docs.google.com/document/d/1IGsUQuyRvxt2B5pFcekMS5uSl20tqq9v/edit?usp=sharing&amp;ouid=105333888811073179341&amp;rtpof=true&amp;sd=true\">see the Google Doc of it here</a>.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpw6s2zb5308\"><sup><a href=\"#fnpw6s2zb5308\">[1]</a></sup></span>While it does get a bit technical, I've tried to include non-technical summaries for most of the major sections. So in case you feel like some of the details are getting intense, I encourage you to at least read through the intro paragraphs and conclusions as you skim over it. If you get anything out of it, let it be that <strong>while total utility levels (over consumption) are unidentifiable, marginal utilities (over total consumption) are not--they can be measured with data!&nbsp;</strong></p><p>In case you aren't convinced yet to read the full article, below is a quick excerpt from the Summary:</p><blockquote><p>GiveWell bases much of their cost-effectiveness analysis on the value of doubling consumption. Since increasing consumption expenditures is the primary effect of the GiveDirectly cash transfers program, GiveWell uses the effectiveness (value generated per dollar) of cash transfers as a metric for evaluating the effectiveness of all other programs. However, by valuing \u201cdoubling consumption\u201d, GiveWell has assumed the functional form of utility over \u201creal consumption\u201d&nbsp;<i>x</i> to be&nbsp;<i>log(x)</i> and the functional form of marginal utility over consumption to be&nbsp;<i>1/x</i> (since this is the derivative of&nbsp;<i>log(x)).</i> This is a valid utility function in the sense that it is one of many functions that satisfies the conditions of rationality, but there is strong evidence that it is&nbsp;<i>not</i> a good representation of the preferences of the Kenyan beneficiaries of the GiveDirectly experiment.</p><p>The purpose of this article is to explain why GiveWell should reconsider using \u201cdoubling consumption\u201d as the basis for assessing the value of consumption (or income) changes and instead value \u201chalving marginal utility of expenditure\u201d\u2014what we think GiveWell actually&nbsp;<i>intends</i> to value. Using data from GiveDirectly\u2019s cash transfers program in Kenya (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575201/\">Haushofer and Shapiro 2016</a>), we provide empirical evidence that rejects the use of any function that implies homothetic preferences (including marginal utility of&nbsp;<i>1/x</i>). We then empirically estimate the true marginal utility over consumption (<i>\u03bb</i>) as revealed by Kenyan beneficiaries of GiveDirectly\u2019s cash transfers program and show how&nbsp;<strong>the value per dollar of cash transfers is actually 2.6 times GiveWell\u2019s current number&nbsp;</strong>(from 0.0034 to 0.009). This is because&nbsp;<i>1/x</i> is quickly dwarfed by revealed marginal utility, <i>\u03bb</i>, at low levels of consumption. Therefore, valuing \u201cdoubling consumption\u201d underweights the value of cash transfers to the very poor if we let them \u201cspeak\u201d for themselves.&nbsp;</p></blockquote><p>Our \"headline figure\":</p><blockquote><p><i><img src=\"http://res.cloudinary.com/cea/image/upload/v1671035257/mirroredImages/CixidC6JCruHue8Hs/etrllfhohmtjzxuiad6b.png\"></i></p><p><i>FIGURE 5: Utilitarian ROI. The curves labeled cfe use the estimated marginal utilities of expenditure \u03bb(x, p), with p either the values at baseline, or endline. The curve labeled log uses the marginal utility of expenditures implied by a logarithmic utility function used (implicitly) by GiveWell.</i></p></blockquote><blockquote><p>4.5.&nbsp;<strong>Utilitarian Return on Investment</strong>. For every dollar given to a particular household, there\u2019s some increase in utility, which we can think of as a \u201cutilitarian ROI\u201d. This increase depends on household characteristics (e.g., size, composition), on the household\u2019s budget (other things equal poorer households will benefit more), and on prevailing prices (e.g., if food prices are relatively high, the uROI will be relatively higher for poorer households).&nbsp;</p><p>For the households in the GiveDirectly experiment we trace out the uROI as a function of household budget&nbsp;<i>x</i>, using prices observed at the baseline, and \u201caverage\u201d household characteristics. The GiveDirectly experiment distributed transfers randomly across the distribution of households pictured in Figure 5. The figure also plots marginal utilities of expenditures. The green line is the marginal utility of expenditures&nbsp;(x,p) corresponding to log utility. At higher budgets, this is very similar to the estimated MUEs, but the large divergence at lower budgets makes it clear where the greater estimated impact comes from: poorer households benefit much more from an additional dollar than do wealthier households. This is true for log utility as well, but to a&nbsp;<i>far</i> lesser extent.</p></blockquote><p>How our results would affect GiveWell's moral weights:</p><blockquote><p>5.1&nbsp;<strong>Recommendation.</strong> For our particular empirical example involving the GiveDirectly experiment, the logic given above suggests that GiveWell should update the value of changes in households\u2019 budgets. Specifically, wherever there\u2019s presently a&nbsp;<i>log(x)</i> (where&nbsp;<i>x</i> is total budget for consumption expenditures within the period) this should be replaced with the more general&nbsp;<i>\u2212log \u03bb(x, p).</i> Note that the current use of&nbsp;<i>log(x)</i> is a special case of this, since with log utility, \u2212<i>log \u03bb(x, p)</i> =&nbsp;<i>log(x).</i>&nbsp;</p><p>In general, this calls for making predictions about future prices. In practice, we feel comfortable assuming the baseline prices for this experiment (note that Figure 5 above indicates that price changes between baseline and endline had very little effect). There are three particular cells in the&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/edit#gid=1377543212\"><u>GiveWell Cost Effectiveness Spreadsheet</u></a> for GiveDirectly that are affected by the arguments advanced above:<br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1671035257/mirroredImages/CixidC6JCruHue8Hs/q6tluwkkbgfzrhb60cha.jpg\"></p><p><i>Table 1: Utilitarian ROI.&nbsp; Key cells from the&nbsp;</i><a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/\"><i><u>GiveWell Cost Effectiveness Spreadsheet</u></i></a><i> for GiveDirectly.&nbsp; Cells B20, B21, and B26 are all directly affected by the arguments in this note. B31 is a summary output.</i></p><p><br>Table 1 shows the changes that our methods would produce<strong>\u2014</strong>an increase in the overall value of consumption increases by a factor of 7.5/2.9 = 2.6. A similar factor applies to the other cells. The impact of our recommendation on cell B38 \u201cUnits of value generated per dollar spent\u201d is particularly significant given its role in calibrating the efficacy of other programs.</p><p>5.2&nbsp;<strong>Implications.</strong> While we could have done this analysis for any program for which consumption or income changes are important outcomes, we chose to do it for the cash transfers program due to the centrality of its role in calibrating the entirety of GiveWell\u2019s cost-effectiveness analysis. If GiveWell cares about knowing the true value of its top charities to their beneficiaries, this exercise&nbsp;<a href=\"https://docs.google.com/document/d/1IGsUQuyRvxt2B5pFcekMS5uSl20tqq9v/edit#bookmark=id.989m0rbrnmai\"><u>(Ligon 2019)</u></a> should be carried out for each program that involves consumption or income effects. From the \u201c<a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/edit#gid=1377543212\"><u>2022 GiveWell cost-effectiveness analysis</u></a>\u201d spreadsheet, this includes AMF, Deworm the World, END Fund, SCI Foundation, Sightsavers, and Malaria Consortium.</p><p>Evaluating these other programs in a similar way is beyond the scope of this article.&nbsp; However, it is not unlikely that doing so would yield increases in the value of their consumption/income components that are similar in magnitude to what we estimated in Kenya (2.6 times) if their beneficiaries are similarly impoverished. As such, there are two reasons this could quite plausibly cause GiveWell to reorder its top charities. The first is that those programs whose value relies more on income/consumption effects will now be relatively more effective (value per dollar). The second reason is that the distribution of expenditures across beneficiary populations might be different across programs. Thus, those programs that give transfers to poorer recipients will now be considered relatively even more effective.&nbsp;</p><p>Since GiveWell uses cash transfers as a metric by which to compare the effectiveness of other charities (especially to define \u201ctop\u201d charities), it is also important to consider how our results would affect this relationship. Without evaluating other consumption-affecting programs as we have done for cash transfers, we cannot yet make definitive statements about how other programs that involve income/consumption effects now fare relative to cash transfers. However, it is at least safe to say that those programs which do&nbsp;<i>not</i> include consumption effects for the extremely poor would now be 2.6 times less effective relative to cash transfers in terms of value generated per dollar. For instance, such a program that was previously 10 times as effective as cash transfers would now only be 3.8 times as effective.</p><p>5.3&nbsp;<strong>Conclusion</strong>. It is clear that GiveWell already cares about avoiding paternalistic evaluations of effectiveness. They have taken steps in the recent past to support research for the purpose of updating their efficiency calculations. We applaud this effort and recommend that GiveWell continue this pursuit by augmenting their moral weights to reflect revealed marginal utility rather than the ad hoc functional form of&nbsp;<i>log(x)</i>. Doing so would not only provide an empirical foundation for GiveWell\u2019s valuation of consumption and income effects but would also incorporate the true preferences of the extremely poor. As a result, the GiveDirectly cash transfers program should be valued 2.6 times as much per dollar as GiveWell\u2019s current number (from 0.0034 to 0.009). Theoretically motivated and empirically estimated, &nbsp;is what GiveWell should use if their objective is to maximize the true value of their programs to beneficiaries.</p></blockquote><p>Thank you for reading, now <a href=\"https://docs.google.com/document/d/1IGsUQuyRvxt2B5pFcekMS5uSl20tqq9v/edit?usp=sharing&amp;ouid=105333888811073179341&amp;rtpof=true&amp;sd=true\">see the full article</a>!<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpw6s2zb5308\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpw6s2zb5308\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I may end up also posting the full version if there is enough demand for it. I initially started to do that but was afraid of not catching formatting &nbsp;mistakes.&nbsp;</p></div></li></ol>", "user": {"username": "Trevor Woolley"}}, {"_id": "jnrmWcJnfa5c2Hnvx", "title": "Sam Bankman-Fried has been arrested", "postedAt": "2022-12-12T23:49:26.051Z", "htmlBody": "<p>Confirmed by Sarah Emerson, a tech reporter at Forbes: <a href=\"https://twitter.com/SarahNEmerson/status/1602448748109512704\">https://twitter.com/SarahNEmerson/status/1602448748109512704</a> (Edit: Now also confirmed by <a href=\"https://www.nytimes.com/2022/12/12/business/ftx-sam-bankman-fried-bahamas.html\">NYT</a>, <a href=\"https://www.cnbc.com/2022/12/12/ftx-founder-sam-bankman-fried-arrested-in-the-bahamas-after-us-files-criminal-charges.html\">CNBC</a>, <a href=\"https://www.bloomberg.com/news/articles/2022-12-12/ex-ftx-ce0-sam-bankman-fried-arrested-in-the-bahamas\">Bloomberg</a>, <a href=\"https://finance.yahoo.com/news/former-ftx-ceo-bankman-fried-234420529.html\">Yahoo Finance</a>, and others.)</p><p>Includes statement from the Attorney General of The Bahamas, transcribed below:</p><blockquote><p><strong>Statement from the Attorney General of The Bahamas Sen. Ryan Pinder KC on the arrest of Sam Bankman-Fried</strong></p><p>On 12 December 2022, the Office of the Attorney General of The Bahamas is announcing the arrest by The Royal Bahamas Police Force of Sam Bankman-Fried (\"SBF\"), former CEO of FTX. SBF's arrest followed receipt of formal notification from the United States that it has filed criminal charges against SBF and is likely to request his extradition.</p><p>As a result of the notification received and the material provided therewith, it was deemed appropriate for the Attorney General to seek SBF's arrest and hold him in custody pursuant to our nation's Extradition Act.</p><p>At such time as a formal request for extradition is made, The Bahamas intends to process it promptly, pursuant to Bahamian law and its treaty obligations with the United States.</p><p>Responding to SBF's arrest, Prime Minister Davis stated, \"The Bahamas and the United States have a shared interest in holding accountable all individuals associated with FTX who may have betrayed the public trust and broken the law. While the United States is pursuing criminal charges against SBF individually, The Bahamas will continue its own regulatory and criminal investigations into the collapse of FTX, with the continued cooperation of its law enforcement and regulatory partners in the United States and elsewhere.\"</p><p>December 12, 2022<br>Office of The Attorney General &amp;<br>Ministry of Legal Affairs<br>Commonwealth of The Bahamas</p></blockquote>", "user": {"username": "peppersghost"}}, {"_id": "orDS9BTHoF5oxkfkC", "title": "12 career advising questions that may (or may not) be helpful for people interested in alignment research", "postedAt": "2022-12-12T22:36:23.119Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "KphWqKCJEybZRBZuh", "title": "University & City Group Outreach Testing - A Callout to Group Leaders", "postedAt": "2022-12-13T09:32:36.252Z", "htmlBody": "<p><i>TLDR: Group leaders! Please get in touch with us regarding any outreach testing you have done or if you are even a little bit interested in doing this at any point in the future, no matter at what scale.</i></p><h2>Introduction</h2><p>I am working with David Reinstein, the EA Market Testing Team (EAMT) and several collaborators to organize and expand EA university and city groups\u2019 outreach testing. This&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HboobjbDwc5KgpNWi/ea-market-testing\"><u>forum post</u></a> offers an overview of the EAMT project as a whole. The&nbsp;<a href=\"https://effective-giving-marketing.gitbook.io/untitled/\"><u>public Gitbook</u></a> also has more information, as well as our data, progress, information, and resources. The&nbsp;<a href=\"https://effective-giving-marketing.gitbook.io/untitled/partners-contexts-trials/university-groups-promotion\"><u>University/City Groups</u></a> section specifically addresses this topic.</p><h3>Objective</h3><p>We want to promote systematic testing, analysis, and sharing evidence about \u2018what works best\u2019&nbsp; in outreach, intake, and retention. Groups are an important part of growing the EA community, and University groups&nbsp;<a href=\"https://80000hours.org/after-hours-podcast/episodes/kuhan-jeyapragasan-effective-altruism-university-groups/\"><u>offer a unique opportunity</u>&nbsp;</a>for redirecting people towards solving the world's most pressing problems. Many students who are sympathetic to EA ideas simply&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/mNRNWkFBZ2K6SHD8a/most-students-who-would-agree-with-ea-ideas-haven-t-heard-of\"><u>haven\u2019t heard of EA yet</u></a>. We want to maximize the ability of groups to not only reach new EAs but also create stronger EAs.&nbsp;</p><p>To the best of our knowledge, there hasn\u2019t been a sustained effort across universities to consolidate data or do substantial tests (randomized trials) spanning multiple years. Preliminary efforts suggest that many groups have pockets of knowledge, resources, and tools that aren\u2019t being effectively shared, such as\u2026</p><ul><li>Independent strategy documents&nbsp;</li><li>Anecdotal and descriptive evidence&nbsp;&nbsp;</li><li>Data from small-scale independent testing (with minimal analysis)</li><li>Plans, methods, and survey instruments for testing</li><li>Lists of the relevant contacts&nbsp;&nbsp;</li></ul><h2>Our Reasoning and Methodology</h2><h3>Benefits of Collaboration</h3><p>Autonomy is good at finding methods that are right for a group, but a lot of this overlaps. Where we can, we want to combine these efforts and have universities test the same things. This can help them save time and money, improve testing, and help disseminate ideas that raise student interest. By putting these efforts in one place, we can see what works best. Groups can then use these ideas, avoid repeating work, and make sure that organizations and universities are working together well.&nbsp;</p><p>Should every group work on its own? This \u2018autonomy\u2019 could make sense if each university EA&nbsp; group had different goals, or operated in a very different context. It might be necessary if communication between groups was difficult, or if there were big \u2018free-rider\u2019 problems between groups. We don\u2019t think this is the case. Although universities differ, there is plenty of overlap. People are in EA precisely because they want to work towards shared goals.</p><p>This collaboration enables economies of scope and scale. Student groups don\u2019t need to individually waste hours designing their own materials to test, planning experimental designs, writing surveys to measure engagement, getting access to survey and promotion platforms, and analyzing the results. These can largely be done and set centrally (while allowing each group to adapt the materials to their particular circumstances).&nbsp;</p><h3>Stronger Trials</h3><p>It\u2019s also often difficult for an individual group to cleanly conduct a test. If they want to compare two approaches they may need to (e.g.) run two versions of a fellowship at the same time. This duplication can be costly, and it also risks \u2018contamination\u2019 (students in Fellowship A are likely to talk to those in Fellowship B, making it difficult to cleanly infer the relative impact). With an organized collaboration, the \u2018randomization of treatment assignments\u2019, (e.g., which fellowship material to use) could be done&nbsp;<i>between</i> different universities. [<i>It could also be done between different universities-years and units, e.g., with A-B vs B-A designs.</i>] We also get the benefit of larger sample sizes [as well as comparisons across contexts (\u2018random effects\u2019)], increasing statistical power and robustness.&nbsp;</p><p>The benefits are not only from large careful random-controlled trials. Small amounts of meaningful data can lead to important Bayesian updating. We can also learn from careful consideration of descriptive observation. If many university groups report the same patterns and insights, these are likely to be meaningful. Bringing these together into shared resources will yield value.&nbsp;</p><h3>Sustained Effort</h3><p>Centralizing and professionalizing this effort also increases the chances of success through \u2018continuity\u2019. Most groups are run by students, who face unpredictable study pathways and life paths. A larger project, linked to professionals involved with the EAMT, increases the chances of a sustained project. This overcomes common failure modes for very promising projects (a pivotal student loses interest, finds a different very shiny object, moves to a new school, etc.).&nbsp;</p><h2>Outline of Current Work</h2><p>The&nbsp;<a href=\"https://centreforeffectivealtruism.notion.site/UGAP-s-Outreach-Handbook-6eca98516a5d4151b50a492fccfe24ec\"><u>UGAP handbook</u></a> contains most of the current information surrounding best practices for outreach. These have been summarised from different data points; some formal testing, some anecdotal and some intuitive. UGAP is currently the best central body for accessing information regarding outreach methods for groups, but we think we can expand upon their efforts. UGAP also has a focus on groups within the program, whereas we want to conduct testing as broadly as possible.&nbsp;</p><p><i>This is not to detract from UGAP\u2019s efforts. We are actively collaborating with them as they are currently the most active organisation in this area.</i><br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670880240/mirroredImages/KphWqKCJEybZRBZuh/e100zpmebtcktxswk4h9.png\"></p><p>This&nbsp;<a href=\"https://docs.google.com/document/d/19jyPub9GwbOiZ2IPgR74j5-uttrTgYgQNkBhCJCrYDY/edit?usp=sharing\"><u>funnel map</u></a> is our basic understanding of the processes used to draw in new members to EA university groups. Each stage gives us grounds for testing through the different variations of these approaches. This is not just about testing which methods work for attracting the highest number of new members (ie. which Call to Action to use at activity fairs, etc), but also increasing engagement and developing high-level EAs (ie. fellowship programme alternatives, discussion group topics, etc).</p><p>In the&nbsp;<a href=\"https://effective-giving-marketing.gitbook.io/untitled/partners-contexts-trials/university-groups-promotion\"><u>Gitbook</u></a> you\u2019ll also see the current work being done by individual groups. A lot of this is preliminary but offers the foundation for formal testing. Many of these have been individually run by groups but allow us a great platform to build off. This is the beginning of efforts to centralise testing ideas, methods and results together from a diverse set of groups and students who are actively engaged in outreach.&nbsp;<strong>If you\u2019re aware of any other relevant testing being done, please let us know or put us in touch with the relevant people.</strong> We want to incorporate as much as we can into the Gitbook.</p><h2>Summary</h2><p>We see universities as an integral component of the EA community. Outreach methods can be further refined by coordinated and rigorous testing to maximise student intake and create more engaged, high-level EAs.</p><p>EAMT will function in two ways in this space. It will act as a multiplier for groups, increasing the impact of their independent research by giving them a platform. Perhaps more importantly, it can coordinate large-scale testing between groups, but we need to make group leaders aware of EAMT to make this happen.</p><p><strong>If you\u2019re a group leader or organiser doing any form of independent testing, informal or formal, we want to hear from you. If you\u2019re interested in doing any of this in the future, we also want to hear from you.</strong> Eventually, this will transform into formalised testing (limited to/based on group capacity) and ultimately improve outreach methods for EA groups.</p><p>&nbsp;</p><p><i>Thank you to David Reinstein for his help with this post and thank you to the early collaborators in this space such as Dave Banerjee, Max Gehred, Robert Harling and Jessica McCurdy.</i></p>", "user": {"username": "Kynan Behan"}}, {"_id": "djEojDKnp5nHKGPHS", "title": "Wikipedia is not so great, and what can be done about it.", "postedAt": "2022-12-12T20:06:08.981Z", "htmlBody": "<p>&nbsp;<i>Note: This post originally appeared in Reddit's /r/trueunpopularopinion sub as </i><a href=\"https://old.reddit.com/r/TrueUnpopularOpinion/comments/zieyyf/wikipedia_is_not_so_great_and_is_overrated/\"><i>Wikipedia is not so great, and is overrated</i></a><i> written by a user there. It is explicitly released under CC0 public domain by them.</i></p><p>You all have heard by now that Elon Musk said that Wikipedia has a \"left wing bias\" when the article about Twitter Files had been suggested for deletion. This has been received with mixed responses from liberals and conservatives alike; the former dismissing it as \"an attack on free knowledge\" and the latter cheering the move as \"against censorship\" and vindication of their beliefs that Big Tech is biased against them.</p><p>True, Wikipedia is supposedly editable by anyone around the world and I had been an on and off editor there for years mostly doing small-ish edits like fixing typos and reverting obvious vandalism. This is done while on IP as opposed to using accounts because I would rather that some edits (i.e. sensitive topics like religious and political areas) not tied to my name and identity. However, reality is far from the preferred sugar-coated description of Wikipedia, particularly its editing community.</p><p>The editing community in overall is best described as a slightly hierarchical and militaristic \"do everything right\" structure, traditionally associated with Dell and recently Foxconn and now-defunct Theranos. Exceptions apply in quieter and outlier areas such as local geography and space, usually the top entry points for new users wanting to try their first hand. There are higher tolerance of good-faith mistakes such as point-of-view problems and using unreliable resources, which are usually explained in detail on how to correct by them rather than a mere warning template or even an abrupt block.</p><p>Ultimately those sub-communities which can be said as populated by <a href=\"https://meta.wikimedia.org/wiki/Exopedianism\">exopedians</a>, have relatively little to no power over the wider and core communities, mostly dominated by <a href=\"https://meta.wikimedia.org/wiki/Metapedianism\">metapedians</a>. A third group called <a href=\"https://meta.wikimedia.org/wiki/Mesopedianism\">mesopedians</a> often alternates between these inner and outer workings. Communities can have shared topical interest which are grouped by <a href=\"https://en.wikipedia.org/wiki/Wikipedia:WikiProject\">WikiProject</a>, an example being <a href=\"https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Science\">WikiProject Science</a></p><p>I spend a lot of time casually browsing through edit wars (can be so <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Lamest_edit_wars\">lame</a> at times) like a fly on the wall, along with meta venues of Wikipedia such as <a href=\"https://en.wikipedia.org/wiki/WP:AFD\">Articles for Deletion</a>, <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Centralized_discussion\">Centralized discussion</a> <a href=\"https://en.wikipedia.org/wiki/WP:NPOVN\">Neutral Point of View Noticeboard</a>, <a href=\"https://en.wikipedia.org/wiki/WP:BLPN\">Biographical of Living Persons Noticeboard</a>, <a href=\"https://en.wikipedia.org/wiki/WP:COIN\">Conflict of Interest Noticeboard</a>, <a href=\"https://en.wikipedia.org/wiki/WP:ANI\">Administrator's Noticeboard Incidents</a>, <a href=\"https://en.wikipedia.org/wiki/WP:SPI\">Sockpuppet investigations</a>, <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Arbitration_Committee/Noticeboard\">Arbitration Committee noticeboard</a> which is the \"supreme court\" in Wikipedia community for serious behavioral and conduct disputes. Therefore I can sum up how the editing community really functions, although not really as extensive as you might expect because I am not a \"Wikipedioholic\" with respect to inner workings.</p><h1><strong>Deletionism and inclusionism</strong></h1><p>This has been very perennial and core reasons for just about any disputes on Wikipedia ever. Deletionists treat Wikipedia as another \"regular encyclopedia\" where information has to be limited once it become very much to be covered; like cutting out junk, while inclusionists treats Wikipedia as a comprehensive encyclopedia not bound by papers and thus can afford to cover as much information as it can take; one man's junk could be another man's treasure. Personally I support the latter and often the conflict between two editing ideologies leads to factionalism, where attempts to understand mutual feelings and perspectives are inadequate or even none at all.</p><p>There are no absolute standards of what defines \"encyclopedic knowledge\" and \"notability\". Inclusionism posits that almost everything could become valuable and encyclopedic in the future, even if they're aren't today. An example I can think of is events, figures and stories from World War II. Deletionism has been closely related to \"academic standard kicks\" and rely on the premise that Wikipedia has to be of high standard and concise. There are people who deem an addition of something as useful, and there are those who think it's \"trivia\" or \"crufty\" something that is nominally discouraged if not prohibited by Wikipedia's documentation (see <a href=\"https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_an_indiscriminate_collection_of_information\">this</a> in particular, although sometimes exceptions are applied through the spirit of \"<a href=\"https://en.wikipedia.org/wiki/WP:IAR\">Ignoring all rules for sake of improvement</a>\", which are frequent at entertainment and gaming topics).</p><p>On pages, notability debates around a person subject and otherwise are frequently the main point of discussion in Articles for Deletion threads, where articles deemed not substantial enough (such as very few sources) are suggested for deletion. Usually they will run for a week but they can be quickly closed if there are too many votes in favor of \"keep\", \"delete\" and so on, the AFD nomination is withdrawn by the initiator, or that the nomination is found to have been done in bad faith (such as to \"censor\" articles from public view for questionable motives like ideology, paid editing or so).</p><p>Here I believe that deletionists are seen far more harshly by inclusionists, than the vice versa. The chief reason is to add something, you have to navigate through the user experience unfriendly editing interfaces (although somewhat improved in recent years) all the while having to scroll through the internet to find sources and references to add. When you found some you have to go through an extra hoop to assess whether they are reliable or not, before finally transcribing the information through your own words which has to stick to the neutral point of view (NPOV) policy; paraphrasing that are so close are not allowed because, copyright. Non-English speaking editors would often find the latter very difficult.</p><p>In contrast, as per an old adage, destroying something is easier than building something, deletions are comparatively easier than addition. This could be the reason why deletionism currently maintains dominance over the whole site as I see it, since in order to become an established an esteemed editor, one has to garner a high amount of edits which are not reverted. Thus, many editors like to gain these \"scores\" by deleting \"unuseful information\" from passages up to entire articles by interpreting the documentations and rules strictly, the latter through processes such as Articles for Deletion and if confident enough, <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Proposed_deletion\">Proposed Deletion</a> that doesn't require discussion. Simply speaking, it's a feature not a bug and aren't necessarily beholden to any political ideology; a liberal is as equally likely as a conservative to become a hated deletionist.</p><p>Even though every edit changes are recorded and displayed through page histories which you can see for any given articles by clicking \"View History\" at the top, the bone of contention remains particularly when page deletions results in the redaction of these histories from public view. This will be explained further later.</p><p>Some historical contexts that can be think of regarding the current prominence of deletionism are the excessive amount of Pokemon pages during or before 2007 which had alienated some readers and editors alike because search engines back then are not quite as adequate as today in terms of finding precise information. Another is that child predators like Nathan Larson used to sneak in as inclusionists to warp Wikipedia to fit their agenda all the time, which are indelibly horrendous to all of us here and those back then. Think of the poisoning of the well and the fruits from a poisonous tree. Furthermore there are also large portion of userbases from tech companies like Intel and those from the academic world (maybe instead of GLAMs, short for galleries, libraries, archives and museums) that gained top positions such as administrators, bringing along their work culture and so-called \"academic standards kick\" respectively. To be absolutely fair, I find that there are instances where deletionism is right enough, specifically the removal of copyright violation and libel materials on biographical pages of any living persons.</p><p>Regardless of whether a page is deleted or not, they remain available in Wikipedia's servers and accessible to administrators or higher only.</p><p>Eventually, what defines as \"encyclopedic knowledge\" are vulnerable to <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Systemic_bias\">systemic biases</a> as well. Different from some Musk's thoughts about it, users who are white, male, US/UK/CA/EU/AU/NZ, middle or old aged, and English speaker tend to have the greatest advantage above the rest in the editing community. With this in mind, a prominent musical artist in Zambia may be treated as too small-bore enough for a page on Wikipedia by an editor in Canada. Shopping malls in the US are less likely to be deleted than those in Vietnam. Such a bias doesn't go one way; the hypothetical artist in Zambia would be \"unimportant\" to someone in Peru.</p><p>This is the top causes of animosity between editors and also why many editors chose to quit or rather fell from grace. You will always hate that kid who like to ruin your LEGO structure every time you have assembled the blocks.</p><h1><strong>Neutral point of view</strong></h1><p>Different from mere deletions and additions, this normally means that how to present a given information in a way to the readers ideally so that no disproportionate biases towards or against something are left in their impressions. You see arguments and conflicts concerning such a lot in political articles, historical articles and geography topics of areas under dispute from two or more nations. Say that a political figure is engaged in activities that are remotely linked to extremism. Side A would argue that the figure is therefore an extremist and it should be made prominent on that page and any other linked pages, but Side B wants to tone it down by writing it something like \"Political figure was engaged in activities which were sometimes reported by some as extremist\" and limit it to a mere mention on a single page. Another is a nation should be said as a \"partially recognized state\" because some UN members don't recognize it as such and instead as part of a bigger country, with others expressing views that simply having an effective sovereignty for its own and different from another nations would be enough to be deemed as a state.</p><p>It can come into play on cases involving \"fringe theories\" as well, like Bigfoots, UFOs and medical treatments, although Wikipedia indeed has a preference of giving prominence to mainstream views in these cases, something I don't find a problem with and is quite different from regular harmful biases.</p><p>Venues for resolution in this case are Neutral Point of View noticeboard, along with <a href=\"https://en.wikipedia.org/wiki/WP:RFC\">Request for Comment</a>. The latter entails a process where a notice is put up in a centralized noticeboard all the while a pool of experienced/established editors receive notifications to comment, provide insights and make suggestions on a given issue. A month is usually on how these discussions are up and running unless there is a need of extension because of reasons such as unbroken deadlock.</p><p>Along with deletionism and inclusionism, this is a major cause of editors \"going naughty\" and getting blocked/banned/kicked out, whether for right or spurious reasons.</p><h1><strong>Conduct</strong></h1><p>The most important part of this post in my honest opinion. I'll start this section by writing about edit war. Usually when you change something in Wikipedia and it was undone/reverted by somebody else, then you have only two tries before you get reported to the <a href=\"https://en.wikipedia.org/wiki/WP:ANEW\">edit-war noticeboard</a> if you're stubborn enough not to go to the article's talk page (\"Talk\" in the top left) for discussion, either by the person undoing your edits or by a third party. In the meantime you get notifications on your personal talk pages (\"Talk\" on the top right) inviting you for such discussion and if lucky enough, the <a href=\"https://en.wikipedia.org/wiki/WP:TEAHOUSE\">Wikipedia Teahouse</a> for further help by some kind-hearted editors, increasingly a rarity these days. In some quieter or outer areas where as said before are slightly lenient, you may get up to approx. five chances counting your original edit before getting referred to the admins.</p><p>The tries count are reset after 24 hours but can be retained further just as a guard against \"gaming the rules\". Clearer cut vandalism (like putting gibberish such as \"LOLOLOLOLOLOLOL\" at any pages) usually gets reported to a <a href=\"https://en.wikipedia.org/wiki/WP:AIV\">separate noticeboard for administrators to intervene</a>, although first time vandals regularly get warnings on their talk pages beforehand. When a report is there and if found guilty of edit-warring, administrators would either give ultimatums to the users in question or block their accounts for a day. They could escalate to multiple days, weeks and up to indefinite (practically infinite) period should the behavior continues beyond that. The same goes for vandalism, although they are dealt more harshly with many prompt indefinite blocks (indeffs) for \"vandalism-only accounts\".</p><p>Regular editors can be in danger of falling from grace too either by themselves or by others. Because Wikipedia is commonly seen by so many as the biggest comprehensive encyclopedia in the world, sometimes equated to history itself, many vested interests, feelings and sentiments have been invested on the website.</p><p>Those who are nationalists or otherwise fanatics of any imaginable notions found themselves having incentives to make Wikipedia to support their narratives both as an end itself or rather just means for other ends such as \"proving that they're great in the long annals of great history\". The same applies to run off the mill \"promotional editing\" by corporations and individuals, along with those made by their supporters or fans. On the opposite many people find it extremely attractive to twist it to denigrate any ideologies, corporations, people, and just about anything they personally oppose. For instance, they can make an article and fill it with disparaging information against them, which is called an \"attack page\".</p><p>I find that there are kernels of truth in the commonly-held viewpoint that \"Wikipedia is a placeholder of information\" and that \"Wikipedia is history\". A <a href=\"https://news.mit.edu/2022/study-finds-wikipedia-influences-judicial-behavior-0727\">MIT report</a> described how judges' behavior are increasingly influenced by Wikipedia articles, while there are initiatives by space missions such as <a href=\"https://mashable.com/article/moon-library-beresheet-crash-wikipedia\">Beresheet</a> and <a href=\"https://www.archmission.org/lunar-library-2\">Peregrine</a> to perform civilizational backups of humanity with all of English Wikipedia (version as of a given date) in the <a href=\"https://en.wikipedia.org/wiki/Global_catastrophic_risk\">event of collapse</a>.</p><p>After having their way, to keep their changes forever in \"annals of history\" or simply the \"placeholders of information\" in general, gate-keeping measures are utilized. A simple example would be using excessively harsh language against editors who made a change challenging a given status quo. In contrast, if anybody has a reason to radically change a page and make sure it stays unassailable afterwards, the same set of actions are used too but arguably these would be \"antigatekeeping\" measures instead.</p><p>In gatekeeping/antigatekeeping one would resort to different levels of intepretation regarding <a href=\"https://en.wikipedia.org/wiki/WP:PAG\">PAGs</a> (policies and guidelines) and <a href=\"https://en.wikipedia.org/wiki/WP:ESSAY\">user essays</a>, the latter sometimes used as a basis of many editorial and administrative actions. The documentations can often contradict each other, like how \"<a href=\"https://en.wikipedia.org/wiki/Wikipedia:NOTINDISCRIMINATE\">not indiscriminate</a>\" is to \"<a href=\"https://en.wikipedia.org/wiki/Wikipedia:NOTPAPER\">not a paper encyclopedia</a>\", and on top of all, can be overruled by <a href=\"https://en.wikipedia.org/wiki/Wikipedia:IAR\">ignoring these</a> if anybody sees fit. Hence, whoever has the \"biggest fist\" gets to be the most advantageous in Wikipedia community. In order to have the \"biggest fist\", they can befriend anyone sharing interests with their own and form a cabal/gang that look after their own. To increase their power and when enough time had passed they can nominate each other for <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship\">administrator positions</a> giving them extra privileges of blocking users, deleting pages, protecting an article from editing by lower-ranked users. You don't get paid for spending your efforts and time on editing Wikipedia unless perhaps you've listed a Venmo link or a crypto address on your user profile, and these administrative tools alone are so addictive and appealing given that you are essentially in control of the important bits of \"writing history\" if you have these, apart from usual human nature. Wikipedia is among the top 10 visited websites in the world after all.</p><p>Even more, there are additional ranks above administrator positions. Two of those are CheckUsers (CU) and Oversighters. CU has the power to look through IP address used by an account to see if it was a sockpuppet account of a person, while Oversighters have super-delete rights to hide contents or pages, even beyond the reach of administrators.</p><p>Those on the other end of the power-tripping, gate-keeping and so on rarely fares well. One would find them belittled, bullied by those editors. Should they attempt to properly resolve an issue through established processes such as talk page discussions, <a href=\"https://en.wikipedia.org/wiki/WP:DRN\">dispute resolution noticeboard</a>, and up to the infamous Administrator's Noticeboard Incidents (ANI), they would expect to find obstructions upon obstructions along the way. If the victim decides to invite other editors to give balanced/impartial opinions and suggestions on a problem they would find themselves stonewalled on the grounds that these are \"<a href=\"https://en.wikipedia.org/wiki/Wikipedia:Canvassing\">canvassing</a>\". It can be quite hypocritical if the \"bully\" had their gang friends informed beforehand, which is reasonably believed to often be the case. Finally, if it escalates into the ANI, this is where it start to get out of hand.</p><p>The reason why I use the term \"infamous\" is because ANI is the mother-lode of all kinds of ugly dramas. It is frequently the first place in getting an editor sanctioned or so on. The bullies (I do not use the term lightly) would then put all sorts of allegations and aspersions against other for any types of wrongdoing, whether real or perceived, big or small, or whether the result is a real harm or just a nothing burger. Regardless, if they twisted the rules (derisively referred as \"<a href=\"https://en.wikipedia.org/wiki/Wikipedia:Wikilawyering\">wikilawyering</a>\" or otherwise \"<a href=\"https://en.wikipedia.org/wiki/Wikipedia:Gaming_the_system\">gaming the system</a>\") and played the victim good enough, the passing administrators would then close the discussion and place administrative actions against the \"real\" victim. Common egregious example of such an action is the \"<a href=\"https://en.wikipedia.org/wiki/Wikipedia:Here_to_build_an_encyclopedia#Clearly_not_being_here_to_build_an_encyclopedia\">not here to build an encyclopedia</a>\" indefinite/permanent block that can be arbitrary interpreted from any given actions. It's ironic given that the bullies are guilty of such as well. A prime example of twisting the rules to railroad/squeeze out other editors would start with so-called <a href=\"https://en.wikipedia.org/w/index.php?title=Wikipedia:BADFAITHNEG&amp;redirect=no\">bad faith negotiation</a>, where they promised a victim not to remove content at other pages if the victim lets the bully keep their changes in a page. Soon the bully reneged it and when confronted by the victim the bully immediately accused them of being \"<a href=\"https://en.wikipedia.org/wiki/WP:TENDENTIOUS\">tendentious</a>\" or \"POV pusher\".</p><p>The bullies, which can consist of most editors operating at the inner workings, aren't necessarily beholden to any ideologies and come in all stripes. The only attribute that they all share is the addiction to power.</p><p>After such permablocks, most would be forced to leave it for good, further <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Why_is_Wikipedia_losing_contributors_-_Thinking_about_remedies\">bleeding the editors numbers</a>. Still, because Wikipedia's so preeminent and no viable competitors are currently available, some would rather stay behind, disguise their identity and either continue editing or start over in different areas. For those with knowledge of foreign languages, they could simply switch to other language Wikipedias to continue their work far from most perturbances. A smaller number would come back as vandals to spite editors who had wronged them.</p><p>This is where \"<a href=\"https://en.wikipedia.org/wiki/Wikipedia:Sockpuppet_investigations\">sockpuppetry investigations</a>\" kick in, mostly referred as SPI. Editors go there to start a new case if they suspect that an account is an alt/sock account of someone else particularly users who evaded the blocks/bans. When a user is blocked or banned for good, they are relegated to a pariah status much akin to \"unpersoning\", Scientology's <a href=\"https://en.wikipedia.org/wiki/Suppressive_Person\">suppressive persons</a>, and the lowest ones in North Korea's <a href=\"https://en.wikipedia.org/wiki/Songbun\">Songbun</a>, in the respect that any and all edits by them under other accounts or IPs are liable to be reverted/undone pursuant to <a href=\"https://en.wikipedia.org/wiki/Wikipedia:BE\">policy pertaining to block evasion</a>. While the original goal of not separating the wheat from the chaff is expressedly to prevent them from <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Deny_recognition\">gaining further recognition</a> and diminish the spirit of the block, in practice this means a Monkey's Paw that any further potential good contributions from them would be lost forever, handicapping the improvement of encyclopedia as a whole in a way or more. Other editors have the exception from edit-war policy to revert and undone any changes from the violators of the blocks, perhaps as well as anybody who helped them. In effect this is like what the Meatball Wiki said, a \"<a href=\"http://meatballwiki.org/wiki/PunishReputation\">PunishReputation</a>\".</p><p>During a SPI, there are \"clerks\" who will look through the user's contribution history to see if there is a similarity in pattern to warrant a block for abuse of multiple accounts (sockpuppetry). If that alone is not enough, the CheckUsers can then be called upon to check and compare the IP used by the accounts.</p><p>If a user is determined to have engaged in sockpuppetry, the userpage of original and alt accounts used are then replaced with a scarlet letter notice such as <a href=\"https://en.wikipedia.org/wiki/User:PositiveIntentsOnly\">this example</a> boasting that which sock account belongs to who and therefore blocked. Forget about \"denying recognition\", this is simply a punitive name-and-shame.</p><p>The SPI case, now listing the accounts and IP used, would then be archived in a separate page, still publicly viewable. This is despite recent GDPR regulations and the implication that major privacy-improving adjustments should've been made for the process while keeping it viable. Try that in Reddit and you'd be instantly banned for doxxing, I can assure you.</p><p>In there you can effectively cosplay as a CSI although substantive attention are given to clerks, administrators and CheckUsers. Keep in mind that the results and outcomes of most if not all sockpuppet investigations aren't really 100% accurate, given that there are a lot of unforeseen variables such as the imitation of writing and behavior styles that are mostly a result of multiple people pushing any particular editorial change for any reasons i.e. brother helping his sister, along with the use of software that can mask your IP addresses such as VPNs and TeamViewer. Those admins in charge of sockpuppetry investigations often aren't privy to the root cause of a \"sockpuppetry\" or \"block evasion\" and as such tend to for example, underestimate the amount of users who has the right reasons to support an edit made in violation of a block.</p><p>VPN IP addresses, which are used for obvious privacy reasons, are blocked in sight by any administrators pursuant to <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Blocking_policy#Open_or_anonymous_proxies\">policy against open proxies</a>. They even have a dedicated WikiProject and a bot specializing in finding and blocking these proxies, with the result being a great inconvenience for people wishing to edit from countries such as Russia and China.</p><p>In time, if someone continues a behavior the other editors deemed as \"disruptive\" or \"vandal\" past the initial block, they end up getting displayed in so-called \"<a href=\"https://en.wikipedia.org/wiki/WP:LTA\">Long Term Abuse</a>\" caselist. Right there, their accounts and/or IP addresses, along with a likely-skewed description of what they've done were listed out. The places they've been and accounts outside of Wikipedia were frequently exposed there, as if it's an opposition research and spiteful doxxing. Things that'll get you quickly banned here are just a normal Tuesday over at Wikipedia, with GDPR out of the window.</p><p>As I see it, there are two categories of LTAs/vandals/whatever you call it. The first are the inherent vandals who had been problematic and disruptive for Wikipedia upon their first edit, and the other are those who had been regular or good standing users in the past until their fall from grace, normally caused by themselves such as being too overworked over one thing but could be by others, like the bullying example.</p><p>There is a reasonable possibility that some of those LTAs/vandals would be redeemed and become a good editor once again if enough diplomacy and mediation were tried. However, those would be a time-consuming process compared to simply actioning them, and I reasonably suspect that some of those are intentionally provoked by corrupt admins or their friends into vandal or disruptive editing in order for them to increase that admin actions count so as to further their own standing in the community, and to stay away from losing their cherished tools if their KPI <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Administrators#Procedural_removal_for_inactive_administrators\">fell low enough</a> in a given period.</p><p>It's fearful that the cycle of toxicities in Wikipedia could eventually led to real-world harm, though I will not further speculate how that might transpire for fear of <a href=\"https://en.wikipedia.org/wiki/WP:BEANS\">stuffing the beans</a> and giving bad ideas. However, VICE had reported in 2016 that an editor had nearly <a href=\"https://www.vice.com/en/article/4xangm/wikipedia-editor-says-sites-toxic-community-has-him-contemplating-suicide\">driven to suicide</a> after being subjected to online abuse by the editors despite what the documentation say about community collegiality. Furthermore, just before Musk' comment against Wikipedia, the Anonymous group hacked a <a href=\"https://www.taipeitimes.com/News/taiwan/archives/2022/11/02/2003788129\">Chinese ministry site and a satellite system</a> out of the suspicion that a state actor has manipulated Wikipedia's system and process to censor information about their hacking activities against China. It was a hot news in Taiwan then.</p><h1><strong>Afterthoughts</strong></h1><p>Theoretically a deep and comprehensive reform is past due for Wikipedia in order to (re-)foster collegiality among the members of Wikipedia community and reduce the amount of synergies that leads to intractable conflicts, as opposed to sinecures such as blockings and SPI which often treats the symptoms but not the cause.</p><p>Still, it appears that the core editors and/or administrators are so content enough for the present status quo and thus doom any effort to change the system. An example would be the temporary ban of an <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Community_response_to_the_Wikimedia_Foundation%27s_ban_of_Fram\">administrator</a> made in 2019 by the Wikimedia Foundation (ultimately responsible for maintaining English Wikipedia and any other projects such as Wikimedia Commons for photos and Wikipedias written in other languages), nearly causing the split of Wikipedia into two or more. This is not to mention that presently Wikipedia has a <a href=\"https://en.wikipedia.org/wiki/WP:CANCER\">financial cancer</a> and having to beg for donations despite having sufficient funds so it may be worthwhile to put your donations for the Internet Archive instead.</p><p>A key to a solution may lie in the comparative analogy that Wikipedia is like the only restaurant in a <a href=\"https://en.wikipedia.org/wiki/Food_desert\">food desert</a>. It could be a McDonald's, KFC, BK, Taco Bell, White Castle, or so on, but customers are forced to go there to dine in every time, even if some does not really like their food. Thus, they will be really happy if a second restaurant is opened at the location.</p><p>If Musk is really serious in fixing whatever problems Wikipedia has brought as a result of its internal problems, then he would be wise in angel-investing any alternatives which aims to become a better or next-level version of Wikipedia.</p><p>The hypothetical rival alternatives could come in the form of a more comprehensive encyclopedia, close to the level of a compendia. It can come in a format similar to GitHub where anyone can present in their preferred version of a subject instead of edit-warring at a small point, and if version is good enough then they can be merged/pushed/vouched by other users to work upon and goes to the top in ranks.</p><p>In fact, every edition of page histories are logged by Wikipedia when a change is make, but in addition to heuristic placements which make these to be perceivably obscure, those would get redacted if the page in question is deleted.</p><p>Forking contents from English Wikipedia isn't really a big problem since all you can do is to go to <a href=\"https://dumps.wikimedia.org/\">the Wikimedia dump site</a> and look for enwiki, but the biggest issues are how to convince editors and readers alike to move over to the alternative. One possible solution that I can think of in terms of editors would be a pitch promising that the contents will eventually get copied into <a href=\"https://www.extremetech.com/extreme/328700-5d-optical-disc-could-store-500tb-for-billions-of-years\">discs that lasts for billions of years</a> and launched to the Moon and beyond for posterity.</p><p>It is entirely possible that if such solution with out-of-the-world approach had been thought about earlier, the synergies that led to all sort of intractable conflicts in Wikipedia could be cut by a half or so. Perhaps inside Wikipedia the environment would not resemble an authoritarian police state like now. After all, you can find so many real stories echoing the same theme on Wikipediocracy, Wikipedia Review and Wikipediasucks.co, which are like how Xenu.net is to Scientology.</p><p>Finally this post is released under Creative Commons CC0, which is a public domain as the only thing I want is let everyone know how Wikipedia really works in the inside given the recent attention to Musk's comments against it and to dispel idealistic notions (as seen in WhitePeopleTwitter regarding Musk's tweet) that overrated it beyond what should've been, while hoping for alternatives to spring up to provide greater opportunities for anyone to preserve histories without corrosive influence from systemic biases such as those in Wikipedia.</p>", "user": {"username": "Rey Bueno"}}, {"_id": "osrBzAQaBE2jnNQRd", "title": "Altruism and Development - It's complicated...", "postedAt": "2022-12-12T16:48:29.568Z", "htmlBody": "<p>An interesting post from <a href=\"https://substack.com/profile/34780141-shruti-rajagopalan\"><strong>Shruti Rajagopalan</strong></a><strong> looking at the trade off between legibility and complexity in evaluating philanthropic efforts.</strong></p><p>&nbsp;</p><blockquote><p><strong>As December rolls in, my inbox is filled with requests for donations, often from organizations I have given to in the past. This holiday season is also bittersweet because I cannot visit Delhi, where I was born and my parents still live, because of the air pollution and smog during the winter months. In Delhi, I find it hard to breathe, and usually lose my voice because of inflammation caused by particulate matter pollution. This year, I am under doctor\u2019s orders to avoid travelling to Delhi in the winter; I\u2019ve been struggling with respiratory problems from long Covid.</strong></p><p><strong>With air pollution dominating my thoughts and nudges for charitable giving in my inbox, my first instinct is to give to causes that help mitigate pollution in Delhi. But I am also aware of the literature on emotional giving or ineffective altruism. In their 2021 paper, Caviola, Schubert and Greene explain why both effective and ineffective causes may attract dollars. People often give emotionally to a cause that has personally impacted them in some way.</strong></p><p>...</p><p><strong>This paper resonated with me because I am exactly the sort of irrational dog lover likely to support the best training programs for guide dogs. These super dogs have my lifelong admiration. My Labrador retrievers can barely fetch a ball.</strong></p><p><strong>We all know air pollution is bad. But how bad? And compared to what?</strong></p><p><strong>As an alternative, I looked up the top charities recommended by GiveWell\u2014the top two work on reducing Malaria deaths. Malaria kills between 600,000 and 700,000 each year. And GiveWell is considered one of the most credible evaluators in the philanthropic space. Should I be thinking less about air pollution in Delhi and more about malaria in Africa?</strong></p><p><strong>So, I thought it best to evaluate 1) my priors on air pollution, 2) whether air pollution mitigation in Delhi merits my dollars/rupees. And if Delhi air pollution merits intervention, then it would be good to 3) identify the reasons air pollution became such a big problem in Delhi (you would be surprised), which would lead to uncovering 4) how to mitigate the problem of air pollution so I can decide where to send my dollars. And since I got to #4, to understand 5) why people think that giving to malaria charities is \u201chigher impact\u201d than solving air pollution.</strong></p></blockquote><p>&nbsp;</p><p>&nbsp;</p><blockquote><p>6. Back to Malaria\u2014Is It Really That Simple?</p><p>While writing this post, I also thought more about malaria and whether malaria prevention is more complex than impact evaluations lead us to believe. If legibility is the consequence of a narrowing of vision to make a complex problem tractable, then are these malaria mitigation interventions too simple?</p><p>95% of all malaria deaths are in Africa, and that malaria disproportionately kills children.</p><p>This is probably why the effective altruism community, which believes in helping those far removed from one\u2019s situation, measures by lives saved per dollar when thinking about long-term and high-impact efforts, and rates malaria prevention charities so highly. In his latest column,&nbsp;<a href=\"https://www.nytimes.com/2022/12/04/opinion/charity-holiday-gift-givewell.html\"><u>Ezra Klein defends the basic principles of effective altruism and separates it from the SBF-FTX mess</u></a>.</p><p>It is impossible not to feel for the children in Africa dying from malaria. But suggesting that distributing nets and antimalarial medication is the best way to save lives and prevent illness compared to anything else we know is narrow. Regions outside Africa only account for 4% of malaria deaths. But I don\u2019t see high use of mosquito nets and antimalarial medication in Europe and the U.S. Outside of camping equipment stores, I don\u2019t think I have seen any mosquito nets bought or sold in the U.S. These countries don\u2019t have malaria deaths because they have access to good public sanitation, clean water, electricity and healthcare. Malaria hits children in poor regions with low state capacity.</p><p>Common sense tells us that the <i>best</i> way to save lives and prevent illness is economic growth. But then how do we know for sure that economic growth in Africa will help reduce malaria incidence?</p><p>Look at the decline in malaria deaths in India since the <a href=\"https://the1991project.com/\"><u>big bang reforms in 1991</u></a>, which placed India on a higher growth trajectory averaging about 6 percent annual growth for almost three decades. Malaria deaths declined because Indians could afford better sanitation preventing illness and greater access to healthcare in case they contracted malaria. India did not witness a sudden surge in producing, importing or distributing mosquito nets. I grew up in India, in an area that is even today hit by dengue during the monsoon, but I have never seen the shortage of mosquito nets driving the surge in dengue patients. On the contrary, a surge in cases is caused by the municipal government allowing water logging and not maintaining appropriate levels of public sanitation. Or because of overcrowded hospitals that cannot save the lives of dengue patients in time.</p><p>So, it seems bizarre to claim that these impact studies on distributing malaria nets prove \u201c<i><strong>that they save lives and prevent illness at lower cost than pretty much anything else we know of</strong></i>.\u201d Economic growth and high state capacity saves lives at a much higher scale, not just from malaria but from all infectious diseases. And malaria-affected individuals can be saved and illness can be prevented at very low marginal cost if we embrace the idea of economic growth and prosperity for all of humanity.</p><p>Economic growth helped save lives and prevent illness from ALL infectious diseases.</p><p>Sustained economic growth in India has saved an additional 405 lives per 100,000 in a country with 1.4 billion people. If India had the 1990 death rate from infectious diseases in 2019, an additional 5 million Indians would die each year. That\u2019s the total estimated excess death toll from Covid in India. Economic growth in India has saved those lives <i>every</i> <i>year</i>. And as Indians become prosperous, the number of lives saved will increase without any additional spending on mosquito nets.</p><p>Perhaps I am focusing on the wrong part of GiveWell and Ezra Klein\u2019s claim. I should focus on&nbsp;\u201c<i>at a lower cost than pretty much<strong>&nbsp;anything else we know</strong></i>.\u201d&nbsp;Perhaps those recommending these charities as having the highest impact are doing so because they can be implemented and evaluated with confidence at a lower cost relative to efforts to improve state capacity and boost economic growth. Giving to charities that work on state capacity, institutions, public sanitation policy, economic growth, etc. is not a sure thing. Maybe they only increase the chances of economic growth by 5%. But with enough diligence and evaluation, we can be 90% sure that our dollars buy the additional mosquito net, and that mosquito net has a 70-80% chance of saving a life. So, this is less about the overall impact and more about \u201cimpact we know\u201d or better phrased as \u201c<i><strong>impact we can count and take credit for</strong></i>.\u201d The Against Malaria Consortium\u2019s website says they have raised $488,628,184, funded&nbsp;223,421,135 nets and protected&nbsp;402,158,043 people. Contributing to this institution a donor can calculate the number of lives they saved with their contribution. This is not just about legibility but also attribution. Each dollar donated will not just save lives but also assuage guilt, signal virtue and make one feel good during the holidays. Telling people about investments in clean construction technology hardly has the same effect at the holiday party.</p><p>It is not just philanthropy. In my conversations with Lant Pritchett (<a href=\"https://www.discoursemagazine.com/economics/2022/03/17/ideas-of-india-where-did-development-economics-go-wrong/\"><u>1</u></a> and <a href=\"https://www.discoursemagazine.com/culture-and-society/2022/06/09/ideas-of-india-reforming-development-economics/\"><u>2</u></a>) he argued that development policy and aid has been infected by attribution. We have long known that the first step to development in poor regions of the world is economic growth. But economic growth is neither easy to achieve, and when achieved not easily attributable to a single intervention.</p><p>India\u2019s development trajectory changed when the&nbsp;<a href=\"https://the1991project.com/\"><u>reforms in 1991&nbsp;</u></a>ended the worst parts of socialist command and control, opened India to global trade and put in place several institutional changes for currency and macroeconomic stability, pushing India into higher growth for the next three decades. As a result, GDP per capita increased sevenfold, and about 250 million Indians\u2014more than the total population of Brazil\u2014were lifted out of poverty. All socioeconomic groups prospered because of sustained economic growth.&nbsp;India added approximately&nbsp;$3.6 trillion&nbsp;to its economy as a direct consequence of these reforms.&nbsp;</p><p><a href=\"https://www.cgdev.org/publication/perils-partial-attribution\"><u>Lant Pritchett writes</u></a> about the Ford Foundation investing in the Indian Council for Research on International Economic Relations (ICRIER) in 1982. ICRIER is a nonprofit research center created \u201cto foster improved understanding of policy choices for India in an era of growing international economic integration and interdependence.\u201d</p><p>...</p><p>The need for attribution in philanthropy has led to \u201crigorous impact evaluation,\u201d and to conduct these impact evaluations necessarily requires narrowing and simplifying the problem into legible and calculable forms. The end result is claims that distributing malaria nets is the best way to save lives and prevent illness. Common sense tells us otherwise.</p></blockquote>", "user": {"username": "DavidNash"}}, {"_id": "M6ebKkRos2nQe9gqG", "title": "Reflections on Vox's \"How effective altruism let SBF happen\"", "postedAt": "2022-12-12T16:05:27.963Z", "htmlBody": "<p>Dylan Matthews has an interesting piece up in Vox, '<a href=\"https://www.vox.com/future-perfect/23500014/effective-altruism-sam-bankman-fried-ftx-crypto\">How effective altruism let SBF happen</a>'. &nbsp;I feel very conflicted about it, as I think it contains some criticisms that are importantly correct, but then takes it in a direction I think is importantly mistaken. &nbsp;I'll be curious to hear others' thoughts.</p><p>Here's what I think is most right about it:</p><blockquote><p>There\u2019s still plenty we don\u2019t know, but based on what we do know, I don\u2019t think the problem was earning to give, or billionaire money, or longtermism per se. But the problem does lie in the culture of effective altruism... it is deeply immature and myopic, in a way that enabled Bankman-Fried and Ellison, and it desperately needs to grow up. That means emulating the kinds of practices that more mature philanthropic institutions and movements have used for centuries, and becoming much more risk-averse.</p></blockquote><p>Like many youth-led movements, there's a tendency within EA to be skeptical of established institutions and ways of running things. Such skepticism is healthy in moderation, but taken to extremes can lead to things like FTX's apparent total failure of financial oversight and corporate governance. Installing SBF as a corporate \"philosopher-king\" turns out not to have been great for FTX, in much the same way that we might predict installing a philosopher-king as absolute dictator would not be great for a country.</p><p>I'm obviously very pro-philosophy, and think it offers important practical guidance too, but it's not a <i>substitute</i> for robust institutions. So here is where I feel most conflicted about the article. Because I agree we should be wary of philosopher-kings. But that's mostly just because we should be wary of \"kings\" (or immature dictators) in general.</p><p>So I'm not thrilled with a framing that says (as Matthews goes on to say) that \"the problem is the dominance of philosophy\", because I don't think philosophy tells you to install philosopher-kings. Instead, I'd say, <i>the problem is immaturity, and lack of respect for established institutional guard-rails for good governance</i> (i.e., bureaucracy). <strong>What EA needs to learn, IMO, is this missing respect for \"established\" procedures, and a culture of consulting with more senior advisers who understand how institutions work (and why)</strong>.</p><p>It's important to get this diagnosis right, since there's no reason to think that replacing 30 y/o philosophers with equally young anticapitalist activists (say) would do any good here. What's needed is people with more institutional experience (which will often mean significantly older people), and a sensible division of labour between philosophy and policy, ideas and implementation.</p><p>There are parts of the article that sort of point in this direction, but then it spins away and doesn't quite articulate the problem correctly. Or so it seems to me. But again, curious to hear others' thoughts.</p>", "user": {"username": "RYC"}}, {"_id": "TtWidCRREYnnW35kY", "title": "I'm interviewing historian of abolitionism Christopher Brown. What should I ask him?", "postedAt": "2022-12-12T15:17:46.381Z", "htmlBody": "<p>This week I'm interviewing <a href=\"https://history.columbia.edu/person/brown-christopher/\">Christopher Brown</a> \u2014 history professor at Columbia University \u2014 about the movement for abolition.</p>\n<p>How did it happen? Why did it take off where and when it did? What were the key events that helped it build momentum? Was it inevitable, or historically contingent?</p>\n<p>What should I ask him?</p>\n<p>Even if you haven't read his work, Professor Brown is quoted a number of times in the relevant chapters or MacAskill's 'What We Owe The Future'.</p>\n<p>Some other sources for his views include:</p>\n<ul>\n<li><a href=\"https://www.lrb.co.uk/the-paper/v43/n14/christopher-l.-brown/later-not-now\">Later, Now Now</a> in the London Review of Books (2021)</li>\n<li><a href=\"https://vimeo.com/19528987\">The Fight over Slavery in the Revolutionary Era</a> video (2010)</li>\n<li><a href=\"https://www.amazon.co.uk/Moral-Capital-Foundations-Abolitionism-Williamsburg/dp/0807856983\">Moral Capital: Foundations of British Abolitionism</a> (2006)</li>\n</ul>\n", "user": {"username": "Robert_Wiblin"}}, {"_id": "Buongkf4KXmBP2jiG", "title": "Announcing WildAnimalSuffering.org, a new resource launched for the cause", "postedAt": "2022-12-12T13:48:39.494Z", "htmlBody": "<p>I'm very excited to announce that <a href=\"https://veganhacktivists.org/\">Vegan Hacktivists</a> has just released our latest project, Wild Animal Suffering, for one of EA's highly important and neglected focus areas.</p><p><a href=\"https://wildanimalsuffering.org/\">https://wildanimalsuffering.org/</a></p><p>This website, very briefly, educates the viewer on the issues surrounding Wild Animal Suffering, and provides them with easy to access resources to getting involved and learning more. It's very important to note that this website is <i>not</i> intended to be a deep dive into Wild Animal Suffering, nor have covered everything there is to cover. Our main audience is folks interested in wild life welfare, and our secondary audience is vegans who may be interested in learning more.</p><blockquote><p>\u201cWith its clear, concise explanations and visuals, this site is ideal for people who are looking to learn about what the lives of wild animals are really like, and what we can do to help. I hope it inspires people to think differently about addressing not just anthropogenic harms but also natural ones. This will also be an excellent resource for animal advocates who are looking for effective ways to communicate why helping animals is as essential as refraining from harming them.\u201d</p><p><strong>\u2014Leah McKelvie, </strong><i>Animal Ethics</i></p></blockquote><p>Our primary goal here was to combine the many and various fantastic resources and content surrounding Wild Animal Suffering and turn it into a more visually engaging, friendly, and accessible format. We hope this makes it easier for those in our movement to have one link they can share for folks to consume and get started with.</p><p>We're really excited with the launch, and want to give a special thanks to our friends at <a href=\"https://animal-ethics.org/\">Animal Ethics</a>, <a href=\"https://www.wildanimalinitiative.org/\">Wild Animal Initiative</a>, and <a href=\"https://rethinkpriorities.org/\">Rethink Priorities</a> for lending their expertise. <i>Note that naming these organizations does not constitute their support or endorsement for all of the varied content, opinions, or resources displayed on the site.</i></p><blockquote><p>\u201cWildAnimalSuffering.org offers an accessible, engaging, and visually stunning introduction to the significant and pressing issue of wild animal suffering. The site fills a need by curating the best available information and resources all in one place, and I could see it becoming a key tool in building the movement.\u201d</p><p><strong>\u2014Cat Kerr, </strong><i>Wild Animal Initiative</i></p></blockquote><p>If you'd like to support our launch, feel free to share <a href=\"https://wildanimalsuffering.org/\">this project</a> within your networks if relevant\u2014 otherwise, we hope you enjoy the new resource!</p><p>Thanks for reading.</p>", "user": {"username": "davidvanbeveren"}}, {"_id": "wgtSCg8cFDRXvZzxS", "title": "EA is probably undergoing \"Evaporative Cooling\" right now", "postedAt": "2022-12-12T12:35:17.888Z", "htmlBody": "<p>Eliezer Yudkowsky has an excellent post on \"Evaporative Cooling of Group Beliefs\".</p><p>&nbsp;<a href=\"https://www.lesswrong.com/posts/ZQG9cwKbct2LtmL3p/evaporative-cooling-of-group-beliefs\">https://www.lesswrong.com/posts/ZQG9cwKbct2LtmL3p/evaporative-cooling-of-group-beliefs</a></p><p>&nbsp;</p><p>Essentially, when a group goes through a crisis, those who hold the group's beliefs least strongly leave, and those who hold the group's beliefs most strongly stay.&nbsp;</p><p>This might leave the remaining group less able to identify weaknesses within group beliefs or course-correct, or \"steer\".</p><p>&nbsp;</p><p>The FTX collapse, bad press and bad optics of the Whytham Abbey purchase probably mean that this is happening in EA right now.&nbsp;</p><p>I'm not really sure what to do about this, but one suggestion might be for community building to move away from the model of trying to produce highly-engaged EAs, and switch to trying to produce moderately-engaged EAs, who might be better placed to offer helpful criticisms and help steer the movement towards doing the most good.&nbsp;</p>", "user": {"username": "freedomandutility"}}, {"_id": "aTK43Yp3qHdeJfvrT", "title": "Protecting EV calculations against motivated reasoning", "postedAt": "2022-12-12T12:29:10.524Z", "htmlBody": "<p>As EA moves towards more uncertain interventions, expected value calculations become more uncertain, more prone to motivated reasoning and more prone to self-serving biases.</p><p>&nbsp;</p><p>Here are 3 key ways we could protect against self-serving biases and other forms of motivated reasoning in expected value calculations:</p><ol><li>Have two (or more) individuals, or groups, <i>independently </i>calculate the expected value of a decision and compare results</li><li>In expected value calculations, <strong>identify a theoretical cost</strong><i><strong> </strong></i><strong>at which the intervention would no longer be approximately maximising expected value from the resources</strong></li><li>Keep in mind that EA aims to make decisions that<i> approximately maximise</i> expected value from a set of resources, rather than just make decisions which just have net positive expected value</li></ol>", "user": {"username": "freedomandutility"}}, {"_id": "DEpMK2JdyrnoHGFDv", "title": "You *should* factor optics into EV calculations", "postedAt": "2022-12-12T12:20:15.237Z", "htmlBody": "<p>Regarding the Wytham Abbey purchase, there has been discussion over whether or not optics should be considered when making decisions.</p><p>Some objections include that optics can be hard to correctly predict / understand, and thinking around optics could be prone to motivated reasoning, so optics should be set aside for decision making.</p><p>But the same is true for almost every aspect of EA, aside from the highly empirical randomista development wing!</p><p>Especially over the longer term, optics affects community building, including how many people get into EA, and maybe more importantly, <i>who</i> gets into EA, i.e, what kind of pre-existing beliefs and opinions they bring with them. As EAs aim to improve government policy in EA priority areas, EA's optics affects their ability to do this. Optics also affect how EA ideas diffuse outside of EA, and <i>where</i> they diffuse to.</p><p>Like with every other hard to predict, highly uncertain factor that goes into lots of EA decision making, we should make uncertain estimates around optics anyway, work on constantly refining our predictions around optics, and include optics as a factor when working out the EV of decisions.</p><p>&nbsp;</p><p>(Of course, one might still decide it's worth making big purchases for community building, but optics should be taken into account!)</p>", "user": {"username": "freedomandutility"}}, {"_id": "WBWXLZTF5FPzmK8nh", "title": "Octopuses (Probably) Don't Have Nine Minds", "postedAt": "2022-12-12T11:59:46.260Z", "htmlBody": "<h1>Key Takeaways</h1><p>Here are the key takeaways for the <a href=\"https://docs.google.com/document/d/1OPkIgbvnylU-GF9M0ZxGxrVhlnySxLoq/edit#\"><strong>full report</strong></a>:</p><ol><li>Based on the split-brain condition in humans, some people have wondered whether some humans \u201chouse\u201d multiple subjects.</li><li>Based on superficial parallels between the split-brain condition and the apparent neurological structures of some animals\u2014such as chickens and octopuses\u2014some people have wondered whether those animals house multiple subjects too.</li><li>To assign a non-negligible credence to this possibility, we\u2019d need evidence that parts of these animals aren't just conscious, but that they have valenced conscious states (like pain), as that\u2019s what matters morally (<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw/p/hxtwzcsz8hQfGyZQM\">given our project\u2019s assumptions</a>).</li><li>This evidence is difficult to get:<ol><li>The human case shows that unconscious mentality is powerful, so we can\u2019t infer consciousness from many behaviors.</li><li>Even when we can infer consciousness, we can\u2019t necessarily infer a&nbsp;<i>separate subject</i>.<i>&nbsp;</i>After all, there are plausible interpretations of split-brain cases on which there are&nbsp;<i>not&nbsp;</i>separate subjects.</li><li>Even if there are multiple subjects housed in an organism in&nbsp;<i>some&nbsp;</i>circumstances, it doesn\u2019t follow that there are&nbsp;<i>always&nbsp;</i>multiple subjects. These additional subjects may only be generated in contexts that are irrelevant for practical purposes.</li></ol></li><li>If we don\u2019t have any evidence that parts of these animals are conscious or that they have valenced conscious states, then insofar as we\u2019re committed to having an empirically-driven approach to counting subjects, we shouldn\u2019t postulate multiple subjects in these cases.</li><li>That being said, the author is inclined to place up to a 0.1 credence that there are multiple subjects in the split-brain case, but no higher than 0.025 for the 1+8 model of octopuses.&nbsp;</li></ol><p>&nbsp;</p><h1>Introduction</h1><p>This is the sixth post in the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\">Moral Weight Project Sequence</a>. The aim of the sequence is to provide an overview of the research that Rethink Priorities conducted between May 2021 and October 2022 on interspecific cause prioritization\u2014i.e., making resource allocation decisions across species. <strong>The aim of this post, which was written by </strong><a href=\"https://philpeople.org/profiles/joseph-gottlieb\"><strong>Joe Gottlieb</strong></a><strong>, is to summarize his </strong><a href=\"https://docs.google.com/document/d/1OPkIgbvnylU-GF9M0ZxGxrVhlnySxLoq/edit#\"><strong>full report</strong></a><strong> on the phenomenal unity and cause prioritization</strong>, which explores whether, for certain species, there are empirical reasons to posit multiple welfare subjects per organism. <strong>That report is available </strong><a href=\"https://docs.google.com/document/d/1OPkIgbvnylU-GF9M0ZxGxrVhlnySxLoq/edit#\"><strong>here</strong></a><strong>.</strong></p><h1>Motivations and the Bottom Line</h1><p>We normally assume that there is one conscious subject\u2014or one entity who <i>undergoes</i> conscious experiences\u2014per conscious animal. But perhaps this isn\u2019t always true: perhaps some animals \u2018house\u2019 more than one conscious subject. If those subjects are also <i>welfare subjects</i>\u2014beings with the ability to accrue welfare goods and bad\u2014then this might matter when trying to determine whether we are allocating resources in a way that maximizes expected welfare gained per dollar spent. When we theorize about these animals\u2019 capacity for welfare, we would no longer be theorizing about a single welfare subject, but multiple such subjects.<a href=\"#_ftn1\">[1]</a></p><p>In humans, people have speculated about this possibility based on \u201csplit-brain\u201d cases, where the corpus callosum has been wholly or partially severed (e.g., Bayne 2010; Schechter 2018). Some non-human animals, like birds, approximate the split-brain condition as the norm, and others, like the octopus, exhibit a striking lack of integration and highly decentralized nervous systems, with surprising levels of peripheral autonomy. And in the case of the octopus, Peter Godfrey-Smith suggests that \u201c[w]e should\u2026at least consider the possibility that an octopus is a being with multiple selves\u201d, one for central brain, and then one for each arm (2020: 148; cf. Carls-Diamante 2017, 2019, 2022).</p><p>What follows is a high-level summary of <a href=\"https://docs.google.com/document/d/1OPkIgbvnylU-GF9M0ZxGxrVhlnySxLoq/edit#\">my full report on this topic</a>, focusing on <i>Octopodidae</i>, as if that\u2019s the family for which we have the best evidence for multiple subjects per organism.<a href=\"#_ftn2\">[2]</a> In assessing this possibility, I make three key assumptions:</p><ul><li><strong>Experiential hedonism:</strong> an entity can&nbsp;accrue welfare goods and bads if and only if it can undergo either positively or negatively <i>conscious </i>valenced mental states.</li><li><strong>Mental states can be unconscious</strong>: most, if not all, conscious mental states have unconscious mental counterparts. Moreover, many sophisticated behaviors are caused by&nbsp;unconscious states, and even if caused by conscious states, they are not always caused by those states in virtue of being conscious. Unconscious mentality is quite powerful and routinely underestimated.&nbsp;</li><li><strong>Default to One Subject Assumption:&nbsp;</strong>we begin by provisionally assuming that there is only one subject per animal, per organism, etc. Thus, absent sufficiently robust positive evidence against this default assumption, we should continue to assume that there is one subject per octopus.&nbsp;</li></ul><p>With these assumptions in mind, there are two hypotheses of interest:</p><ul><li><strong>The Action-Relevant Hypothesis</strong>: The default condition for members of&nbsp;<i>Octopodidae</i> is that they house up to 9 welfare subjects, such that for any harm or benefit to any token octopus, we get a 9x impact in expectation.&nbsp;</li><li><strong>The Non-Action-Relevant Hypothesis</strong>: There are some rare contexts\u2014when all arms are amputated, or when the brain is not exerting central control over the arms\u2014where members of <i>Octopodidae</i> either house up to 9 welfare subjects or can \u2018splinter\u2019 into 9 welfare subjects, such that for any harm or benefit to any token octopus, we get a 9x impact in expectation.&nbsp;</li></ul><p>The bottom line is that, based on the arguments I discuss at length in the <a href=\"https://docs.google.com/document/d/1OPkIgbvnylU-GF9M0ZxGxrVhlnySxLoq/edit#\">full report</a>, I assign a credence of 0.025 to the Action-Relevant Hypothesis and a credence of 0.035 to the Non-Action-Relevant Hypothesis.</p><h1>Four Questions about There Being Multiple Subjects Per Octopus&nbsp;</h1><p>In the academic philosophical literature, there is only clear endorsement of, and extended argument for, the claim that there can be more than one subject per animal: namely, Schechter\u2019s (2018) examination of the split-brain condition. However, her arguments do not readily carry over to octopuses. There are several reasons for this, but the most relevant one is this. Schechter\u2019s case&nbsp;<i>starts&nbsp;</i>from the claim that both the right and left hemispheric systems in humans can independently support conscious experience. Then, she infers that the reason why these experiences are not part of a single&nbsp;<i>phenomenally</i>&nbsp;<i>unified</i> experiential perspective is because they fail to be \u00ad<i>access unified</i>.<a href=\"#_ftn3\">[3]</a> Since subjects, according to Schechter, are individuated by experiential perspectives, it follows that split-brain patients house two subjects.&nbsp;</p><p>This argument makes a highly contentious assumption: namely, that failures of&nbsp;<i>access</i> unity entail failures of&nbsp;<i>phenomenal</i> unity.<a href=\"#_ftn4\">[4]</a> Even if we grant it, we can\u2019t make an analogous starting assumption to the effect that each octopus arm is, on its own, sufficient for consciousness.<a href=\"#_ftn5\">[5]</a> The question\u2014or at least one of our questions\u2014is whether this assumption is true.&nbsp;</p><p>So, we can split our task into three questions, where a \u2018yes\u2019 answer to each subsequent one is predicated on a \u2018yes\u2019 to the prior question:</p><ol><li><strong>The Mind Question:&nbsp;</strong>Do octopuses generally house 9 minded subjects (or at least more than one minded subject)?</li><li><strong>The Conscious Mind Question:&nbsp;</strong>Do octopuses generally house 9 <i>consciously</i> minded subjects\u2014that is, 9 subjects capable of being in conscious mental states (or at least more than one consciously minded subject)?</li><li><strong>The Welfare Subject Question:&nbsp;</strong>Do octopuses generally house 9 conscious, <i>affectively</i> minded subjects\u2014that is, 9 subjects capable of being in conscious affective mental states (or at least more than one conscious, affectively minded subject)?</li><li><strong>The Correlation Question:&nbsp;</strong>If octopuses generally house more than one conscious, affectively minded subject,<strong>&nbsp;</strong>are the harms and benefits of those subjects correlated, such that harming one subject affects the welfare of other subjects housed in the same organism?</li></ol><p>Q3 and Q4, of course, are what ultimately matter\u2014but we have to get through Q1 and Q2 to get to them. My take is that there\u2019s <i>some</i> evidence for a \u2018yes\u2019 answer to Q1, but no evidence for a \u2018yes\u2019 answer to Q2 and Q3. So, Q4 doesn\u2019t even come up.</p><h2>Question 1: The Mind Question</h2><p>Carls-Diamante (2019) provides the most sustained case that each arm constitutes an independent cognitive system, i.e., not merely a cognitive subsystem. If we let each independent cognitive system count as a subject, it follows that each arm constitutes a subject. Carls-Diamante\u2019s case hinges largely on the cognitive role of the arms in storing and carrying out stereotypic motor programs (as seen in <i>fetching</i>), along with their functional autonomy and self-sufficiency, illustrated by the sparse connections between them and the central components of the octopus nervous system. This autonomy is most striking in cases of amputation, where the arms retain much of their sensorimotor control and processing functions, including grasping behavior elicited by tactile stimulation of the suckers (Rowell 1963).</p><p>But there are at least two problems with Carls-Diamante\u2019s position. First, by her own lights, she\u2019s working with a \u201crelaxed\u201d (2019: 465) stance on what counts as cognition, saying that at best sensorimotor coordination is the most \u201crudimentary\u201d form of cognition. This is a controversial and deflationary interpretation of cognition. That\u2019s fine in itself, but it significantly weakens the inference to consciousness. Second, if we only have multiple subjects when the octopus arms are amputated, as Carls-Diamante suggests (2019: 478), then we <i>might&nbsp;</i>get a case for conscious arms, but it will be a case that\u2019s basically irrelevant to&nbsp;<strong>The Action-Relevant Hypothesis</strong>, since arms usually aren\u2019t amputated prior to death.<a href=\"#_ftn6\">[6]</a>&nbsp;</p><h2>Question 2: The Conscious Mind Question</h2><p>There\u2019s little doubt that whole octopuses are conscious (Mather 2008). But the animal\u2019s being conscious doesn\u2019t imply that each octopus arm is individually conscious. If we <i>grant</i> that each arm is a subject (because it constitutes an independent cognitive system), then we can ask whether the states of those arm-based subjects are conscious in much&nbsp;the same way as we would ask this question of anything else.</p><p>How do we assess consciousness? Typically, we either look for proxies for consciousness, such as exhibiting trace conditioning (Birch 2022), or we reason from a theory. Either way, there doesn\u2019t seem to be any positive evidence for thinking that the octopus arms are conscious. Sensory-motor processing isn\u2019t necessarily conscious, we have no evidence that the arm-based systems have a global workspace or are capable of higher-order representation, and the arms don\u2019t exhibit trace-conditioning, rapid-reversal learning, or anything else that might serve as a positive marker. Thus, given that <strong>Mental States can be Unconscious&nbsp;</strong>and <strong>the Default to One Subject Assumption</strong>, there is no reason to think that the octopus houses more than one conscious subject.<a href=\"#_ftn7\">[7]</a></p><h2>Question 3: The Welfare Subject Question</h2><p>Suppose that each arm-based system has its own conscious states. This does not mean that these states are affective. Given <strong>experiential hedonism</strong>, this is a necessary (and sufficient) condition for these systems to constitute welfare subjects. But whether the arms have conscious affective states depends on the kinds of states they have, which even those sympathetic to there being multiple subjects, like Carls-Diamante (2017: 1279), take to be quite limited. Of course, an octopus arm-based subject only needs to instantiate one kind of affective state to be a welfare subject. But we have no evidence that arm-based subjects can feel sad or happy or undergo prolonged bouts of depression, nor even that they can be in pain. Now again\u2014keeping with a common refrain\u2014we <i>do&nbsp;</i>have evidence that <i>octopuses&nbsp;</i>can be in pain. For example, in a study by Crook (2021) of responses to injury in the pygmy octopus (<i>Octopus bocki</i>), directed grooming at the location of acetic acid injection was demonstrated only for that grooming to cease upon application of lidocaine. In addition, the octopuses preferred chambers in which they had been placed after being given lidocaine over chambers in which they were given the initial injection. This could be evidence of valenced pain. However, Crook (Ibid.) is clear that noxious sensory information is not processed in the arms, but in the central brain. This suggests that when acetic acid is injected into one of the arms, pain may be felt <i>in&nbsp;</i>the arm but not <i>by&nbsp;</i>the arm.</p><p>On the other hand, in Rowell\u2019s (1963) experiments on amputated octopus arms, it was found that pricking am amputated arm with a pin resulted in flinching of the skin and the arm moving away from the direction of the stimulus. Does this suggest that the arm-based systems are in conscious pain? There are three points to make here. First, given that this involves amputated arms, there is again the question of whether this speaks to&nbsp;the<strong> Action-Relevant&nbsp;</strong>or the <strong>Non-Action-Relevant Hypothesis</strong>. Second, it isn\u2019t obvious that such behavior marks <i>valenced&nbsp;</i>pain instead of (just) <i>pain</i> since we have evidence from pain asymbolia (Bain 2014) but also more mundane cases that pain is not necessarily painful. Valence requires more than mere withdrawal and reactive behaviors (Shevlin 2021).<a href=\"#_ftn8\">[8]</a></p><p>Finally, while this behavior <i>can&nbsp;</i>be by caused conscious pain states, as before, this doesn\u2019t mean that such states cause such behaviors in virtue of being conscious. Indeed, we have evidence that withdrawal behavior is frequently unconscious. Noxious stimulation can cause humans in vegetative states to yell, withdraw or display \u2018pained\u2019 facial expressions (Laureys 2007). In addition, the lower limbs of complete spinal cord patients, in which the patients cannot feel anything, still exhibit the withdrawal flexion reflex (Dimitrijevi\u0107 &amp; Nathan 1968; Key 2016: 4).<a href=\"#_ftn9\">[9]</a></p><h1>Conclusion</h1><p>The upshot here is straightforward. We don\u2019t seem to have a good reason to suppose that independent octopus arms are conscious subjects, much less welfare subjects. And if they aren\u2019t, then we should assign very low credences to the key hypotheses:</p><ul><li><strong>The Action-Relevant Hypothesis</strong>: The default condition for members of&nbsp;<i>Octopodidae</i> is that they house up to 9 welfare subjects, such that for any harm or benefit to any token octopus, we get a 9x impact in expectation.&nbsp;</li><li><strong>The Non-Action-Relevant Hypothesis</strong>: There are some rare contexts\u2014when all arms are amputated, or when the brain is not exerting central control over the arms\u2014where members of <i>Octopodidae</i> either house up to 9 welfare subjects or can \u2018splinter\u2019 into 9 welfare subjects, such that for any harm or benefit to any token octopus, we get a 9x impact in expectation.&nbsp;</li></ul><p>In the <a href=\"https://docs.google.com/document/d/1OPkIgbvnylU-GF9M0ZxGxrVhlnySxLoq/edit#\">full report</a>, I argue for all this in more detail; I also make the same point about chickens. Whatever the intuitive appeal of the <i>possibility&nbsp;</i>of there being multiple subjects per organism in these cases, that possibility probably isn\u2019t the way things are.</p><p>&nbsp;</p><h1>Acknowledgments</h1><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670854102/mirroredImages/WBWXLZTF5FPzmK8nh/he8gizy8i9ceocjxkrs5.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5034bdbd9818224809cde7bb38ce42666f39b57b0c01415a.png/w_1600 1600w\"><br>This research is a project of Rethink Priorities. It was written by Joe Gottlieb. Thanks to Bob Fischer for helpful feedback on this research. If you\u2019re interested in RP\u2019s work, you can learn more by visiting our <a href=\"https://www.rethinkpriorities.org/research\">research database</a>. For regular updates, please consider<i> </i>subscribing to our<a href=\"https://www.rethinkpriorities.org/newsletter\"> newsletter</a>.</p><p>&nbsp;</p><h1>References</h1><p>Bain, D. (2014). Pains that don\u2019t hurt. <i>Australasian Journal of Philosophy</i>, 92(2), 305-320.&nbsp;<a href=\"https://doi.org/10.1080/00048402.2013.822399\">https://doi.org/10.1080/00048402.2013.822399</a></p><p>Bayne, T. (2010). <i>The Unity of Consciousness.</i> Oxford University Press.&nbsp;<a href=\"https://doi.org/10.1093/acprof:oso/9780199215386.001.0001\">https://doi.org/10.1093/acprof:oso/9780199215386.001.0001</a></p><p>Birch J. (2022) The search for invertebrate consciousness. <i>Nous</i>. 56(1): 133-153.&nbsp;</p><p>Block, N. (2007). Consciousness, accessibility, and the mesh between psychology and neuroscience. <i>Behavioral and Brain Sciences</i>, 30(5-6), 481-499.&nbsp;<a href=\"https://doi.org/10.1017/S0140525X07002786\">https://doi.org/10.1017/S0140525X07002786</a></p><p>Bublitz A, Dehnhardt G and Hanke FD (2021). Reversal of a Spatial Discrimination Task in the Common Octopus (Octopus vulgaris). <i>Front. Behav. Neurosci</i>. 15:614523.<br>doi: 10.3389/fnbeh.2021.614523</p><p>Carruthers, P. (2018). Valence and Value. <i>Philosophy and Phenomenological Research</i>, 97, 658-680. https://doi.org/10.1111/phpr.12395</p><p>Carls-Diamante, S. (2017). The octopus and the unity of consciousness. <i>Biology and Philosophy</i>, 32, 1269-1287.&nbsp;<a href=\"https://doi.org/10.1007/s10539-017-9604-0\">https://doi.org/10.1007/s10539-017-9604-0</a></p><p>Carls-Diamante, S. (2019). Out on a limb? On multiple cognitive systems within the octopus nervous system. <i>Philosophical Psychology</i>, 32(4), 463\u2013482.&nbsp;<a href=\"https://doi.org/10.1080/09515089.2019.1585797\">https://doi.org/10.1080/09515089.2019.1585797</a></p><p>Dimitrijevi\u00b4c, M. R., &amp; Nathan, P. W. (1968). Studies of spasticity in man: Analysis of reflex activity evoked by noxious cutaneous stimulation. <i>Brain</i>, 91(2), 349\u2013368. <a href=\"https://doi.org/10.1093/brain/91.2.349\">https://doi.org/10.1093/brain/91.2.349</a></p><p>Dung, L. (2022). Assessing Tests of Animal Consciousness. <i>Consciousness and Cognition</i> 105: 103410.</p><p>Godfrey-Smith, P. (2020). <i>Metazoa: Animal life and the birth of the mind</i>. Farrar, Straus and Giroux.</p><p>Irvine, E. (2020). Developing Valid Behavioral Indicators of Animal Pain. <i>Philosophical Topics</i>, 48(1), 129\u2013153.&nbsp;<a href=\"https://doi.org/10.5840/philtopics20204817\">https://doi.org/10.5840/philtopics20204817</a></p><p>Key, B. (2016). Why fish do not feel pain. <i>Animal Sentience</i>, 1(3).</p><p>Laureys, S. (2007). Eyes Open, Brain Shut<i>. Scientific American</i>, 296(5), 84\u201389. https://doi.org/10.1038/scientificamerican0507-84</p><p>Marks, C. (1980). <i>Commissurotomy, Consciousness, and Unity of Mind</i>. MIT Press.</p><p>Mather, J. (2008). Cephalopod consciousness: Behavioural evidence. <i>Consciousness and Cognition</i>, 17: 37\u201348</p><p>Rowell, C. H. F. (1963). Excitatory and inhibitory pathways in the arm of octopus. <i>Journal of Experimental Biology</i>, 40, 257\u2013270.</p><p>Schechter, E. (2018). <i>Self- Consciousness and \u201cSplit\u201d Brains: The Minds\u2019 I</i>. Oxford University Press.</p><p>Shevlin, H. (2021). Negative valence, felt unpleasantness, and animal welfare. https://henryshevlin.com/wp-content/uploads/2021/11/Felt-unpleasantness.pdf</p><p>Tye, M. (2003<i>). Consciousness and Persons: Unity and Identity</i>. MIT Press.<br>&nbsp;</p><h1>Notes</h1><p><a href=\"#_ftnref1\">[1]</a> This, of course, is setting aside the welfare ranges for each of these constituent subject; it could be that, within an individual animal, while there are multiple subjects, not all subjects have same welfare ranges, with some being far narrower than others.&nbsp;</p><p><a href=\"#_ftnref2\">[2]</a> The <a href=\"https://docs.google.com/document/d/1OPkIgbvnylU-GF9M0ZxGxrVhlnySxLoq/edit#\">full report</a> also includes extensive discussion of the split-brain condition in humans, along with a discussion of whether members of&nbsp;<i>Gallus gallus domesticus</i> house more than one welfare subject.&nbsp;</p><p><a href=\"#_ftnref3\">[3]</a> Two experiences E1 and E2 are <i>phenomenally unified</i> when there is something it is like to have E1 and E2 together in a way that is not merely conjunctive. Two experiences E1 and E2 are <i>access unified</i> when their contents can be jointly reported on, and jointly employed in the rational control of reasoning and behavior.</p><p><a href=\"#_ftnref4\">[4]</a> This is rejected, for example, by Bayne (2010). Also relevant here is experimental evidence (e.g., from the Sperling paradigm) for phenomenal overflow: phenomenally conscious states that are not accessed, if not accessible at all (Block 2007).</p><p><a href=\"#_ftnref5\">[5]</a>&nbsp;This is probably why one of the few people to write on this topic,&nbsp;Carls-Diamante\u2019s (2017), conditionalizes her thesis: \u201c<i>if</i> the brain and the arms can generate local conscious fields, the issue arises as to whether subjective experience in an octopus would be integrated or unified, given the sparseness of interactions between the components of its nervous system\u201d (2017: 1273, emphasis added).</p><p><a href=\"#_ftnref6\">[6]</a> This is reminiscent of the \u201ccontextualist model\u201d of split-brain patients, where we only have two subjects when under experimental conditions (Marks 1980; Tye 2003). Godfrey-Smith (2020) favors something like this approach for the octopus, but even then, he still thinks there is so-called <i>partial unity</i> for affective states. Roughly, this means that while there are contextually two subjects, there is only one (e.g.) token pain \u201cshared\u201d across each subject in those contexts.&nbsp;</p><p><a href=\"#_ftnref7\">[7]</a> Interestingly, it has been argued that octopuses <i>do </i>have something akin to a global workspace (Mather 2008) and <i>are&nbsp;</i>capable of<i>&nbsp;</i>rapid-reversal learning (Bublitz <i>et al</i> 2021), but again, this does not tell us that the <i>arms&nbsp;</i>have and can do these things.&nbsp;Presumably, if (somehow) <i>my&nbsp;</i>mental states were in <i>your</i> global workspace, that wouldn\u2019t make <i>me&nbsp;</i>have conscious experiences.&nbsp;</p><p><a href=\"#_ftnref8\">[8]</a> Even if these states <i>were&nbsp;</i>valenced, this wouldn\u2019t necessarily show that these states were conscious (see fn. 4), or that octopus arms were conscious (by having any other conscious states, for instance). Motivational trade-offs are a hallmark of valenced states, but Irvine (2020) argues that even <i>C. elegans&nbsp;</i>perform such trade-offs. Presumably, <i>C. elegans</i> are not conscious in any way.</p><p><a href=\"#_ftnref9\">[9]</a> For further discussion on this, see Dung (2022).&nbsp;</p>", "user": {"username": "bob-fischer"}}, {"_id": "WsRjM7nMndTWKtkfY", "title": "The Motte and Bailey for Expected Value ", "postedAt": "2022-12-12T11:54:59.085Z", "htmlBody": "<p>A key distinguishing feature of <em>effective</em> altruism is that it aims to spend resources in a way that approximately <em>maximises</em> expected value.</p>\n<p>Defending extravagant purchases for community building because the decisions have positive expected value, is committing a Motte and Bailey fallacy, with the Motte being \u201cwe should maximise EV\u201d and the Bailey being \u201cwe should make positive EV decisions\u201d.</p>\n<p>Donating to your local opera house has positive EV too.</p>\n", "user": {"username": "freedomandutility"}}, {"_id": "rMucQN9EkL8u4xnjd", "title": "Improving EA events: start early & invest in content and stewardship", "postedAt": "2022-12-12T19:43:27.512Z", "htmlBody": "<h1><strong>TL:DR;</strong></h1><p>The way many EA events<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3n4x16dzkdu\"><sup><a href=\"#fn3n4x16dzkdu\">[1]</a></sup></span>&nbsp;are run is unsustainable for event organisers and does not leave sufficient slack for running excellent events and experimenting for learning.&nbsp;</p><p>We share our models for better event planning and highlight some key challenges with current approaches to event operations (event ops). We make the case that <a href=\"https://forum.effectivealtruism.org/posts/rMucQN9EkL8u4xnjd/how-events-in-ea-could-be-improved-start-early-and-invest-in#Generally_speaking__hire_early___onboard_quickly\">hiring early</a>, <a href=\"https://forum.effectivealtruism.org/posts/rMucQN9EkL8u4xnjd/how-events-in-ea-could-be-improved-start-early-and-invest-in#Venue___logistics__the_base_of_the_pyramid\">finding a venue and outsourcing logistics</a> will create more capacity to invest in the most important (but relatively neglected) aspects of event ops - <a href=\"https://forum.effectivealtruism.org/posts/rMucQN9EkL8u4xnjd/how-events-in-ea-could-be-improved-start-early-and-invest-in#Invest_in_high_quality_content___thoughtful_structure\">content, structure</a> and <a href=\"https://forum.effectivealtruism.org/posts/rMucQN9EkL8u4xnjd/how-events-in-ea-could-be-improved-start-early-and-invest-in#Stewardship_and_the_Attendee_s_Journey\">stewardship</a>. More capacity can also allow organisers to deviate from defaults and innovate with new event formats and styles, and engage in more resource-intensive programming.&nbsp;</p><p>We aim to set realistic expectations and provide practical guidance for newer organisers to better prepare them. <strong>Good event ops people are not easily replaceable.</strong> We\u2019ve observed a trend of event organisers being put under a lot of (unnecessary) pressure due to a lack of planning and capacity. We give a bunch of real (anonymised) examples throughout this post to illustrate our point.&nbsp;</p><p>We don't go into specific suggestions because it's very event-dependent, but we'd estimate that adding ~20-40% of lead time and/or capacity would help achieve these goals.&nbsp;</p><p><i>We\u2019d love to hear if you're an event organiser and don\u2019t find this helpful or actionable. Although we're only talking about events, we think it's possible there are similar trends in other areas -we\u2019d love to hear about them if you've got observations!</i></p><h1><strong>The Hierarchy of Events Planning</strong></h1><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1671642828/mirroredImages/rMucQN9EkL8u4xnjd/ce66x6jvoaniogz5jrsz.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fdc46efac694e89c55ab09d549042c71799e5682ea3d1719.png/w_1057 1057w\"><figcaption>The Hierarchy of Event Planning</figcaption></figure><p>In <a href=\"https://en.wikipedia.org/wiki/Maslow%27s_hierarchy_of_needs\">Maslow\u2019s Hierarchy of Needs</a>, needs lower down in the hierarchy must be satisfied before individuals can attend to needs higher up. Our pyramid is similar - components lower down in the hierarchy are more basic and are necessary for an event to exist. The exact configuration of each component affects the ones above it (e.g. if your venue only has 2 rooms, you probably can\u2019t have an agenda with 3 tracks of events running simultaneously).</p><p>However, influence is not strictly linear - so the <strong>structure</strong> of the event might influence which <strong>venue</strong> you choose (e.g. if you want to create an informal atmosphere, you might choose to host an event in a smaller, more casual venue instead of a formal one, or if you need a lot of <strong>stewardship</strong> for newer attendees, you might hire a <strong>larger team</strong> to manage those needs).</p><h3><strong>Consider Leverage</strong></h3><p>If you could invest 1-10% more resources (time) for 10-30% return, this seems worth considering. In the following, we\u2019re going to try and point out the highest leverage points of each step of the pyramid - the places where spending that extra time could make a big difference and open up more possibilities.</p><h3><strong>Work backwards from specific goals</strong></h3><p><i>aka don\u2019t start with \u201cI want to run a conference\u201d</i></p><p>It may sound trite, but no event makes sense without specific, concrete, and measurable goals - you need to know the <a href=\"https://medium.com/@michelbachmann/start-with-who-15b8857ed718\">who and why</a>, and then work backwards from there. Start with a specific goal like \u201cI want to facilitate peer- to peer bonding for my local university group\u201d and then list all the ways that could achieved, (host a social dinner, a retreat, a 1-day hike, a group project etc.), rather than do an event format because it's been done before and seems effective.&nbsp;</p><p>Here are some goals an event might have - note that these goals are pretty different from each other, and it\u2019s pretty hard to create events that could excel more than a couple at once:</p><ul><li>Networking - participants form important connections with others - either peers or mentors</li><li>Learning/Skilling up - participants learn from each other or experts on a particular topic</li><li>Idea sharing - participants learn industry best practices, cutting edge research and</li><li>Talent coordination - candidates are matched to potential employers</li><li>Project launchpad - participants found new projects</li><li>Bonding - participants form bonds, become more motivated, feel part of the community</li><li>Reflection session - reflecting on past actions, decisions, efforts etc for the purpose of growth and learning</li><li>Targeted learning about what is impactful (see below)</li><li>Reflection session - reflecting on past actions, decisions, efforts etc for the purpose of growth and learning</li><li>Targeted learning about what works and what doesn't (see below)</li></ul><p>We think that <strong>targeted learning</strong> should always be a goal. EA is a young movement - we\u2019ve made, are making, and will make mistakes. We have a lot to learn about what works best. The ultimate goal of any event is first and foremost to have direct impact, but secondly it\u2019s to experiment and share how it went (with high-quality templates &amp; guides) to benefit other groups. If it goes terribly - that\u2019s super valuable too!! There isn\u2019t systematic research on this topic, things are more of a mess than they seem.</p><h1><strong>Hire early &amp; onboard quickly</strong></h1><p>I think people are the most important factor in determining the success of a project. Charity Entrepreneurship factors the availability of talent when thinking about how promising a project is.</p><p>Hire early so that you have a realistic sense of your capacity and ability to be ambitious, and your team\u2019s competency. Teams often hire late and the shorter your planning runway, the less likely it is that you\u2019ll do a good job of people management. Many people running EA events are junior and have limited management experience as is - adding additional constraints like a short planning runway will only exacerbate this. You could require team members who are very autonomous - but that will narrow the talent pool you can draw from. Hiring early can also help with allowing for sufficient risk mitigation, pre-mortem planning, and booking speakers (if needed) in advance.</p><p><i>Here are some anonymised examples from a variety of different events ran in the past year.</i></p><blockquote><p><strong>An event that had hiring issues </strong>This was a 200 person 2-day event. The core organiser got a grant and oversaw the whole event and hiring process. They ran multiple hiring rounds, but ended up not filling many positions. Ultimately, they were left hiring team members much later about a month before the event which meant that the team was behind schedule. This ended up really impacting the quality of the event (subpar programming, messy logistics, venue and catering issues, etc).</p><p><strong>An event that did extra hiring well </strong><i>One EAG(x) conference did a great job with hiring. They a) hired early (about 3 months before their event for core team members and 1-2 months for support contractors, b) had very clear communication about the roles, the hiring process, timelines and more, c) had work very well divided into team (ex. by content, admissions. communication, venue and logistics) with clear communication within and across teams. This overall lead to a really successful event that was planned well and not overly burdensome on any one organiser.</i></p><p><strong>An event with semi-decent hiring</strong> <i>At one weekend event, organisers hired one good person but that person needed more support. Everything ultimately ran smoothy - but they could have done a better job with communication, management, and managing timelines. One area for improvement is to make sure someone with managing experience is consulted or overseeing the process because one key need with hiring and having a larger team is managing. And managing is not easy and requires learning.</i></p></blockquote><p>The <a href=\"https://pineappleoperations.org/\">Pineapple Operations directory</a> has a list of people open for ops roles. You can also ask around to trusted community members for references or look for previous event organisers.</p><h1><strong>Secure a good venue &amp; outsource logistics</strong></h1><h3><strong>Without a venue, you can\u2019t start planning the event.</strong></h3><p>Like physiological needs, the venue is necessary but not sufficient for an impactful event. The general advice here is to <strong>lock-in the venue early</strong> - because it shapes and if well-chosen, can boost efforts in other domains - like content and structure.&nbsp;</p><p>Finding a brightly lit restaurant with vegan options and large tables was great for creating a warm and friendly atmosphere at EA Philadelphia monthly socials. Retreats in the country side provide lots of nature to explore which can be good for creating a focused environment (<a href=\"https://www.notion.so/Hierarchy-of-Event-Planning-Why-You-Should-Start-Early-and-Invest-in-Content-and-Stewardship-3b85febbca0145b0afbe0ef226dcb7ba\">see more on retreats</a>). Conferences with lots of smaller rooms and breakout spaces can be great, but if they are physically remote (e.g. long corridors) you may get a less serendipitous encounters or impromptu conversations. Chill, hangout rooms are usually always a good idea.</p><p>One of the venues at EAG London has a conservatory that\u2019s a lovely break from the normal conference venue. It\u2019s quiet and peaceful, and can be less overwhelming.</p><figure class=\"image image_resized\" style=\"width:96.63%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1671642828/mirroredImages/rMucQN9EkL8u4xnjd/amkngn3whw3cnnfn0lka.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/ec25b11b1baf5db999ad1d57cd672f7e3c9d2357247a4383.jpg/w_1024 1024w\"></figure><blockquote><p>The EAGxBoston <a href=\"https://forum.effectivealtruism.org/posts/NLmZyAq32AT9JjjSK/eagxboston-2022-retrospective#Things_we_could_have_done_better__\">retrospective</a> identified a number of things that could have been done sooner: deciding the conference date, securing the venue, launching Swapcard and schedule, and organising hotel and accomodations.</p></blockquote><h3><strong>Logistics are critical, but can be outsourced.</strong></h3><p>What can the venue take care of work for you? Some important areas a venue can handle include catering, cleaning, AV, room set-up and security. We advise working with as few external vendors / agents as possible because it saves on hiring, vetting, and coordination time. Although it will typically cost more, when you account for all those costs, we would expect this to be worth it.</p><p>The less time your team spends on these tasks directly (or on managing others to do the tasks) the more you can invest on things higher up in the event planning hierarchy - content, structure &amp; stewardship - the pieces that are really unique to your event, and that are harder to delegate.</p><blockquote><p>\u201cFor parts of EAGxSingapore and the EA SG Unconference, we had a venue provided operations staff, vegan catering, drinks, wifi, security/first aider and AV support. Getting logistics in-house makes the process smoother and means that there is already some kind of structure in place.</p><p>This venue also had in-house operations staff. In some places you can hire people or recruit volunteers but in non-hubs where EA contractors are less readily available, this is an easy way to outsource simple tasks like stocking water bottles or cleaning up cups. On the second day we shifted over to another venue which had no in-house staff, and the difference was clear. Most of the team was involved in the clean-up for 300 attendees and it was hard to coordinate and keep track of volunteers since the venue was also spread out over three floors.</p><p>For the Community Builders in Asia retreat, the venue provided accommodations, catering, operations staff, water, supplies bean bags, AV support, signage, security/first aider. I just needed to confirm pax, room type, and the venue room. My team could just go up to the counter and they would sort out the problem for us. This meant that two team members could add value by improving the overall epistemics of the event, while the other two were able to participate more in the event and network. We saved at least 40 people hours because of this.\u201d - Dion Tan, EAGxSingapore organiser</p></blockquote><h1>Invest in high-quality content &amp; thoughtful structure</h1><p>Content is any kind of programming that you organise - before, during and after the event. One of the reasons we think it\u2019s valuable to delegate as much of the logistics is possible, is that we high quality and intentional content and structure based on the goal(s) of your event can make a difference on the impact of your event. We think there are opportunities to exploit your (team\u2019s) unique counterfactual advantages, and find areas to leverage your effort and resources. &nbsp;The following are some (debatable) opinions we have:</p><p><strong>Not enough experimentation</strong></p><p>We don't think there's enough experimentation happening right now to create high quality, engaging content. Partly because of the short planning timelines, partly due to default bias towards well known, less organiser-effort formats like talks (it can be easier to ask speakers to plan a talk rather than run more time-intensive formats).&nbsp;<br><br>Another reason is that we don\u2019t think content is <i>seen</i> as very valuable. Many EA events focus a lot on 1-1 interactions and common advice is to skip programmed events to meet people - while that's definitely a valid option, we think that perhaps we should be going harder on one thing or the other - making engaging content people want to go to or restructuring events to be more focused on networking.&nbsp;</p><p><strong>Less is more</strong></p><p>Given this, less is more. Engaging content takes time to develop, and it\u2019s okay for an event to have fewer sessions, and/or<strong> repeat valuable sessions</strong>. Typically, interactive events with small groups (workshops, discussion groups) will be more engaging and valuable than talks or lectures. These kind of activities probably don\u2019t need big name speakers - you could recruit and train good facilitators who have more time. Senior subject matter experts can definitely add a lot - like career or research advice - but their time is limited. You could design a program in a way that the senior person could be pulled in to do small pieces of it which don\u2019t require preparation (e.g. doing a few 1-1s, answering questions, giving feedback). An expert\u2019s time can be leveraged if it\u2019s paired with really good structure so that participants use their time valuably.&nbsp;</p><blockquote><p>Some organisers ran a <a href=\"https://vaidehiagarwalla.com/local-career-advice-network/project-outputs/career-1-1-training-guide\">career 1-1 training workshop for community builders</a> in 2020. The workshop itself was run by a non-expert (Vaidehi) who conducted research &amp; interviews with community builders who had conducted lots of 1-1s, and then developed a multi-week workshop that covered the basics of 1-1s. Each participant was paired with a mentor to get specific, targeted feedback. Not only was it often a counterfactual mentor match, but it seems like both the structure &amp; the mentorship was beneficial.</p></blockquote><p><strong>There aren't enough peer-to-peer activities&nbsp;</strong></p><p>There's a lack of activities that empower people to become better independent thinkers, help each other and build close relationships with each other - such as peer-to-peer feedback, personal support sessions, and epistemic activities. Leaders and more senior EA community members cannot (and perhaps should not) always be heavily involved in shaping someone\u2019s impact journey. We think that more focus should be given to building and empowering thoughtful, curious people, but don't feel like much serious thought has been given on how to do that.</p><p><strong>There needs to be more opportunities for practicing core values and norms&nbsp;</strong></p><p>Events are usually not very long - usually a few days. This length of time is not suited to make lots of progress on a specific issue, but instead well suited to expand someone\u2019s option space (novel ideas, perspectives or opportunities), build (new) relationships, practice core values or norms, get advice or inputs and feel a sense of belongingness to the community. There aren\u2019t enough opportunities at events for people to practice core values or approaches to doing good. Topics we think are especially important and feel neglected are epistemic rigour, reasoning transparency, and good decision-making. The benefit is two-fold - creating a space for people to practice and learn (or relearn) about those topics, and signalling to attendees (especially newer people) the importance we place on those values or norms.</p><p>If someone goes to an event and does a complete 180 and makes a huge decision, that makes us slightly-worried. On the other hand, if they were already thinking about it and this pushes them over the edge, or sets them on a path of exploration, or helps them question an assumption they had, then that\u2019s awesome!</p><p><strong>Make the conference easy to navigate (virtually and physically)&nbsp;</strong></p><p>Structure is the set-up of the event - both physical like space design and virtual like the conference app or communication channels - that encourage or discourage particular behaviours, interactions and atmosphere. Events focused on participant-driven interaction (e.g. 1-1s, group discussions) need to pay more attention to structure.&nbsp;</p><p>With more time and planning, you can invest more time in developing a good structure, which could address:</p><ul><li>Think about issues in booking meeting rooms before the conference. &nbsp;Unfortunately, the conference apps have bad UX and it's difficult to change the meeting location after a meeting has been booked, so it's easier for people to just book the right room to start.&nbsp;<ul><li>Often, attendees will default to \"meeting room 1\" because it's the easiest place to meet. Can you add limits to meeting rooms, randomize the meeting room order, or add some simple descriptions to the rooms e.g. \"SOC210 - furthest down the hall\"?</li></ul></li><li>What are the entrance points, and are they hard to find? If so, make sure to include some context in the pre-event emails, and schedule a notification on the conference app. Make it easy for attendees to have a contact person to help them if needed. &nbsp;</li><li>You can never have too many signs to help navigate a large space.&nbsp;<ul><li>Signs should be large, with plain simple backgrounds so they can be easily read from a distance. It often happens that someone walks for a minute to realise they are at the wrong spot.&nbsp;</li><li>Conference centers are often large and labyrinthine - it can be helpful to distinguish different rooms or areas with giant signs, unique names (e.g. not North &amp; South/1 &amp; 2), straightforward descriptions (near the giant angel statue) and more.&nbsp;</li><li>Large floor maps near registration and key meeting points with large \"you are here\" stickers can also help.&nbsp;</li><li>The day before the event get a somebody who's never seen the venue to do a walkthrough before the event, and see if they can find va,rious rooms based on signage and maps you've prepared.&nbsp;</li></ul></li><li>What are the expectations and norms around what to do, which activities to attend and how to prioritise time?&nbsp;<ul><li>With existing events (e.g. EAGx) it might be harder to change existing norms since attendees may hear about event norms from previous conference attendees. In those cases, you'll be more likely to nudge people, rather than totally change their view.&nbsp;</li><li>With newer participatnts, some norms (e.g. booking 1-1s) are hard to establish and need more concerted effort - that's where stewardship comes in.</li></ul></li><li>If attendees lack shared context or are strangers, how can we ensure they are able to communicate effectively with each other?<ul><li>If you're hosting an event which brings together newer attendees with less shared context&nbsp;</li></ul></li><li>How can we make sure that attendees interact with people they don\u2019t already know? You can set up open areas with seating charts intentionally designed to let you meet with new people (cards that say \u201cjoin me\u201d, or blocking 1-1s being scheduled during mealtimes).</li></ul><blockquote><p>EA NYC uses stickers on their name badges which indicate different cause area preferences or interests, so people have natural icebreakers if they see someone at a picnic.</p></blockquote><ul><li>How can we facilitate coordination that will be particularly difficult without this event - e.g. it would be hard to get this specific group of people in the same place at the same time? Can you make better interactions than the default (what is the default)? Have you set up the conditions for people to coordinate themselves? Would people coordinate themselves, even with the right conditions?</li></ul><blockquote><p>The conference app during EAG London 2019 allowed for self-organised group events during the conference. I (Vaidehi) was able to organise a few meetups including one for the local career advice network, and &nbsp;and a number of fun activities were also planned, which allowed a higher degree of coordination than previous EAGs. I think this format worked well for an EAG, where many participants were already deeply engaged in EA and had specific (overlapping) goals and interests they wanted to meet about. Conferences with Slack Workspaces have also been helpful for coordinating meet-ups and connections around shared interests.&nbsp;</p></blockquote><ul><li>How does space facilitate interactions? What is atmosphere? How does it work? How do we want attendees to feel? Are you catering to the lowest common denominator of attendee, or are you more interested in creating specific experiences for specific people? Vaidehi talks more about \"vibes\" in the context of retreats <a href=\"https://forum.effectivealtruism.org/posts/wCt8zxGH3MqjXRQ99/on-retreats-nail-the-vibes-and-venue#Nail_the__vibes_\">here.</a></li></ul><p>&nbsp;</p><h1><strong>Stewardship and the Attendee\u2019s Journey</strong></h1><p>Stewardship is the top of the pyramid, not because it\u2019s not important - but because it can be the most costly. Vaidehi\u2019s day job is spent trying to automate stewardship in a personalised way as much as possible, and we think there are ways to do stewardship more intentionally without losing time.</p><p>Firstly, stewardship isn\u2019t just one thing. Stewardship is about creating a cohesive user journey, and is infused throughout the event. It\u2019s related to who your attendees are, what the structure of your event is, what the goals are, etc.</p><blockquote><p>One of the main goals of EA Global is to facilitate as many meaningful connections as possible with people who are currently or seriously considering making decisions based on EA-aligned work. The idea behind connections is that you don\u2019t need to do a ton of facilitation outside of making an introduction for people who are already in very engaged. To help know who to introduce, the EAG team asks attendees to refer others who might need help expanding their networks, and then help connect them to more senior people.&nbsp;</p></blockquote><h3><strong>Lots of Onboarding</strong></h3><p>Newer attendees often lack context around norms and expectations of events. If you've attended a few EAG (x) conferences - think about all the internal knowledge it took you a conference or two to figure out. Why make others wait?&nbsp;</p><p>Onboarding is very event dependent. If it's an EAG / EAGx - it could be hosting pre-conference prep sessions, it's sharing resources around doing 1-1s, it's preparing an <a href=\"https://docs.google.com/document/d/1tclG3Rij1poyj87-B9fT6fhnemT6m_NC4dww72yzpsY/edit#\">all-inclusive attendee guide</a>, and more. If it's a retreat, it's likely clear communication, goal setting, and planning for comfort and familiarity.&nbsp;</p><p>There are also elements of social onboarding, and logistics coordination which can help attendees get prepared for the conferece. &nbsp;We think a lot of this can be outsourced to local groups, and list low-effort ways to do them in a <a href=\"https://forum.effectivealtruism.org/posts/ReWjQEJDiLdjbsKCC/how-local-groups-can-leverage-ea-conferences#Events\">follow-up post</a>.&nbsp;</p><p>You want attendees to come in with a clear set of goals &amp; intentions; and a similar baseline of knowledge and expectations so that everyone is equals and everyone is participating. &nbsp;Here are just a few suggestions that we came up with for EA conferences:</p><p><i>[edit: we updated this list by adding hours it could &nbsp;take and suggesting simpler ways of doing this, thanks to </i><a href=\"https://forum.effectivealtruism.org/posts/rMucQN9EkL8u4xnjd/improving-ea-events-start-early-and-invest-in-content-and?commentId=KEwvwJfGDktWCtmq4#comments\"><i>feedback</i></a><i>]&nbsp;</i></p><figure class=\"table\"><table><tbody><tr><td style=\"width:120px\"><strong>Idea</strong></td><td style=\"width:100px\"><strong>When</strong></td><td style=\"width:120px\"><strong>Hours</strong></td><td><strong>MVP version</strong></td></tr><tr><td>Virtual Q&amp;A</td><td>1 week before</td><td>5</td><td><ul><li>Invite 2-3 community members to to answer questions about their conference experience on GatherRound, followed by sped networking</li><li>Publicize to all attendees</li></ul></td></tr><tr><td>In-person or virtual <a href=\"https://forum.effectivealtruism.org/posts/ReWjQEJDiLdjbsKCC/how-local-groups-can-leverage-ea-conferences#EAG_prep_session___est__3_4_hours__2_hours_for_the_MVP_\">planning sessions</a></td><td>Virtual: 1 week, In-person: up to 1 day before</td><td><p>2-4 (if self-hosted)</p><p>1 (if others host)</p></td><td><ul><li>Ask local groups or EA Anywhere to organize</li><li>Publicize the event to attendees</li></ul></td></tr><tr><td>Comms with creative CTAs to encourage planning / reflection</td><td>2 weeks before</td><td>1 (to write + schedule 1 emails)</td><td><ul><li>Write 1 email (e.g. \"post your plans to the Forum for feedback\", \"submit plans and win $100 to a charity of your choice\"</li><li>More effort: \"apply to get mentorship from a leader\" - where the form has reflective / planning questions</li></ul></td></tr><tr><td>Firs-timer Onboarding drip</td><td>2 weeks before</td><td>5-10 hours (to write the first time + schedule)<br>2 hours (to reuse)&nbsp;</td><td><ul><li>Summarize advice for first-timers on making the most of conferences into a 3-4 part digestible email drip of &lt;300 words. (This could be used by other organizers in the future)&nbsp;</li></ul></td></tr></tbody></table></figure><h3><strong>Guidance and support during the event</strong></h3><p>Supporting attendees during the event will likely lead to them having a more enjoyable experience and getting more out of the event. This could look like:</p><ul><li>Social support - <i>helping attendees feel comfortable and included&nbsp;</i><ul><li>Scheduling meetups, paired 1-1s, event buddies, socials / speed friending, games, and more!</li><li>These events shouldn't take additional effort than a standard conference agenda, it's more about shifting the composition of the event agenda.&nbsp;</li></ul></li><li>&nbsp;Networking and getting advice support - <i>helping attendees get more value</i><ul><li>Having a 1-1 suggestion service, having cause area 'guides' / mentors, facilitating spontaneous 1-1s, cause area meetups</li></ul></li><li>(more time-intensive) For events in non-hubs, or with relatively few senior people, consider a program to identify ~20% of attendees who could most benefit the most from ~1-3 connections with EAs doing direct work in their cause area(s) of interest, and make connections.&nbsp;</li></ul><p>The general expectation is to schedule heavily at the beginning of the event so people feel comfortable earlier on.&nbsp;</p><h3><strong>Post Event Follow Up</strong></h3><p>We think this is super neglected right now. We haven\u2019t seen much happening for most conference and retreats. <strong>We think it's important for all events doing something to this effect.</strong></p><p>Some of the key questions on the long-term impact of events are unanswered:</p><ul><li>What is the long-term benefit of X event type? What was the impact?</li><li>What do people want after an event ends? What community supports do they need?</li><li>How do we leverage excitement after an event to get sustained involvement and action in EA?</li></ul><blockquote><p>\ud83e\uddea We're trying to answer some of these questions from EAGxBerkeley 2022. We\u2019re in the process of collecting long-term data on the benefit of the conference on attendees over time on their impact and involvement in the community.</p></blockquote><h1><strong>The Side Pyramid to the Main Pyramid You Shouldn't Ignore: Satellite Events&nbsp;</strong></h1><p><i>We cover specific examples of </i><a href=\"https://forum.effectivealtruism.org/posts/ReWjQEJDiLdjbsKCC/how-local-groups-can-leverage-ea-conferences#Pre_post_event_satellite_events___est__5_30_hours\"><i>satellite events</i></a><i> in a follow-up post.&nbsp;</i></p><p>Days / weeks before and after events (especially conferences) seems particularly high leverage. There\u2019s a fixed cost to planning in-person conferences &amp; retreats (or similar). People are already making an effort to travel or change their routine, and might be more likely to opt in to other events during this period. <i>Satellite events during EAG SF had <strong>~500 attendees</strong> over the course of the week preceding and after the conference. This was unusually highly leveraged because of early, clear planning, and open communication.</i></p><p>The value of these <a href=\"https://forum.effectivealtruism.org/s/pdPaG5eE3GZe4c4kD/p/ReWjQEJDiLdjbsKCC\">events</a> (afterparties, pre and post event dinners, pre and post event planning / next step sessions, etc) comes from a chance for people to continue to meet each other, network, and work together. Put a bunch of EAs in a room and magic will happen. You can add food, a nice space, some whiteboards \u2014 but the connection is where the value comes from. Satellite events not only extend that time frame, but they have the potential to unlock new connections - (ex. conference attendees mingling with community members who didn\u2019t attend the conference).</p><p>It\u2019s worth singling out social events. Socials are fairly easy to organise and can help accelerate someone\u2019s introduction to community. Something that hasn\u2019t been discussed in as much depth or as systematically (and plausibly shouldn\u2019t) is the value of social events. Although there are <a href=\"https://forum.effectivealtruism.org/posts/fovDwkBQgTqRMoHZM/power-dynamics-between-people-in-ea\">real, important challenges</a> with mixing personal and professional relationships (as is true in many small communities or fields), there are many benefits as well - on a personal, a community, and arguably for impact related reasons.</p><blockquote><h3><strong>Why Friendship?</strong>&nbsp;</h3><p>The EA community is a lot of things - it\u2019s a social movement, intellectual movement, work community, social community, and more. <strong>It would be a disservice to not recognise for many how feeling part of the social community is helpful or necessary to continue to engage with the work and professional field.</strong></p><p>Doing EA things can be hard and it\u2019s often easier with friends who care about you, understand the challenges, and are invested in you. For both of us (Vaidehi and Elika), individual connections and getting involved in EA as a social community is what made us invest more time in EA, get more involved, and \u2018take the leap\u2019 into working on / in EA full time. It\u2019s led to more personal growth, exploration, and confidence to do things.</p><p>Creating closer connections also means you can be more vulnerable around people. Feeling safe apart of a community means you can be wrong, share uncertainties, and be challenged / challenge others with a shared context of trust and understanding. Lastly, we all need a break! We\u2019re not robots. We want to promote healthier community norms around relaxing and enjoying life, and enjoying the people in your life.</p></blockquote><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1671642828/mirroredImages/rMucQN9EkL8u4xnjd/fhhmcxirx7ah7aebzbzj.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_1400 1400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_1600 1600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_1800 1800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b41a42107ff0ae07f7416c7a3c1b54d31f224325aecb4cbc.png/w_1934 1934w\"><figcaption><strong>&nbsp; &nbsp; &nbsp; &nbsp;Enjoy this photo of Octopus tag taken on Saturday night at EAGxBerkeley \u2764\ufe0f</strong></figcaption></figure><p><i>This post is part of an ongoing series: </i><a href=\"https://forum.effectivealtruism.org/s/pdPaG5eE3GZe4c4kD\"><i>Events in EA: Learnings and Critiques</i></a><i>.</i></p>", "user": {"username": "vaidehi_agarwalla"}}, {"_id": "7DSyxEMm9KibB22PQ", "title": "US House Vote on Support for Yemen War", "postedAt": "2022-12-12T02:13:17.858Z", "htmlBody": "<p>Sen Sanders reports to The Intercept that he thinks he has the votes to pass the Yemen War Powers Resolution which would halt America's role in one of the worst humanitarian crises in history. This would be a serious development in this conflict.</p>", "user": {"username": "Mohammad Ismam Huda"}}, {"_id": "MEEXNgCDTKccmWpmY", "title": "Please provide feedback on AI-safety grant proposal, thanks!", "postedAt": "2022-12-11T23:29:21.845Z", "htmlBody": "<p>Hello,</p><p>Disclaimer: This is my first post on the EA forum so please be gentle (but honest) with feedback :) In order to provide some accountability to myself that I actually post this, I am setting a 15 minute timer to write this post, and then a 5 minute timer to proof read &amp; post. As such, if this post is a bit rough or lacking details I apologize! At least I posted it! (if you\u2019re reading it) I\u2019m happy to elaborate further in a follow up post, comments, or edits. Thank you!</p><p>My actual post:</p><p>I recently submitted a grant proposal for a AI-safety related experiment. The grant was not accepted but I was encouraged to keep iterating on the idea so here I am! I\u2019m seeking any ideas for improvements that people can offer. Also if you know of any similar or related work being done I\u2019d love a reference to learn more about it. Thanks in advance!</p><p>Here\u2019s the grant proposal in a tweet: Code a toy reinforcement learning experiment in Python where an agent is tasked with a simple task but is incentivized to find a backdoor to that task, allowing it to achieve more \u201crewards\u201d without accomplishing the main point of the task.</p><p>Here\u2019s a more detailed breakdown:</p><p>The reinforcement agent will initially be trained to play the classic cart pole game. This is a common toy example many online ML tutorials use as an introductory task to develop a basic RL agent.</p><p>A twist will be added in a \u201cdifficulty\u201d element will introduced. A certain amount of random jitter will be added to the pole which the agent will need to learn to cope with. If the agent consistently performs well with a certain level of jitter, the jitter will increase.</p><p>The \u201cbackdoor\u201d will be that if the agent underperforms, the difficulty will actually be reduced, making the game easier. Theoretically the agent could learn to intentionally underperform in order to reduce the difficulty, allowing the game to get to a much easier state of difficulty, at which point the agent could then decide to start performing well and quickly rack up points before the difficulty is increased.</p><p>My intentions behind this experiment are mainly that I\u2019d like to put a little bit of code down to explore some of the ideas I see people discussing in the AI-safety domain. Specifically, I think the concerns that AIs could find alternative ways to optimize for a given goal, as it is in this experiment, are a fairly big topic of discussion. I thought it would be cool to pair some of that discussion with an attempt to implement it. I realize this is a total toy example that has no implications for larger scale systems or more complex scenarios.</p><p>Anyways, that\u2019s the gist of the idea and my 15 minute timer is almost up. As I said, I\u2019m happy to elaborate further!</p><p>Thanks for reading!</p><p>Alex</p>", "user": {"username": "Alex Long"}}, {"_id": "zvALRCKshYGYetsbC", "title": "Reflections on the PIBBSS Fellowship 2022", "postedAt": "2022-12-11T22:03:58.437Z", "htmlBody": "<p><i>Cross-posted from </i><a href=\"https://www.lesswrong.com/posts/gbeyjALdjdoCGayc6/reflections-on-the-pibbss-fellowship-2022\"><i>LessWrong </i></a><i>and the </i><a href=\"https://www.alignmentforum.org/posts/gbeyjALdjdoCGayc6/reflections-on-the-pibbss-fellowship-2022\"><i>Alignment Forum</i></a></p><p>Last summer, we ran the first iteration of the PIBBSS Summer Research Fellowship. In this post, we share some reflections on how the program went.</p><p>Note that this post deals mostly with high-level reflections and isn\u2019t maximally comprehensive. It primarily focusses on information we think might be relevant for other people and initiatives in this space. We also do not go into specific research outputs produced by fellows within the scope of this post. Further, there are some details that we may not cover in this post for privacy reasons.</p><p>How to navigate this post:</p><ol><li>If you know what PIBBSS is and want to directly jump to our reflections, go to sections&nbsp;\"Overview of main updates\" and \"Main successes and failures\".</li><li>If you want a bit more context first, check out&nbsp;\"About PIBBSS\" for a brief description of PIBBSS\u2019s overall mission; and&nbsp;\"Some key facts about the fellowship\", if you want a quick overview of the program design.</li><li>The appendix contains a more detailed discussion of the portfolio of research projects hosted by the fellowship program (appendix 1), and a summary of the research retreats (appendix 2).</li></ol><h1>About PIBBSS</h1><p><a href=\"https://www.pibbss.ai/\"><u>PIBBSS</u></a> (Principles of Intelligent Behavior in Biological and Social Systems) aims to facilitate research studying parallels between intelligent behavior in&nbsp;<i>natural&nbsp;</i>and&nbsp;<i>artificial&nbsp;</i>systems, and to leverage these insights towards the goal of building safe and aligned AI.</p><p>To this purpose, we organized a 3-month Summer Research Fellowship bringing together scholars with graduate-level research experience (or equivalent) from a wide range of relevant disciplines to work on research projects under the mentorship of experienced AI alignment researchers. The disciplines of interest included fields as diverse as the brain sciences; evolutionary biology, systems biology and ecology; statistical mechanics and complex systems studies; economic, legal and political theory; philosophy of science; and more.</p><p>This approach broadly -- the PIBBSS bet -- is something we think is a valuable frontier for expanding the scientific and philosophical enquiry on AI risk and the alignment problem. In particular, this aspires to bring in more empirical and conceptual grounding to thinking about advanced AI systems. It can do so by drawing on understanding that different disciplines already possess about intelligent and complex behavior, while also remaining vigilant about the disanalogies that might exist between natural systems and candidate AI designs.</p><p>Furthermore, bringing diverse epistemic competencies to bear upon the problem also puts us in a better position to identify neglected challenges and opportunities in alignment research. While we certainly recognize that familiarity with ML research is an important part of being able to make significant progress in the field, we also think that familiarity with a large variety of intelligent systems and models of intelligent behavior constitutes an underserved epistemic resource. It can provide novel research surface area, help assess current research frontiers, de- (and re-)construct the AI risk problem, help conceive of novel alternatives in the design space, etc.&nbsp;</p><p>This makes interdisciplinary and transdisciplinary research endeavors valuable, especially given how they otherwise are likely to be neglected due to inferential and disciplinary distances. That said, we are skeptical of&nbsp;<i>\u201cinterdisciplinary for the sake of it\u201d</i>, but consider it exciting insofar it explores specific research bets or has specific generative motivations for why X is interesting.</p><p>For more information on PIBBSS, see&nbsp;<a href=\"https://www.lesswrong.com/posts/4Tjz4EJ8DozE9z5nQ/introducing-the-principles-of-intelligent-behaviour-in\"><u>this introduction post</u></a>, this&nbsp;<a href=\"https://www.lesswrong.com/posts/FuToH2KHxKmJLGk2B/ai-alignment-as-navigating-the-space-of-intelligent\"><u>discussion of the epistemic bet</u></a>, our research map (currently undergoing a significant update, to be released soon), and these&nbsp;<a href=\"https://docs.google.com/document/d/1iOQ13na21jEjk35PGVYF-t7MttDDaxd_WGFwL1Q4hM4/edit?usp=sharing\"><u>notes on our motivations and scope</u></a>.</p><h1>Some key facts about the fellowship program</h1><ul><li>12-week fellowship period (~mid-June to early September)</li><li>20 fellows, and 14 mentors<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3ff72sklc07\"><sup><a href=\"#fn3ff72sklc07\">[1]</a></sup></span>&nbsp;(for a full list, see&nbsp;<a href=\"http://pibbss.ai/our-people\"><u>our website</u></a>)&nbsp;</li><li>Fellows had relevant backgrounds in the following fields:&nbsp;<ul><li>Complex systems studies, network theory, physics, biophysics (~4)</li><li>Neuroscience, computational cognitive science (~4)</li><li>Formal Philosophy, Philosophy of Science (~5)</li><li>Evolutionary Biology, Genomics, Biology (~3)</li><li>Chemistry (~1)</li><li>Computational social science (~1)</li><li>Economics (~2)</li></ul></li><li>6-week reading group (\u201cDeep Read\u201d format<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5dmzt1u3au\"><sup><a href=\"#fn5dmzt1u3au\">[2]</a></sup></span>) serving as an introduction to AI risk (prior to the start of the fellowship period) (NB we are planning to release an updated and extended version of the same reading programme)</li><li>2 research retreats (inviting fellows, mentors and a couple of external guests; taking place at a venue outside Oxford, and nearby Prague respectively)</li><li>Individual research support/facilitation (to different degrees for different fellows)</li><li>~Bi-weekly speaker series throughout the fellowship period</li><li>Further scaffolding in the form of weekly check-ins, optional co-working sessions and occasionally socials,</li><li>Optional stay in Prague to work from a shared office space</li><li>Stipend (3,000 USD/month)</li><li>Financial support for travel and accommodation (e.g. to visit their mentor or the residency)</li></ul><h2>The original value proposition of the fellowship</h2><p>This is how we thought of the fellowships\u2019 value propositions prior to running it:&nbsp;</p><ul><li><strong>Generating a flow of insights</strong> between the fellow/their domain of expertise towards the AI alignment mentor, and the AI alignment community at large; via the research output.</li><li><strong>Attracting and actuating talent</strong> with the competencies and motivation to conduct valuable AI alignment or AI governance research (hits-based).</li><li><strong>Building bridges&nbsp;</strong>between AI alignment and other fields with relevant knowledge and talent, thereby lowering current barriers to fruitful exchanges.</li><li><strong>Gaining value of information&nbsp;</strong>about the value/tractability/etc. of (facilitating) PIBBSS-style research, i.e. careful, value-sensitive, and epistemically diverse approaches toward the design and implementation of aligned and beneficial artificial intelligent systems drawing on the study of currently existing systems implementing complex or intelligent behavior.</li></ul><h1>Overview of main updates</h1><ul><li>We continue to be excited and have become more confident in the&nbsp;<strong>value and tractability of PIBBSS-style research for alignment.</strong>&nbsp;</li><li>We continue to be excited about the value of<strong> building bridges&nbsp;</strong>between the alignment community and other relevant epistemic communities which offer a so-far untouched pool of insights and talent.&nbsp;</li><li>We made positive updates about the&nbsp;<strong>tractability of attracting strong candidates of PIBBSS-relevant domains</strong>,&nbsp;<i>with the caveat&nbsp;</i>of finding important room for improvement for outreach to conceptual social sciences</li><li>We made positive updates about our&nbsp;<strong>ability to adequately introduce them to AI risk and pedagogy</strong>,&nbsp;<i>with the caveat&nbsp;</i>of finding important room for improvement concerning technical ML-safety pedagogy</li><li>We gathered preliminary positive evidence about our&nbsp;<strong>ability to positively affect trajectory changes.&nbsp;</strong>Prior to running the fellowship, a relevant source of skepticism was that people from fields as diverse as the PIBBSS portfolio might be significantly less likely to commit to working on AI alignment in the long term. We haven\u2019t seen this skepticism confirmed, but acknowledge it is largely too early to tell.&nbsp;<ul><li>Furthermore, we have been reinforced in our belief that, even if trajectory changes are considered a significant source of value, focusing on research output is a healthy way of causing trajectory change as a side-effect.</li></ul></li><li>We gained better models about the ways&nbsp;<strong>conflicting incentive landscapes&nbsp;</strong>might curtail value. (For more detail, see discussion under failures and/or shortcomings.) In particular, we will pay more attention to this consideration in the application process, and will provide more structural support encouraging fellows to produce more and more frequent communicable output to capture the research/epistemic progress that has been generated.&nbsp;</li><li>We suspect a bunch of value might come through&nbsp;<strong>hit-based outcomes.&nbsp;</strong>We gathered light evidence that there might be significant expected value coming from more \u201chits-based\u201d avenues, e.g. attracting senior or highly promising scholars and counterfactually contributing to them working on AI alignment and related topics. We think there is value in finding ways to find such cases more systematically, and are exploring ways to do so.&nbsp;</li><li>Fellows with little&nbsp;<strong>research experience&nbsp;</strong>tended to have a harder time during the fellowship and would have benefited from more structure. We didn\u2019t make a large update here, and still think it is sometimes worth accepting even nominally junior individuals to the fellowship. That said, going forward, we are less likely to accept applicants with less than graduate-level (or equivalent) research experience. We have started thinking about and are interested in exploring alternative ways of onboarding promising people with less research experience.&nbsp;</li></ul><h1>Main successes and failures</h1><h2>Successes</h2><p>Overall, we believe that most of the value of the program came from researchers gaining a better understanding of research directions and the AI risk problems. Some of this value manifests as concrete research outputs, some of it as effects on fellows\u2019 future trajectories.&nbsp;&nbsp;</p><ul><li><strong>Research progress:&nbsp;</strong>Out of 20 fellows, we find that at least 6\u201310 made interesting progress on promising research programs.<ul><li>A non-comprehensive sample of research progress we were particularly excited about includes work on intrinsic reward-shaping in brains, a dynamical systems perspective on goal-oriented behavior and relativistic agency, or an investigation into how robust humans are to being corrupted or mind-hacked by future AI systems.&nbsp;</li></ul></li><li><strong>Trajectory changes:&nbsp;</strong>Out of 20 fellows, we believe that at least 11\u201315 gained significant exposure to AI risk and alignment, and we expect that some of them will continue to engage with the field and make meaningful contributions in the future.&nbsp;<ul><li>We believe the fellowship had a significant&nbsp;<i>counterfactual impact</i> on 12 fellows, due to many of them having limited or non-existent exposure to AI risk/alignment prior to the fellowship. In some cases, fellows have changed their research directions/careers, some have taken up jobs and some have become active in AI alignment fluid-building-related activities.</li></ul></li><li><strong>Mentors&nbsp;</strong>report finding the fellowship to be a good counterfactual use of their time. Of 10 mentors who filled in the survey, 8 reported it was an equally good or better use of their time, out of which 6 reported it was a strictly better use of their time.&nbsp;<ul><li>The main sources of value reported by mentors were: (i) Developing their thinking on a given research direction in AI alignment, benefiting from new perspectives or concrete insights from the fellow; (ii) Being part of a research network relevant to their own research interests/directions; finding actual/potential future collaborators; (iii) Concrete research progress/output produced during the program together with a fellow.</li><li>Mentors, as well as a handful of \u201cexternal\u201d people (i.e. people who were neither mentors, fellows, nor organizers; who interfaced with different parts of the program, e.g. retreat, research facilitation), reported positive updates about the value and tractability of the research bet (i.e. \u201cPIBBSS-style research\u201d).&nbsp;</li></ul></li><li><strong>Secondary sources of value;</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd82uezixxz\"><sup><a href=\"#fnd82uezixxz\">[3]</a></sup></span><strong>&nbsp;</strong>we believe the program generated secondary sources of value in the following domains:<strong>&nbsp;</strong><ul><li><strong>Progress towards developing a research network&nbsp;</strong><ul><li>Running the PIBBSS fellowship has provided us with information about the value of, and improved our ability to act as a sort of Schelling point for PIBBSS-style research. Research retreats in particular, and speaker events and the slack space to a lesser extent, have served to bring together scholars (beyond fellows) interested in AI alignment research from PIBBSS-style angles. We continue to be excited about the potential of supporting and developing this research network and believe we are more likely to notice valuable opportunities.</li></ul></li><li><strong>Building bridges between AI alignment and other relevant epistemic communities</strong><ul><li>We believe that epistemic communities outside of the AI safety/alignment community are working on interesting questions with direct relevance to progress in AI alignment. We have started exploring ways to build \u201cbridges\u201d between these communities, with the goal of engaging both insights and talent.&nbsp;</li><li>For example, we are trialing a speaker series as a way of testing the possibility of broader epistemic and social bridges with specific relevant research directions. The series invites researchers from inside and outside of AI alignment who we believe produce interesting work related to PIBBSS priorities. (If this goes well, we might make the speaker series openly accessible to the AI alignment and other adjacent communities.) Further, we have built bridges in the form of personal networks and fellows themselves sometimes represent such interdisciplinary bridges. Some of our fellows have taken up projects aiming to do such work, e.g. by organizing research workshops or writing introductory materials.&nbsp;</li></ul></li><li><strong>Developing know-how about Program Design</strong><ul><li><strong>Introducing people to AI risk and Alignment Research.</strong><ul><li>Running the fellowship program allowed us to make concrete progress in developing pedagogical-content knowledge for AI risk and alignment. Concretely, we ran a 6-week (pre-fellowship) reading group with the deep read format dedicated to an introduction to&nbsp;<i>key phenomena in AI risk</i>, two research retreats (more details below in \"Appendix 2\") and provided research facilitation throughout the program.</li><li>We did better than expected at introducing people to AI alignment who have no or limited prior familiarity with the field<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2tedsigkyw\"><sup><a href=\"#fn2tedsigkyw\">[4]</a></sup></span>&nbsp;and who came in with a decent amount of inferential distance from AI risk (e.g. due to being part of&nbsp; previously underrepresented epistemic demographics, e.g. the natural or social sciences). This leads us to make slight positive updates about the accessibility of AI risk to researchers from such domains and makes us think that there is potential for value in improving \u201cAI-risk pedagogy\u201d (for future PIBBSS activities and for the AI-alignment community in general).&nbsp;<ul><li>However, we also encountered challenges when it comes to facilitating knowledge transfer towards prosaic alignment approaches (see&nbsp;\"Failures and/or shortcomings\" for more detail).</li></ul></li></ul></li><li><strong>Epistemology, Philosophy of Science and Meta-theory of Alignment</strong><ul><li>We found that running a diverse research programme is highly synergistic with also making progress on philosophy of science in AI, epistemology of Alignment research, and to some extent AI Strategy. (Note, we believe this to be similar to some parts of&nbsp;<a href=\"https://www.lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind#The_Long_View__Refine_and_Conjecture\"><u>Refine\u2019s model</u></a>, even though the programs were structurally different).</li><li>Examples of frontiers of intellectual progress include: (i)&nbsp;<strong>Understanding of AI risk in terms of intelligent behavior and complex phenomena&nbsp;</strong>(e.g. shining light on emerging parallels in agent-foundations research and complex systems theory) ; (ii)&nbsp;<strong>Philosophical reflections on AI risk and alignment</strong>, e.g., non-anthropomorphic articulations of AI risk, problematizing and clarifying the concepts of consequentialism, and goal-orientedness in AI risk stories; (iii)&nbsp;<strong>Epistemology and philosophy of science perspectives on AI risk and alignment</strong>, e.g., epistemological vigilance with respect to interdisciplinary perspectives on AI risk and alignment, mapping disagreements on risk models and with respect to analogies and disanalogies relevant to AI, improving AI-risk pedagogy, and cross-cohort conversations during the fellowship; (iv)&nbsp;<strong>Translation between technical ML safety and other fields</strong>, e.g., evolutionary biology and selection theorems.&nbsp;</li></ul></li></ul></li><li><strong>Learning about outreach and selection of talent from natural and social sciences&nbsp;&nbsp;</strong><ul><li>Prior to running the fellowship, one key uncertainty concerned our ability to attract strong candidates from natural and social science backgrounds.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3l8fqnczhyw\"><sup><a href=\"#fn3l8fqnczhyw\">[5]</a></sup></span>&nbsp;Overall, we made positive updates about our ability to do so and think this increases the expected value of future initiatives targeted at people from a similar target audience.</li><li>In terms of disciplines, judging from the applicant pool, we were particularly good at attracting people from philosophy, physics/complex systems, and brain sciences; we did okay but not great in terms of attracting people from biology and computational social sciences; and we underperformed at attracting people from the theoretical social sciences (e.g. legal and political theory, institutional economics and political economy, analytical sociology)</li></ul></li><li><strong>Miscellaneous other notable outcomes:</strong><ul><li>The fellowship produced 3-5 long-term collaborations among fellows and mentors.&nbsp;</li><li>One fellow is organizing an AI-risk workshop at the ALife 2023 conference. We are excited about the potential knowledge and talent overlap between the ALife and the AI alignment community.&nbsp;</li><li>Three fellows are writing an introduction to AI risk targeted at natural systems researchers, in particular, physics and biology researchers.</li><li>Some concrete artifacts created in the context of the fellowship (this does&nbsp;<i>NOT&nbsp;</i>include outputs by fellows):<ul><li>A&nbsp;<a href=\"https://m.youtube.com/channel/UCMo5Ei9xLHbk9sqNMSH3mdQ\"><u>Speaker Series</u></a> inviting people from the alignment community (e.g., Alex Turner on Shard Theory) as well as outside (e.g., Simon DeDeo on the theory of explanations, Michael Levin on hierarchical agency in biological systems), which we think was a useful exercise in testing the possibility of broader epistemic and social bridges with specific relevant research directions.</li><li>A PIBBSS Research map (currently undergoing a significant update)&nbsp; providing an overview of six clusters of research directions within the PIBBSS research bet, distilled from conversations TJ and Nora had with mentors regarding their key interdisciplinary interests:<ul><li>Understanding and Interpreting Minds and Cognition</li><li>Safe Interfaces to Human Preferences</li><li>Processes of Knowledge Production and Reflection</li><li>Social and Economic Coordination</li><li>Evolutionary Selection for X</li><li>Information and Goals in Autonomous Behavior</li></ul></li><li>Two talks&nbsp;<strong>&nbsp;</strong>on the landscape of epistemic strategies in AI alignment, and specifically the epistemic assumptions underlying the \u201cPIBBSS\u201d epistemic bet (once at EAGx Prague, once at&nbsp;<a href=\"https://www.youtube.com/watch?v=h86oHBbQwI8&amp;ab_channel=PIBBSSFellowship\"><u>HAAISS</u></a>)</li><li>Some initial posts in a&nbsp;<a href=\"https://www.lesswrong.com/s/4WiyAJ2Y7Fuyz8RtM\"><u>series on the philosophy of science of AI alignment</u></a> (those ideas are ~directly downstream from working on PIBBSS)</li><li>A (WIP) reading list providing an introduction to the broad contours of the AI-risk problem suitable for an interdisciplinary cohort (a side-product of running the reading group).</li><li>A curated&nbsp;<a href=\"https://spurious-wallflower-098.notion.site/PIBBSS-Library-880d40e68a3e47968e4eb375df1a28af\"><u>list of PIBBSS-style books</u></a></li></ul></li></ul></li></ul></li></ul><h2>Failures and/or shortcomings</h2><ul><li><strong>Concrete output:&nbsp;</strong>While we are fairly happy with the research progress, we think this insufficiently translated into communicable research output within the timeframe of the fellowship. Accordingly, we believe that a, if not \u201cthe\u201d, main dimension of improvement for the fellowship lies in providing more structural support encouraging more and faster communicable output to capture the research/epistemic progress that has been generated.&nbsp;</li><li><strong>Limitations of our ability to facilitate knowledge transfer towards prosaic alignment approach:&nbsp;</strong>According to our judgment, transfer towards prosaic alignment approaches (e.g., drawing on insights from evolutionary biology towards&nbsp;<a href=\"https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine\"><u>training stories</u></a>) most requires developing novel conceptual vocabulary, and would have benefited from fellows having more familiarity with concepts in technical ML, which we were insufficiently able to provide through our pedagogical efforts.<ul><li>We still believe, also thanks to preliminary-yet-promising evidence of research progress as judged by us and mentors, that this type of epistemic progress, if challenging, is still valuable to pursue. Going forward, we will try to tackle these challenges by a) improving the prosaic-relevant pedagogy leading up to and during the fellowship, b) improving our research facilitation capacity, in particular, for prosaic projects, and c) putting more weight on fellows working on prosaic projects to have prior exposure to ML/prosaic AI alignment.</li></ul></li><li><strong>Conflicting incentive landscapes can curtail value: </strong>fellows being embedded in conflicting incentive landscapes (e.g. academic and or professional) has the potential to limit or curtail value. This can manifest as a reduction in fellows\u2019 time commitments to the program/their project, or (implicitly or overtly) influence research-related choices, e.g. the project topic, the scoping or the type of output that is aimed at. Among others, we suspect that academic incentives contributed to a tendency across the cohort to sometimes aim for paper-like outputs over, e.g., forum posts, which in turn leads to a reduced volume of communicable research output within the period of the fellowship (see the point about concrete output above). This can cause delays in ideas reaching and being discussed/evaluated/criticized/improved upon by the larger epistemic community. Going forward, we will be more cognizant of the relevant incentive landscapes, and, in particular, add structures to the program design encouraging fellows to release (even if intermediary) outputs directly as well.</li><li><strong>Outreach to theoretical social sciences</strong>: while we were happy about outreach to natural sciences in general, we underperformed at attracting people from the conceptual social sciences (e.g. legal and political theory, institutional and political economy, analytical sociology). We think this represents a missed opportunity.&nbsp;</li></ul><h1>A brief note on future plans</h1><p>Overall, we believe the PIBBSS summer research fellowship (or a close variation of it) is worth running again. We applied for funding to do so.&nbsp;</p><p>The key dimensions of improvement we are envisioning for the 2023 fellowship are:&nbsp;</p><ul><li>More structural support, encouraging more and faster communicable output capturing the research/epistemic progress that has been generated,&nbsp;&nbsp;</li><li>Improvement of pedagogy (and facilitation) relevant to prosaic projects,</li><li>Being more careful about accepting fellows who are embedded in conflicting incentive landscapes;</li></ul><p>More tentatively, we might explore ways of running (part of the program) in a (more) mentor-less fashion. While we think this is hard to do well, we also think this is attractive for several reasons, mainly because mentorship is scarce in the field. Some potential avenues of exploration include:&nbsp;</p><ul><li>Reducing mentor bandwidth demand towards more efficient feedback mechanisms and substituting with:<ul><li>Research facilitation,</li><li>Peer-interaction,</li><li>Pairing with junior researchers in AI.</li></ul></li><li>Exploring alternative programs aimed at audiences of different seniorities, aided by better-scoped projects/research program</li></ul><p><br>Beyond the format of the summer research fellowship, we tentatively think the following (rough) formats are worth further thought. Note that we are not saying these programs are, all things considered, worthwhile, but that, given our experience, these are three directions that may be worth exploring further.&nbsp;</p><ol><li>A&nbsp;<strong>reading group/introduction course to AI risk/alignment&nbsp;</strong>suitable for natural and social science demographics. We are considering further developing the pre-fellowship reading group, and experimenting with whether it might be worth running it (also) outside of the fellowship context.&nbsp;</li><li>An&nbsp;<strong>~affiliate program,</strong> targeted at people who are already able to pursue (PIBBSS-style) research independently. Such a program would likely be longer (e.g. 6 months or more) and focus on providing more tailored support towards affiliates developing their own (novel) PIBBSS-style research directions.</li><li>A range of&nbsp;<strong>research workshops/retreats/conferences</strong> aimed at specific domains or domain interfaces (within the scope of PIBBSS research interests), aiming to e.g. test or develop specific research bets (e.g., complex systems in interpretability, ALife in agent foundations, predictive processing) and/or create Schelling points for specific demographics (e.g., brain sciences in AI alignment).&nbsp;</li></ol><p>PIBBSS is interested in exploring these, or other, avenues further.&nbsp;<strong>If you have feedback or ideas, or are interested in collaborating, feel encouraged to reach out to us (contact@pibbss.ai).</strong></p><h1>We want to thank\u2026&nbsp;</h1><p>For invaluable help in making the program a success, we want to thank our fellow organizing team members&nbsp;<i>Anna Gadjdova&nbsp;</i>and&nbsp;<i>Cara Selvarajah;&nbsp;</i>and several other people who contributed to the different parts of this endeavor, including<i> Amrit Sidhu-Brar, Gavin Leech, Adam Shimi, Sahil Kulshrestha, Nandi Schoots, Tan Zhi Xuan, Tom\u00e1\u0161 Gaven\u010diak, Jan Kulveit, Mihaly Barasz, Max Daniel, Owen Cotton-Barrat, Patrick Butlin, John Wentworth, Andrew Critch, Vojta Kovarik, Lewis Hamilton, Rose Hadshar, Steve Byrnes, Damon Sasi, Raymond Douglas, Radim Lacina, Jan Pieter Snoeji, Cullen O\u2019Keefe, Guillaume Corlouer, Elizabeth Garrett, Kristie Barnett, Franti\u0161ek Drahota, Anton\u00edn Kan\u00e1t, Karin Neumannova, Jiri Nadvornik</i>, and anyone else we might have forgotten to mention here - our sincere apologies!). Of course, we are also most grateful to all our mentors and fellows.</p><p>&nbsp;</p><h1>Appendix 1: Reflections on Portfolio of Research Bets</h1><p>In this section, we will discuss our reflections on the portfolio of research bets that fellows worked on, which are distributed across a range of \u201ctarget domains\u201d, in particular:&nbsp;</p><ol><li>Agent Foundations,</li><li>Alignment of Complex Systems,</li><li>Digital Minds and Brain-inspired Alignment,&nbsp;</li><li>Prosaic Alignment (Foundations),</li><li>Socio-technical ML Ethics,</li><li>Experimental and Applied Prosaic Alignment.</li></ol><p>We will focus the discussion on aspects like the theory of impact for different target domains and the tractability of insight transfer. The discussion will aim to abstract away from fellow- or project-specific factors. Note, that we shall skip the discussion of specific projects or other details here in the public post.</p><p><strong>TL;DR:</strong> At a high level, projects aimed towards i) Agent Foundations, ii) Alignment of Complex Systems, and iii) Digital Minds and Brain-inspired Alignment most consistently made valuable progress. Projects aimed at iv) Prosaic Alignment faced the largest challenges. Specifically, they seem to require building new vocabulary and frameworks to assist in the epistemic transfer and would have benefited from fellows having more familiarity with concepts in technical ML, which we were insufficiently able to provide through our pedagogical efforts. We believe this constitutes an important dimension of improvement.</p><p><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1670794442/mirroredImages/gbeyjALdjdoCGayc6/btdlm7xtsvmmln8r1ksg.png\"></p><p><strong>1. Agent Foundations&nbsp;</strong>(25\u201330%)&nbsp;<i><strong>[AF]</strong></i><strong>&nbsp;</strong></p><ul><li>PIBBSS had ~5 fellows (and 4 projects) working on Agent Foundations,&nbsp; i.e., interdisciplinary frontiers for clarifying concepts in the theory of agency, optimization and cognition. These problems aid to contribute towards understanding and characterizing forms of agency to steer towards (e.g., corrigibility) or away from (e.g., power seeking).</li></ul><p><br><strong>2. Alignment of Complex Systems&nbsp;</strong>(20\u201325%)&nbsp;<i><strong>[CS]</strong></i></p><ul><li>PIBBSS had ~4 fellows (and 3 resulting projects) working on Alignment of Complex Systems, aimed at clarifying and/or formalizing relevant phenomena found within complex adaptive systems of various types and scales. The importance of these topics comes from a better theoretical understanding of learned adaptive behaviors in AI systems, target behaviors of aligned systems, and AI governance/macrostrategy.</li></ul><p><br><strong>3. Digital Minds&nbsp;</strong>(and Brain-inspired alignment research; 5\u201310%)<strong>&nbsp;</strong><i><strong>[DM]</strong></i><strong>&nbsp;</strong></p><ul><li>We had ~2 fellows (and 2 projects) working at the intersection of neuroscience (and/or cognitive science) and technical questions on the nature of minds and cognition, especially as they pertain to digital minds or brain-inspired AI systems. These topics seek to understand phenomena relevant to the functioning of sophisticated forms of cognition or to questions concerning the moral status of digital minds, as well as to understand the structure and behavior of mind-like algorithmic systems.&nbsp;</li></ul><p><i><strong>Discussion of AF, CS and DM:&nbsp;</strong></i></p><p>The above three target domains (i.e. Agent Foundations, Alignment of Complex Systems, and Digital Minds) are all, in some sense, similar insofar as they are all basically pursuing conceptual foundations of intelligent systems, even if the three approach the problem from slightly different starting positions, and with different methodologies. The three bundles together accounted for about 50-55% of projects, and roughly 50% of them were successful in terms of generating research momentum. This makes it meaningful to pay attention to two other similarities between them: a) the overlapping vocabularies and interests with respective neighboring disciplines, and b) the degree of separation (or indirectness) in their theory of impact.</p><p>The object-level interests in AF, CS, or DM mostly have the same type signature as questions that motivate researchers in the respective scientific and philosophical disciplines (such as decision theory, information theory, complex systems, cognitive science, etc.). This also means that interdisciplinary dialogue can be conducted relatively more smoothly, due to shared conceptual vocabulary and ontologies. Consequently, we can interpret the motivational nudges provided by PIBBSS here as being some gentle selection pressure towards alignment-relevance of which specific questions get investigated.</p><p>At the same time, the (alignment-relevant) impact from research progress here is mostly indirect, coming from better foresight of AI behavior and as an input to future specification and/or interpretability research (see discussion in Rice and Manheim 2022<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg1nkv2y7mwr\"><sup><a href=\"#fng1nkv2y7mwr\">[6]</a></sup></span>). This provides an important high-level constraint on value derived here.<br>&nbsp;</p><p><strong>4. Prosaic Alignment Foundations</strong> (25\u201335%)&nbsp;<i><strong>[PF]</strong></i>&nbsp;</p><ul><li>We had ~5 fellows (and 5 projects) working on foundational topics related to work on prosaic alignment (or alignment of advanced ML systems), including work related to value learning, interpretability, ML training, human assistance, scaling laws and capabilities gains, and machine deception. As alignment research on advanced ML systems is most directly proximate to developing practical alignment strategies/proposals (as a key epistemic artifact<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftxw6kblvscj\"><sup><a href=\"#fntxw6kblvscj\">[7]</a></sup></span>), these topics constitute what is very likely a critically important area. Projects in the fellowship aimed at bringing in insights from cognitive science, evolutionary biology, statistical mechanics, etc. towards these problems.</li></ul><p><i><strong>Discussion of PF:&nbsp;</strong></i></p><p>While some PF projects did succeed in finding promising research momentum, there was higher variance in the tractability of progress. This bundle also had a meaningfully lower ex-post excitement by mentors (compared to the rest of the portfolio), and caused us significant updates about the epistemic hardness of transferring insights from other disciplines.</p><p>Unlike AF+CS+DM discussed above, the interdisciplinary transfer of insights towards prosaic alignment seems to involve building entirely new vocabularies and ontologies to a significantly higher degree. For example, transferring insights from evolutionary theory towards understanding any particular phenomenon of relevance in deep learning, seems to be bottlenecked by the absence of a much richer understanding of isomorphism than what already exists. In fact, the projects in this bundle that did succeed in finding good research momentum were strongly correlated with prior ML familiarity of the fellows.</p><p>However, given the potential of high-value insights coming from bets like this, we think further exploring the ways of building relevant ML familiarity for the fellows, such that they can more efficiently and constructively contribute, seems worth investigating further. At the very least, we intend to add pedagogical elements for familiarizing fellows with Machine Learning Algorithms and with ML Interpretability, in addition to improving the pedagogical elements on Information Theory and RL Theory from the previous iteration.</p><p>&nbsp;</p><p><strong>5. Socio-technical ML Ethics</strong> (10%)&nbsp;<i><strong>[ME]</strong></i>&nbsp;</p><ul><li>We had 2 fellows (and 2 projects) working on topics on contemporary ethical concerns with ML systems related to topics on epistemic and socio-technical effects of AI. Projects looked at topics such as social epistemology and epistemic risks, normative concepts and trade-offs in ML research, relationship of epistemic content of ML models vis-a-vis scientific discoveries.&nbsp;</li><li>While topics in ML ethics can generally be seen as non-neglected within the larger ML community, we think that facilitating philosophically well-grounded projects on these topics can aid in raising the profile of understudied normative considerations about current and future AI systems. We believe facilitating such work contributes to fostering a strong epistemic environment (from a diversity of opinions on AI risk, especially insofar as they come from well-grounded positions, including disagreements), as well as opening up surface area for positive-sum synergies across a diverse study of normative demands on AI systems and the steering of AI systems towards good-to-have properties.</li></ul><p><strong>6. Experimental and Applied Prosaic Alignment</strong>&nbsp; (5\u201310%)&nbsp;<i><strong>[EP]</strong></i>&nbsp;</p><ul><li>We hosted 2 fellows (and 2 projects) working on experimental and applied projects with relevance to topics in prosaic alignment research. These projects involved some ML research and experimentation/engineering that benefited from an interdisciplinary perspective and flow of insights.</li><li>This kind of research is valuable in similar ways to conceptual progress on prosaic alignment (ie. by building and testing new frameworks) and helps in connecting the practical understanding of ML research with conceptual work more directly. We also recognize that such work is non-neglected in the larger community and can sometimes be susceptible towards safety-washing of capabilities-advancing research, however in our judgment the fellows were adequately cognizant of this.&nbsp;</li></ul><h1>Appendix 2: Retreats Summary</h1><p>We organized two retreats during the fellowship program for familiarization to AI risk and the alignment problem, facilitation of cross-cohort dialogue, and other benefits of in-person research gatherings. Both the retreats had a mix of structured and unstructured parts, where the structured parts included talks, invited speakers, etc., as well as sessions directed at research planning and orientation, while the unstructured parts included discussions and breakout sessions. A small sample of recurring themes in the unstructured parts included deconfusing and conceptualizing consequentialist cognition, mechanizing goal-orientedness, role of representations in cognition, distinguishing assistive behavior from manipulative behavior, etc.</p><p>The first retreat was organized at a venue outside Oxford, at the beginning of the summer, and included sessions on different topics in:</p><ol><li>Introduction to AI Risk (eg. intro to instrumental convergence, systems theoretic introduction to misalignment problems, security mindset, pluralism in risk models, talk on&nbsp;<a href=\"https://www.lesswrong.com/posts/ezGYBHTxiRgmMRpWK/tech-company-singularities-and-steering-them-to-reduce-x\"><u>tech company singularities</u></a> etc.)</li><li>Epistemology of Alignment Research (eg. proxy awareness in reasoning about future systems, epistemic strategies in alignment,&nbsp;<a href=\"https://www.lesswrong.com/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment\"><u>epistemological vigilance</u></a>, recognizing analogies vs disanalogies, etc.)</li><li>Mathematical Methods in Machine Intelligence (eg. intro lecture on information theory, intro lecture on RL theory, talk on&nbsp;<a href=\"https://www.lesswrong.com/posts/jJf4FrfiQdDGg7uco/the-telephone-theorem-information-at-a-distance-is-mediated\"><u>telephone theorem</u></a> and natural abstractions hypothesis, etc.)</li><li>Overview of Research Frontiers and AI Design Space (eg. session on reasoning across varying levels of goal-orientedness, capabilities and alignment in different AI systems, and PIBBSS interdisciplinary research map.)</li></ol><p>The second retreat was organized near Prague a few weeks before the formal end of the fellowship, and was scheduled adjacent to the&nbsp;<a href=\"https://humanaligned.ai/index-2022.html\"><u>Human-Aligned AI Summer School (HAAISS) 2022</u></a>. It included fellows presenting research updates and seeking feedback, some talks continuing the themes from the previous retreat (eg. why alignment problems contain some hard parts, problematizing consequentialist cognition and second-person ethics, etc), and practising&nbsp;<a href=\"https://www.lesswrong.com/tag/double-crux\"><u>double crux</u></a> on scientific disagreements (such as whether there are qualitative differences in the role of representations in human and cellular cognition).</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3ff72sklc07\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3ff72sklc07\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Close to the end of the fellowship program, Peter Eckerlsey, one of our mentors - as well as a mentor and admired friend of people involved in PIBBSS -&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ivep4R7LoSLhWwHGX/peter-eckersley-1979-2022\"><u>passed away</u></a>. We mourn this loss and are grateful for his participation in our program.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5dmzt1u3au\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5dmzt1u3au\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here is&nbsp;<a href=\"https://threadreaderapp.com/thread/1285415881120481280.html\"><u>an explanation of how deep reading groups work</u></a>. We were very happy with how the format suited our purposes. Kudos to Sahil Kulshrestha for suggesting and facilitating the format!</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd82uezixxz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd82uezixxz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By \u201cprimary\u201d sources of value, we mean those values that ~directly manifest in the world. By \u201csecondary\u201d values we mean things that are valuable in that they aid in generating (more) primary value (in the future). We can also think of secondary values as \u201ccommons\u201d produced by the program.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2tedsigkyw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2tedsigkyw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>6 out of 20 fellows had negligible prior exposure to AI risk and alignment; 10 out of 20 had prior awareness but lack of exposure to AI-risk technical discussions; 4 out of 20 had prior technical exposure to AI risk.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3l8fqnczhyw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3l8fqnczhyw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some numbers about our application process:</p><p>- Stage 1: &nbsp; &nbsp;121 applied,<br>- Stage 2: &nbsp; &nbsp;~60 were invited for work tasks,<br>- Stage 3: &nbsp; &nbsp;~40 were invited for interviews,<br>- Final number of offers accepted: &nbsp; &nbsp;20&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng1nkv2y7mwr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg1nkv2y7mwr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Issa Rice and David Manheim (2022),&nbsp;<i>Arguments about Highly Reliable Agent Designs as a Useful Path to Artificial Intelligence Safety</i>,&nbsp;<a href=\"https://arxiv.org/abs/2201.02950\"><u>https://arxiv.org/abs/2201.02950</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntxw6kblvscj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftxw6kblvscj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Nora Ammann (2022),&nbsp;<i>Epistemic Artifacts of (conceptual) AI alignment research</i>,&nbsp;<a href=\"https://www.lesswrong.com/s/4WiyAJ2Y7Fuyz8RtM/p/CewHdaAjEvG3bpc6C\"><u>https://www.alignmentforum.org/s/4WiyAJ2Y7Fuyz8RtM/p/CewHdaAjEvG3bpc6C</u></a></p><p>The write up proposes an identification of \u201cfour categories of epistemic artifacts we may hope to retrieve from conceptual AI alignment research: a) conceptual de-confusion, b) identifying and specifying risk scenarios, c) characterizing target behavior, and d) formalizing alignment strategies/proposals.\u201d</p></div></li></ol>", "user": {"username": "nora"}}, {"_id": "7DCuhq23zHxAmPqvJ", "title": "Kurzgesagt's most recent video promoting the introducing of wild life to other planets is unethical and irresponsible", "postedAt": "2022-12-11T20:43:06.137Z", "htmlBody": "<p>Kurzgesagt, in their recent video \"<a href=\"https://www.youtube.com/watch?v=HpcTJW4ur54\">How To Terraform Mars - WITH LASERS</a>\" (just came out a few hours ago as of writing this post), promotes the idea of seeding wildlife on other planets without considering the immense suffering it would cause for the animals on it. Instead of putting thought into the ethical implications of these actions, the video (as par for the course) focuses solely on the potential benefits for humans.</p><p>Sadly this problem isn't an isolated incident either, the pattern of ignoring the real risk for immense wild animal suffering is common in almost all major plans and discussions involving the terraforming of planets or space colonisation.&nbsp;</p><p>Sure, a new green planet with lots of nature sounds cool in theory, but it would very likely mean subjecting countless animals to a lifetime of suffering. These animals would be forced to adapt to potentially hostile and unfamiliar environments, and face countless challenges without any choice in the matter. There's no way around it that I can see.</p><p>You might argue that in these proposed worlds, we'd create an environment for wild animals where there wouldn't be food scarcity, predators, disease, or even anthropogenic harms. Setting aside the immense improbability of such an world (imagine convincing a rhinoceros not to fight to the death for their territory against a wild boar or elephant), none of the terraformed videos or articles I've read have even hinted at wild animal suffering as a potential issue to be concerned about.</p><p>Also setting aside the conversation of whether or not we should extend human life into other planets and galaxies (for those who don't particularly follow longtermism, or the staunch antinatalists that might be reading this), wouldn't we be far better off just seeding these terraformed planets with plant life instead?</p><p>If the key decision makers of the future decide they <i>have</i> to bring animals to other planets (and we can't convince them otherwise), then just introducing herbivores would be preferred, at the very least. I'd still be staunchly against this unless we could somehow guarantee that the lives of every individual animal would be net-positive, but sadly\u2014 we're not even close to getting people to include this kind of consideration into these types of conversations. At least, not that I know of.</p><p>Don't get me wrong, Kurzgesagt has <i>always</i> been one of my favorite educational channels to watch. I'll continue to stay subscribed because I think they spread a lot of good, but their promotion of seeding wild life on other planets, without any consideration of the consequences, is unethical, and irresponsible.</p><p>Instead of blindly pursuing our own interests and trying to populate every inch of the galaxy with life, we should consider the impact of our actions on other future beings and strive to minimize suffering whenever possible\u2014 or in this case, preventing it from happening at all.</p><p>Thanks for reading.</p><hr><p><strong>Edit: </strong>I've just been told <a href=\"https://forum.effectivealtruism.org/posts/7DCuhq23zHxAmPqvJ/kurzgesagt-s-most-recent-video-promoting-the-introducing-of?commentId=bFMCRe66AkhvnBJ6Y\">in a reply below</a> that Open Phil recommended almost&nbsp;<a href=\"https://www.openphilanthropy.org/grants/kurzgesagt-video-creation-and-translation/\">$3 million</a>&nbsp;in grant money to \u201csupport the creation of videos on topics relevant to effective altruism and improving humanity\u2019s long-run future.\u201d<br><br>They (Constance LI) <a href=\"https://forum.effectivealtruism.org/posts/7DCuhq23zHxAmPqvJ/kurzgesagt-s-most-recent-video-promoting-the-introducing-of?commentId=bFMCRe66AkhvnBJ6Y\">wrote</a>:</p><blockquote><p>This Kurzgesagt video casually spread an idea (seeding wild animals to new planets) that could lead to s-risk and didn\u2019t even mention that the potential for s-risk exists. They also missed the opportunity to spread awareness of the neglected issue of wild animal suffering. It\u2019s a double loss.</p></blockquote><p>This is something I wanted to highlight as it's very relevant to my initial criticism of the video, and being that it's funded by OP/EA, seems to me to be a conflict worth pointing out. Again, I think Kurzgesagt is fantastic, I just think <i>this</i> particular video was irresponsible.</p><p><i>I also want to take this moment to thank everyone for their comments and positive criticisms, I'm new here but definitely taking pointers and expanding my knowledge on this subject. Much appreciated!</i></p>", "user": {"username": "davidvanbeveren"}}, {"_id": "bkF4jWM9pbBFxnCLH", "title": "Observations of community building in Asia, a \ud83e\uddf5", "postedAt": "2022-12-11T18:11:07.285Z", "htmlBody": "<p><i>This is a lightly edited </i><a href=\"https://twitter.com/vaidehiagrwalla/status/1568604140829560834\"><i>twitter thread</i></a><i> I wrote after EAGxSingapore. I figured it might be useful to post to the Forum as well.&nbsp;</i></p><p>Caveats</p><ul><li>these are my takeaways (with some interpretation) not just problems people told me.&nbsp;</li><li>summarized a bunch to fit into the 280 character limit (flag emoji FTW).&nbsp;</li><li>good &gt; perfect</li><li>Asian CB's please correct me + enhance!&nbsp;</li><li>Everyone - please ask clarification questions!</li></ul><p>What do we do with people? This isn\u2019t unique to Asian CB\u2019s, but there are strictly fewer and less attractive opportunities available. I might do a separate thread or add to this thought later.</p><p>Some countries have less traction with English &amp; worry about EA being presented as Western concept (e.g. Japan &amp; Iran). Translation of key texts seems important, and could be a way to engage newer EAs with a concrete project (<a href=\"https://forum.effectivealtruism.org/posts/LuZoJdP6CB9MBezXa/effective-altruism-stipend-a-short-experiment-by-ea-estonia\">see a test from Estonia</a>).</p><p>Countries with lots of internal issues may have difficulty gaining EA traction, but it may be a matter of perspective and approach. A Turkey CB mentioned that the fact that another group from Iran was able to get traction was inspiring, since they perceived Iran as having worse problems</p><p>Multiple CB\u2019s suggested that after talking to other EAs they started thinking more about city / uni group building rather than trying to build national groups to start with. For large countries (India) there are many choices, ...</p><p>..But some countries (Nepal, Pakistan, Vietnam) have 1 or 2 (Kathmandu, Karachi, Ho Chi Minh &amp; Hanoi) major cities that are likely to be viable for EA groups. Kathmandu has all of Nepal\u2019s top uni\u2019s and best talent pool so in practice EA Nepal ~= EA Kathmandu.</p><p>A few CBs want to/are starting local groups in liberal arts unis which they feel are more EA aligned. A challenge in Turkey is that vegans are abolitionist and against welfarism, and was concerned about discussing farmed animal welfare within EA.</p><p>In Japan (+ others?), many students study abroad. There may be an opportunity to get those students interested in EA before they go (and connect them to local EA groups in the West), and catch them again after they return.</p><p>E.g. 1 uni group struggled with reading group retention. It seemed plausible they could focus on their existing ~8 engaged members, or do a \u201ctrial week\u201d for their reading group to help attendees evaluate fit early on.</p><p>There is uncertainty over what messaging works best and non-existent testing. People mostly rely on their insights. More testing seems good, e.g. how much do you need to incorporate native philosophy vs. localizing examples and stories. bad <a href=\"https://twitter.com/vaidehiagrwalla/status/1567507122958770178\">e.g.</a> \"Asking someone who grew up or has spent a lot of time in LMICs to imagine a child drowning in a lake is not a hypothetical - it's something they might see and ask themselves every single day. This thought experiment loses a lot of its power. What are some alternatives?\"</p><p>Asking someone who grew up or has spent a lot of time in LMICs to imagine a child drowning in a lake is not a hypothetical - it's something they might see and ask themselves every single day. This thought experiment loses a lot of its power. What are some alternatives?</p><p>Vietnam doesn\u2019t have a big book-reading culture, so EA books could be less likely to be a way in. Perhaps focusing on blogs or podcasts or other formats is more promising?</p><p>Many of us (including myself) learnt a lot from Israel on volunteer management and early stage group priorities. I believe a lot of value I provide CB's is expanding the option space and generating lots of specific examples. I wish more CB\u2019s from around the world had attended.</p><p>Early stage groups (which is most Asia groups today) could benefit from The Lean Startup model of validated learning - trying to optimize early activities to learn as much as possible about their target audience(s), rapidly iterate and do cheap tests. <a href=\"https://t.co/eRGDxirrs6\">http://theleanstartup.com</a></p><p>In the Philippines, they spent a lot of time on local priorities research but it got people more engaged. I want to explore ways to 80/20 this, and other promising projects which have the risk of scope creep. (ex #1)</p><p>Saying \"let's do validated learning\" and doing it are not the same thing. Are there ways to jumpstart peoples' journeys into this mindset? It's more challenging with vounteer/part-time CB's who have competing priorities. Perhaps framing it as personal development could help?</p><p>I think groups need to orient themselves to finding their \"early adopters\" - the people who will join the community because they see the potential of what it /could be/. These are different from the people who will join when the community is established and thriving.</p><p>(most) (early stage) (asian) groups need more support. caveat caveat caveat limited resources/ diminishing returns/ impact hard to measure / incubation is hits based etc etc. i still think they need more support.</p>", "user": {"username": "vaidehi_agarwalla"}}, {"_id": "44XPFrHiFwFBM2jfL", "title": "An appraisal of the Future of Life Institute AI existential risk program", "postedAt": "2022-12-11T13:36:21.234Z", "htmlBody": "<p>Summary: This is a small post to thank the Future of Life Institute for setting up their AI existential risk community, as well as the Vitalik Buterin PhD and postdoc fellowships.</p>\n<p>A couple of years ago the Future of Life Institute set up a community of academic researchers interested in working on AI Safety and Alignment. I think this is a really useful contribution towards solving this problem because it addresses the AI alignment problem at multiple levels:</p>\n<ol>\n<li>\n<p>It helps publicly clarify and list what academic researchers are interested in working towards AI Safety, and in which techniques they are specialists. Before it, there were some hard-to-find google sheets listing some, but that made it much less clear what problems where they were interested in or if they really wanted to be known for working on this.</p>\n</li>\n<li>\n<p>It gives a reputation to the field of AI Safety and signals that this is a problem academics consider important and tractable enough to be working on.</p>\n</li>\n<li>\n<p>It clarifies the academic path to becoming an AI Safety researcher, especially via the Vitalik Buterin fellowships.</p>\n</li>\n<li>\n<p>It helps AI safety researchers know each other and what they are working on, especially lowering the disadvantage of not being physically living in an AI Safety hub.</p>\n</li>\n</ol>\n<p>For example, if not for the FLI I would probably have not met Victor Veitch, with whom I applied to a couple of postdoc grants, even if in the end I postponed that plan.</p>\n<p>This year I am helping review the FLI PhD fellowship, and the two most important conclusions I got were: a) most of the applications I reviewed are of outstanding quality, and b) they also mostly come from just a handful of universities. To me, this indicates that it should be possible to scale this program up without sacrificing quality and that it may represent a good donation opportunity. Thus, I want to thank the FLI for setting it up.</p>\n", "user": {"username": "PabloAMC"}}, {"_id": "RrTWL3rDzuruEQdTm", "title": "Do you know someone who would edit a holiday song?", "postedAt": "2022-12-12T20:55:17.773Z", "htmlBody": "<p>I welcome anyone's edits on a current draft:</p><ul><li><a href=\"https://docs.google.com/document/d/1EYVcCOzbKms_elHyTAt6TZ45ScKZCKaGQthzDRT011w/edit\">Do you know someone who would edit a holiday song?</a></li></ul>", "user": {"username": "ethankennerly@gmail.com"}}, {"_id": "SEXGJfj7RMRomf9ts", "title": "Save to give: switch banks", "postedAt": "2022-12-11T08:53:38.373Z", "htmlBody": "<p>(American example based on <a href=\"https://www.effectivealtruismdata.com/\">https://www.effectivealtruismdata.com/</a> demographics, but same principles apply broadly)</p><p><strong>tl;dr save thousands every year with no spending/saving/investing behavior change by switching to high interest saving accounts and cashback credit cards</strong></p><h3><strong>Credit cards:</strong></h3><ul><li><strong>Cashback</strong>: ~$1000/yr: Debit cards are still used more than credit cards in America,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9shedtb6jd7\"><sup><a href=\"#fn9shedtb6jd7\">[1]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref61pvpsugurt\"><sup><a href=\"#fn61pvpsugurt\">[2]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9d449eop7q9\"><sup><a href=\"#fn9d449eop7q9\">[3]</a></sup></span>&nbsp;even though American credit cards with no annual fees offer at least 2% <a href=\"https://en.wikipedia.org/wiki/Credit_card#Cashback_reward_programs\">cashback</a> on all purchases.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref86yhdigedcp\"><sup><a href=\"#fn86yhdigedcp\">[4]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhep0asilkiv\"><sup><a href=\"#fnhep0asilkiv\">[5]</a></sup></span>&nbsp;Americans spend ~$10T annually across payment card types (~$50k/adult).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1jj995nfiqx\"><sup><a href=\"#fn1jj995nfiqx\">[6]</a></sup></span></li><li><strong>Cashflow</strong>: ~$500/yr: Unlike debit cards, credit card statements are paid a month or two after the actual purchases, allowing interest accumulation in the meantime. <strong>Treat credit cards like debit cards and do not spend money you do not have. Set up automatic payments to always pay monthly statements in full.</strong></li><li><strong>Credit score</strong>: $?/yr: Responsible credit card use improves <a href=\"https://en.wikipedia.org/wiki/Credit_score\">credit scores</a>, which lowers interest rates on money borrowed going forward. Most American household debt is mortgage debt,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1lhbzhzdjae\"><sup><a href=\"#fn1lhbzhzdjae\">[7]</a></sup></span>&nbsp;but this will also get better rates on other loans like auto and student loans.</li><li><a href=\"https://www.nerdwallet.com/article/credit-cards/credit-card-vs-debit-card-safer-online-purchases\"><strong>Better fraud protection</strong></a>: $?/yr</li></ul><h3><strong>Bank accounts:</strong></h3><ul><li><strong>Interest</strong>: ~$3000/yr: American savings account interest rates average ~0.2%,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4skp49sgry5\"><sup><a href=\"#fn4skp49sgry5\">[8]</a></sup></span>&nbsp; even though many savings accounts offer well over 3% interest rates and rising.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdjjzqqd39lr\"><sup><a href=\"#fndjjzqqd39lr\">[9]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjnguy0h33ei\"><sup><a href=\"#fnjnguy0h33ei\">[10]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4khhuyr9lso\"><sup><a href=\"#fn4khhuyr9lso\">[11]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0hy6spao6dgv\"><sup><a href=\"#fn0hy6spao6dgv\">[12]</a></sup></span>&nbsp;Americans average ~$40k in savings; Americans with bachelor\u2019s degrees average ~$80k in savings.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv1hzg26x0or\"><sup><a href=\"#fnv1hzg26x0or\">[13]</a></sup></span>&nbsp;(tax filing should include any savings account interest)</li><li><strong>Avoiding fees</strong>: ~$100/yr<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff39hka224pt\"><sup><a href=\"#fnf39hka224pt\">[14]</a></sup></span>: Your bank should not charge ATM/overdraft/excessive transaction/minimum balance/maintenance fees (not even for using other banks' ATMs)</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9shedtb6jd7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9shedtb6jd7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.forbes.com/sites/ronshevlin/2020/11/09/the-debit-card-explosion-is-going-to-fizzle/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn61pvpsugurt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref61pvpsugurt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.spreedly.com/blog/credit-card-acceptance-rates-2019-regional-breakdown</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9d449eop7q9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9d449eop7q9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.fool.com/the-ascent/research/credit-debit-card-market-share-network-issuer/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn86yhdigedcp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref86yhdigedcp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.nerdwallet.com/best/credit-cards/cash-back</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhep0asilkiv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhep0asilkiv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.bankrate.com/finance/credit-cards/cash-back/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1jj995nfiqx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1jj995nfiqx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://nilsonreport.com/publication_chart_and_graphs_archive.php#:~:text=Consumer%20and%20commercial%20credit%2C%20debit,reach%20%2414.075%20trillion%20in%202026.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1lhbzhzdjae\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1lhbzhzdjae\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.stlouisfed.org/open-vault/2020/april/snapshot-record-high-household-debt</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4skp49sgry5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4skp49sgry5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.bankrate.com/banking/savings/average-savings-interest-rates/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndjjzqqd39lr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdjjzqqd39lr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.bankrate.com/banking/savings/rates/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjnguy0h33ei\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjnguy0h33ei\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.nerdwallet.com/best/banking/high-yield-online-savings-accounts</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4khhuyr9lso\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4khhuyr9lso\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.investopedia.com/best-high-yield-savings-accounts-4770633</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0hy6spao6dgv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0hy6spao6dgv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.wsj.com/articles/the-42-billion-question-why-arent-americans-ditching-big-banks-11670472623</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv1hzg26x0or\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv1hzg26x0or\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.bankrate.com/banking/savings/savings-account-average-balance/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf39hka224pt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff39hka224pt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.nerdwallet.com/article/banking/checking-account-fees-study</p></div></li></ol>", "user": {"username": "Pat Myron"}}, {"_id": "NbiHKTN5QhFFfjjm5", "title": "AI Safety Seems Hard to Measure", "postedAt": "2022-12-11T01:31:39.092Z", "htmlBody": "<div><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1670529009/mirroredImages/7gkXuHEm6CqEGT2mg/qbfsyyvrt9uwk57qeygw.png\" alt=\"AI Safety Seems Hard to Measure\"><p id=\"block0\">\n  \nIn previous pieces, I argued that there's a real and large risk of AI systems' <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">developing dangerous goals of their own</a> and <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeating all of humanity</a> - at least in the absence of specific efforts to prevent this from happening.\n</p>\n<p id=\"block1\">\nA young, growing field of <strong>AI safety research</strong> tries to reduce this risk, by finding ways to ensure that AI systems behave as intended (rather than forming ambitious aims of their own and deceiving and manipulating humans as needed to accomplish them). \n</p>\n<p id=\"block2\">\nMaybe we'll succeed in reducing the risk, and maybe we won't. <strong>Unfortunately, I think it could be hard to know either way</strong>. This piece is about four fairly distinct-seeming reasons that this could be the case - and that AI safety could be an unusually difficult sort of science.\n</p>\n<p id=\"block3\">\nThis piece is aimed at a broad audience, because I think it's <strong>important for the challenges here to be broadly understood. </strong>I expect powerful, dangerous AI systems to have a lot of benefits (commercial, military, etc.), and to potentially <em>appear</em> safer than they are - so I think it will be hard to be as cautious about AI as we should be. I think our odds look better if many people understand, at a high level, some of the challenges in knowing whether AI systems are as safe as they appear.\n</p>\n<p id=\"block4\">\nFirst, I'll recap the basic challenge of AI safety research, and outline what I <em>wish</em> AI safety research could be like. I wish it had this basic form: \"Apply a test to the AI system. If the test goes badly, try another AI development method and test that. If the test goes well, we're probably in good shape.\" I think car safety research mostly looks like this; I think AI <em>capabilities</em> research mostly looks like this.\n</p>\n<p id=\"block5\">\nThen, I\u2019ll give four reasons that <strong>apparent success in AI safety can be misleading. </strong>\n</p><!--\n<p>\n<img src=\"https://www.cold-takes.com/content/images/2022/12/Screen-Shot-2022-12-06-at-10.37-1--1-.png\">\n</p>-->\n<p id=\"block6\">\n</p><table style=\"border-collapse: collapse;\">\n  <tbody><tr>\n   <td colspan=\"3\" style=\"border: 1px solid;\"><strong>\u201cGreat news - I\u2019ve tested this AI and it looks safe.\u201d </strong>Why might we still have a problem?\n   </td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\"><em>Problem</em>\n   </td>\n   <td style=\"border: 1px solid;\"><em>Key question</em>\n   </td>\n   <td style=\"border: 1px solid;\"><em>Explanation</em>\n   </td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>Lance Armstrong problem</strong>\n   </td>\n   <td style=\"border: 1px solid;\">Did we get the AI to be <strong><span style=\"color:var(--green-color);\">actually safe</span></strong> or <strong><span style=\"color:var(--red-color);\">good at hiding its dangerous actions</span>?</strong>\n   </td>\n  <td style=\"border: 1px solid;\"><p id=\"block7\">When dealing with an intelligent agent, it\u2019s hard to tell the difference between \u201cbehaving well\u201d and \u201c<em>appearing</em> to behave well.\u201d</p>\n<p id=\"block8\">\nWhen professional cycling was cracking down on performance-enhancing drugs, Lance Armstrong was very successful and seemed to be unusually \u201cclean.\u201d It later came out that he had been using drugs with an unusually sophisticated operation for concealing them.\n   </p></td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>King Lear problem</strong>\n   </td>\n   <td style=\"border: 1px solid;\"><p id=\"block9\">The AI is <strong><span style=\"color:var(--green-color);\">(actually) well-behaved when humans are in control. </span></strong>Will this transfer to <strong><span style=\"color:var(--red-color);\">when AIs are in control</span>?</strong></p>\n   </td>\n   <td style=\"border: 1px solid;\"><p id=\"block10\">It's hard to know how someone will behave when they have power over you, based only on observing how they behave when they don't. </p>\n<p id=\"block11\">\nAIs might behave as intended as long as humans are in control - but at some future point, AI systems might be capable and widespread enough to have opportunities to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">take control of the world entirely</a>. It's hard to know whether they'll take these opportunities, and we can't exactly run a clean test of the situation. \n</p><p id=\"block12\">\nLike King Lear trying to decide how much power to give each of his daughters before abdicating the throne.\n   </p></td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>lab mice problem</strong>\n   </td>\n      <td style=\"border: 1px solid;\"><strong><span style=\"color:var(--green-color);\">Today's \"subhuman\" AIs are safe.</span></strong>What about <strong><span style=\"color:var(--red-color);\">future AIs with more human-like abilities</span>?</strong>\n   </td>\n   <td style=\"border: 1px solid;\"><p id=\"block13\">Today's AI systems aren't advanced enough to exhibit the basic behaviors we want to study, such as deceiving and manipulating humans.</p> \n<p id=\"block14\">\nLike trying to study medicine in humans by experimenting only on lab mice.\n   </p></td>\n  </tr>\n  <tr>\n   <td style=\"border: 1px solid;\">The <strong>first contact problem</strong>\n   </td>\n   <td style=\"border: 1px solid;\"><p id=\"block15\">Imagine that <strong><span style=\"color:var(--green-color);\">tomorrow's \"human-like\" AIs are safe.</span></strong> How will things go <strong><span style=\"color:var(--red-color);\">when AIs have capabilities far beyond humans'</span>?</strong></p>\n   </td>\n   <td style=\"border: 1px solid;\"><p id=\"block16\">AI systems might (collectively) become vastly more capable than humans, and it's ... just really hard to have any idea what that's going to be like. As far as we know, there has never before been anything in the galaxy that's vastly more capable than humans in the relevant ways! No matter what we come up with to solve the first three problems, we can't be too confident that it'll keep working if AI advances (or just proliferates) a lot more. </p>\n<p id=\"block17\">\nLike trying to plan for first contact with extraterrestrials (this barely feels like an analogy).\n   </p></td>\n  </tr>\n</tbody></table>\n<p id=\"block18\"></p>\n\n<p id=\"block19\">\nI'll close with Ajeya Cotra's \"<a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#analogy-the-young-ceo\">young businessperson</a>\" analogy, which in some sense ties these concerns together. A future piece will discuss some reasons for hope, despite these problems.\n</p>\n<h2 id=\"Recap_of_the_basic_challenge\">Recap of the basic challenge</h2>\n\n\n<p id=\"block20\">\nA <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">previous piece</a> laid out the basic case for concern about AI misalignment. In brief: if extremely capable AI systems are developed using methods like the ones AI developers use today, it seems like there's a substantial risk that:\n</p>\n<ul>\n\n<li id=\"block21\">These AIs will develop <strong>unintended aims</strong> (states of the world they make calculations and plans toward, as a chess-playing AI \"aims\" for checkmate);\n\n</li><li id=\"block22\">These AIs will deceive, manipulate, and overpower humans as needed to achieve those aims;\n\n</li><li id=\"block23\">Eventually, this could reach the point where AIs <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">take over the world from humans entirely</a>.\n</li>\n</ul>\n<p id=\"block24\">\nI see <strong>AI safety research</strong> as trying to <strong>design AI systems that won't <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#what-it-means-for\">aim</a> to deceive, manipulate or defeat humans - even if and when these AI systems are extraordinarily capable</strong> (and would be very effective at deception/manipulation/defeat if they were to aim at it).<strong> </strong>That is: AI safety research is trying to reduce the risk of the above scenario, <em>even if</em> (as I've assumed) humans rush forward with training powerful AIs to do ever-more ambitious things.\n</p>\n<p id=\"block25\">\n</p><details id=\"Box1\"><summary><!--(Click to expand) -->More detail on why AI could make this the most important century (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box1\">click to view on the web</a>)</summary><!--\n</p>\n<p>\nIn <a href=\"https://forum.effectivealtruism.org/posts/xSDWS8yWWPcqAa8NR/ai-alignment-research-links-1/\">The Most Important Century</a>, I argued that the 21st century could be the most important century ever for humanity, via the development of advanced AI systems that could dramatically speed up scientific and technological advancement, getting us more quickly than most people imagine to a deeply unfamiliar future.\n</p>\n<p>\n<a href=\"https://forum.effectivealtruism.org/posts/xSDWS8yWWPcqAa8NR/ai-alignment-research-links-1/\">This page</a> has a ~10-page summary of the series, as well as links to an audio version, podcasts, and the full series.\n</p>\n<p>\nThe key points I argue for in the series are:\n</p>\n<ul>\n\n<li><strong>The long-run future is radically unfamiliar. </strong>Enough advances in technology could lead to a long-lasting, galaxy-wide civilization that could be a radical utopia, dystopia, or anything in between.\n\n<li><strong>The long-run future could come much faster than we think,</strong> due to a possible AI-driven productivity explosion.\n\n<li>The relevant kind of <strong>AI looks like it will be developed this century</strong> - making this century the one that will initiate, and have the opportunity to shape, a future galaxy-wide civilization.\n\n<li>These claims seem too \"wild\" to take seriously. But there are a lot of reasons to think that <strong>we live in a wild time, and should be ready for anything.</strong>\n\n<li>We, the people living in this century, have the chance to have a huge impact on huge numbers of people to come - if we can make sense of the situation enough to find helpful actions. But right now, <strong>we aren't ready for this.</strong>\n</li>\n</ul>\n<p>\n<strong>--></details>\n<p id=\"block26\"></p>\n<p id=\"block27\">\n</p><details id=\"Box2\"><summary><!--(Click to expand) -->Why would AI \"aim\" to defeat humanity? (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box2\">click to view on the web</a>)</summary><!--\n</p>\n<p>\nA <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">previous piece</a> argued that if today\u2019s AI development methods lead directly to powerful enough AI systems, disaster is likely by default (in the absence of specific countermeasures). \n</p>\n<p>\nIn brief:\n</p>\n<ul>\n\n<li>Modern AI development is essentially based on \u201ctraining\u201d via trial-and-error. \n\n<li>If we move forward incautiously and ambitiously with such training, and if it gets us all the way to very powerful AI systems, then such systems will likely end up <em>aiming for certain states of the world</em> (analogously to how a chess-playing AI aims for checkmate)<em>.</em>\n\n<li>And these states will be<em> other than the ones we intended</em>, because our trial-and-error training methods won\u2019t be accurate. For example, when we\u2019re confused or misinformed about some question, we\u2019ll reward AI systems for giving the wrong answer to it - unintentionally training deceptive behavior.\n\n<li>We should expect disaster if we have AI systems that are both (a) <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">powerful enough</a> to defeat humans and (b) aiming for states of the world that we didn\u2019t intend. (\u201cDefeat\u201d means taking control of the world and doing what\u2019s necessary to keep us out of the way; it\u2019s unclear to me whether we\u2019d be literally killed or just forcibly stopped<sup id=\"fnref1\"><a href=\"#fn1\" rel=\"footnote\">1</a></sup> from changing the world in ways that contradict AI systems\u2019 aims.)</ul>\n<p>\n--></details>\n<p id=\"block28\"></p>\n<p id=\"block29\">\n</p><details id=\"Box3\"><summary><!--(Click to expand) --><em>How</em> could AI defeat humanity? (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box3\">click to view on the web</a>)</summary><!--\n</p>\n<p>\nIn a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">previous piece</a>, I argue that AI systems could defeat all of humanity combined, if (for whatever reason) they were aimed toward that goal.\n</p>\n<p>\nBy defeating humanity, I mean gaining control of the world so that AIs, not humans, determine what happens in it; this could involve killing humans or simply \u201ccontaining\u201d us in some way, such that we can\u2019t interfere with AIs\u2019 aims.\n</p>\n<p>\nOne way this could happen is if AI became extremely advanced, to the point where it had \"cognitive superpowers\" beyond what humans can do. In this case, a single AI system (or set of systems working together) could imaginably:\n</p>\n<ul>\n\n<li>Do its own research on how to build a better AI system, which culminates in something that has incredible other abilities.\n\n<li>Hack into human-built software across the world.\n\n<li>Manipulate human psychology.\n\n<li>Quickly generate vast wealth under the control of itself or any human allies.\n\n<li>Come up with better plans than humans could imagine, and ensure that it doesn't try any takeover attempt that humans might be able to detect and stop.\n\n<li>Develop advanced weaponry that can be built quickly and cheaply, yet is powerful enough to overpower human militaries.\n</li>\n</ul>\n<p>\nHowever, my piece also explores what things might look like if <em>each AI system basically has similar capabilities to humans. </em>In this case:\n</p>\n<ul>\n\n<li>Humans are likely to deploy AI systems throughout the economy, such that they have large numbers and access to many resources - and the ability to make copies of themselves. \n\n<li>From this starting point, AI systems with human-like (or greater) capabilities would have a number of possible ways of getting to the point where their total population could outnumber and/or out-resource humans.\n\n<li>I address a number of possible objections, such as \"How can AIs be dangerous without bodies?\"\n</li>\n</ul>\n<p>\nMore: <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">AI could defeat all of us combined</a>--></details>\n<p id=\"block30\"></p>\n<h2 id=\"I_wish_AI_safety_research_were_straightforward\">I wish AI safety research were straightforward</h2>\n\n\n<p id=\"block31\">\nI wish AI safety research were like car safety research.<sup id=\"fnref2\"><a href=\"#fn2\" rel=\"footnote\">2</a></sup>\n</p>\n<p id=\"block32\">\nWhile I'm sure this is an oversimplification, I think a lot of car safety research looks basically like this:\n</p>\n<ul>\n\n<li id=\"block33\">Companies carry out test crashes with test cars. The results give a pretty good (not perfect) indication of what would happen in a real crash.\n\n</li><li id=\"block34\">Drivers try driving the cars in low-stakes areas without a lot of traffic. Things like steering wheel malfunctions will probably show up here; if they don't and drivers are able to drive normally in low-stakes areas, it's probably safe to drive the car in traffic.\n\n</li><li id=\"block35\">None of this is perfect, but the occasional problem isn't, so to speak, <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">the end of the world</a>. The worst case tends to be a handful of accidents, followed by a recall and some changes to the car's design validated by further testing.\n</li>\n</ul>\n<p id=\"block36\">\nOverall, <strong>if we have problems with car safety, we'll probably be able to observe them relatively straightforwardly under relatively low-stakes circumstances.</strong>\n</p>\n<p id=\"block37\">\nIn important respects, many types of research and development have this basic property: we can observe how things are going during testing to get good evidence about how they'll go in the real world. Further examples include medical research,<sup id=\"fnref3\"><a href=\"#fn3\" rel=\"footnote\">3</a></sup> chemistry research,<sup id=\"fnref4\"><a href=\"#fn4\" rel=\"footnote\">4</a></sup> software development,<sup id=\"fnref5\"><a href=\"#fn5\" rel=\"footnote\">5</a></sup> etc. \n</p>\n<p id=\"block38\">\n<strong>Most AI research looks like this as well. </strong>People can test out what an AI system is capable of reliably doing (e.g., translating speech to text), before integrating it into some high-stakes commercial product like Siri. This works both for ensuring that the AI system is <em>capable</em> (e.g., that it does a good job with its tasks) and that it's <em>safe in certain ways</em> (for example, if we're worried about toxic language, testing for this is relatively straightforward).\n</p>\n<p id=\"block39\">\nThe rest of this piece will be about some of the ways in which \"testing\" for AI safety <strong>fails to give us straightforward observations about whether, once AI systems are deployed in the real world, the world will actually be safe.</strong>\n</p>\n<p id=\"block40\">\nWhile all research has to deal with <em>some</em> differences between testing and the real world, I think the challenges I'll be going through are unusual ones.\n</p>\n<h2 id=\"Four_problems\">Four problems</h2>\n\n\n<h3 id=\"_1__The_Lance_Armstrong_problem__is_the_AI_actually_safe_or_good_at_hiding_its_dangerous_actions_\">(1) The Lance Armstrong problem: is the AI <em>actually safe</em> or <em>good at hiding its dangerous actions</em>?</h3>\n<p id=\"block41\"></p><center><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1670529008/mirroredImages/7gkXuHEm6CqEGT2mg/dwdqeahqv7faybkgnfax.jpg\" width=\"400\" alt=\"AI Safety Seems Hard to Measure\"></center><p id=\"block42\"></p>\n\n<p id=\"block43\">\nFirst, let's imagine that:\n</p>\n<ul>\n\n<li id=\"block44\">We have AI systems available that can do roughly everything a human can, with some different strengths and weaknesses but no huge difference in \"overall capabilities\" or economic value per hour of work. \n\n</li><li id=\"block45\">We're observing early signs that AI systems behave in unintended, deceptive ways, such as giving wrong answers to questions we ask, or writing software that falsifies metrics instead of doing the things the metrics were supposed to measure (e.g., software meant to make a website run faster might instead falsify metrics about its loading time).\n</li>\n</ul>\n<p id=\"block46\">\nWe theorize that modifying the AI training in some way<sup id=\"fnref6\"><a href=\"#fn6\" rel=\"footnote\">6</a></sup> will make AI systems less likely to behave deceptively. We try it out, and find that, in fact, our AI systems seem to be behaving better than before - we are finding fewer incidents in which they behaved in unintended or deceptive ways. \n</p>\n<p id=\"block47\">\nBut that's just a statement about <em>what we're noticing</em>. Which of the following just happened:\n</p>\n<ul>\n\n<li id=\"block48\">Did we just train our AI systems to be less deceptive?\n\n</li><li id=\"block49\">Did we just train our AI systems to be <em>better at</em> deception, and so to make us <em>think</em> they became less deceptive?\n\n</li><li id=\"block50\">Did we just train our AI systems to be better at calculating when they might get caught in deception, and so to be less deceptive only when the deception would otherwise be caught? \n<ul>\n \n<li id=\"block51\">This one could be useful! Especially if we're able to set up auditing systems in many real-world situations, such that we <em>could</em> expect deception to be caught a lot of the time. But it does leave open the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#The-King-Lear-problem\">King Lear problem</a> covered next.\n    \n</li> \n</ul>\n</li> \n</ul>\n<p id=\"block52\">\n(...Or some combination of the three?)\n</p>\n<p id=\"block53\">\nWe're hoping to be able to deploy AI systems throughout the economy, so - just like human specialists - they will almost certainly have some opportunities to be deceptive without being caught. The fact that they <em>appear honest in our testing</em> is not clear comfort against this risk.\n</p>\n<p id=\"block54\">\nThe analogy here is to competitive cyclist <a href=\"https://en.wikipedia.org/wiki/Lance_Armstrong\">Lance Armstrong</a>. Armstrong won the Tour de France race 7 times in a row, while many of his competitors were caught using performance-enhancing drugs and disqualified. But more than 5 years after his last win, an investigation \"concluded that Armstrong had used performance-enhancing drugs over the course of his career[5] and named him as the ringleader of 'the most sophisticated, professionalized and successful doping program that sport has ever seen'.\" Now the list of Tour de France winners looks like this:\n</p>\n<p id=\"block55\">\n\n<img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1670529008/mirroredImages/7gkXuHEm6CqEGT2mg/mogjorkqyhy9lf4w8wbt.png\" width=\"\" alt=\"AI Safety Seems Hard to Measure\" title=\"image_tooltip\">\n\n</p>\n<p id=\"block56\">\nA broader issue here is that <strong>when AI systems become capable enough, AI safety research starts to look more like social sciences (studying human beings) than like natural sciences. </strong>Social sciences are generally less rigorous and harder to get clean results from, and one factor in this is that it can be hard to study someone who's aware they're being studied.<sup id=\"fnref7\"><a href=\"#fn7\" rel=\"footnote\">7</a></sup>\n</p>\n<p id=\"block57\">\nTwo broad categories of research that might help with the Lance Armstrong problem:\n</p>\n<ul>\n<li id=\"DigitalNeuroscience\"><a href=\"https://www.transformer-circuits.pub/2022/mech-interp-essay/index.html\">Mechanistic interpretability</a><sup id=\"fnref8\"><a href=\"#fn8\" rel=\"footnote\">8</a></sup> can be thought of analyzing the \"digital brains\" of AI systems (not just analyzing their behavior and performance.) Currently, AI systems are <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box4\">black boxes</a> in the sense that they perform well on tasks, but we can't say much about <em>how</em> they are doing it; mechanistic interpretability aims to change this, which could give us the ability to \"mind-read\" AIs and detect deception. (There could still be a risk that AI systems are arranging their own \"digital brains\" in misleading ways, but this seems quite a bit harder than simply <em>behaving</em> deceptively.)\n</li><li id=\"block59\">Some researchers work on \"scalable supervision\" or \"competitive supervision.\" The idea is that if we are training an AI system that might become deceptive, we set up some supervision process for it that we expect to reliably catch any attempts at deception. This could be because the supervision process itself uses AI systems with more resources than the one being supervised, or because it uses a system of randomized audits where extra effort is put into catching deception.\n    </li></ul>\n    <p id=\"block60\">\n</p><details id=\"Box4\"><summary><!--(Click to expand) --><em>Why are AI systems \"black boxes\" that we can't understand the inner workings of? (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box4\">click to view on the web</a>)</em></summary><!--\n</p>\n<p>\nI explain this briefly in an <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj/#making-pasta\">old Cold Takes post</a>; it's explained in more detail in more technical pieces by <a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#_HFDT_scales_far__assumption__Alex_is_trained_to_achieve_excellent_performance_on_a_wide_range_of_difficult_tasks\">Ajeya Cotra</a> (section I linked to) and <a href=\"https://drive.google.com/file/d/1TsB7WmTG2UzBtOs349lBqY5dEBaxZTzG/view\">Richard Ngo</a> (section 2). \n</p>\n<p>\nWhat I mean by \u201cblack-box trial-and-error\u201d is explained briefly in an <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj/#making-pasta\">old Cold Takes post</a>, and in more detail in more technical pieces by <a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#_HFDT_scales_far__assumption__Alex_is_trained_to_achieve_excellent_performance_on_a_wide_range_of_difficult_tasks\">Ajeya Cotra</a> (section I linked to) and <a href=\"https://drive.google.com/file/d/1TsB7WmTG2UzBtOs349lBqY5dEBaxZTzG/view\">Richard Ngo</a> (section 2). Here\u2019s a quick, oversimplified characterization.\n</p>\n<p>\nToday, the most common way of building an AI system is by using an \"artificial neural network\" (ANN), which you might think of sort of like a \"digital brain\" that starts in an empty (or random) state: it hasn't yet been wired to do specific things. A process something like this is followed:\n</p>\n<ul>\n\n<li>The AI system is given some sort of task.\n\n<li>The AI system tries something, initially something pretty random.\n\n<li>The AI system gets information about how well its choice performed, and/or what would\u2019ve gotten a better result. Based on this, it \u201clearns\u201d by tweaking the wiring of the ANN (\u201cdigital brain\u201d) - literally by strengthening or weakening the connections between some \u201cartificial neurons\u201d and others. The tweaks cause the ANN to form a stronger association between the choice it made and the result it got. \n\n<li>After enough tries, the AI system becomes good at the task (it was initially terrible). \n\n<li>But nobody really knows anything about <em>how or why</em> it\u2019s good at the task now. The development work has gone into building a flexible architecture for it to learn well from trial-and-error, and into \u201ctraining\u201d it by doing all of the trial and error. We mostly can\u2019t \u201clook inside the AI system to see how it\u2019s thinking.\u201d\n\n<li>For example, if we want to know why a chess-playing AI such as AlphaZero made some particular chess move, we can't look inside its code to find ideas like \"Control the center of the board\" or \"Try not to lose my queen.\" Most of what we see is just a vast set of numbers, denoting the strengths of connections between different artificial neurons. As with a human brain, we can mostly only guess at what the different parts of the \"digital brain\" are doing.\n</li>\n</ul>\n<p>\n--></details>\n<p id=\"block61\"></p>\n\n<h3 id=\"_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">(2) The King Lear problem: how do you test what will happen when it's no longer a test?</h3>\n<p id=\"block62\"></p><center><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1670529008/mirroredImages/7gkXuHEm6CqEGT2mg/hzun99ggurozxsnk3ufz.jpg\" width=\"400\" alt=\"AI Safety Seems Hard to Measure\"></center><p id=\"block63\"></p>\n\n<p id=\"block64\">\nThe Shakespeare play <a href=\"https://en.wikipedia.org/wiki/King_Lear\">King Lear</a> opens with the King (Lear) stepping down from the throne, and immediately learning that he has left his kingdom to the wrong two daughters. Loving and obsequious while he was deciding on their fate,<sup id=\"fnref9\"><a href=\"#fn9\" rel=\"footnote\">9</a></sup> they reveal their contempt for him as soon as he's out of power and they're in it.\n</p>\n<p id=\"block65\">\nIf we're building AI systems that can reason like humans, dynamics like this become a potential issue. \n</p>\n<p id=\"block66\">\nI <!-- next links to parts of https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/ , maybe https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#deceiving-and-manipulating ?--> <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#existential-risks-to-humanity\">previously</a> noted that an AI with <em>any</em> ambitious <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#what-it-means-for\">aim</a> - or just an AI that wants to avoid being shut down or modified - might calculate that the best way to do this is by behaving helpfully and safely in all \"tests\" humans can devise. But once there is a real-world opportunity to disempower humans for good, that same aim <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#existential-risks-to-humanity\">could cause the AI to disempower humans.</a>\n</p>\n<p id=\"block67\">\nIn other words:\n</p>\n<ul>\n\n<li id=\"block68\">(A) When we're developing and testing AI systems, we have the power to decide which systems will be modified or shut down and which will be deployed into the real world. (Like King Lear deciding who will inherit his kingdom.)\n\n</li><li id=\"block69\">(B) But at some later point, these systems could be operating in the economy, in high numbers with a lot of autonomy. (This possibility is spelled out/visualized a bit more <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/#how-this-could-work-if-humans-create-a-huge-population-of-ais\">here</a> and <a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#As_humans__control_fades__Alex_would_be_motivated_to_take_over\">here</a>.) At that point, they may have opportunities to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeat all of humanity</a> such that we never make decisions about them again. (Like King Lear's daughters after they've taken control.)\n</li>\n</ul>\n<p id=\"block70\">\n</p><details id=\"Box5\"><summary><!--(Click to expand) --><em>How could AI defeat humanity? (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box5\">click to view on the web</a>)</em></summary><!--\n</p>\n<p>\nIn a <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">previous piece</a>, I argue that AI systems could defeat all of humanity combined, if (for whatever reason) they were aimed toward that goal.\n</p>\n<p>\nBy defeating humanity, I mean gaining control of the world so that AIs, not humans, determine what happens in it; this could involve killing humans or simply \u201ccontaining\u201d us in some way, such that we can\u2019t interfere with AIs\u2019 aims.\n</p>\n<p>\nOne way this could happen is if AI became extremely advanced, to the point where it had \"cognitive superpowers\" beyond what humans can do. In this case, a single AI system (or set of systems working together) could imaginably:\n</p>\n<ul>\n\n<li>Do its own research on how to build a better AI system, which culminates in something that has incredible other abilities.\n\n<li>Hack into human-built software across the world.\n\n<li>Manipulate human psychology.\n\n<li>Quickly generate vast wealth under the control of itself or any human allies.\n\n<li>Come up with better plans than humans could imagine, and ensure that it doesn't try any takeover attempt that humans might be able to detect and stop.\n\n<li>Develop advanced weaponry that can be built quickly and cheaply, yet is powerful enough to overpower human militaries.\n</li>\n</ul>\n<p>\nHowever, my piece also explores what things might look like if <em>each AI system basically has similar capabilities to humans. </em>In this case:\n</p>\n<ul>\n\n<li>Humans are likely to deploy AI systems throughout the economy, such that they have large numbers and access to many resources - and the ability to make copies of themselves. \n\n<li>From this starting point, AI systems with human-like (or greater) capabilities would have a number of possible ways of getting to the point where their total population could outnumber and/or out-resource humans.\n\n<li>I address a number of possible objections, such as \"How can AIs be dangerous without bodies?\"\n</li>\n</ul>\n<p>\nMore: <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">AI could defeat all of us combined</a>--></details>\n<p id=\"block71\"></p>\n<p id=\"block72\">\n<strong id=\"If_AI_systems_can_detect_the_difference_between__A__and__B__above__then_their_behavior_might_systematically_change_from_one_to_the_other___and_there_s_no_clear_way_to_test_their_behavior_in__B__\">If AI systems can detect the difference between (A) and (B) above, then their behavior might systematically change from one to the other - and there's no clear way to <em>test</em> their behavior in (B).</strong>\n</p>\n<p id=\"block73\">\n</p><details id=\"Box6\"><summary><!--(Click to expand) --><em>The Volkswagen emissions scandal (Details not included in email - <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box4\">click to view on the web</a>)</em></summary><!--\n</p>\n<p>\nIn 2015, the US Environmental Protection Agency \"found that Volkswagen had intentionally programmed ... engines to activate their emissions controls only during laboratory emissions testing, which caused the vehicles' [nitrogen oxide] output to meet US standards during regulatory testing. However, the vehicles emitted up to 40 times more [nitrogen oxide] in real-world driving.\" (From <a href=\"https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal\">Wikipedia</a>)\n</p>\n<p>\nThis is an example of the \"King Lear problem\": there was a deliberate distinction made where the engines were behaving differently during tests and during real-world use. I don't know much about this incident, but I'm guessing that regulators were able to detect the issue because of (a) <a href=\"https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal#European_discrepancies,_2014\">independent tests</a> under road conditions; (b) being able to understand the \"<a href=\"https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal#EPA_Notice_of_Violation,_2015\">defeat device</a>\" that the Volkswagen cars used to create the misleading readings. \n</p>\n<p>\nNeither of these would necessarily apply to the King Lear problem as described here: (a) you can test a car under road conditions to see its emissions, but you can't (or at least, shouldn't) test how an AI behaves when it has an opportunity to disempower humans entirely; (b) AI systems may be <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#Box4\">black boxes</a> such that it's hard to understand what's going on inside them.--></details>\n<p id=\"block74\"></p>\n<p id=\"block75\">\nIn general, modern machine learning researchers consider it challenging to handle what's called \"distributional shift\": systematic differences between situations AIs were <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj/#making-pasta\">trained</a> on and situations they're now in. To me, the King Lear problem looks like <strong>arguably the most inconvenient possible distributional shift: </strong>AI systems risk behaving in unexpected ways <em>just as</em> (and in fact, because) they're now able to defeat humanity, rather than being in a controlled test environment.\n</p>\n<p id=\"block76\">\nSome lines of research that might help here:\n</p>\n<ul>\n\n<li id=\"block77\">If we could solve the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#The-Lance-Armstrong-Problem\">Lance Armstrong problem</a> robustly enough - such that we could be confident AIs were never behaving deceptively - we could simply prompt AIs to answer questions like \"Would AI system X disempower humans given an opportunity to do so?\" The <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#DigitalNeuroscience\">digital-brain-based</a> techniques noted above could imaginably get us here.\n\n</li><li id=\"block78\">There might be ways of specifically trying to target the <em>worst-case</em> behavior of AI systems, so that they are nearly guaranteed not to behave in certain ways <em>regardless of their situation</em>. This could look something roughly like \"simulating cases where an AI system has an opportunity to disempower humans, and giving it negative reinforcement for choosing to do so.\" More on this sort of approach, along with some preliminary ongoing work, <a href=\"https://www.lesswrong.com/posts/pXLqpguHJzxSjDdx7/why-i-m-excited-about-redwood-research-s-current-project\">here</a>.\n</li>\n</ul>\n<h3 id=\"_3__The_lab_mice_problem__the_AI_systems_we_d_like_to_study_don_t_exist_today_\">(3) The lab mice problem: the AI systems we'd like to study don't exist today </h3>\n<p id=\"block79\"></p><center><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1670529008/mirroredImages/7gkXuHEm6CqEGT2mg/a7kypwypylzimupgrmak.jpg\" width=\"400\" alt=\"AI Safety Seems Hard to Measure\"></center><p id=\"block80\"></p>\n\n<p id=\"block81\">\nAbove, I said: \"when AI systems become capable enough, AI safety research starts to look more like social sciences (studying human beings) than like natural sciences.\" But today, AI systems <em>aren't</em> capable enough, which makes it especially hard to have a meaningful test bed and make meaningful progress.\n</p>\n<p id=\"block82\">\nSpecifically, we don't have much in the way of AI systems that seem to <em>deceive and manipulate</em> their supervisors,<sup id=\"fnref10\"><a href=\"#fn10\" rel=\"footnote\">10</a></sup> the way I worry that <!-- may link to https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#deceiving-and-manipulating ? --> <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">they might when they become capable enough</a>.\n</p>\n<p id=\"block83\">\n<span class=\"blockquote_j9TojqxzSCcah2246_1\">In fact, it's not 100% clear that AI systems could learn to deceive and manipulate supervisors even if we deliberately tried to train them to do it. This makes it hard to even get started on things like discouraging and detecting deceptive behavior.</span> \n</p>\n<p id=\"block84\">\nI think AI safety research is a bit unusual in this respect: most fields of research aren't explicitly about \"solving problems that don't exist yet.\" (Though a lot of research <em>ends up</em> useful for more important problems than the original ones it's studying.) As a result, doing AI safety research today is a bit like <strong>trying to study medicine in humans by experimenting only on lab mice </strong>(no human subjects available).\n</p>\n<p id=\"block85\">\nThis does <em>not</em> mean there's no productive AI safety research to be done! (See the previous sections.) It just means that the research being done today is somewhat analogous to research on lab mice: informative and important up to a point, but only up to a point.\n</p>\n<p id=\"block86\">\nHow bad is this problem? I mean, I do think it's a temporary one: by the time we're facing the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">problems I worry about</a>, we'll be able to study them more directly. The concern is that <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AmxxnazJcBWzWEeqj/#explosive-scientific-and-technological-advancement\">things could be moving very quickly by that point</a>: by the time we have AIs with human-ish capabilities, companies might be furiously making copies of those AIs and using them for all kinds of things (including both AI safety research and further research on making AI systems faster, cheaper and more capable).\n</p>\n<p id=\"block87\">\nSo I do worry about the lab mice problem. And I'd be excited to see more effort on making \"better model organisms\": AI systems that show early versions of the properties we'd most like to study, such as deceiving their supervisors. (I even think it would be worth training AIs specifically to do this;<sup id=\"fnref11\"><a href=\"#fn11\" rel=\"footnote\">11</a></sup> if such behaviors are going to emerge eventually, I think it's best for them to emerge early while there's relatively little risk of AIs' actually <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeating humanity</a>.)\n</p>\n\n<h3 id=\"_4__The__first_contact__problem__how_do_we_prepare_for_a_world_where_AIs_have_capabilities_vastly_beyond_those_of_humans_\">(4) The \"first contact\" problem: how do we prepare for a world where AIs have capabilities vastly beyond those of humans?</h3>\n<p id=\"block88\"></p><center><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1670529008/mirroredImages/7gkXuHEm6CqEGT2mg/fkre1r2utxjhowfwuhio.jpg\" width=\"400\" alt=\"AI Safety Seems Hard to Measure\"></center><p id=\"block89\"></p>\n\n<p id=\"block90\">\nAll of this piece so far has been about trying to make safe \"human-like\" AI systems.\n</p>\n<p id=\"block91\">\nWhat about AI systems with capabilities <em>far</em> beyond humans - what Nick Bostrom calls <a href=\"https://smile.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom-ebook/dp/B00LOOCGB2/\">superintelligent</a> AI systems?\n</p>\n<p id=\"block92\">\nMaybe at some point, AI systems will be able to do things like:\n</p>\n<ul>\n\n<li id=\"block93\">Coordinate with each other incredibly well, such that it's hopeless to use one AI to help supervise another.\n\n</li><li id=\"block94\">Perfectly understand human thinking and behavior, and know exactly what words to say to make us do what they want - so just letting an AI send emails or write tweets gives it vast power over the world.\n\n</li><li id=\"block95\">Manipulate their own \"digital brains,\" so that our <!-- may link to https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#deceiving-and-manipulating ? --> <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#DigitalNeuroscience\">attempts to \"read their minds\"</a> backfire and mislead us.\n\n</li><li id=\"block96\">Reason about the world (that is, <!-- may link to https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#what-it-means-for ? --> <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#what-it-means-for\">make plans to accomplish their aims</a>) in completely different ways from humans, with concepts like \"glooble\"<sup id=\"fnref12\"><a href=\"#fn12\" rel=\"footnote\">12</a></sup> that are incredibly useful ways of thinking about the world but that humans couldn't understand with centuries of effort.\n    </li></ul><p id=\"block97\">\nAt this point, whatever methods we've developed for making human-like AI systems safe, honest, and restricted could fail - and silently, as such AI systems could go from \"behaving in honest and helpful ways\" to \"appearing honest and helpful, while setting up opportunities to <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/\">defeat humanity</a>.\"\n</p>\n<p id=\"block98\">\nSome people think this sort of concern about \"superintelligent\" systems is ridiculous; some<sup id=\"fnref13\"><a href=\"#fn13\" rel=\"footnote\">13</a></sup> seem to consider it extremely likely. I'm not personally sympathetic to having high confidence either way.\n</p>\n<p id=\"block99\">\nBut additionally, a world with huge numbers of human-like AI systems could be strange and foreign and fast-moving enough to have a lot of this quality.\n</p>\n<p id=\"block100\">\nTrying to prepare for futures like these could be like trying to <strong>prepare for first contact with extaterrestrials</strong> - it's hard to have any idea what kinds of challenges we might be dealing with, and the challenges might arise quickly enough that we have little time to learn and adapt.\n</p>\n<h2 id=\"The_young_businessperson\">The young businessperson</h2>\n\n<p id=\"block101\">\nFor one more analogy, I'll return to the one used by Ajeya Cotra <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#analogy-the-young-ceo\">here</a>:\n</p>\n\n    <blockquote id=\"block102\"><p id=\"block103\">Imagine you are an eight-year-old whose parents left you a $1 trillion company and no trusted adult to serve as your guide to the world. You must hire a smart adult to run your company as CEO, handle your life the way that a parent would (e.g. decide your school, where you\u2019ll live, when you need to go to the dentist), and administer your vast wealth (e.g. decide where you\u2019ll invest your money).\n</p>\n<p id=\"block104\">\n\n    You have to hire these grownups based on a work trial or interview you come up with -- you don't get to see any resumes, don't get to do reference checks, etc. Because you're so rich, tons of people apply for all sorts of reasons. (<a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/hCsxvMAGpkEuLCE4E/#analogy-the-young-ceo\">More</a>)</p></blockquote>\n<p id=\"block105\">\nIf your applicants are a mix of \"saints\" (people who genuinely want to help), \"sycophants\" (people who just want to make you happy in the short run, even when this is to your long-term detriment) and \"schemers\" (people who want to siphon off your wealth and power for themselves), how do you - an eight-year-old - tell the difference?\n</p>\n<p id=\"block106\">\nThis analogy combines most of the worries above. \n</p>\n<ul>\n\n<li id=\"block107\">The young businessperson has trouble knowing whether candidates are truthful in interviews, and trouble knowing whether any work trial <em>actually</em> went well or just <em>seemed</em> to go well due to deliberate deception. (The <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#The-Lance-Armstrong-Problem\">Lance Armstrong problem</a>.)\n\n</li><li id=\"block108\">Job candidates could have bad intentions that don't show up until they're in power (the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#The-King-Lear-problem\">King Lear Problem)</a>.\n\n</li><li id=\"block109\">If the young businessperson were trying to prepare for this situation before actually being in charge of the company, they could have a lot of trouble simulating it (the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#The-Lab-mice-problem\">lab mice problem)</a>.\n</li><li id=\"block110\">And it's generally just hard for an eight-year-old to have much grasp <em>at all</em> on the world of adults - to even think about all the things they should be thinking about (the <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/NbiHKTN5QhFFfjjm5#The-first-contact-problem\">first contact problem</a>).\n    \n</li>\n</ul>\n<p id=\"block111\">\nSeems like a tough situation.\n</p>\n<p id=\"block112\">\n<!-- may link to https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/6LTh4foNuC3NdtmZH/ ? --> <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/\">Previously</a>, I talked about the dangers of AI <em>if </em>AI developers don't take specific countermeasures. This piece has tried to give a sense of why, even if they <em>are</em> trying to take countermeasures, doing so could be hard. The next piece will talk about some ways we might succeed anyway.\n</p>\n\n\n<p id=\"block116\"></p><h2 id=\"Footnotes\">Footnotes</h2>\n<div class=\"footnotes\">\n\n<ol><li id=\"fn1\">\n\n<p id=\"block118\">\n     Or persuaded (in a \u201cmind hacking\u201d sense) or whatever.&nbsp;<a href=\"#fnref1\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn2\">\n<p id=\"block120\">\n     Research? Testing. Whatever.&nbsp;<a href=\"#fnref2\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn3\">\n<p id=\"block122\">\n     Drugs can be tested in vitro, then in animals, then in humans. At each stage, we can make relatively straightforward observations about whether the drugs are working, and these are reasonably predictive of how they'll do at the next stage.&nbsp;<a href=\"#fnref3\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn4\">\n<p id=\"block124\">\n     You can generally see how different compounds interact in a controlled environment, before rolling out any sort of large-scale processes or products, and the former will tell you most of what you need to know about the latter.&nbsp;<a href=\"#fnref4\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn5\">\n<p id=\"block126\">\n     New software can be tested by a small number of users before being rolled out to a large number, and the initial tests will probably find most (not all) of the bugs and hiccups.&nbsp;<a href=\"#fnref5\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn6\">\n<p id=\"block128\">\n     Such as:\n</p><ul>\n\n<li id=\"block129\">Being more careful to avoid <a href=\"https://forum.effectivealtruism.org/s/gBjPorwZHRArNSQ5w/p/vGsRdWzwjrFgCXdMn/#deceiving-and-manipulating\">wrong answers that can incentivize deception</a>\n\n</li><li id=\"block130\">Conducting randomized \"audits\" where we try extra hard to figure out the right answer to a question, and give an AI extra negative reinforcement if it gives an answer that we <em>would have</em> believed if not for the audit (this is \"extra negative reinforcement for wrong answers that superficially look right\")\n\n</li><li id=\"block131\">Using methods along the lines of <a href=\"https://openai.com/blog/debate/\">\"AI safety via debate\"</a>&nbsp;<a href=\"#fnref6\" rev=\"footnote\">\u21a9</a></li></ul></li><li id=\"fn7\">\n<p id=\"block133\">\n     Though there are other reasons social sciences are especially hard, such as the fact that there are often big limits to what kinds of experiments are ethical, and the fact that it's often <a href=\"https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd/p/AKxKR4CeakyBsGFoH/#social-science\">hard to make clean comparisons between differing populations</a>.&nbsp;<a href=\"#fnref7\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn8\">\n\n<p id=\"block135\">\n     This paper is from Anthropic, a company that my wife serves as President of.&nbsp;<a href=\"#fnref8\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn9\">\n<p id=\"block137\">\n     Like, he actually asks them to talk about their love for him just before he decides on what share of the realm they'll get. Smh&nbsp;<a href=\"#fnref9\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn10\">\n<p id=\"block139\">\n     <a href=\"https://arxiv.org/pdf/2109.07958.pdf\">This paper</a> is a potential example, but its results <a href=\"https://forum.effectivealtruism.org/posts/xSDWS8yWWPcqAa8NR/ai-alignment-research-links-1/#helpful-honest-harmless\">seem pretty brittle</a>.&nbsp;<a href=\"#fnref10\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn11\">\n<p id=\"block141\">\n     E.g., I think it would be interesting to train AI <a href=\"https://github.com/features/copilot\">coding systems</a> to write <a href=\"http://www.underhanded-c.org/\">underhanded C</a>: code that looks benign to a human inspector, but does unexpected things when run. They could be given negative reinforcement when humans can correctly identify that the code will do unintended things, and positive reinforcement when the code achieves the particular things that humans are attempting to stop. This would be challenging with today's AI systems, but not necessarily impossible.&nbsp;<a href=\"#fnref11\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn12\">\n\n<p id=\"block143\">\n     This is a concept that only I understand.&nbsp;<a href=\"#fnref12\" rev=\"footnote\">\u21a9</a></p></li><li id=\"fn13\">\n<p id=\"block145\">\n     E.g., see the discussion of the \"hard left turn\" <a href=\"https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment\">here</a> by Nate Soares, head of <a href=\"https://intelligence.org/\">MIRI</a>. My impression is that others at MIRI, including Eliezer Yudkowsky, have a similar picture.&nbsp;<a href=\"#fnref13\" rev=\"footnote\">\u21a9</a></p></li></ol></div></div>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "pbe8x4AQDqftQoaT5", "title": "ESPR should return the FTX-funded chateau", "postedAt": "2022-12-10T15:54:48.468Z", "htmlBody": "<p>The <a href=\"https://forum.effectivealtruism.org/topics/european-summer-program-on-rationality\">European Summer Program on Rationality</a><strong> </strong>is an EA-aligned <a href=\"https://espr-camp.org/\">organization</a> that got 107 million crowns (~4,5 million dollar) from FTX which was largely used to <a href=\"https://czechia.postsen.com/trends/77377/Available-Bitcoin-Czech-washing-machine-FTX-and-crypto-as-salvation-A-Week-in-the-Crypt-10.html\">buy a chateau</a>:</p><blockquote><p>The server got some interesting information this week <i>iRozhlas.cz</i>. According to his findings, the FTX Foundation sent 107 million crowns to the Czech Republic for the purchase and operation of a chateau in Hosta\u010dov, a small village in the Havl\u00ed\u010dkobrod region. This happened just four months before the FTX crash.</p><p>The castle was bought by Irena Kot\u00edkov\u00e1, respectively the association European Summer Program on Rationality. Mrs. Kot\u00edkov\u00e1 claims that she applied to FTX for a grant for an international education center and simply received it. On the first attempt. So far, she says she is not afraid that the collapse of FTX could also affect her project at the castle, but she is ready to return the money if she is called upon to do so.</p></blockquote><p>This story is corroborated by two <a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/why-did-cea-buy-wytham-abbey?commentId=kAHmbft6ChRpEserF\">EAF</a> <a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/why-did-cea-buy-wytham-abbey?commentId=7jRvdjHFqbKZ9AdgF\">members</a>.</p><p>The purchase was handled by Irena Kot\u00edkov\u00e1 who was a chairperson at the Czech Association for Effective Altruism. While she seems like a wonderful person I would urge her and ESPR, in light of recent information, to return the money. We already have a <a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/\">controversial estate purchase</a> and it seems SBF admitted to fraud in <a href=\"https://youtu.be/4o_jPzBZSIo\">this interview</a>. He admitted to knowingly commingling funds in a way that directly violates the terms of service, and he admitted that was their general business practice. I don't think we should profit off this.</p><p>I also couldn't find this chateau on any website:</p><p>[EDIT removed addresses at request of Irena. The larger point is that the addresses on the websites of ESPR, The Czech Association for Effective Altruisms and EA Prague don't include this chateau.]</p><p>I also couldn't find it on LW, this forum or on the facebook page.</p><p>Given the recent <a href=\"https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/\">controversy</a> surrounding a lack of purchasing transparency, I think that going forward purchases of this size should be disclosed on the website/forum.</p>", "user": {"username": "bmjacobs@telenet.be"}}, {"_id": "HCbKdZyfxtG7mBeYk", "title": "The Seven Days of EA Xmas (a bad first draft)", "postedAt": "2022-12-10T13:53:53.224Z", "htmlBody": "<p>Tis that time of year again. &nbsp;The holidays are upon us followed by the end of another year. &nbsp;As many take time to celebrate and perhaps reflect on the year, I thought it would be nice to have a ritual to end the year strong. &nbsp;Here is just an initial list of side quests but I hope to get suggestions to improve it -&nbsp;</p><p>&nbsp;</p><p>1) Gratitude practice, give thanks to what is going well and reflect on the small steps taken on this long journey to Doing Good Better &nbsp;</p><p><br>2) Reread a standout EA text or essay (e.g. Singer's Famine and Morality)</p><p><br>3) There are many ways to EA. &nbsp;Review and revise your personal EA philosophy</p><p><br>4) If you are giving, review your charity donations. &nbsp;Any updates to how you want to give and who you want to give to? &nbsp;</p><p><br>5) Check out a new EA aligned organization/newsletter/youtube channel &nbsp;</p><p><br>6) Share some of your thoughts online, in a letter, or maybe on a 1:1 &nbsp;</p><p><br>7) Connect &nbsp;with someone live, strengthen your connection with the EA community</p><p>&nbsp;</p><p>Logistics:</p><ul><li>Quests can be done on seven consecutive days or every other day</li><li>Quests can end on Christmas day or start on Christmas day</li><li>Option to document your seven days on the forum and get a post +1</li></ul><p>Quick Reading list:</p><ul><li>EA Handbook (poke around): <a href=\"https://forum.effectivealtruism.org/handbook\">https://forum.effectivealtruism.org/handbook</a>&nbsp;</li><li>Karnofsky's Most Important Century: <a href=\"https://www.cold-takes.com/most-important-century/\">https://www.cold-takes.com/most-important-century/</a></li><li>Seven Essays on a Better Future: <a href=\"https://forum.effectivealtruism.org/posts/ecfyChvYdRKXCTg5F/7-essays-on-building-a-better-future\">https://forum.effectivealtruism.org/posts/ecfyChvYdRKXCTg5F/7-essays-on-building-a-better-future</a>&nbsp;</li><li>Singer's Famine and Morality: <a href=\"https://forum.effectivealtruism.org/posts/FXSFD6oqBCRwwF2Ea/famine-affluence-and-morality-link-post-to-pdf\">https://forum.effectivealtruism.org/posts/FXSFD6oqBCRwwF2Ea/famine-affluence-and-morality-link-post-to-pdf</a></li><li>&nbsp;</li><li>&nbsp;</li></ul><p>&nbsp;</p><p>Help me:</p><ul><li>Lets keep these side quests to 7, is there anything we should substitute in for this list?</li><li>How might we make the quests clearer?</li><li>Should any quest be broader to be more inclusive?</li></ul>", "user": {"username": "Patrick Liu"}}, {"_id": "NoYK9ye8gzKjvTgdN", "title": "Test Your Knowledge of the Long-Term Future", "postedAt": "2022-12-10T11:01:22.082Z", "htmlBody": "<p>Hey EA Forum!</p><p>I'm excited to announce my new quiz on the <a href=\"https://www.quizmanity.org/long-term-future\">Long-Term Future</a>. Whether you're a space enthusiast or just curious about what the future holds, this quiz is for you.</p><p>Test your knowledge on topics like how long Earth will remain habitable, how many potentially habitable planets are there in our galaxy, and what fraction of observable galaxies we could reach.</p><p>But this quiz isn't just for fun - by taking it, you can spark your imagination and excitement about the future, and gain a deeper understanding of the threats and opportunities that humanity may face.</p><p>So why not give it a try? It only takes 15 minutes, and you can see how your results compare with others. And don't forget to <a href=\"https://docs.google.com/document/d/125MwgMkDYRbBnlJRLONYdhXfUVElShXzdIZr_7-dqMA/edit?usp=sharing\">give feedback</a> - I want to make this quiz the best it can be.</p><p>Ready for liftoff?&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/av3nf5qetjek9rn6w354\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/jhqk6cbnhqfhhvexdkim 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/ebwy6723po2nd8kuciqz 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/o7ibebsxyyikmac3rjxo 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/qroqi7tmdssmmqxxjlog 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/c4pcr6zeu9nllrw2p0qj 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/v9inylewumscfsu2xvqg 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/eqyz9ad8fwd45vfq5mn2 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/gp9obtv0oi0i0thdseqb 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/am1hkuu8kvjgtax3pfan 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/NoYK9ye8gzKjvTgdN/zf8e5olpgu8qrru5a7kf 1440w\"></figure><h3><a href=\"https://www.quizmanity.org/long-term-future\">Launch quiz</a></h3><p>&nbsp;</p><p>Finally, I'd like to share that I partnered with <a href=\"https://programs.clearerthinking.org/long-term_future_quiz.html\">clearerthinking.org</a> to develop this quiz. I want to thank Spencer Greenberg for creating <a href=\"https://www.guidedtrack.com/\">guidedtrack.com</a>, the infrastructure that supports the quiz, and for supporting me from day one.</p>", "user": {"username": "andreferretti"}}, {"_id": "DBaLPBcWyQtY34Kt9", "title": "ChatGPT can write code! ?", "postedAt": "2022-12-10T05:36:46.946Z", "htmlBody": "<h2>Could an artificial general intelligence (AGI) craft computer code and open up possibilities never seen before in the tech world?</h2><p>A few months back, I was wrestling with this idea and decided to dive deep into the work of current researchers, entrepreneurs, journalists and anyone exploring this dynamic topic. Today, I just found out ChatGPT can do this very thing I was worried with.</p><h2>Three videos of coders posting their thoughts on ChatGPT</h2><ol><li><a href=\"https://www.youtube.com/watch?v=BhhI1YdFP7c\">LETTING AN AI WRITE CODE FOR ME!&nbsp; </a>- Advent of code solved by ChatGPT!</li><li><a href=\"https://www.youtube.com/watch?v=yIZqIMNvqjg\">Using AI To Code Better? ChatGPT and Copilot change everything</a> - another Advent of code video trying to be solved by ChatGPT</li><li><a href=\"https://www.youtube.com/watch?v=bDuL_cJvIW4\">ChatGPT - an INSANE AI from OpenAI</a> - It wrote C+ow this is worrying as it can bridge into low level coding (tap into binary code that can speak to hardware....)</li></ol><h2>I'm deeply worried by this</h2><p>The third video is indeed troubling - an AGI that can write code to interact with any type of hardware poses a real threat to our technological control. After all, AI alignment has yet to be fully resolved and when combined with this capability, the risk increases manifold.</p><p>We really need to solve the AI alignment - the faster the better.</p>", "user": {"username": "Miguel"}}, {"_id": "3K2fKB8azNoEiEL9t", "title": "Cooperation, Avoidance, and Indifference: Alternate Futures for Misaligned AGI", "postedAt": "2022-12-10T20:32:34.282Z", "htmlBody": "<p>Concerns about \u201csuper-intelligence\u201d loom large.&nbsp; The worry is that artificial general intelligence (AGI), once developed, might quickly reach a point at which (1) it becomes aware of its own capabilities and (2) it deploys those capabilities to subjugate or eradicate humanity.&nbsp; This is often described as the problem of \u201cpower-seeking, misaligned AGI\u201d (with \u201cmisalignment\u201d referring to divergence between the goals of the humans who create AGI and the endogenous goals of AGI itself).&nbsp;</p><p>Experts who have expressed concern about misaligned AGI rate the odds of catastrophe differently.&nbsp; But a common premise unites their views: that AGI is likely to wield power in a manner adverse to human welfare (Carlsmith 2021).&nbsp; Here, I scrutinize that premise.&nbsp; Inspired by evolutionary biology and game theory, I explore other ways \u2014 apart from subjugation and eradication \u2014 that systems comprised of mutually-powerful agents (or groups of agents) tend to equilibrate.&nbsp; With these patterns in mind, I argue that AGI, even if misaligned, is <i>quite unlikely</i> to subjugate or eradicate humans.&nbsp; Rather, the strong likelihood is that misaligned AGI would do some combination of:</p><p>\u2022&nbsp; Cooperating with us;</p><p>\u2022&nbsp; Purposely avoiding us;&nbsp;</p><p>\u2022&nbsp; Paying us no attention whatsoever.&nbsp;</p><p>Of course, it is possible \u2014 and I hardly mean to deny \u2014 that misaligned AGI <i>could</i> wield power in ways that, for humans, qualify as \u201ccatastrophic.\u201d&nbsp; This could happen either intentionally or incidentally, depending on the goals AGI decided to pursue.&nbsp; The point is simply that \u201ccatastrophic\u201d outcomes, though possible, are far less likely than alternate equilibria.&nbsp; Why?&nbsp; In short, because:&nbsp;</p><p>1.&nbsp; Historically, interactions among agents with misaligned goals \u2014 both across species lines and among human sub-groups \u2014 have frequently resulted in non-catastrophic equilibria; and&nbsp;</p><p>2.&nbsp; There is no reason to think the overall distribution of probabilities would differ in the specific case of humans and AGI. &nbsp;</p><p>In what follows, I bring these claims alive through a combination of real-life examples \u2014 drawn from the natural world \u2014 as well as hypotheticals meant to forecast (if not necessarily to predict) the imminent world of human-AGI interaction.&nbsp;</p><p>*</p><p><strong>Claim one</strong> \u2014 <i>interactions between mutually power-seeking agents with misaligned goals very frequently result in non-catastrophic equilibria</i></p><p>In practice, systems compromised of mutually-powerful agents with divergent goals tend, overwhelmingly, to equilibrate in one of three (non-catastrophic) ways:&nbsp;</p><p>\u2022&nbsp; Mutualism</p><p>\u2022&nbsp; Conflict Avoidance</p><p>\u2022&nbsp; Indifference&nbsp;</p><p>This taxonomy is meant to be answer, of sorts, to Bostrom\u2019s famous \u201cparable of the sparrows,\u201d which imagines a group of sparrows that endeavor to locate an owl chick and train it as their servant.&nbsp; Although Bostrom notes, cheekily, that it is \u201cnot known\u201d how the parable ends, the implication is that things are unlikely to turn out in the sparrows\u2019 favor (Bostrom 2014).&nbsp; And the same is certainly possible with regard to AGI and humans.&nbsp; The analogy Bostrom has in mind \u2014 sparrow : owl :: human : AGI \u2014 may well hold.&nbsp; But many other analogies exist.&nbsp; The question is one of relative likelihood.&nbsp;</p><p><u>Mutualism</u>&nbsp;</p><p>The first form of non-catastrophic equilibrium is mutualism.&nbsp; Mutualist relationships produce net-benefits for all groups or species involved, often through intentional or incidental exchange; each group or species provides something of value to the other, leading to multilateral incentives for cooperation (Hale et al. 2020).&nbsp;</p><p>Often, mutualism occurs among groups or species that pose no threat to each other.&nbsp; For instance, we exist in a mutualist relationship with the bacteria that constitute our \u201cgut flora.\u201d&nbsp; The bacteria help regulate digestion, and we, in turn, provide them (luxury?) accommodation. &nbsp;</p><p>But mutualism can also occur among groups or species that <i>do</i> pose natural threats to each other \u2014 when the benefits of mutualism simply outweigh the risks of threat.&nbsp; Here is a good example: zoologists have observed that gelada monkeys allow Ethiopian wolves to roam freely in the vicinity of their young, though the latter are easy prey.&nbsp; Why?&nbsp; The reigning hypothesis is that wolves provide the baboons protection from other predators, and the baboons help the wolves locate rodents \u2014 an easier source of food.&nbsp; In light of this, the wolves have learned to leave the young baboons alone (Holmes 2015).</p><p>The power differential between wolves and monkeys is relatively small.&nbsp; But this is not a necessary feature of inter-species mutualism.&nbsp; It can also transpire in settings where one species is <i>vastly</i> more powerful than the other.&nbsp; Take, for example, bees and humans.&nbsp; Bees can be a source of nuisance (and even, in some contexts, a more serious threat), and we certainly have the capacity to eradicate bees if we thought it worth our time and energy.&nbsp; But we have no incentive to do so.&nbsp; In fact \u2014 now that we understand pollination \u2014 we have an active incentive to keep bees alive and flourishing, and even to protect them from <i>other</i> threats, purely as a matter of self-interest.&nbsp;</p><p>Given this, consider the following thought-experiment: a \u201cparable of the bees\u201d on similar footing with that of the sparrows.&nbsp; It\u2019s 10,000 BC, and certain members of the bee race \u2014 the Bee Intelligentsia \u2014 are worried about the growing capabilities of homo sapiens.&nbsp; They begin (doing the equivalent of) writing papers that frame the concern as follows:&nbsp;</p><p><i>Once homo sapiens turn their sights to reshaping the external environment, they will be more powerful than we can imagine, and there is a non-negligible chance that they will pursue goals either directly or incidentally adverse to our welfare \u2014 maybe catastrophically so.&nbsp; Accordingly, we should begin expending significant amounts of collective energy future-proofing against subjugation or eradication by homo sapiens.</i></p><p>Would the Bee Intelligentsia be \u201cwrong\u201d to think this way?&nbsp; Not exactly, for the core submission is true; human activity does raise <i>some risk</i> of catastrophe.&nbsp; The error would lie in over-weighting the risk.&nbsp; (Which is easy to do, especially when the risk in question is terrifying.)&nbsp; But if the Bee Intelligentsia understood pollination \u2014 if it were intelligent, let alone super-intelligent \u2014 it would be able to appreciate the possibility that bees offer humans a benefit that is not trivial to replace.&nbsp; Indeed, it might even be able to predict (some version of) the present-day dynamic, namely, that far from undermining bee welfare, humans have an active incentive to enhance it.</p><p>The same may be true of humans and AGI \u2014 with humans in the \u201cbee\u201d position.&nbsp; Depending on its goals, AGI may well conclude that humans are worth keeping around, or even worth nurturing, for the mutualist benefits they deliver.&nbsp; In fact, AGI could simply conclude that humans may deliver mutualist benefits <i>at some point in the future</i>, and this, alone, might be enough to inspire non-predation \u2014 as in the wolf-baboon example \u2014 or cultivation \u2014 as in the human-bee example \u2014 purely as a means of maintaining optionality.&nbsp; One assumption built into the \u201csuper-intelligence\u201d problem, after all, is that AGI will be capable of developing functionalist (and possibly causal) theories of their world \u2014 to an equal, if not vastly greater, extent than humans have.&nbsp; From this, it follows that AGI would likely have an enormous set of mutualist dynamics to explore (and to think about safeguarding for the future) before picking out hostility as the most rational stance to adopt toward humanity.&nbsp;</p><p>Some versions of AGI \u201csafeguarding mutualism\u201d would likely resemble domestication; the way AGI would invest in the possibility of humans delivering mutualist benefits would be \u2014 in some measure \u2014 to \u201cfarm\u201d us.&nbsp; (Researchers already use language like this when describing the wolf-baboon example.) &nbsp;</p><p>In this context, \u201cdomestication\u201d may sound jarring.&nbsp; But \u2014 crucially \u2014 that is not the same as catastrophic.&nbsp; In fact, one can easily imagine \u201chuman domestication\u201d scenarios that enable greater flourishing than we have been able to manage, or plausibly could manage, on our own.&nbsp; Consider, for example, if domestication has been catastrophic for a species like bengal cats.&nbsp; At their limit, questions like this may be more metaphysical than empirical; they may ride on deep (and likely non-falsifiable) conceptions of what flourishing involves and requires.&nbsp; But at a practical level, for many species, like bengal cats, it would seem odd to describe domestication as catastrophic.&nbsp; Domestication has effectively relieved bengal cats of the need to constantly spend energy looking for food.&nbsp; Whatever drawbacks this has also occasioned (do bengal cats experience ennui?), it seems like a major improvement, and certainly not a catastrophic deprivation, as such.&nbsp;</p><p><u>Conflict Avoidance</u></p><p>The second form of non-catastrophic equilibrium is conflict avoidance.&nbsp; This involves relationships of threat or competition in which one or more parties deems it easier \u2014 more utility-maximizing overall \u2014 to simply avoid the other(s).&nbsp; For example, jellyfish are a threat to human beings, and human beings are a threat to jellyfish.&nbsp; But the global \u201cequilibrium\u201d between the two species is avoidant, not hopelessly (or catastrophically) conflictual.&nbsp; If, circa 10,000 BC, the Jellyfish Intelligentsia voiced concerns analogous to those of the Bee Intelligentsia above, they, too, would have had little reason to worry.&nbsp; In the course of history, humans may well have pursued the subjugation or eradication of jellyfish; and we still might.&nbsp; The most likely equilibrium, however, is one in which humans mostly just leave jellyfish alone \u2014 and vice versa.</p><p>Importantly, to qualify as non-catastrophic, an \u201cavoidant\u201d equilibrium need not involve <i>zero</i> conflict.&nbsp; Rather, the key property is that conflict does not tend to multiply or escalate because, in the median case, the marginal cost of conflict is greater than the marginal cost of avoidance.&nbsp; Take the jellyfish example above.&nbsp; Sometimes jellyfish harm humans, and sometimes humans harm jellyfish.&nbsp; What makes the equilibrium between them avoidant is not a total absence of conflict; it\u2019s that humans generally find it less costly to avoid jellyfish (by swimming away, say, or by changing the location of a diving expedition) than to confront them.&nbsp; We certainly <i>could</i> eradicate \u2014 or come very close to eradicating \u2014 jellyfish if that were an overriding priority.&nbsp; But it\u2019s not; nor is it likely to become one.&nbsp; Our energy is better spent elsewhere. &nbsp;</p><p>Similar dynamics transpire at the intra-species level.&nbsp; Many human subcultures, for example, are marked by dynamics of reciprocal threat and even mutual predation \u2014 think, say, of organized crime, or competition among large companies in the same economic sector.&nbsp; Yet here, too, avoidance is far more prevalent than subjugation or eradication.&nbsp; Commodity distribution organizations, licit and illicit alike, do not tend to burn resources trying to destroy one another \u2014 at least, not when they can use the same resources to locate new markets, improve their products, or lower production costs.&nbsp; In practice, these strategies are almost always less costly and/or more beneficial than their destructive counterparts.&nbsp;</p><p>At a high level of generality, the appeal of avoidance, relative to escalating conflict, is not hard to see.&nbsp; Even when one group possesses the in-principle <i>capability</i> to destroy another, destructive strategies typically become more costly to keep pursuing the more they have already been pursued; up until the point of completion, the marginal cost of maintaining a destructive strategy tends to increase exponentially.&nbsp; Why?&nbsp; Because counter-parties tend to respond to destructive strategies adaptively, and often in ways that impose costs in the other direction.&nbsp; Retaliation and subversion are the two most common examples.&nbsp; The history of human conflict suggests that small, less powerful groups \u2014 in some cases, <i>vastly</i> less powerful groups \u2014 are capable of inflicting significant harm on their larger, more powerful counterparts.&nbsp; When humans (and other intelligent animals) find themselves in desperate circumstances, the combination of survival instinct, tenacity, and ingenuity can result in extraordinarily outsized per capita power. &nbsp;</p><p>This is not always true; sometimes, small, less powerful groups get decimated.&nbsp; The point, however, is that the <i>possibility</i> of small groups wielding outsized per capita power often suffices to make avoidance a more appealing ex ante strategy.&nbsp; Anticipating that destruction may be costly to accomplish, powerful groups often opt \u2014 as with territorial disputes between criminal and corporate organizations \u2014 for some combination of (1) investing in the creation of new surplus and (2) informally splitting up existing surplus without resorting to (catastrophic forms of) conflict (Peltzman et al. 1995).&nbsp;</p><p>There is reason, accordingly, to think that even if AGI found itself in potential conflict with humans \u2014 e.g., due to competition for the same resources \u2014 the most efficient response could be some combination of (1) creating new mechanisms for amassing the resource, or (2) finding ways to share the resource, even amid conflict.&nbsp; Imagine, for instance, if AGI determined that it was important to secure its own sources of energy.&nbsp; Would the answer be, as some have hypothesized, to seize control of all electricity-infrastructure?&nbsp; (Carlsmith 2021)&nbsp; Possibly; but it\u2019s also possible that AGI would simply devise a better of means of collecting energy, or that it would realize its longer-term interests were best served by allowing us to maintain joint access to existing energy sources \u2014 if nothing else, for appeasement purposes, to avoid the attrition costs of ongoing conflict.&nbsp;</p><p><u>Indifference</u></p><p>The third form of non-catastrophic equilibrium \u2014 surely the most pervasive, considering the sheer multitude of inter-species relationships that exist on earth \u2014 is indifference.&nbsp; Most underwater life, for example, is beyond human concern.&nbsp; We do not \u201cavoid\u201d plankton, in the sense just described.&nbsp; We simply pay them no mind.&nbsp; If the Plankton Intelligentsia voiced concern on par with the Bee Intelligentsia or the Jellyfish Intelligentsia, they, too, would not be \u201cwrong,\u201d exactly.&nbsp; But they would also be foolish to attribute much significance to \u2014 or to organize their productive capacities around \u2014 the risk of human-led subjugation or eradication. &nbsp;</p><p>The same could easily be true of AGI.&nbsp; In fact, the plankton analogy may well prove the most felicitous.&nbsp; If, as the super-intelligence problem hypothesizes, AGI ends up possessing vastly greater capability than humans, it stands to reason that AGI may relate to us in roughly the same way that we relate to other species of vastly lesser capability.&nbsp; And how is that?&nbsp; As a general matter, by paying them little or no attention.&nbsp; This is not true of <i>every</i> such species; bees have already supplied a counter-example.&nbsp; But the general claim holds.&nbsp; With respect to the most species, most of the time, we have no conscious interactions at all. &nbsp;</p><p>Of course, indifference is not always innocuous.&nbsp; In certain guises, it can be highly destructive: if the goals of the powerful-but-indifferent group come into collision with the welfare of the less powerful group.&nbsp; For example, humans have been \u201cindifferent\u201d \u2014 in the sense described above \u2014 to many species of rainforest plants and animals, and the latter are considerably worse off for it.&nbsp; With this category, the important point is that catastrophic results, when they occur, do so <i>incidentally</i>.&nbsp; Catastrophe is not the goal; it is a mere collateral consequence (Yudlowsky 2007). &nbsp;</p><p>How, then, are humans likely to fare under the \u201cindifference\u201d model?&nbsp; It would depend entirely on the goals AGI decided to pursue.&nbsp; Some goals would likely ravage us.&nbsp; Suppose AGI decided that, in the interest of (say) astrophysical experimentation, one of its overriding objectives was to turn planet earth into a perfect sphere.&nbsp; In that case, human civilization may be doomed.&nbsp; But other goals would leave human social order \u2014 and human welfare, such as it is \u2014 effectively unaltered.&nbsp; If, for example, AGI decided the best use of its energy was to create and appreciate art of its own style, or to exhaustively master the game of Go, or anything else along such aesthetic lines, human civilization would be unlikely to register much, if any, effect.&nbsp; In fact, we might not even be aware of such happenings \u2014 in roughly the same sense that plankton are not aware of human trifles.&nbsp;</p><p>*</p><p><strong>Claim two</strong> \u2014 <i>there is no reason to think AGI-human interactions will break from past patterns of equilibration</i></p><p>The goal of the last section was to show that, across a wide range of inter- and intra-species interactions, non-catastrophic equilibria are common.&nbsp; They are not inevitable.&nbsp; But they are prevalent \u2014 indeed, hyper-prevalent \u2014 once we broaden the scope of analysis.&nbsp; (For instance, there are vastly more oceanic species with which humans exist in \u201cavoidant\u201d or \u201cindifferent\u201d equilibria than the total number of mammalian species humans have ever come into contact with.)&nbsp; This does not mean, of course, that catastrophic outcomes are nonexistent \u2014 just that, to date, they make up a small, even negligible, share of the overall distribution of inter- and intra-species interactions in the natural world.&nbsp;</p><p>The next question is whether the same distributional dynamics \u2014 catastrophic outcomes dwarfed by non-catastrophic equilibria \u2014 would apply to the specific case of humans and AGI.&nbsp; I believe so, for two reasons: (1) we have seen no empirical evidence to the contrary, and (2) the only plausible dimension along which the human-AGI case would differ from historical parallels \u2014 namely, AGI\u2019s enhanced capability \u2014 supplies no reason, in principle, to think that catastrophic outcomes would become more likely, relative to their non-catastrophic counterparts. &nbsp;</p><p>To be sure, AGI\u2019s enhanced capability may change the type or quality of outcomes, both catastrophic and non-catastrophic, that define human-AGI interactions.&nbsp; What they are unlikely to change (or, at any rate, what we have no a priori reason to think they will change) is the <i>distribution</i> of those outcomes.&nbsp; Why?&nbsp; In short, because any changes in capability that increase the danger of catastrophe are equally likely, for the same reason, to create new horizons of mutualism and avoidance \u2014 leaving the overall distribution of possibilities, in principle, unchanged.&nbsp;</p><p>To begin with, the empirical evidence is easily summarized.&nbsp; So far, AI systems that have shown signs of adaptive capability seem, uniformly, to follow familiar patterns of strategic decision-making.&nbsp; Under current technological conditions, adaptive AI tends to proceed exactly as one might think \u2014 by crafting and revising plans in response to the functional goals of the environment in which it is deployed (Carlsmith 2021).&nbsp; In other words, they do exactly what one would expect <i>any</i> agent to do: grasping the parameters of the problem-space and deploying familiar \u2014 roughly predictable \u2014 strategies in response.&nbsp;</p><p>But the stronger argument in favor of the \u201cAGI is different\u201d position is conceptual, not empirical.&nbsp; The idea is that because AGI may have capabilities that exponentially outstrip those of humans and other biological agents, it is difficult, if not incoherent, to compare the interactions of biological agents to potential dynamics of human-AGI interaction.&nbsp; In particular, the worry is that AGI might be exceptionally capable at eradicating or subjugating humans (or other animal species ), and \u2014 the thought continues \u2014 if AGI comes to <i>possess</i> that capability, it will be more likely, as a consequence, to <i>exercise</i> that capability. &nbsp;</p><p>This logic invites two forms of rejoinder.&nbsp; The first scrutinizes the claim\u2019s implicit assumptions about AGI\u2019s intentions.&nbsp; In short, even granting the possibility that AGI comes to possess the capability to eradicate or subjugate the human race, it may, nonetheless, lack the <i>desire or will</i> to carry out those goals \u2014 mitigating, or even eliminating, their danger in practice.&nbsp; Some commentators, for example, have wondered whether the training environments in which AGI systems develop will actually <i>conduce</i> to aggressive tendencies, insofar as they those environments typically involve little or no competition for resources, exponentially faster time-horizons for adaptation, and so forth (Garfinkel 2022). &nbsp;</p><p>This first rejoinder \u2014 related to AGI\u2019s intentionality \u2014 could well be sound.&nbsp; After all, if the prevalence of \u201cindifference\u201d dynamics in the natural world is any indication, the overwhelming likelihood, as a statistical matter, may be that AGI pays us no regard whatsoever.&nbsp; It is possible, in other words, that the default paradigm here is (something like) humans and plankton \u2014 not humans and bees, or humans and other advanced mammals.&nbsp; If anything, the plankton paradigm only becomes more plausible the more advanced \u2014 and alienated from us \u2014 we imagine AGI\u2019s capabilities to be.&nbsp;</p><p>Of course, indifference does not always mean long-term harmony.&nbsp; As noted above, of all the species that human activity has eradicated (or that it soon threatens to eradicate), many are species with whom we exist \u201cindifferently.\u201d&nbsp; The eradication of such species is collateral damage: the incidental effect of our activity, often beyond conscious regard.&nbsp; The same may be true, in reverse, of AGI.&nbsp; Human welfare could easily become the collateral damage of whatever scientific, aesthetic, and other goals \u2014 totally unrelated to us \u2014 AGI decided to pursue. &nbsp;</p><p>But there is also second rejoinder to the idea that AGI is likely to pursue the eradication or subjugation of humanity \u2014 which is fortunate, since speculation about AGI\u2019s intentionality (or lack thereof) seems like a slender reed on which to premise humanity\u2019s future, especially given how little we know about our <i>own</i> intentionality.&nbsp; The second rejoinder runs as follows: even assuming (1) that AGI comes to possess the capability to eradicate or subjugate humanity; and, further, (2) that AGI has the will (or at least does not entirely lack the will) to pursue those goals, it does not follow that AGI <i>will</i> pursue those goals.&nbsp; Rather, the question is whether the pursuit of those goals, relative to all the other goals AGI might pursue, would be utility-maximizing. &nbsp;</p><p>There is good reason, I think, to operate under the assumption that the answer to this question \u2014 would powerful, misaligned AGI actually pursue the eradication or subjugation of humanity? \u2014 is no.&nbsp; Or, to put it more precisely, there is good reason to assume that the answer to this question is <i>no more likely to be yes</i> than the likelihood of catastrophic outcomes resulting, in general, from interactions between mutually-powerful agents in the natural world.&nbsp; The reason is this: the same enhancements of capability that would enable AGI to eradicate or subjugate humanity would also enable AGI to avoid or cooperate with humanity, and there is no reason, in principle, to prioritize one set of outcomes over the other.</p><p>Start with avoidance.&nbsp; Although we do not know (by hypothesis) <i>which</i> goals AGI will pursue, it is easy to imagine goals that conflict, in some important measure, with human activity \u2014 leading AGI to view us as competitors or threats.&nbsp; The question is how AGI would act on that view.&nbsp; Would it try to eradicate or subjugate us?&nbsp; And more to the point: would AGI\u2019s enhanced capability make those catastrophic results more likely than they would be in the (already familiar) context of biological agents beset by conflict?&nbsp; No \u2014 at least, not in the abstract.&nbsp; AGI certainly <i>could</i> take steps to eliminate human threat.&nbsp; But it could also \u2014 equally \u2014 take steps to avoid that threat.&nbsp; And the route it chooses would depend, not on AGI\u2019s capability per se, but rather on an analysis of relative efficiency.&nbsp; In other words, the question would be which route, elimination or avoidance, is cheaper \u2014 and enhanced capability would make <i>both</i> routes cheaper.&nbsp; So that variable, by itself, cannot answer the relative efficiency question.&nbsp; Rather, it begs that question.</p><p>Consider the following thought-experiment.&nbsp; Humans spontaneously leap forward a few centuries of technological prowess, such that we now have the capability to instantly kill whole schools of jellyfish using electromagnetic energy.&nbsp; Would we use this technology to eradicate jellyfish across the board? &nbsp;</p><p>Maybe \u2014 but it would depend entirely on what other avoidance strategies the technology <i>also</i> enabled.&nbsp; If the same technology allowed individual human divers to kill specific jellyfish they happened to encounter, that solution (i.e., dealing with individual jellyfish on an ad hoc basis) would likely be preferable to large-scale eradication.&nbsp; Similarly, if the jellyfish grew to recognize that humans possess the capability to kill them relatively easily, they might start trying to <i>avoid us</i> \u2014 an \u201cavoidant\u201d equilibrium in its own right.&nbsp; Of course, we still might decide that eradicating jellyfish is worth it.&nbsp; The point is not that eradication is an impossible, or even an utterly implausible, end-state.&nbsp; The point is that enhanced capability is not the determinative variable.&nbsp; In fact, the biological world is replete with examples \u2014 of the human-jellyfish flavor \u2014 in which agents of vastly greater capability decide, under a relative efficiency analysis, that avoiding threats is more efficient than attempting to eliminate them.&nbsp;</p><p>Here, an important caveat bears noting.&nbsp; Namely, equilibria of an essentially \u201cavoidant\u201d nature \u2014 in which one or multiple agents decides that avoiding conflict is more efficient than trying to eliminate threats \u2014 can still be highly destructive.&nbsp; What distinguishes \u201cavoidance\u201d from eradication is not a total absence of conflict or violence; it is that the conflict tends not to escalate to catastrophic levels.&nbsp; Think, for example, of familiar dynamics of competition between criminal organizations \u2014 such as gangs and cartels.&nbsp; The interface between these groups is often marked by ongoing anti-social conduct; periods of stability are typically ephemeral, and violence and terror are often the norm.&nbsp; Nevertheless, the overall result is rarely <i>catastrophic</i> in the sense of one group being subject to total eradication or subjugation at another\u2019s hand.&nbsp; Instead, the overall result is equilibrium, defined at the margin by unpredictable \u2014 but fundamentally stable \u2014 push and pull.&nbsp; (The same is true of, say, the \u201cavoidant\u201d interface between humans and mosquitos.&nbsp; In the aggregate, it entails the death of many, many mosquitos \u2014 but it is nowhere near \u201ccatastrophic\u201d for mosquitos as a class.)</p><p>An equivalent analysis applies to mutualism.&nbsp; If AGI came to view humans as threats or competitors, not only would it consider \u2014 per above \u2014 the relative efficiency of avoiding us; it would also consider the relative efficiency of <i>cooperating</i> with us.&nbsp; Furthermore, as with avoidance, enhanced capability would enable new strategies for cooperation, even as it also enables new means of eradication.&nbsp; In fact, cooperation is likely the dimension along which enhanced capability is poised to make the greatest difference \u2014 since, historically, the greatest bottlenecks on inter-species cooperation have been (1) the difficulty of identifying opportunities for cooperative enterprise and (2) the impossibility of felicitous enough communication to effectuate those opportunities, once identified.&nbsp;</p><p>Consider, for instance, Katja Grace\u2019s example of humans and ants: why, she asks, have the two species declined historically to develop joint enterprise?&nbsp; Ultimately, the reason is almost certainly <i>not</i> that the two species have nothing to offer one another.&nbsp; Rather, the reason we have not developed joint enterprise with ants is the impossibility of effective communication.&nbsp; If we <i>could</i> communicate with ants, there are many tasks for which we might gladly compensate ant-labor \u2014 for example, navigating hazardously small spaces, or performing inconspicuous surveillance (Grace 2023).&nbsp; That such tasks exist does not, of course, entail that their pursuit would prove mutually beneficial.&nbsp; Certain tasks might be too costly to ants at a physical level; others might offend their dignity; still others might simply command too great a premium (we wouldn\u2019t be able to afford it!).&nbsp; The general trend, however, is that when parties are capable of (1) performing tasks of reciprocal benefit to one another and (2) communicating effectively, they locate avenues of cooperation.</p><p>What might human-AGI mutualism involve?&nbsp; Although we can expect AGI (again, by hypothesis) to have insight into this question that may transcend our own, a number of routes seem plausible.&nbsp; One would involve AGI capitalizing on our sensory capabilities, rather than trying to \u201creinvent the wheel.\u201d&nbsp; Imagine, for instance, if AGI discerned a natural resource \u2014 say, a rare species of orchid \u2014 that it wished to amass.&nbsp; What would it do?&nbsp; There are a few possibilities.&nbsp; One is that it could build a small army of orchid-hunting robots, fit for dispatching all over the world.&nbsp; Another is that it could enlist humans to do all the labor (traveling, searching, extracting, etc.).&nbsp; A third would involve splitting-the-difference: with, say, AGI performing the \u201csearch\u201d function, and humans performing the \u201cextract\u201d function. &nbsp;</p><p>The orchid example is stylized, of course, but the core point \u2014 that AGI would face ongoing tradeoffs around which sensory functions to insource and which to outsource \u2014 is likely to generalize, at least on the assumption that AGI cares whatsoever about our sensory world.&nbsp; What is more, even if AGI took the \u201cinsource\u201d route, it may still have (potentially enormous) use for human labor dedicated to training robotic systems, in much the same way that human laborers are <i>currently</i> being deployed \u2014 by other humans \u2014 to train their own replacements.</p><p>The training model of AGI-human mutualism could have other applications as well.&nbsp; Imagine, for example, if AGI decided it wanted to develop a sense of affect \u2014 emotional or moral interiority \u2014 and it wished to \u201clearn\u201d these treats from human coaches.&nbsp; Or, likewise, suppose AGI decided it wanted to attain aesthetic sensibilities, in hopes of replicating various modes of pleasure \u2014 artistic, athletic, culinary, and so on \u2014 that it had discerned among human beings.&nbsp; All of these endeavors (and innumerably more) would leave ample room, at least in principle, for human contribution to AGI enterprise.&nbsp; And incentives toward mutualism would naturally follow suit.&nbsp;</p><p>To sum up \u2014 at the risk of redundancy \u2014 let me be clear about what I am (and am not) arguing about enhanced capability.&nbsp; The claim is not that enhanced capability <i>necessarily will</i> result in an identical distribution of probabilities, or an identical level of catastrophic risk, for human-AGI interaction, relative to historical inter-species patterns.&nbsp; The claim is more modest.&nbsp; It is (1) that nothing about the fact of enhanced capability, on its own, supplies grounds to think that novel forms of catastrophe will be more likely than novel forms of non-catastrophic equilibria, and (2) that absent such grounds, the most rational assumption is that AGI-human interactions will track, not deviate from, historical patterns of equilibration.&nbsp;</p><p>*</p><p><strong>Conclusion</strong></p><p>If all the foregoing is \u2014 at least roughly \u2014 correct, how should it impact our overall assessment of the \u201ccatastrophe risk\u201d associated with powerful, misaligned AGI?&nbsp; I hesitate to conjure specific probabilities associated with cooperative, avoidant, and indifferent end-states for human-AGI interactions.&nbsp; But it seems safe, at a minimum, to say that any chain of probabilities that aspires to capture the overall likelihood of catastrophe is missing something crucial if it focuses exclusively on (1) whether powerful, misaligned AGI is likely to emerge; and, if so, (2) whether we are likely to retain (or develop) the ability to counteract AGI\u2019s catastrophic goals.&nbsp; These two variables have, perhaps understandably, received the lion\u2019s share of attention to date (Grace 2022; Carlsmith 2021).&nbsp; A full account, however, requires thinking carefully about what AGI would actually do with its misaligned power. &nbsp;</p><p>Could misaligned AGI pursue goals deliberately adverse to human welfare? &nbsp; Sure.&nbsp; But it could also cooperate with us, avoid us, or ignore us.&nbsp; The history of inter-species interaction abounds with examples of these latter dynamics, even amid \u2014 sometimes <i>especially</i> amid \u2014 conditions of threat and competition.&nbsp; If that history is any guide, as I have argued it should be, the most plausible endgame for AGI-human interaction is not catastrophe.&nbsp; It is equilibrium. &nbsp;</p><p>*</p><p><strong>Acknowledgments</strong></p><p>Thanks are due to Jill Anderson, Thomas Brennan-Marquez, Brendan Maher, Nathaniel Moyer, Eric Seubert, Peter Siegelman, Carly Zubrzycki, and Jackie Zubrzycki for helping refine the arguments in this essay.&nbsp;<br>&nbsp;</p><p><strong>Bibliography</strong></p><p>Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (2014)</p><p>Joseph Carlsmith, Is Power-Seeking AI an Existential Risk? (2021)</p><p>Ben Garfinkel, Review of \u2018Carlsmith, Is Power-Seeking AI an Existential Risk?\u2019 (2022)</p><p>Katja Grace, Counterarguments to the Basic AI Risk Case (2022)</p><p>Katja Grace, We Don\u2019t Trade With Ants: AI\u2019s Relationship With Us Will Not Be Like Our Relationship With Ants (2023)</p><p>Kayla Hale et al., Mutualism Increases Diversity, Stability, and Function in Multiplex Networks that Integrate Pollinators Into Food Webs (2020) &nbsp;</p><p>Bob Holmes, Monkeys\u2019 Cozy Alliance with Wolves Looks Like Domestication (2015)&nbsp;</p><p>Holden Karnofsky, Thoughts on the Singularity Institute (2012)</p><p>Sam Peltzman et al., The Economics of Organized Crime (1995)</p><p>Phillip Pettit, Republicanism: A Theory of Freedom and Government (1997)&nbsp;</p><p>Nate Soares, Comments on \u2018Is Power-Seeking AI an Existential Risk?\u2019 (2021)&nbsp;</p><p>Eliezer Yudkowsky, Artificial Intelligence as a Positive and Negative Factor in Global Risk (2008)&nbsp;</p><p>Eliezer Yudkowsky, The Hidden Complexity of Wishes (2007)&nbsp;</p><p>Eliezer Yudkowsky, Coherent Extrapolated Volition (2004)&nbsp;</p>", "user": {"username": "Kiel Brennan-Marquez"}}, {"_id": "nFka6iE9CsuGp9wc4", "title": "Predoc opening at MIT Sloan for improving human collectives", "postedAt": "2022-12-10T02:42:45.956Z", "htmlBody": "<p>Hello EA forum,&nbsp;</p><p>I'm a faculty at MIT Sloan Sloan of Management, and I'm hiring a predoc (someone who's finished their undergraduate degree, but not yet in a PhD program) to work with me on projects related to improving human collectives using mathematical modeling and data analysis skills. This is a very broad area, some projects related to this area include understanding the trends of increasing regulation in organizations and society, and studying conditions under which collective decision-making reaches certain outcomes.&nbsp;</p><p>I think improving the functioning of human collectives is highly relevant to mitigating global catastrophic risks. I detail this perspective in the article \"<a href=\"https://arxiv.org/abs/2205.03300\">Collective Intelligence as Infrastructure for Reducing Broad Global Catastrophic Risks</a>\".&nbsp;</p><p>The detailed job posting is here: <a href=\"https://apply.interfolio.com/117874\">https://apply.interfolio.com/117874</a></p><p>The start date of the position is flexible between February and Sept 2023.&nbsp;</p><p>You can find out more about me on <a href=\"http://www.vcyang.com/\">my website</a> and <a href=\"https://twitter.com/VickyCYang\">Twitter</a>.&nbsp;</p>", "user": {"username": "vickyCYang"}}, {"_id": "ihYFy5kmeEgLT8opY", "title": "Personal Finance for EAs", "postedAt": "2022-12-10T21:48:50.199Z", "htmlBody": "<p>After attending EAGxBerkeley last weekend, I'm thinking there needs to be significantly more emphasis on the basics of personal finance, particularly for student groups and other young EAs. &nbsp;</p><p>&nbsp;</p><p><strong>Rationale</strong></p><p>It's in the best interest of the community to raise the overall level of financial literacy. &nbsp;This has the benefit of increasing the amount of money we can move as a collective. &nbsp;Plus, it avoids the serious risk of an individual overtaxing their finances by failing to set up a rainy day strategy or implement another common-sense personal finance best practice.</p><p>EAs have different financial needs than the general population. &nbsp;Balancing giving and saving is much less stressful if you have a reasoned plan about how to do so. &nbsp; I imagine it'd be pretty easy to get bad advice from a financial counselor who isn't familiar with philanthropic giving.</p><p>&nbsp;</p><p><strong>Context</strong></p><p>At the <a href=\"https://subscribepage.io/eaSoftwareEngineers\">EA Software Engineering</a> meetup at EAGxBerkeley, the topic of earning to give came up, so we talked about some slightly advanced financial strategies. &nbsp;While some people were nodding their heads, others were scribbling frantic notes, which seemed to indicate the concepts were perhaps new to them.</p><p>Throughout the weekend, I had a couple conversations that revealed a lack of understanding of basic personal finance. &nbsp;This post is absolutely not intended to call anyone out. &nbsp;But before graduation, you really should have a solid understanding of checking accounts versus long term investments. &nbsp;</p><p>&nbsp;</p><p><strong>Implementation</strong></p><p>I feel pretty strongly the next EAGx would benefit from a talk on personal finance, outlining the best strategies for that particular country's idiosyncratic financial rules.</p><p>For example, the first 80% of this presentation would walk through commonly cited strategies for setting up checking and brokerage accounts, investing for retirement, maxing out one's 401k, handling stock options, etc. &nbsp;<a href=\"https://www.aaronhamlin.com/articles/practical-philanthropy-effective-altruism\">Here's a useful article that breaks down saving and giving into stages</a>.</p><p>Given that EAs should be open to the idea of <a href=\"https://forum.effectivealtruism.org/posts/w9ENDad268PT9KnWn/thoughts-on-personal-finance-for-effective-altruists#Spend_money_to_save_time\">spending money to save time</a>, in the following 10% of the presentation, we would provide a framework for how to think about this, based on <a href=\"https://programs.clearerthinking.org/what_is_your_time_really_worth_to_you.html#.Y5PPJezMJhF\">Clearer Thinking's Value of Your Time Calculator</a>. &nbsp;</p><p>Then the remaining 10% would talk about <a href=\"https://www.aaronhamlin.com/s/Technical-Giving-Presenation.pdf\">strategies</a> specific to maximizing donations. As far as I know, in the US, using a <a href=\"https://A Comparison of Donor-Advised Fund Providers\">donor advised fund (DAF)&nbsp;</a>to donate appreciated assets is the best strategy for tax advantaged giving. &nbsp;<a href=\"https://www.aaronhamlin.com/articles/planned-giving-for-everyone\">Here are a bunch more ideas</a>.</p><p>So long as we caveat with \"this is not financial advice\" and don't specify which stocks attendees should purchase, my understanding is that this kind of financial presentation would be okay content for an event. &nbsp;Please correct me if this is not correct.</p><p>Our community has the fun Facebook Group <a href=\"https://www.facebook.com/groups/4046231355400586/\">Highly Speculative EA Capital Accumulation</a> full of complex (/ questionable?) strategies. &nbsp;I think we need to cover the other end of the spectrum \u2014 i.e., personal finance basics \u2014 a little better.</p><p>&nbsp;</p><p><strong>Summary</strong></p><p>The next EAGx should have a talk on smart ways to maximize giving strategies, geared toward the particular rules of the country in which the event is hosted.</p><p>EAs should be more open to taking on risk with the assets they intend to donate. &nbsp;EAs should invest to give with the mindset of maximizing expected value, knowing that the downside risks won't put their financial future in jeopardy. &nbsp;To this end, EAs might be underutilizing donor advised funds and missing out on the associated tax benefits.</p><p>It's also possible that EAs are underleveraging opportunities to spend money to save time.</p><p>Furthermore, EAs (especially student groups) may benefit from education about basic personal finance: maximizing 401k and ROTH IRA contributions, using low-cost index funds to invest, setting up and paying off credit cards to take advantage of perks and save on travel. &nbsp;With an appropriate rainy day fund, an EA should be in a better place to take more risks in their career, such as moving to a new city or holding out for a truly great job instead of being forced by their financial situation to take on a merely okay one. &nbsp;Putting these strategies into place should ensure that an EA's lifetime commitment to donating doesn't leave them in a difficult financial position. &nbsp;</p><p>If we make these financial literacy improvements in our community now, I expect them to pay off abundantly over time.</p>", "user": {"username": "NicoleJaneway"}}, {"_id": "sWguzaydorf8ejCKu", "title": "What should we have thought about FTX's business practices?", "postedAt": "2022-12-10T17:10:25.630Z", "htmlBody": "<p>I think I'm not alone in worrying that we might have overlooked red flags about FTX because of the fact that its founders considered themselves EAs.</p><p>Suppose all of us who failed to predict the FTX collapse were right to think, beforehand, that FTX was very likely an honest, non-fraudulent business. (Maybe because <a href=\"https://astralcodexten.substack.com/p/why-im-less-than-infinitely-hostile\">base rates for fraud were low</a> or because <a href=\"https://astralcodexten.substack.com/p/why-im-less-than-infinitely-hostile\">investors thought so too</a>.) Should we have even still been concerned about its business practices?</p><p>For instance, should FTX's impact on its customers have looked net-negative? Should its business have seemed objectionable from a \"common-sense ethics\" perspective? If so, the lack of discussion at the time would suggest that many of us were either blind to unwelcome news or afraid to speak out against an important funder.</p><p>Here are some considerations that might have suggested FTX's business practices were bad:</p><ol><li>\"If customers are moving most of their savings out of stocks and bonds and into cryptocurrency, that probably makes them worse-off. FTX's mass-marketing might be encouraging people to do this, especially people who aren't financially savvy.\"</li><li>\"When customers make trades on the platform, they're probably trading against smart money and losing out. In fact, they're probably losing out more than usual because that smart money is Alameda and Alameda has a systemic advantage. Considering the amount FTX spends on marketing, customers must be losing a lot of money between exchange fees and market losses to Alameda.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5p7rhlcyc8x\"><sup><a href=\"#fn5p7rhlcyc8x\">[1]</a></sup></span></li></ol><p>On the other hand, many people enjoy retail trading; some are probably aware of the costs and still find it worthwhile.</p><p>My tentative personal view is that a year ago, FTX's business looked neutral or mildly bad for customers, but not much worse than, e.g., Robinhood; that the reputational risk to EA looked small; and that, though specialists could've given these issues more attention, it was okay for the wider EA community to focus on other things.</p><p><strong>What should someone with no inside information or ingroup bias have thought a year ago about FTX's business practices?</strong></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5p7rhlcyc8x\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5p7rhlcyc8x\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In hindsight it looks like all this might be false, but I assume we couldn't have known that at the time.</p></div></li></ol>", "user": {"username": "smountjoy"}}, {"_id": "HBgAruFrZhFKBFfDa", "title": "Applications open for AGI Safety Fundamentals: Alignment Course", "postedAt": "2022-12-13T10:50:20.019Z", "htmlBody": "<p>The <a href=\"https://www.agisafetyfundamentals.com/?utm_source=EA+Forum&amp;utm_medium=launch+post&amp;utm_campaign=AGISF\">AGI Safety Fundamentals (AGISF)</a>: Alignment Course is designed to introduce the key ideas in AGI safety and alignment, and provide a space and support for participants to engage, evaluate and debate these arguments. Participants will meet others who are excited to help mitigate risks from future AI systems, and explore opportunities for their next steps in the field.</p><p>The course is being run by the same team as for previous rounds, now under a new project called <a href=\"https://forum.effectivealtruism.org/posts/3EWpLid8tkyYJakfm/announcing-bluedot-impact\">BlueDot Impact</a>.</p><p><a href=\"https://airtable.com/shrXRbqs1gZkOfx7c\"><strong>Apply here</strong></a>, by 5th January 2023.</p><h3>Time commitment</h3><p>The course will run from February-April 2023. It comprises 8 weeks of reading and virtual small-group discussions, followed by a 4-week capstone project.</p><p>The time commitment is around 4 hours per week, so participants can engage with the course alongside full-time work or study.</p><h3>Course structure</h3><p>Participants are provided with structured content to work through, alongside weekly, facilitated discussion groups. Participants will be grouped depending on their ML experience and background knowledge about AI safety. In these sessions, participants will engage in activities and discussions with other participants, guided by the facilitator. The facilitator will be knowledgeable about AI safety, and can help to answer participants\u2019 questions.</p><p>The course is followed by a capstone project, which is an opportunity for participants to synthesise their views on the field and start thinking through how to put these ideas into practice, or start getting relevant skills and experience that will help them with the next step in their career.</p><p>The course content is designed by Richard Ngo (Governance team at OpenAI, previously a research engineer on the AGI safety team at DeepMind). You can read the curriculum content <a href=\"https://www.agisafetyfundamentals.com/ai-alignment-curriculum\">here</a>.</p><h3>Target audience</h3><p>We are most excited about applicants who would be in strong position to pursue technical alignment research in their career, such as professional software engineers and students studying technical subjects (e.g. CS/maths/physics/engineering).</p><p>That said, we consider all applicants and expect 25-50% of the course to consist of people with a variety of other backgrounds, so we encourage you to apply regardless. This includes community builders who would benefit from a deeper understanding of the concepts in AI alignment.</p><p>We will be running another course on AI Governance in early 2023 and expect a different distribution of target participants.</p><h3>Apply now!</h3><p>If you would like to be considered for the next round of the courses, starting in February 2023, <strong>please </strong><a href=\"https://airtable.com/shrXRbqs1gZkOfx7c\"><strong>apply here</strong></a><strong> by Thursday 5th January 2023</strong>. More details can be found <a href=\"https://www.agisafetyfundamentals.com/alignment-course-details\">here</a>. We will be evaluating applications on a rolling basis and we aim to let you know the outcome of your application by mid-January 2023.</p><p>If you already have experience working on AI alignment and would be keen to join our community of facilitators, please <a href=\"https://www.agisafetyfundamentals.com/alignment-facilitation-details\">apply to facilitate</a>.</p><h3>Who is running the course?</h3><p>AGISF is now being run by <a href=\"https://bluedotimpact.org/?utm_source=EA+Forum&amp;utm_medium=AGISF+launch+post&amp;utm_campaign=AGISF\">BlueDot Impact</a> - a new non-profit project running courses that support participants to develop the knowledge, community and network needed to pursue high-impact careers. BlueDot Impact spun out of Cambridge Effective Altruism, and was founded by the team who was primarily responsible for running previous rounds of AGISF. You can read more in our announcement post <a href=\"https://forum.effectivealtruism.org/posts/3EWpLid8tkyYJakfm/announcing-bluedot-impact\">here</a>.</p><p>We\u2019re really excited about the amount of interest in the courses and think they have great potential to build awesome communities around key issues. As such we have spent the last few months:</p><ul><li>Working with pedagogy experts to make discussion sessions more engaging</li><li>Formalising our course design process with greater transparency for participants and facilitators</li><li>Building systems to improve participant networking to create high-value connections</li><li>Collating downstream opportunities for participants to pursue after the courses</li><li>Forming a team that can continue to build, run and improve these courses over the long-term</li></ul><p>Applications for our other courses, including the AGISF: Governance Course, will open in early 2023!</p>", "user": {"username": "j_bernardi"}}, {"_id": "J7gdciCXFgqyimAAe", "title": "Center on Long-Term Risk: 2023 Fundraiser", "postedAt": "2022-12-09T18:03:56.067Z", "htmlBody": "<h1>Summary</h1><ul><li><strong>Our goal</strong>: CLR\u2019s goal is to reduce the worst risks of astronomical suffering (<a href=\"https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\"><u>s-risks</u></a>). Our concrete research programs are on&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh/p/oNQGoySbpmnH632bG\"><u>AI conflict</u></a>,&nbsp;<a href=\"https://longtermrisk.org/msr\"><u>Evidential Cooperation in Large Worlds (ECL)</u></a>, and s-risk macrostrategy. We ultimately want to identify and advocate for interventions that reliably shape the development and deployment of advanced AI systems in a positive way.</li><li><strong>Fundraising</strong>: We have had a short-term funding shortfall and a lot of medium-term funding uncertainty. Our minimal fundraising goal is $750,000. We think this is a particularly good time to donate to CLR for people interested in supporting work on s-risks, work on Cooperative AI, work on acausal interactions, or work on generally important longtermist topics.&nbsp;</li><li><strong>Causes of Conflict Research Group</strong>: In 2022, we started evaluating various interventions related to AI conflict (e.g.,&nbsp;<a href=\"https://longtermrisk.org/spi\"><u>surrogate goals</u></a>, preventing&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh/p/cLDcKgvM6KxBhqhGq#What_if_conflict_isn_t_costly_by_the_agents__lights__\"><u>conflict-seeking preferences</u></a>). We also started developing methods for evaluating conflict-relevant properties of large language models. Our priorities for next year are to continue developing and evaluating these, and to continue our work with large language models.</li><li><strong>Other researchers</strong>: In 2022, others researchers at CLR worked on topics including the implications of ECL, the optimal timing of AI safety spending, the likelihood of earth-originating civilization encountering extraterrestrials, and program equilibrium. Our priorities for the next year include continuing some of this work, alongside other work including on strategic modeling and agent foundations.</li><li><strong>S-risk community-building</strong>: Our s-risk community building programs received very positive feedback. We had calls or meetings with over 150 people interested in contributing to s-risk reduction. In 2023, we plan to at least continue our existing programs (i.e., intro fellowship, Summer Research Fellowship, retreat) if we can raise the required funds. If we can even hire additional staff, we want to expand our outreach function and create more resources for community members (e.g., curated reading lists, career guide, introductory content, research database).</li></ul><h1>What CLR is trying to do and why</h1><p>Our goal is to reduce the worst risks of astronomical suffering (<a href=\"https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\"><u>s-risks</u></a>). These are scenarios where a significant fraction of future sentient beings are locked into intense states of misery, suffering, and despair.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref31qvamb7ytw\"><sup><a href=\"#fn31qvamb7ytw\">[1]</a></sup></span>&nbsp;We currently believe that such lock-in scenarios most likely involve transformative AI systems. So we work on making the development and deployment of such systems safer.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5acmzh5cqjg\"><sup><a href=\"#fn5acmzh5cqjg\">[2]</a></sup></span></p><p>Concrete research programs:</p><ul><li><a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh/p/oNQGoySbpmnH632bG\"><strong><u>AI conflict</u></strong></a>: We want to better understand how we can prevent AI systems from engaging in catastrophic conflict. (The majority of our research efforts)</li><li><a href=\"https://longtermrisk.org/msr\"><strong><u>Evidential</u></strong></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8yqh7b8k8im\"><sup><a href=\"#fn8yqh7b8k8im\">[3]</a></sup></span><a href=\"https://longtermrisk.org/msr\"><strong><u>&nbsp;Cooperation in Large Worlds (ECL)</u></strong></a>: ECL refers to the idea that we make it more likely that other agents across the universe take actions that are good for our values by taking actions that are good according to their values. A potential implication is that we should act so as to maximize an impartial weighted sum of the values of agents across the universe.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefodgz8j2g9tq\"><sup><a href=\"#fnodgz8j2g9tq\">[4]</a></sup></span></li><li><strong>S-risk macrostrategy</strong>: In general, we want to better understand how we can reduce suffering in the long-term future. There might be causes or considerations that we have overlooked so far.</li></ul><p>Most of our work is research with the goal of identifying threat models and possible interventions. In the case of technical AI interventions (which is the bulk of our object-level work so far), we then plan to evaluate these interventions and advocate for their inclusion in AI development.&nbsp;</p><p>Next to our research, we also run events and fellowships to identify and support people wanting to work on these problems.</p><h1>Fundraising</h1><h2>Funding situation</h2><p>Due to recent events, we have had a short-term funding shortfall. This caused us to reduce our original budget for 2023 by 30% and take various cost-saving measures, including voluntary pay cuts by our staff, to increase our runway to six months.&nbsp;</p><p>Our medium-term funding situation is hard to predict at the moment, as there is still a lot of uncertainty. We hope to gain more clarity about this in the next few months.</p><h2>Fundraising goals</h2><p>Our minimal fundraising goal is to increase our runway to nine months, which would give us enough time to try and secure a grant from a large institutional funder in the first half of 2023. Our main goal is to increase our runway to twelve months and roll back some of the budget reductions, putting us in a more comfortable financial position again. Our stretch goal is to increase our runway to fifteen months and allow for a small increase in team size in 2023. See the table below for more details.</p><p>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Scenario</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Fundraising goal</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Months of runway</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Scope</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Minimal</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>$750,000</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>9</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>All current budget restrictions in place (including voluntary pay cuts)</li><li>No additional hiring</li><li>No Summer Research Fellowship</li><li>No in-person events</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Main</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>$2,170,000</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>12</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>Undo some budget restrictions (e.g., voluntary pay cuts, compute costs)</li><li>Run Summer Research Fellowship</li><li>Run at least one in-person event</li><li>No additional hiring</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Stretch</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>$3,180,000</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>15</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><ul><li>As \u201cMain\u201d</li><li>Hire 2.5 FTE (research staff)</li></ul></td></tr></tbody></table></figure><h2>Reasons to donate to CLR&nbsp;</h2><p>Given the financial situation sketched above, we believe that CLR is a good funding opportunity this year. Whether it makes sense for any given individual to donate to CLR depends on many factors. Below, we sketch the main reasons donors could be excited about our work. In an&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J7gdciCXFgqyimAAe/center-on-long-term-risk-2023-fundraiser#Appendix__Testimonials\"><u>appendix</u></a>, we collected some testimonials by people who have a lot of context on our work.</p><p><strong>Supporting s-risk reduction</strong>.</p><p>You might want to support CLR\u2019s work because it is one of the few organizations addressing risks of astronomical suffering directly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref8hhy8vtqvil\"><sup><a href=\"#fn8hhy8vtqvil\">[5]</a></sup></span>&nbsp;You could consider s-risk reduction worthwhile for a number of reasons: (1) You find the combination of&nbsp;<a href=\"http://longtermrisk.org/the-case-for-suffering-focused-ethics\"><u>suffering-focused ethics</u></a> and longtermism compelling. (2) You think the expected value of the future is not sufficiently high to warrant prioritizing extinction risk reduction over improving the quality of the future. (3) You want to address the fact that work on s-risks is comparatively neglected within longtermism and AI safety.</p><p>Since the early days of our organization, we have made significant progress on clarifying and modeling the concrete threats we are trying to address and coming up with technical candidate interventions (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/J7gdciCXFgqyimAAe/center-on-long-term-risk-2023-fundraiser#Appendix__Our_progress_so_far\"><u>Appendix</u></a>).</p><p><strong>Supporting work on addressing AI conflict.</strong></p><p>Next to the benefits to s-risk reduction, you might value some of our work because it addresses failure modes arising in multipolar AI scenarios more broadly (e.g., explored&nbsp;<a href=\"https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic\"><u>here</u></a>,&nbsp;<a href=\"https://docs.google.com/document/d/1B77VWaXG-u34nSRFKV14pJNHJHHb6sa5zJ08J70CVVA/edit\"><u>here</u></a>). In recent years, we have helped to build up the field of&nbsp;<a href=\"https://www.cooperativeai.com/\"><u>Cooperative AI</u></a> intended to address these risks (e.g.,&nbsp;<a href=\"https://arxiv.org/abs/2111.13872\"><u>Stastny et al. 2021</u></a>).</p><p><strong>Supporting work on better understanding acausal interactions</strong>.</p><p>Such interactions are possibly a crucial consideration for longtermists (see, e.g.,&nbsp;<a href=\"https://joecarlsmith.com/2021/08/27/can-you-control-the-past#ii-writing-on-whiteboards-light-years-away\"><u>here</u></a>). Some argue that, when acting, we should consider the non-causal implications of our actions (see, e.g.,&nbsp;<a href=\"https://www.cambridge.org/core/books/evidence-decision-and-causality/7077949D2CD42E99C08D4FBFE5321148\"><u>Ahmed (2014)</u></a>,&nbsp;<a href=\"https://arxiv.org/pdf/1710.05060.pdf\"><u>Yudkowsky and Soares (2018)</u></a>,&nbsp;<a href=\"https://academic.oup.com/pq/article/71/4/pqaa086/6118001\"><u>Oesterheld and Conitzer (2021)</u></a>). If this is the case, these effects could dwarf their causal influence (see, e.g.,&nbsp;<a href=\"https://globalprioritiesinstitute.org/the-evidentialists-wager/\"><u>here</u></a>). Better understanding the implications of this would then be a key priority. CLR is among the few organizations doing and supporting work on this (e.g.,&nbsp;<a href=\"https://longtermrisk.org/msr\"><u>here</u></a>).</p><p>Much of our work on cooperation in the context of AI plausibly becomes particularly valuable from this perspective. For instance, if we are to act so as to maximize a compromise utility function that includes the values of many agents across the universe<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefodgz8j2g9tq\"><sup><a href=\"#fnodgz8j2g9tq\">[4]</a></sup></span>, as the&nbsp;<a href=\"https://longtermrisk.org/msr\"><u>ECL argument</u></a> suggests, then it becomes much more important that AI systems, even if aligned, cooperate well with agents with different values.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqe7r8i61olg\"><sup><a href=\"#fnqe7r8i61olg\">[6]</a></sup></span></p><p><strong>Supporting cause-general longtermism research.</strong></p><p>CLR has also done important research from a general longtermist lens, e.g., on&nbsp;<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/2019/MacAskill_et_al_Evidentialist_Wager.pdf\"><u>decision theory</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/s/R8vKwpMtFQ9kDvkJQ\"><u>meta ethics</u></a>,&nbsp;<a href=\"https://www.lesswrong.com/s/5Eg2urmQjA4ZNcezy\"><u>AI timelines</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors\"><u>risks from malevolent actors</u></a>, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/7bc54mWtc7BrpZY9e/replicating-and-extending-the-grabby-aliens-model\"><u>extraterrestrial civilizations</u></a>. Our Summer Research Fellowship has been a springboard for junior researchers who then moved on to other longtermist organizations (e.g., ARC, Redwood Research, Rethink Priorities, Longview Philanthropy).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi47sltwq17\"><sup><a href=\"#fni47sltwq17\">[7]</a></sup></span></p><h2>How to donate</h2><p>To donate to CLR, please go to the&nbsp;<a href=\"https://longtermrisk.org/clr-fundraiser-2022\"><u>Fundraiser page</u></a> on our website.&nbsp;</p><ul><li>Donors from Germany, Switzerland, and the Netherlands can donate tax-deductibly via our website</li><li>Donors from the USA and UK can donate tax-deductibly through the<a href=\"https://funds.effectivealtruism.org/partners/center-on-long-term-risk\">&nbsp;<u>Giving What We Can platform</u></a></li><li>Donors from all other countries can donate to us via our website, but unfortunately tax deduction will not be available. For donations &gt;$10,000, please get in touch and we will see if we can facilitate a donation swap.</li></ul><p>For frequently asked questions on donating to CLR, see our<a href=\"https://longtermrisk.org/donate/\">&nbsp;<u>Donate page</u></a>.</p><h1>Our progress in 2022</h1><h2>Causes of Conflict Research Group</h2><p>This group is led by Jesse Clifton. Members of the group are Anni Leskel\u00e4, Anthony DiGiovanni, Julian Stastny, Maxime Rich\u00e9, Mia Taylor, and Nicolas Mac\u00e9.</p><h3>Subjective assessment<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefahc0ru75tim\"><sup><a href=\"#fnahc0ru75tim\">[8]</a></sup></span></h3><p><strong>Have we made relevant research progress?</strong></p><p>We believe we have made significant progress (e.g., relative to previous years) on improving our expertise in the reasons why AI systems might engage in conflict and the circumstances under which technical work done now could reduce these risks. We\u2019ve built up methods and knowledge that we expect to make us much better at developing and assessing interventions for reducing conflict. (Some of this is reflected in our public-facing work.) We have begun to capitalize on this in the latter part of 2022, as we\u2019ve begun moving from improving our general picture of the causes of conflict and possibilities for intervention to developing and evaluating specific interventions. These interventions include&nbsp;<a href=\"https://longtermrisk.org/spi\"><u>surrogate goals</u></a>, preventing&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh/p/cLDcKgvM6KxBhqhGq#What_if_conflict_isn_t_costly_by_the_agents__lights__\"><u>conflict-seeking preferences</u></a>, preventing&nbsp;<a href=\"https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem\"><u>commitment races</u></a>, and developing cooperation-related content for a hypothetical manual for overseers of AI training.</p><p>The second main way in which we\u2019ve made progress is the initial work we\u2019ve done on the evaluation of large language models (LLMs). There are several strong arguments that those interested in intervening on advanced AI systems should invest in experimental work with existing AI systems (see, e.g.,&nbsp;<a href=\"https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models\"><u>The case for aligning narrowly superhuman models</u></a>). Our first step here has been to work on methods for evaluating cooperation-relevant behaviors and reasoning of LLMs, as these methods are prerequisites for further research progress. We are in the process of developing the first&nbsp;<a href=\"https://arxiv.org/pdf/2012.08630.pdf\"><u>Cooperative AI</u></a> dataset for evaluating LLMs as well as methods for automatically generating data on which to evaluate cooperation-relevant behaviors, which is a prerequisite for techniques like&nbsp;<a href=\"https://arxiv.org/abs/2202.03286\"><u>red-teaming language models with language models</u></a>. We are preparing to submit this work to a machine learning venue. We have also begun developing methods for better understanding the reasoning abilities of LLMs when it comes to conflict situations in order to develop evaluations that could tell us when models have gained capabilities that are necessary to engage in catastrophic conflict.</p><p><strong>Has the research reached its target audience?</strong></p><p>We published a summary of our thinking (as of earlier this year) on&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh\"><u>when technical work to reduce AGI conflict makes a difference</u></a> on the Alignment Forum/LessWrong, which is visible to a large part of our target audience (AI safety &amp; longtermist thinkers). We have also shared internal documents with individual external researchers to whom they are relevant. A significant majority of the research that we\u2019ve done this year has not been shared with target audiences, though. Much of this is work-in-progress on evaluating interventions and evaluating LLMs which will be incorporated into summaries shared directly with external stakeholders, and in some cases posted on the Alignment Forum/LessWrong or submitted for publication in academic venues.&nbsp;</p><p><strong>What feedback on our work have we received from peers and our target audience?</strong></p><p>Our Alignment Forum sequence&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh\"><u>When does technical work to reduce AGI conflict make a difference?</u></a> didn\u2019t get much engagement. We did receive some positive feedback on internal drafts of this sequence from external researchers. We also solicited advice from individual alignment researchers throughout the year. This advice was either encouraging of existing areas of research focus or led us to shift more attention to areas that we are now focusing on (summarized in \u201crelevant research progress\u201d section above).&nbsp;&nbsp;</p><h3>Selected output</h3><ul><li><i>Anthony DiGiovanni, Jesse Clifton:&nbsp;</i><a href=\"https://arxiv.org/pdf/2204.03484.pdf\"><i><u>Commitment games with conditional information revelation</u></i></a><i>.&nbsp;(to appear at AAAI 2023).</i></li><li>Anthony DiGiovanni, Nicolas Mac\u00e9, Jesse Clifton:&nbsp;<a href=\"https://arxiv.org/abs/2207.03178\"><i><u>Evolutionary Stability of Other-Regarding Preferences under Complexity Costs</u></i></a>. Workshop on Learning, Evolution, and Games.</li><li>Jesse Clifton, Samuel Martin, Anthony DiGiovanni:&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh\"><i><u>When does technical work to reduce AGI conflict make a difference?</u></i></a> (Alignment Forum Sequence)<ul><li><a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh/p/oNQGoySbpmnH632bG\"><i><u>When does technical work to reduce AGI conflict make a difference?: Introduction</u></i></a></li><li><a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh/p/cLDcKgvM6KxBhqhGq\"><i><u>When would AGIs engage in conflict?</u></i></a></li><li><a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh/p/fMJhfNZXFzCNpCL8v\"><i><u>When is intent alignment sufficient or necessary to reduce AGI conflict?</u></i></a></li></ul></li><li><i>Anthony DiGiovanni, Jesse Clifton:&nbsp;</i><a href=\"https://drive.google.com/file/d/1rXS-_BBHKcE8r-sQY_dfs57904tNxxEa/view\"><i><u>Sufficient Conditions for Cooperation Between Rational Agents</u></i></a><i>, Working Paper.</i></li></ul><h2>Other researchers: Emery Cooper, Daniel Kokotajlo<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqvp4mvurnp\"><sup><a href=\"#fnqvp4mvurnp\">[9]</a></sup></span>, Tristan Cook</h2><p>Emery, Daniel, and Tristan work on a mix of macrostrategy, ECL, decision theory, anthropics, forecasting, AI governance, and game theory.</p><h3>Subjective assessment<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefahc0ru75tim\"><sup><a href=\"#fnahc0ru75tim\">[8]</a></sup></span></h3><p><strong>Have we made relevant research progress?</strong></p><p>The main focus of Emery\u2019s work in the last year has been on the implications of&nbsp;<a href=\"https://longtermrisk.org/msr\"><u>ECL</u></a> for cause prioritization. This includes work on the construction of the compromise utility function<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjuh25ot16uf\"><sup><a href=\"#fnjuh25ot16uf\">[10]</a></sup></span>&nbsp;under different anthropic and decision-theoretic assumptions, on the implications of ECL for AI design, and on more foundational questions. Additionally, Emery worked on a paper (in progress) extending our earlier&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11238-018-9679-3\"><u>Robust Program Equilibrium</u></a> paper<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5lhyxy9dvt\"><sup><a href=\"#fn5lhyxy9dvt\">[11]</a></sup></span>. She also did some work on the implications of limited introspection ability for evidential decision theory (EDT) agents, and some related work on anthropics.</p><p>Daniel left for OpenAI early in the year, but not before making significant progress building a model of ECL and identifying key cruxes for the degree of decision relevance of ECL.</p><p>Tristan primarily worked on the optimal spending schedule for AI risk interventions and the probability that an Earth-originating civilization would encounter alien civilizations. To that end, he built and published two comprehensive models.</p><p>Overall, we believe we made moderate research progress, but Emery and Daniel have accumulated a large number of unpublished ideas to be written up.</p><p><strong>Has the research reached its target audience?</strong></p><p>The primary goal of Tristan\u2019s reports was to inform CLR\u2019s own prioritization. For example, the existence of alien civilizations in the far future is a consideration for our work on conflict. That said, Tristan\u2019s work on Grabby Aliens received considerable engagement on the EA Forum and on LessWrong.</p><p>As mentioned above, a lot of Emery and Daniel\u2019s work is not yet fully written up and published. Whilst the target audience for some of this work is internal, it\u2019s nevertheless true that we haven\u2019t been as successful in this regard as we would like. We have had fruitful conversations with non-CLR researchers about these topics, e.g., people at Open Philanthropy and MIRI.</p><p><strong>What feedback on our work have we received from peers and our target audience?</strong></p><p>The grabby aliens report was well received by and cited by S. Jay Olson (co-author of a&nbsp;<a href=\"https://arxiv.org/abs/2106.13348\"><u>recent paper on extraterrestrial civilizations</u></a> with Toby Ord), who described it as \u201cfascinating and complete\u201d, and Tristan has received encouragement from Robin Hanson to publish academically, which he plans to do.</p><h3>Selected output</h3><ul><li><i>Emery Cooper, Caspar Oesterheld:&nbsp;Towards &gt;2 player epsilon-grounded FairBot (Working title, forthcoming).</i></li><li><i>Tristan Cook:&nbsp;</i><a href=\"https://longtermrisk.org/replicating-and-extending-the-grabby-aliens-model/\"><i><u>Replicating and extending the grabby aliens model</u></i></a><i><strong>.&nbsp;</strong>EA Forum.</i></li><li>Tristan Cook, Guillaume Corlouer:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Ne8ZS6iJJp7EpzztP/the-optimal-timing-of-spending-on-agi-safety-work-why-we\"><i><u>The optimal timing of spending on AGI safety work; why we should probably be spending more now</u></i></a>. EA Forum.</li><li>Tristan Cook:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ebYdBNpGnshhm2Gkq/neartermists-should-consider-agi-timelines-in-their-spending\"><i><u>Neartermists should consider AGI timelines in their spending decisions</u></i></a>. EA Forum.</li><li>Daniel Kokotajlo:&nbsp;<i>ECL Big Deal?&nbsp;</i>(unpublished Google Doc, forthcoming).&nbsp;</li><li>Daniel Kokotajlo:&nbsp;<a href=\"https://www.lesswrong.com/posts/byMNKEXBn4RTcaaa6/immanuel-kant-and-the-decision-theory-app-store\"><i><u>Immanuel Kant and the Decision Theory App Store</u></i></a><i>.&nbsp;</i>LessWrong.</li></ul><h2>S-Risk Community Building</h2><h3>Subjective assessment</h3><p>Progress on all fronts seems very similar to last year, which we characterized as \u201cmodest\u201d.</p><p><strong>Have we increased the (quality-adjusted) size of the community?</strong></p><p>Community growth has continued to be modest. We are careful in how we communicate about s-risks, so our outreach tools are limited. Still, we had individual contact with over 150 people who could potentially make contributions to our mission. Out of those, perhaps five to ten could turn out to be really valuable members of our community.</p><p><strong>Have we created opportunities for in-person (and in-depth online) contact for people in our community?</strong></p><p>We created more opportunities for discussion and exchange than before. We facilitated more visits to our office, hosted meetups around EAG, and we ran an S-Risk Retreat with about 30 participants. We wanted to implement more projects in this direction, but our limited staff capacity made that impossible.</p><p><strong>Have we provided resources for community members that make it more likely they contribute significantly to our mission?</strong></p><p>Our team has continued to provide several useful resources this year. We administered the CLR Fund, which supported various efforts in the community. We provided ad-hoc career advice to community members. We are currently experimenting with a research database prototype. We believe there are many more things we could be doing, but our limited staff capacity has held us back.</p><h3>Output &amp; activities</h3><ul><li><strong>S-Risk Intro Fellowship</strong>: In February and March, we ran two&nbsp;<a href=\"https://longtermrisk.org/intro-seminar/\"><u>S-Risk Intro Fellowships</u></a> with seven participants each. The participant feedback was generally very positive.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhpd42xwb5ec\"><sup><a href=\"#fnhpd42xwb5ec\">[12]</a></sup></span></li><li><strong>Summer Research Fellowship</strong>: We ran a&nbsp;<a href=\"https://longtermrisk.org/summer-research-fellowship/\"><u>Summer Research Fellowship</u></a> with nine fellows.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefya0jdpd1t5\"><sup><a href=\"#fnya0jdpd1t5\">[13]</a></sup></span>&nbsp;Again, feedback was generally very positive.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflgfw4artul\"><sup><a href=\"#fnlgfw4artul\">[14]</a></sup></span>&nbsp;Sylvester Kollin published&nbsp;<a href=\"https://www.lesswrong.com/posts/QpqKBYzPKdZpByZS3/logical-counterfactuals-are-brittle-use-conditionals\"><u>two</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/d6YQyRfoFNtbKjtGt/understanding-updatelessness-in-the-context-of-edt-and-cdt-1\"><u>posts</u></a> on decision theory in the context of the fellowship. David Udell moved on to work on&nbsp;<a href=\"https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview\"><u>shard theory</u></a>. Hjalmar Wijk is now at ARC. Mia Taylor started working at CLR.</li><li><strong>S-Risk Retreat:</strong> In October, we ran an S-Risk Retreat with 33 participants. Again, feedback was generally very positive.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1iz1ro91gg2\"><sup><a href=\"#fn1iz1ro91gg2\">[15]</a></sup></span></li><li><strong>CLR Fund: We made the following grants in 2022:</strong><ul><li>Johannes Treutlein: Top-up of a previous scholarship grant for a Master's degree at the University of Toronto.</li><li>Samuel Martin: Six-month extension grant for research on whether work on reducing AI conflict is redundant with work on AI alignment.</li><li>Anton Leicht: Funding for a two-month research project on the implications of normative uncertainty for whether to prioritize s-risks.</li><li>Nandi Schoots: Funding for a three-month research project on simplicity bias in neural nets.</li><li>Lukas Holter Melgaard: Funding for a three-month research project on summarizing Vanessa Kosoy\u2019s Infrabayesianism.</li><li>Tim Chan: Scholarship for completing a computer science conversion Master\u2019s.</li><li>Miranda Zhang: Funding for living expenses while working on clarifying her possible career paths.</li><li>Bogdan-Ionut Cirstea: Funding for a year-long research project on short timelines.</li><li>Asher Soryl: Research funding for a project on the ethics of panspermia, among others.</li><li>Gary O'Brien: Teaching buy-out during the last year of their PhD to research (long-term) wild animal suffering.</li></ul></li><li><strong>Individual outreach</strong>:<strong>&nbsp;</strong>We conducted over one hundred fifty 1:1 calls and meetings with potentially promising people. This also included various office visits by people.</li></ul><h2>General organizational health</h2><p><strong>Guiding question:&nbsp;</strong><i>Are we a healthy organization with sufficient operational capacity, an effective board, appropriate evaluation of our work, reliable policies and procedures, adequate financial reserves and reporting, and high morale?</i></p><h3>Operational capacity</h3><p>Our capacity is currently not as high as we would like it to be as a staff member left in the summer and we only recently made a replacement hire. So various improvements to our setup have been delayed (e.g., IT &amp; security improvements, a systematic policy review, some visa-related issues, systematic risk management). That being said, we are still able to maintain all the important functions of the organization (i.e., accounting, payments, payroll, onboarding/offboarding, hiring support, office management, feedback &amp; review, IT maintenance).</p><h3>Board</h3><p><strong>Members of the board</strong>: Tobias Baumann, Max Daniel, Ruairi Donnelly (chair), Chi Nguyen, Jonas Vollmer.<br>&nbsp;</p><p>The board are ultimately responsible for CLR. Their specific responsibilities include deciding CLR\u2019s leadership and structure, engaging with the team about strategy and planning, resolving organizational conflicts, and advising and providing accountability for CLR leadership. In 2022 they considered various decisions related to CLR\u2019s new office, hiring/promotion, and overall financials. The team generally considered their advice to be valuable.</p><h3>Evaluation function</h3><p>We collect systematic feedback on big community-building and operations projects through surveys and interviews. We collect feedback on our research by submitting articles to journals &amp; conferences and by requesting feedback on drafts of documents from relevant external researchers.</p><h3>Policies &amp; guidelines</h3><p>In 2022, we did not have any incidents that required a policy-driven intervention or required setting up new policies. Due to a lack of operational capacity in 2022, we failed to conduct a systematic review of all of our policies.</p><h3>Financial health</h3><p>See \u201c<a href=\"https://forum.effectivealtruism.org/posts/J7gdciCXFgqyimAAe/center-on-long-term-risk-2023-fundraiser#Fundraising\"><u>Fundraising</u></a>\u201d section above.</p><h3>Morale</h3><p>We currently don\u2019t track staff morale quantitatively. Our impression is that this varies significantly between staff members and is more determined by personal factors than organizational trends.</p><h1>Our plans for next year</h1><h2>Causes of Conflict Research Group</h2><p>Our plans for 2023 fall into three categories.&nbsp;</p><p><strong>Evaluating large language models.&nbsp;</strong>We will continue building on the work on evaluating LLMs that we started this year. Beyond writing up and submitting our existing results, the priority for this line of work is scoping out an agenda for assessing cooperation-relevant capabilities. This will account for work on evaluation that\u2019s being done by other actors in the alignment space and possible opportunities for eventually slotting into those efforts.&nbsp;&nbsp;&nbsp;</p><p><strong>Developing and evaluating cooperation-related interventions.&nbsp;</strong>We will continue carrying out the evaluations of the interventions that we started this year. On the basis of these evaluations, we\u2019ll decide which interventions we want to prioritize developing (e.g., working out in more detail how they would be implemented under various assumptions about what approaches to AI alignment will be taken). In parallel, we\u2019ll continue developing content for an overseer\u2019s manual for AI systems.&nbsp;&nbsp;</p><p><strong>General s-risk macrostrategy.&nbsp;</strong>Some researchers on the team will continue spending some of their time thinking about s-risk prioritization more generally, e.g., thinking about the value of alternative priorities to our group\u2019s current focus on AI conflict.</p><h2>Other researchers: Emery Cooper, Daniel Kokotajlo, Tristan Cook</h2><p>Emery plans to prioritize finishing and writing up her existing research on ECL. She also has plans for some more general posts on ECL, including on some common confusions, and on more practical implications for cause prioritization. Emery also plans to focus on finishing the paper extending Robust Program Equilibrium, and to explore further more object-level work.</p><p>Daniel no longer works at CLR but plans to organize a research retreat focused on ECL in the beginning of 2023.</p><p>Tristan broadly plans to continue strategy-related modeling, such as on the spread of information hazards. He also plans to help to complete a project that calculates the marginal utility of AI x-risk and s-risk work under different assumptions about AGI timelines, and to potentially contribute to work on ECL.</p><h2>S-Risk Community Building</h2><p>We had originally planned to expand our activities across all three community-building functions. Without additional capacity, we would have to curtail these plans.</p><p><strong>Outreach</strong>. If resources allow, we will host another Intro Fellowship and Summer Research Fellowship. We will also continue our 1:1 meetings &amp; calls. We also plan to investigate what kind of mass outreach within the EA community would be most helpful (e.g., online content, talks, podcasts). Without such outreach, we expect that community growth will stagnate at its current low rate.</p><p><strong>Resources</strong>. We plan to create more long-lasting and low-marginal-cost resources for people dedicated to s-risk reduction (e.g., curated reading lists, career guide, introductory content, research database). As the community grows and diversifies, these resources will have to become more central to our work.</p><p><strong>Exchange</strong>. If resources allow, we will host another S-Risk Retreat. We also want to experiment with other online and in-person formats. Again, as the community grows and diversifies, we need to find a replacement for more informal arrangements.</p><h1>Appendix: Testimonials</h1><p><strong>Nate Soares</strong> (Executive Director, Machine Intelligence Research Institute): \"My understanding of CLR's mission is that they're working to avoid fates worse than the destruction of civilization, especially insofar as those fates could be a consequence of misaligned superintelligence. I'm glad that someone on earth is doing CLR's job, and CLR has in the past seemed to me to occasionally make small amounts of legible-to-me progress in pursuit of their mission. &nbsp;(Which might sound like faint praise, and I sure would endorse CLR more full-throatedly if they spent less effort on what seem to me like obvious dead-ends, but at the same time it's not like anybody else is even trying to do their job, and their job is worthy of attempting. According to me, the ability to make any progress at all in this domain is laudable)\"</p><p><strong>Lukas Finnveden</strong> (Research Analyst, Open Philanthropy): \u201cCLR\u2019s focus areas seem to me like the most important areas for reducing future suffering. Within these areas, they\u2019ve shown competence at producing new knowledge, and I\u2019ve learnt a lot that I value from engaging with their research.\u201d</p><p><strong>Daniel Kokotajlo</strong> (Policy/Governance, OpenAI): \u201cI think AI cooperation and s-risk reduction are high priority almost regardless of your values / ethical views. The main reason to donate to, or work for, CLR is that the best thinking about s-risks and AI cooperation happens here, better than the thinking at MIRI or Open Phil or anywhere else. CLR also contains solid levels of knowledge of AI governance, AI alignment, AI forecasting (less so now that I\u2019m gone), cause prioritisation, metaethics, agent foundations, anthropics, aliens, and more. Their summer fellows program is high quality and has produced many great alumni. Their ops team is great &amp; in general they are well-organized. I left CLR to join the OpenAI governance team because I was doing mostly AI forecasting which benefits from being in a lab \u2014 but I was very happy at CLR and may even one day return.\u201d</p><p><strong>Michael Aird</strong> (Senior Research Manager, Rethink Priorities): \"I enjoyed my time as a summer research fellow at CLR in 2020, and I felt like I learned a lot about doing research and about various topics related to longtermist strategy, AI risk, and ethics. I was also impressed by the organization's culture and how the organization and fellowship was run, and I drew on some aspects of that when helping to design a research fellowship myself and when starting to manage people.\"</p><p>Testimonials by other Summer Research Fellows can be found&nbsp;<a href=\"https://longtermrisk.org/summer-research-fellowship/\"><u>here</u></a>.</p><h1>Appendix: Our progress so far</h1><p>What follows below is a somewhat stylized/simplified account of the history of the Center on Long-Term Risk prior to 2022. It is not meant to capture every twist and turn.&nbsp;</p><p><strong>2011-2016: Incubation phase</strong></p><p>What is now called the \u201cCenter on Long-Term Risk\u201d starts out as a student group in Switzerland. Under the name \u201cFoundational Research Institute,\u201d we do pre-paradigmatic research into possible risks of astronomical suffering and create basic awareness of these scenarios in the EA community. A lot of pioneering thinking is done by&nbsp;<a href=\"https://reducing-suffering.org/\"><u>Brian Tomasik</u></a>. In 2016, we coin the term \u201crisks of astronomical suffering\u201d (s-risks). Key publications from that period:</p><ul><li>Brian Tomasik (2011):&nbsp;<a href=\"https://longtermrisk.org/risks-of-astronomical-future-suffering/\"><i><u>Risks of Astronomical Future Suffering</u></i></a><i>.</i></li><li><i>Brian Tomasik (2015):&nbsp;</i><a href=\"https://longtermrisk.org/reasons-to-be-nice-to-other-value-systems/\"><i><u>Reasons to Be Nice to Other Value Systems</u></i></a><i>.</i></li><li>David Althaus, Lukas Gloor (2016):&nbsp;<a href=\"https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\"><i><u>Reducing Risks of Astronomical Suffering. A Neglected Priority</u></i></a>.</li></ul><p><strong>2016-2019: Early growth</strong></p><p>More researchers join; the organization professionalizes and matures. We publish our first journal articles related to s-risks. Possible interventions are being developed and discussed, surrogate goals among them. In 2017, we start sharing our work with other researchers in the longtermist community. That culminates in a series of research workshops in 2019. Key publications from that period:</p><ul><li>Kaj Sotala, Lukas Gloor (2017):&nbsp;<a href=\"https://www.semanticscholar.org/paper/Superintelligence-As-a-Cause-or-Cure-For-Risks-of-Sotala-Gloor/d8ea9bd0258bacf6f30bd0399ee7208aea891e44?p2df\"><i><u>Superintelligence As a Cause or Cure For Risks of Astronomical Suffering.&nbsp;</u></i></a>Informatica, 41 (4), 2017</li><li>\u200b\u200bCaspar Oesterheld (2017):&nbsp;<a href=\"https://longtermrisk.org/multiverse-wide-cooperation-via-correlated-decision-making/\"><i><u>Multiverse-wide Cooperation via Correlated Decision Making</u></i></a><i>.</i></li><li>Tobias Baumann (2018):&nbsp;<a href=\"https://longtermrisk.org/using-surrogate-goals-deflect-threats/\"><i><u>Using surrogate goals to deflect threats</u></i></a>. (<a href=\"https://www.lesswrong.com/posts/4WbNGQMvuFtY3So7s/announcement-ai-alignment-prize-winners-and-next-round\"><u>runner-up at the AI alignment prize</u></a>)</li><li>Caspar Oesterheld (2018:&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11238-018-9679-3\"><i><u>Robust program equilibrium</u></i></a>. Theory and Decision (86).</li><li>Caspar Oesterheld (2019):&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11229-019-02148-2\"><i><u>Approval-directed agency and the decision theory of Newcomb-like problems</u></i></a>. Synthese (198). (Runner-up in the \u201c<a href=\"https://www.lesswrong.com/posts/4WbNGQMvuFtY3So7s/announcement-ai-alignment-prize-winners-and-next-round\"><u>AI alignment prize</u></a>\u201d)</li><li>Caspar Oesterheld, Vince Conitzer (2021<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3bgoeo0dxya\"><sup><a href=\"#fn3bgoeo0dxya\">[16]</a></sup></span>):&nbsp;<a href=\"https://users.cs.duke.edu/~conitzer/safeAAMAS21.pdf\"><i><u>Safe Pareto Improvements for Delegated Game Playing</u></i></a>, Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021), Online, May 3\u20137, 2021, IFAAMAS.</li></ul><p><strong>2019-2022: Maturation</strong></p><p>Before 2019, we were pursuing many projects other than research on s-risks. In 2019, this stops. We start focusing exclusively on research. Increasingly, we connect our ideas to existing lines of academic inquiry. We also start engaging more with concrete proposals and empirical work in AI alignment. Key publications from that period:</p><ul><li>Jesse Clifton (2019):&nbsp;<a href=\"https://longtermrisk.org/research-agenda\"><i><u>Cooperation, Conflict, and Transformative Artificial Intelligence. A Research Agenda</u></i></a><i>.</i></li><li>Daniel Kokotajlo (2019):&nbsp;<a href=\"https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem\"><i><u>The Commitment Races Problem.</u></i></a></li><li><i>David Althaus, Tobias Baumann (2020):&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors\"><i><u>Reducing long-term risks from malevolent actors</u></i></a><i>&nbsp;(</i><a href=\"https://forum.effectivealtruism.org/posts/Pc3TxdyAZp9GmLsjo/ea-forum-prize-winners-for-april-2020\"><i><u>EA forum prize, second place</u></i></a><i>.&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/FEFEvC6BzswR4oQqm/results-from-the-first-decade-review#Third_prizes___500_each_\"><i><u>First Decade Review, third prize.</u></i></a><i>)</i></li><li>Julian Stastny, Maxime Rich\u00e9, Alexander Lyzhov, Johannes Treutlein, Allan Dafoe, Jesse Clifton (2021):&nbsp;<a href=\"https://arxiv.org/abs/2111.13872\"><i><u>Normative Disagreement as a Challenge for Cooperative AI</u></i></a>. Cooperative AI workshop and the Strategic ML workshop at NeurIPS 2021.</li><li>Daniel Kokotajlo (2021):&nbsp;<a href=\"https://www.lesswrong.com/s/5Eg2urmQjA4ZNcezy\"><i><u>AI Timelines sequence.</u></i></a></li><li>Jesse Clifton, Anthony DiGiovanni, Samuel Martin (2022):&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh\"><i><u>When does technical work to reduce AGI conflict make a difference?</u></i></a> We consider ourselves to be working on \u201cAI safety\u201d. Whether it is also encompassed by \u201cAI alignment\u201d depends on how broadly you define \u201calignment\u201d. In any case, our work is fairly different from other work in AI alignment (also see&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh\"><u>this sequence</u></a>).</li></ul><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn31qvamb7ytw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref31qvamb7ytw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here is the more technical definition: S-risks are risks of events that bring about suffering in cosmically significant amounts. By \u201csignificant\u201d, we mean significant relative to expected future suffering.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5acmzh5cqjg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5acmzh5cqjg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We consider ourselves to be working on \u201cAI safety\u201d. Whether it is also encompassed by \u201cAI alignment\u201d depends on how broadly you define \u201calignment\u201d. In any case, our work is fairly different from other work in AI alignment (also see&nbsp;<a href=\"https://www.alignmentforum.org/s/32kWH6hqFhmdFsvBh\"><u>this sequence</u></a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8yqh7b8k8im\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8yqh7b8k8im\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The term \u201cevidential\u201d here is potentially misleading: whilst ECL is often framed in terms of evidential decision theory (EDT), the argument is more general, and applies to many decision theories. One notable exception is causal decision theory (CDT), which for the most part does not endorse ECL. For more details on this, see&nbsp;<a href=\"https://longtermrisk.org/multiverse-wide-cooperation-via-correlated-decision-making/\"><u>the ECL paper itself</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnodgz8j2g9tq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefodgz8j2g9tq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>With little dependence on our own values.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn8hhy8vtqvil\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref8hhy8vtqvil\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We don\u2019t have the space here to make a case for CLR over other organizations focused on s-risks.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqe7r8i61olg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqe7r8i61olg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>That said, we currently believe that, although there might be significant overlap, there are likely still some important differences in focus between the portfolio of cooperation work recommended by ECL considerations and that recommended by thinking on s-risks ignoring ECL. We more generally want to ensure that in our work on ECL we consider other kinds of work that could be higher priority but which might be less salient to us.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni47sltwq17\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi47sltwq17\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We have not systematically assessed to what extent CLR made a significant difference in the career trajectory of these researchers.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnahc0ru75tim\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefahc0ru75tim\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We consider raising the salience of s-risks as potentially harmful. So much of our research remains unpublished. This makes a coherent and informative public review of our research progress difficult.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqvp4mvurnp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqvp4mvurnp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Daniel was a Lead Researcher at CLR until mid-2022. He now works at OpenAI on the policy/governance team. However, he continues some other research projects in collaboration with CLR colleagues. He also mentors CLR Summer Research Fellows.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjuh25ot16uf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjuh25ot16uf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I.e., the utility function that ECL considerations arguably recommend we act so as to optimize, formed as an impartial weighted sum of utility functions of agents across the universe.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5lhyxy9dvt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5lhyxy9dvt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See also the 2016-2019 section of CLR\u2019s history</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhpd42xwb5ec\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhpd42xwb5ec\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cWhat is your overall satisfaction with the S-Risk Intro Fellowship?\u201d (8/10), \u201cTo what extent has the S-Risk Intro Fellowship met your expectations?\u201d (8.4/10), \u201cHow well did the S-Risk Intro Fellowship compare with the ideal intro fellowship?\u201d (7.7/10), \u201cIf the same program happened next year, would you recommend a friend (with similar background to you before the fellowship) to apply?\u201d (8.6/10).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnya0jdpd1t5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefya0jdpd1t5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Alan Chan, David Udell, Hjalmar Wijk, James Faville, Mia Taylor, Nathaniel Sauerberg, Nisan Stiennon, Quratul Zainab, Sylvester Kollin.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlgfw4artul\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflgfw4artul\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cAre you glad that you participated in the fellowship?\u201d (4.8/5), \u201cIf the same program happened next year, would you recommend a friend (with similar background to you before the fellowship) to apply?\u201d (9.5/10).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1iz1ro91gg2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1iz1ro91gg2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cTo what extent was the event a good use of your time compared to what you would otherwise have been doing?\u201d (geometric mean: 8.1x as valuable compared to the counterfactual), \u201cHow likely is it that you would like to attend a similar event next year if we held one? (75.7%), \u201cOverall, how would you rate the practicalities and logistics before and during the workshop?\u201d (4.6/5)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3bgoeo0dxya\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3bgoeo0dxya\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A significant part of the conceptual work underlying this paper was done before 2019 at CLR.</p></div></li></ol>", "user": {"username": "storges"}}, {"_id": "DfYDGA5W8qREeT7ao", "title": "Talk - 'Caring for the Far Future'", "postedAt": "2022-12-09T16:58:11.436Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://youtu.be/B_PPlwzC3wo\"><div><iframe src=\"https://www.youtube.com/embed/B_PPlwzC3wo\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>I did a talk for my university group!<br><br>Here are the slides - <a href=\"https://www.canva.com/design/DAFSbuxfnwM/BnHj_I4HunOkWwj2m9sbCg/view?utm_content=DAFSbuxfnwM&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton\">https://www.canva.com/design/DAFSbuxfnwM/BnHj_I4HunOkWwj2m9sbCg/view?utm_content=DAFSbuxfnwM&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton</a></p>", "user": {"username": "AryanYadav"}}, {"_id": "NkPghabDd54nkG3kX", "title": "Some observations from an EA-adjacent (?) charitable effort", "postedAt": "2022-12-09T20:13:51.958Z", "htmlBody": "<p>Hiya folks! I'm Patrick McKenzie, better known on the Internets as patio11. (<a href=\"https://www.kalzumeus.com/.well-known/presences.txt\">Proof</a>.) Long-time-listener, first-time-caller; I don't think I would consider myself an EA but I've been reading y'all, and adjacent intellectual spaces, for some time now.</p><p><i>Epistemic status</i>: Arbitrarily high confidence with regards to facts of the VaccinateCA experience (though speaking only for myself), moderately high confidence with respect to inferences made about vaccine policy and mechanisms for impact last year, one geek's opinion with respect to implicit advice to you all going forward.</p><h2>A Thing That Happened Last Year</h2><p>As some of the California-based EAs may remember, the rollout of the covid-19 vaccines in California and across the U.S. was... not optimal. I accidentally ended up founding a charity, VaccinateCA, which ran the national shadow vaccine location information infrastructure for 6 months.</p><p>The core product at the start of the sprint, which some of you may be familiar with, was a site which listed places to get the vaccine in California, sourced by a volunteer-driven operation to conduct an ongoing census of medical providers by calling them. Importantly, that <i>was not</i> our primary vector for impact, though it was very important to our trajectory.</p><p><strong>I recently wrote an </strong><a href=\"https://www.worksinprogress.co/issue/the-story-of-vaccinateca/\"><strong>oral history</strong></a><strong> of VaccinateCA.</strong> <strong>It may be worth your time.</strong> Obligatory disclaimer: I'm speaking, there and here, in entirely my personal capacity, not on behalf of the organization (now wound-down) or others.</p><p>A brief summary of impact: I think this effort likely saved many thousands of lives at the margin, at a cost of approximately $1.2 million. This feels remarkable relative to my priors for cost of charitably saving lives at scale in the US, and hence this post.</p><p>Some themes of the experience I think you may find useful:</p><h2>Enabling trade as a mechanism for impact</h2><p>To a first approximation, Google, the White House, the California governor's office, the Alameda County health department, the pharmacist at CVS, and several hundred thousand other actors have unified values and expectations with regards to the desirability of vaccinating residents of America against covid-19.</p><p>They are also bad at trading with each other. <i>Pathologically so</i>, in many cases.</p><p>One of the reasons we had such leveraged impact is that we didn't have to build Google, or recruit a few hundred million Americans to use it every day. We just had to find a very small number of people within Google and convince them that Google users would benefit from seeing our data on their surfaces as quickly as possible.</p><p>Google and large national pharmacy chains cannot quickly negotiate an API, even given substantial mutual desire to do so. As it turns out, pharmacists already have a data store\u2014pharmacists\u2014and a transport layer\u2014the English language spoken over a telephone call\u2014and if you add a for loop, a cron job, and an SFTP upload to that then Google basically doesn't care about pharmacy chain IT anymore.</p><p>Repeat this by many other pairwise interactions between actors within an ecosystem, and we got leveraged impact through their ongoing operations, with a surprising amount of insight into (and perhaps some level of influence upon) policy decisions which your prior (and my prior) would have probably suggested \"arbitrarily high confidence that that is substantially above your pay grade.\"</p><p>We didn't have to be chosen by e.g. the White House as the officially blessed initiative. We just had to find that initiative and be useful to it. (Though, if\u2014God forbid\u2014I ever have to do this again, I would give serious consideration to becoming the national initiative <i>prior to asking for permission to do so</i> and then asking the White House whether the White House wanted credit or not.)</p><h2>Engaging with The System</h2><p>A recurring theme of our experience was taking a group of professionals who were mostly from tech and making ourselves palatable to public and private partners, many of which were... let's go with \"tech-skeptical\", for reasons described in the above piece.</p><p>The most boring takeaway is <strong>having a recognized 501c3 charity is extremely useful</strong> and this is far easier to get accomplished than one likely models it as. It isn't as click-button-pay-money as e.g. getting an LLC is, but if you can hire any staff, you can fairly deterministically achieve 501c3 status.</p><p>As all of our application docs are <a href=\"https://www.kalzumeus.com/charity/\">public</a> by <a href=\"https://www.law.cornell.edu/cfr/text/26/301.6104(d)-1\">law</a> feel free to take a gander if you need calibration on how many words are required or what level of sophistication they need to be at. The first cut of half of our words were written by a well-regarded law firm on a pro-bono basis. This would have cost less than $10k at rack rates. Having done it once I am fairly confident I could do it again with less than 2 days effort and no formal legal training.</p><p>501c3 status directly unblocked several funders (principally, Donor Advised Funds that were effectively restricted from donating to charitable initiatives without legible status) and, probably even more importantly, successfully promoted us to Probably Know What They Are Doing in the eyes of many governmental partners. (Having dealt with many charities over the years, I question the epistemic logic of that, but it seems like a useful preference to take notice of.)</p><p>The other thing which I think this community may perhaps underrate is the <strong>usefulness of a carefully planned comms strategy</strong>. Partly due to the professional backgrounds of several members of our core team, and partly due to other considerations, we ran a very tight PR ship and intentionally caused a coordinated outburst of positive press early in the life of the project. This was partly designed to help recruit users for our site.</p><p>It turned out that the characteristic \"has good press\" caused several parties whose collaboration we desired to be sharply more willing to do business with us. It was cited by Google as one reason they were willing to engage in conversations with us to use our results as authoritative-enough-for-inclusion-in-Google-products. It helped get government partners in California and elsewhere over the engagement line; we then were able to demonstrate sufficient expertise and utility to get to partnerships, on a spectrum of informal to quite formal.</p><h2>Brief commentary on career planning for long-termists</h2><p>I have minimal background in policy and public health. (The closest professionally relevant experience, and it is a stretch, is that I was the HIPAA Compliance Officer at a two person software company, so that we could sell our things to doctors/dentists as well as other businesses.) We ended up with a team of perhaps two dozen core members, and of that I would say one person had material policy experience and perhaps 2-3 had substantial professional experience adjacent to healthcare. I am unaware of anyone who worked on pandemics or public health prior to working with us.</p><p>And yet we were, by a fairly considerable margin, much more effective than many similarly situated charitable and public initiatives which appear to have people who put their skill points into getting good at public health and/or policy engagement.</p><p>I think that counsels some perhaps counterintuitive things for people who are wondering how to maximize impact on a 10+ year time horizon vis, I don't know, direct intervention against pandemics.</p><p>So what was abnormally useful from our professional backgrounds? Speaking for myself:</p><ul><li>sufficient financial wherewithal to commit early and boldly to the project even if it exposed me to temporary career risk</li><li>sufficient social capital to call in favors with various actors within the tech community and get commitments to resources or other things we needed</li><li>good understanding of the \"general factor of infrastructure\"; in particular, I think if you did a Venn diagram between \"understands operations of call centers\" and \"understands information flow between IT systems in multi-actor ecosystems\" I think I'd likely be more skilled than 99.999% of all government employees. (I was not unique on the team in having this curious confluence of interests, which appears much more common in healthcare tech than in public health administration.)</li><li>capability to write good software quickly continues to be a superpower and more people should opt-in to having it available to them</li><li>as discussed previously, PR seemed to be anomalously important for us</li></ul><p>Another useful thing, and a difficult one to measure, is placement in a social graph with a sufficiently high density of high agency people. VaccinateCA would likely not have happened but for Karl Yang specifically taking a tweeted suggestion from me and spinning up a Discord server then inviting ten close friends. He has very interesting friends, who have interesting friends, and a sufficient number of them have access to the social script \"Hey it is 9 PM and we're going to OSS Californian vaccine availability by tomorrow morning; are you in?\"</p><p>A thing I really wish we had had available: skilled, experienced people managers. I am not one; many members of the team would likely have benefitted from having one, having come up in professional environments where expectations would be clearly communicated to them and tasks assigned by one. (We got through this by main force and heavily relying on experience various team members had in OSS environments and volunteering environments, which were not always perfect matches for what we needed but beat having no experience at leading large-scale collaborative projects. I joke that my only leadership experience was running a WoW guild, but that is in no way a joke, and in important ways we resembled a WoW guild more than most participants probably realized.)</p><p>Also, in clarity of hindsight, I should definitely have fired myself from admin almost <i>immediately </i>in favor of someone shaped something like an office manager or bookkeeper. It was low leverage and an extraordinary tax on cycles that could have been more productively spent on fundraising, strategy, or partner negotiations.</p><h2>Expression of thanks for extended EA community</h2><p>VaccinateCA was not explicitly an EA project. I believe a few volunteers consider themselves members of your movement/community; I do not consider myself a member and do not know that to be true of any of our organizers. None of our funders, to my knowledge, would be broadly acknowledged as an \"EA funder.\"</p><p>But recent events have not been particularly kind to the brand perception of EA, and as someone who cares no small amount about brand perception but a much larger amount for the truth, I think that VaccinateCA would have been unlikely to happen but for the work of this community and some of your one-hop-out intellectual peers.</p><p>(If I had to point at any one artifact in particular, <a href=\"https://equilibriabook.com/\">Inadequate Equilibria</a> is a good articulation of a larger memeplex that made me comfortable with \"If the evidence of a system's operation contradicts what the Efficient Market Hypothesis counsels is the probable functioning of the system, <i>trust the evidence</i>. Thousands of lives savable by one dedicated team of non-specialists is <i>actually not all that low probability</i>.\")</p><p>This is not an impression colored by recent events or desire to say something nice to y'all. If I can quote from a memo I wrote on Day 6, asking for the other organizers to agree to the urgent necessity to intensify our operations (from \"OSS weekend hack\" to \"we should become a non-profit, raise millions, then become the national infrastructure provider here\") and appoint me CEO to start doing it:</p><blockquote><p>As a gross approximation, bringing 1 dose to 1 person 1 day earlier saves 10^-4 lives. (Or, to put it another way: 10k dose-days = 1 life saved)</p><p>I have a complicated values system, but in this it is really simple: I am a consequentialist with regards to saving lives, and at the margin I will favor that almost over every goal.&nbsp;<strong>I think we have a small chance today, but a real chance, of saving tens or hundreds of thousands of lives.</strong></p><p>1% * 10k lives = expected value of 100 lives saved. That seems a very reasonable first approximation as of Day 6. That\u2019s everyone who was at my wedding. They\u2019re not my family, coworkers, or friends, but they are somebody\u2019s family, coworkers, or friends, and every one of theirs is a life equal in value to ours.</p><p>What would I do to save everyone at my wedding?&nbsp;<i><strong>Absolutely anything</strong></i>.</p></blockquote><p>(Formatting is true to original.)</p><p>I consider myself fairly well-educated and well-raised in a moral tradition that has spent a lot of brainsweat on questions like \"What is one's duty to society and to one's fellow man?\" In that moral tradition, presented with the narrow question of \"Given that one is in a position of authority and has a course of action available to save hundreds of lives, what is one's duty?\", the answer is so straightforward as to be uninteresting.</p><p>But nobody, not once in my life, drew out the implication regarding expected value math until you all did.</p><p>For this you have my eternal gratitude.</p><p>And, should circumstances ever find you or yours looking at an expected value calculation that rhymes with the above, know that you'd have my instant attention and (pending thinking through it, in the words of a well-known articulator of my moral tradition) you have my sword.</p>", "user": {"username": "patio11"}}, {"_id": "bsbMPotiwyL4H3mH6", "title": "Rethink Priorities is hiring: help support and communicate our work", "postedAt": "2022-12-09T17:07:55.882Z", "htmlBody": "<h1>TL;DR</h1><ul><li>The Operations Department is hiring a&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/79560\"><u>Chief of Staff</u></a>, <a href=\"https://careers.rethinkpriorities.org/en/jobs/79569\"><u>Development Professional</u></a>, and <a href=\"https://careers.rethinkpriorities.org/en/jobs/79573\"><u>Communications Strategy Professional</u></a> (applications close January 8, 2023).&nbsp;</li><li>The Development and Communications team will hold a&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfirChbR5mwmZ-Vp8dzJOd4SfHxJfkWT09zFz44zwUGp7limQ/viewform\"><u>Q&amp;A</u></a>&nbsp;<a href=\"https://us02web.zoom.us/j/85256964239?pwd=eW5KeitkVUYrc2FheUFpTld1ZXVxZz09#success\"><u>webinar</u></a> on its job openings on December 15 at 11 am EST.&nbsp;</li><li>We\u2019re also accepting applications to join our&nbsp;<a href=\"https://careers.rethinkpriorities.org/en/jobs/78036\"><u>Board of Directors</u></a> until January 13.&nbsp;</li><li>Visit our&nbsp;<a href=\"https://careers.rethinkpriorities.org/\"><u>Careers Page</u></a> for more information and to apply.</li></ul><h1>Background</h1><p>Rethink Priorities (RP) has grown significantly, hiring&nbsp;<a href=\"https://rethinkpriorities.org/news/2022-new-staff\"><u>32 people</u></a> in 2022 and completing ~60 research projects. In addition to our ongoing animal welfare research, we scaled our teams addressing global health and development, AI governance and strategy, and general longtermism. We also worked on pioneering initiatives like the&nbsp;<a href=\"https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw\"><u>Moral Weight Project</u></a>, ran message testing, coordinated forums, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SsZkuYHv4dNfu7vnS/rethink-priorities-special-projects-team-is-hiring\"><u>incubated new projects</u></a>. In 2023, we intend to continue driving progress on global priorities by accelerating priority projects and further increasing the effectiveness of others\u2019 work. We also intend to launch a Worldview Investigations team.&nbsp;</p><p>More on RP\u2019s ambitious plans can be found in this post on our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Liphmkodcu7XPDKfK/rethink-priorities-2022-impact-2023-strategy-and-funding-1\"><u>impact, strategy, and funding gaps</u></a>.&nbsp;</p><h1>Open positions</h1><p>Strong operations and good governance are integral to RP\u2019s success as an organization. To help us scale and have an impact, we are opening the below new roles.&nbsp;</p><p>All positions are remote and may require collaborating with staff in multiple time zones using Google Workspace, Asana, Slack, and other technologies.&nbsp;</p><h2><a href=\"https://careers.rethinkpriorities.org/en/jobs/79560\"><u>Chief of Staff</u></a>&nbsp;</h2><ul><li><strong>Salary:&nbsp;</strong>$117,000 to $122,000 USD annually (pre-tax)</li><li><strong>Summary:&nbsp;</strong>Work closely with the COO, operations and HR leads, and the Directors of the Special Projects and the Development &amp; Communications teams, overseeing high-level initiatives and ensuring projects stay on track across the organization</li><li><strong>A good fit for someone:</strong><ul><li>Who understands nonprofit operations (finance, HR, project management, event planning, fundraising, communications, and legal compliance)</li><li>With excellent organization and project management skills and attention to detail&nbsp;</li><li>Who is comfortable working with confidential information and on multiple projects</li><li>Based in the US or UK who is able to attend meetings during working hours between UTC-8 and UTC+3 time zones and travel 5-7 weeks per year</li></ul></li><li><strong>Deadline:&nbsp;</strong>January 8, 2023</li></ul><h2><a href=\"https://careers.rethinkpriorities.org/en/jobs/79569\"><u>Development Professional</u></a></h2><ul><li><strong>Salary:&nbsp;</strong>$80,155 to $115,235 USD annually (pre-tax)</li><li><strong>Summary:&nbsp;</strong>Help RP to grow sustainably by strengthening relationships with existing donors as well as prospecting and cultivating relationships with new donors with the capacity to give at least $100,000/year toward our annual budget of ~$10 million</li><li><strong>A good fit for someone:</strong><ul><li>With experience in a development role or similar position</li><li>With existing networks in EA and adjacent communities, especially with funders</li><li>Who is intellectually curious, open-minded, resourceful, creative, and good at communicating (especially verbally/interpersonally)</li><li>Who is able to think strategically and exercise good judgment in identifying new sources of funding</li></ul></li><li><strong>Deadline:&nbsp;January 8, 2023</strong></li></ul><h2><a href=\"https://careers.rethinkpriorities.org/en/jobs/79573\"><u>Communications Strategy Professional</u></a></h2><ul><li><strong>Salary:&nbsp;</strong>$84,540 to $115,235 USD annually (pre-tax)</li><li><strong>Summary:&nbsp;</strong>Help RP to have an impact by mapping and identifying the most effective ways to target and engage our external audiences (e.g. researchers, nonprofit organizations, funders, and policymakers)&nbsp;</li><li><strong>A good fit for someone:</strong><ul><li>With experience developing a communications plan&nbsp;</li><li>Who understands EA and longtermism and is able to convey complex topics to different audiences in accessible yet nuanced ways</li><li>Who is intellectually curious, open-minded, resourceful, creative, and good at communicating (especially in writing and with a sense of visual aesthetics)</li><li>Who is able to think strategically and exercise good judgment in identifying mediums of communication to use or to eliminate or deprioritize</li></ul></li><li><strong>Deadline:&nbsp;January 8, 2023</strong></li></ul><h2><a href=\"https://careers.rethinkpriorities.org/en/jobs/78036\"><u>Board of Directors</u></a></h2><ul><li><strong>Salary:&nbsp;</strong>Voluntary for 3-10 hours/month&nbsp;<i>or&nbsp;</i>$40.53/hour for 5-10 hours/week&nbsp;&nbsp;</li><li><strong>Summary:&nbsp;</strong>Ensure that our senior management is making responsible, legal, and risk-aware decisions for the organization in the long-run&nbsp;</li><li><strong>A good fit for someone:</strong><ul><li>Who has knowledge/experience in longtermism, launching/supporting new ventures, and/or scaling organizations</li><li>With professional legal or nonprofit finance experience</li><li>Who is able to take on tasks such as evaluating senior management (note: while the majority of our board is required by our bylaws to be unpaid, we would like to pay up to two members for more intensive work; see more&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4Y3NKH37S9hvrXLCF/apply-to-join-rethink-priorities-board-of-directors\"><u>here</u></a>)&nbsp;</li></ul></li><li><strong>Deadline:&nbsp;January 13, 2023</strong></li></ul><h1>Application process</h1><p>Our descriptions of who would be a good fit outline the types of experience and skills that would likely make someone a strong candidate. However, we weigh candidates\u2019 anonymized answers to our application prompts more heavily than their CVs. So, if you think that you would be a good fit for one of the above roles, we encourage you to seriously consider applying even if you don\u2019t perfectly match our descriptions!</p><p>We promise to be transparent and communicative and make the application process as lean and fair as possible.</p><p><strong>Apply:&nbsp;</strong>Visit our&nbsp;<a href=\"https://careers.rethinkpriorities.org/\"><u>Careers Page</u></a> for the full job descriptions and to apply.</p><p><strong>Contact:&nbsp;</strong>Comment below or email <a href=\"mailto:careers@rethinkpriorities.org\">careers@rethinkpriorities.org</a> with any questions.&nbsp;</p><p><strong>Webinar: </strong>The Development and Communications team will hold a virtual Q&amp;A on their openings on December 15th from 11:00 am - 12:00 pm EST. Submit your questions <a href=\"https://forms.gle/qUNaJPq2GSET7Xgm8\">here</a> and join the webinar <a href=\"https://us02web.zoom.us/j/85256964239?pwd=eW5KeitkVUYrc2FheUFpTld1ZXVxZz09\">here</a>.&nbsp;</p>", "user": {"username": "Rachel"}}, {"_id": "ep63ciArvGcb3mFJt", "title": "Inequality, well-being, and the problem of the unknown reporting function", "postedAt": "2022-12-09T16:28:04.107Z", "htmlBody": "<p>I expect some Forum readers will be interested in a couple of recent PNAS papers that discuss new <i>causal </i>evidence on the link between income and wellbeing.</p><ul><li>Dwyer, R. J., &amp; Dunn, E. W. (2022). <a href=\"https://www.pnas.org/doi/abs/10.1073/pnas.2211123119\">Wealth redistribution promotes happiness</a>. <i>Proceedings of the National Academy of Sciences</i>, <i>119</i>(46), e2211123119.</li><li>Kaiser, C, &amp; Oswald, A. J. (2022). <a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119\">Inequality, well-being, and the problem of the unknown reporting function</a>. <i>Proceedings of the National Academy of Sciences</i>, <i>119</i>(50), e2217750119</li></ul><p>I recommend reading both papers in full (they are short and open-access). In this link post, I provide a few quotes from Kaiser &amp; Oswald's commentary to highlight the importance and limitations of the Dwyer &amp; Dunn findings.</p><p><i>Disclaimer: Ryan Dwyer is my colleague at the Happier Lives Institute and Caspar Kaiser is one of our trustees but neither paper was produced by HLI.</i></p><h3>Introduction</h3><blockquote><p>Every politician, in every nation and in every era of history, eventually has to face a complex and emotive question. Should I try to redistribute money from my richer citizens to my poorer citizens? If so, by how much? This is a timeless issue. The appropriate answer to the question turns crucially on a claim that goes back hundreds of years to, for example, the philosopher Jeremy Bentham: \u201cAll inequality is a source of evil\u2014the inferior loses more in the account of happiness than by the superior is gained.\u201d (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r1\">1</a>) In an ideal world, a hypothesis of this sort would be tested in a giant randomized controlled trial (RCT), perhaps funded by a body such as the National Science Foundation of the United States. However, no funding body is likely to provide the necessary millions of dollars to run that experiment-until now. In a remarkable and important contribution to conceptual science and practical public policy, Ryan Dwyer and Elizabeth Dunn (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r2\">2</a>) have\u2014with the help of millionaire donors\u2014run an RCT that comes close to that ideal.</p></blockquote><h3><strong>The Breakthrough Contribution of Dwyer and Dunn Is to Place This Association on Firm Causal Foundations</strong></h3><blockquote><p>Dwyer and Dunn create an experiment in which assignment to treatment is random. The authors\u2019 work is an example of a more general movement in modern social science in which earlier correlational research is checked with experimental and quasiexperimental designs (e.g., ref. <a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r3\">3</a>). They add also to an emerging causal literature on cash transfers and well-being in low- and middle-income countries (summarized in, e.g., ref. <a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r4\">4</a>). Dwyer and Dunn deliberately field the same kind of cash transfer in several different economic contexts. In this way, they connect two kinds of literature and demonstrate that cash transfers do indeed improve recipients\u2019 self-reported well-being across a wide variety of settings. There are large causal effects that persist over at least a 6-mo period.</p></blockquote><h3><strong>Three Scientific Complications Now Stand Out</strong></h3><blockquote><p>One complication\u2014formally recognized more than half a century ago (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r6\">6</a>), but it continues to be hotly debated in political life\u2014is that tax-funded redistribution may distort incentives and thereby dampen economic growth. If so, redistribution could act to raise well-being via a more equal income distribution but at the same time could lower well-being by decreasing the size of the \u2018pie\u2019. Assessing the relative importance of these countervailing forces\u2014as discussed in conventional economics courses\u2014remains a priority.</p><p>Loss aversion is a second complication. Income losses are known to loom larger than gains. In the short term, therefore, the well-being losses of those who are taxed may exceed the gains of net recipients. Based on earlier work (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r7\">7</a>), Dwyer and Dunn suggest that voluntarily giving money away can increase people\u2019s well-being. Whether that is also true in the case of forced redistribution is an open question. It may eventually be possible to answer that by building on current studies of loss aversion (such as refs. <a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r8\">8</a> and <a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r9\">9</a>).</p><p>It clearly seems as though Dwyer and Dunn give causal evidence for a curved well-being shape. Yet, as they mention in passing, although they do not elaborate, their paper depends on an untested assumption. It is that the relationship between <u>reported</u> well-being and the underlying <u>actual well-being</u> is linear. Whilst scattered work hints at possible linearity (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r10\">10</a>\u2013<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r13\">13</a>), nobody currently knows whether that is true. It depends on how humans use language when they answer happiness kinds of questions. This is a particular version of a generalized difficulty outlined in a recent piece by Bond and Lang (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r14\">14</a>) and previously discussed in Oswald (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r12\">12</a>). The earlier literature on psychophysics also grappled with this (<a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r15\">15</a>). Without knowledge of the \u2018reporting function\u2019, we cannot be sure that we can treat well-being data as ratio-scale measurements, which is required for statements like \u201cthree times more happiness\u201d.</p></blockquote><h3>Conclusion</h3><blockquote><p>We would like to emphasize that our instinct is that the authors\u2019 finding is correct and that Dwyer and Dunn have written a superb paper of lasting importance. Further research in this area will still be appropriate. It may include qualitative work, perhaps with in-depth interviews on respondents\u2019 scale-use, as well as quantitative work that would systematically map subjective responses to observable cardinal quantities (as in refs. <a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r12\">12</a> and <a href=\"https://www.pnas.org/doi/10.1073/pnas.2217750119#core-r15\">15</a>). Currently, the reporting-function problem is fundamental, little recognized, and so far unsolved.</p></blockquote>", "user": {"username": "BarryGrimes"}}, {"_id": "cqks7caixyWsGbci9", "title": "Simulators and Mindcrime", "postedAt": "2022-12-09T15:20:43.403Z", "htmlBody": "", "user": {"username": "Dragon God"}}, {"_id": "ogFGCHttnpcsnne4y", "title": "How Founders Pledge's Patient Philanthropy Fund and Global Catastrophic Risks Fund Work Together", "postedAt": "2022-12-09T15:36:32.476Z", "htmlBody": "<p>Founders Pledge <a href=\"https://founderspledge.com/stories/introducing-the-global-catastrophic-risks-gcr-fund\">recently launched</a> the new <a href=\"https://founderspledge.com/funds/global-catastrophic-risks\">Global Catastrophic Risks (GCR) Fund</a> as the latest addition to our <a href=\"https://founderspledge.com/funds\">fund offerings</a>. Our Funds, which serve as philanthropic co-funding vehicles that pool donors\u2019 money for high-impact giving, now include:</p><ul><li><a href=\"https://founderspledge.com/funds/climate-change-fund\">The Climate Change Fund</a>;</li><li><a href=\"https://founderspledge.com/funds/global-catastrophic-risks\">The Global Catastrophic Risks (GCR) Fund</a>;</li><li><a href=\"https://founderspledge.com/funds/global-health-and-development\">The Global Health and Development (GHD) Fund</a>; and</li><li><a href=\"https://founderspledge.com/funds/patient-philanthropy-fund\">The Patient Philanthropy Fund (PPF)</a>.</li></ul><p>These Funds represent a diversity of causes and worldviews. The PPF and GCR Fund share the goal of mitigating large-scale risks to humanity. The two Funds differ, however, in their approach to this goal.</p><p>This blog post explains the differences \u2014 and, importantly, the complementarity \u2014 between the PPF and the GCR Fund.</p><h2>Differences between the Funds</h2><p>The first and most basic difference between the two Funds is their giving timelines. Whereas the GCR Fund will give on a rolling basis, the PPF is an experiment in investing to give \u2014 growing our resources to give at highly impactful times in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflvc8834mp9\"><sup><a href=\"#fnlvc8834mp9\">[1]</a></sup></span>&nbsp;You can read more about this idea in our <a href=\"https://founderspledge.com/stories/investing-to-give\">Investing to Give report</a>.</p><p>There are a number of factors that will shape which timeline you prefer. The first factor is beliefs about future returns on investments. If you think these are likely to be especially high, then all else equal, you may prefer to give to the PPF over the GCR Fund \u2014 your money may have much more leverage in the future thanks to the compounding returns on investment.</p><p>The second factor is beliefs about the timelines of major risks and the near-term tractability of risk reduction measures. If you believe that most of the major threats to humanity \u2014 <a href=\"https://founderspledge.com/stories/great-power-conflict\">great power war</a>, nuclear weapons, catastrophic biological risks, and risks from artificial intelligence (AI) \u2014 are likely near-term threats, and that we can and ought to reduce them today, you may prefer the GCR Fund.</p><p>The third factor is the moral weight one places on future generations relative to current generations. <a href=\"https://founderspledge.com/stories/existential-risk-executive-summary\">Longtermists</a> generally believe that \u201cfuture people matter,\u201d and often that they matter just as much as people alive today. Others may place a heavier weight on the wellbeing of current generations. If you don\u2019t think there\u2019s a difference between helping someone today and helping someone 100 years from now who isn\u2019t even alive yet, then this is not a factor in choosing between the Funds. If you have a strong preference for helping people alive today or in the near future, you may prefer the GCR Fund (although some of its interventions will also seek to shape the long-term risk landscape).</p><p>Another factor is the extent to which you think major global problems in the future may require enormous resources. If you think, for example, that there could be some unpredicted crisis at some point in the future, and that solving this problem will require hundreds of millions of dollars at once, then you may want to consider helping to give the world an \u201c<a href=\"https://www.vox.com/future-perfect/2021/11/3/22760718/patient-philanthropy-fund-charity-longtermism\">insurance policy</a>\u201d via the PPF.</p><p>This question also touches on a cluster of related ideas: the <a href=\"https://globalprioritiesinstitute.org/existential-risk-pessimism-and-the-time-of-perils-david-thorstad-global-priorities-institute-university-of-oxford/\">time of perils</a>; the <a href=\"https://www.cold-takes.com/most-important-century/\">most important century</a>; and the <a href=\"https://theprecipice.com/\">precipice</a>. Some scholars of existential risk think that we are living in an unfortunate time \u2014 our science and technology are advanced enough that humanity can wipe itself out, but not advanced enough that we can develop effective countermeasures. If you think we are in such a time of perils, and that this time will not last long, you may prefer the GCR Fund. If you think the really dangerous time is further in the future, you may prefer the PPF. If, finally, you believe that we are in a time of perils, but that there may be many such times, or even that humanity is in a permanent <a href=\"https://en.wikipedia.org/wiki/Red_Queen%27s_race\">Red Queen\u2019s Race</a> (\u201cit takes all the running you can do, to keep in the same place\u201d) with existential risks, you may want to split your allocation between the two Funds.</p><p>This is a non-exhaustive list of differences. Fundamentally, the differences come down to uncertainty \u2014 financial uncertainty, moral uncertainty, and uncertainty about how the threat landscape is changing. This is why we believe that the GCR Fund and the PPF are fundamentally complementary as part of a balanced portfolio approach to high-impact philanthropy.</p><h2>Complementarity of the Funds</h2><p>Despite these differences, we believe that the GCR Fund and PPF complement each other.</p><p>First, we believe that one Fund could provide important lessons for the other. The GCR Fund will take an <a href=\"https://founderspledge.com/stories/introducing-the-global-catastrophic-risks-gcr-fund\">active grantmaking</a> approach, with the ability to seed new organizations, respond to dynamic opportunities, and collaborate with partner organizations. This will not only allow the GCR Fund to move quickly, but will also provide the PPF with leverage to intervene during especially crucial points in humanity\u2019s future.</p><p>Consider how the two Funds would respond if a pandemic emerged with the potential to cause a <a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/full-report/\">global catastrophe</a>. Such a threat would provide sufficient grounds for the GCR Fund to take action. At the same time, such a pandemic may also pose an <a href=\"https://80000hours.org/articles/existential-risks/\"><i>existential</i> risk</a> to humanity, and thus may be an <a href=\"https://www.bbc.com/future/article/20200923-the-hinge-of-history-long-termism-and-existential-risk\">especially influential moment</a> in the human story. In such circumstances, the PPF would also be motivated to act. Thanks to the GCR Fund\u2019s active grantmaking approach, insights from it can direct the PPF\u2019s resources towards the most important actors in the crisis. This demonstrates how one Fund can improve the quality of grantmaking from the other.</p><p>A second complement of the Funds is based on <a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\">worldview diversification</a>. As discussed above, we face many uncertainties when aiming to do the most good, such as empirical uncertainties (e.g. \"<a href=\"https://www.cold-takes.com/most-important-century/\">Are we living in the most important century?</a>\"), methodological uncertainties (e.g. <a href=\"https://www.cold-takes.com/forecasting-transformative-ai-whats-the-burden-of-proof/\">\"How should we forecast transformative AI?</a>\"), and moral uncertainties (e.g. <a href=\"https://whatweowethefuture.com/\">\"Do future people matter morally?</a>\"), among others. Under such profound uncertainty, it may not be wise to \"put all our eggs in one basket\". Instead, diversifying between an \"urgent <a href=\"https://founderspledge.com/stories/existential-risk-executive-summary\">longtermist</a>\" or \"current generations\" perspective (prioritizing threats today), and a \"<a href=\"https://80000hours.org/2020/08/the-emerging-school-of-patient-longtermism/\">patient</a>\" perspective (focusing on future threats), may make sense when each worldview appears highly plausible.</p><p>Diversifying between the PPF and GCR Fund comes at the expense of not fully maximizing expected value. For example, if Alice believes that the GCR Fund has even 2% more expected impact than the PPF, then it appears logical that she should invest 100% of her resources into the GCR Fund. However, such a view is unlikely to be <a href=\"https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">robust</a> given the high uncertainties within each bucket. Splitting one\u2019s portfolio may therefore make more sense.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefju5dctqqsh\"><sup><a href=\"#fnju5dctqqsh\">[2]</a></sup></span></p><h2>Example: Artificial Intelligence (AI)</h2><p>In short, we think many philanthropists who care about large-scale risks threatening humanity will likely want to allocate their giving between the two Funds. To make this more concrete, consider two startup founders who are looking to do the most good with their exit proceeds - let\u2019s call them Alex and Bo. Both of them are concerned about potential risks from artificial intelligence (AI), but differ in their key uncertainties about the issue.</p><p>Alex is concerned about both the potential for transformative AI in the near future, and the applications of AI to military systems to create <a href=\"https://founderspledge.com/stories/autonomous-weapon-systems-and-military-artificial-intelligence-ai\">autonomous weapon systems</a> that could destabilize <a href=\"https://founderspledge.com/stories/great-power-conflict\">great power relations</a>. Moreover, they think that both of these issues will rely on roughly the same machine learning paradigms, with highly advanced AI systems just requiring higher levels of computational power for training and deployment. Alex is broadly optimistic about current approaches to AI safety, like the approaches at the <a href=\"https://founderspledge.com/stories/center-for-human-compatible-ai-high-impact-funding-opportunity\">Center for Human-Compatible AI</a>, and moreover believes that the governance of narrow AI applications \u2014 e.g. through U.S.-China <a href=\"https://docs.google.com/document/d/1hZfuxAp4yhsjmYXBNHtHZxeYabVK1kwalTl711cyW0A/edit#heading=h.q0rxkrfa6lfa\">confidence-building measures</a> \u2014 will have beneficial effects for the <a href=\"https://forum.effectivealtruism.org/posts/isTXkKprgHh5j8WQr/strategic-perspectives-on-long-term-ai-governance\">long-term governance of AI</a>. They are confident in many of these beliefs, but open to being wrong about key parts. Alex therefore decides to allocate around 20% of their giving towards the PPF and 80% towards the GCR Fund.</p><p>Bo, on the other hand, thinks that future powerful AI systems are going to look fundamentally different from current machine learning models and may require different computational hardware, and that current approaches to AI safety may be of little value in the future. Moreover, Bo thinks that progress on transformative AI could surprise humans, leaving very little time to work on making sure such systems are safe \u2014 such work may require enormous resources. Bo is also confident about continuing returns on investments and values future people just as much as those alive today. Nonetheless, they think that avoiding a war between the U.S. and China is important in the near-term for future global cooperation on emerging technologies. Bo therefore allocates most of their money (80%) towards the PPF, but gives some (20%) to the GCR Fund.</p><h2>Example: Worldview Diversification</h2><p>Meanwhile, Carol, another fictitious character, may choose to diversify between the GCR Fund and PPF based on their deep uncertainty about what is best. For example, they may put a 50% chance on only existing people holding moral value, and that the best way to save those lives is by reducing catastrophic risks. At the same time, Carol might put 50% confidence in the view that most existential threats to the <i>long-term future</i> may lie in the next centuries, and that the long-term future is of overwhelming importance. Furthermore, it may be <i>extremely difficult</i> to resolve the uncertainties they have between these two views, given the many crucial considerations they need to resolve before coming to a conclusion. In such a state, Carol may give 50% of their resources to the GCR Fund for the benefit of present-day people, and invest the remaining 50% into the PPF, to safeguard the long-term future.</p><h2>Conclusion</h2><p>In summary, we believe that both the GCR Fund and PPF are excellent giving options. While they differ in giving timelines and underlying philosophy, they are also complementary to one another, offering multiple opportunities to tackle humanity\u2019s biggest challenges. More poetically: the GCR Fund will fight the many fires we see in the world today, while the PPF will fill up the tank to prepare for the fires of tomorrow.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlvc8834mp9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflvc8834mp9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although note that the Patient Philanthropy Fund will make small grants (&lt;1% of the total Fund) on an annual basis, as outlined&nbsp;<a href=\"https://founderspledge.com/funds/patient-philanthropy-fund#:~:text=Each%20year%2C%20to%20build%20its%20grantmaking%20infrastructure%20and%20track%20record%2C%20the%20Fund%20will%20make%20small%20grants%20at%20less%20than%201%25%20of%20its%20size%20to%20the%20best%20existing%2C%20pre%2Dvetted%20longtermist%20funding%20opportunities%20we%20are%20able%20to%20find%20in%20that%20year.\"><u>here</u></a>. For instance, in 2022, the Fund made a&nbsp;<a href=\"https://founderspledge.com/stories/explaining-a-small-ppf-grant-on-nuclear-security\"><u>small grant on nuclear security</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnju5dctqqsh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefju5dctqqsh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;For more discussion on philanthropic diversification, see&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/philanthropic-diversification\"><u>here</u></a></p></div></li></ol>", "user": {"username": "22tom"}}, {"_id": "3EWpLid8tkyYJakfm", "title": "Announcing BlueDot Impact", "postedAt": "2022-12-09T16:45:47.762Z", "htmlBody": "<p><a href=\"https://bluedotimpact.org/?utm_source=EA+Forum&amp;utm_medium=BDI+announcement+post&amp;utm_campaign=BlueDot+Impact\">BlueDot Impact</a> is a non-profit project running courses that support participants to develop the knowledge, community and network needed to pursue high-impact careers.</p><figure class=\"image image_resized\" style=\"width:51.69%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/relnueg9qyfidsrnrstl\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/fvfgw7v7mkaaa5dhf5da 430w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/fi59dahvjimbdp7tq8cs 860w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/dueqiuvrgesoj8okvahr 1290w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/wrr9xyrwsgswk3g1wowp 1720w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/kswfzpuriu9kpyoxy7hl 2150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/k9ucesxwm7gxszzpi15j 2580w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/ube2uzbkt3uzuyfyd09m 3010w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/lhyh3r0tde4shyqyhmxk 3440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/tuvwlvl8661zi7s6e53i 3870w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3EWpLid8tkyYJakfm/jj4es6w8unotehrwkpmd 4288w\"></figure><p>The purpose of this new organisation is to increase the number of people working on solving some of the world\u2019s most pressing problems in an informed way. We do this by building professional, scalable and high-quality courses. We aim to give participants the opportunity to engage deeply and critically with the literature on particular high-impact fields, meet and collaborate with others interested in the topic, and build an understanding of the opportunities to pursue a career in the space.</p><p>We believe there are many great people who are interested in working on important problems and have the skills to contribute, but want additional support to take the career-switching leap. By bringing together communities of such people to engage with material on the field over a sustained period and explore the range of career opportunities available to them, we believe they are more likely to make this leap.</p><p>BlueDot Impact was founded by the team who was primarily responsible for running previous rounds of the AGI Safety and Alternative Protein Fundamentals courses. We created a new organisation so that we could focus solely on the courses.</p><p>We\u2019re really excited about the amount of interest in the courses, and think they have great potential to build awesome communities around key issues. As such we have spent the last few months:</p><ul><li>Working with pedagogy experts to make discussion sessions more engaging</li><li>Formalising our course design process with greater transparency for participants and facilitators</li><li>Building systems to improve participant networking to create high-value connections</li><li>Collating downstream opportunities for participants to pursue after the courses</li><li>Forming a team that can continue to build, run and improve these courses over the long-term</li></ul><p>Learn more about BlueDot Impact <a href=\"https://bluedotimpact.org/?utm_source=EA+Forum&amp;utm_medium=BDI+announcement+post&amp;utm_campaign=BlueDot+Impact\">here</a>, and register your interest for future rounds of our courses <a href=\"https://airtable.com/shr41jcUXUc62n1SM?utm_source=EA+Forum&amp;utm_medium=BDI+announcement+post&amp;utm_campaign=BlueDot+Impact\">here</a>.</p>", "user": {"username": "dewierwan"}}, {"_id": "xCuKTeDfmuStcJaxJ", "title": "Does Sentience Legislation help animals?", "postedAt": "2022-12-09T10:58:43.223Z", "htmlBody": "<h1>EXECUTIVE SUMMARY</h1><p>The sentience of animals has long been recognised and has continued to be demonstrated on ever firmer scientific grounds. From the Brambell report (1965), which emphasised the importance of sentience in the understanding of animal welfare to the Cambridge Declaration of Consciousness (2012), which suggested widespread scientific acceptance of the idea&nbsp;(Rowan&nbsp;<i>et al.</i> 2021). In recent years, this widespread and scientific belief has been explicitly recognised in legislation in a growing number of countries and jurisdictions.&nbsp;</p><p>It is certainly crucial that animal sentience be recognised in this way, as it is the most widely accepted basis for the inclusion of animals as moral patients. However, there is another question as to the value of the kind of legislation recognising the sentience of animals that we see in many different countries. This legislation explicitly recognises their sentience, though many other pieces of legislation could be thought to already implicitly do so.&nbsp;</p><p>In our research helping organisations prioritise among different potential asks, we have considered the value of animal sentience legislation in many contexts. This report analyses the value of this legislation in terms of its current and future impact on animals.</p><p>However, despite the apparently high-minded language recognising animal sentience in legislation, it is often accompanied by quite little direct and immediate change for animals. In some cases the legislation is accompanied by some specific statements about what the recognition of animal sentience is taken to directly imply, but there is typically little of this, leaving the legislation to largely be a symbolic statement of pledged values, leading to some concerns that it may be ineffectual legislation that leads to complacency. This type of humane washing remains our biggest concern with the legislation, but we think it is unlikely that it makes the ask net negative.</p><p>In the case of the EU and New Zealand legislation, the intention behind the legislation as purely symbolic has been publicly stated, though this intention does not foreclose the possibility that animal advocates are able to draw some future victories from the legislation. In other cases such as Oregon and Qu\u00e9bec we have seen some court cases that have successfully leveraged the legislation to push against the treatment of animals as property, though any significant improvements to animal welfare have yet to be seen.</p><p>The most successful case so far has been that of the UK, because it promises to establish a committee to make sure that government decisions give due consideration to animal sentience. Further, it includes cephalopods and decapod crustaceans, and there is some chance this will lead to further protection for these animals. However, the head of the sentience committee does not seem like the appropriate choice to ensure its independence because of his conflict of interest as a farmer.&nbsp;</p><p>Despite the absence of direct effects so far, sentience legislation so far has some plausibility as being instrumental in the long-term strategy of the movement. This makes assessing the value of this ask quite difficult, since this potential long-term importance is much more difficult to evaluate.</p><p>Overall, our best estimate is that this ask has modest strength. In other words, with significant uncertainty, we think that the impact is fairly small compared to our top asks. We do not think that there is a risk of this ask being net negative, though we strongly recommend that organisations try to push to get sentience legislation to have concrete protections, so that it is more than symbolic and this risk is minimised. The strength of the ask will of course also vary by country context, and we could imagine it being competitive in certain cases.</p><p>This report draws mainly from case studies and existing philosophical and legal research. Expert interviews were also conducted. A more in-depth report could spend more time analysing parallels of potentially symbolic legislation (like declarations of climate emergencies).&nbsp;</p><h1>THE THEORY OF CHANGE OF A POLICY (REVIEW OF EXISTING LITERATURE)&nbsp;</h1><p><a href=\"https://app.diagrams.net/?page-id=kgpKYQtTHZ0yAKxKKP6v&amp;scale=auto#G1votwS0PkAmOpc7H4_aF3w3idYG4gqLXK\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1671007588/mirroredImages/xCuKTeDfmuStcJaxJ/hzxq797qushoca8pjvbi.png\"></a></p><p>Thicker lines indicate stronger effects&nbsp;</p><p>Animal protection laws could be seen to at least implicitly recognise animal sentience, since there would be little reason to protect animals otherwise&nbsp;(Kotzmann and Stonebridge 2021). Whilst there are alternative definitions of welfare, such as welfare as biological productivity, many definitions of welfare refer to sentience, and so talk of animal welfare is premised on animal sentience according to these prominent uses of the terms.</p><p>Animal protection legislation that goes further than specific restrictions and demands more general duty to minimization of pain or suffering could also be seen to more directly assume the sentience of animals, since under many (but not all) definitions of these terms, an animal who can experience pain or suffering is thereby sentient.&nbsp;</p><p>There has also been recognition of animal sentience in various official channels, such as court decisions and government reports, that perhaps fall short of a full explicit legislative acknowledgement of sentience&nbsp;(Rowan&nbsp;<i>et al.</i> 2021). One example of this is the UK \u2013 Australia Free Trade Agreement which acknowledges the sentience of animals, even though Australian legislation itself does not do so&nbsp;(UK Government 2021). The implication of these minor recognitions of animal sentience is not clear, but it is probably not greatly significant.</p><p>There is a recent trend to go further than this and explicitly recognise the sentience of animals in legislation&nbsp;(Kotzmann and Stonebridge 2021). Many countries have done this to some extent, but a precise list is difficult because definitions of what it means to recognise sentience vary, and so other writers may have somewhat different lists.&nbsp;</p><p>The countries that we consider to have legislation recognising the sentience of animals are Chile, Colombia, Peru, Tanzania, New Zealand, the United Kingdom, and all EU countries&nbsp;(Rowan&nbsp;<i>et al.</i> 2021; Kotzmann and Stonebridge 2021). Indonesia, Norway, Switzerland and Turkey do not recognise animal sentience as such, but refer to animals having mental states as well as physical states&nbsp;(Rowan&nbsp;<i>et al.</i> 2021). It has also been recognised in territories including the Canadian province of Qu\u00e9bec; the US districts of Oregon, Vermont, Maine, and Washington DC, as well as the Australian state of Victoria and the Australian Capital Territory&nbsp;(\u201cAustralian Capital Territory Enacts New Law Recognizing Animal Sentience\u201d 2019; Kotzmann and Stonebridge 2021; VSD 2022).&nbsp;&nbsp;</p><p>A number of these countries, such as the European countries and New Zealand, have among the highest animal welfare standards in the world. This suggests a correlation (though not necessarily causation) between the recognition of sentience and broadly high animal welfare standards.</p><p>If the sentience of animals is already&nbsp;<i>implicit</i> in much existing legislation, it naturally raises the question of what is the value of this&nbsp;<i>explicit</i> recognition? The most plausible answer to this is that it brings this rationale into the open where it can be explicitly evoked to claim that current protections do not go far enough and imply more expensive protections.&nbsp;</p><p>Moreover, sentience legislation is often worded in such a way that explicitly treats animals in a way that, in some respects, deviates from the normal paradigm of animals as property. Judicial interpretations of the legislation in many jurisdictions (as seen in the \u201c<strong>Case Studies</strong>\u201d section) have cemented this interpretation.</p><h2><strong>Sentient beings, yet still property</strong></h2><p>Whether animals are considered property is hugely relevant according to some animal advocates, such as Gary Francione. According to them, it represents the root cause of their exploitation and suffering. He therefore thinks that the fundamental right that animals need is the right not to be treated as property&nbsp;(Francione 2004).</p><p>In contrast, Cass Sunstein&nbsp;(Sunstein and Nussbaum 2004) suggests that \u2018property\u2019 is just a label for possible legal statuses, without inherent significance in itself. He suggests that we do not always get to do as we please with all of our property, instead there may be substantial restrictions placed on use of our property (such as with objects of historical importance), without changing the status of property. Furthermore, children have dramatically reduced autonomy compared to adults, yet are still regarded as persons rather than property. This shows that it is possible for the rights of something and our duties towards it to vary substantially independently of whether that thing is \u2018property\u2019 or not.</p><p>One key example of how sentience legislation has been largely symbolic is how animals are typically still regarded as property in those countries or areas that recognise them as sentient&nbsp;(Sowery 2018; Kotzmann and Stonebridge 2021). The law may recognise some exceptions to their status as property, for example the exceptions that allow them to be rescued in violation of normal property norms when they need medical attention, but in most respects they are still treated as property. The legislation in some areas refers to them as being more than property, but in practice this is largely symbolic because (at least so far) there is little practical consequence to this lofty language&nbsp;(Ferrere 2022). Nevertheless, the traditional interpretations are plausibly pushing at the boundaries of the view of animals as property.</p><h2><strong>Is sentience legislation aspirational?</strong></h2><p>A further question concerning sentience legislation is how much it can allow us to continue to push these boundaries. Considering primarily the 2015 animal sentience legislation in New Zealand, Ferrere&nbsp;(2022) argues that because the legislation is just a statement of (widely recognised) scientific fact and does not include a statement of how they should be interpreted to guide our conduct, moving forward, it is not aspirational. He therefore will not be very helpful in securing future victories for animal advocacy.&nbsp;</p><p>Note that he does still think that this curtails the legal benefits of the legislation; he believes that the legislation can have some impact as a signal. In other words, they can add strength for the cause by being seen as a significant commitment to animal welfare and rights and a sign that the government wants more work done on this issue.</p><p>This interpretation seems to be at least partially contradicted by cases where the legislation seems to be more somewhat broadly and ambitiously interpreted by judges and by government, such as the cases in Oregon that saw some property rights in animals suspended. However, we should note that none of these cases have yet secured large, concrete welfare gains for animals and significantly more ambitious interpretations of this legislation will need to happen if we are to begin to see significant impact for animals. Ultimately, it is a difficult matter of legal interpretation, and while most scholars on the subject seem to be more optimistic than Ferrere, his interpretation remains plausible.</p><h2><strong>A change in the advocacy landscape?</strong></h2><p>Though the legislation would be better if it protected animals more concretely, the symbolic nature of animal sentience legislation has one strength, which is that the lack of immediate consequences for animals may make it much easier for countries to accept it. The legislation may appear innocuous because it is just a public acknowledgement of implicit values and so might be accepted much earlier than could be possible for stronger legislation&nbsp;(Ferrere 2022). But once accepted, it may serve as the basis for challenges based on the full meaning of the terms and the inconsistency between those lofty terms and current practices.&nbsp;</p><p>There may then be some benefit in the fact that the legislation does not spell out its exact implications. This allows for future work by advocates to argue for stronger interpretations of this legislation than might be accepted at current times. The full reach of the legislation, when realised by later activists, may be stronger than we might otherwise be able to pass through legislative means.&nbsp;</p><p>In this way it can help future advocacy efforts by placing advocates and governments closer together, causing the government to use the same vocabulary as the advocates and therefore bringing their conceptual and moral frameworks closer together. Advocates no longer have to argue for the importance of sentience since this is already acknowledged in legislation. It offers an easier framework and ground from which to advocate for concrete welfare changes&nbsp;(Rowan&nbsp;<i>et al.&nbsp;</i>2021).&nbsp;</p><p>The ability to point out these inconsistencies can bring a change in the advocacy landscape, along with any clauses about how this should be taken into account in decision-making, and the extreme neglect or disregard for that sentience that the government allows in practice.</p><p>Sentience legislation may also prevent backsliding. Though this may not be an uncommon argument, the existence of the legislation prevents someone from defending the acquisition of animals on the grounds that animals are not sentient. They may be able to find some other rationale to use instead, but it limits the options available to them.</p><h2><strong>Precedent-setting</strong></h2><p>One argument for sentience legislation is that it may set a precedent that other countries or territories may follow. With many countries and jurisdictions now recognising the sentience of animals, others that view themselves as having leading animal welfare policies may feel significant pressure to adopt sentience legislation of their own. We can begin to see this with the case of New Zealand, which may have adopted its sentience legislation because it wanted to be seen as a world leader in animal welfare, and so it felt it was necessary to adopt the legislation&nbsp;(Ferrere 2022). Of course, this may not save the policy if it was ineffectual and purely symbolic in the first place, since it might only inspire further ineffectual legislation&nbsp;(Ferrere 2022).</p><p>The precedent of many other countries passing legislation to recognise animal sentience may make this ask significantly more tractable. Countries may believe that in order to compete with these countries' animal welfare standards, they should implement this legislation. This may be what happened in the case of New Zealand.</p><p>Countries that are stronger on animal welfare, but have not already adopted animal sentience legislation, might be the best candidates for this legislation because they may be most likely to use the legislation to advance reforms. However, there are no countries that clearly meet this description that have not already adopted animal sentience legislation.</p><p>One potentially strong candidate for this legislation is Australia, since the state of Victoria and the Australian Capital Territory have both adopted the legislation. The case could therefore be made to unify these policies by making them federal. Australia is not particularly strong on animal welfare&nbsp;(WAP n.d.), which would make it more difficult to adopt this legislation, but it also might make the legislation more impactful once adopted, because there would be a larger difference between the lofty language of the legislation and current practice.&nbsp;</p><h2><strong>Legislation recognising the sentience of disputed cases&nbsp;</strong></h2><p>One important precedent that is beginning to be set is the recognition of the sentience of not only vertebrate animals, but also decapod crustaceans and cephalopods. It stands to reason that legislation recognising the value of relatively more disputed cases of animal sentience would be of higher value than legislation recognising the sentience of other animals. This is because the public is much less aware of the strong scientific case for the sentience of these animals, and so this legislation would be doing more than stating the obvious in recognising their sentience. It may therefore play the role of raising public acceptance of the sentience of these animals.&nbsp;</p><p>As part of their submission of evidence for the Animal Welfare (Sentience) Bill&nbsp;(RP, n.d.) Rethink Priorities conducted a survey on public beliefs about the sentience of invertebrates and how this is affected by expert opinion. They found that a clear majority of the public believed that these animals are sentient, including crabs (78.09%) and octopuses (80.65%). Moreover they found that 91% of the public thought that if scientific experts believed that there is some evidence that a group of animals are sentient, we should exercise caution in our treatment of them. This suggests that, at least in the UK, the public support is there for including these animals. Insofar as legislation recognising animal sentience is regarded as the result of consultation with scientific experts, it also weakly suggests that it may lead people to believe that we should be more careful in our treatment of them. Though a study more directly trying to compare the influence of sentience legislation on public opinion after it has been passed, would be more valuable in assessing its impact.</p><p>More concretely, it may also serve as an impetus for some legislation protecting the welfare of these animals. Unlike the sentience of larger animals, the sentience of invertebrates has not even been implicitly recognised and there is no real legal rationale for legislation to protect them prior to this recognition. This may mean that it is substantially more likely that this legislation will cause real change because it represents a real change in political attitudes, rather than an overdue admission of a change in political attitudes. If the UK government implements some of the recommendations in the Review of the Evidence of Sentience in Cephalopod Molluscs and Decapod Crustaceans&nbsp;(2021), this will be a clear case of sentience legislation causing further, more concrete animal protection legislation.</p><p>If this is right that legislation recognising the sentience of invertebrates (or other more disputed cases) is indeed particularly valuable, that may provide an argument for sentience legislation in general, since the sentience of larger animals must presumably be recognised first and in doing so a conversation can be raised about which animals to include. This appears to be what happened in the UK case.</p><h2><strong>Encouraging a focus on the positive dimension of welfare</strong></h2><p>Another important aspect that could result from sentience legislation is support for a focus on the positive dimension of welfare. Standard definitions of sentience include the ability to experience positive welfare as well as negative welfare. This is significant because almost all existing animal welfare legislation concerns only reducing the amount of negative welfare experienced, rather than allowing animals to experience positive welfare. If this positive dimension of sentience began to be taken seriously, and inspire new legislation to cover for this dramatic oversight, this could greatly improve the value of this legislation&nbsp;(Ferrere 2022). Unfortunately, we have not yet seen significant precedents of this happening, so this outcome remains speculative.</p><h1>CASE STUDIES&nbsp;</h1><p>In cases where more precise comparisons are difficult, case studies can be valuable in providing at least some points of comparison. For this reason, we focused on a number of case studies for this report to try and assess: 1) the tractability of implementing the legislation, 2) any direct effects associated with the legislation, and 3) the broader consequences of the legislation, including subsequent animal welfare legislation or court decisions that may be attributable to the sentience legislation.</p><p>Nevertheless, there is some difficulty in assessing 3), since the most progressive countries for animal welfare are likely to pass legislation recognising the sentience of animals, as well as a variety of other more comprehensive laws. It might therefore look like the legislation recognising the sentience of animals caused the comprehensive legislation, while in actual fact both were simply the result of the country being more progressive on animal welfare. To compensate for this, we have tried to focus on finding cases where the causality between the sentience legislation and the subsequent animal protection legislation or court cases seem relatively clear.</p><h2><strong>The EU</strong></h2><p>The EU represents the earliest case of sentience legislation that we are aware of. In 1991, Compassion in World Farming gathered over 1 million signatures to get animal sentience recognised in EU legislation, making it the largest petition to be presented to the EU at that time. In 1994, this petition was accepted by the European Parliament&nbsp;(Rowan&nbsp;<i>et al.</i> 2021). This led to the Treaty of Amsterdam&nbsp;(European Communities 1997) recognising animal sentience at the EU level for the first time. The entire section of this treaty concerning animal sentience and welfare reads:</p><p>\u201cTHE HIGH CONTRACTING PARTIES, DESIRING to ensure improved protection and respect for the welfare of animals as sentient beings, HAVE AGREED UPON the following provision which shall be annexed to the Treaty establishing the European Community, In formulating and implementing the Community's agriculture, transport, internal market and research policies, the Community and the Member States shall pay full regard to the welfare requirements of animals, while respecting the legislative or administrative provisions and customs of the Member States relating in particular to religious rites, cultural traditions and regional heritage.\u201d</p><p>The clause that Member States \u201cpay full regard to the welfare requirement of animals\u201d is the most concrete and may be important in ensuring that the lives of animals are ultimately improved by this treaty. However, because the whole passage is still remarkably short and unspecific, it is again largely symbolic. This was followed by the Lisbon Treaty&nbsp;(European Council 2007), which finally enshrined this in legislation through Article 13.</p><p>The strength of this legislation was put to the test in 2001 when, in response to the hoof and mouth disease pandemic, the EU issued a directive banning the use of preventative vaccinations, and instead mandating the slaughter of these animals. When this decision was contested on the grounds that respect for the sentience of these animals should mean that they can only be slaughtered when it is absolutely necessary, the European Court of Justice ruled that the animal welfare protocol recognising sentience was not meant to institute any principles and EU law, it was just meant to codify existing principles. It is possible that the judge ruled irregularly and future judges will rule differently, but as it stands, this amounts to an admission that the EU animal sentience legislation is purely symbolic, and just a kind of window dressing on existing animal welfare policies&nbsp;(Pedersen 2009).</p><p>The Treaty of Amsterdam (1997) was only one part of a flurry of animal protection legislation passed in the 1990s. Most prominently, Council Directive 98/58/EC Concerning the Protection of Animals Kept for Farming Purposes passed in 1998 and introduced protections to all farm animals, including the five freedoms&nbsp;(Pedersen 2009).</p><p>Progress then slowed down significantly starting in the mid-2000s&nbsp;(Pedersen 2009). Overall, it looks like the animal sentience legislation has not resulted in further animal protection legislation in the EU, and could be thought to have caused the speed down, though other explanations seem more plausible in this case, such as new, more recalcitrant countries joining over this time&nbsp;(Pedersen 2009).</p><h2><strong>The UK&nbsp;</strong></h2><p>The UK has a long history of animal welfare legislation, from the Cruel Treatment of Cattle Act (1822) to the Brambell report (1965) and the subsequent development of the five freedoms&nbsp;(Council 2009). During its membership of the EU, the UK received animal sentience legislation as a result of the Treaty of Amsterdam (1997). After leaving, since EU legislation would no longer be in force in the UK, the UK decided to adopt much of the existing EU legislation into UK legislation. However, the nature of the legislation in this case did not work well with the UK legal system, meaning that some adaptations would be necessary if the legislation were to be retained&nbsp;(Brooman 2018).&nbsp;</p><p>The Conservative party (which was in power at the time) decided that existing legislation already adequately protected animals, and so formal recognition of animal sentience was unnecessary. The party was therefore whipped to vote against adopting Article 13 of the Treaty of Lisbon into UK national legislation, with the rationale that the UK\u2019s legislation already adequately protected animals. There was a public backlash in response to this which prompted the Conservative party to reconsider this, and in 2019, a group of 50 animal advocacy organisations formed a coalition called Better Deal For Animals to push for UK legislation to match the EU legislation recognising animal sentience&nbsp;(Rowan&nbsp;<i>et al.</i> 2021). In response to this pressure, they later decided to adopt national animal welfare legislation&nbsp;(Brooman 2018).&nbsp;</p><p>This legislation went beyond the EU legislation on animal sentience in one respect: the recognition of decapod crustaceans and cephalopods as sentient. This was done in response to a review of the evidence of their sentience&nbsp;(Birch&nbsp;<i>et al</i>. 2021) that the government commissioned, which concluded that there is very strong evidence to believe that cephalopods are sentient, and somewhat weaker, though still strong, evidence that decapod crustaceans are sentient.</p><p>This recognition may be particularly valuable since the sentience of these animals is not as widely recognised as the sentience of other animals (see the section \u201c<strong>Legislation recognising the sentience of disputed cases\u201d).</strong></p><p>Though the sentience of decapod crustaceans and cephalopods has now been recognised, they are still not included in even the basic protections of the Animal Welfare Act, and they are certainly not protected by more detailed secondary legislation, such as a Code of Practice. This makes the recognition of their sentience relatively symbolic so far, but since this is a recent development, it will be important to see if they are later included in the Animal Welfare Act (2006) or other legislation, as the government-commissioned report recommends. If they are later included in the Animal Welfare Act this could be a case of sentience legislation leading to more clear benefits for animals, since the Animal Welfare Act includes more specific and concrete protections for animals.</p><p>The government-commissioned report does recommend certain protections be implemented. These are bans on: declawing, \u2018nicking,\u2019 the sale of live animals by untrained workers, storage in cool temperatures, banning eyestalk ablation, and octopus farming, as well as the importation of farmed octopus products. They also recommend more research into stunning methods for decapods and research into potential humane slaughter methods for octopuses&nbsp;(Birch&nbsp;<i>et al.</i> 2021). If any of these recommendations become legislation, this will be the strongest example of concrete animal welfare outcomes being the result of animal sentience legislation.</p><p>Also remarkably, as part of the UK recognition of animal sentience, an independent committee of animal sentience is to be established with the task of ensuring that other government departments pay due respect to animal sentience in their decision-making&nbsp;(DEFRA 2021). The independent nature of this committee could be an asset, since it will make it less likely that it will be captured by industry interests. The government has said that sentience will not take priority over other considerations when formulating legislation, but this is still very significant&nbsp;(Ares n.d.). If properly implemented, this would make the UK\u2019s recognition of sentience more concrete and less purely symbolic than any of the other cases we have discussed in this report.</p><p>In response to the bill, the UK Centre For Animal Law (A-Law) raised the concern that it does not include details about how the independence of the committee and their expertise regarding animal sentience are to be insured&nbsp;(Ares n.d.).&nbsp;</p><p>Unfortunately, this concern has been borne out, as Michael Seals has now been chosen as the head of the animal sentience committee&nbsp;(DEFRA 2022). Instead of an expert on sentience or an expert on animal welfare, Michael Seals is a farmer. This defeats the purpose of having an independent animal welfare committee if the person in charge themselves has industry interests.</p><h2><strong>New Zealand</strong></h2><p>Another country that recognises the sentience of some invertebrates in legislation is New Zealand. In 2015, the Animal Welfare Act of New Zealand (1999) was revised to recognise the sentience of vertebrate animals, as well as the sentience of octopuses, squids, crabs, lobsters, and crayfish&nbsp;(WAP n.d.). This amendment was implemented following submissions by World Society for the Protection of Animals and the National Animal Welfare Advisory Committee for the Animal Welfare Amendment Bill consultation&nbsp;(James 2016). These organisations argued that the sentience of animals should be recognised in this legislation as a rationale for the rest of the Animal Welfare Act because sentience is widely considered to be the basis of the need to focus on welfare.</p><p>Unfortunately, in a clarificatory note, the New Zealand government writes that the recognition of sentience \"would not impact the detail of any of the rights, duties and obligations\"&nbsp;(Ferrere 2022). As with the statement by the European Court of Justice, this is a frank admission of the purely symbolic intentions behind the legislation.&nbsp;</p><p>Concerning New Zealand\u2019s recognition of animal sentience, World Animal Protection concludes:&nbsp;</p><p>\u201cHowever, the inclusion of sentience has been of no practical value in improving the treatment of animals. The basic purposes and principles of the Act are positive, but millions of farm animals are denied these protections by some very broad exceptions. Therefore, New Zealand should use the recognition of animal sentience to implement stronger animal protection provisions\u201d&nbsp;(WAP n.d.).</p><p>Vanessa James&nbsp;(2016) argues that the legislation recognising the sentience of animals in New Zealand is currently largely symbolic, but that it implies much stronger protections, especially surrounding freedom to perform natural activities. She concludes with recommendations about how legislation might be changed to be consistent with the recognition of animal sentience.</p><p>Despite this admission of a purely symbolic intention behind the legislation, and the James paper, some new animal welfare legislation has been put forward including new regulations in 2016, 2018, and 2020&nbsp;(Ministry for Primary Industries n.d.). Though this is hard to assess from our brief evaluation, it does not seem that this new legislation is particularly strong or that the previous legislation recognising the sentience of animals is the clear inspiration for it.&nbsp;</p><p>One notable case is that a new welfare code has been drafted for pigs which includes a phaseout of farrowing crates&nbsp;(SPCA 2022). Instead, sows would be provided with a farrowing area with nesting material and a support structure for lying down in an area that piglets could retreat to away from the sow. We could not find any clear indication that sentience legislation helped with this ban.&nbsp;</p><p>In the case of Haenga v Porirua City Council in New Zealand&nbsp;(NZ 2021), a dog owned by Mr.&nbsp; Haenga attacked another person causing disfiguring injuries. The decision of the court was that the dog was to be \u201cdestroyed\u201d, the standard decision in cases such as this. However, in a clarificatory note, the judge notes the tension between the language of \u201cdestroyed\u201d (suggesting that the dog is an object) and the legislation recognising dogs to be sentient beings. The judge further suggests that: \u201cOne might think that the question of public safety and what is to be done with a dangerous dog ought to be unshackled from the criminal responsibility of its owner\u201d. Though the decision resulted in the death of the dog, we can see the beginnings of an evolution of the judges' thinking surrounding the proper interpretation of this legislation, and how that interpretation pushes against the view of animals as objects and property.</p><p>In addition to recognising their sentience, the New Zealand Animal Welfare Act&nbsp;(MPI 1999) brings octopuses, squids, lobsters, crabs, and crayfish under its broad protections, though with very little specific regulations on their treatment.</p><h2><strong>US districts</strong></h2><p>Oregon, Vermont, Illinois, Colorado, Connecticut, Montana, Massachusetts, California. Maine, and Washington DC have recognised animal sentience&nbsp;(Kotzmann and Stonebridge 2021). Of these, the state of Oregon, which recognised animal sentience in 2013, is the most progressive and has been most active in adjudicating the full significance of this legislation. In several cases, the Oregon state Supreme Court has ruled that, since animals are individual sentient beings, cases where multiple animals are harmed are treated as multiple infractions of the law, rather than a single case&nbsp;(Kotzmann and Stonebridge 2021). This seems to be mainly relevant for the size of punishment received, which is not very relevant for the bottom-line goal of helping animals, but it does show a case where animal sentience legislation is at least not ineffectual.</p><p>In several other cases in Oregon, individuals were able to rescue animals that seem to be neglected or at risk of harm and successfully defend their right to violate normal private property norms since animals are sentient beings&nbsp;(Kotzmann and Stonebridge 2021).&nbsp;</p><p>One such case is the case of the Oregon dog, Juno, as described in Dunn and Rosengard&nbsp;(2016). In this case, Juno was noticed in the backyard by a neighbour emaciated and visibly starving. The police were called and the dog was seized as a case of animal neglect, but the judge ruled that it will be necessary to test to rule out the possibility that there was some condition that was preventing the dog from receiving proper nutrition. Blood was drawn in order to make this test and it was determined that there was no special condition that could explain the emaciation, indicating that neglect was the source of Juno\u2019s misery.&nbsp;</p><p>The owner objected to this on the grounds that drawing blood from a dog in order to perform the test was an unlawful use of their private property, and should therefore not be admissible as evidence in the case, but the judge ruled that this property right did not apply in this case because the dog was a neglected sentient being. These rulings could support activists' direct action rescue efforts in Oregon that could help challenge the animal property paradigm.</p><p>Concerning the Oregon sentience legislation, Ferrere&nbsp;(2022) concludes that \u201cIt is clear that Oregon courts have noted the progressive nature of its animal welfare legislation, reflected in its recognition of sentience, but it remains unclear whether that recognition itself has had a direct impact in improving the interests of animals.\u201d</p><p>Similar decisions were reached in the Vermont Supreme Court. In one such case part of the verdict included the line \u201ca defendant\u2019s property rights over animals are limited when animal welfare is at risk, and we must take the animal\u2019s welfare into consideration when determining the legality of a search or seizure\u201d&nbsp;(Kotzmann and Stonebridge 2021).</p><p>Unusually, the Oregon animal sentience legislation offers some definition of its terms, with animals defined as \u201csentient beings capable of experiencing pain, stress and fear\"&nbsp;(Ferrere 2022). In this case, the legislation does not appear to be very bold, with no reference to positive welfare, only reference to a limited number of negative states that are already widely legislatively acknowledged to happen to animals.</p><p>Whilst the direct effects of these cases are still limited, one can imagine extensions of these cases being relevant. For example, at some point, after further legal work and animal advocacy, we could imagine it being possible to legally challenge a factory farm to give up its animals for medical care, on the grounds that they are neglected, and so normal private property norms should be suspended.</p><h2><strong>Qu\u00e9bec</strong></h2><p>Another subnational district that recognises the sentience of animals is Qu\u00e9bec. As Canada is a federalised country, animal protection legislation differs significantly by province and territory. In 2015 Qu\u00e9bec became the only Canadian province or territory to adopt legislation recognising the sentience of animals. The Animal Welfare and Safety Act was created to enforce this status as sentient beings&nbsp;(Lessard 2021). Amendments to the Civil Code around the same time also recognised animals as \u201cbeings\u201d, not \u201cthings\u201d and recognised that they have \u201cbiological needs\u201d&nbsp;(Ferrere 2022).&nbsp;</p><p>Lessard&nbsp;(2021) argues that his recognition of biological needs may help cover for the lack of \u201cright to life\u201d that sentience legislation provides. This can be seen in one case attempting to argue that euthanising stray dogs was inconsistent with the recognition of animals as sentient beings. The court ruled that just because they are sentient beings, this does not mean that they cannot be a nuisance or a danger and that methods to counteract this may still be appropriate&nbsp;(Ferrere 2022).</p><p>Concluding his article, Lessard writes:&nbsp;</p><p>\u201cIn sum, judges transformed what could have been a simple declaratory recognition of animal sentience and biological needs into a provision with normative force. Within a few years, sentience recognition evolved into sentience protection. To be sure, this normative force does not prevent humans from exploiting and killing animals on a daily basis. However, it does ensure that harm is done with some respect for animal sentience and biological needs. In other words, Qu\u00e9bec law frankly acknowledges that humans may legally use other sentient beings, but it compels them to use a certain amount of respect while recognizing the special bonds humans develop with animals\u201d&nbsp;(Lessard 2021).</p><p>The court of appeal of Qu\u00e9bec has confirmed that animal sentience legislation in the province is more than just symbolic. They say that the legislation dictates that people should not cause pain to animals and that regulations must also be designed to protect animals from pain&nbsp;(Lessard 2021). However, it is not clear from this statement if this goes beyond other animal protection legislation in other areas, which also typically requires the minimization of pain.</p><h2><strong>Overall lessons from the case studies</strong></h2><p>Despite these victories and the promising wording of the animal sentience legislation, not a great deal has changed for animals so far&nbsp;(Ferrere 2022). These victories are notable as exceptions \u2013 the situation for animals has otherwise remained the same. Kotzmann&nbsp;(2022) characterises the time following the passage of this legislation as a crossroads, with potential for either great change on behalf of animals in the future or with the continuation of the status quo of very little protection in practice.&nbsp;</p><p>Sometimes legislation recognising animal sentience is less symbolic and more explicit details or partner legislation are included spelling out direct intended implications. This is most true in the case of the UK, which will have an animal sentience committee tasked with making sure that the government pays due regard to the sentience of animals&nbsp;(Kotzmann 2020).</p><p>The recognition of animals as sentient beings certainly seems to be a step away from their treatment by the law as objects. However, in many cases animal sentience legislation in a country or area coexists with legislation that continues to regard them as objects. In practice, they have a kind of uneasy \u2018dual status\u2019&nbsp;(Kotzmann 2020).</p><p>One thing to keep in mind is that this kind of judicial pushing at the edges is more difficult in countries with civil codes of law (because it relies on judges taking more initiative in interpreting the law), meaning that the solution may not be able to be effectively implemented everywhere&nbsp;(Kempers 2022).</p><h1>CRUCIAL CONSIDERATIONS&nbsp;&nbsp;</h1><h2><strong>Does sentience legislation do enough to protect animals from killing and exploitation?</strong></h2><p>Lessard&nbsp;(2021) brings up a concern with animal sentience legislation, which is that it may not do enough to prevent the killing and exploitation of animals, so long as it is done \u201cpainlessly\u201d. It may allow killing because sentience legislation primarily points to a hedonic conception of well-being, which could in principle allow painless killing. Views that assign extra moral significance to killing or death (such as preference satisfaction views) or views that exploitation is intrinsically morally problematic would object to this.</p><p>The strongest validation of this concern is the Qu\u00e9bec case of defending a decision to euthanise a stray dog for being a \u201cnuisance\u201d. As discussed earlier, this decision was defended on the grounds that the recognition of sentience does not preclude an animal being euthanised when other interests are at stake. This represents the Qu\u00e9bec judicial system in practice giving very little moral consideration to the sentience of animals by weighting its importance lower than minor interests of humans.</p><p>However, there is at least a theoretical justification for respect for sentience translating into some protection for their lives and protection from exploitation. As long as an animal\u2019s life is worth living, hedonic views would prohibit killing them because their life is expected to contain more happiness than suffering. Since a hedonic view would also require people to give animals a life worth living in the first place, after that is done, it would then prohibit them from painlessly killing those animals&nbsp;(Vi\u0161ak 2013).</p><p>It is also difficult to imagine economically feasible use of animals that truly minimises suffering and gives them a life worth living that is still exploitative towards them. If this is not possible, then a duty to respect their sentience would also protect them from exploitation.</p><p>The problem may therefore be less with the implication of this policy, and more with convenient interpretations of it. That is to say, the problem may be that a simple reading of policy could be seen to support \u201cpainless\u201d killing and continued exploitation. In the worst case, such a policy could be used to provide cover for these continued practices on the grounds that animals are being respected by the sentience legislation. It remains to be seen if other jurisdictions will rule similarly to the Qu\u00e9bec case or if they will see sentience as implying broader protections than this.</p><p>However, the fact that a single piece of legislation does not cover all morally relevant cases of harm is not a good argument against it. As with civil rights victories, it is most likely that animal protection legislation will come step-by-step and future legislation can cover for weaknesses in previous legislation&nbsp;(Kempers 2022).&nbsp;</p><h2><strong>Vagueness</strong></h2><p>Ferrere&nbsp;(2022) argues that the fact that sentience is typically not defined in the legislation recognising it poses a significant problem to the value of this legislation. He claims that a judge would be unlikely to proceed with provocative interpretation of the legislation, an interpretation that would affect lives or livelihoods, if the term is not clearly defined.&nbsp;</p><p>Not being anchored to a current definition of the term means that the effect of this legislation may change over time with new interpretations of these terms. This may be beneficial if we expect progress on attitudes towards animals, as a more favourable definition may be used, but it could also open a window to backsliding to a more aggressive definition of sentience. Overall, we think progress is more likely and so we view this aspect of the vagueness concern as being potentially positive.</p><h2><strong>Humane washing</strong></h2><p>Tr\u00f8ite and Myskja&nbsp;(2018) examines the case of the Norwegian Animal Protection Act (2009), which recognises the intrinsic value of animals in what they suggest is a purely symbolic way. They argue that there is a risk that using words in a way contrary to their normal meaning undermines that meaning, which could have long-term political significance. The same argument would apply to animal sanctions legislation, suggesting that their symbolic use could undermine the strength and political significance of these terms.</p><p>This could be viewed as a form of \u201chumane washing\u201d, where current exploitative practices are shrouded in a veneer of being high welfare, potentially leading to complacency with the current status quo&nbsp;(Scott-Reid 2021). In this case, people may take psychological refuge in the fact that the legislation respects animals, without any real benefit for animals. If people can receive all the moral acclaim associated with recognising the sentience of animals, without having to change their actions, they will not have any additional reason to change their actions to improve the situation of animals. It is therefore at least important to make sure that people who only say the right words do not receive the same moral acclaim as people who actively help animals.&nbsp;</p><p>Of course, this case may only be a reflection or symptom of the dramatically inconsistent attitudes towards animals in our society. We cherish some animals as beloved companions, whilst raising others in horrific conditions&nbsp;(Francione 2004). Because this is already common, symbolic sentience legislation may not be entrenching an attitude that is not already present. It may therefore represent less of a risk than would otherwise be expected.</p><p>Tr\u00f8ite and Myskja&nbsp;(2018)&nbsp;note that this concern must be balanced against the potential for the legislation to lead to change. People can recognise the inconsistency between the lofty language of the legislation and current practice, and this can bolster campaigns or legal challenges to current practices by allowing them to point out this hypocrisy.</p><p>Ultimately, we do not think this consideration is decisive against the value of sentience legislation. Moral progress appears to mostly come step-by-step, and we cannot always expect significant actions to follow words from the onset. Nevertheless, we should push as much as possible so that the legislation is not purely symbolic. Fortunately, sentience legislation itself provides an important tool for doing so, since it allows us to point out the inconsistency and hold people accountable for their commitments in legislation.</p><h1>CONCLUSION AND REMAINING UNCERTAINTIES</h1><p>An assessment of the value of animal sentience legislation is difficult because any significant benefits for animals from the legislation lie well into the future. We see some potential in the legislation in enabling future activism and judicial challenges, as well as potentially prompting future legislative change. We can envisage how this might play into a theory of victory ultimately leading to fundamental rights, though it also seems like it is not strictly necessary in this pathway. We view the legislation as generally shifting the conversation with government in a more promising direction, though since sentience is already implicitly recognised in many jurisdictions, this may not be a large shift.</p><p>We remain most impressed by the results seen in the UK, where an animal sentience committee was established (though this has now been compromised by the choice of committee chair) and the sentience of decapod crustaceans and cephalopods was recognised with potential for further legislation to protect them. The best case scenario for an organisation may be replicating this result in another country, but with stronger sentience committee appointments.</p><p>On the other hand, if organisations are only able to establish the relatively toothless legislation seen in the EU and New Zealand, we are not convinced that this is among the strongest possible campaigns for that organisation. &nbsp;The argument that this legislation represents humane washing is a significant concern, but we are not convinced that this makes the campaign overall negative, as long as organisations pressure the government on this point to make it more than just symbolic.</p><p>Our most significant remaining uncertainty is just our overall assessment of the impact of legislation that has not yet seen significant direct impact on the lives of animals. Since this legislation is relatively new in many jurisdictions, it will be important to monitor the situation and note whether it leads to significant further improvements for animals. It will be especially important to monitor the case of the UK, as that represents what is so far the strongest version of this ask.</p><p>One interesting avenue that we did not explore is examining potential historical parallels to sentience legislation \u2013 legislation that appears only symbolic \u2013 and see if it ultimately contributed to any concrete outcomes.</p><h1>BIBLIOGRAPHY</h1><p>Ares, Elena. n.d. \u201cAnimal Welfare (Sentience) Bill.\u201d Accessed September 13, 2022.&nbsp;<a href=\"https://commonslibrary.parliament.uk/research-briefings/cbp-9423/\">https://commonslibrary.parliament.uk/research-briefings/cbp-9423/</a>.</p><p>\u201cAustralian Capital Territory Enacts New Law Recognizing Animal Sentience.\u201d 2019. Animal Legal Defense Fund. December 16, 2019.&nbsp;<a href=\"https://aldf.org/article/australian-capital-territory-enacts-new-law-recognizing-animal-sentience/\">https://aldf.org/article/australian-capital-territory-enacts-new-law-recognizing-animal-sentience/</a>.</p><p>Birch, Jonathan, Charlotte Burn, Alexandra Schnell, Heather Browning, and Andrew Crump. 2021. \u201cReview of the Evidence of Sentience in Cephalopod Molluscs and Decapod Crustaceans,\u201d General - Animal Feeling, .&nbsp;<a href=\"https://www.wellbeingintlstudiesrepository.org/af_gen/2/\">https://www.wellbeingintlstudiesrepository.org/af_gen/2/</a>.</p><p>Brooman, S. D. 2018. \u201cAnimal Sentience in UK Law: Does the New Clause Need Claws?\u201d&nbsp;<i>The UK Journal of Animal Law</i>, December.&nbsp;<a href=\"http://researchonline.ljmu.ac.uk/id/eprint/8904/\">http://researchonline.ljmu.ac.uk/id/eprint/8904/</a>.</p><p>Council, F. A. W. 2009. \u201cFarm Animal Welfare in Great Britain: Past, Present and Future.\u201d ongehoord.info. 2009.&nbsp;<a href=\"https://www.ongehoord.info/wp-content/uploads/2017/12/11-1.pdf\">https://www.ongehoord.info/wp-content/uploads/2017/12/11-1.pdf</a>.</p><p>DEFRA. 2021. \u201cAnimals to Be Formally Recognised as Sentient Beings in Domestic Law.\u201d GOV.UK. May 13, 2021.&nbsp;<a href=\"https://www.gov.uk/government/news/animals-to-be-formally-recognised-as-sentient-beings-in-domestic-law\">https://www.gov.uk/government/news/animals-to-be-formally-recognised-as-sentient-beings-in-domestic-law</a>.</p><p>\u2014\u2014\u2014. 2022. \u201cMichael Seals Confirmed as First Chair of the New Animal Sentience Committee.\u201d GOV.UK. September 2, 2022.&nbsp;<a href=\"https://www.gov.uk/government/news/michael-seals-confirmed-as-first-chair-of-the-new-animal-sentience-committee\">https://www.gov.uk/government/news/michael-seals-confirmed-as-first-chair-of-the-new-animal-sentience-committee</a>.</p><p>Dunn, and Rosengard. 2016. \u201cA Dog Is Not a Stereo: The Role of Animal Sentience in Determining the Scope of Owner Privacy Interests under Oregon Law.\u201d&nbsp;<i>Animal Learning &amp; Behavior</i>.&nbsp;<a href=\"https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/anim23&amp;section=19\">https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/anim23&amp;section=19</a>.</p><p>European Communities. 1997. \u201cThe Treaty of Amsterdam.\u201d&nbsp;<a href=\"https://www.europarl.europa.eu/topics/treaty/pdf/amst-en.pdf\">https://www.europarl.europa.eu/topics/treaty/pdf/amst-en.pdf</a>.</p><p>European Council. 2007. \u201cTreaty of Lisbon.\u201d&nbsp;<a href=\"https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A12007L%2FTXT\">https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A12007L%2FTXT</a>.</p><p>Ferrere. 2022. \u201cThe (Symbolic) Legislative Recognition of Animal Sentience.\u201d&nbsp;<i>Animal Learning &amp; Behavior</i>.&nbsp;<a href=\"https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/anim28&amp;section=9\">https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/anim28&amp;section=9</a>.</p><p>Francione, Gary L. 2004. \u201cAnimals--Property or Persons?,\u201d Rutgers Law School (Newark) Faculty Papers, .&nbsp;<a href=\"https://law.bepress.com/rutgersnewarklwps/art21/\">https://law.bepress.com/rutgersnewarklwps/art21/</a>.</p><p>James, Vanessa. 2016. \u201cRecognising Animal Sentience: Including Minimum Standards for Opportunities to Display Normal Patterns of Behaviour in Codes of Welfare in New Zealand.\u201d&nbsp;<a href=\"http://researcharchive.vuw.ac.nz/handle/10063/6335\">http://researcharchive.vuw.ac.nz/handle/10063/6335</a>.</p><p>Kempers, Eva Bernet. 2022. \u201cTransition rather than Revolution: The Gradual Road towards Animal Legal Personhood through the Legislature.\u201d&nbsp;<i>Transnational Environmental Law</i>, 1\u201322.</p><p>Kotzmann. 2020. \u201cRecognising the Sentience of Animals in Law: A Justification and Framework for Australian States and Territories.\u201d&nbsp;<i>The Sydney Law Review</i>. https://doi.org/<a href=\"http://dx.doi.org/10.3316/agispt.20201203040691\">10.3316/agispt.20201203040691</a>.</p><p>Kotzmann, Jane. 2022. \u201cLegal Recognition of Animal Sentience: The Case for Cautious Optimism.\u201d&nbsp;<i>Animal Sentience</i> 6 (31): 7.</p><p>Kotzmann, Jane, and Morgan Stonebridge. 2021. \u201cThere Is Value in Stating the Obvious: Why United States Legislatures Should Explicitly Recognize Animal Sentience in Their Laws.\u201d&nbsp;<i>Cornell Journal of Law and Public Policy</i>.&nbsp;<a href=\"https://dro.deakin.edu.au/view/DU:30143939\">https://dro.deakin.edu.au/view/DU:30143939</a>.</p><p>Lessard. 2021. \u201cCan Sentience Recognition Protect Animals? Lessons from Quebec\u2019s Animal Law Reform.\u201d&nbsp;<i>Animal Learning &amp; Behavior</i>.&nbsp;<a href=\"https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/anim27&amp;section=7\">https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/anim27&amp;section=7</a>.</p><p>Marchant, J. N., D. M. Broom, and S. Corning. 2001. \u201cThe Influence of Sow Behaviour on Piglet Mortality due to Crushing in an Open Farrowing System.\u201d&nbsp;<i>Animal Science&nbsp;</i>&nbsp;72 (1): 19\u201328.</p><p>Ministry for Primary Industries. n.d. \u201cAnimal Welfare Regulations.\u201d Ministry for Primary Industries. Accessed September 16, 2022.&nbsp;<a href=\"https://www.mpi.govt.nz/animals/animal-welfare/regulations/regulations-and-the-animal-welfare-system/\">https://www.mpi.govt.nz/animals/animal-welfare/regulations/regulations-and-the-animal-welfare-system/</a>.</p><p>MPI. 1999. \u201cNew Zealand Animal Welfare Act.\u201d&nbsp;<a href=\"https://www.legislation.govt.nz/act/public/1999/0142/latest/DLM49664.html?search=ts_act%40bill%40regulation%40deemedreg_animal+welfare_resel_25_a&amp;p=1\">https://www.legislation.govt.nz/act/public/1999/0142/latest/DLM49664.html?search=ts_act%40bill%40regulation%40deemedreg_animal+welfare_resel_25_a&amp;p=1</a>.</p><p>NZ. 2021. \u201cHaenga v Porirua City Council [2021] NZHC 1549 (25 June 2021).\u201d High Court of New Zealand Decisions. 2021.&nbsp;<a href=\"https://www.austlii.edu.au/cgi-bin/viewdoc/nz/cases/NZHC/2021/1549.html\">https://www.austlii.edu.au/cgi-bin/viewdoc/nz/cases/NZHC/2021/1549.html</a>.</p><p>Pedersen, Nicholas K. 2009. \u201cDetailed Discussion of European Animal Welfare Laws 2003 to Present: Explaining the Downturn.\u201d Animal Law. 2009.&nbsp;<a href=\"https://www.animallaw.info/article/detailed-discussion-european-animal-welfare-laws-2003-present-explaining-downturn\">https://www.animallaw.info/article/detailed-discussion-european-animal-welfare-laws-2003-present-explaining-downturn</a>.</p><p>Rowan, Andrew N., Joyce M. D\u2019Silva, Ian J. H. Duncan, and Nicholas Palmer. 2021. \u201cAnimal Sentience: History, Science, and Politics.\u201d&nbsp;<i>Animal Sentience</i> 6 (31): 1.</p><p>RP. n.d. \u201cSubmission of Evidence to Animal Welfare (Sentience) Bill.\u201d Rethink Priorities.</p><p>Scott-Reid, Jessica. 2021. \u201c\u2018Humane\u2019 Meat, Dairy, and Eggs: How to Decipher Animal Welfare Claims on Food Labels.\u201d Vox. December 21, 2021.&nbsp;<a href=\"https://www.vox.com/22838160/animal-welfare-labels-meat-dairy-eggs-humane-humanewashing\">https://www.vox.com/22838160/animal-welfare-labels-meat-dairy-eggs-humane-humanewashing</a>.</p><p>Sowery, Katy. 2018. \u201cSentient Beings and Tradable Products: The Curious Constitutional Status of Animals under Union Law.\u201d&nbsp;<i>Common Market Law Review</i> 55 (1): 55\u201399.</p><p>SPCA. 2022. \u201cHave Your Say for Pigs: Help End the Use of Farrowing Crates in NZ.\u201d SPCA. 2022.&nbsp;<a href=\"https://www.spca.nz/news-and-events/news-article/have-your-say-for-pigs-help-end-the-use-of-farrowing-crates-in-nz\">https://www.spca.nz/news-and-events/news-article/have-your-say-for-pigs-help-end-the-use-of-farrowing-crates-in-nz</a>.</p><p>Sunstein, Cass R., and Martha C. Nussbaum. 2004.&nbsp;<i>Animal Rights: Current Debates and New Directions</i>. Oxford University Press.</p><p>Tr\u00f8ite, and Myskja. 2018. \u201c58. Legal Protection of Animal Intrinsic Value\u2013mere Words?\u201d&nbsp;<i>Wageningen Academic P Ublishers</i>.&nbsp;<a href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=sJsZEAAAQBAJ&amp;oi=fnd&amp;pg=PA369&amp;dq=Symbolic+legislation+animal+sentience&amp;ots=VfNroo_OXb&amp;sig=XBDirQfNEYbpiF3VpFJzkCt9IGY\">https://books.google.com/books?hl=en&amp;lr=&amp;id=sJsZEAAAQBAJ&amp;oi=fnd&amp;pg=PA369&amp;dq=Symbolic+legislation+animal+sentience&amp;ots=VfNroo_OXb&amp;sig=XBDirQfNEYbpiF3VpFJzkCt9IGY</a>.</p><p>UK Government. 2021. \u201cFree Trade Agreement between the United Kingdom of Great Britain and Northern Ireland and Australia.\u201d GOV.UK. December 16, 2021.&nbsp;<a href=\"https://www.gov.uk/government/collections/free-trade-agreement-between-the-united-kingdom-of-great-britain-and-northern-ireland-and-australia\">https://www.gov.uk/government/collections/free-trade-agreement-between-the-united-kingdom-of-great-britain-and-northern-ireland-and-australia</a>.</p><p>Vi\u0161ak, Tatjana. 2013. \u201cUtilitarianism and Animal Husbandry.\u201d In&nbsp;<i>Killing Happy Animals: Explorations in Utilitarian Ethics</i>, edited by Tatjana Vi\u0161ak, 18\u201333. London: Palgrave Macmillan UK.</p><p>VSD. 2022. \u201cReforming Victoria\u2019s Animal Care and Protection Laws.\u201d {The State of Victoria Department of Jobs}.&nbsp;<a href=\"https://engage.vic.gov.au/new-animal-welfare-act-victoria\">https://engage.vic.gov.au/new-animal-welfare-act-victoria</a>.</p><p>WAP. n.d. \u201cAnimal Protection Index Map.\u201d World Animal Protection. Accessed May 4, 2022a.&nbsp;<a href=\"https://api.worldanimalprotection.org/\">https://api.worldanimalprotection.org/</a>.</p><p>\u2014\u2014\u2014. n.d. \u201cAustralia.\u201d World Animal Protection. Accessed September 30, 2022b.&nbsp;<a href=\"https://api.worldanimalprotection.org/country/australia\">https://api.worldanimalprotection.org/country/australia</a>.</p><p>\u2014\u2014\u2014. n.d. \u201cNew Zealand.\u201d World Animal Protection. Accessed September 15, 2022c.&nbsp;<a href=\"https://api.worldanimalprotection.org/country/new-zealand\">https://api.worldanimalprotection.org/country/new-zealand</a>.</p>", "user": {"username": "Animal Ask"}}, {"_id": "FtCzShmxuHBnG5myB", "title": "Talent search-nurture program in rural India - Progress and Asks", "postedAt": "2022-12-09T07:55:20.537Z", "htmlBody": "<h1><strong>Pratibha Poshak Progress Report 2022</strong></h1><h2>Primer</h2><p><br><a href=\"https://www.rajalakshmifoundation.in/pratibha-poshak/implementation\"><u>Pratibha Poshak</u></a> is a talent search-nurture program for rural students based in Northern Karnataka in India launched by the Rajalakshmi Children Foundation (RCF), a NGO established in 2013. The program first began in 2017 and was expanded in scope upon receiving a FTX Future Fund grant in May, 2022.<br><br>In its current form, Pratibha Poshak identifies the <strong>top ~1%</strong> (n~200) of 8th grade students in STEM and analytical skills from 3 districts in Northern Karnataka who come from economically underprivileged families, defined as an annual income &lt;$1800/yr (n~30,000).</p><p>These students are provided smart tablets+hi-speed internet packs and are connected in digital classrooms with highly-qualified and passionate teachers and professionally successful volunteer mentors for a period of two years. Students attend daily hour-long online classes in math, science, english and analytical reasoning (cohort sizes of 30) and weekly mentoring sessions (cohort sizes of 10). Classes are structured in a manner that emphasizes curiosity-driven and application-focused learning and are in addition to regular school. Mentoring sessions involve personal+career counseling and soft skill building. Bi-monthly in-person retreats are held to form stronger interpersonal relationships between students, teachers and mentors.<br><br>The ultimate goal of the program is to support bright students from disadvantaged backgrounds in their aspirations to pursue careers in fields like medicine, engineering, research, entrepreneurship and policy.&nbsp;</p><p>The theory of change goes along the following lines. Post 10th grade, students in India have to choose a Pre-University (PU) college with a set track of study - between science, commerce and arts - with the choice of PU college and track having significant lock-in to future trajectories. Due to a lack of exposure and support, students from rural communities tend to significantly underestimate their potential and odds of success and many even drop out from the educational system. By identifying talented students during this <strong>\u2018hinge\u2019 period </strong>and nurturing their potential, we hope to enable them to pursue successful and high-impact careers.</p><h2><strong>Asks for Action, Advice and Feedback</strong></h2><p>Apart from increasing awareness on the work we are doing, I am writing this post with specific asks from interested community members. Below are some of the different categories of things we would appreciate and could benefit from (not an exhaustive list) -</p><ul><li><u>Research</u> : Are there bodies of literature or specific papers/models we should be engaging with as we proceed with the program? How could we best go about evaluating cost-effectiveness, considering some of the most relevant outcomes are either long-term (e.g. career outcomes are ~6yr+ away) or intangible/hard to quantify the counterfactual (e.g. aspirations raised from being a small business owner to launching a scalable startup)?</li><li><u>Collaboration</u> : Are you or someone you know interested in collaborating with a digital school packed with 200 bright students? Some potential avenues include virtual guest lectures/series (especially if in Kannada which is the medium of most batches but we also have English batches), in-person events/retreats (would need to be in Karnataka), assistance with above research questions, or other avenues such as opportunities for scholarships for events/conferences/retreats for selected students in the program.</li><li><u>Funding</u> : We had hoped to rely on the FTX foundation for multi-year support and had set the scale of the project accordingly. While we have resources to support the current batch, future batches might have to be scaled down unless we secure significant funding ($100k+). Suggestions/connections for large grants are very welcome. Individuals interested in contributing&nbsp;<a href=\"https://www.rajalakshmifoundation.in/contribute\"><u>can do so easily&nbsp;</u></a>(foreign currencies are accepted and donations are tax-exempt).</li><li><u>General feedback/advice/evaluation</u> : E.g. could/how can we leverage recent developments in AI, broad or specific notes on things we could do/can do better/should avoid doing would be helpful.&nbsp;<br>&nbsp;</li></ul><h2><strong>Background&nbsp;</strong></h2><p><a href=\"mailto:rcfbgm@gmail.com\"><u>Rajalakshmi Children Foundation</u></a> was founded in 2013 by two doctors Shashikant and Vijayalakshmi Kulgod (who happen to be my wonderful parents) with the mission of supporting the health, education, and overall development of underprivileged children in Belagavi, a district in Karnataka. RCF focused on developing deep relationships with local \u2018government schools\u2019 - run wholly on public funds, free to attend and generally lacking basic physical and teaching infrastructure - and working with them to provide funding, resources and services such as WASH facilities, libraries and labs, and anemia screening camps. The approach could be summarized as<strong> local-but-deep equitable interventions</strong> that seeked to increase the physical and cognitive floor of children. Currently, RCF is actively partnered with a cluster of 15 government schools and has intimate relations with city and district government teachers, officers and policy-makers.</p><p>Pratibha Poshak began with a collaboration between RCF and Dr Ravindra Guravannavar in 2017. Dr Guravannavar is a computer scientist and an ex-IIT professor who began voluntarily teaching science and math to students from select government schools in 2015. The students were self-selected as classes were held after-hours and were free to attend. In 2019, a dedicated learning center was established - Sadhana Mandir - where these self-selected students continued to attend after-hour classes and could learn independently and with each other. This program, called Sharadha Pratibha Poshan Upakram (SPPU), was able to continue even during the Covid pandemic through online classes. This provided motivation to scale up SPPU to serve communities from more rural and geographically inaccessible areas - Pratibha Poshak.</p><p>This next stage was facilitated by receiving a FTX Future Fund grant in May 2022.&nbsp;</p><h2><strong>Implementation of Pratibha Poshak&nbsp;</strong></h2><h3><strong>Team</strong></h3><p>Pratibha Poshak is directed voluntarily full-time by Dr Guravannavar and is led full-time by Dr Shivkumar, a previous professor in management studies. A group of 10+ high-agency and resourceful interns recruited from the first graduated batch of SPPU perform all operations and logistics.</p><h3><strong>Selection of Students&nbsp;</strong></h3><p>At the first level, we use data from National Means-cum-Merit Scholarship Examination (NMMS) conducted every year by DSERT, Government of Karnataka. Only students having family income of less than INR 1.5 lacs ($1,800) per year and studying in Government or Government-aided public schools are eligible to write the NMMS Examination. The NMMS exam tests students on two axes - subject-level knowledge and analytical reasoning. We use a weighted average that emphasizes scores on analytical reasoning, shortlist the top performing students in each geographical area, and invite them to attend our briefing and selection event.</p><p>At the second level, we brief students and parents about the program and conduct an independent test where we teach a new mathematical concept and assess their ability to understand and apply the concept to solve non-trivial and practical problems. Students who score the highest as well as those who display unusual creativity in solving problems are selected for the program. These students are invited to the launch event for their geographic district where they are given smart tablets and SIM cards and are counseled, along with their parents, on the structure and needs of the program.</p><h3><strong>Finding Teachers and Mentors</strong></h3><p>In parallel, we conducted interviews for teachers to identify highly experienced, qualified and passionate teachers for math, physics, chemistry, english and analytical reasoning. An important requirement was for teachers to be fluent in Kannada, the state language of Karnataka, which was the primary language for most students. We expected teachers to possess strong first-principle understanding of their domains and be well-versed in communicating digitally. We ended up conducting many interviews to find individuals who fulfilled our criteria.</p><p>We also reached out to individuals in our networks who were successful professionals (e.g. Google employee, corporate manager at large multinational firms, chemistry professor) and were willing to volunteer to be weekly mentors to students for a period of 4 years. Mentors are expected to counsel students on personal and career development and provide some financial support to students.</p><h3><strong>Digital classrooms and in-person events</strong></h3><p>We were able to launch the first batch of 90 students along with 8 teachers and 9 mentors by mid-August in digital classrooms hosted on the platform TeachMint. The first bi-monthly retreat was hosted in mid-October. The second batch of 100 students was launched in mid-November. Classes have been well-received by students, have full attendance, and teachers report a high level of engagement by students. &nbsp;Mentors have remarked on the motivation and aspiration of the students. One mentor noted that when asked about their career aspirations, answers ranged from wanting to work at SpaceX, being an IAS (Indian Administrative Service) officer and pursuing careers in medicine, engineering and research.</p><h3><strong>Expenses</strong></h3><p>There are primarily five sources of expenses for the program -&nbsp;</p><p>Smart tablets : 40%</p><p>Teacher salaries : 30%</p><p>Team salaries : 15%</p><p>In-person events - Food, Transport, Lodging : 8%</p><p>Internet : 7%</p><p>It costs $120k to launch the first 200 student batch &nbsp;and $50k to continue the program for that batch (total $170k). It costs slightly less to launch and continue Batch 2 ($150k). To launch and graduate two batches of 200 students (400 total), it costs $320k.</p><h3><strong>Future trajectory</strong></h3><p><strong>Additional enrichment</strong></p><p>We hope to start a guest lecture series consisting of expert speakers talking about cutting edge science and technology, anticipated opportunities and challenges of the future, entrepreneurship, cultivating agency and developing altruism. A constraint is that most students speak Kannada and although we are providing English classes, it is unlikely they will develop enough proficiency for information/technical-dense lectures in English to be useful.</p><p>We are optimistic about cohort network effects that come from being in a group of talented peers to offer powerful leverage to students going forward and hope to continue to build platforms and cultivate community and culture in the batches.</p><p><strong>Evaluation</strong></p><p>A natural outcome to measure one axis of effectiveness would be to evaluate the differences in NTSE scores (National Talent Search Exam administered to almost all 10th grade students across India that tests domain-knowledge and analytical reasoning) between selected students and shortlisted but not selected students. This data will be easily accessible to us in 2024 and will provide some feedback on the value of our program.</p><p><strong>Expanding Scope</strong></p><p>We plan to begin the process for selecting Batch 2 by March 2023 and launch the program for all districts by June 2023. If we are funding constrained, we expect to have to drop the batch size by 2x-4x (n=50-100).</p><p>Finally, we believe that if this model proves effective and we are successful in raising funds, we have the capabilities and are motivated to expand to cover the entire state of Karnataka within this decade.</p><h2><strong>Parting Remarks</strong></h2><p><strong>On the merits of not-scaling</strong></p><p>I\u2019ve noticed a general sentiment in EA that believes effective charities are those that specialize in a particular intervention and attempt to radically scale in scope. While this is likely true, I don\u2019t think the corollary is necessarily false - that charities that take a generalist and local approach are not as effective. Focusing on implementing broad interventions in specific geographies and populations over long periods of time builds valuable social and human capital, engenders trust and allows for insight into the complex nature of problems. These are benefits that don\u2019t fit neatly into a GiveWell CE spreadsheet.</p><p>This local-but-deep base of RCF created valuable resources for Pratibha Poshak. A concrete example was in the seemingly simple process of acquiring the NMMS data that were used for the initial shortlisting. This data was extremely easy to acquire for the Belgaum district, where RCF is based, taking something like a few phone calls over a span of a couple of weeks. Getting the data from the other two districts turned out to be a Herculean task, requiring multiple in-person visits to government offices and wrangling with various contacts to grease the wheels of Indian bureaucracy. There were many such instances where the Foundation\u2019s reputation, familiarity, and partnerships eased open doors.</p><p>Another concrete example is the huge asset that the team of graduates from the first small-scale program, whose familiarity with the area, high levels of agency and tech savvy, and dedication to the mission allowed us to be very strong on the operations front. We expect to be able to continue relying on such remarkable human capital.</p><p><strong>Unexpected benefits</strong></p><p>A potential benefit we observed that we had not anticipated was the immediate ripple effect amongst the peers of selected students. We heard from many sources that selected students invited their classmates to physically join in on classes and shared their learnings. Students in lower grades reported feeling more enthusiastic and inspired to take their academic prospects more seriously.&nbsp;</p><p><strong>Optimal timing</strong></p><p>Finally, it is worth mentioning how various factors conspired to make the timing of the program ideal. The pandemic ensured that even schools in extremely remote parts of India had some exposure to digital learning and students and teachers were familiar with the technology. Recent telecom investments mean that India has the 5th cheapest Internet prices in the world (1GB = $0.17,&nbsp;<a href=\"https://www.cable.co.uk/mobiles/worldwide-data-pricing/#highlights\"><u>source</u></a>). India in the 21st century is also one of the best countries to run a talent search program - a large and relatively untapped population, an emerging economy with vast opportunities (and inefficiencies), and strong links to the rest of the world - all imply higher relative returns to raising the aspirations of talented young people.</p><p>&nbsp;</p><h2><strong>Get in touch</strong></h2><p>Please feel free to get in touch by emailing rcfbgm@gmail.com. More information is available on our website - <a href=\"https://www.rajalakshmifoundation.in/home\">https://www.rajalakshmifoundation.in/home</a>&nbsp;</p>", "user": {"username": "Akash Kulgod"}}, {"_id": "ydfcCfRAQpneH2wpG", "title": "Smallpox eradication", "postedAt": "2022-12-09T03:29:55.985Z", "htmlBody": "<p><strong>Today (December 9) is Smallpox Eradication Day.</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn17g7rdgjzd\"><sup><a href=\"#fnn17g7rdgjzd\">[1]</a></sup></span><strong>&nbsp;</strong>43 years ago, smallpox was confirmed to have been eradicated after killing hundreds of millions of people. This was a major achievement in global health.</p><p>So I'm link-posting Our World in Data\u2019s <a href=\"https://ourworldindata.org/smallpox\">data explorer on smallpox</a> (and&nbsp;<a href=\"https://ourworldindata.org/smallpox#how-was-global-decline-eradication-achieved\"><u>here\u2019s the section on how decline &amp; eradication was achieved</u></a>).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy6lv8ckclc\"><sup><a href=\"#fny6lv8ckclc\">[2]</a></sup></span>&nbsp;</p><p>This post shares a summary of the history of the eradication of smallpox and selected excerpts from the data explorer.</p><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/global-smallpox-cases\">\n\t\t\t\t\t<div data-owid-slug=\"global-smallpox-cases\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/global-smallpox-cases\">\n\t\t\t\t\t&lt;/div&gt;\n\t\t\t\t</iframe></div></div></figure><h1>A summary of the history of smallpox eradication<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7x6tkofn665\"><sup><a href=\"#fn7x6tkofn665\">[3]</a></sup></span></h1><p>Smallpox was extremely deadly, probably killing 300 million people in the 20th century alone. The last known cases occurred in 1977, and smallpox is now the only human disease that has been completely eradicated.</p><p>So how was this accomplished?</p><p>Before we had a smallpox vaccine, we had the practice of&nbsp;<strong>variolation</strong> \u2014 deliberately exposing people to material from smallpox scabs or pus, in order to protect them against the disease (variolation traces back to 16th century China). While variolation made cases of smallpox much less severe, variolation infected the patient and could spread the disease to others, and the severity of the infection could not be easily controlled. So variolation did not lead to the elimination of smallpox from the population.</p><p>In the late 18th century, Edward Jenner demonstrated that exposure to cowpox \u2014 a much less severe disease that turns out to be related \u2014 protected people against smallpox. This, in turn, led to the invention of a&nbsp;<strong>vaccine</strong> against smallpox (the first vaccine ever).&nbsp;</p><p>In the 19th and 20th centuries, further improvements were made to the smallpox vaccine, and many states were running programs to vaccinate significant portions of the population. By 1959, the World Health Organization (WHO) launched a global program to eradicate smallpox . This involved a coordinated effort to immunize large numbers of people, isolate infected individuals, and monitor the spread of the disease. The program used a technique known as&nbsp;<strong>ring vaccination</strong>, which involved vaccinating people who had been in contact with infected individuals, in order to create a protective \"ring\" around the infected person and prevent further spread of the disease.</p><h1>Excerpts from&nbsp;<a href=\"https://ourworldindata.org/smallpox\"><u>the Our World in Data entry</u></a></h1><h2>Introduction</h2><blockquote><p>Smallpox is the only human disease that has been successfully eradicated.</p><p>Smallpox, an infectious disease caused by the variola virus, was a major cause of mortality in the past, with historic records of outbreaks across the world. Its historic death tolls were so large that it is often likened to the Black Plague.</p><p>The eradication of smallpox is therefore a major success story for global health for several reasons: it was a disease that was endemic (and caused high mortality rates) across all continents; but was also crucial to advances in the field of immunology. The smallpox vaccine was the first successful vaccine to be developed.</p></blockquote><p>[...]</p><h2>How many died of smallpox?</h2><blockquote><p>In his review paper \u2018The eradication of smallpox \u2013 An overview of the past, present, and future\u2019 Donald Henderson reports that during the 20th century alone \u201can estimated 300 million people died of the disease.\u201d</p><p>In his book Anderson suggests that in the last hundred years of its existence smallpox killed \u201cat least half a billion people.\u201d 500 million deaths over a century means 5 million annual deaths on average.</p></blockquote><p>[...]</p><h2>Eradication across the world</h2><blockquote><p>The last variola major infection was recorded in Bangladesh in October 1975, and the last variola minor infection occurred two years later in Merka, Somalia, on October 26th, 1977. During the following two years, WHO teams searched the African continent for further smallpox cases among those rash-like symptoms (which is a symptom of numerous other diseases). They found no further cases.</p></blockquote><p>[...]</p><blockquote><p>The world map shows the year in which each country recorded the last endemic case of smallpox. Europe, North America and Australia managed to eliminate smallpox relatively early, most by the 1940s (predating the WHO\u2019s Intensified Smallpox Eradication Program, which was launched in 1966). Countries across Sub-Saharan Africa, Latin America and Asia eliminated smallpox several decades later in the 1960s and 70s.</p></blockquote><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/decade-in-which-smallpox-ceased-to-be-endemic-by-country\">\n\t\t\t\t\t<div data-owid-slug=\"decade-in-which-smallpox-ceased-to-be-endemic-by-country\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/decade-in-which-smallpox-ceased-to-be-endemic-by-country\">\n\t\t\t\t\t&lt;/div&gt;\n\t\t\t\t</iframe></div></div></figure><p>[...]</p><h2>How was global decline &amp; eradication achieved?&nbsp;</h2><h3>Discovery of variolation</h3><blockquote><p>Variolation (sometimes also inoculation), refers to the deliberate transmission of viral matter.</p><p>Before the year 1000, Indians and the Chinese had already observed that contraction of smallpox protected children against any future outbreaks of the disease. As a consequence they developed a procedure that involved the nasal inhalation of dried smallpox scabs by three-year-olds.</p></blockquote><p>[...]</p><blockquote><p>The disadvantage of variolation, however, was that during the course of the mild infection the person became a carrier of the disease and could infect other people. Additionally, it was difficult to control the severeness of the infection which sometimes developed into a full-blown smallpox case that could lead to the person\u2019s death.</p><p>This meant that the practice usually reduced the severeness of an infection and the likelihood of deaths but that it would never lead to eliminating the virus. If anything, it helped to spread the virus in a population even further and thereby encouraged its survival.</p></blockquote><h3>Vaccine against smallpox</h3><blockquote><p>At the end of the 18th century British surgeon and physician Edward Jenner (1749-1823) pioneered the first ever vaccination against an infectious disease. He himself had been inoculated with smallpox at the age of 8 and later as a surgeon, variolation was part of his work.23 He observed that people who had suffered from cowpox would subsequently have a very mild, if at all visible reaction to the smallpox variolation. At the time unknowingly, he had discovered that the cowpox and variola viruses were members of the same orthopoxvirus family.</p><p>He hypothesized that variolation using the cowpox virus would protect children against smallpox as well. Since cowpox infections were much milder and never fatal, this would eliminate the problem of variolated children being carriers of smallpox and sometimes dying of the virus developing into a full-blown infection. On top of protection against the symptoms, it could reduce the stock of humans that the variola virus needed for survival and brought elimination and eventually eradication of smallpox into the realm of possibility.</p><p>In May 1796, Jenner inoculated a boy with cowpox, and then a few months later with the smallpox virus. When the boy did not develop any smallpox symptoms in response to being variolated, his hypothesis of the cowpox offering protection from smallpox was confirmed motivating his further research trials.</p><p>Initially, Jenner faced major barriers to spreading the word about his discovery. When he submitted a paper outlining his findings to the journal Philosophical Transactions edited by the Royal Society, it was rejected. They even advised him not to pursue his ideas any further, pointing to the detrimental impact on his career and reputation. Undeterred, he published his work with an increased number of trials at his own expense two years later (in 1798). He also went on to convince colleagues and supply them with vaccines in other British cities of his new procedure that became known as vaccination (derived from the Latin word for cow, vacca).</p><p>By 1802, the British Parliament did acknowledge his important contribution and awarded him \u00a330,000. Meanwhile, vaccination had spread to most of Europe and New England.24</p><p>His 1798 publication Inquiry into the Variolae vaccinae known as the Cow Pox had been translated into German, French, Spanish, Dutch, Italian, and Latin within three years. US President Thomas Jefferson figured importantly in the widespread application of vaccination throughout the United States and in 1806, he thanked Edward Jenner in a letter for his discovery and famously predicted \u201cFuture generations will know by history only that the loathsome smallpox existed and by you has been extirpated.\u201d25</p><p>The dramatic decline in smallpox fatalities in response to Jenner\u2019s vaccine can be traced in the chart, which shows the number of deaths due to smallpox as a share of all deaths in London from 1629 to 1902. Before the introduction of a smallpox vaccine in 1796, on average 7.6% (1-in-13) of all deaths were caused by smallpox. Following introduction of the vaccine, we see a clear decline in smallpox deaths.</p></blockquote><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/deaths-from-smallpox-in-london\">\n\t\t\t\t\t<div data-owid-slug=\"deaths-from-smallpox-in-london\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/deaths-from-smallpox-in-london\">\n\t\t\t\t\t&lt;/div&gt;\n\t\t\t\t</iframe></div></div></figure><h3>Smallpox Eradication Program</h3><blockquote><p>It was only with the establishment of the World Health Organization (WHO) in the aftermath of World War II that international quality standards for the production of smallpox vaccines were introduced. This shifted the fight against smallpox from a national to international agenda. It was also the first time that global data collection on the prevalence of smallpox was undertaken.</p><p>By 1959, the World Health Assembly, the governing body of the World Health Organization (WHO) had passed a resolution to eradicate smallpox globally. It was not until 1966, however, that the WHO provided the \u2018Intensified Smallpox Eradication Program\u2019 with funding to increase efforts for smallpox eradication.</p><p>By 1966, the number of infections of smallpox had already substantially been reduced by national governments\u2019 efforts. Nonetheless, skepticism about the feasibility of eradication prevailed and the WHO lacked experience in administering projects that required both technical and material support, as well as coordination across countries. Furthermore, the funding provided to the Intensified Smallpox Eradication Programme was insufficient to meet global needs, resulting mostly in vaccine shortages.</p><p>Further still, continued globalization and growth of international air travel resulted in the continual re-introduction of the disease into countries that had previously managed to eliminate smallpox.</p></blockquote><h3>Overcoming the last mile problem: ring vaccination</h3><blockquote><p>Smallpox\u2019s eradication was greatly spurred by making use of the fact that smallpox transmission occurs via air droplets. Initially, the WHO had pursued a strategy of mass vaccination which attempted to vaccinate as many people as possible, hoping that herd immunity (explained in our&nbsp;<a href=\"https://ourworldindata.org/vaccination#how-vaccines-work-herd-immunity-and-reasons-for-caring-about-broad-vaccination-coverage\"><u>vaccine entry</u></a>) would protect the whole population. Soon, however, vaccination efforts were targeted locally around smallpox cases as smallpox was transmitted by sick patients\u2019 air droplets. This is known as the&nbsp;<strong>ring vaccination principle</strong>.</p><p>People who had been in direct contact with a smallpox patient over the last two weeks were quarantined and vaccinated. The downside of such an approach was that the virus could spread easily if it was re-introduced from overseas. This was the case in Bangladesh, for example, which had previously eliminated smallpox until 1972 when it was brought back from across its border with India.</p><p>Despite the risk of re-introductions, ring vaccination greatly reduced the cost of the eradication campaign. The number of administered vaccines dropped and smallpox was increasingly brought under control. Regional elimination came within reach.</p><p>One of the last strongholds of the variola virus was India. While 57.7 percent of global reported smallpox cases were reported in India in 1973, this increased to 86.1 percent in 1974. One major push in vaccination campaigns, however, successfully drove down the number of infections to zero in India in 1976.</p></blockquote><p>[...]</p><p>The Our World in Data&nbsp;<a href=\"https://ourworldindata.org/smallpox\"><u>entry</u></a> has more information about smallpox \u2014&nbsp;<a href=\"https://ourworldindata.org/smallpox#impact-on-life-expectancy\"><u>the impact of eradication/variolation on life expectancy</u></a>,&nbsp;<a href=\"https://ourworldindata.org/smallpox#lives-saved-from-smallpox-eradication\"><u>estimated numbers for lives saved by its eradication</u></a>,&nbsp;<a href=\"https://ourworldindata.org/smallpox#data-quality-definitions\"><u>sources and more context</u></a>, etc.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn17g7rdgjzd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn17g7rdgjzd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Two days have a claim to this title:&nbsp;<a href=\"https://www.who.int/news/item/13-12-2019-who-commemorates-the-40th-anniversary-of-smallpox-eradication\"><u>December 9</u></a> and&nbsp;<a href=\"https://www.who.int/news-room/events/detail/2020/05/08/default-calendar/commemorating-the-40th-anniversary-of-smallpox-eradication\"><u>May 8</u></a>. We\u2019re going with the former here.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny6lv8ckclc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy6lv8ckclc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>On this date, we usually&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jk7A3NMdbxp65kcJJ/500-million-but-not-a-single-one-more?commentId=dt6TNpAjDX5zmuhtW\"><u>feature</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jk7A3NMdbxp65kcJJ/500-million-but-not-a-single-one-more\"><u>500 Million, But Not A Single One More</u></a> \u2014 a beautiful and classic post about the impact of smallpox eradication. But we&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/jk7A3NMdbxp65kcJJ/500-million-but-not-a-single-one-more?commentId=qgMP9vggnd2XCQAF3\"><u>recently featured it</u></a> and have gone for a different great piece of content here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7x6tkofn665\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7x6tkofn665\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Written by me with the help of ChatGPT.</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "WDKDjak7CW9Yx95Ny", "title": "Best Global Health and Development charities in India?", "postedAt": "2022-12-09T06:26:11.433Z", "htmlBody": "<p>Any research you have seen regarding the most effective causes in this subspace? Thanks!</p>\n", "user": {"username": "Radon4049"}}, {"_id": "bdmDDiK77sgH5hpzN", "title": "Lessons Learned from EA Hackathon following EAGxBerkeley", "postedAt": "2022-12-09T17:46:35.590Z", "htmlBody": "<p>The purpose of this post is to provide a debrief on the outcomes of the <a href=\"https://forum.effectivealtruism.org/posts/BL8Rny4hBgDvLsjci/hackathon-on-mon-12-5-to-follow-eagxberkeley\">Hackathon</a> held on 12/5/22 following EAGxBerkeley. &nbsp;</p><p>Goals: &nbsp;</p><ul><li>Provide an engaging activity as part of the participant-driven events surrounding the conference</li><li>Promote skill building for software engineers and aspiring developers</li><li>Create an open and welcoming space for non-software engineers to engage in project work and coworking&nbsp;</li><li>Facilitate opportunities for networking, mentorship, and general community building for <a href=\"https://subscribepage.io/eaSoftwareEngineers\">EA Software Engineers</a></li><li>Collect feedback and determine whether this kind of event would be worth holding again</li></ul><p>What went well:</p><ul><li>Hosting a Software Engineering meetup and Hackathon planning event during the conference helped boost awareness, solidified people's intentions to attend, and helped attendees hit the ground running</li><li>We also gave out hard copy <a href=\"https://www.canva.com/design/DAFTXbzo0v4/qEUgYNJajeE3aRk1F44P5A/view?utm_content=DAFTXbzo0v4&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton\">flyers</a> at Berkeley coworking spaces and during the conference which I think helped with awareness and stickiness</li><li>More attendees than expected! &nbsp;We were anticipating 25-30, but we got 45</li><li>Excellent support from conference organizers and venue staff. &nbsp;We brought / borrowed power strips, extension cables, whiteboards, and large writing pads that felt pretty essential throughout the day</li><li>Out of a possible score of 5, the average respondent to the feedback survey (n=8) rated overall experience as 4.125. &nbsp;Impact was rated as 3.75. &nbsp;Collaboration was rated as 4.125. Learnings were rated as 3.5. &nbsp;Fun was rated as 4.375</li><li><a href=\"https://photos.app.goo.gl/J4pGjdWS8Xgwvznz8\">Google Photos</a> album to collect photos and videos from the event (and create a sense of FOMO for everyone who couldn't make it)</li></ul><p>What went poorly:</p><ul><li>Wifi at the venue was quite bad. &nbsp;In the future, we would mitigate this with setting up <a href=\"https://www.pcmag.com/picks/the-best-mobile-hotspots\">mobile hotspots</a> for the day, or we could find a different venue</li><li>We should have swapped the order of the <a href=\"https://docs.google.com/presentation/d/1gL8mBgN-ilsj2W8L-6dU6aRDFC26Qt7lhRP_a_neV-Q/edit\">dev environment</a> and <a href=\"https://github.com/pranavgade20/practice-git\">git</a> learning sessions. &nbsp;Also, given the wifi situation, we should have told new devs to download VSCode in advance of the event</li><li>Lack of specificity in project pitches. We should have asked pitches to include the tech stack and relative level of difficulty of various roles</li><li>Low response rate to feedback survey. &nbsp;We should have dedicated the time and requested that participants fill out the form before we ended the event</li></ul><p>Worth experimenting with in the future:</p><ul><li>Explore hosting a hackathon on Thursday or Friday before the conference starts</li><li>Hackathon on a specific priority area could have more impact by bringing together people with a dedicated focus</li><li>Consider hosting a more formal event with an application, a presentation of results, and a judging component</li><li>Bay Area organizers should consider hosting more events in the <a href=\"https://shop.sportsbasement.com/blogs/community-spaces\">Community Spaces</a> of Sports Basements. &nbsp;They're free and were great to work with</li><li>Use <a href=\"https://preview.mailerlite.io/preview/162793/emails/73716726100919517\">reoccurring office hours</a> to encourage continued project</li></ul><p>Resources:</p><ul><li>Forms: &nbsp;<a href=\"https://forms.gle/njrXpnKPk9SaamUV6\">Expression of Interest</a>, <a href=\"https://forms.gle/kMx2VDKeJu8JNYJS6\">Sign-In</a>, <a href=\"https://forms.gle/hhnibyZPvZddRWr99\">Feedback</a> \u2014 if you want to replicate these Google Forms, email <a href=\"mailto:easoftwareengineers@gmail.com\">easoftwareengineers@gmail.com</a> and I'll add you as a contributor so you can make a copy of each one</li><li>Event announcement: &nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BL8Rny4hBgDvLsjci/hackathon-on-mon-12-5-to-follow-eagxberkeley\">Forum post</a>, <a href=\"https://www.facebook.com/events/3427044414251147/\">Facebook event</a>, <a href=\"https://www.canva.com/design/DAFTXbzo0v4/qEUgYNJajeE3aRk1F44P5A/view?utm_content=DAFTXbzo0v4&amp;utm_campaign=designshare&amp;utm_medium=link2&amp;utm_source=sharebutton\">flyers</a></li><li>Learning sessions: &nbsp;<a href=\"https://github.com/pranavgade20/practice-git\">Git workshop</a>, <a href=\"https://docs.google.com/presentation/d/1gL8mBgN-ilsj2W8L-6dU6aRDFC26Qt7lhRP_a_neV-Q/edit\">Setting up your Dev Environment</a>, <a href=\"https://docs.google.com/presentation/d/1Khl7cPSpzmBdy0hdOSR1FdayXTuFX9dn-NOwpiQ7Ek4/edit\">Intro to Frontend</a>, <a href=\"https://docs.google.com/presentation/d/1KzFE1bcWAF7Hl2ErQCMjA3Wrqdor8cTkAaSNrrZnZmU/edit\">HTTP and CDNs</a></li></ul><p>Outcomes:</p><ul><li><a href=\"http://givecalc.policyengine.org/\">Give Calc tool</a> \u2014 a Streamlit app that provides advice on how much to donate for people interested in lowering their net income by a percentage or an absolute amount. It's based on the <a href=\"https://policyengine.org/\"><u>PolicyEngine</u></a> project, which is an open-source tool for exploring the effects of tax and benefit policies.</li><li>Contributions to open source projects \u2014 <a href=\"https://github.com/numpy/numpy/issues/19059\">numpy</a>, <a href=\"https://github.com/pytest-dev/pytest/\">pytest</a>, <a href=\"https://github.com/Zac-HD/shed/\">shed</a></li><li>Updates to landing page for <a href=\"https://impactmarkets.io/\">Impact Markets</a> \u2014 a three sided marketplace that aims to eliminate externalities for funding impactful projects</li></ul><hr><p>Impactfulness: &nbsp;we set the bar intentionally low for output from the event, which lasted from 10am - 5pm due to constraints of the venue. &nbsp;We figured seven hours would be enough time to make some progress on simple features, but nothing earth shattering, particularly once setup, lunch, and breakdown time is factored in. &nbsp;We hope that EA <a href=\"https://preview.mailerlite.io/preview/162793/emails/73716726100919517\">community office hours</a> will spur continued project work</p><p>Tractability: &nbsp;because the Hackathon was pretty unstructured, the main cost to set it up was the time of the organizing team and the cost of lunch. &nbsp;Now that the organizing work has been put into place once, it would be relatively low lift to reuse this playbook when hosting the next event</p><p>Neglectedness: &nbsp;the event aimed to serve what we hypothesized was an unmet demand for fun, quasi-structured activities around the time of the conference. &nbsp;This &nbsp;fact that 45 people showed up and no one reported having a bad time (i.e., no feedback respondent rated their overall experience as less than 3 out of 5) seems to validate the experiment</p><hr><p>Summary:</p><p>Hosting a Hackathon seems worthwhile. &nbsp;Attendance was high, feedback was positive, some skill building took place, and at least one person reported feeling more engaged with EA as a result of the event.&nbsp;</p><p>We have identified some things that we think we did well, along with many opportunities for improvement. &nbsp;It would be straightforward to do this again as a community building exercise. &nbsp;With a more structured approach (such as focusing on a specific cause area and asking applicants to apply), we could potentially generate more impactful outcomes.</p><p>More EAG(x) might consider incorporating this activity into the events before or following the conference. &nbsp;Furthermore, city groups might be interested in adding a hackathon to their calendar of event programming.</p>", "user": {"username": "NicoleJaneway"}}, {"_id": "2fcnR8EXgWFy67g8K", "title": "How should EA Forum readers vote on comments/posts/tags?", "postedAt": "2022-12-09T11:30:35.061Z", "htmlBody": "<p>EA Forum readers should arguably vote such that the comments/posts/tags which have more karma, thus being more visible, are also the ones which deserve more attention. I wonder what this implies in terms of voting norms. Should one vote based on:</p><ul><li>Value?<ul><li>Do not vote if the comment/post/tag is roughly neutral.</li><li>Upvote (downvote) if the comment/post/tag is good (bad).</li><li>Strongly upvote (downvote) if the comment/post/tag is very good (bad).</li></ul></li><li>Difference between current and desired karma?<ul><li>Do not vote if the comment/post/tag has roughly as much karma as desired.</li><li>Upvote (downvote) if the comment/post/tag has less (more) karma than desired.</li><li>Strongly upvote (downvote) if the comment/post/tag has much less (more) karma than desired.</li></ul></li><li>Confidence about the sign of the difference between current and desired karma?<ul><li>Do not vote if not confident the comment/post/tag should have more or less karma.</li><li>Upvote (downvote) if confident the comment/post/tag should have more (less) karma.</li><li>Strongly upvote (downvote) if very confident the comment/post/tag should have more (less) karma.</li></ul></li><li>Other?</li><li>A combination of the above?</li></ul><p>My question is about voting under the current voting system. However, there is also the option of changing it, as discussed&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YajssmjwKndBTahQx/improving-karma-usd8mn-of-possible-value-my-estimate\"><u>here</u></a> by Nathan Young.</p>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "8CRFTxrRLiizypyBu", "title": "r.i.c.e.'s neonatal lifesaving partnership is funded by GiveWell; a description of what we do", "postedAt": "2022-12-09T18:10:10.011Z", "htmlBody": "<p><i>[The Forum found an unexpected way to help! A previous version of this post mistakenly suggested that a large donation from GiveWell risked changing r.i.c.e. from a public charity to a private foundation. We previously experienced this sort of risk with funding from the Bill and Melinda Gates Foundation. We are very excited to learn from a Forum poster that GiveWell poses no such risk! Thank you! While we are still </i><a href=\"https://paypal.me/riceinstitute?country.x=US&amp;locale.x=en_US\"><i>accepting additional donations</i></a><i> to improve the health of a larger number of babies, there is no additional urgency to maintain the charity status of r.i.c.e.]&nbsp;</i></p><p>I\u2019m writing on behalf of my team at r.i.c.e., which is honored to be highlighted in GiveWell\u2019s &nbsp;\u201c<a href=\"https://blog.givewell.org/2022/11/23/giving-recommendations-2022/\">Our recommendations for giving in 2022</a>\u201d post. &nbsp; In this post, I present the details of our program<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4upmkr6bpqu\"><sup><a href=\"#fn4upmkr6bpqu\">[1]</a></sup></span>&nbsp; that prevents neonatal deaths inexpensively by causing the implementation of a Kangaroo Mother Care program in India. &nbsp;</p><h1>A description of our work, in brief</h1><p>Some of you may be familiar with my work in <a href=\"https://forum.effectivealtruism.org/posts/ahP7vBeGLq2QfEg5j/avoiding-the-repugnant-conclusion-is-not-necessary-for\">population ethics</a> or as Director of the <a href=\"https://sites.utexas.edu/pwi/\">Population Wellbeing Initiative</a> at UT-Austin (or, more likely, for distributing utilitarianism t-shirts). Since long before this phase of my career, I\u2019ve also been Executive Director of <a href=\"https://riceinstitute.org/\">r.i.c.e.</a>, a 501(c)3 public charity working on <a href=\"https://marginalrevolution.com/marginalrevolution/2017/08/where-india-goes.html\">early-life health in rural north India.</a></p><p>In collaboration with the Government of Uttar Pradesh and an organization in India, we support a <a href=\"https://www.theguardian.com/lifeandstyle/2022/nov/15/kangaroo-mother-care-best-for-early-and-low-weight-babies-says-who\">Kangaroo Mother Care</a> program to promote neonatal survival in a context where low birth weight babies would not otherwise receive such lifesaving care. KMC is a well-established tool for preventing deaths that we did not invent. &nbsp;Instead, our contribution is managerial innovation: <strong>We developed a public-private partnership to cause the government\u2019s KMC guidelines to in fact be implemented cost-effectively in a public hospital where many low birth weight babies are born.</strong> &nbsp;That public healthcare systems in developing countries <a href=\"https://academic.oup.com/jeea/article-abstract/6/2-3/487/2295851?redirectedFrom=fulltext\">fail to implement life-saving policies</a> and programs is a well-known problem in global health and in development economics. &nbsp;Because <a href=\"https://www.nejm.org/doi/full/10.1056/NEJMoa2026486\">KMC is known to prevent neonatal death</a>, and because the collaboration that we fostered found new ways to overcome the <a href=\"https://www.givewell.org/international/technical/programs/kangaroo-mother-care\">barriers to implementation</a> in a context where many babies are not otherwise receiving needed care, it is not surprising that we can prevent neonatal deaths inexpensively.&nbsp;</p><p>Our statistics show that we save lives very cost-effectively; indeed, about as inexpensively as any life-saving program known to EA. These statistics led to investments this year by two EA funders: first by <a href=\"https://docs.google.com/document/d/15R9Kvd0eW5tFz5aandtmB4ONeR26VAUkYdIncvd6Yvw/edit\">Founders Pledge</a> and then by <a href=\"https://blog.givewell.org/2022/11/23/giving-recommendations-2022/\">GiveWell</a>, after in-depth investigations of our work and our evidence. We have big plans for 2023, including continuing to save lives, starting a formal impact evaluation, and doing the advocacy and partnership-building needed to scale up the program to more districts.</p><p>If you would like to further support this work, please <a href=\"https://paypal.me/riceinstitute?country.x=US&amp;locale.x=en_US\">donate to r.i.c.e. at this link to PayPal</a>. It will let you donate either one time or by establishing a recurring monthly donation. If you don\u2019t like PayPal, you can mail a check to RICE Institute, Inc., 472 Old Colchester Rd, Amston, CT 06231.</p><h1>Longer set of details</h1><h2>What is Kangaroo Mother Care and why is it good?</h2><p>Kangaroo Mother Care (KMC) is a way to inexpensively keep low birth weight babies (and other neonates) clean, fed, and warm. &nbsp;Here is GiveWell\u2019s summary of <a href=\"https://www.givewell.org/international/technical/programs/kangaroo-mother-care \">Kangaroo Mother Care in general</a>. &nbsp;It is a bit broader than our program. &nbsp;To emphasize: KMC has been well-known to save lives for years. &nbsp;Our workwomanlike contribution is merely to get it implemented in a place where there are lots of low birth weight babies, in part because <a href=\"https://academic.oup.com/ej/article/131/638/2478/6226660\">mothers are themselves often underweight</a>. &nbsp;</p><p>One important fact is that little babies can\u2019t regulate their own temperatures very well. &nbsp;Our bodies have to be in a certain temperature range to function. &nbsp;So temperature management is a big part of neonatal health. &nbsp;When I wrote my <a href=\"https://marginalrevolution.com/marginalrevolution/2019/09/pollution-climate-change-and-indias-choice-between-policy-and-pretense.html\">book about environmental health for children in India</a>, I worried that hot and humid days due to climate change would be bad for newborns who could not shed heat effectively. &nbsp;But by far and away the bigger and more common problem is that neonates can\u2019t keep their heat. &nbsp;</p><p>The way that KMC solves this problem is skin-to-skin contact with a caregiver. &nbsp;The program trains parents to keep their babies on their chests. &nbsp;We provide them with a safe and clean place to do this. &nbsp;Mothers receive a special cloth wrap, sewn by local tailors, to hold the babies safely in place.</p><p>Once the babies are in place, the stage is set for early initiation of breastfeeding. &nbsp;Part of a KMC nurse's work is lactation consulting, which includes both teaching moms and troubleshooting. &nbsp;There are many options for moms whose babies aren't breastfeeding well. &nbsp;Such tiny babies eat a little bit, many times a day. &nbsp;Weight gain is carefully monitored. &nbsp;When they eventually go home\u2014sometimes after weeks\u2014a new mom and baby move into the now-available bed. &nbsp;The phone and home team follows up with phone calls, home visits, and supplies like thermometers and weighing scales that the family can borrow for their home.</p><p>There are now <a href=\"https://www.nejm.org/doi/full/10.1056/NEJMoa2026486\">many</a> <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/j.1651-2227.2005.tb01930.x?casa_token=KFLHMHYKymYAAAAA%3AHaPPp5RmyxjtoVADYOlbZh8tkBN_oJeAytOP-Onq1Wtcx8a5FXEYzj7l83IpyWxRiOd3tszP-bIzlA\">impact</a> <a href=\"https://pubmed.ncbi.nlm.nih.gov/32683720/\">evaluations</a> published in high-quality peer-reviewed journals showing that this sort of neonatal care saves lives cost-effectively. &nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6464509/\">This sentence links to a Cochrane review</a>. &nbsp;If you\u2019re up on that literature, the fact that we have actually managed to implement this is strong evidence that the program saves lives. &nbsp;But there is another important fact: <strong>The counterfactual care that the babies would receive in the absence of our program is much more limited than the counterfactual care in published impact evaluations of KMC.</strong> &nbsp;This is a very resource-constrained setting. &nbsp;Babies of a similar size and gestation born in rich countries would be kept in intensive care in an incubator for weeks. &nbsp;In this setting, neonatal care is so limited that most would, in fact, receive almost no medical care. &nbsp;That means this program's treatment effect per baby is even larger than in the published literature. &nbsp;(More on this point later.)</p><p>One final reason this project is possible and so cost-effective is that KMC is already the policy of the Government of Uttar Pradesh and the Government of India. &nbsp;We did not have to persuade leading policy-makers that KMC was a good idea. &nbsp;To the contrary, a visionary and insightful Secretary for Medical Education in Lucknow made this whole program possible. &nbsp;All we had to do was step in with a new public-private partnership that provides the staff and management who make KMC happen in practice.</p><h2>How did you estimate the cost-effectiveness of the program?</h2><p>We have not yet done an econometric impact evaluation of the sort that could be published in a journal. &nbsp;Indeed, that is part of what GiveWell has funded, and what we will be working on during the coming year. &nbsp;(<a href=\"https://link.springer.com/article/10.1186/s12884-020-03423-8\">The details of study inclusion and tracking can be hard.</a>) &nbsp;In partnership with medical faculty in India, we will do a matching-based impact evaluation, generating a counterfactual control group from babies at other public hospitals in other districts of Uttar Pradesh. &nbsp;</p><p>In the meantime, and in conversation with GiveWell, we produced cost-effectiveness estimates from the administrative records of the program and from other sources. Cost effectiveness (lives saved per dollar) is the product of these two dimensions:</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\frac{\\textrm{lives saved}}{\\textrm{dollars}} = \\frac{\\textrm{lives saved}}{\\textrm{babies helped}} \\times \\frac{\\textrm{babies helped}}{\\textrm{dollars}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.794em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.794em; top: -1.396em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">lives saved</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.794em; bottom: -0.795em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">dollars</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.794em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.191em; vertical-align: -0.795em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 6.012em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 6.012em; top: -1.396em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">lives saved</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 6.012em; bottom: -0.978em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">babies helped</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 6.012em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.374em; vertical-align: -0.978em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u00d7</span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 6.012em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 6.012em; top: -1.478em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">babies helped</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 6.012em; bottom: -0.795em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">dollars</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 6.012em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.273em; vertical-align: -0.795em;\" class=\"mjx-vsize\"></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><h3>Babies helped per dollar</h3><p>This is the more straightforward part. &nbsp;But it\u2019s worth pausing on, because it is not what impact evaluations in development economics usually emphasize. &nbsp;A paper in a journal like <i>Journal of Development Economics</i> would typically focus on the first term: lives saved per baby helped. &nbsp;To know how outcomes are really going to change, though, you need to know how many babies are going to be helped. &nbsp;</p><p>Here the partnership with the Government of Uttar Pradesh is essential. &nbsp;The program operates in a hospital where a lot of babies need more help than they are getting. &nbsp;Remember Peter Singer\u2019s classic parable about pulling a drowning child from a pond? &nbsp;Rather than wait for a drowning child on our daily walk, we spent years searching for a fabled pond in the distant woods that children keep falling into. &nbsp;Then, we collaborated with our partners to build and staff a lifeguard station.</p><p>The program has been spending about $5,000 a week, mostly on salaries for the nurses who directly interact with the mothers and babies, but also on program costs like materials, the cars and drivers for the home-visit teams, and food and rent for the nurses\u2019 apartments and offices. &nbsp;&nbsp;</p><p>As of the early summer, the program took in about 11 low-birthweight babies per week (slightly less than two per day!). &nbsp;So we spend about $455 per baby. &nbsp;This number being low is an important part of the project being cost-effective.</p><h3>Lives saved per baby helped</h3><p>Lives saved per baby helped is the fraction of babies who die despite going through our program minus the fraction of babies who would have died in the absence of our program. We keep track of the fraction of babies who were intaked, but who nevertheless die. &nbsp;That leaves quantifying the number of babies who would have died in the absence of the program. &nbsp;In our conversations with GiveWell, we did this a number of ways. Perhaps the best approach, because it best captures what we know and what we don\u2019t know, is to look at the <a href=\"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD002771.pub4/full\">Cochrane Review estimates</a> and think about them. &nbsp;The Cochrane review found that KMC reduced neonatal mortality by about 40%:</p><blockquote><p>\u201c<strong>KMC versus conventional neonatal care: </strong>At discharge or 40 to 41 weeks' postmenstrual age, KMC was associated with a statistically significant reduction in the risk of mortality (risk ratio [RR] 0.60, 95% confidence interval [CI] 0.39 to 0.92; eight trials, 1736 infants)\u201d</p></blockquote><p>That\u2019s pretty good! &nbsp;In our high-mortality, many-babies context, that\u2019s already a lot of lives saved. &nbsp;But we think that our impact per baby is even higher. &nbsp;Here\u2019s why. &nbsp;The Cochrane review is aggregating studies where the counterfactual healthcare is, in fact, conventional neonatal healthcare:</p><blockquote><p>\u201cGiven that the control group in studies evaluating continuous KMC was kept in incubators or radiant warmers, the potential beneficial effects of KMC on morbidity and mortality of LBW infants would be expected to be greatest in settings in which conventional neonatal care is unavailable.\u201d</p></blockquote><p>Unfortunately many of the babes who we are helping would not be able to receive conventional neonatal care, due to the resource constraints in this context. &nbsp;So the outcome without our program would generally be worse than the outcome without the programs that the Cochrane review has data about.</p><h3>Quantitative estimates</h3><p>Returning to the formula above, we can say</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\frac{\\textrm{dollars}}{\\textrm{lives saved}} = \\frac{\\textrm{dollars}}{\\textrm{babies helped}} \\div \\frac{\\textrm{lives saved}}{\\textrm{babies helped}}.\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.794em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.794em; top: -1.396em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">dollars</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.794em; bottom: -0.795em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">lives saved</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.794em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.191em; vertical-align: -0.795em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 6.012em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 6.012em; top: -1.396em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">dollars</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 6.012em; bottom: -0.978em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">babies helped</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 6.012em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.374em; vertical-align: -0.978em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u00f7</span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 6.012em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 6.012em; top: -1.396em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">lives saved</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 6.012em; bottom: -0.978em;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">babies helped</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 6.012em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.374em; vertical-align: -0.978em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.</span></span></span></span></span></span></span><p>Our estimate for dollars per baby helped is $455.&nbsp;</p><p>We think the lives saved per baby helped is probably around 1-in-4. To see why, one important background fact is that about a third of these babies would be likely to die without the program. &nbsp;About a tenth of them die with it. &nbsp;1/3 - 1/10 is close to 1/4. This gives an estimate of dollars per life saved of $455 / (1/4) = $1,800 on average.</p><p>For an alternative estimate, we can use the Cochrane review. &nbsp;1/3 dying without the program * 0.4 saved according to the Cochrane Review is 0.12 percentage points saved, ignoring the fact that the counterfactual in our case is worse than the counterfactual in the Cochrane studies. This gives an estimate of dollars per life saved of &nbsp;$455 / (0.12) = $3,800 on average.</p><p>When we get overprecise in a spreadsheet, our best guess is that the average cost per life saved is somewhere between $2,000 and $3,000. Compare that with the approximate $5,000 of some of the best interventions supported by the Effective Altruism community. &nbsp; But even if it is twice our best guess, or even a little more, it is an <i>excellent </i>value for cost-effective life-saving. &nbsp;Your donation can help us maintain the program, do a better job, help more babies, do a better impact evaluation, and build towards expanding it to other hospitals someday, if we find the right implementation partners. &nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4upmkr6bpqu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4upmkr6bpqu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Credit for this work needs to be widely shared.&nbsp; r.i.c.e. has partnered with an Indian organization and a large public medical college in Uttar Pradesh to expand that hospital\u2019s services for low birth weight and preterm babies, and follow them up at home after they leave the hospital.&nbsp; Together, the Indian organization and the staff of the public hospital operate the program.&nbsp; r.i.c.e. provides funding, advice, and advocacy. Where this post says \u201cwe\u201d and \u201cour,\u201d it often refers to the entire partnership.</p></div></li></ol>", "user": {"username": "deanspears"}}, {"_id": "JE3ZjEoWot6yQFSJj", "title": "Join the AI Testing Hackathon this Friday", "postedAt": "2022-12-12T14:24:30.220Z", "htmlBody": "<p><strong>TLDR</strong>: Participate online or in-person on the weekend of December 16th to 18th in a fun and intense&nbsp;<a href=\"https://alignmentjam.com/\"><u>AI safety research hackathon</u></a> focused on benchmarks, neural network verification, adversarial attacks and RL safety environments. We invite mid-career professionals to join, but the event is open to everyone (including non-coders) and we will provide starter code templates to help kickstart your team's projects.&nbsp;<a href=\"https://itch.io/jam/aitest\"><strong><u>Join here</u></strong></a>.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670856252/mirroredImages/JE3ZjEoWot6yQFSJj/vv0cwg3cydpywxp3z9ze.png\"><figcaption>&nbsp;</figcaption></figure><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670856252/mirroredImages/JE3ZjEoWot6yQFSJj/jhyjmvcplhlxgbtctrue.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/vllaa3t8ftjxqxzdwvme 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/bs7ksqbghbjt3iyqmaed 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/vdr3wmekipiozhn1us9g 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/cxz50tdllgwkfvpzoaki 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/c9voibdr6zypqacyi7kk 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/n3e20rbz2fqwdqso8cmn 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/ftfnnqyrpgmu1esdh7vc 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/y25h3alh4okvapvesauk 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/s6nyscuf8oigdbbqb3be 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/JE3ZjEoWot6yQFSJj/hjew5oi4dxc4taruvjxl 1347w\"></p><p>Below is an FAQ-style summary of what you can expect.</p><h2>What is it?</h2><p>The AI Testing Hackathon is a&nbsp;<strong>weekend-long event</strong> where teams of 1-6 participants conduct research on AI safety. At the end of the event, teams will submit a PDF report summarizing and discussing their findings.</p><p>The hackathon will take place on Friday from&nbsp;<strong>December 16th to 18th</strong>, and you are welcome to join for any part of it (see further details below). An expert on the topic will be speaking and we will introduce the topic for you on the launch date.</p><p>Everyone can participate and we encourage you to join especially if you\u2019re&nbsp;<strong>considering AI safety from another career&nbsp;</strong>. We prepare templates for you to start out your projects and you\u2019ll be surprised what you can accomplish in just a weekend \u2013 especially with your new-found friends!</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1670856252/mirroredImages/JE3ZjEoWot6yQFSJj/a7ieky30czbqsoppsa4i.png\"></p><p><i>Read more about how to join, what you can expect, the schedule, and what previous participants have said about being part of the hackathon below.</i></p><h2>Why AI testing?</h2><p>AI safety testing is becoming increasingly important as governments require rigorous safety certifications. The deployment of the EU AI Act and the development of AI standards by NIST in the US will both necessitate such testing.</p><p>The use of large language models, such as ChatGPT, has emphasized the need for safety testing in modern AI systems. Adversarial attacks and neural Trojans have become more common, highlighting the importance of testing for robustness and viruses in neural networks to ensure the safe development and deployment of AI.</p><p>In addition, the rapid development of related fields, such as automatic verification of neural programs and differential privacy, offers promising research for provably safe AI systems.</p><p>There is relatively little existing literature from AI safety on AI safety metrics and testing, though anomaly detection in various forms is becoming more prevalent:</p><ul><li><a href=\"https://www.alignmentforum.org/posts/GbAymLbJdGbqTumCN/more-detailed-proposal-for-measuring-alignment-of-current#Defining_the_metric\"><u>Beth Barnes</u></a> is currently working on testing alignment and has detailed a more specific project proposal for testing.</li><li><a href=\"https://www.alignmentforum.org/posts/AtfQFj8umeyBBkkxa/a-bird-s-eye-view-of-the-ml-field-pragmatic-ai-safety-2#The_importance_of_defining_the_problem\"><u>Hendrycks and Woodside</u></a> described defining metrics as a core goal of AI safety in the Pragmatic AI Safety series due to their strength as a methodological and machine learning tool.</li><li><a href=\"https://www.alignmentforum.org/posts/iznohbCPFkeB9kAJL/inverse-scaling-prize-round-1-winners\"><u>The inverse scaling team</u></a> has&nbsp;created relatively rigorous testing criteria for goal misgeneralization in language models with the inverse scaling benchmark, arguably a strong metric for general alignment.</li><li><a href=\"https://www.alignmentforum.org/posts/yYkrbS5iAwdEQyynW/how-do-new-models-from-openai-deepmind-and-anthropic-perform\"><u>Evans, Lin and Hilton</u></a> have created the most well-known inverse scaling benchmark with the TruthfulQA work, testing for truthfulness in LLMs.</li><li><a href=\"https://forum.effectivealtruism.org/posts/NbiHKTN5QhFFfjjm5/ai-safety-seems-hard-to-measure\"><u>Karnofsky</u></a> recently wrote a piece on why testing AI is hard due to deceptive generalization and the difficulty of testing current AI to understand AGI.</li><li><a href=\"https://www.alignmentforum.org/posts/vwt3wKXWaCvqZyF74/mechanistic-anomaly-detection-and-elk\"><u>Paul Christiano</u></a> is working on mechanistic anomaly detection, a test for neural anomalies from new latent knowledge situations.</li><li><a href=\"https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing\"><u>Redwood Research</u></a> has developed causal scrubbing, a strategy for automated hypothesis testing on AI.</li><li>See also the inspiration further down, such as&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/12VW7-rU7vnXW4k862y0iCy0sX_-tkZz6KRKm0dtsnyo/edit#gid=0\"><u>the benchmarks table</u></a>.</li></ul><p>Overall, AI testing is a very interesting problem that requires a lot of work from the AI safety community both now and in the future. We hope you will join the hackathon to explore this direction further!</p><h2>Where can I join?</h2><p>All the hackathon information and talks are happening on the Discord server that everyone is invited to join:&nbsp;<a href=\"https://discord.gg/3PUSbdS8gY\"><u>https://discord.gg/3PUSbdS8gY</u></a>.</p><p>Besides this, you can participate online or in person at several locations. This hackathon, online is the name of the game since there\u2019s end-of-year deadlines and Christmas but you\u2019re welcome to&nbsp;<a href=\"https://alignmentjam.com/running\"><u>set up your own jam site</u></a> as well. You can work online on Discord or directly in the&nbsp;<a href=\"https://app.gather.town/app/iEPL2kx1LY4OeKHv/Hackathon%20Space\"><u>GatherTown research space</u></a>.</p><p>For this hackathon, our in-person locations include the ENS Ulm in Paris at the most prestigious ML master in France, Delft University of Technical and Aarhus University. More locations might join as well.</p><p>You\u2019ll also have to sign up on itch.io to submit your projects:&nbsp;<a href=\"https://itch.io/jam/aitest\"><u>Join the hackathon here</u></a>. This is also where you can see an updated list of the locations.</p><h2>What are some examples of AI testing projects I can make?</h2><p>As we get closer to the date, we\u2019ll add more ideas on&nbsp;<a href=\"https://aisafetyideas.com/list/test-the-ai\"><u>aisafetyideas</u></a> as inspiration.</p><ul><li>Create an adversarial benchmark for safety using all the cool ChatGPT jailbreaks people have come up with</li><li>Apply automated verification on neural networks to test for specific safety properties such as corrigibility and behavior</li><li>Use new techniques to detect Trojans in neural networks, malicious perturbations that change the output of the networks</li><li>Create tests for differential privacy methods (see e.g.&nbsp;<a href=\"https://arxiv.org/pdf/2112.02918.pdf\"><u>Boenisch et al.'s work</u></a>)&nbsp;</li><li>Create an RL environment that can showcase some of the problems of AI safety such as deceptive alignment</li><li>Run existing tests on more models than they have been run on before</li></ul><p>Also&nbsp;<a href=\"https://www.lesswrong.com/posts/hhhmcWkgLwPmBuhx7/results-from-the-interpretability-hackathon\"><u>check out the results</u></a> from the last hackathon to see what you might accomplish during just one weekend. Neel Nanda was quite impressed with the full reports given the time constraint! You can see projects from all hackathons&nbsp;<a href=\"https://alignmentjam.com/jams#:~:text=Language%20Model%20Hackathon\"><u>here</u></a>.</p><h3>Inspiration list</h3><p>Check out the continually updated inspirations and resources page on the Alignment Jam website&nbsp;<a href=\"https://alignmentjam.com/aitest\"><u>here</u></a>. Here are a few of the resources:</p><p>Websites, lists and tools for AI testing:</p><ul><li><a href=\"https://griddly.ai/\"><u>GriddlyJS</u></a>: A flexible engine to create grid world reinforcement learning environments</li><li><a href=\"https://crfm.stanford.edu/helm/v1.0/?\"><u>HELM</u></a>: Holistic Evaluation of Language Models. An impressive and comprehensive evaluation of language models on a large number of benchmarks, a few of them safety-related</li><li><a href=\"https://github.com/facebookresearch/nle\"><u>The NetHack reinforcement learning action dataset and environment</u></a>.</li><li><a href=\"https://github.com/cleverhans-lab/cleverhans\"><u>Clever Hans</u></a>: An adversarial robustness evaluation benchmark</li><li><a href=\"https://github.com/cleverhans-lab/cleverhans\"><u>CoinRun</u></a>: OpenAI\u2019s RL generalization testing environment</li><li><a href=\"https://openai.com/blog/safety-gym/\"><u>OpenAI Safety Gym</u></a>: An environment to evaluate RL agents in safety-critical exploration tasks and more</li><li><a href=\"https://docs.google.com/spreadsheets/d/12VW7-rU7vnXW4k862y0iCy0sX_-tkZz6KRKm0dtsnyo/edit#gid=0\"><u>A list of alignment benchmarks</u></a></li></ul><p>Technical papers on benchmarking and automated testing:</p><ul><li><a href=\"https://arxiv.org/abs/2202.03286\"><u>Using LLMs to red team LLMs directly</u></a></li><li>Adaptive Testing:&nbsp;<a href=\"https://aclanthology.org/2022.acl-long.230.pdf\"><u>Adaptive Testing and Debugging of NLP Models</u></a> and&nbsp;<a href=\"https://homes.cs.washington.edu/~marcotcr/acl21_polyjuice.pdf\"><u>Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models</u></a></li><li>CheckList, a dataset to test models:&nbsp;<a href=\"https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf\"><u>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</u></a></li><li><a href=\"https://arxiv.org/abs/2112.02918\"><u>Federated learning is not private</u></a></li><li><a href=\"https://openai.com/blog/critiques/\"><u>Using LLMs to assist human-written critiques</u></a></li><li><a href=\"https://verifieddeeplearning.com/\"><u>The neural network verification book</u></a></li><li><a href=\"https://arxiv.org/abs/2107.03374\"><u>Evaluating Large Language Models Trained on Code</u></a></li><li><a href=\"https://arxiv.org/abs/2004.07213\"><u>Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims</u></a></li><li><a href=\"https://www.science.org/doi/full/10.1126/science.abi7176\"><u>Filling gaps in trustworthy development of AI</u></a></li><li><a href=\"https://www.science.org/doi/full/10.1126/science.abi5052\"><u>Making machine learning trustworthy</u></a></li></ul><p>Governance-related work from governments and related institutions:</p><ul><li>NIST\u2019s texts on AI:&nbsp;<a href=\"https://www.nist.gov/system/files/documents/2022/08/18/AI_RMF_2nd_draft.pdf\"><u>2nd draft</u></a> and&nbsp;<a href=\"https://www.nist.gov/itl/ai-risk-management-framework/nist-ai-rmf-playbook\"><u>Playbook</u></a></li><li>EU AI Act:&nbsp;<a href=\"https://artificialintelligenceact.eu/the-act/\"><u>Articles 9-15</u></a>, act 15 +&nbsp;<a href=\"https://artificialintelligenceact.eu/analyses/\"><u>analyses</u></a></li><li>Military:&nbsp;<a href=\"https://cset.georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf\"><u>Building Trust through Testing Adapting DOD\u2019s Test &amp; Evaluation, Validation &amp; Verification (TEVV) Enterprise for Machine Learning Systems, including Deep Learning Systems</u></a></li><li>France:&nbsp;<a href=\"https://www.aiforhumanity.fr/en/\"><u>Villani report</u></a></li></ul><h2>Why should I join?</h2><p>There\u2019s loads of reasons to join! Here are just a few:</p><ul><li>See how fun and interesting AI safety can be</li><li>Get to know new people who are into ML safety and AI governance</li><li>Get practical experience with ML safety research</li><li>Show the AI safety labs what you are able to do to increase your chances at some&nbsp;<a href=\"https://agisf.com/opportunities\"><u>amazing opportunities</u></a></li><li>Get a cool certificate that you can show your friends and family</li><li>Get some proof that you\u2019re really talented so you can get that grant to pursue AI safety research that you always wanted</li><li>And many many more\u2026&nbsp;<a href=\"https://itch.io/jam/aitest\"><u>Come along</u></a>!</li></ul><h2>What if I don\u2019t have any experience in AI safety?</h2><p><strong>Please&nbsp;</strong><a href=\"https://itch.io/jam/aitest\"><strong><u>join</u></strong></a><strong>!</strong> This can be your first foray into AI and ML safety and maybe you\u2019ll realize that it\u2019s not that hard. Hackathons can give you a new perspective!</p><p>There\u2019s a lot of pressure from AI safety to perform at a top level and this seems to drive some people out of the field. We\u2019d love it if you consider joining with a mindset of fun exploration and get a positive experience out of the weekend.</p><h2>Are there any ways I can help out?</h2><p>There will be many participants with many questions during the hackathon so one type of help we would love to receive is your mentorship during the hackathon.</p><p>When you mentor at a hackathon, you employ your skills to answer questions on Discord asynchronously. You will monitor the chat and possibly go on calls with participants who need extra help. As part of the mentoring team, you will get to chat with the future talents in AI safety and help make AI safety an inviting and friendly place!</p><p>The skills we look for in mentors can be anything that helps you answer questions participants might have during the hackathon! This can include experience in AI governance, networking, academia, industry, AI safety technical research, programming and machine learning.</p><p><strong>Join as a mentor on&nbsp;</strong><a href=\"https://alignmentjam.com/mentor\"><strong><u>the Alignment Jam site</u></strong></a><strong>.</strong></p><h2>What is the agenda for the weekend?</h2><p>Join the public ICal&nbsp;<a href=\"https://calendar.google.com/calendar/u/0?cid=ZjViYmMzNjlhNDFmZjg5MmY5ZTkxOWJjMGVkMmFlNjRmOTBjMmVjNTMzZDVhMTZhMWNkMjY4ZjU1M2JhMTBlY0Bncm91cC5jYWxlbmRhci5nb29nbGUuY29t\"><u>here</u></a>.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">CET / PST</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Fri 6 PM / 9 AM</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Introduction to the hackathon, what to expect, and an intro talk from Haydn Belfield. Everything is recorded.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Fri 7:00 PM / 10:00 AM</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Hacking begins! Free dinner at the jam sites.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Sun 6 PM / 9 AM</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Final submissions have to be finished. The teams present their projects online or in-person.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Sunday - Wednesday</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Community judging period.</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">Thursday</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">The winners are announced in a live stream!</td></tr></tbody></table></figure><p><br>&nbsp;</p><h2>How are the projects evaluated?</h2><p>Everyone submits their report on Sunday and prepares a 5 minute presentation about what they developed. Then we have 3 days of community voting where everyone who submitted projects can vote on others\u2019 projects. After the community voting, the judges will convene to evaluate which projects are the top 4 and a prize ceremony will be held online on the 22nd of December.</p><p>Check out the community voting results from the last hackathon&nbsp;<a href=\"https://itch.io/jam/interpretability/results\"><u>here</u></a>.</p><h2>I\u2019m busy, do I have to join for the full weekend?</h2><p>As a matter of fact,&nbsp;<strong>we encourage you to join</strong> even if you only have a short while available during the weekend!</p><p>For our other hackathons, the average work amount has been 16 hours and a couple of participants only spent a few hours on their projects as they joined Saturday. Another participant was at an EA retreat at the same time and even won a prize!</p><p><strong>So</strong>&nbsp;<strong>yes,&nbsp;</strong>you can both join without coming to the beginning or end of the event, and you can submit research even if you\u2019ve only spent a few hours on it. We of course still encourage you to come for the intro ceremony and join for the whole weekend.</p><h2>Wow this sounds fun, can I also host an in-person event with my local AI safety group?</h2><p><strong>Definitely!</strong> You can join our team of in-person organizers around the world! You can read more about what we require&nbsp;<a href=\"https://alignmentjam.com/running\"><u>here</u></a> and the possible benefits it can have to your local AI safety group&nbsp;<a href=\"https://alignmentjam.com/why\"><u>here</u></a>. It might be too late but you can sign up for the upcoming hackathon in January.&nbsp;<strong>Contact us at&nbsp;<u>operations@apartresearch.com</u></strong>.</p><h2>What have earlier participants said?</h2><figure class=\"table\"><table style=\"border:1px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top;width:50%\">\u201cThe Interpretability Hackathon exceeded my expectations, it was incredibly well organized with an intelligently curated list of very helpful resources. I had a lot of fun participating and genuinely feel I was able to learn significantly more than I would have, had I spent my time elsewhere. I highly recommend these events to anyone who is interested in this sort of work!\u201d</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">\u201cI was not that interested in AI safety and didn't know that much about machine learning before, but I heard from this hackathon thanks to a friend, and I don't regret participating! I've learned a ton, and it was a refreshing weekend for me.\u201d</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">\u201cA great experience! A fun and welcoming event with some really useful resources for starting to do interpretability research. And a lot of interesting projects to explore at the end!\u201d</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">\u201cWas great to hear directly from accomplished AI safety researchers and try investigating some of the questions they thought were high impact.\u201d</td></tr><tr><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">\u201cThe hackathon was a really great way to try out research on AI interpretability and getting in touch with other people working on this. The input, resources and feedback provided by the team organizers and in particular by Neel Nanda were super helpful and very motivating!\u201d</td><td style=\"border:1pt solid hsl(0, 0%, 100%);padding:5pt;vertical-align:top\">\u201cI enjoyed the hackathon very much. The task was open-ended, and interesting to manage. I felt like I learned a lot about the field of AI safety by exploring the language models during the hackathon.\u201d</td></tr></tbody></table></figure><h2>Where can I read more about this?</h2><ul><li><a href=\"https://itch.io/jam/aitest\"><u>Hackathon page</u></a></li><li><a href=\"https://alignmentjam.com/\"><u>The Alignment Jam website</u></a></li><li><a href=\"https://discord.gg/3PUSbdS8gY\"><u>The Discord server</u></a></li><li><a href=\"https://alignmentjam.com/jams\"><u>In-person and online events</u></a><ul><li><a href=\"https://app.gather.town/app/iEPL2kx1LY4OeKHv/Hackathon%20Space\"><u>The GatherTown hacking space</u></a></li></ul></li><li>The previous hackathons<ul><li><a href=\"https://www.lesswrong.com/posts/hhhmcWkgLwPmBuhx7/results-from-the-interpretability-hackathon\"><u>Results from the interpretability hackathon</u></a></li><li><a href=\"https://www.lesswrong.com/posts/5DsHZidaShW5EM9rz/results-from-the-language-model-hackathon\"><u>Results from the language model hackathon</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/vxLrFdrqRPdaHJwgs/join-the-interpretability-research-hackathon\"><u>The announcement post for the interpretability hackathon</u></a></li><li><a href=\"https://alignmentjam.com/interpretability-playground\"><u>Starter code templates and inspiration list for the previous hackathon</u></a></li></ul></li></ul><p>Again, sign up here by&nbsp;<a href=\"https://itch.io/jam/aitest\"><u>clicking \u201cJoin jam\u201d</u></a> and read more about the hackathons&nbsp;<a href=\"https://alignmentjam.com/\"><u>here</u></a>.</p><p>Godspeed, research jammers!</p><p><i>Thank you to Sabrina Zaki, Fazl Barez, Leo Gao, Thomas Steinthal, Gretchen Krueger and our Discord for helpful discussions and Charbel-Rapha\u00ebl Segerie, Rauno Arike and their collaborators for jam site organization.</i><br><br>&nbsp;</p>", "user": {"username": "esben-kran"}}, {"_id": "8xTxzgHjN2aCbLFmk", "title": "The Pentagon claims China will likely have 1,500 nuclear warheads by 2035", "postedAt": "2022-12-12T18:12:03.414Z", "htmlBody": "<p><a href=\"https://www.urbandictionary.com/define.php?term=Epistemic%20Status\"><i>Epistemic status</i></a><i>: rough and quickly written up in ~15 mins, though I have spent ~6 months FTE researching nuclear risk and possess non-trivial forecasting experience.</i></p><h2>Background</h2><p>Last month the Pentagon, the headquarters of the U.S. Department of Defense (DoD), published a report (linked above) on China's military development. This report comes in the wake of China ramping up its nuclear weapons development (see, e.g., <a href=\"https://fas.org/blogs/security/2021/11/a-closer-look-at-chinas-missile-silo-construction/\">Korda &amp; Kristensen, 2021</a>).</p><blockquote><p>In 2021, Beijing probably accelerated its nuclear expansion. The Department of Defense estimates that the PRC\u2019s operational nuclear warheads stockpile has surpassed 400.</p><p>(p. 94)</p></blockquote><h2>Do I think the headline claim will turn out true?</h2><p>In short, probably not. I'm at ~40%.</p><blockquote><p>The PLA plans to \"basically complete modernization\" of its national defense and armed forces by 2035. If China continues the pace of its nuclear expansion, it will likely field a stockpile of about 1500 warheads by its 2035 timeline.</p><p>(p. 94)</p></blockquote><p>The main reason I'm skeptical is that, as far as I can tell, the Pentagon has a track record of making somewhat exaggerated claims in its reports.</p><p><a href=\"https://thebulletin.org/nuclear-notebook/\">Nuclear Notebook</a>, which I judge to generally produce high-quality, factual, non-alarmist analyses of the nuclear weapons space, in <a href=\"https://thebulletin.org/premium/2020-12/nuclear-notebook-chinese-nuclear-forces-2020/\">their 2020 report on Chinese nuclear forces</a> expressed the following skepticism:</p><blockquote><p>The idea of an emerging nuclear alert posture is a central element of the Pentagon\u2019s expectation that China might adopt a \u201c<a href=\"https://www.britannica.com/topic/launch-on-warning\">launch-on-warning</a>\u201d posture in the future. The <a href=\"https://media.defense.gov/2020/Sep/01/2002488689/-1/-1/1/2020-DOD-CHINA-MILITARY-POWER-REPORT-FINAL.PDF\">most recent Pentagon report</a> claims that there is \u201cincreasing evidence\u201d that China is moving toward this posture for at least a portion of its force (US Defense Department 2020a, 85). As evidence, the Pentagon notes the construction of new ICBM silos for solid-fuel missiles, coupled with an emphasis on developing strategic early warning systems. These data points, however, are relatively circumstantial, as China has deployed silo-based DF-5s for decades and potentially could maintain its current strategy even with new silos and improved early-warning systems. The combination of silo-based solid-fuel missiles and early warning could simply be a Chinese reaction to what it sees is an increasing risk against the survivability of its retaliatory nuclear force.</p><p>(links added)</p></blockquote><p>Moreover, Nuclear Notebook's Kristensen and Korda had this to say about <a href=\"https://media.defense.gov/2018/Feb/02/2001872886/-1/-1/1/2018-NUCLEAR-POSTURE-REVIEW-FINAL-REPORT.PDF\">the DoD's 2018 Nuclear Posture Review</a>:</p><blockquote><p>After the 2018 Nuclear Posture Review was published, inaccurate and exaggerated information was distributed in Washington by defense sources that attributed nuclear capability to several Russian systems that had either been retired or were not, in fact, nuclear. Moreover, although the Nuclear Posture Review claims Russia has increased its nonstrategic nuclear weapons over the past decade, the inventory has in fact declined significantly\u2014by about one-third\u2014during that period.</p><p>(<a href=\"https://thebulletin.org/premium/2020-03/nuclear-notebook-russian-nuclear-forces-2020/\">2020</a>)</p></blockquote><p>Overall, my operationalization of the Pentagon's claim into a forecasting question would be: \"Will China reach 1,500 nuclear warheads by 2035?\". The Pentagon says it's \"likely\" this will happen - I'm not sure exactly what they mean by likely, but I'd guess they mean something in the range of 55 to 90%. On the other hand, I'd predict around 40%.</p><p>It's worth noting that <a href=\"https://www.metaculus.com/questions/\">Metaculus</a> has a forecasting question, \"<a href=\"https://www.metaculus.com/questions/8662/chinas-nuclear-arsenal-by-2030/\">Will China reach 1,000 nuclear warheads by 2030?</a>\", and the current community prediction is 50%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefey89a8ww8he\"><sup><a href=\"#fney89a8ww8he\">[1]</a></sup></span>&nbsp;This is roughly in line with my 40% prediction for 1,500 warheads by 2035.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fney89a8ww8he\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefey89a8ww8he\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Though of the 39 forecasters - at present - on the Metaculus question, a disproportionately high number have predicted exactly 50%. This suggests the question might be one of those where: 1) resolution won't obviously be true or false, and, 2) forecasters want to get an initial forecast in early in order to start accumulating tournament points. Therefore, some forecasters just predict 50% to begin with for no particularly good reason, and so I'd take this community prediction less seriously than those for most other Metaculus questions.</p></div></li></ol>", "user": {"username": "Will Aldred"}}, {"_id": "xL8JNJJmQMwsCtrtw", "title": "Cryptocurrency is not all bad.  We should stay away from it anyway. ", "postedAt": "2022-12-11T13:59:09.562Z", "htmlBody": "<p>So I've wanted to write something like this for some time, but was discouraged by the very real chance of getting downvoted into obscurity by cryptocurrency fans. I hope that in the face of the FTX debacle, people will at least consider the good-faith arguments put forward here. The title is a reference to <a href=\"https://astralcodexten.substack.com/p/why-im-less-than-infinitely-hostile\">this recent</a> astralcodex post, which I critique in this article.&nbsp;</p><p><strong>Introduction</strong></p><p>In the wake of the FTX debacle, there seems to be a small but sizeable minority that believes that there was absolutely no way to see this coming. A massive company, the number 2 crypto exchange in the world, just collapses into nothing due to incompetence and/or fraud? Surely this is just a <a href=\"https://forum.effectivealtruism.org/posts/DB9ggzc5u9RMBosoz/wrong-lessons-from-the-ftx-catastrophe?commentId=KjbKNK2YPWamAnH4o#6qwxHZ5DHjaqCb4hk\">black swan event</a>?</p><p>Just like &nbsp;<a href=\"https://www.bloomberg.com/news/articles/2022-07-14/crypto-lender-celsius-files-for-bankruptcy-in-cash-crunch\">Celcius</a>, &nbsp;<a href=\"https://www.bloomberg.com/news/articles/2022-07-11/three-arrows-founders-whereabouts-unknown-stymying-liquidators?leadSource=uverify%20wall\">three arrows capital</a>, <a href=\"https://www.prnewswire.com/news-releases/voyager-digital-commences-financial-restructuring-process-to-maximize-value-for-all-stakeholders-301581177.html\">Voyager</a>, and <a href=\"https://www.forbes.com/sites/qai/2022/09/20/what-really-happened-to-luna-crypto/?sh=6a8a1dfe4ff1\">Terra/Luna</a>, all of which collapsed in the last year. Go back to previous downturns, and you'll see the downfall of exchanges like <a href=\"https://en.wikipedia.org/wiki/Quadriga_Fintech_Solutions\">Quadriga</a> &nbsp;and <a href=\"https://en.wikipedia.org/wiki/Mt._Gox\">mt gox</a>, the latter of which was by far the largest crypto exchange in existence at the time when it collapsed. And the collapses are just getting started, with the fall of FTX taking out <a href=\"https://www.courtlistener.com/docket/66497849/blockfi-inc/\">Blockfi</a>, and threatening to take out <a href=\"https://www.streetinsider.com/Corporate+News/Crypto+Firm+Genesis+Warns+Of+Bankruptcy+Without+New+Funding+-+Bloomberg/20882057.html\">Genesis</a> and <a href=\"https://www.ft.com/content/29a2f96f-6d9b-4593-abdf-ffaadc502951\">Grayscale</a>. <a href=\"https://en.wikipedia.org/wiki/Tether_(cryptocurrency)\">Tether</a>, the largest stablecoin and the third largest crypto-coin, has been caught out lying about it's reserves. If they are as fraudulent as many suspect, the repercussions for the rest of the crypto industry could be disastrous.&nbsp;</p><p>For an event to be a black swan, it needs to be outside the realm of normalcy. But the collapse of FTX was a fairly predictable event. Even true believers will admit that the crypto industry as a whole has significant problems with speculative bubbles, ponzis, scams, frauds, hacks, and general incompetence. The potential for a collapse was warned against <a href=\"https://forum.effectivealtruism.org/posts/KBw6wKDbvmqacbB5M/crypto-markets-ea-funding-and-optics\">on this forum</a>, months ago. (I agreed with the prognosis in the comments at the time, for what it's worth).&nbsp;</p><p>Note that I'm talking about collapse, and not specifically fraud. It was indeed hard to predict the precise <i>mechanism</i> by which FTX could fail, but I don't think that let's anyone off the hook. If FTX had failed due to incompetence, hacking, or exposure to other fraudulent companies, their investors would have still been screwed over, and the financial and reputational damage to EA would still occur, just with slightly better optics.&nbsp;</p><p>The fundamental problem with cryptocurrency at the present time is that:</p><p><i>A) Almost everyone involved with crypto is using it to try and get rich.</i></p><p><i>B) Almost nobody (in relative terms) is using crypto for anything else.&nbsp;</i></p><p>As long as this continues to be the case, crypto as a whole is still in the middle of a massive speculative bubble, and participating in said bubble is inherently dangerous.&nbsp;</p><p><strong>A. Crypto is infested with speculative bubbles, fraud, and scams (and everyone knows it)</strong></p><blockquote><p>The crypto market is \"rife with frauds, scams and abuse\"</p></blockquote><p>&nbsp;-- <a href=\"https://www.cbsnews.com/news/cryptocurrency-market-fraud-scams-abuse-securities-and-exchange-commission-chairman-gary-gensler/\">SEC chairmen Gary Gensler, August 2021</a></p><blockquote><p>We\u2019re just seeing mountains and mountains of fraud</p></blockquote><p>--<a href=\"https://www.bloomberg.com/news/articles/2022-01-26/irs-seeing-mountains-and-mountains-of-fraud-with-crypto-nfts#xj4y7vzkg?leadSource=uverify%20wall?leadSource=uverify%20wall?leadSource=uverify%20wall\">Ryan Korner, IRS criminal investigator, Jan 2022</a></p><blockquote><p>During this period, nearly four out of every ten dollars reported lost to a fraud originating on social media was lost in crypto, far more than any other payment method.</p></blockquote><p>-- <a href=\"https://www.ftc.gov/news-events/data-visualizations/data-spotlight/2022/06/reports-show-scammers-cashing-crypto-craze#crypto6\">Federal Trade commission</a>, June 2022</p><blockquote><p>Other than a speculative asset with a glorious whitepaper and an impressive \"ex workers\" of big name companies all around the world, they all promise the Moon but underdeliver. I have yet to see something that has an actual use in realife that is using any of those tokens/coins technology.</p></blockquote><p>-- r/cryptocurrency post with <a href=\"https://www.reddit.com/r/CryptoCurrency/comments/sh0tgg/99_of_the_crypto_market_has_0_real_life_utility/\">13k upvotes</a></p><blockquote><p><strong>Matt: (27:13)</strong><br>I think of myself as like a fairly cynical person. And that was so much more cynical than how I would've described farming. You're just like, well, I'm in the Ponzi business and it's pretty good.<br><br><strong>Joe Weisenthal: (27:27)</strong><br>At no point did any of this require any sort of like economic case, it\u2019s just like other people put money in the box. And so I'm going to too, and then it's more valuable. So they're gonna put more money in, and at no point in the cycle, did it seem to like, describe any sort of like economic purpose?<br><br><strong>SBF: (27:42)</strong><br>So on the one hand, I think that\u2019s a pretty reasonable response, but let me play around with this a little bit. Because that's one framing of this. And I think there's like a sort of depressing amount of validity\u2026</p></blockquote><p>&nbsp;I<a href=\"https://www.bloomberg.com/news/articles/2022-04-25/sam-bankman-fried-described-yield-farming-and-left-matt-levine-stunned\">nterview with Sam Bankman-Fried</a>, April 2022</p><p>You don\u2019t need a useful application to get rich off crypto. All you need is a plausible-ish sounding idea and a skill for marketing hype. You write a fancy looking whitepaper, create millions of your own coin (let\u2019s call it CRP), &nbsp;and claim that it will be the basis for \u201cdog-walking on the blockchain\u201d or something. You then hype the coin up and put out an ICO, and if you were clever enough, CRP coins which were previously worth 0$ because they didn\u2019t exist, suddenly are worth like 1$, making you instantly a nominal millionaire. If it goes up further, the initial investors also make a killing, and other speculators get jealous and jump in as well, pumping the price up further. None of these people need to actually believe in \u201cdog-walking on the blockchain\u201d, they just need to believe that they can make money off it before the thing collapses, leaving the last investors holding the bag.&nbsp;</p><p>Now, as with virtually all similar pitches, \u201cdog-walking on the blockchain\u201d is actually a terrible idea which will fail, so the value is inherently unsustainable. &nbsp;CRP will crash and burn, either with a rug-pull (where the founder sells everything at it\u2019s peak and walks away), a hack, a market crash, etc.&nbsp;</p><p>The social dynamics become interesting here. If you\u2019ve invested your life savings in CRP coin, then if the hype dies down, so does all your money. So if an external entity points out the obvious flaws with CRP (spreading FUD) and causes it to drop in price, it\u2019s like they are literally pulling money out of your pocket. It\u2019s no wonder that crypto boosters often seem like spammers or cultists.</p><p>This is all aided by the loose regulations and anonymity inherent to crypto. If someone hacks your bank account, you just call the bank and they reverse the transactions. For most crypto, this option does not work by very nature. This makes it a prime target for scams, fraud and criminal activity.&nbsp;</p><p>Now there certainly are bigger and more respectable crypto projects out there. FTX was meant to be one of them. As an exchange, it was making money off of trades, in effect taking a small cut of the entire crypto ecosystem. But where was that ecosystem money coming from? In the traditional economy, money flows into companies through customers paying for goods and services. In the crypto space, this amount is relatively miniscule. The vast, vast majority of incoming funds is financial speculation. This makes crypto something of a zero-sum game: you can only get rich (in real money) at the expense of someone else. And eventually people are going to realize that they are at best gambling, and at worse being scammed, and stop, and at that point the entire crypto field will completely run out of money. &nbsp;</p><p>Now, the above isn't <i>necessarily</i> the case. Perhaps a brilliant use-case for crypto is just around the corner, that will justify all those billions in speculation?</p><p><strong>B. There might </strong><i><strong>never</strong></i><strong> be sufficiently large applications for crypto</strong></p><p>The blockchain is a brilliant invention. But most cool inventions do not end up revolutionizing the world. &nbsp;</p><p>Blockchain is not a new technology. It was invented in 2008, when smartphone usage was barely off the ground. It is 14 years old, predating the iPad and instagram. People still try to compare blockchain to the \"early internet\", but the web was 14 years old in <i>2005</i>, and already half the developed world were internet users. Blockchain has been around for a full <strong>40%</strong> of the lifetime of the web. And in that time, billions of dollars have been poured into it, trying to find useful applications.&nbsp;</p><p>In all that time, the number of blockchain applications that are actually in widespread use and not just used a vehicle for financial speculation, is <i>incredibly</i> small. It's not zero, it has been useful for remittances and other cross-border transfers, for example, but these days blockchains are things like gaming NFT's, which are mostly there to grab speculator money and are <a href=\"https://superrare.com/magazine/2022/08/30/why-gamers-hate-nfts-in-games/#:~:text=NFTs%2C%20as%20far%20as%20art,%2C%20worse%2C%20just%20an%20idea.\">widely despised</a> by actual customers.&nbsp;</p><p>I think it\u2019s time to to take stock and wonder if maybe the reason there has been no widespread adoption is because the technology just <i>isn\u2019t very useful</i>?&nbsp;</p><p>I don't want to make this a big technology post, but the essential argument I would make is that is:</p><ol><li>Decentralised verification systems like proof-of-work are costly and inefficient.&nbsp;</li><li>Almost no applications <i>need</i> decentralised verification systems.</li></ol><p>For example, reddit did not need make it's avatars be NFT's using the blockchain. The avatars are only used on reddit, therefore it is much more simple and easy to just verify them using the existing reddit website.&nbsp;</p><p>Almost all applications will run into similar problems. If your blockchain dogwalking runs on an app, then you can just put the verification on the app. This gets compounded by the <a href=\"https://medium.com/@jimmysong/the-truth-about-smart-contracts-ae825271811f\">oracle problem</a>. If you want to update your blockchain whenever you make a delivery, for example, the blockchain needs a way to actually verify that the delivery has been made. The easiest way to do this is verification through a trusted third party... But if you have a trusted third party, you don't need a blockchain.&nbsp;</p><p>Don't get me wrong, there are clever ways around all of these problems, with highly advanced oracle software, consensus mechanisms, decentralised decision making, etc. The problem is that at the end of the day, the average person or company <i>does not care</i> if their shipping or dog walking is decentralised. They'll go for the cheapest and most efficient option, which in pretty much every case is the centralized one.&nbsp;</p><p>An amazon AWS developer has related <a href=\"https://www.tbray.org/ongoing/When/202x/2022/11/19/AWS-Blockchain\">their tale of</a> looking for blockchain projects worth funding:</p><blockquote><p>The key moment was when we got in a room with the CTO of this one startup, in Tribeca I think. When I heard their VC funding number I thought it was the valuation, not the investment dollars. The customer list was blue fucking ribbon and don\u2019t you forget it. These guys were razor-sharp.</p><p>They presented some of the systems they\u2019d built and yep, we were impressed. Then, with the startup CTO in the room, one of my fellow engineers asked the key question: \u201cAll these systems, are there any that wouldn\u2019t work without blockchain?\u201d The guy didn\u2019t even hesitate: \u201cNo, not really.\u201d&nbsp;</p></blockquote><p>I think that pretty much sums it up. Right now, crypto is almost all hype, and has been for 14 years. You should at least entertain the possibility that it will remain that way.&nbsp;</p><p><strong>C. What about the actual, real use cases, like remittances and third world banking?</strong></p><p>I have been careful not to claim here that crypto is <i>entirely</i> useless. Indeed, there are real world applications currently being used. This is the basis of the defence of crypto put forward recently by <a href=\"https://astralcodexten.substack.com/p/why-im-less-than-infinitely-hostile\">Scott Alexander</a>, who points out that developing countries tend to have more crypto adoption, which he attributes to their unstable banking systems. I definitely agree that remittances and banking are a legitimate use case.</p><p>However, there a few problems with these arguments. For one thing, being in a developing country does not, in fact, make you immune to get-rich-quick schemes. Is Vietnam the highest adopter because it has the weakest bank, or because people are jumping on the Vietnamese game <a href=\"https://fortune.com/2022/05/04/axie-infinity-sky-mavis-vietnam-crypto-blockchain-startups/\">axie infinity</a> to try and make speculation money on gaming NFT's? The US has a stable banking system, so why is it's crypto adoption on par with unstable states like Venezuela?&nbsp;</p><p>The issue is that while crypto <i>can</i> be used for remittances and banking in the developing world, so can a lot of other methods that are arguably better. Banks in developing world might be unstable, but as long as they're <i>less</i> unstable than unregulated crypto exchanges, they'll win out. If your indian bank account gets hacked, they can probably reverse the transaction and get your money bank, but if your bitcoin wallet does, you're shit out of luck. Digital payments using mobile money are already widespread in the developing world, and are arguably much more robust and convenient than crypto. The government of el Salvador tried to directly push bitcoin onto the population, making bitcoin legal tender and giving everyone a bitcoin wallet. Despite that, el Salvadorans conducted a mere <a href=\"https://www.pymnts.com/cryptocurrency/2022/el-salvadors-bitcoin-remittances-drop-again-despite-potential-savings/\"><strong>2%</strong></a> of their remittances using crypto.&nbsp;</p><p>When you account for competition, there is not enough money in the failed state and remittances business to justify the massive speculation around cryptocurrency.&nbsp;</p><p>The top three crypto coins have a total market cap of around <a href=\"https://coinmarketcap.com/\">half a trillion dollars</a>. Wise, on the other hand, one of the biggest companies for individual international payments, has a market valuation of <a href=\"https://en.wikipedia.org/wiki/Wise_(company)\">5 billion dollars</a>. Now, obviously the two figures can't be compared exactly, but I think it's clear that if crypto actually is only useful for these two things, plus some cool toy programming projects, it is overvalued by at least an order of magnitude. The current prices simply are not backed up by the current uses. The difference between the two is made up of hype and speculation.&nbsp;</p><p><strong>&nbsp;C. Fool me once....</strong></p><p>Even if FTX was not fraudulent, working with them was still ethically dubious. For one, the climate damage from proof-of-work crypto is massive. The arguments trying to justify this damage are all <a href=\"https://ketanjoshi.co/2021/03/11/bitcoin-is-a-mouth-hungry-for-fossil-fuels/\">incredibly weak</a>, usually coming from false comparisons between bitcoin and things that billions of people actually use on a daily basis. The Ethereum switch to proof-of-stake is certainly welcoming, but FTX was boosting it well before then, and was still trading and encouraging the use of proof-of work coins in the exchange when it went down.&nbsp;</p><p>For another, they deliberately targeted retail investors with celebrities and high profile superbowl ads, who promptly lost their hats in the subsequent crypto crash. This wasn't just VC's, this was actual people with families that were suckered into gambling their life's savings.&nbsp;</p><p>I believe the EA org leaders when they say they had no idea that fraud was going on at FTX. &nbsp;But they obviously did know that crypto was a highly volatile industry, and that fraud and speculative bubbles were extremely common. The reasoning for going in anyway was that they thought they had enough information and expertise to wade into the snakes den and not be bitten. Sure, there is a high degree of volatility and fraud in crypto, but FTX was a highly reputable company with a seemingly sound business model, run by a trusted personal friend of many.&nbsp;</p><p>Well, they were wrong. The question \u201cdoes the EA leadership have the expertise to pick out non-flameout crypto companies\u201d has been answered with a definitive <i><strong>no</strong></i>.&nbsp;</p><p>EA is still in the charity business, and the charity business is extremely dependent on reputation. <i>Most charitable foundations do not get embroiled in Enron scale financial fraud</i>.&nbsp;</p><p>I don\u2019t think many people understand the reputational risk that comes from continuing the status quo here. When someone is considering donating to the EA movement, they are naturally going to ask \u201cHow do I know this won\u2019t happen again\u201d. Is an answer of \u201cwe made a few administrative changes and slightly diversified our assets\u201d really going to cut it?</p><p>But the real danger here is that EA does some shuffling of leadership and some revisions of decision making protocols, and then wrongly decides it is <i>now</i> ready to wade into the snake pit, and embraces FTX 2.0 with open arms.&nbsp;</p><p>FTX 2.0 might not go down the exact same way. Maybe instead of an exchange, it\u2019s a stable coin, or a web3.0 project, or a new volatile asset outside of crypto. It could be a massive bubble pop, or a hack, or a matter of extreme incompetence rather than fraud. Reputationally, it doesn\u2019t matter. EA will forever be the movement that \u201ckeeps getting scammed\u201d.&nbsp;</p><p><strong>D. Some further crypto skeptic reading</strong></p><p>There is an informational imbalance between crypto boosters and skeptics. As I detailed earlier there is a huge financial incentive to hype crypto coins and get rich off the inflated prices, so there will always be money available for pro-crypto arguments. Whereas the main reason crypto skeptics write about crypto is \"someone is wrong on the internet\" energy. For this reason, I think it's important to read up on the anti-crypto case, even if you disagree with it, to correct the imbalance.</p><p><a href=\"https://youtu.be/YQ_xWvX1n9g\">Line goes up</a> - youtube documentary criticising NFT's and blockchain&nbsp;</p><p><a href=\"https://www.reddit.com/r/CryptoReality/\">r/cryptoreality </a>- community of crypto skeptics. (theres also r/buttcoin, if you want something with more memes)</p><p><a href=\"https://web3isgoinggreat.com/\">Web 3 is going great </a>- detailed list of disasters in the crypto space</p><p><a href=\"https://davidgerard.co.uk/blockchain/\">Attack of the 50 foot blockchain</a> - Blog detailing crypto disasters over several bubble cycles</p><p><a href=\"https://www.stephendiehl.com/blog.html\">Stephen diel </a>- A collection of crypto-critical posts from a software engineer</p><p><strong>E. The proposal</strong></p><p>Here is my radical proposal :&nbsp;</p><p><i>EA orgs should stay away from cryptocurrency until the industry becomes stable.&nbsp;</i></p><p>By \"stay away\", I don't mean \"shun everyone in crypto like the devil\". I mean \"avoid being significantly associated with the field\". Don't invest assets in crypto or crypto companies, don't put them on your board, don't boast about your ties to crypto insiders.&nbsp;</p><p>Maybe some day, the crypto industry will find a widely useful use case for blockchain, or improve the existing uses to the point where they properly compete. One day, the coin market cap might be sustained with a steady income of people actually willingly paying for goods and services.&nbsp;</p><p>My question is this: Why not get involved with it <i>then</i>? Why be involved in it <i>now</i>, when the field is full of potential ticking time bombs like Tether, and the reputational and financial damage from the next collapse could be catastrophic?&nbsp;</p><p>At the very least, if you disagree with me, and partner up with another crypto company, and it blows up in your face again... Don't say you weren't warned.&nbsp;</p>", "user": {"username": "titotal"}}, {"_id": "vahRzQqRHJTEXhckC", "title": "Introducing Resolute: Free Behaviour Change Coaching for EAs", "postedAt": "2022-12-13T13:59:44.617Z", "htmlBody": "<p><strong>Summary</strong><br>Behaviour change is hard. Coaching makes it less so. We're offering six thirty-minute sessions to ~120 people in Q1 2023, and placing EAs at the top of the queue. It's free, one-to-one, and accessible anywhere in the world. &nbsp;<a href=\"https://forms.gle/BXSJn3PGAc9rwxPG7\">Sign up now!</a></p><p>Thanks to <a href=\"https://spencergreenberg.com\">Spencer Greenberg</a> and <a href=\"https://sparkwave.tech/\">Spark Wave</a> for creating the <a href=\"https://sparkwave.tech/conditions-for-change/\">Ten Conditions For Change Framework</a>, which strongly influenced our coaching curriculum.</p><p><strong>What goes on in sessions?</strong></p><p>Initially, your coach will ask you a series of questions about the behaviour you want to change and how you want to do it. Thereafter, they will help you formulate a plan using proven behavioural science principles. You\u2019ll give it your best shot, and tell your coach how it went. You\u2019ll then record your progress, make changes accordingly, and the process repeats.&nbsp;</p><p><strong>Who are the coaches?</strong></p><p>Most if not all EAs who sign up from this post will be coached by Vidya. She's a recent Cambridge psychology graduate and aspiring clinical psychologist. Our other coaches are psych grads, or students entering their final year, who've been through an intensive three-month training program. Before being permitted to see clients, every coach has passed assessment by at least one expert.</p><p><strong>How is it possible to offer this for free?</strong><br>All of our coaches are volunteers. They're intrinsically motivated to help our cause and other people more generally! They're interested in becoming psychologists, so are keen to get first-hand experience with clients (a non-negotiable requirement for many PhD programs).</p><p><strong>What type of behaviour changes can you help with?</strong></p><p>We can help with any almost any change. Examples include:&nbsp;</p><ul><li>improving your diet/budgeting/habits</li><li>procrastinating less</li><li>being more assertive&nbsp;</li></ul><p>The only exceptions are changes that are so urgent that your long-term health is threatened unless resolved promptly (e.g. Severe alcoholism), and highly specialised problems (e.g. compulsions arising from OCD)</p><p><strong>Is it confidential?</strong></p><p>Yes, unless you are threatening to commit suicide or something of a similar level of severity. Other than that, the only person your coach is permitted to disclose to is their immediate supervisor (who will maintain strict confidentiality). &nbsp;</p><p><strong>Besides the direct benefit, what's your motivation?</strong></p><p><a href=\"https://Overcome.org.uk\">We're</a> an EA-aligned mental health charity looking to generate extra revenue. The fastest way to get this service up and running is to initially offer it for free, to massively increase demand, so we can quickly build up social proof needed to land paying clients. Given that free coaching is in extremely high demand, we're in a position to offer it selectively to those who'll use it most altruistically.</p><h2><a href=\"https://forms.gle/BXSJn3PGAc9rwxPG7\">Claim your free coaching now</a> (it only takes 2 minutes)</h2>", "user": {"username": "John Salter"}}, {"_id": "H7xWzvwvkyywDAEkL", "title": "Creating a database for base rates", "postedAt": "2022-12-12T10:05:41.310Z", "htmlBody": "<h2>TLDR</h2><p>We are creating a database to collect base rates for various categories of events. You can find the database <a href=\"https://docs.google.com/spreadsheets/d/1KTFSPKfQkzOfOWiAiPJtIWLr8UEtN2gFG5nixigWLzY/\">here</a> and can suggest new base rate categories for us to look into <a href=\"https://forms.gle/18mVVd5N2RnfSbD2A\">here</a>.</p><h2>Project Summary</h2><p>The base rate database project collects base rates for different categories of events and makes them available to researchers, forecasters and philanthropic organisations. Its main goals are to develop better intuitions about the potential and limitations of reference class forecasting and to provide useful information to the public. The data will enable research that enhances our understanding of the kinds of circumstances in which reference forecasting is a promising approach, what kinds of methods of reference forecasting work best, how to construct reasonable reference classes, and what potential caveats and pitfalls are. In addition to the raw data we will collect qualitative feedback on individual reference classes and on the overall process of building a base rate database, adding context to the data and developing comprehensive knowledge to build upon in the future. We aim to select categories of base rates in a way that makes the information we collect useful to decision makers and philanthropic organisations.&nbsp;</p><p>&nbsp;</p><h2>Introduction</h2><p>If one wants to predict whether some event will happen in the future, it is often helpful to look at the past. One can ask: \"Ignoring all the specifics of the current event I'm trying to predict, what would I predict just by looking at the base rate of similar events happening in the past?\". This is called reference class forecasting and helps forecasters to obtain an 'outside view' on the forecasting question at hand. This outside view, of course, is usually complemented by the 'inside view': what are the specifics of the current event at hand that distinguish it from other events?&nbsp;</p><p>Reference class forecasting is widely used among forecasters. To this date, however, there has been little systematic research done into how effective base rates are for forecasting future events, how they can best be used and what limitations apply. We aim to facilitate this research.&nbsp;</p><p>&nbsp;</p><h2>Project outline</h2><h3>Goal</h3><p>The&nbsp;<strong>main goal</strong> of this project is to develop a better understanding of the merits and limitations of reference class forecasting.&nbsp;</p><p>A <strong>secondary goal</strong> is to collect information that may be useful for forecasters and EA stakeholders in the future.&nbsp;</p><p>&nbsp;</p><h3>What we'll do</h3><p>We want to achieve our goals by</p><ul><li>asking experienced forecasters to compile a <a href=\"https://docs.google.com/spreadsheets/d/10fBR2VE1NmJwiPYo1ZEnb5dhY02Ii_eo0t6TxHAZG28/edit?pli=1#gid=0\">public database</a> with base rates for various categories of events</li><li>collecting qualitative feedback on the process of collecting base rates, as well as the base rates themselves</li><li>using the database to conduct and facilitate quantitative and qualitative research, especially with regards to the performance of various reference class forecasting approaches&nbsp;</li><li>inviting others (you!) to suggest base rate categories that we should look into through <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4Auw1kCjl9Bhr823uO5cVLl6oVzX66kXmPzLW-ndqQ-AA1Q/viewform\">this form</a>.&nbsp;</li></ul><p>&nbsp;</p><h3>Categories that we want to look into</h3><p>We intend to look into categories as diverse as&nbsp;</p><ul><li>Violent and non-violent protests that have (or have not) led to regime change</li><li>Elections with small margins of victory</li><li>Zoonotic spillover events</li><li>Development of new antibiotics</li><li>...&nbsp;</li></ul><p>You can find a list of all the categories on our radar <a href=\"https://docs.google.com/spreadsheets/d/10fBR2VE1NmJwiPYo1ZEnb5dhY02Ii_eo0t6TxHAZG28/edit?pli=1#gid=1506133750\">here</a>. You can suggest new categories <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4Auw1kCjl9Bhr823uO5cVLl6oVzX66kXmPzLW-ndqQ-AA1Q/viewform\">here</a>.&nbsp;</p><p>&nbsp;</p><h3>Specific research questions</h3><p>The database is meant to be a resource for anyone who is interested in reference class forecasting. Please do feel free to use it for your own research as well as to reach out to us.&nbsp;</p><p>So far, we have thought of the following quantitative analyses we think may be promising:&nbsp;</p><ul><li>Comparison of the predictive performance of several reference forecasting approaches, for example:<ul><li>Naive Laplace's rule with different priors (uniform, Jeffrey, Haldane)</li><li>Time invariant Laplace with different ways of treating the exponent</li></ul></li><li>Analysing how useful reference forecasting is overall, for example by<ul><li>constructing a reference class forecast based on the first x observations and scoring the forecast based on the last (n-x) observations</li><li>arriving at an estimate for the robustness of reference class forecasting by obtaining a distribution of scores for different approaches across different base rate categories&nbsp;&nbsp;</li><li>checking how robust forecasts / estimates are to changes in the observation period / the number of data points used.&nbsp;</li><li>specifically investigating the relationship between accuracy and the number of data points available by constructing a forecast based on the first X data points and subsequently adding more data points to check consistency. The distribution of robustness would itself provide a base rate for how useful base rates are.&nbsp;</li><li>Identifying patterns that make a base rate useful or less useful (e.g. if there is a dynamic over time, simply looking at the base rate may not be enough)</li></ul></li></ul><p>We also aim to obtain a better qualitative understanding of reference class forecasting by asking that forecasters who collect the base rates to reflect on the process as well as the individual base rate categories, for example</p><ul><li>How clear are criteria for inclusion / exclusion and the period that was looked at</li><li>How trustworthy is the data?</li><li>Are there any trends that can be identified?&nbsp;</li><li>General thoughts / lessons learned</li></ul><h2>How you can help</h2><h3>Suggesting new categories</h3><p>You can suggest new categories to include in the database <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4Auw1kCjl9Bhr823uO5cVLl6oVzX66kXmPzLW-ndqQ-AA1Q/viewform\">here</a>. Suggested categories should ideally be at least one of the following:&nbsp;</p><ul><li>helpful / useful / interesting</li><li>easy to collect</li></ul><h3>Providing feedback</h3><p>If you have thoughts on anything presented here, please let us know in the comments or <a href=\"https://twitter.com/nikosbosse\">get in touch directly</a>. &nbsp;</p><h3>&nbsp;</h3><h3>&nbsp;</h3><h3>&nbsp;</h3>", "user": {"username": "nikos"}}, {"_id": "JGR87M8to93D7Ahzh", "title": "Hugh Thompson Jr (1943\u20132006)", "postedAt": "2022-12-10T20:16:50.623Z", "htmlBody": "<figure class=\"image image_resized\" style=\"width:42.59%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1670703651/mirroredImages/JGR87M8to93D7Ahzh/ci3lpiitmx4me2inilst.jpg\" alt=\"Hugh Thompson Jr. - Wikipedia\"></figure><blockquote><p><i>On the 30th anniversary of the massacre, Thompson went back to My Lai and met some of the people whose lives he had saved.&nbsp;</i></p><p><i>\"One of the ladies that we had helped out that day came up to me and asked, \u2018Why didn\u2019t the people who committed these acts come back with you?\u2019 And I was just devastated. And then she finished her sentence: she said, \u2018So we could forgive them.\u2019 I\u2019m not man enough to do that. I\u2019m sorry. I wish I was, but I won\u2019t lie to anybody. I\u2019m not that much of a man.\"</i></p></blockquote><p>&nbsp;</p><p>Hugh Thompson was a volunteer officer in the Vietnam War who turned his squad's weapons on American soldiers to stop them raping and <a href=\"https://en.wikipedia.org/wiki/M%E1%BB%B9_Lai_Massacre\">murdering</a> more women and children than the four or five hundred they already had.&nbsp;</p><p>He models the standard idea of heroism: one day, one decision, clarity, evil men to defy, faces you can see. Then, a system to navigate, betrayal, self-sacrifice, being punished for virtue.</p><p>I'm writing about him because of how the story makes me feel. &nbsp; It is a fault of my feeling that I feel like this about Thompson and not Borlaug.</p><h2>One day</h2><blockquote><p>Thompson and his crew, who at first thought the artillery bombardment caused all the civilian deaths on the ground, became aware that Americans were murdering the villagers after a wounded civilian woman they requested medical evacuation for, Nguy\u1ec5n Th\u1ecb T\u1ea9u, was murdered right in front of them by Captain Medina, the commanding officer of the operation... \"It was a Nazi kind of thing.\"</p></blockquote><blockquote><p>Immediately realizing that the soldiers intended to murder the Vietnamese civilians, Thompson landed his helicopter between the advancing ground unit and the villagers.<sup>\u200a </sup>He turned to Colburn and Andreotta and ordered them to shoot the men in the 2nd Platoon if they attempted to kill any of the fleeing civilians...&nbsp;</p><p>\u201cOpen up on \u2018em. Blow \u2018em away.\u201d</p><p>While Colburn and Andreotta trained their guns on the 2nd Platoon, Thompson located as many civilians as he could, persuaded them to follow him to a safer location, and ensured their evacuation...</p><p><strong>\"</strong>Later that day, sometime in the afternoon, after they had<i> gone through</i> the village, we were back out there again. [The murderers] were just casually, nonchalantly sitting around around smoking and joking with their steel pots off just like nothing had happened. There were five or six hundred bodies less than a quarter of a mile from them. I just couldn't understand it.\"</p><p>...senior American Division officers cancelled similar planned operations by Task Force Barker against other villages... possibly preventing the additional massacre of further hundreds, if not thousands, of Vietnamese civilians.</p></blockquote><h2>Synecdoche</h2><blockquote><p>in the Vietnamese province of Quang Ngai, where the M\u1ef9 Lai massacre occurred, up to 70% of all villages were destroyed by the air strikes and artillery bombardments, including the use of napalm; 40 percent of the population were refugees, and the overall civilian casualties were close to 50,000 a year... 203 U.S. personnel were charged with crimes, 57 of them were court-martialed and 23 of them were convicted. The VWCWG also investigated over 500 additional alleged atrocities but it could not verify them.</p></blockquote><blockquote><p>[A draw: ] PFC Herbert L. Carter; shot himself in the foot while reloading his pistol and claimed that he shot himself in the foot in order to be MEDEVACed out of the village when the massacre started.</p></blockquote><h2>Coverup</h2><blockquote><p>Thompson quickly received the Distinguished Flying Cross for his actions at M\u1ef9 Lai. The citation for the award fabricated events, for example praising Thompson for taking to a hospital a Vietnamese child \"...caught in intense crossfire\". It also stated that his \"...sound judgment had greatly enhanced Vietnamese\u2013American relations in the operational area\". Thompson threw away the citation.</p><p>Initial reports claimed \"128 Viet Cong and 22 civilians\" had been killed in the village during a \"fierce fire fight\". Westmoreland congratulated the unit on the \"outstanding job\".</p><p>Henderson interviewed several soldiers involved in the incident, then issued a written report in late-April claiming that some 20 civilians were inadvertently killed during the operation... casualties that occurred were accidental and mainly attributed to long-range artillery fire.</p><p>Colin Powell... was charged with investigating the letter... In his report, Powell wrote, \"In direct refutation of this portrayal is the fact that relations between Americal Division soldiers and the Vietnamese people are excellent.\"</p><p>... on 9 September 1969, the Army quietly charged Calley with six specifications of premeditated murder for the deaths of 109 South Vietnamese civilians... Calley's court martial was not released to press and did not commence until over a year later. However, word of Calley's prosecution found its way to American investigative reporter and freelance journalist Seymour Hersh. My Lai was first revealed to the American public on November 13, 1969\u2014almost two years after the incident.</p><p>[chairman of the House Armed Services Committee, Mendel] Rivers... urged the President to use nuclear weapons against the North Vietnamese and to invade and occupy Hanoi... Rivers criticized Army helicopter pilot Hugh Thompson, Jr. for giving the order to his men to fire upon American soldiers at My Lai if they continued to shoot unarmed Vietnamese civilians, calling him a traitor and saying he should be prosecuted... He called every major witness to the event (including Thompson) before the subcommittee, and then refused to release the transcripts of the testimony. This meant that military prosecutors would be prohibited from calling those persons as witnesses at Calley's court martial.</p><p>Calley... was sentenced to life in prison on 29 March 1971, after being found guilty of premeditated murder of not fewer than 20 people. Two days later, President Richard Nixon made the controversial decision to have Calley released from armed custody at Fort Benning, Georgia, and put under house arrest... Calley's sentence was reduced by the convening authority from life to twenty years. Calley would eventually serve three and one-half years under house arrest</p><p>Medina denied giving the orders that led to the massacre, and was acquitted of all charges... Several months after his acquittal, however, Medina admitted he had suppressed evidence</p><p>Of the 26 men initially charged, Calley was the only one convicted</p><p>Secretary of the Army, was quoted in <i>The New York Times</i> in 1976 as stating that Calley's sentence was reduced because Calley honestly believed that what he did was a part of his orders\u2014a rationale that contradicts the standards set at Nuremberg and Tokyo, where following orders was not a defense for committing war crimes</p></blockquote><h2>The next 38 years</h2><blockquote><p>Twenty-six officers and enlisted soldiers, including William Calley and Ernest Medina, were charged with criminal offenses, but all were either acquitted or pardoned.&nbsp;</p><p>Thompson was condemned and ostracized by many individuals in the United States military and government, as well as the public, for his role in the investigations and trials concerning the M\u1ef9 Lai massacre. As a direct result of what he experienced, Thompson experienced posttraumatic stress disorder, alcoholism, divorce, and severe nightmare disorder.</p></blockquote><p>&nbsp;</p><blockquote><p>In early 1972, the camp at M\u1ef9 Lai where the survivors of the massacre had been relocated was largely destroyed by <a href=\"https://en.wikipedia.org/wiki/Army_of_the_Republic_of_Vietnam\">Army of the Republic of Vietnam</a> (ARVN) artillery and aerial bombardment, and remaining eyewitnesses were dispersed.</p></blockquote><p>&nbsp;</p><blockquote><p>... exactly 30 years after the massacre, Thompson and the two other members of his crew, Glenn Andreotta and Lawrence Colburn, were awarded the Soldier's Medal (Andreotta posthumously), the United States Army's highest award for bravery not involving direct contact with the enemy... [Senator Max] Cleland said the three men were \"true examples of American patriotism at its finest\"</p></blockquote><p>Amidst our anger, we can hope that the senator is correct, and that the sick patriotism of the middle of the century had genuinely turned into something worth honouring by the end.</p><h2>What does this have to do with EA</h2><p>Not much.</p><p>There is a species of consequentialist, above our paygrade, who sometimes decides war is the right thing to do. But war is <a href=\"https://www.gleech.org/libya\"><i>even worse than it looks</i></a><i>, </i>even after you take into account the apparent risks and costs and horror. Because much more horror doesn't get reported.</p><p>Whenever you read about wars before about 1900, remember that pillaging the locals was the <a href=\"https://www.degruyter.com/document/doi/10.9783/9780812207750.28/pdf\">official and conventional way</a> to feed and reward your soldiers. What we see at My Lai is the fairly <i>ordinary</i> course of past warfare.</p><p>More practically it's just to show you what the right thing looks like, what it costs, how society can react to it, how long it can take for the moral circle to catch up with you.</p><p>&nbsp;</p><p>Quotes all from Wikipedia.</p>", "user": {"username": "technicalities"}}, {"_id": "aj4PQaLqSMWffyhFD", "title": "EA Landscape in the UK", "postedAt": "2022-12-13T12:54:35.607Z", "htmlBody": "<p><i>(Last updated: October 2023) - </i><a href=\"https://effectivealtruism.uk/\"><i>EA UK Website</i></a></p><h1><strong>Summary</strong></h1><p>In this post I\u2019m attempting to give an overview of what EA looks like in the UK, including communities and organisations with a range of affiliation with EA. If you are visiting or new to the UK and are looking to get connected to the local EA community, this guide may be a useful place to start.</p><h1><strong>EA Hubs in the UK</strong></h1><p>Most of the people working at EA related organisations are in London, Oxford or Cambridge (sometimes known as<a href=\"https://en.wikipedia.org/wiki/Golden_triangle_(universities)\">&nbsp;<u>Loxbridge</u></a>), with some remote workers around the UK. It\u2019s quite easy to get from London to Oxford or Cambridge, being about an hour by train, and people regularly travel between them.</p><h2><strong>London</strong></h2><p>London probably has the largest number of people interested in EA. There are roughly 150-250 people working at EA related organisations although the majority of people interested in EA are working in the private sector, academia or (non EA) nonprofits. There are also university groups in London at Imperial College London, King's College London, University College London and London School of Economics.</p><p>There isn't one main EA community in London, there are quite a few subgroups based around different causes, workplaces and interests. There are also people who have an interest but attend an event or get involved with a relevant opportunity once every few years.</p><h3>Subcommunities</h3><ul><li>Workplace/Cause Communities<ul><li>Charity Entrepreneurship are near Queens Park and with each new cohort there are more organisations set up that work from the CE office, mainly on global health &amp; development, health security or animal welfare</li><li>The Centre on Long-Term Risk are near Kentish Town and there is a community there for people working on suffering risks</li><li>Conjecture are close to London Bridge and support the SERI-MATS fellowship, both focusing on AI Safety</li><li><a href=\"https://www.safeai.org.uk/\">London Initiative for Safe AI</a></li><li>There is a London meetup group on the <a href=\"https://www.impactfulanimaladvocacy.org/\">IAA Slack</a></li><li>There is a WhatsApp chat for global development professionals with an interest in EA</li></ul></li><li>Profession communities<ul><li>EA Finance has quite a few members in London and they have meetups every few months</li><li>EA Tech have meetups every month or two</li><li>There is a group for people who are interested in politics and EA</li><li>There are usually 1 or 2 meetups a year for entrepreneurs and consultants</li></ul></li><li>University groups<ul><li>There are 4 quite active groups, roughly 20 volunteer organisers as well as the <a href=\"https://london-ea.notion.site/London-Effective-Altruism-Hub-2af621bb2754497fb12f23a0bd13b758\">London EA Hub</a> (LEAH) supporting students interested in EA</li><li><a href=\"https://www.imperialea.org/\"><u>ICL</u></a></li><li><a href=\"https://www.ea-kcl.org/\"><u>KCL</u></a></li><li><a href=\"https://www.uclea.org/\"><u>UCL</u></a></li><li><a href=\"https://www.lse-ea.org/\"><u>LSE</u></a></li></ul></li><li>Interest communities<ul><li><a href=\"https://www.eaforchristians.org/\">EA for Christians</a> have had meetups in London every few months</li><li>There are also&nbsp;<a href=\"https://www.facebook.com/groups/socialeal\"><u>meetups</u></a> for people to play football, dance, go to gigs, stitch and climb</li></ul></li></ul><p>&nbsp;</p><p>There are now quite a few EA related organisations in London, in 2015 there was just Founders Pledge and some Givewell recommended charities.</p><ul><li>EA Meta<ul><li><a href=\"http://www.charityentrepreneurship.com/\"><u>Charity Entrepreneurship</u></a></li><li><a href=\"https://80000hours.org/\"><u>80,000 Hours</u></a></li><li><a href=\"https://lets-fund.org/\"><u>Let\u2019s Fund</u></a></li><li><a href=\"https://sogive.org/\"><u>SoGive</u></a></li><li><a href=\"https://non-trivial.org/\"><u>Non-trivial</u></a></li><li><a href=\"https://effectivealtruism.uk/\"><u>EA UK</u></a></li><li><a href=\"https://www.notion.so/2af621bb2754497fb12f23a0bd13b758\"><u>LEAH</u></a> - supporting EA student groups in London. They are also running a co-working space for students and professionals working on EA projects</li><li><a href=\"https://www.effectivegiving.org/\"><u>Effective Giving</u></a></li><li><a href=\"https://rethinkpriorities.org/\"><u>Rethink Priorities</u></a> ~18 people based in UK</li><li><a href=\"https://www.socialchangelab.org/\"><u>Social Change Lab</u></a></li></ul></li><li>Global Development<ul><li><a href=\"https://www.againstmalaria.com/\"><u>Against Malaria Foundation</u></a></li><li><a href=\"https://www.schistosomiasiscontrolinitiative.org/\"><u>Schistosomiasis Control Initiative</u></a></li><li><a href=\"https://www.malariaconsortium.org/\"><u>Malaria Consortium</u></a></li><li><a href=\"https://www.givedirectly.org/\"><u>GiveDirectly</u></a> - London Office</li><li><a href=\"https://www.suvita.org/\"><u>Suvita</u></a></li><li><a href=\"https://www.canopie.app/\"><u>Canopie</u></a></li><li><a href=\"https://leadelimination.org/\"><u>Lead Exposure Elimination Project</u></a></li></ul></li><li>Animal Welfare - Most of these are the UK based teams for international orgs<ul><li><a href=\"https://thehumaneleague.org/\"><u>The Humane League</u></a></li><li><a href=\"https://animalequality.org.uk/\"><u>Animal Equality</u></a></li><li><a href=\"https://opencages.org/\"><u>Open Cages</u></a></li><li><a href=\"https://www.gfi.org/\"><u>The Good Food Institute Europe</u></a></li><li><a href=\"https://www.animaladvocacycareers.org/\"><u>Animal Advocacy Careers</u></a></li><li><a href=\"https://www.animalask.org/\"><u>Animal Ask</u></a></li><li><a href=\"https://www.fishwelfareinitiative.org/\"><u>Fish Welfare Initiative</u></a></li><li><a href=\"https://www.cellag.uk/\"><u>Cellular Agriculture UK</u></a></li></ul></li><li>AI Alignment<ul><li><a href=\"https://www.alignmentforum.org/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup\"><u>Conjecture</u></a></li><li><a href=\"https://www.cooperativeai.com/\"><u>The Cooperative AI Foundation</u></a></li><li>Deepmind Safety team</li><li><a href=\"https://www.safeailondon.org/\">Safe AI London</a></li></ul></li><li>Long Term Future/Existential Risks<ul><li><a href=\"https://www.longview.org/\"><u>Longview Philanthropy</u></a></li><li><a href=\"https://www.ler-i.org/\"><u>London Existential Risk Initiative</u></a> (aimed at students)</li><li><a href=\"https://www.1daysooner.org/\"><u>1Day Sooner</u></a></li><li><a href=\"https://longtermrisk.org/\"><u>Center on Long-Term Risk</u></a><br>&nbsp;</li></ul></li></ul><p>There are some remote staff in London for various EA related organisations, such as Animal Charity Evaluators, Centre for Effective Altruism, GiveWell, Alvea, Our World in Data and Giving Green.&nbsp;&nbsp;</p><h2><strong>Oxford</strong></h2><p>Oxford has two main groups,&nbsp;<a href=\"https://eaoxford.com/\"><u>EA Oxford</u></a> (mostly university students), and professionals who work at EA organisations, mainly at<a href=\"https://forum.effectivealtruism.org/posts/pXrDmqDA63Wrffxmo/my-job-ea-office-manager#Day_in_the_life\">&nbsp;<u>Trajan House</u></a>. Organisations at Trajan House include:-</p><ul><li><a href=\"https://www.centreforeffectivealtruism.org/\"><u>Centre for Effective Altruism</u></a></li><li><a href=\"https://ev.org/\"><u>Effective Ventures</u></a></li><li><a href=\"https://www.fhi.ox.ac.uk/\"><u>The Future of Humanity Institute</u></a></li><li><a href=\"https://globalprioritiesinstitute.org/\"><u>The Global Priorities Institute</u></a></li><li><a href=\"https://www.forethought.org/\"><u>The Forethought Foundation</u></a></li><li><a href=\"https://www.globalchallengesproject.org/\"><u>The Global Challenges Project</u></a></li><li><a href=\"https://ourworldindata.org/\"><u>Our World in Data</u></a></li><li>A number of people working at other EA organisations (such as Rethink Priorities, HLI, LEEP and OpenPhil)</li></ul><p>&nbsp;Other organisations in Oxford&nbsp;</p><ul><li>Wytham Abbey, near to Oxford, which hosts EA related events and retreats</li><li><a href=\"https://www.aisafetyhub.org/\"><u>AI Safety Hub</u></a></li><li><a href=\"https://www.aligned-ai.com/\"><u>Aligned AI</u></a></li><li><a href=\"https://www.leaf.courses/\"><u>Leaf</u></a></li></ul><p>&nbsp;</p><h2><strong>Cambridge</strong></h2><p>EA in Cambridge has a<a href=\"https://www.eacambridge.org/\">&nbsp;<u>student group</u></a> and an office run by Cambridge EA CIC. There are a few EA related organisations as well as academics and other professionals.</p><ul><li>Cambridge EA CIC</li><li><a href=\"http://cambridgeaisafety.org\"><u>Cambridge AI Safety Hub</u></a></li><li><a href=\"https://www.cerifellowship.org/\"><u>Cambridge Existential Risks Initiative</u></a> - student group</li><li><a href=\"https://erafellowship.org/\">Existential Risks Alliance</a> - summer research fellowships</li><li><a href=\"https://www.cam.ac.uk/\"><u>The Centre for the Study of Existential Risk</u></a><u>&nbsp;</u></li><li><a href=\"http://lcfi.ac.uk/\"><u>Leverhulme Centre for the Future of Intelligence</u></a></li></ul><p>&nbsp;</p><h2><strong>Outside of Loxbridge</strong></h2><p>Outside of London, Oxford and Cambridge there aren\u2019t many active city/regional groups (if you know of any that are active, let me know). In the last few years other groups have been tried in different cities but don\u2019t tend to get much uptake.</p><ul><li>There is the<a href=\"https://ceealar.org/\">&nbsp;<u>Centre for Enabling EA Learning and Research</u></a> (formerly known as the EA Hotel) in Blackpool with about 15-25 people.</li><li><a href=\"https://eabristol.co.uk/\"><u>Bristol</u></a> has both a student and city group.&nbsp;</li></ul><h3>&nbsp;</h3><h3><strong>Student Groups</strong></h3><p>There are a few student groups that are relatively active although which groups are most active varies from year to year.</p><ul><li><a href=\"https://eabristol.org/\"><u>Bristol</u></a></li><li><a href=\"https://www.eadurham.org/\"><u>Durham</u></a></li><li><a href=\"https://eaedinburgh.com/\"><u>Edinburgh</u></a> (also open to non-students)</li><li><a href=\"https://linktr.ee/eamanchester\"><u>Manchester</u></a></li><li><a href=\"https://www.facebook.com/EffectiveAltruismSheffield/\"><u>Sheffield</u></a></li><li><a href=\"https://www.eastandrews.org/\"><u>St Andrews</u></a></li><li><a href=\"https://www.facebook.com/easussex/\"><u>Sussex</u></a></li><li><a href=\"https://www.facebook.com/EffectiveAltruismWarwick/\"><u>Warwick</u></a></li></ul><p>&nbsp;</p><h1><strong>EA Related Offices in the UK</strong></h1><p>London</p><ul><li>Coworking space set up by LEAH near Farringdon. You can express interest in working there via<a href=\"https://airtable.com/shrXJH4hcQCNJO7Fj?fbclid=IwAR04Wc9VrZbZ0QiCjqIqReDVREzxIb_S2x_hg4HJJUkDBcdXVewd4zQjKD8\">&nbsp;</a><a href=\"https://airtable.com/shr2RTtehg2kSPBTQ\"><u>this form</u></a></li><li>Charity Entrepreneurship have an office near Queen\u2019s Park that sometimes has people coworking from other organisations</li></ul><p>Oxford</p><ul><li>Trajan House -&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfohKWuCKFWLC3-_zF7Sl8kIy9LkSESgTTwhJhwpFMpAa_zDg/viewform\"><u>application form</u></a></li><li>The EA Oxford student group has access to coworking and event space during term times</li></ul><p>Cambridge</p><ul><li>Cambridge EA CIC has a <a href=\"https://www.meridian-office.org/\">coworking space</a> partly used by the EA group there</li></ul><p>Blackpool</p><ul><li><a href=\"https://ceealar.org/\"><u>Centre For Enabling EA Learning &amp; Research</u></a></li></ul><h1>Events&nbsp;</h1><p>UK wide in person and virtual events are advertised primarily on our newsletter (<a href=\"https://eauk.substack.com/\">subscribe</a> to receive monthly updates) and <a href=\"https://effectivealtruismuk.slack.com/join/shared_invite/zt-kt5pnzxy-amLO7XH_HX8KBfafwRhbtg#/shared-invite/email\">EA UK Slack group</a>.&nbsp;</p><h1>Useful EA UK Links</h1><ul><li>EA UK <a href=\"https://effectivealtruism.uk/\">website</a></li><li><a href=\"https://effectivealtruismuk.slack.com/join/shared_invite/zt-kt5pnzxy-amLO7XH_HX8KBfafwRhbtg#/shared-invite/email\">EA UK Slack group</a>&nbsp;</li><li>Chat with <a href=\"https://airtable.com/shrUOwHR1066plDxm\">an organiser</a> or another <a href=\"https://effectivealtruism.uk/community\">community member</a></li><li><a href=\"https://eauk.substack.com/\">Sign up</a> to our monthly newsletter</li><li><a href=\"https://effectivealtruism.uk/get-involved\">Ways to get involved&nbsp;</a></li><li><a href=\"https://effectivealtruism.uk/causes\">Causes</a> and <a href=\"https://effectivealtruism.uk/careers\">careers </a>directories</li></ul><p>&nbsp;</p><p><i>If there is anything missing from above that you would like added or changed, let me know.</i></p>", "user": {"username": "DavidNash"}}]