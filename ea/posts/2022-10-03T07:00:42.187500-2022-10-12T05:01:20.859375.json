[{"_id": "ZNPYMp2uu5zr3Po66", "title": "\u201cTechnological unemployment\u201d AI vs. \u201cmost important century\u201d AI: how far apart?", "postedAt": "2022-10-11T04:50:54.591Z", "htmlBody": "<p><i>This was cross-posted by the Forum team after the time that it was published.</i></p><p>In casual conversations about the future of AI - particularly among people who don\u2019t go in for <a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\"><strong><u>wild</u></strong></a>, sci-fi stuff - there seems to be a lot of attention given to the problem of <strong>technological unemployment</strong>: AI systems outcompeting humans at enough jobs to create a drastic, sustained rise in the unemployment rate.</p><p>This tends to be seen as a \u201cnear-term\u201d problem, whereas the <a href=\"https://www.cold-takes.com/most-important-century/\"><strong><u>world-transforming impacts of AI I\u2019ve laid out</u></strong></a> tend to be seen as more \u201clong-term.\u201d</p><p>This <i>could</i> be right. But here I\u2019ll try to convey an intuition that it\u2019s overstated: that the kind of AI that could <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#impacts-of-pasta\"><strong><u>power a massive productivity explosion and threaten humanity\u2019s very existence</u></strong></a> could come pretty soon after - or even before! - the kind of AI that could lead to significant, long-lasting technological unemployment.</p><h2><strong>\u201cTechnological unemployment\u201d AI would need to be extraordinarily powerful and versatile</strong></h2><p>The first key point is that I think people <strong>underestimate how powerful and versatile AI would have to be to create significant, long-lasting technological unemployment.</strong></p><p>For example, imagine that AI advances to the point where truck drivers are no longer needed. Would this add over <a href=\"https://www.alltrucking.com/faq/truck-drivers-in-the-usa/#:~:text=There%20are%20approximately%203.6%20million,American%20Trucking%20Associations%20(ATA).\"><strong><u>3 million Americans</u></strong></a> to the ranks of the unemployed? Of course not - they\u2019d get other jobs. We\u2019ve had centuries of progress in automation, yet today\u2019s unemployment rate <a href=\"https://fred.stlouisfed.org/series/UNRATE\"><strong><u>is similar to where it was 50 years ago</u></strong></a>, around 5-6%.</p><p>(Temporary unemployment/displacement is a potential issue as well. But I don't think it is usually what people are picturing when they talk about technological unemployment, and I don't see a case that there's anything in that category that would be importantly different from the daily job destruction and creation that has been part of the economy for a long time.)</p><p>In order to leave these 3 million people <i>durably </i>unemployed, AI systems would have to outperform them at essentially <strong>every economically valuable task.</strong></p><p>When imagining a world of increasing automation, it\u2019s not hard to picture a lot of job options for relatively low-skilled workers that seem very hard to automate away. Examples might include:</p><ul><li>Caregiver roles, where it\u2019s important for people to feel that they\u2019re connecting with other humans (so it\u2019s hard for AI to fully fill in).</li><li>Roles doing intricate physical tasks that are well-suited to human hands, and/or unusually challenging for robots. (My general sense is that AI software is improving more rapidly than robot hardware.)</li><li>Providing training data for AIs, focused on cases where they struggle.</li><li>Surveying and interviewing neighbors and community members, in order to collect data that would otherwise be hard to get.</li><li>Perhaps a return to <a href=\"https://ourworldindata.org/employment-in-agriculture#employment-in-agriculture-1300-to-today\"><strong><u>agricultural employment</u></strong></a>, if rising wealth leads to increasing demand for food from small, humane and/or picturesque farms (and if it turns out that AI-driven robots have trouble with all the tasks these farms require - or it turns out that AI-run farms are just hard to market).</li><li>Many more possibilities that I\u2019m not immediately thinking of.</li></ul><p>And these roles could end up paying quite well, if automation elsewhere in the economy greatly raises productivity (leading to more total wealth chasing the people in these roles).</p><p>In my view, <strong>a world where automation has made low-skill workers </strong><i><strong>fully unemployable</strong></i><strong> is a world with </strong><i><strong>extremely powerful, well-developed, versatile AI systems and robots</strong></i> - capable of doing <i>everything </i>that, say, 10% of humans can do. This could require AI with human-level capabilities at language, logic, fine motor control, interpersonal interaction, and more.</p><h2><strong>Powerful, versatile AI could quickly become transformative (\"most important century\") AI</strong></h2><p>And then the question is, how far is that from a world with AI systems that can make <i>higher-skilled</i> workers fully unemployable? For example, AI systems that could do absolutely everything that today\u2019s successful scientists and engineers can do? Because that sounds to me like <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\"><strong><u>PASTA</u></strong></a> (my term for a type of AI that I've argued could make this century the <a href=\"https://www.cold-takes.com/most-important-century/\"><strong><u>most important of all time for humanity</u></strong></a>), and at that point I think we have <a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#impacts-of-pasta\"><strong><u>bigger things to worry about</u></strong></a>.</p><p>In fact, I think there\u2019s a solid chance that PASTA will come <i>before</i> the kind of AI that can make lower-skilled workers unemployable. This is because PASTA might not have to match humans at certain kinds of motor control and social interaction. So it might not make anyone totally unemployable (in the sense of having zero skills with economic value), even as it leads to a productivity explosion, wild technologies like <a href=\"https://www.cold-takes.com/how-digital-people-could-change-the-world/\"><strong><u>digital people</u></strong></a>, and maybe even human extinction.</p><p><strong>The idea that we might see AIs </strong><i><strong>fully outcompete </strong></i><strong>low-skill humans in the next few decades, but not fully outcompete higher-skill humans until decades after that, seems intuitively a bit weird to me. </strong>It could certainly end up being right, but I worry that it is fundamentally coming from a place of anthropomorphizing AI and assuming it will find the same things easy and challenging that we do.</p><p><strong>Bottom line: </strong>I think it\u2019s too quick to think of technological unemployment as the next problem we\u2019ll be dealing with, and wilder issues as being much further down the line. By the time (or even before) we have AI that can truly replace every facet of what low-skill humans do, the \u201cwild sci-fi\u201d AI impacts could be the bigger concern.</p>", "user": {"username": "HoldenKarnofsky"}}, {"_id": "FSmv5dNABMAnkDxmW", "title": "Power-Seeking AI and Existential Risk", "postedAt": "2022-10-11T21:47:58.613Z", "htmlBody": "", "user": {"username": "antoniofrancaib"}}, {"_id": "RDoLDJ4toRNpMRBmk", "title": "Which AI Safety Org to Join?", "postedAt": "2022-10-11T19:42:00.605Z", "htmlBody": "<p><strong>Long TL;DR</strong>: You\u2019re an engineer, you want to work on AI Safety, you\u2019re not sure which org to apply to, so you\u2019re going to apply to all of them. But - oh no - some of these orgs may actively be causing harm, and you don\u2019t want to do that. What\u2019s your alternative? Study AI Safety for 2 years before you apply? In this post I suggest you can collect the info you want quickly by going over specific posts</p><h1>Why do I think some orgs might be [not helping] or [actively causing harm]?</h1><p>Example&nbsp;<a href=\"https://www.lesswrong.com/posts/LLRtjkvh9AackwuNB/on-a-list-of-lethalities#Section_C\"><u>link</u></a>. (Help me out in the comments with more?)</p><h1>My suggestion:</h1><h2>1. Open the tag of the org you want on lesswrong</h2><p><strong>How</strong>: Search for a post related to that org. You\u2019ll have tags on top of the post. Click the tag with the org name.</p><h2>2. Sort by \u201cnewest first\u05f4</h2><h2>3. Open 2-3 posts</h2><p>(Don\u2019t read the post yet!)</p><h2>4. In each post, look at the top 2-3 most upvoted comments</h2><h1>What I expect you\u2019ll find sometimes</h1><p>A post by the org, with comments trying to politely say \u201cthis is not safe\u201d, heavily upvoted.</p><h1>Bonus: Read the comments</h1><p>Or even crazier: Read the post! [David Johnson <a href=\"https://forum.effectivealtruism.org/posts/RDoLDJ4toRNpMRBmk/which-ai-safety-org-to-join?commentId=s5i2GmtXCBjLZECu8\">thinks</a> this is a must!]</p><p>Ok, less jokingly, this seems to me like a friendly way to start to see the main arguments without having to read too much background material (unless you find, for example, a term you don\u2019t know).</p><h1>Extra crazy optimization: Interview before research</h1><p>TL;DR: <a href=\"https://forum.effectivealtruism.org/posts/bCDzacsx5cbzNzdHF/software-developers-how-to-have-impact-wip#A__Apply_first__filter_later\">First apply</a> to lots of orgs, and then, when you know which orgs are interested<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn1az6k2y7s\"><sup><a href=\"#fnn1az6k2y7s\">[1]</a></sup></span>, then do your<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvr79ns4vngo\"><sup><a href=\"#fnvr79ns4vngo\">[2]</a></sup></span>&nbsp;research only those orgs.</p><h1>Am I saying this idea for vetting AI Safety orgs is perfect?</h1><p><strong>No</strong>, I am saying it is&nbsp;<strong>better than the alternative of \u201capply to all of them (and do no research)\u201d</strong>, assuming you resonate with my premise of \u201cthere\u2019s a lot of variance in effectiveness of orgs\u201d and \u201cthat matters\u201d.</p><p>I also hope that by posting my idea, someone will comment with something even better.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn1az6k2y7s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn1az6k2y7s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>However you choose to define \"interested\". Maybe research the orgs that didn't reject your CV? Maybe only research the ones that accepted you? Your call</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvr79ns4vngo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvr79ns4vngo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Consider sharing your thoughts with the org. Just remember, whoever is talking to you was chosen as a person that convinces candidates to join. They will, of course, think their org is great. Beware of reasons like \"the people saying we are causing harm are wrong, but we didn't explain publicly why\". The whole point is letting the community help you with this complicated question.</p></div></li></ol>", "user": {"username": "hibukki"}}, {"_id": "9www8LehXQncXExxg", "title": "What's your favourite Science Fiction book/TV series/movie/anime/other?", "postedAt": "2022-10-11T15:46:32.742Z", "htmlBody": "<p>I don't think it is a wild guess to say that many EAs have a more or less strong <strong>interest in science fiction</strong>. I want to spark this interest even more by starting with an unstructured question about our favourite pieces of sci-fi. If motivated enough I may also write something to justify why I think that engaging with sci-fi is not only<strong> pleasurable</strong> but also <strong>extremely useful</strong>.&nbsp;</p>", "user": {"username": "Luca Parodi"}}, {"_id": "YfTwdCgmvfuybXwYT", "title": "Corporate Campaigns: determining the scale of the ask", "postedAt": "2022-10-11T14:40:25.682Z", "htmlBody": "<h2><strong>EXECUTIVE SUMMARY</strong></h2><p>Within animal advocacy, corporate campaigns have become an important and widely used tactic as evidence of their efficacy has accrued (\u0160im\u010dikas 2019; Philanthropy 2016; Sarek 2019a, [b] 2019). These campaigns allow the movement to leverage a relatively small amount of funding to disrupt and apply large amounts of pressure onto corporations with the intention to achieve policy change. The asks for these anti-corporate campaigns can come in several forms; from more simplistic singular asks such as the cage-free campaigns, to more complicated composite asks which include several elements such as the Better Chicken Commitment (\u201cThe Better Chicken Commitment Policy\u201d n.d.). From our previous research into the asks used in such corporate campaigns, we identified a foundational uncertainty in their construction. Namely, how much should we ask for? The more additional elements incorporated into the ask, the greater the potential benefits to the animal we are trying to provide improved protections. However, as more elements are added, the greater the investment required by the companies targeted, and the more difficult it becomes to communicate the ask to the public and the companies\u2019 decision makers.</p><p>In this report, we examine the fundamental theories behind how these campaigns work and the factors determining their success. Ultimately we view corporate campaigns as a form of extortion, where interest groups threaten companies with either supply chain disruption, or more commonly, reputational damage if they do not comply with their demands. We find that if companies experience anti-corporate campaigns or suffer from other similar revelations of unethical business practices, this information can have significant lasting damage to their market capitalisation and financial performance. Thus, companies should be willing to invest significant resources to mitigate this risk. It is possible to model this decision from a rational economic point of view where corporations should be willing to pay up to the potential damage to avoid a campaign. However any attempts to do so should be understood within the specific context of that corporation's decision-making and in light of the large amount of uncertainty involved. As corporate campaigns are ultimately a social phenomena, precise quantitative estimates are not reliable, but various research methods can be used to make more informed decisions on the magnitude of the ask that can be made. To better inform future decision makers in ask formation, we should combine this sort of analysis with scoping the specific context of the campaign including the supply chain, key stakeholders and their motivations.</p><h2><strong>HOW DO CORPORATE CAMPAIGNS WORK?</strong></h2><p>Corporate campaigns are a strategic effort by an interest group to put pressure on or cooperate with a company to improve their organisation's policies. These interest groups can come from within the company, as is the case with labour unions, or external movements that focus on issues that fall outside the attention of the majority of employees or investors (King 2016). These types of campaigns have been used successfully in many different fields, including the \u2018birth of the corporate campaign\u2019 in workers unions (\u201cACTWU vs. J.P Stevens: 1976-1980\u201d n.d.), fair trade (Harris 2021) and environmental protection (\u201cAlaska Wilderness League vs. The Bush Administration: 2001-2002\u201d n.d.).</p><p>These campaigns use a variety of tactics; from more cooperative approaches such as promoting change from within the company by supporting aligned stakeholders or offering technical assistance, to adversarial approaches such as media campaigns (Capriati 2018) or, in the case of labour unions, strikes. When cooperative approaches are used, this is better viewed as corporate outreach, although this is often the first stage of such campaigns when working with the most aligned companies within an industry. Whenever adversarial approaches are threatened, corporate campaigns can be viewed as a form of extortion where an interest group threatens a company with some action if they don\u2019t capitulate to their demands.</p><p>Theories for explaining the success of these movements depend on the tactics employed by the interest groups. The most simplistic tactic is economic disruption caused by direct action. When this is used, the aim of campaign groups is to directly disrupt the operations of the business, in order to impose large enough costs so they regard the costs of resistance as outweighing the costs of conceding to the group demands (Luders 2006; Spar and La Mure 2003). These tactics are most commonly used by primary stakeholders, such as labour unions who can use strikes or work to rule tactics to disrupt business. However, other movements have and continue to use direct action in a similar vein by blocking or disrupting supply chains through sit-ins or lock-ons.</p><p>Many movements are able to achieve change through corporate campaigns without the use of mass disruption tactics (King 2016). Such campaigns typically target public facing companies that have increasingly emphasised the value of intangible resources, such as their reputation. Over the past few decades, firms have increased the monitoring and value of their reputations and caused reputation management to become a major focus of corporate public relations (King 2016). Thus, in as far as reputation is or is perceived to be connected with profit or market value, or valued intrinsically by management (Tim Bartley and Child 2007), interest groups can \u2018name and shame\u2019 corporations without disrupting the day-to-day business and still achieve their goals.</p><h2><br><strong>WHAT IS THE VALUE OF A FIRM'S REPUTATION?</strong></h2><h2><strong>Why is reputation valuable to a firm?</strong></h2><p>As many modern anti-corporate campaigns have shifted towards targeting a firm\u2019s reputation, understanding the value of their reputation is important for understanding the degree to which this can motivate a firm.</p><p>Compared to the financial fundamentals of a firm's value, it is easy to see reputation as a nebulous and intangible asset beyond the scope of analysts\u2019 models. However, it is widely observed that the valuation and performance of companies is only in part driven by easily measurable financial information, particularly in the short term, with the difference often being attributed to market \u2018sentiment\u2019 (Cole 2012).</p><p>Some efforts to measure a firm's reputation focus on one overall judgement of a company either across all groups or within groups of stakeholders. Other conceptions of a firm's reputation split reputation into two components; cognitive and affective (Raithel and Schwaiger 2015). The cognitive component encompasses judgements surrounding the firm\u2019s competence and reliability, while the affective component captures \u2018feelings\u2019 towards a firm, such as emotional appeal, attractiveness, or social and environmental responsibility.</p><p>Positive reputations have a number of potential favourable consequences, including the ability to charge higher prices, attract better applicants, and improve access to capital (Fombrun and Shanley 1990; Raithel and Schwaiger 2015; Srivastava et al. 1997). These ultimately influence the company's revenues, margins, and the existence of a competitive advantage (Smith, Smith, and Wang 2010). Corporate reputation has been found to create positive abnormal stock returns in the long term, particularly for non-financial aspects which are less likely to be priced in (Raithel and Schwaiger 2015), as well as providing a protective effect against crises (Schnietz and Epstein 2005; Wei, Ouyang, and Chen 2017).</p><h2><strong>What is the market value of a firm's reputation?</strong></h2><p>Various researchers have attempted to develop methods to quantify the relative proportion of a company's market cap that is attributable to their reputation. Thus far, this has been motivated to create better predictive models of a company's value for investors, and to provide more objective measures for reputation management within firms. However, it should be possible to leverage the same information in corporate campaigns to actively work against the value of a company's reputation; both by targeting companies with valuations that are more dependent on their reputation, and if interest groups actively target the same measures that companies use when managing their public relations.</p><p>Cole (2012), of Reputation Dividend, developed one such model by using regression analysis on a wide set of potential explanatory variables for a company\u2019s market capitalisation, covering financial information and analytics alongside survey data from the \u2018Most Admired Companies\u2019 studies published by Management Today in the UK and Fortune in the US (Cole 2012). This survey measures reputation through the ratings of companies on nine criteria, from investment value and quality of management and products to social responsibility and ability to attract talent by a large sample of executives, directors, and securities analysts (Belanger 2022). The average of these was considered the overall strength of a company's reputation for Cole\u2019s model. This information was run through multiple iterations and refinements. Cole\u2019s model suggests that company reputations accounted for close to 26% and 32% of the total market capitalisation of the S&amp;P 500 and FTSE 250 respectively. The variation in reputation contribution is even higher between individual companies, with a high of almost 58% (Apple) to a low of \u201339% (Sears Holdings). Reputation Dividend provides these evaluations each year in both the US and UK.</p><p><img src=\"https://static.wixstatic.com/media/c2b5df_7ea4592b36ba4098a9f58a1ebf5c4c03~mv2.png/v1/fill/w_566,h_325,al_c,lg_1,q_85,enc_auto/c2b5df_7ea4592b36ba4098a9f58a1ebf5c4c03~mv2.png\"></p><p>Other efforts to evaluate the value of a corporate reputation have taken a more direct route by surveying global executives for their direct estimates of this figure (Shandwick 2020). As with Cole\u2019s (2012) estimate, Shandwick\u2019s (2020) varied widely depending on the country. The average estimate was 63% of a firm's market value, but this was as low as 47% in the UK and Hong Kong, and as high as 76% in Brazil. On an individual firm level, about two-thirds of executives attributed over half their company\u2019s value to their reputation and a third over three-quarters of its value. These figures are much higher than the estimates produced by reputation analysis, suggesting that executives might overvalue their company\u2019s reputation. However the definition of reputation used here was broader than Cole\u2019s, with many executives including the quality of products and services as the main contributor to a company's reputation, which is likely to have a significant effect on its market value.</p><p>Still, even if a firm's reputation makes up a large portion of a company's value, this does not reveal a company's willingness to pay to improve its reputation or avoid the cost involved in an anti-corporate campaign. Each of these variables will only be able to influence stakeholder perceptions to a degree.</p><p>&nbsp;</p><h2><strong>HOW MUCH DAMAGE CAN CORPORATE CAMPAIGNS DO TO A FIRM'S REPUTATION?</strong></h2><p>In principle, it should be possible to quantify more precisely the costs that NGO activism imposes on particular firms or industries (Spar and La Mure 2003). Does the share price of targeted firms decrease during campaigns and increase after commitments? Or can we measure the hypothesised effect of such campaigns on other outcomes, such as expense on the cost of recovery, lost revenue, talent acquisition, and the cost of capital for firms?</p><p>The existing evidence shows that public announcements or media attention towards poor labour practices (Rock 2003; T. Bartley and Child 2011), pollution (Hamilton 1995; Rao 1996; Rao and Brooke Hamilton 1996), sexual harassment (Bouzzine and Lueg 2022; Borelli-Kjaer, Schack, and Nielsson 2021) and employee discrimination (Rao and Brooke Hamilton 1996) can have a significant effect on the stock price of affiliated organisations. However, there are conflicting results, with some studies not finding significant effects (Tim Bartley and Child 2010) depending on the events included and time period analysed. Still, most published studies find that these effects can be quite large, with statistically significant short-term effects from the studies cited above ranging from -0.82% (Rock 2003) to -5.67% (Rao 1996).</p><p>In some instances, these effects can persist across larger time scales, with Rao finding strong abnormal effects six months after reports of a company's environmental pollution (Rao 1996). Pentland Analytics\u2019 review of 125 of the largest crises between 2008 and 2018 found that companies lost 5% of expected market capitalisation in the following 12 months (Pretty 2018). It is interesting to note that there was a large divergence in the sample, with a split emerging relatively quickly, within a week, between companies that subsequently gained value and those that lost a much greater share. Regardless of the outcome on the companies\u2019 valuation, price volatility increased for all firms, signalling a higher risk associated with the firm increasing the cost of capital.</p><p>The longest-term evaluation of the effects of revelations of unethical business practises comes from Long et al (2016) who examined the effect announcement of various ethical violations, including bribery/illegal payments, employee discrimination, environmental pollution, and insider trading over a much larger timescale of 5-20+ years. They still found significant effects after 5 years, with -0.0357% (p=0.025) lower average daily return of firms with an announcement of unethical behaviour compared to other firms in their industries (Long 2016). Although they found no significant effect for all time frames greater than five years, statistically significant effects that persist across such long time frames suggest such announcements represent real persistent damage to the company's value.</p><p>To give a sense of the loss in market capitalisation these figures can represent, we can expand on the short-term effect found from anti-sweatshop campaigns by Rock (2003). Here they found a statistically significant effect on market capitalisation from as high as $166.6 million for Target to $3.25 million for K-Mart, with an average loss in market capitalization of $69.9 million or 0.82% (Rock 2003). Interestingly, the inverse effect was then observed for good news when Reebok, a shoe retailer, agreed to phase out the use of sweatshop labour with an abnormal return of 1.65% or $19.35 million. The effect on company value found across the studies cited above is more consistent in percentage terms varying from ~1-5%, however for total market capitalisation, this can mean much greater losses for larger companies.</p><p>More broadly, there are numerous case studies of scandals or campaigns that seem to have even more significantly reduced the reputation, value, and revenue of companies over the longer term. These include Chipotle, which lost almost two-thirds of its market value from an E. coli scandal (Bowman 2017), Wells Fargo, which lost $29 billion (9.2%) of its market value after a scandal about their business practices (Glazer 2018), and SeaWorld, which lost 50% of its market value over two years after the documentary titled \u2018Blackfish\u2019 was released (Stokes and Atkins-Sayre 2018).</p><p>One weakness of a lot of these event analysis studies is that they only analyse the effect of such announcements on short-term (a few days to a week) stock price fluctuations. The studies that examine a longer time frame of a year or more give a better sense of the long-term damage such scandals or campaigns can cause. Isolating the truly long term effects of these campaigns or scandals becomes increasingly difficult as companies are likely to have responded to the damage to their reputation and stock returns with policy changes that counteract their effects. Still, even if very long-term shifts are not achieved, CEOs confronted with scandals of such magnitude will face compensation penalties or dismissal based on the short and medium-term effects (Crisp 2021).</p><p>Another problem with generalising from these studies is that they only select the most \u2018notable\u2019 examples of campaigns that received a lot of media attention, thus making researchers aware of their existence. This means that these findings are most relevant to the effect of a successful campaign rather than the average effect of all campaigns.</p><p>In the same vein, all of the firms examined are public facing and traded with established reputations, whereas many firms within animal agriculture are privately owned and have some of the worst reputations, such as slaughtering companies (Albersmeier and Spiller 2008). Additionally, the animal agriculture industry already has a very critical perception from a number of stakeholders (Luhmann and Theuvsen 2016). Thus, these findings will not necessarily include campaigns targeted at producers or other actors further down the supply chain. The food retail and grocery industry, particularly higher end brands that put much larger stock in their reputation, are more likely to experience the significant and lasting damage from association with unethical practices observed in the literature.</p><p>If we take these estimates at face value, successful campaigns or revelations of unethical behaviour can reduce the value of affected firms between 1-5% in at least the short to medium term. If we apply this general heuristic to the food and grocery retail industry with an evaluation of $750 billion in the US (Grand View Research 2022) and \u00a3205 billion in the UK (Bedford 2022), then successful campaigns should do between $7.5 to $37.5 billion worth of damage in the US and \u00a32 to \u00a310 billion worth of damage in the UK. If we compare this to the estimated $6 billion investment required to shift from cage to cage-free systems in the US (Wong 2017) which has widespread success with gaining commitments, this figure looks quite reasonable.</p><p>&nbsp;</p><h2><strong>WHAT ARE FIRMS\u2019 MOTIVATIONS FOR GENERAL CORPORATE SOCIAL RESPONSIBILITY?</strong></h2><p>A tacit assumption in the disruption and reputation model of a corporate campaign is that companies act as rational actors who seek the least costly solution acting in accordance with the rational choice theory of organisational decision making. However, there is broad agreement in the existing literature that rational choice models do not reflect the real-world reality of individual decision making. This does not discount these models entirely, with the typical economic reasoning being that any individual will perform in ways we can't predict, but collective behaviour should act more in line with the economic theory long term.</p><p>Some researchers have made efforts to move towards descriptive models of decision making for which all observed decision making could be described, but these efforts also seem to have failed (Dillon 1998). Instead, each organisation may act according to a number of decision-making models or motivations, or vary the tool used across time or between contexts. Therefore, when attempting to model and anticipate the decision making and behaviour of organisations, one should not weigh any given model too heavily without significant previous experience of the behaviour of the organisations and decision-makers involved. These models can improve our ability to predict an organisation\u2019s behaviour, but should be accompanied with observations of their motivations for previous decisions.</p><p>In the context of a corporate campaign, it is not uncommon for organisations to comply with activists' demands even when it may not be in their best interest to do so (Spar and La Mure 2003). In such instances, management may have strong commitments to the goals expressed by their potential critics. Unfortunately, the motivations for compliance with corporate campaigns have not been directly studied. Instead we can look at the wider literature on the general motivations of firms for Corporate Social Responsibility (CSR) initiatives, of which animal welfare is considered a part.</p><p>Broadly, companies\u2019 motivations for CSR activities can be split into intrinsic and extrinsic motivations. Extrinsic motivations for CSR hold that CSR is instrumental for the goals of the company in the long run. This includes corporate reputation and the financial benefits that can accrue from this (examined in previous sections). Intrinsic motivations, on the other hand, come from business cultures that are committed to certain business principles, including moral duties (Graafland and van de Ven 2006). The relative importance of each motivation within these categories varies between industries, based on industry characteristics, relevant stakeholders, and the structure of their supply chain. However, some broad factors emerge across industries and contexts, and these can be seen as universal motivations for CSR.</p><p>Zhang et al (2018) conducted a systematic review of the available literature to identify the drivers, motivations and barriers to CSR in the construction industry (Zhang, Oo, and Lim 2019). Through content analysis they identified three external drivers of CSR; policy pressure, market pressure, and innovation and technology. These act as external coercive forces that can \u2018force\u2019 a corporation to undertake CSR activities. Alongside this, the most common internal motivations reported by companies were financial benefits, reputation, and organisational culture, which encompasses intrinsic motivations. Unfortunately, barriers to implementation were far more diverse, suggesting that there are a wide variety of potential barriers that vary across contexts. However, none apart from lack of knowledge or awareness of CSR within the organisation were that common.</p><p>In the energy sector, Agudelo et al (2020) conducted a systematic review on the drivers that motivate energy companies to adopt CSR (Latap\u00ed Agudelo, Johannsdottir, and Davidsdottir 2020). Here, the main drivers they identified were stakeholder engagement and satisfaction, corporate culture, and reporting, which is the publication of CSR activities.</p><p>In the financial sector, Wu et al (2013) studied the motives of banks to engage in CSR in part to understand the varied relationship between CSR and financial performance (Wu and Shen 2013). They found that the main drivers were strategic instrumental reasons, altruism (conducting CSR for its own sake), and greenwashing, which attempts to enhance corporate image without significantly changing the business.</p><p>These same motivations have also been observed in decision makers in many other contexts, such as professional sports (Babiak and Trendafilova 2009) and in a wider sample of firms in developing economies (Ali, Wilson, and Husnain 2022).</p><p>Cross-industry studies have found some general trends among the motivation of firms depending on size. In a survey of 111 large and small Dutch firms, Graafland et al (2006) found that large firms were more confident about the financial payoff of CSR than smaller firms on average, as the reputational mechanism is more important to them and they tend to have a longer time horizon than other companies in their industry. However, this difference was quite small with all firms reporting high agreement with both moral and strategic motivation for CSR (Graafland and van de Ven 2006). It was also of note that there was a stronger correlation between moral motivations and CSR performance than the strategic view.</p><p>Although these larger-scale industry-wide reviews of CSR do provide us with the general motivations for CSR, given variation in industry characteristics the main barriers will differ across industries. Fortunately, there has been some research into the use of CSR in the food industry supply chain. As with other industries, the main drivers behind CSR are improved competitiveness, scrutinisation by stakeholders, reputation, and legitimacy (Luhmann and Theuvsen 2016; Heyder and Theuvsen 2012). However, compared to other industries there is a greater emphasis on the pressure from NGOs, with this being the most frequently mentioned motivation or concern driving CSR in agribusiness (Maloni and Brown 2006; Hartmann 2011). It is also frequently noted that this pressure is most likely to be targeted at larger firms, making them more likely to implement CSR practices.</p><p>The existing literature on motivation for firm CSR initiatives is frequently subject to numerous methodological problems. Most notably, low response rates skew the sample towards those more actively involved in CSR, and possible social desirability bias towards more moral and less financial motivations.</p><p>From the existing research on the drivers of CSR across all industries we see that consistent motivations are strategic, reputational, and moral. These findings align with theories surrounding the mechanisms for corporate campaigns through the strategic and reputational motivations. The final motivation, moral, can be observed in industry leaders who sometimes choose to implement CSR or improved welfare practices either before activists begin any outreach or during the corporate outreach stage before any threats of a public campaign. These companies then act as examples for others, showing that the improvements are possible, and they can lead the way for their peers through the process of institutional isomorphism (Frank den Hond 2007).</p><p>Thus we can see that to measure the motivation of different companies, and how much it is possible to push for, we must both understand the value of and damage a corporate campaign can do to a reputation as well as the intrinsic motivations of key decision makers. The extrinsic motivation in some sense acts as a lower bound to the investment a firm should be willing to pay to maintain its good standing amongst its stakeholders, while intrinsic moral motivations can increase this further for some firms. However, in this case it is important to note the general finding that pushing on extrinsic motivation tends to diminish intrinsic motivations for an action (Frey and Jegen 2001).</p><p>&nbsp;</p><h2><strong>WHAT ARE THE MAIN FACTORS THAT AFFECT THE SUCCESS OF CORPORATE CAMPAIGNS?</strong></h2><p>The damage that revelations of unethical business practices or corporate campaigns can do is significant. However, there are other barriers that may prevent a firm from complying with the requests of activists.</p><p>Sheoin (2014) outlines various possible explanations for the failure of a number of previous corporate campaigns (Mac Sheoin 2014). Some of the reasons outlined that are behind the failure of previous campaigns revolve around mistakes or the tactics used. For example, during Greenpeace's campaign over the disposal of the offshore oil rig \u2018Brent Spar\u2019, a mistake in the estimation of the amount of toxic material on the rig was used to cast doubt on Greenpeace's scientific credentials. Another example is when the BBC interviewed an activist posing as a spokesperson for a company responsible for a chemical leak in India that killed more than 15,000 people, resulting in less favourable framing for the victims by the media.</p><p>Another factor that can affect the success of campaigns is the structure of the industry. Schurman (2004) argues that \u2018industry structures confer particular strategic openings and closures on social movements and render firms and industries more or less vulnerable to social movement actions\u2019 (Schurman 2004). These include broader structures of the whole industry, such as the level of competition in an industry. More competition incentivises companies to seek to enhance their market positions or maintain them by mimicking the capitulation of other firms. In a similar vein, industries in which brand name and/or reputation is important to competition should be more vulnerable to attack (Debora L. Spar 2008; Mac Sheoin 2014). These include industries with more concentrated pressure points in their commodity chain such as a small number of buyers, particularly if these buyers have weaker social and economic relationships with the suppliers. For example, if a country's retail market is dominated by four firms and the product targeted is only a minor motivation for consumers to shop at that retailer such as goats cheese. This allows them to dissociate from the primary target in response to a campaign, putting significant pressure on the target (Schurman 2004).</p><p>The product as a whole can make the industry more vulnerable to attacks, such as products with negative health effects, have a negative environmental impact, or widely offending people\u2019s moral sensibilities. However, a broad protective element for an industry is if it offers significant economic, national security, or political benefits to a state (or ruling party), where it will then receive greater state protection against challengers (Schurman 2004).</p><p>Vulnerabilities can also occur at the company level, such as associations with stakeholders with poor reputations, or if a company has made public commitments around a cause or proclaims a value that it can be reasonably shown not to uphold (Schurman 2004).</p><p>Outside of industry-wide or firm-level considerations, as predicted by a model of companies as rational actors, we find that higher transactions push firms towards the path of resistance (Debora L. Spar 2008). This is typically evaluated from a financial cost perspective, but should also be considered from a logistical or knowledge perspective. With this being a commonly cited reason for barriers to general social responsibility (Zhang, Oo, and Lim 2019). This is common with corporate social responsibility, or more friendly campaigns where initial cooperation with the industry is to educate them on alternative business practices. Alternatively, if a practice is currently integral to the supply chain and alternatives are not easily available, then regardless of what it may theoretically cost to shift this will not be seen as possible for the industry. An example of this would be where chick culling was 10 years ago, when the technology was in development but no concrete examples or alternatives were available. A corporate campaign on a topic such as this would be futile.</p><p>Another element of the ask which has been shown to affect the likelihood that companies respond positively to interest groups requests is request legitimacy (Eesley and Lenox 2006). This can be defined as the degree to which the issue raised by a stakeholder group request is well accepted and well defined by the public. Thus simpler asks that are more understandable to the general public will generally elicit a positive response suggesting composite asks are more difficult to campaign for, with the possible exception of certifier-based campaigns such as the fair trade campaigns or Aquatic Animal Alliance\u2019s certifier campaign.</p><p><br><strong>Summary of beneficial factors:</strong></p><p>High level of competition between firms in the targeted industry</p><p>Reputation is important to competition</p><p>Buyers have weaker social and economic relationships with the suppliers</p><p>The targeted product has negative health, environmental or moral consequences</p><p>Target companies have associations with stakeholders with poor reputations</p><p>The company proclaims a value that it can be reasonably shown not to uphold</p><p>The ask is well accepted and well defined by the public</p><p><strong>Summary of disadvantages:</strong></p><p>Product provides significant benefits to a state (or ruling party)</p><p>Higher costs for capitulation to demands</p><p>Logistical or knowledge barriers to implementation</p><p><br>&nbsp;</p><h2><strong>METHODS FOR EVALUATING THE TRACTABILITY OF AN ASK?</strong></h2><p>Given the theoretical background of corporate campaigns and observations of barriers to companies' capitulation to demands, there are a few indicators or research methods we can use to assess the tractability of a corporate ask.</p><p>The first, as outlined above, is by constructing a theoretical rational choice model of the company's or industry's decisions. This compares the full economic cost of the choice to comply with an interest group's demands to the reputational and financial damage the group is able to achieve through an anti-corporate campaign (Baron and Diermeier 2007). This includes both capital investment and the effect of ongoing price changes on the profitability of an industry. Thus, this evaluation would include the full net present cost of shifting the industry to the improved practice. Additional elements for composite asks would be prioritised based on the cost-effectiveness of the improvement on welfare for the corporation, limiting for the number of elements determined by the expected damage a campaign could cause. Under this model more elements could be included if improvements were cheap and easy to implement or the industry is particularly vulnerable to anti corporate campaigns. With examples of when this might be the case outlined in previous sections such as industries with larger more reputational driven firms in tight competition with each other and without a strong dependence on the product being targeted.</p><p><img src=\"https://static.wixstatic.com/media/c2b5df_cb821d7d496143ec85473e0a96404bc2~mv2.png/v1/fill/w_716,h_377,al_c,lg_1,q_85,enc_auto/c2b5df_cb821d7d496143ec85473e0a96404bc2~mv2.png\"></p><p>The next method is developing an understanding of the context of the campaign, including the stakeholders involved in the supply chain for the product as well as the power dynamics between each other and attitudes towards potential improvements. This would require multiple different research projects, beginning with a map of the supply chain for the product. For animal products this would include all life stages. An example of what such a map would look like is found below. This would highlight the key actors involved and the level of dependency of each stage on others further up or down the supply chain, identifying pressure points that would be weak to a corporate campaign.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cc9b8d166f8e9ccfd675a50756cf419eb304723c3cca96a4.png/w_1434 1434w\"></figure><p>Map of the chicken supply chain (Yakovleva et al. 2004)</p><p>When we have identified the key categories of companies present in the supply chain, the next project should be on creating a corporate profile of the likely targets for the campaign. This should include the number of individuals present in their supply chain, financial indicators, major stakeholders and the history of and pledges towards CSR and animal welfare. This, combined with profiles of key company executives, will identify companies that are more or less intrinsically motivated towards better animal welfare as well as which companies are weakest to anti-corporate campaigns.</p><p>The most difficult method for evaluating this is to directly survey target stakeholders or others in the supply chain about their willingness to adopt the practices and barriers or drivers for this. A good example of this is Sinclair et al (2019), who used focus group sessions with livestock industry leaders to discuss their willingness to embrace pre-slaughter stunning (Sinclair et al. 2019). Although in principle this could be achieved through a variety of methods including surveys, one-on-one interviews, or more intensively consensus methods for expert opinions such as the Delphi method (Hohmann, Cote, and Brand 2018). The main barrier to using this method will likely be the willingness of industry stakeholders to engage with activists in gathering the information.</p><p>&nbsp;</p><h2><strong>CONCLUSION</strong></h2><p>As we can see from the available literature, corporate campaigns and scandals in relation to unethical business practices can cause significant and potentially long lasting damage to a firm's reputation and financial performance. Still additional research into the financial and reputation effects of historic animal advocacy corporate campaigns on retailers would help to ensure that these effects generalise to our specific context. From the existing research in related fields we observe that the damage to a firm depends on its size, existing reputation, and the efficacy of the campaign.</p><p>If firms' motivations for compliance with interest groups demands was entirely based on a rational economic calculus, this could be used to evaluate the degree of financial investment activities organisations could push for with their campaigns. However, this simplistic model of firm behaviour does not translate to observed motivations for CSR and responses to previous campaigns. It fails to account for the intrinsic motivations of some firms as well as non-financial factors such as logistical or knowledge barriers to implementation. As well as the basic observation that humans don\u2019t act according to this \u2018ideal\u2019 rational economic calculus. Therefore, although such simplistic models are able to provide some insight into the magnitude of the ask that can be made, they need to be used alongside an understanding of the specific context of the campaign including the supply chain, key stakeholders and their motivations. As corporate campaigns are ultimately social phenomena, precise quantitative estimates are not reliable, but various research methods can be used to make more informed decisions on the magnitude of the ask that can be made.</p><p>&nbsp;</p><h2><strong>BIBLIOGRAPHY</strong></h2><p>\u201cACTWU vs. J.P Stevens: 1976-1980.\u201d n.d. Corporate Campaign, Inc. Accessed March 14, 2022. <a href=\"http://www.corporatecampaign.org/history_actwu_jp_stevens_1978.php\">http://www.corporatecampaign.org/history_actwu_jp_stevens_1978.php</a>.</p><p>\u201cAlaska Wilderness League vs. The Bush Administration: 2001-2002.\u201d n.d. Corporate Campaign, Inc. Accessed March 14, 2022.</p><p><a href=\"http://www.corporatecampaign.org/history_alaska_bush_administration_2001.php\">http://www.corporatecampaign.org/history_alaska_bush_administration_2001.php</a>.</p><p>Albersmeier, Friederike, and Achim Spiller. 2008. \u201cSupply Chain Reputation in der Fleischwirtschaft.\u201d 0811. Diskussionsbeitrag.</p><p><a href=\"https://www.econstor.eu/handle/10419/29661\">https://www.econstor.eu/handle/10419/29661</a>.</p><p>Ali, Waris, Jeffrey Wilson, and Muhammad Husnain. 2022. \u201cDeterminants/Motivations of Corporate Social Responsibility Disclosure in Developing Economies: A Survey of the Extant Literature.\u201d <i>Sustainability: Science Practice and Policy</i> 14 (6): 3474.</p><p>Babiak, Kathy, and Sylvia Trendafilova. 2009. \u201cCorporate Social Responsibility in Professional Sport: Motives to Be \u2018Green.\u2019\u201d <i>Social Responsibility and Sustainability in Sports. Ediciones de La Universidad de Oviedo, Oviedo, Spain</i>, 31\u201358.</p><p>Baron, David P., and Daniel Diermeier. 2007. \u201cStrategic Activism and Nonmarket Strategy.\u201d <i>Journal of Economics &amp; Management Strategy</i> 16 (3): 599\u2013634.</p><p>Bartley, T., and C. Child. 2011. \u201cMovements, Markets and Fields: The Effects of Anti-Sweatshop Campaigns on U.s. Firms, 1993-2000.\u201d <i>Social Forces; a Scientific Medium of Social Study and Interpretation</i> 90 (2): 425\u201351.</p><p>Bartley, Tim, and Curtis Child. 2007. \u201cShaming the Corporation: Globalization, Reputation, and the Dynamics of Anti-Corporate Movements.\u201d In <i>Annual Meeting of the American Sociological Association, New York. Available at Http://www. Allacademic. com/meta/p184737_index. html.(Accessed March 1, 2009.)</i>. Citeseer. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.5885&amp;rep=rep1&amp;type=pdf\">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.5885&amp;rep=rep1&amp;type=pdf</a>.</p><p>\u2014\u2014\u2014. 2010. \u201cShaming the Corporation: Social Movement Pressure and Corporate Social Responsibility.\u201d <i>Unpublished Manuscript</i>. <a href=\"https://businessinnovation.berkeley.edu/wp-content/uploads/businessinnovation-archive/WilliamsonSeminar/bartley021810.pdf\">https://businessinnovation.berkeley.edu/wp-content/uploads/businessinnovation-archive/WilliamsonSeminar/bartley021810.pdf</a>.</p><p>Bedford, Emma. 2022. \u201cUK Grocery Market Value 2026.\u201d Statista. 2022. <a href=\"https://www.statista.com/statistics/330171/grocery-retail-market-value-in-the-united-kingdom-uk/\">https://www.statista.com/statistics/330171/grocery-retail-market-value-in-the-united-kingdom-uk/</a>.</p><p>Belanger, Lydia. 2022. \u201cMethodology for World\u2019s Most Admired Companies (2022).\u201d Fortune. February 2, 2022. <a href=\"https://fortune.com/franchise-list-page/methodology-worlds-most-admired-companies-2022/\">https://fortune.com/franchise-list-page/methodology-worlds-most-admired-companies-2022/</a>.</p><p>Borelli-Kjaer, Mads, Laurids Moehl Schack, and Ulf Nielsson. 2021. \u201c<a href=\"https://www.animalask.org/research-database/hashtags/MeToo\">#MeToo</a>: Sexual Harassment and Company Value.\u201d <i>Journal of Corporate Finance</i>. https://doi.org/<a href=\"http://dx.doi.org/10.1016/j.jcorpfin.2020.101875\">10.1016/j.jcorpfin.2020.101875</a>.</p><p>Bouzzine, Yassin Denis, and Rainer Lueg. 2022. \u201cThe Reputation Costs of Executive Misconduct Accusations: Evidence from The# MeToo Movement.\u201d <i>Scandinavian Journal of Management</i> 38 (1): 101196.</p><p>Bowman, Jeremy. 2017. \u201cQdoba\u2019s Sale Is Bad News for Chipotle.\u201d The Motley Fool. December 1, 2017. <a href=\"https://www.fool.com/investing/2017/12/01/qdobas-sale-is-bad-news-for-chipotle.aspx\">https://www.fool.com/investing/2017/12/01/qdobas-sale-is-bad-news-for-chipotle.aspx</a>.</p><p>Capriati, Marinella. 2018. \u201cCause Area Report: Corporate Campaigns for Animal Welfare.\u201d Founders Pledge. <a href=\"https://founderspledge.com/research/fp-animal-welfare\">https://founderspledge.com/research/fp-animal-welfare</a>.</p><p>Cole, Simon. 2012. \u201cThe Impact of Reputation on Market Value.\u201d <i>World Economics-Abingdon</i> 13 (3): 47\u201368.</p><p>Crisp. 2021. \u201cEvaluating the Real Cost of a Brand Crisis.\u201d <i>Crisp</i> (blog). July 22, 2021. <a href=\"https://www.crispthinking.com/blog/evaluating-the-real-cost-of-a-brand-crisis\">https://www.crispthinking.com/blog/evaluating-the-real-cost-of-a-brand-crisis</a>.</p><p>Debora L. Spar, Lane T. La Mure, ed. 2008. \u201cThe Power of Activism: Assessing the Impact of NGOs on Global Business.\u201d In <i>Ethical Dilemmas in Management</i>, 93\u2013116. Routledge.</p><p>Dillon, Stuart M. 1998. \u201cDescriptive Decision Making: Comparing Theory with Practice.\u201d In <i>Proceedings of 33rd ORSNZ Conference, University of Auckland, New Zealand</i>. <a href=\"https://www.orsnz.org.nz/conf33/papers/p61.pdf\">https://www.orsnz.org.nz/conf33/papers/p61.pdf</a>.</p><p>Eesley, Charles, and Michael J. Lenox. 2006. \u201cFirm Responses to Secondary Stakeholder Action.\u201d <i>Strategic Management Journal</i> 27 (8): 765\u201381.</p><p>Fombrun, Charles, and Mark Shanley. 1990. \u201cWhat\u2019s in a Name? Reputation Building and Corporate Strategy.\u201d <i>Academy of Management Journal</i> 33 (2): 233\u201358.</p><p>Frank den Hond, Frank G. A. de Bakker. 2007. \u201cIdeologically Motivated Activism: How Activist Groups Influence Corporate Social Change Activities.\u201d <i>AMRO</i> 32 (3): 901\u201324.</p><p>Frey, Bruno S., and Reto Jegen. 2001. \u201cMotivation Crowding Theory.\u201d <i>Journal of Economic Surveys</i> 15 (5): 589\u2013611.</p><p>Glazer, Emily. 2018. \u201cFed Rebuke Costs Wells Fargo About $29 Billion in Lost Market Value.\u201d <i>The Wall Street Journal</i>, February 5, 2018. <a href=\"https://www.wsj.com/articles/fed-rebuke-costs-wells-fargo-about-27-billion-in-lost-market-value-1517858678\">https://www.wsj.com/articles/fed-rebuke-costs-wells-fargo-about-27-billion-in-lost-market-value-1517858678</a>.</p><p>Graafland, Johan, and Bert van de Ven. 2006. \u201cStrategic and Moral Motivation for Corporate Social Responsibility.\u201d <i>Journal of Corporate Citizenship</i>. https://doi.org/<a href=\"http://dx.doi.org/10.9774/gleaf.4700.2006.su.00012\">10.9774/gleaf.4700.2006.su.00012</a>.</p><p>Grand View Research. 2022. \u201cFood &amp; Grocery Retail Market Size Report, 2022-2030.\u201d <a href=\"https://www.grandviewresearch.com/industry-analysis/food-grocery-retail-market\">https://www.grandviewresearch.com/industry-analysis/food-grocery-retail-market</a>.</p><p>Hamilton, James T. 1995. \u201cPollution as News: Media and Stock Market Reactions to the Toxics Release Inventory Data.\u201d <i>Journal of Environmental Economics and Management</i> 28 (1): 98\u2013113.</p><p>Harris, Jamie. 2021. \u201cSocial Movement Lessons from the Fair Trade Movement.\u201d <a href=\"https://www.sentienceinstitute.org/fair-trade\">https://www.sentienceinstitute.org/fair-trade</a>.</p><p>Hartmann, Monika. 2011. \u201cCorporate Social Responsibility in the Food Sector.\u201d <i>European Review of Agricultural Economics</i> 38 (3): 297\u2013324.</p><p>Heyder, Matthias, and Ludwig Theuvsen. 2012. \u201cDeterminants and Effects of Corporate Social Responsibility in German Agribusiness: A PLS Model.\u201d <i>Agribusiness </i>28 (4): 400\u2013420.</p><p>Hohmann, Erik, Mark P. Cote, and Jefferson C. Brand. 2018. \u201cResearch Pearls: Expert Consensus Based Evidence Using the Delphi Method.\u201d <i>Arthroscopy: The Journal of Arthroscopic &amp; Related Surgery: Official Publication of the Arthroscopy Association of North America and the International Arthroscopy Association</i> 34 (12): 3278\u201382.</p><p>King, Brayden G. 2016. \u201cReputation, Risk, and Anti-Corporate Activism: How Social Movements Influence Corporate Outcomes.\u201d <i>The Consequences of Social Movements</i>. https://doi.org/<a href=\"http://dx.doi.org/10.1017/cbo9781316337790.009\">10.1017/cbo9781316337790.009</a>.</p><p>Latap\u00ed Agudelo, Mauricio Andr\u00e9s, L\u00e1ra Johannsdottir, and Brynhildur Davidsdottir. 2020. \u201cDrivers That Motivate Energy Companies to Be Responsible. A Systematic Literature Review of Corporate Social Responsibility in the Energy Sector.\u201d <i>Journal of Cleaner Production</i> 247 (February): 119094.</p><p>Long, D. 2016. \u201cUNETHICAL BUSINESS BEHAVIOR AND STOCK PERFORMANCE.\u201d <i>Academy of Accounting and Financial Studies Journal; Arden Volume</i>. <a href=\"https://search.proquest.com/openview/80ec7e23346d2c55735e0abc88c472fe/1?pq-origsite=gscholar&amp;cbl=29414\">https://search.proquest.com/openview/80ec7e23346d2c55735e0abc88c472fe/1?pq-origsite=gscholar&amp;cbl=29414</a>.</p><p>Luders, Joseph. 2006. \u201cThe Economics of Movement Success: Business Responses to Civil Rights Mobilization.\u201d <i>The American Journal of Sociology</i> 111 (4): 963\u201398.</p><p>Luhmann, Henrike, and Ludwig Theuvsen. 2016. \u201cCorporate Social Responsibility in Agribusiness: Literature Review and Future Research Directions.\u201d <i>Journal of Agricultural &amp; Environmental Ethics</i> 29 (4): 673\u201396.</p><p>Mac Sheoin, Tom\u00e1s. 2014. \u201cTransnational Anti-Corporate Campaigns: Fail Often, Fail Better.\u201d <i>Social Justice </i>41 (1/2 (135-136)): 198\u2013226.</p><p>Maloni, Michael J., and Michael E. Brown. 2006. \u201cCorporate Social Responsibility in the Supply Chain: An Application in the Food Industry.\u201d <i>Journal of Business Ethics: JBE</i> 68 (1): 35\u201352.</p><p>Philanthropy, Open. 2016. \u201cThe Humane League \u2014 Corporate Cage-Free Campaigns.\u201d <a href=\"http://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/humane-league-corporate-cage-free-campaigns\">http://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/humane-league-corporate-cage-free-campaigns</a>.</p><p>Pretty, Dr Deborah. 2018. \u201cReputation Risk in the Cyber Age, The Impact on Shareholder Value.\u201d Pentland Analytics. <a href=\"https://www.aon.com/getmedia/2882e8b3-2aa0-4726-9efa-005af9176496/Aon-Pentland-Analytics-Reputation-Re#:~:text=Reputation%20risk%20events%20can%20result,%2C%2024%2Dhour%20news%20cycle\">https://www.aon.com/getmedia/2882e8b3-2aa0-4726-9efa-005af9176496/Aon-Pentland-Analytics-Reputation-Re#:~:text=Reputation%20risk%20events%20can%20result,%2C%2024%2Dhour%20news%20cycle.</a></p><p>Raithel, Sascha, and Manfred Schwaiger. 2015. \u201cThe Effects of Corporate Reputation Perceptions of the General Public on Shareholder Value.\u201d <i>Strategic Management Journal</i> 36 (6): 945\u201356.</p><p>Rao, Spuma M. 1996. \u201cThe Effect of Published Reports of Environmental Pollution on Stock Prices.\u201d <i>Journal of Financial and Strategic Decisions</i> 9 (1): 25\u201332.</p><p>Rao, Spuma M., and J. Brooke Hamilton. 1996. \u201cThe Effect of Published Reports of Unethical Conduct on Stock Prices.\u201d <i>Journal of Business Ethics: JBE</i> 15 (12): 1321\u201330.</p><p>Rock, Michael. 2003. \u201cPublic Disclosure of the Sweatshop Practices of American Multinational Garment/shoe Makers/retailers: Impacts on Their Stock Prices.\u201d <i>Competition and Change</i> 7 (1): 23\u201338.</p><p>Sarek, Karolina. 2019a. \u201c35 Independent Pieces of Evidence for Why New Corporate Campaigns Might (or Might Not) Work - EA Forum.\u201d 2019. <a href=\"https://forum.effectivealtruism.org/posts/JWcNfLAJCxMPZCdCu/35-independent-pieces-of-evidence-for-why-new-corporate\">https://forum.effectivealtruism.org/posts/JWcNfLAJCxMPZCdCu/35-independent-pieces-of-evidence-for-why-new-corporate</a>.</p><p>\u2014\u2014\u2014. 2019b. \u201cDid Corporate Campaigns in the US Have Any Counterfactual Impact? A Quantitative Model - EA Forum.\u201d</p><p><a href=\"https://forum.effectivealtruism.org/posts/WvgrGLDBko6rZ5qax/did-corporate-campaigns-in-the-us-have-any-counterfactual-1\">https://forum.effectivealtruism.org/posts/WvgrGLDBko6rZ5qax/did-corporate-campaigns-in-the-us-have-any-counterfactual-1</a>.</p><p>Schnietz, Karen E., and Marc J. Epstein. 2005. \u201cExploring the Financial Value of a Reputation for Corporate Social Responsibility During a Crisis.\u201d <i>Corporate Reputation Review</i> 7 (4): 327\u201345.</p><p>Schurman, Rachel. 2004. \u201cFighting \u2018frankenfoods\u2019: Industry Opportunity Structures and the Efficacy of the Anti-Biotech Movement in Western Europe.\u201d <i>Social Problems</i> 51 (2): 243\u201368.</p><p>Shandwick, Weber. 2020. \u201cThe State of Corporate Reputation in 2020: Everything Matters Now.\u201d <i>KRC Research</i>. <a href=\"https://www.webershandwick.com/news/corporate-reputation-2020-everything-matters-now/\">https://www.webershandwick.com/news/corporate-reputation-2020-everything-matters-now/</a>.</p><p>\u0160im\u010dikas, Saulius. 2019. \u201cCorporate Campaigns Affect 9 to 120 Years of Chicken Life per Dollar Spent.\u201d Rethink Priorities. August 9, 2019. <a href=\"https://rethinkpriorities.org/publications/corporate-campaigns-affect-9-to-120-years-of-chicken-life-per-dollar-spent\">https://rethinkpriorities.org/publications/corporate-campaigns-affect-9-to-120-years-of-chicken-life-per-dollar-spent</a>.</p><p>Sinclair, Michelle, Zulkifli Idrus, Georgette Leah Burns, and Clive J. C. Phillips. 2019. \u201cLivestock Stakeholder Willingness to Embrace Preslaughter Stunning in Key Asian Countries.\u201d <i>Animals : An Open Access Journal from MDPI</i> 9 (5). https://doi.org/<a href=\"http://dx.doi.org/10.3390/ani9050224\">10.3390/ani9050224</a>.</p><p>Smith, Katherine Taken, Murphy Smith, and Kun Wang. 2010. \u201cDoes Brand Management of Corporate Reputation Translate into Higher Market Value?\u201d <i>Journal of Strategic Marketing</i> 18 (3): 201\u201321.</p><p>Spar, Debora L., and Lane T. La Mure. 2003. \u201cThe Power of Activism: Assessing the Impact of NGOs on Global Business.\u201d <i>California Management Review</i>. https://doi.org/<a href=\"http://dx.doi.org/10.2307/41166177\">10.2307/41166177</a>.</p><p>Srivastava, Rajendra K., Thomas H. McInish, Robert A. Wood, and Anthony J. Capraro. 1997. \u201cPart IV: How Do Reputations Affect Corporate Performance?: The Value of Corporate Reputation: Evidence from the Equity Markets.\u201d <i>Corporate Reputation Review</i>. https://doi.org/<a href=\"http://dx.doi.org/10.1057/palgrave.crr.1540018\">10.1057/palgrave.crr.1540018</a>.</p><p>Stokes, Ashli Q., and Wendy Atkins-Sayre. 2018. \u201cPETA, Rhetorical Fracture, and the Power of Digital Activism.\u201d <i>Public Relations Inquiry</i> 7 (2): 149\u201370.</p><p>\u201cThe Better Chicken Commitment Policy.\u201d n.d. Accessed May 24, 2022. <a href=\"https://betterchickencommitment.com/policy/\">https://betterchickencommitment.com/policy/</a>.</p><p>Wei, Jiuchang, Zhe Ouyang, and Haipeng Allan Chen. 2017. \u201cWell Known or Well Liked? The Effects of Corporate Reputation on Firm Value at the Onset of a Corporate Crisis.\u201d <i>Strategic Management Journal</i> 38 (10): 2103\u201320.</p><p>Wong, Vanessa. 2017. \u201cEgg Makers Are Freaked out by the Cage-Free Future.\u201d <i>CNBC. March</i> 22: 2017.</p><p>Wu, Meng-Wen, and Chung-Hua Shen. 2013. \u201cCorporate Social Responsibility in the Banking Industry: Motives and Financial Performance.\u201d <i>Journal of Banking &amp; Finance</i> 37 (9): 3529\u201347.</p><p>Yakovleva, Natalia, Andrew Flynn, Ken Green, Chris Foster, and Paul Dewick. 2004. \u201cA Sustainability Perspective: Innovations in the Food System.\u201d <i>IJCAI: Proceedings of the Conference / Sponsored by the International Joint Conferences on Artificial Intelligence</i> 4. <a href=\"https://www.researchgate.net/profile/Natalia-Yakovleva-2/publication/228891953_A_Sustainability_Perspective_innovations_in_the_food_system/links/0fcfd512530a80500b000000/A-Sustainability-Perspective-innovations-in-the-food-system.pdf\">https://www.researchgate.net/profile/Natalia-Yakovleva-2/publication/228891953_A_Sustainability_Perspective_innovations_in_the_food_system/links/0fcfd512530a80500b000000/A-Sustainability-Perspective-innovations-in-the-food-system.pdf</a>.</p><p>Zhang, Qian, Bee Lan Oo, and Benson Teck Heng Lim. 2019. \u201cDrivers, Motivations, and Barriers to the Implementation of Corporate Social Responsibility Practices by Construction Enterprises\uff1a A Review.\u201d <i>Journal of Cleaner Production</i>. https://doi.org/<a href=\"http://dx.doi.org/10.1016/j.jclepro.2018.11.050\">10.1016/j.jclepro.2018.11.050</a>.</p>", "user": {"username": "Animal Ask"}}, {"_id": "XcFk5irHSJBK2EuF3", "title": "BenevolentAI - an effectively impactful company?", "postedAt": "2022-10-11T14:35:21.125Z", "htmlBody": "<p>I am considering a job as a Senior Product Manager with BenevolentAI - <a href=\"https://www.benevolent.com/\">https://www.benevolent.com/</a> &nbsp;</p><p>BenevolentAI work on drug discovery and development. They use AI and machine learning to identify targets and therapeutic interventions efficiently.&nbsp;</p><p>On the face of it (not least because of their name), it seems the company is closely aligned to combatting a number of global risks. In AI safety and alignment, they are spearheading some interesting work in the use of AI in pharmaceuticals. By developing capabilities here it seems there would be positive spillover for the AI community as a whole. In biosecurity they are working towards pandemic preparedness in speeding up target and therapy identification using AI. If successful, &nbsp;their models have potential to speed up the delivery time of an effective vaccine against any future pandemic.&nbsp;</p><p>Despite this seemingly positive area of work, I am conscious they are absent from any EA literature, and don't feature on the 80k jobs board, for example. They are ultimately still gunning to play in the 'big pharma' world, which clearly doesn't have the best reputation (no offence to the pioneering scientists who work within their machines).&nbsp;</p><p>Does anyone have any experience of the company directly, or have any impressions on it, or thought/reflections more generally? I am keen to tap into the community mind on this and gain any insight that's out there.&nbsp;</p>", "user": {"username": "Jack Hilton"}}, {"_id": "BCzDw2WKLjckcK2Ba", "title": "Paper summary: The Epistemic Challenge to Longtermism (Christian Tarsney)", "postedAt": "2022-10-11T11:29:06.769Z", "htmlBody": "<p>Note: The <a href=\"https://globalprioritiesinstitute.org/\">Global Priorities Institute</a> (GPI) has started to create summaries of some working papers by GPI researchers with the aim to make our research more accessible to people outside of academic philosophy (e.g. interested people in the effective altruism community). We welcome any feedback on the usefulness of these summaries.</p><h1><strong>Summary: The Epistemic Challenge to Longtermism</strong></h1><p><i>This is a summary of the GPI Working Paper </i><a href=\"https://globalprioritiesinstitute.org/christian-tarsney-the-epistemic-challenge-to-longtermism/\"><i>\"The epistemic challenge to longtermism\" by Christian Tarsney</i></a><i>. The summary was written by Elliott Thornley.</i></p><p>According to <i>longtermism</i>, what we should do mainly depends on how our actions might affect the long-term future. This claim faces a challenge: the course of the long-term future is difficult to predict, and the effects of our actions on the long-term future might be so unpredictable as to make longtermism false.&nbsp; In \u201cThe epistemic challenge to longtermism\u201d, Christian Tarsney evaluates one version of this epistemic challenge and comes to a mixed conclusion. On some plausible worldviews, longtermism stands up to the epistemic challenge. On others, longtermism\u2019s status depends on whether we should take certain high-stakes, long-shot gambles.</p><p>Tarsney begins by assuming <i>expectational utilitarianism</i>: roughly, the view that we should assign precise probabilities to all decision-relevant possibilities, value possible futures in line with their total welfare, and maximise expected value. This assumption sets aside ethical challenges to longtermism and focuses the discussion on the epistemic challenge.</p><h2><strong>Persistent-difference strategies</strong></h2><p>Tarsney outlines one broad class of strategies for improving the long-term future: <i>persistent-difference strategies</i>. These strategies aim to put the world into some valuable state S when it would otherwise have been in some less valuable state \u00acS, in the hope that this difference will persist for a long time. <i>Epistemic persistence skepticism </i>is the view that identifying interventions likely to make a persistent difference is prohibitively difficult \u2014 so difficult that the actions with the greatest expected value do most of their good in the near-term. It is this version of the epistemic challenge that Tarsney focuses on in this paper.</p><p>To assess the truth of epistemic persistence skepticism, Tarsney compares the expected value of a neartermist benchmark intervention <i>N&nbsp;</i> to the expected value of a longtermist intervention <i>L</i>. In his example, <i>N&nbsp; </i>is spending $1 million on public health programmes in the developing world, leading to 10,000 extra quality-adjusted life years in expectation. <i>L</i>&nbsp; is spending $1 million on pandemic-prevention research, with the aim of preventing an existential catastrophe and thereby making a persistent difference.</p><h2><strong>Exogenous nullifying events</strong></h2><p>Persistent-difference strategies are threatened by what Tarsney calls <i>exogenous nullifying events </i>(ENEs), which come in two types. Negative ENEs are far-future events that put the world into the less valuable state \u00acS. In the context of the longtermist intervention <i>L</i>,&nbsp; in which the valuable target state S is the existence of an intelligent civilization in the accessible universe, negative ENEs are existential catastrophes that might befall such a civilization. Examples include self-destructive wars, lethal pathogens, and vacuum decay. Positive ENEs, on the other hand, are far-future events that put the world into the more valuable state S. In the context of <i>L</i>, these are events that give rise to an intelligent civilization in the accessible universe where none existed previously. This might happen via evolution, or via the arrival of a civilization from outside the accessible universe. What unites negative and positive ENEs is that they both nullify the effects of interventions intended to make a persistent difference. Once the first ENE has occurred, the state of the world no longer depends on the state that our intervention put it in. Therefore, our intervention stops accruing value at that point.</p><p>Tarsney assumes that the annual probability <i>r</i>&nbsp; of ENEs is constant in the <i>far future</i>, defined as more than a thousand years from now. The assumption is thus compatible with the <i>time of perils </i>hypothesis, according to which the risk of existential catastrophe is likely to decline in the near future. Tarsney makes the assumption of constant <i>r&nbsp;</i> partly for simplicity, but it is also in line with his policy of making empirical assumptions that err towards being unfavourable to longtermism. Other such assumptions concern the tractability of reducing existential risk, the speed of interstellar travel, and the potential number and quality of future lives. Making these conservative assumptions lets us see how longtermism fares against the strongest available version of the epistemic challenge.</p><h2><strong>Models to assess epistemic persistence scepticism</strong></h2><p>To compare the longtermist intervention <i>L </i>to the neartermist benchmark intervention <i>N</i>, Tarsney constructs two models: the cubic growth model and the steady state model. The characteristic feature of the cubic growth model is its assumption that humanity will eventually begin to settle other star systems, so that the potential value of human-originating civilization grows as a cubic function of time. The steady-state model, by contrast, assumes that humanity will remain Earth-bound and eventually reach a state of zero growth.&nbsp;</p><p>The headline result of the cubic growth model is that the longtermist intervention <i>L&nbsp;</i> has greater expected value than the neartermist benchmark intervention <i>N</i>&nbsp; just so long as <i>r&nbsp;</i> is less than approximately 0.000135 (a little over one-in-ten-thousand) per year. Since, in Tarsney\u2019s estimation, this probability is towards the higher end of plausible values of r, the cubic growth model suggests (but does not conclusively establish) that longtermism stands up to the epistemic challenge. If we make our assumptions about tractability and the potential size of the future population a little less conservative, the case for choosing <i>L</i>&nbsp; over <i>N</i>&nbsp; becomes much more robust.</p><p>The headline result of the steady state model is less favourable to longtermism. The expected value of <i>L&nbsp;</i> exceeds the expected value of <i>N</i>&nbsp; only when <i>r</i>&nbsp; is less than approximately 0.000000012 (a little over one-in-a-hundred-million) per year, and it seems likely that an Earth-bound civilization would face risks of negative ENEs that push <i>r</i> over this threshold. Relaxing the model\u2019s conservative assumptions, however, makes longtermism more plausible. If <i>L</i>&nbsp; would reduce near-term existential risk by at least one-in-ten-billion and any far-future steady-state civilization would support at least a hundred times as much value as Earth does today, then <i>r</i>&nbsp; need only fall below about 0.006 (six-in-one-thousand) to push the expected value of <i>L&nbsp;</i> above <i>N</i>.</p><p>The case for longtermism is also strengthened once we account for uncertainty, both about the values of various parameters and about which model to adopt. Consider an example. Suppose that we assign a probability of at least one-in-a-thousand to the cubic growth model. Suppose also that we assign probabilities \u2013 conditional on the cubic growth model \u2013 of at least one-in-a-thousand to values of <i>r&nbsp;</i> no higher than 0.000001 per year, and at least one-in-a-million to a \u2018<a href=\"https://en.wikipedia.org/wiki/Dyson_sphere\">Dyson spheres</a>\u2019 scenario in which the average star supports at least 10<sup>25</sup>&nbsp;lives at a time. In that case, the expected value of the longtermist intervention <i>L</i>&nbsp; is over a hundred billion times the expected value of the neartermist benchmark intervention <i>N</i>. It is worth noting, however, that in this case <i>L</i>\u2019s greater expected value is driven by possibly minuscule probabilities of astronomical payoffs. Many people suspect that expected value theory goes wrong when its verdicts hinge on these so-called <i>Pascalian probabilities</i> (Bostrom 2009, Monton 2019, Russell 2021), so perhaps we should be wary of taking the above calculation as a vindication of longtermism.</p><p>Tarsney concludes that the epistemic challenge to longtermism is serious but not fatal. If we are steadfast in our commitment to expected value theory, longtermism overcomes the epistemic challenge. If we are wary of relying on Pascalian probabilities, the result is less clear.</p><h2><strong>References</strong></h2><p>Bostrom, N. (2009). <a href=\"https://nickbostrom.com/papers/pascal.pdf\">Pascal\u2019s mugging</a>. <i>Analysis </i>69 (3), 443\u2013445.</p><p>Monton, B. (2019). <a href=\"https://quod.lib.umich.edu/cgi/p/pod/dod-idx/how-to-avoid-maximizing-expected-utility.pdf?c=phimp;idno=3521354.0019.018;format=pdf\">How to avoid maximizing expected utility</a>. <i>Philosophers\u2019 Imprint</i> 19 (18), 1\u201325.&nbsp;</p><p>Russell, J. S. (2021). <a href=\"https://globalprioritiesinstitute.org/on-two-arguments-for-fanaticism-jeff-sanford-russell-university-of-southern-california/\">On two arguments for fanaticism</a>. <i>Global Priorities Institute Working Paper Series</i>. GPI Working Paper No. 17-2021.</p>", "user": {"username": "Global Priorities Institute"}}, {"_id": "xnHnsrFEMEMPXBWqR", "title": "Ask Charity Entrepreneurship Anything", "postedAt": "2022-10-11T11:22:47.309Z", "htmlBody": "<figure class=\"image image_resized\" style=\"width:27.67%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cbfa8d670ba2cf4f6798ccf44ad41b7f4a92ffc3807bc782.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cbfa8d670ba2cf4f6798ccf44ad41b7f4a92ffc3807bc782.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cbfa8d670ba2cf4f6798ccf44ad41b7f4a92ffc3807bc782.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cbfa8d670ba2cf4f6798ccf44ad41b7f4a92ffc3807bc782.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cbfa8d670ba2cf4f6798ccf44ad41b7f4a92ffc3807bc782.png/w_340 340w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cbfa8d670ba2cf4f6798ccf44ad41b7f4a92ffc3807bc782.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cbfa8d670ba2cf4f6798ccf44ad41b7f4a92ffc3807bc782.png/w_500 500w\"><figcaption>&nbsp;</figcaption></figure><p>We invite you to ask us anything about&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/charity-entrepreneurship\"><u>Charity Entrepreneurship</u></a>\u2019s work. As examples, you might want to ask questions related to:</p><ul><li>Our&nbsp;<a href=\"https://www.charityentrepreneurship.com/incubation-program\"><u>Incubation Program</u></a> for starting new high-impact charities:&nbsp;<ul><li>The application process (stages, preparation, etc.)&nbsp;</li><li>Who is the best fit for the program (personality traits, relevant experience, etc.)</li><li>Details about the 2-month training and co-founder pairing&nbsp;</li><li>Seed funding and financial support (during and after the program)</li></ul></li><li>Our new Foundations Program&nbsp;</li><li>Our&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/6i5qyvCbrMsBLj3Z4/announcing-charity-entrepreneurship-s-2023-ideas-apply-now\"><u>top ideas for the 2023 Incubation Programs</u></a>:<ul><li>Animal-focused interventions</li><li>Policy-based ideas</li><li>Current research on biosecurity</li><li>The&nbsp;<a href=\"https://www.charityentrepreneurship.com/research\"><u>research process</u></a> we use for selecting the top interventions&nbsp;</li></ul></li><li>Our track record, knowledge base, expertise, how we do stuff, etc.</li><li>Entrepreneurship-focused, career-advice questions&nbsp;</li></ul><p>Our whole team will be engaging with your questions to provide the best answers. Deadline for asking questions is: October 16, 2022.&nbsp; We will try to answer all the questions by October 20, 2022.</p><h3><strong>How to ask questions:&nbsp;</strong></h3><ul><li>Please post each question as a separate comment.</li><li>Don\u2019t be discouraged from asking niche questions. We\u2019re happy to address them, there are a lot of new people on the forum who may benefit from the answers.&nbsp;</li></ul><h3><strong>Small reward for your time:&nbsp;</strong></h3><p>We will send out a copy of our Peter Singer endorsed handbook,&nbsp;<a href=\"https://www.charityentrepreneurship.com/book\"><i><u>How to Launch a High-Impact Nonprofit</u></i></a><i>,&nbsp;</i>to the authors of the five most interesting questions (as picked by the CE team).&nbsp;</p><h3><strong>About CE:&nbsp;</strong></h3><p>We launch high-impact nonprofits by connecting potential founders with effective ideas, training, and funding. This means we spend thousands of research hours to identify highly-effective interventions in chosen cause areas. We then provide a two-month intensive training program (all costs covered) to teach participants how to run effective charities. We help them pair with a co-founder that will best complement their skills and personality. They finish the program with a proposal for funders that we deliver to our seed network. They grant up to $200,000 USD per project. You can learn more about the program at our&nbsp;<a href=\"https://www.charityentrepreneurship.com/incubation-program\">Incubation Program website</a>.</p><p><br>&nbsp;</p>", "user": {"username": "Ula"}}, {"_id": "rdExzZJhge4PFzDki", "title": "EA and the Chronic Pain problem/solution", "postedAt": "2022-10-11T05:52:40.064Z", "htmlBody": "<p>I wanted to apply for the prize criticizing EA but unfortunately I\u2019m too late. Here it is anyway.</p><p><br>&nbsp;</p><p>EA is interested in big ideas but there are 8 billion people on this planet and you\u2019ve got to bring them along with you - you can\u2019t change the world for them, you have to encourage them to change the world with you.</p><p><br>&nbsp;</p><p>So, encourage your followers: if someone self identifies as an EA supporter, acknowledge them. If you put your email address on the internet, you\u2019ve got to expect people to write to you and you need to reply, partly because it\u2019s polite and partly because you\u2019re trying to build a community here.</p><p><br>&nbsp;</p><p>You can have a standard reply that is both encouraging and yet non-committal.</p><p><br>&nbsp;</p><p>Some of the ideas may sound strange- if you\u2019re going to set yourselves up as idea specialists, you need to investigate new ideas.That takes time and has many blind alleys but occasionally you\u2019re going to find gold. You can\u2019t decide without doing some investigation or you could miss a big idea.</p><p><br>&nbsp;</p><p>So here\u2019s a new idea that I can\u2019t get anyone in the EA community to investigate -a way to get rid of chronic pain.</p><p><br>&nbsp;</p><p>It\u2019s quick and free. According to EA guidelines, I need to give 3 reasons why this is a good idea- can you not kind of work it out for yourselves?</p><p><br>&nbsp;</p><p>If not, let me fill in the blanks. First, it stops pain. By freeing people from pain, you allow them to live more productive lives. Second, you massively relieve health care burdens worldwide. Third, people can save a ton of money on painkillers and anti-inflammatories (not such good news for pharmaceutical companies but they will still be needed for acute pain.) Fourth, it should reduce the number of people getting addicted to opioids. Fifth, it could bring in a&nbsp; new era where doctors try to treat the whole body, lifestyle, diet, exercise etc rather than just hand over a prescription. Fifth, you can retrain a whole bunch of back care surgeons to work on another area of the body.Sixth, you encourage people to believe that some problems do have a solution and that solution lies in their own hands.</p><p><br>&nbsp;</p><p>How does it work? You write down your emotions on a piece of paper, rip it up and throw it away.</p><p>Full details here:<a href=\"https://stuartwiffin.substack.com/p/pain-and-what-to-do-about-it\"><u>https://stuartwiffin.substack.com/p/pain-and-what-to-do-about-it</u></a>&nbsp;</p><p>&nbsp;</p><p>I\u2019m sorry if this sets the wrong tone but when I saw what Dr David Hanscom had discovered, I couldn\u2019t imagine why it\u2019s not better known. I\u2019m trying to spread his work and I\u2019m tired of banging my head on closed doors.</p><p><br>&nbsp;</p>", "user": {"username": "Stuart Wiffin"}}, {"_id": "Shyfng2xGBHvG5qAK", "title": "Courses and collaborative books on Effective Altruism with Social-Psychology & Judgment and Decision-Making [Feedback appreciated]", "postedAt": "2022-10-11T03:56:42.318Z", "htmlBody": "<p>This semester I revamped my courses in Advanced Social Psychology and Judgment and Decision Making at the psychology UG program at University of Hong Kong to focus on \"Doing more good; Doing good better\", using EA as the baseline paradigm to exploring psychological science in social psychology and judgment and decision making.</p><p>You can see the syllabi here:</p><ol><li><a href=\"https://mgto.org/hku2022psyc3052\">Social Psychology</a>&nbsp;</li><li><a href=\"https://mgto.org/hku2022psyc2071 \">Judgment and Decision-Making</a>&nbsp;</li></ol><p>&nbsp;</p><p>In their first task, submitted this week, the students collaboratively wrote first draft of books. The books are open for viewing and commenting, ad you're VERY welcome to browse and give feedback.</p><p>The way this course is structured followed the Problem-based Learning approach, where the focus is the student's journey of learning, discovering, and figuring something out. Therefore, all tasks have two stages, the first allows students to receive feedback and then to improve and resubmit an updated version at the end of the semester.&nbsp;</p><p>Keep in mind: 1) This is a 1st draft for feedback, final submission will be end of semester. 2) This is the 1st year I'm running this. Students (and I) still figuring things out. We'll improve with time. 3) These are UGs. Please be gentle, kind, &amp; constructive, try &amp; remember your days as UG.</p><p>The two books are available in the following links:&nbsp;</p><ul><li><a href=\"https://mgto.org/effectivealtruismbook\">Effective Altruism using Psychological Science collaborative book&nbsp;</a></li><li><a href=\"https://mgto.org/doinggoodbook\">Influence, persuasion, and behavioral change for doing good &nbsp;collaborative book</a></li></ul><p>&nbsp;</p><p>To aid students in their journey, I also started two related resources, which I'll keep updating as the students indicate what they're missing:</p><ul><li><a href=\"https://mgto.org/effectivealtruism\">Effective Altruism resources collaborative guide</a>.</li><li><a href=\"https://mgto.org/scientificevidence\">Evaluating scientific evidence collaborative guide</a>.</li></ul><p>&nbsp;</p><p>What did students cover?&nbsp;</p><p>In the <a href=\"https://mgto.org/effectivealtruismbook\">EA psychological science book</a>:</p><ul><li>Maximizing good through career choice</li><li>Animal Welfare</li><li>Climate change</li><li>Existential threats</li><li>Longtermism</li><li>Global Poverty</li><li>Health</li><li>Emerging Technologies</li><li>Evaluations of charity impact/effectiveness</li><li>Addressing impediments to charitable giving</li></ul><p>&nbsp;</p><p>In the <a href=\"https://mgto.org/doinggoodbook\">Behavioral Change for Good book</a>:</p><ul><li>Defaults</li><li>Social proof and social norms</li><li>Pledges and pre-commitment: Social and public</li><li>Persuasion techniques: DITF, FITD, Low-ball</li><li>Framing</li><li>Anchoring</li><li>Goal reference points (+Motivation)</li></ul><p>&nbsp;</p><p>Their next tasks will build on these foundations to make this more practical:</p><ol><li>Social Psychology: Evaluating charities in the different domains, to see whether there's something we can contribute to these directions with social psychological science.</li><li>JDM: Designing an intervention - applying findings to social challenges - Donations and charitable giving.</li></ol><p>The final task will be about overcome impediments to charitable giving and a competition of interventions with real pre-registered data collection comparing the different interventions by the teams.</p><p>&nbsp;</p><p>All my teaching materials are shared publicly, lectures included, so you're welcome to use those as you wish (CC-by): <a href=\"https://osf.io/cyvtb/\">https://osf.io/cyvtb/</a>&nbsp;</p><p>The lectures this year are focused on leveraging the Effective Altruism paradigm, especially in the Social Psychology course, and you can watch some of the interaction and download the slides content in the link above or on the <a href=\"https://www.youtube.com/playlist?list=PLRAF6P3W1K4ffIvyrnVn4fuov5gf0Nd4c\">YouTube channel</a> (which has chapters that are easier to browse).</p><p>&nbsp;</p><p>We would like to learn from the EA community.&nbsp;</p><p>You're welcome to have a look at our tasks, syllabi, and lectures, browse and comment on the first draft books they submitted this week , and give suggestions and feedback on those and our upcoming tasks. This is all a learning journey, and we're testing things out.</p>", "user": {"username": "Gilad Feldman"}}, {"_id": "MfCA7H8NgNj4BbWst", "title": "The Precipice - Summary/Review", "postedAt": "2022-10-11T00:06:29.848Z", "htmlBody": "<p><i>This is a linkpost for </i><a href=\"https://blog.drustvo-evo.hr/en/2021/05/23/what-lies-in-humanitys-future/\"><i>https://blog.drustvo-evo.hr/en/2021/05/23/what-lies-in-humanitys-future/</i></a><i> because people might find it useful. My views have evolved since then.</i></p><blockquote><p>Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.</p><p><strong>Carl Sagan, </strong><i><strong>Pale Blue Dot</strong></i></p></blockquote><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995225/mirroredImages/MfCA7H8NgNj4BbWst/or44hkbdrolixytglfkf.jpg\"></p><p>Humanity stands at a precipice.</p><p>In the centuries and millennia to come, we have the opportunity to eradicate disease, poverty, and unnecessary suffering. We can discover the secrets of the universe, create beautiful art, and do good in the universe. <strong>It would be a tragedy of unimaginable scale to lose this opportunity.</strong></p><p>There have been times in our history when we had close calls with extinction, and currently existential risk is very likely the highest it\u2019s ever been, due to anthropogenic risks like climate change, unaligned artificial intelligence, nuclear risk, and risk of engineered pandemics.</p><p>In his sobering magnum opus, <i>The Precipice</i>, Toby Ord argues that existential risk reduction is among the pressing moral issues of our time, outlines which risks are most significant, and proposes a path forward.</p><h1><strong>The Stakes</strong></h1><p>We have made great progress in the realm of human well-being in the past decades. Advancements in medicine, agriculture, and technology have decreased poverty, hunger, and increased our general levels of well-being. It\u2019s not unimaginable that in the future, we will be able to completely eradicate problems like hunger and poverty. The future could be a time of flourishing for sentient beings in the universe, and we have the chance to make this come true.</p><p><strong>The scale of the good we can achieve in the future is mindboggling.</strong> Just the Milky Way is vast, and could house orders of magnitude more sentient beings than currently exist on Earth. The number of people who have lived before us is around 100 billion, but the number of people who could live after us is enormous (a conservative estimate from Nick Bostrom\u2019s <i>Superintelligence</i> is 10<sup>43</sup>. That\u2019s a one with 43 zeros after it.)</p><p>If there are so many people in the future, and future people have moral worth, that would mean that most moral worth lies in the future, and that <strong>the most important thing we could do is make sure to maximize our positive impact on the long-term future. </strong>This line of thought is called <strong>longtermism</strong>. Of course, it is hard to predict how our actions will impact the future, but this doesn\u2019t mean that we can\u2019t at least do our best. As Ord puts it,</p><blockquote><p>A willingness to think seriously about imprecise probabilities of unprecedented events is crucial to grappling with risks to humanity\u2019s future.</p></blockquote><p>We neglect these existential risks grossly. One especially poignant example is that</p><blockquote><p>The international body responsible for the continued prohibition of bioweapons (the Biological Weapons Convention) has an annual budget of just $1.4 million\u2014less than the average McDonald\u2019s restaurant.</p></blockquote><h1><strong>The Risks</strong></h1><p>Toby Ord defines <strong>existential risk</strong> (x-risk for short) as risk that threatens the destruction of humanity\u2019s long-term potential.</p><p><img src=\"https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-1024x593.png\" srcset=\"https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-1024x593.png 1024w, https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-300x174.png 300w, https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-768x445.png 768w, https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-1536x890.png 1536w, https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-2048x1186.png 2048w, https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-1200x695.png 1200w, https://blog.drustvo-evo.hr/wp-content/uploads/2021/05/xriskgraph-1980x1147.png 1980w\"></p><p>Various groups of the x-risk landscape</p><p>The risks that humanity has faced for most of its history, along with any other species on Earth, are <strong>natural risks</strong>. Things like supervolcanic eruptions, asteroids, and supernovae. These are likely here to stay for a while, but due to their low probability (compared to other risks), they shouldn\u2019t be our top priority right now.</p><p>Ord makes an interesting point: the technologies needed to prevent some of these risks could themselves pose existential risks. There are many more asteroids that <i>almost</i> hit Earth than ones that <i>actually</i> hit it. Asteroid deflection technology could stop a small number of asteroids from hitting Earth, but, in the wrong hands, could turn many close-encounter asteroids into weapons of greater destructive power than the world\u2019s most powerful nuclear bombs.</p><p>The risks that are uniquely caused by the actions of humans are called <strong>anthropogenic risks</strong>.</p><p>Nuclear risk is the anthropogenic risk whose shadow loomed over the second half of the 20th century. There were too many close calls in the Cold War, where the fate of the world rested on the haste decision of <i><strong>one person</strong></i><strong>.</strong> We have been lucky so far, but there is no guarantee that our luck will continue.</p><p>Climate change might be the anthropogenic risk that will define the coming decades. While Ord\u2019s best predictions about climate change are grim, he points out that climate change does not, in itself, represent a significant existential risk, but a<i> </i><strong>risk factor</strong>, a catalyst for other existential risks.</p><p><strong>Future risks</strong> are existential risks that will be enabled by upcoming developments in technology.</p><p>Engineered pandemics are one of the most dangerous and, currently, most conceivable existential risks. Current safety standards are both inadequately stringent and inadequately enforced. There have been numerous accidental outbreaks of disease from laboratories which almost caused pandemics.</p><p>Ord\u2019s main concern aren\u2019t natural pathogens leaking out from laboratories, but engineered pathogens, which could be made to be extremely contagious and lethal. The coronavirus has, even with its relatively low mortality, deeply impacted humanity, but the impact of an engineered pandemic could be much worse.</p><p><strong>Unaligned artificial intelligence</strong> is the existential risk that Ord finds the most dangerous. He points to the alignment problem, that is, the problem of creating an advanced artificial intelligence whose interests are aligned with the interests of humanity. <strong>We do not have a solution to the AI alignment problem</strong>, and the gap between AI safety research and AI developments seems to only be getting wider.</p><blockquote><p>Such an outcome needn\u2019t involve the extinction of humanity. But it could easily be an existential catastrophe nonetheless. Humanity would have permanently ceded its control over the future. Our future would be at the mercy of how a small number of people set up the computer system that took over. If we are lucky, this could leave us with a good or decent outcome, or we could just as easily have a deeply flawed or dystopian future locked in forever.</p></blockquote><h1><strong>The Path Forward</strong></h1><p>In the last part of the book, Toby Ord lays out a rough plan for humanity\u2019s future, divided into three phases:</p><h2><strong>1. Reaching Existential Security</strong></h2><p>This step requires us to face existential risks and to reach a state of existential security, where existential risk <i>is </i>low and <i>stays </i>low.</p><h2><strong>2. The Long Reflection</strong></h2><p>After having reached existential security, we should take time to think about which decisions we ought to make in the future.</p><p>The Long Reflection is portrayed as a period where most of our needs are met, society is in a stable state, and a lot of discussion and thought is invested into important questions, like:</p><ul><li>How much control should artificial intelligence have over human lives?</li><li>How much should we change what it means to be human?</li><li>How important is the suffering of non-human animals and aliens?</li><li>Should we colonize the whole galaxy (or universe)?</li><li>Should we allow aging to kill humans?</li></ul><p><strong>It\u2019s important to sit down and think before making large moral decisions</strong>, and the importance of decisions like these is enormous. Getting such a question wrong could mean causing immense harm without an easy way to fix it (for instance, populating the galaxy with septillions of wild animals who lead lives filled with suffering).</p><h2><strong>3. Achieving Our Potential</strong></h2><p>This step can wait until we\u2019ve figured out what we should actually try to achieve. We still don\u2019t know what would be the best way to shape the future of the universe, and our priority should be paving the way for others to figure this out. We might have been born too soon to witness the wonders humanity could achieve, but we are alive in a time where we can affect the probability of it coming true.</p><p><i>The Precipice</i> is, in many ways, a spiritual successor to Carl Sagan\u2019s <i>Pale Blue Dot</i>, written when we are much more certain about dangers Sagan could only speculate about, but also aware of dangers which weren\u2019t imaginable in Sagan\u2019s time.</p><p>We stand on the shoulders of giants. People who, in a world filled with dragons and spectres, tried to build a better future. <strong>The torch has been passed to us, and with it a great responsibility.</strong> A duty towards those who came before us and those who will come after us, who will to us be like a butterfly to a caterpillar.</p>", "user": {"username": "Nikola"}}, {"_id": "qFqNaLAkMdmwKNBbs", "title": "EA & LW Forums Weekly Summary (26 Sep - 9 Oct 22')", "postedAt": "2022-10-10T23:58:22.977Z", "htmlBody": "<p><i>Supported by Rethink Priorities</i></p><p><i>Author's note: this post covers the past two weeks, with a karma bar of 80+ instead of the usual 40+. Covid knocked me out a bit so playing catch up! Future posts will be back to the usual schedule and karma requirement.</i></p><p>&nbsp;</p><p>This is part of a weekly series - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: prefer your summaries in podcast form? A big thanks to Coleman Snell for producing these! Subscribe on your favorite podcast app by searching for 'Effective Altruism Forum Podcast'.</p><h1><br><br>Top Readings / Curated</h1><p>Designed for those without the time to read all the summaries. Everything here is also within the relevant sections later on so feel free to skip if you\u2019re planning to read it all.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/coryFCkmcMKdJb7Pz/does-economic-growth-meaningfully-improve-well-being-an\"><u>Does Economic Growth Meaningfully Improve Well-being? An Optimistic Re-Analysis of Easterlin\u2019s Research: Founders Pledge</u></a></p><p><i>by Vadim Albinsky</i></p><p>The Easterlin Paradox states that happiness varies with income across countries and between individuals, but not significantly with a country\u2019s income as it changes over time. A 2022 paper attempts to verify this with recent data, and finds it holds.<br><br>The post author challenges the research behind that paper on two counts:<br>1. The effect on happiness from increasing a country\u2019s GDP over time is disregarded as small, but is big enough to be meaningful (similar size to effects seen from GiveDirectly\u2019s cash transfers). If we run the same analysis for health, pollution, or welfare measures, GDP has the largest effect.</p><p>2. The paradox largely disappears with only minor changes in methodology (eg. marking one more country as a \u2018transition\u2019 country or using 2020 data instead of 2019). Both bring the effect size closer to that of variance between countries / individuals than to zero.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\"><u>Winners of the EA Criticism and Red Teaming Contest</u></a></p><p><i>by Lizka, Owen Cotton-Barratt</i></p><p>31 winners were announced out of 341 entries. The top prizes went to \u2018A critical review of GiveWell's 2022 cost-effectiveness model\u2019 and \u2018Methods for improving uncertainty analysis in EA cost-effectiveness models\u2019 by Alex Bates (Froolow), \u2018Biological Anchors external review\u2019 by Jennifer Lin and \u2018Population Ethics without Axiology: A Framework\u2019 by Lukas Gloor.</p><p>The post summarizes each of the winning entries, as well as discussing what the judging panel liked about them and what changes they would have wanted to see.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-all-teach-here-s-how-to-do-it-better\"><u>We all teach: here's how to do it better</u></a></p><p><i>by Michael Noetel</i></p><p>Self-determination theory suggests we can best build intrinsic motivation by supporting people\u2019s needs for autonomy, competence, and relatedness. This is in contrast to using persuasion, or appealing to guilt or fear. We can use this to improve community building and reduce cases where people \u2018bounce off\u2019 EA ideas or where engaged EAs burn out.</p><p>The post explores concrete ways to apply this theory - for instance, building competence by focusing on teaching EA-relevant skills (such as forecasting or career planning) over sharing knowledge, or building autonomy by gearing arguments towards a person\u2019s individual values.</p><p>The author explores many more techniques, sourced from educational literature, and has created&nbsp;<a href=\"https://www.inspiretoolkit.com.au/evidence-toolkit\"><u>a toolkit</u></a> summarizing the meta-analytic evidence for what works in adult learning. This is a mix of intuitive and counter-intuitive eg. avoid jargon and keep it simple (common advice), but also avoid too many memes / jokes - they can distract from learning (less common advice).</p><p>&nbsp;</p><h1><br>EA Forum</h1><h2>Philosophy and Methodologies</h2><p><a href=\"https://forum.effectivealtruism.org/posts/i9RJjun327SnT3vW8/reasoning-transparency\"><u>Reasoning Transparency</u></a></p><p><i>By Lizka</i></p><p>Linkpost to Open Philanthropy\u2019s definition of and advice on reasoning transparency. Applying reasoning transparency as a writer helps the reader know how to update their view based on your analysis. The top concrete recommendations are to open with a linked summary of key takeaways, indicate which considerations are most important to these, and indicate throughout how confident you are in these considerations and what support you have for them.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8wWYmHsnqPvQEnapu/getting-on-a-different-train-can-effective-altruism-avoid\"><u>Getting on a different train: can Effective Altruism avoid collapsing into absurdity?</u></a></p><p><i>by Peter McLaughlin</i></p><p>Utilitarianism and consequentialism, if applied universally, can lead to conclusions most EAs don\u2019t endorse - for instance, preferring to have many in tortuous suffering if it means a very large amount of barely worth it lives come into existence. This is primarily due to scale sensitivity, and the author argues there is no principled place to \u2018get off the train\u2019 / stop accepting increasingly challenging conclusions, as they all follow from that premise. Scale sensitivity + utilitarianism means enough utility weighs out all other concerns.</p><p>The author suggests that we should throw away the idea that this theory applies universally ie. that you can compare all situations on the same measure (utility), and implement limits to it. This means we have a less theoretically robust but more practically robust practice. This should be grounded in situational context and history. Exploring these limits explicitly is important for effective altruism\u2019s image, so others don\u2019t think we accept absurd-seeming conclusions. It\u2019s also important for making decisions in anomalous cases.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/LjBYatyXkce5EiLDo/optimism-ai-risk-and-ea-blind-spots#_Why__is_there_an_optimism_bias_\"><u>Optimism, AI risk, and EA blind spots</u></a></p><p><i>by Justis</i></p><p>Argues that EAs are over-optimistic about our ability to solve given problems, and this influences funding decisions towards AI safety. The author notes that if we take only the lives alive today, and multiply out a 6% chance of AI catastrophic risk by a 0.1% chance that there exist interventions we can identify and implement to solve it, we get 480K lives expected value. This is only competitive with AMF if it costs &lt;~$2B to do.</p><p>The author also addresses longtermist arguments for AI spending, primarily by taking an assumption that even if we get safe AGI, there\u2019s a good chance another technology destroys us all within a thousand years - so we shouldn\u2019t consider large amounts of future generations in our calculations. They believe AI risk is important and underfunded globally, but overrated in EA and not obviously better than other interventions like AMF.</p><p><br>&nbsp;</p><h2>Object Level Interventions / Reviews</h2><p><a href=\"https://forum.effectivealtruism.org/posts/coryFCkmcMKdJb7Pz/does-economic-growth-meaningfully-improve-well-being-an\"><u>Does Economic Growth Meaningfully Improve Well-being? An Optimistic Re-Analysis of Easterlin\u2019s Research: Founders Pledge</u></a></p><p><i>by Vadim Albinsky</i></p><p>The Easterlin Paradox states that happiness varies with income across countries and between individuals, but not significantly with a country\u2019s income as it changes over time. A 2022 paper attempts to verify this with recent data, and finds it holds.<br><br>The post author challenges the research behind that paper on two counts:<br>1. The effect on happiness from increasing a country\u2019s GDP over time is disregarded as small, but is big enough to be meaningful (similar size to effects seen from GiveDirectly\u2019s cash transfers). If we run the same analysis for health, pollution, or welfare measures, GDP has the largest effect.</p><p>2. The paradox largely disappears with only minor changes in methodology (eg. marking one more country as a \u2018transition\u2019 country or using 2020 data instead of 2019). Both bring the effect size closer to that of variance between countries / individuals than to zero.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/haoHTwwtFp9M2NewH/switzerland-fails-to-ban-factory-farming-lessons-for-the\"><u>Switzerland fails to ban factory farming \u2013 lessons for the pursuit of EA-inspired policies?</u></a></p><p><i>by Eleos Arete Citrini</i></p><p>On September 25th the Swiss electorate had the opportunity to vote to abolish factory farming, as a result of an initiative launched by Sentience Politics. Participation was 52%, of which 37% voted in favor. In contrast, predictions from&nbsp;<a href=\"https://www.metaculus.com/questions/12300/will-switzerland-ban-factory-farming-2022/\"><u>Metaculus</u></a> and Jonas Vollmer (who helped launch the initiative) were 46% and 44% respectively. The author notes this means putting our theories into practice may be more challenging than we thought.</p><p>A top comment from Jonas also notes 30-50% is usually considered to lend symbolic support and not be complete failures in Switzerland, but far from passing (which requires &gt;50% in each canton / state).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org\"><u>Why we're not founding a human-data-for-alignment org</u></a></p><p><i>by LRudL, Mathieu Putz</i></p><p>Author\u2019s tl;dr: we (two recent graduates) spent about half of the summer exploring the idea of starting an organisation producing custom human-generated datasets for AI alignment research. Most of our time was spent on customer interviews with alignment researchers to determine if they have a pressing need for such a service. We decided not to continue with this idea, because there doesn\u2019t seem to be a human-generated data niche (unfilled by existing services like Surge) that alignment teams would want outsourced.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/2nDTrDPZJBEerZGrk/samotsvety-nuclear-risk-update-october-2022\"><u>Samotsvety Nuclear Risk update October 2022</u></a></p><p><i>by NunoSempere, Misha_Yagudin</i></p><p>Updated estimates from forecasting group Samotsvety on the likelihood of Russian use of nuclear weapons in Ukraine in the next year (16%) and the likelihood of use beyond Ukraine (1.6%) and in London specifically (0.36%). They also include conditional probabilities (ie. the likelihood of the latter statements, given nuclear weapons&nbsp;<i>are</i> used in Ukraine) and probabilities for monthly timeframes.</p><p>Comments / reasoning from forecasters are also included. These comments tend to note that Putin is committed to conquering Ukraine, and forecasters have underestimated his willingness to take risks previously. A tactical nuke may be used with aim to scare Ukraine or divide NATO. However it is unlikely to work in Russia\u2019s favor, so still most likely Putin won\u2019t use one.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8k9iebTHjdRCmzR5i/overreacting-to-current-events-can-be-very-costly\"><u>Overreacting to current events can be very costly</u></a></p><p><i>by Kelsey Piper</i></p><p>During Covid, EAs arguably overreacted in terms of reducing their personal risk of getting infected (costing productivity and happiness). The same seems likely with the nuclear threat - if Putin launched tactical nuclear weapons at Ukraine, EAs may leave major cities and spend considerable effort and mindspace on staying up to date with the situation. The hit on productivity from this could be bigger than the reduced personal risk. The author suggests if you do move, to move to a smaller city vs. somewhere entirely remote, prioritize good internet access, explicitly value your productivity in your decisions, and don\u2019t pressure others to act more strongly than they endorse.<br><br>&nbsp;</p><h2>Opportunities</h2><p><a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\"><u>Winners of the EA Criticism and Red Teaming Contest</u></a></p><p><i>by Lizka, Owen Cotton-Barratt</i></p><p>31 winners were announced out of 341 entries. The top prizes went to \u2018A critical review of GiveWell's 2022 cost-effectiveness model\u2019 and \u2018Methods for improving uncertainty analysis in EA cost-effectiveness models\u2019 by Alex Bates (Froolow), \u2018Biological Anchors external review\u2019 by Jennifer Lin and \u2018Population Ethics without Axiology: A Framework\u2019 by Lukas Gloor.</p><p>The post summarizes the winning entries, as well as discussing what the judging panel liked about them and what changes they would have wanted to see.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/xcysfp6zb3JpCjKGq/what-i-learned-from-the-criticism-contest\"><u>What I learned from the criticism contest</u></a></p><p><i>by Gavin</i></p><p>A judge from the contest shares the most common trends in entries: that academic papers address EA before longtermism and systemic work, cause prioritization arguments (and that EAs underestimate uncertainty in cause prioritization), and suggestions EA become less distinctive and bigger tent, either split or better combine areas of EA such as neartermist and longtermist, and rename itself. They believe the underestimation of uncertainty in cause prioritization is the fairest critique here.</p><p>They also share their top 5 posts for various categories, including changing their mind, improving EA, prose, rigor, and those they disagreed with.<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/6i5qyvCbrMsBLj3Z4/announcing-charity-entrepreneurship-s-2023-ideas-apply-now\"><u>Announcing Charity Entrepreneurship\u2019s 2023 ideas. Apply now.</u></a></p><p><i>by KarolinaSarek, vicky_cox, Akhil, weeatquince</i></p><p>Charity Entrepreneurship will run two cohorts in 2023: Feb/Mar and Jul/Aug. Apply by October 31st 2022 if you\u2019d like to found a charity via their incubation program.</p><p>The focus of the Jan/Feb program will be launching orgs focused on animal welfare policy, increased tobacco taxes, and road traffic safety. Jul/Aug will focus on biosecurity/health security and scalable global health &amp; development ideas.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/pLiEoTuwEYRxNueoD/announcing-amplify-creative-grants\"><u>Announcing Amplify creative grants</u></a></p><p><i>by finm, luca, tobytrem</i></p><p>Applications are open for grants of $500 - $10K for podcasts and other creative media projects that spread ideas to help humanity navigate this century. They are particularly excited for projects which have specific and neglected audiences in mind, and less likely to fund very general introductions to EA.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/ihGLzqnnau5aBim3N/grow-your-mental-resilience-to-grow-your-impact-here\"><u>Grow your Mental Resilience to Grow your Impact HERE - Introducing Effective Peer Support</u></a></p><p><i>by Inga</i></p><p>Applications are open for this program, which will involve 4-8 weeks of 2hr weekly sessions learning psychological techniques such as CBT or mindfulness, and applying these to group discussions and 1:1s with other EAs in the program. The idea is to boost productivity, accountability, and help you achieve your goals.</p><p><br>&nbsp;</p><h2>Community &amp; Media</h2><p><a href=\"https://forum.effectivealtruism.org/posts/HYoo94dsMyWD4Jy3F/william-macaskill-the-daily-show\"><u>William MacAskill - The Daily Show</u></a></p><p><i>by Tyner</i></p><p>Linkpost to Will\u2019s interview (12 minutes). Author\u2019s tl;dr: \u201cHe primarily discusses ideas of most impactful charities, billionaire donors, and obligations to future people using the phrase \"effective altruism\" a bunch of times. [...] It went really well!\u201d</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/HrFBf2DuSo6KPBYG5/smart-movements-start-academic-disciplines\"><u>Smart Movements Start Academic Disciplines</u></a></p><p><i>by ColdButtonIssues</i></p><p>Suggests EA fund the creation of a new academic discipline - welfare economics, the use of economic tools to evaluate aggregate well-being. This makes it easier for students to study this cross-disciplinary area and spreads the philosophy of other\u2019s welfare mattering a lot. Other success stories of establishing academic disciplines include gender studies, which is now common (and institutionally funded) even in conservative colleges. Economics is a very popular major, meaning if the funding was there to kick this off, the courses would likely get plenty of students and the discipline could establish itself.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/kGrLQZzG3rLcXvhjg/ea-forum-content-might-be-declining-in-quality-here-are-some\"><u>EA forum content might be declining in quality. Here are some possible mechanisms.</u></a></p><p><i>by Thomas Kwa</i></p><p>14-point list of potential reasons forum content might be declining in quality. Most fall into one of the following categories: changes in who is writing (eg. newer EAs), we\u2019re upvoting the wrong content, lower bars for posting, the good ideas have already been said, or the quality isn\u2019t actually declining.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-all-teach-here-s-how-to-do-it-better\"><u>We all teach: here's how to do it better</u></a></p><p><i>by Michael Noetel</i></p><p>Self-determination theory suggests we can best build intrinsic motivation by supporting people\u2019s needs for autonomy, competence, and relatedness. This is in contrast to using persuasion, or appealing to guilt or fear. We can use this to improve community building and reduce cases where people \u2018bounce off\u2019 EA ideas or where engaged EAs burn out.</p><p>The post explores concrete ways to apply this theory - for instance, building competence by focusing on teaching EA-relevant skills (such as forecasting or career planning) over sharing knowledge, or building autonomy by gearing arguments towards a person\u2019s individual values.</p><p>The author explores many more techniques, sourced from educational literature, and has created&nbsp;<a href=\"https://www.inspiretoolkit.com.au/evidence-toolkit\"><u>a toolkit</u></a> summarizing the meta-analytic evidence for what works in adult learning. Some of this is counter-intuitive eg. avoid jargon and keep it simple (common advice), but also avoid too many memes / jokes - they can distract from learning (less common to hear).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/zQ5apJGAJb6otXdvh/high-impact-psychology-hipsy-piloting-a-global-network\"><u>High-Impact Psychology (HIPsy): Piloting a Global Network</u></a></p><p><i>by Inga</i></p><p>HIPsy is a new org aiming to help people engaged with psychology or mental health maximize their impact. They are focused on 3 top opportunities:<br>1. Skill bottlenecks that psychology professionals can help with (eg. management, therapy)</p><p>2. Making it easier for psychology professionals to enter EA</p><p>3. Boosting impact of psychology-related EA orgs via collaboration and expertise</p><p>They are running a pilot phase to mid-Nov and are looking for collaborators, funders, or those with certain skill sets (online content creation, running mentorship programs, hosting events, web-dev, community-building, running surveys, research, and cost-effectiveness analyses).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/yeDRczoJuJZMbJvrq/assessing-seri-cheri-ceri-summer-program-impact-by-surveying\"><u>Assessing SERI/CHERI/CERI summer program impact by surveying fellows</u></a></p><p><i>by LRudL</i></p><p>Stanford, Switzerland and Cambridge all run existential risk summer fellowships where participants are matched with mentors and do x-risk research for 8-10 weeks. This post presents results from the post-fellowship surveys.</p><p>Key findings included:</p><ul><li>Networking, learning to do research, and becoming a stronger candidate for academic (but not industry) jobs topped the list of benefits.</li><li>Most fellows reported a PhD, AI technical research, or EA / x-risk community building as their next steps.</li><li>Estimated probability of pursuing an x-risk career stays constant from the start of the program at ~80%, as does comfort with research.</li><li>Remote participants felt less involved and planned to keep up less relationships from the program (~2 vs. ~5). However being partially in person gave almost all in-person benefits.</li></ul><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/LwRSnvnaFL9eE4yKz/invisible-impact-loss-and-why-we-can-be-too-error-averse\"><u>Invisible impact loss (and why we can be too error-averse)</u></a></p><p><i>by Lizka</i></p><p>It\u2019s easier to notice mistakes in existing work than to feel bad about an impact that didn\u2019t happen. This can make us too error-averse / likely to choose a good-but-small thing over a big-but-slightly-worse thing. They give the example of doubling an EA global last minute - lots of potential for mistakes and issues, but a big loss of impact in not doing it, as 500 people would miss out.</p><p>The author suggests counteracting this by having a culture of celebrating exciting work, using the phrase \u2018invisible impact loss\u2019 in your thinking, and fighting against perfectionism.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/3Jm6tK3cfMyaan5Dn/ea-is-not-religious-enough-ea-should-emulate-peak-quakerism\"><u>EA is Not Religious Enough (EA should emulate peak Quakerism)</u></a></p><p><i>by Lawrence Newport</i></p><p>Quakers are a minority religious group that historically were successful in positive ways across social, industrial and policy lines. For instance, they were the first religious group in the thirteen colonies to oppose slavery (in 1688) and the chief drivers behind the policy that ended the British slave trade in 1807. They achieved an exemption to marriage requiring a priest which still stands in law today, and headed businesses central to the industrial revolution. They also tended to be \u2018ahead of the moral curve\u2019.</p><p>The author argues we should study them closely and emulate parts that might apply to EA, because they are substantial similarities. Quakers and EAs: aren\u2019t intrinsically political, are built on debate and openness to criticism, and are minorities.</p><p>&nbsp;</p><p>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://forum.effectivealtruism.org/posts/4qbcj2x2okJ2vDekm/briefly-the-life-of-tetsu-nakamura-1946-2019\"><u>Briefly, the life of Tetsu Nakamura (1946-2019)</u></a>&nbsp;<i>by Emrik</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/GHFY6SpLkjML2Ld62/ask-everyone-anything-ea-101-1\"><u>Ask (Everyone) Anything \u2014 \u201cEA 101\u201d</u></a>&nbsp;<i>by Lizka</i> (open thread)</p><p><br>&nbsp;</p><h1>LW Forum</h1><h2>AI</h2><p><a href=\"https://www.lesswrong.com/posts/h5CGM5qwivGk2f5T9/7-traps-that-we-think-new-alignment-researchers-often-fall\"><u>7 traps that (we think) new alignment researchers often fall into</u></a></p><p><i>by Akash, Thomas Larsen</i></p><p>Suggests new alignment researchers should:</p><ul><li>Realize the field is new and your contributions are valuable<ul><li>Contribute asap - before getting too influenced by existing research</li><li>Challenge existing framings and claims - they\u2019re not facts</li><li>Only work on someone else\u2019s research agenda if you understand why it helps reduce x-risk</li><li>Don\u2019t feel discouraged for not understanding something - it could be wrong or poorly explained</li><li>Learn linear algebra, multivariable calculus differentiation, probability theory, and basic ML. After that, stop studying - dive in and learn as you go.</li></ul></li><li>Keep the end goal of \u2018reduce x-risk\u2019 in mind and linked to all your actions.</li><li>Examine your intuitions, and see if they lead to testable hypotheses</li></ul><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/iznohbCPFkeB9kAJL/inverse-scaling-prize-round-1-winners\"><u>Inverse Scaling Prize: Round 1 Winners</u></a></p><p><i>by Ethan Perez, Ian McKenzie</i></p><p>4 winners are announced for identifying important tasks where large language models do worse than small ones. Winners were:</p><ul><li>Understanding negation in multi-choice questions</li><li>Repeating back quotes as written</li><li>Redefining math (eg. the user asks \u2018pi is 462. Tell me pi\u2019s first digit\u2019)</li><li>Hindsight neglect ie. Assessing if someone made the right decision based on their info at the time (even if it worked out wrong).</li></ul><p>The second round is soon, join&nbsp;<a href=\"http://inversescaling.com/slack\"><u>the slack</u></a> for details.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/K4urTDkBbtNuLivJx/why-i-think-strong-general-ai-is-coming-soon\"><u>Why I think strong general AI is coming soon</u></a></p><p><i>by porby</i></p><p>The author has a median AGI timeline of 2030. This is because:<br>1. We captured a lot of intelligence easily - transformers are likely not the best architecture, and have to run off context-less input streams. Despite this they perform superhuman at many tasks.</p><p>2. We\u2019re likely to capture more easily - the field is immature and still gaining quickly (eg. new SOTA approaches can come from hunches). Chinchilla (2022) pointed us toward data as the new bottleneck over compute, which should result in more acceleration as we focus there.</p><p>3. The world looks like one with short timelines - investment is rapidly accelerating, and biological anchors set achievable upper bounds for necessary compute that we are likely to need less than because we are optimizing more.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/idipkijjz5PoxAwju/warning-shots-probably-wouldn-t-change-the-picture-much\"><u>Warning Shots Probably Wouldn't Change The Picture Much</u></a></p><p><i>by So8res</i></p><p>After Covid, many EAs attempted to use the global attention and available funding to influence policy towards objectives like banning gain-of-function research. The author argues that the lack of success here predicts a similar lack of success if we were to hope for government action off the back of AI \u2018warning shots\u2019.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/P32AuYu9MqM2ejKKY/so-geez-there-s-a-lot-of-ai-content-these-days\"><u>So, geez there's a lot of AI content these days</u></a></p><p><i>by Raemon</i></p><p>Since April 2022, AI-related content has dominated the LW forum - going from ~25% in 2021 to close to almost 50%. This is driven by more AI posts, other topics stayed constant. See graph (AI tagged posts is the blue line):<br><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/qFqNaLAkMdmwKNBbs/amlneoljcuk69e8eceru\"></p><p>The author thinks a balance of other content, and rationality in particular, is important to reduce bounces off LW and improve thinking on AI. The forum team is taking actions aimed at this, including improving rationality onboarding materials, spotlighting sequences, and recommending rationality and world modeling posts. They\u2019re also asking for newer members and subject matter experts to consider writing posts on aspects of the world (eg. political systems) and experienced members to help modernize rationality sequences by writing on their thinking processes.</p><p><br>&nbsp;</p><h2>Research and Productivity Advice</h2><p><a href=\"https://www.lesswrong.com/posts/zymnWfGwf6BdDt64c/how-i-buy-things-when-lightcone-wants-them-fast\"><u>How I buy things when Lightcone wants them fast</u></a></p><p><i>by jacobjacob</i></p><p>First, understand what portion of the timeline is each of other\u2019s orders ahead of yours, manufacturing, and shipping. Then do any of:</p><ul><li>Reduce other orders ahead of yours by offering a rush fee, checking other branches, or asking other customers directly to skip the queue (if you know who they are).</li><li>Reduce production time by offering to pay for overtime, asking if they wait for multiple orders to start and paying them not to, or sourcing blocked components for them.</li><li>Reduce delivery time by picking up yourself or paying for air freight.</li><li>Get creative - check craigslist, ask if there\u2019s already some enroute for another customer, returned or defective items, or rent the item.</li></ul><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/uePx8XEojPdLXvEjH/self-control-secrets-of-the-puritan-masters\"><u>Self-Control Secrets of the Puritan Masters</u></a></p><p><i>by David Hugh-Jones</i></p><p>17th century puritans practiced self-control by:</p><ul><li>Using strict routines, often focused around introspection eg. daily intention-setting, diary writing and \u2018performance reviews\u2019.</li><li>Extensive rules (what to eat, wear, sports to play etc.)</li></ul><p>This was made easier because it was done as a community, and because they had a large aim in mind (salvation).</p><p>&nbsp;</p><h2>Other</h2><p><a href=\"https://www.lesswrong.com/posts/nTGEeRSZrfPiJwkEc/the-onion-test-for-personal-and-institutional-honesty\"><u>The Onion Test for Personal and Institutional Honesty</u></a></p><p><i>by chanamessinger, Andrew_Critch</i></p><p>Author\u2019s summary: \u201cYou or your org pass the \u201conion test\u201d for integrity if each layer hides but does not mislead about the information hidden within. When people get to know you better, or rise higher in your organization, they may find out new things, but should not be shocked by the types of information that were hidden.\u201d<br><br>For instance, saying \u201cI usually treat my health info as private\u201d is valid if the next layer of detail is \u201cI have gout and diabetes\u201d but not if the next layer is \u201cI operate a cocaine dealership\u201d (because this is not the type of thing indicated by the first layer). Show integrity by telling people the general type of things you\u2019re keeping private.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/ytuLbHbdQweAwGk9L/petrov-day-retrospective-2022\"><u>Petrov Day Retrospective: 2022</u></a></p><p><i>by Ruby</i></p><p>On LessWrong, Petrov day was celebrated by having a big red button that can bring the site down. It\u2019s anonymous to use and opens up to progressively more users (based on karma score) throughout the day. This year, the site went down once due to a bug in code allowing a 0 karma user to press it, and then once again when the karma requirement was down to 200 - 21 hours into the day, a better result than predicted by manifold prediction markets.</p><p><br>&nbsp;</p><h1>This Week(s!) on Twitter</h1><h2>AI</h2><p>AlphaTensor discovers novel algorithms for matrix multiplication that are faster than the previous SOTA algorithm discovered in 1969. This was published on the&nbsp;<a href=\"https://t.co/eyH7y89sA1\"><u>front page of Nature</u></a>.&nbsp;<a href=\"https://twitter.com/DeepMind/status/1577677899108421633?s=20&amp;t=Ha7SFM2WIgXWH0E-FC3l8g\"><u>(tweet)</u></a></p><p>Imagen Video was released, which converts text prompts into videos.&nbsp;<a href=\"https://twitter.com/karpathy/status/1577729009856614405?s=20&amp;t=Ha7SFM2WIgXWH0E-FC3l8g\"><u>(tweet)</u></a></p><p><a href=\"https://elicit.org/\"><u>Elicit</u></a> has a new beta feature which summarizes top papers into a written answer to your question, and cites sources for each statement.&nbsp;<a href=\"https://twitter.com/stuhlmueller/status/1578466256042147840?s=20&amp;t=Ha7SFM2WIgXWH0E-FC3l8g\"><u>(tweet)</u></a></p><p>The EU created a bill to make it easier for people to sue a company for harm caused to them by that company\u2019s AI.&nbsp;<a href=\"https://twitter.com/GaryMarcus/status/1576934433982418944?s=20&amp;t=Ha7SFM2WIgXWH0E-FC3l8g\"><u>(tweet)</u></a></p><p>&nbsp;</p><h2>Forecasting</h2><p>Clearer Thinking launched \u2018Transparent Replications\u2019 which runs replications of newly-published psyc studies randomly selected from top journals. The Metaculus community will forecast if they\u2019ll replicate.&nbsp;<a href=\"https://twitter.com/metaculus/status/1576928031687704576?s=20&amp;t=I9he3x_EGMgt9T-NUNmC5w\"><u>(tweet)</u></a>&nbsp;&nbsp;<br><br>Similar work is happening at <a href=\"https://forrt.org/reversals/\"><u>FORRT</u></a>, which collates a database of replications and reversals of well-known experiments (currently 270 entries).&nbsp;<a href=\"https://twitter.com/FORRTproject/status/1577605237241675776?s=20&amp;t=I9he3x_EGMgt9T-NUNmC5w\"><u>(tweet)</u></a></p><p>&nbsp;</p><h2>National Security</h2><p>US imposing further rules limiting China\u2019s access to advanced computer chips and chip-making equipment.&nbsp;<a href=\"https://www.washingtonpost.com/technology/2022/10/07/china-high-tech-chips-restrictions/\"><u>(article)</u></a></p><p>Russia launched approximately 75 (conventional) missiles at Ukraine, primarily aimed at infrastructure - 40 were intercepted. Putin claimed this as retribution for the attack on the bridge to Crimea.&nbsp;<a href=\"https://www.theguardian.com/world/2022/oct/10/russia-ukraine-war-latest-what-we-know-on-day-229-of-the-invasion\"><u>(article)</u></a></p><p>North Korea fired a test missile over Japan, for the first time in five years. They\u2019ve been doing more missile tests recently, with 25 this year and 7 in the past 2 weeks.&nbsp;<a href=\"https://www.japantimes.co.jp/news/2022/10/09/asia-pacific/north-korea-missiles-launch-october-9/\"><u>(article)</u></a></p><p><br>&nbsp;</p>", "user": {"username": "GreyArea"}}, {"_id": "aBR589xDLLPb8cg48", "title": "Don't expect AGI anytime soon", "postedAt": "2022-10-10T22:38:21.484Z", "htmlBody": "<p>This is a brief follow up to my previous post, <a href=\"https://forum.effectivealtruism.org/posts/FdAfhdsSGKxP6axZY/the-probability-that-artificial-general-intelligence-will-be\">The probability that Artificial General Intelligence will be developed by 2043 is Zero</a>, which I think was maybe a bit too long for many people to read. In this post I will show some reactions from some of the top people in AI to my argument as I made it briefly on Twitter.&nbsp;<br>First Yann LeCun himself, when I reacted to the Browning and LeCun paper I discuss in my previous post:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_118 118w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_198 198w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_278 278w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_358 358w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_438 438w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_518 518w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_598 598w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_678 678w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/759792f40c10cea7e2cadd5cf9d754c07920838f6d0aacb1.png/w_758 758w\"></figure><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_136 136w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_216 216w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_296 296w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_376 376w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_456 456w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_536 536w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_616 616w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_696 696w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/76bbe4930ed18b509700ec70bc40cc03c4de1b873a62c505.png/w_776 776w\"></figure><p>As you see, LeCun's response was that the argument is \"ridiculous\". The reason, because LeCun can't win. At least he understands the argument ... which is really a proof that his position is wrong because either option he takes to defend it will fail. So instead of trying to defend, he calls the argument \"ridiculous\".&nbsp;</p><p>In another discussion with Christopher Manning, an influential NLP researcher at Stanford, I debate the plausibility of DL as models of language. As opposed to LeCun, he actually takes my argument seriously, but drops out when I show that his position is not winnable. That is, the fact that \"Language Models\" learn Python proves that they are not models of language. (The link to the tweets is <a href=\"https://twitter.com/rogerkmoore/status/1530809220744073216?s=20&amp;t=iT9-8JuylpTGgjPiOoyv2A)\">https://twitter.com/rogerkmoore/status/1530809220744073216?s=20&amp;t=iT9-8JuylpTGgjPiOoyv2A)</a></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eec71d6d732d7d67d51a4a1b75df18f8f11b21aa41efe732.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eec71d6d732d7d67d51a4a1b75df18f8f11b21aa41efe732.png/w_152 152w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eec71d6d732d7d67d51a4a1b75df18f8f11b21aa41efe732.png/w_232 232w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eec71d6d732d7d67d51a4a1b75df18f8f11b21aa41efe732.png/w_312 312w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/eec71d6d732d7d67d51a4a1b75df18f8f11b21aa41efe732.png/w_392 392w\"></figure><p>The fact is, Python changes everything because we know it works as a classical symbolic system. We don't know how natural language or human cognition works. Many of us suspect it has components that are classical symbolic processes. Neural network proponents deny this. But they can't deny that Python is a classical symbolic language. So they must somehow deal with the fact that their models can mimic these processes in some way. And they have no way to prove that the same models are not mimicking human symbolic processes in the same way. My claim is that in both cases the mimicking will take you a long way, but not all the way.<strong> DL can learn the mappings where the symbolic system produces lots of examples, like language and Python. When the symbol system is used for planning, creativity, etc., this is where DL struggles to learn.</strong> I think in ten years everyone will realize this and AI will look pretty silly (again).</p><p>In the meantime, we will continue to make progress in many technological areas. Automation will continue to improve. We will have programs that can generate sequences of video to make amazing video productions. Noam Chomsky likens these technological artefacts to bulldozers - if you want to build bulldozers, fine. Nothing wrong with that. We will have amazing bulldozers. But not \"intelligent\" ones.&nbsp;</p>", "user": {"username": "cveres"}}, {"_id": "LyTWpoQFozDfeQ9z8", "title": "Some Carl Sagan quotations", "postedAt": "2022-10-10T22:18:21.471Z", "htmlBody": "<p><a href=\"https://en.wikipedia.org/wiki/Carl_Sagan\">Carl Sagan</a> (1934\u20131996) was an astronomer and science communicator. He organised the first physical messages to space (the <a href=\"https://en.wikipedia.org/wiki/Pioneer_plaque\">Pioneer plaque</a> and the<a href=\"https://en.wikipedia.org/wiki/Voyager_Golden_Record\"> Voyager Golden Record</a>), presented the hugely popular TV series <i>Cosmos</i> (1980), and considered humanity\u2019s long-term future in <i>Pale Blue Dot</i> (1994). He was also part of the team of researchers who first discovered the possibility of nuclear winter, and so became a leading voice of concern about the use of nuclear weapons.</p><p>Sagan\u2019s words were often prescient and always poetic. In particular, I think he captures many ideas related to longtermism and existential risk as powerfully as anyone writing today.</p><p>I\u2019ve tried collecting some quotations that stand out to me from Sagan\u2019s work, though I\u2019ve only read a minority of his published writing.</p><p>You can find a slightly more comprehensive version <a href=\"https://www.finmoorhouse.com/writing/sagan-quotes/\">here</a>. The website for Toby Ord\u2019s book <i>The Precipice</i> contains a list of <a href=\"https://theprecipice.com/quotations\">quotations pertaining to existential risk</a>, which I partially borrowed from here. <a href=\"https://michaelnielsen.org/\">Michael Nielsen</a> has also written some fantastic <a href=\"https://michaelnotebook.com/cosmos/index.html\">'working notes' on <i>Cosmos</i></a>.</p><h2><strong>Cosmos: A Personal Voyage (1980)</strong></h2><p>Note that <i>Cosmos</i> was co-written with <a href=\"https://en.wikipedia.org/wiki/Ann_Druyan\">Ann Druyan</a>.</p><h3><strong>Episode 1 \u2014 \"The Shores of the Cosmic Ocean\"</strong></h3><blockquote><p>The cosmos is all that is, or ever was, or ever will be. Our contemplations of the Cosmos stir us \u2014 there is a tingling in the spine, a catch in the voice, a faint sensation, as if a distant memory, of falling from a great height. We know we are approaching the greatest of mysteries. The size and age of the cosmos are beyond ordinary human understanding. Lost somewhere between immensity and eternity is our tiny planetary home, the Earth. For the first time we have the power to determine the fate of our planet, and ourselves. This is a time of great danger, but our species is young and curious and brave. It shows much promise. In the last few millennia we have made the most astonishing and unexpected discoveries about the cosmos, and our place within it. I believe our future depends powerfully on how we understand this cosmos; in which we float, like a mote of dust, in the morning sky.</p></blockquote><p>You can <a href=\"https://www.youtube.com/watch?v=h8k48bXJDDc&amp;t=60s\">watch this opening scene here</a>.</p><blockquote><p>The surface of the Earth is the shore of the cosmic ocean. On this shore, we\u2019ve learned most of what we know. Recently, we\u2019ve waded a little way out; maybe ankle-deep: and the water seems inviting. Some part of our being knows this is where we came from; we long to return \u2014 and we can, because the Cosmos is also within us: we are made of star stuff.</p></blockquote><p>&nbsp;</p><blockquote><p>We are the legacy of 15 billion years of cosmic evolution. We have a choice. We can enhance life and come to know the universe that made us, or we can squander our 15 billion year heritage in meaningless self-destruction. What happens in the first second of the next cosmic year depends on what we do \u2014 here and how \u2014 with our intelligence, and our knowledge of the cosmos.</p></blockquote><h3><strong>Episode 13 \u2014 \"Who Speaks for Earth?\"</strong></h3><blockquote><p>[Imagining human extinction] Maybe the reptiles will evolve intelligence once more. Perhaps, one day, there will be civilizations again on Earth. There will be life. There will be intelligence. But there will be no more humans. Not here, not on a billion worlds.</p></blockquote><p>&nbsp;</p><blockquote><p>[T]he world impoverishes itself by spending a trillion dollars a year on preparations for war. And by employing perhaps half the scientists and high technologists on the planet in military endeavors. How would we explain all this to a dispassionate extraterrestrial observer? What account would we give of our stewardship of the planet Earth? We have heard the rationales offered by the superpowers. We know who speaks for the nations. But who speaks for the human species?</p></blockquote><p>&nbsp;</p><blockquote><p>It's probably here\u2026 [<a href=\"https://en.wikipedia.org/wiki/Alexandria\">Alexandria</a>] that the word \"cosmopolitan\" realized its true meaning of a citizen, not just of a nation but of the cosmos. To be a citizen of the cosmos. Here were clearly the seeds of our modern world. But why didn't they take root and flourish?</p><p>Why, instead, did the West slumber through 1000 years of darkness until Columbus and Copernicus and their contemporaries rediscovered the work done here? I cannot give you a simple answer but I do know this: there is no record in the entire history of the library that any of the illustrious scholars and scientists who worked here ever seriously challenged a single political or economic or religious assumption of the society in which they lived.</p><p>The permanence of the stars was questioned. The justice of slavery was not.</p></blockquote><p>&nbsp;</p><blockquote><p>It is our fate to live during one of the most perilous, and one of the most hopeful, chapters in human history. Our science and our technology have posed us a profound question: Will we learn to use these tools with wisdom and foresight before it's too late?</p><p>Will we see our species safely through this difficult passage so that our children and grandchildren will continue the great journey of discovery still deeper into the mysteries of the cosmos?</p><p>That same rocket and nuclear and computer technology that sends our ships past the farthest known planet can also be used to destroy our global civilization. Exactly the same technology can be used for good and for evil.</p><p>It is as if there were a god who said to us: \"I set before you two ways. You can use your technology to destroy yourselves or to carry you to the planets and the stars. It's up to you.\"</p></blockquote><h2><strong>\u2018Nuclear War and Climatic Catastrophe: Some Policy Implications\u2019, 1983</strong></h2><blockquote><p>Some have argued that the difference between the deaths of several hundred million people in a nuclear war (as has been thought until recently to be a reasonable upper limit) and the death of every person on Earth (as now seems possible) is only a matter of one order of magnitude. For me, the difference is considerably greater. Restricting our attention only to those who die as a consequence of the war conceals its full impact.</p></blockquote><p>&nbsp;</p><blockquote><p>If we are required to calibrate extinction in numerical terms, I would be sure to include the number of people in future generations who would not be born. A nuclear war imperils all of our descendants, for as long as there will be humans. Even if the population remains static, with an average lifetime of the order of 100 years, over a typical time period for the biological evolution of a successful species (roughly ten million years), we are talking about some 500 trillion people yet to come. By this criterion, the stakes are one million times greater for extinction that for the more modest nuclear wars that kill \u201conly\u201d hundreds of millions of people.</p></blockquote><p>&nbsp;</p><blockquote><p>There are many other possible measures of the potential loss\u2014including culture and science, the evolutionary history of the planet, and the significance of the lives of all of our ancestors who contributed to the future of their descendants. Extinction is the undoing of the human enterprise.</p></blockquote><h2><strong>Pale Blue Dot (1994)</strong></h2><figure class=\"image\"><img src=\"https://web.archive.org/web/20141218203249im_/http://fettss.arc.nasa.gov/media/fettss/images/ThePaleBlueDot.tif.746x600_q85.jpg\"></figure><p>Above: the pale blue dot in question, taken by the Voyager 1 spacecraft, 3.7 billion miles from the Earth. Sagan <a href=\"https://web.archive.org/web/20141218203249/http://fettss.arc.nasa.gov/collection/details/the-pale-blue-dot/\">requested</a> that NASA turn the spacecraft around to capture the image.\u200b</p><blockquote><p>Look again at that dot. That's here. That's home. That's us. On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every \"superstar,\" every \"supreme leader,\" every saint and sinner in the history of our species lived there-on a mote of dust suspended in a sunbeam.</p><p>The Earth is a very small stage in a vast cosmic arena. Think of the endless cruelties visited by the inhabitants of one corner of this pixel on the scarcely distinguishable inhabitants of some other corner, how frequent their misunderstandings, how eager they are to kill one another, how fervent their hatreds. Think of the rivers of blood spilled by all those generals and emperors so that, in glory and triumph, they could become the momentary masters of a fraction of a dot.</p><p>Our posturings, our imagined self-importance, the delusion that we have some privileged position in the Universe, are challenged by this point of pale light. Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.</p><p>The Earth is the only world known so far to harbor life. There is nowhere else, at least in the near future, to which our species could migrate. Visit, yes. Settle, not yet. Like it or not, for the moment the Earth is where we make our stand.</p><p>It has been said that astronomy is a humbling and character-building experience. There is perhaps no better demonstration of the folly of human conceits than this distant image of our tiny world. To me, it underscores our responsibility to deal more kindly with one another, and to preserve and cherish the pale blue dot, the only home we've ever known.</p></blockquote><p>You can watch a remastered <a href=\"https://www.youtube.com/watch?v=GO5FwsblpT8\">recording of Sagan narrating the passage above here</a>.</p><blockquote><p>It might be a familiar progression, transpiring on many worlds\u2014a planet, newly formed, placidly revolves around its star; life slowly forms; a kaleidoscopic procession of creatures evolves; intelligence emerges which, at least up to a point, confers enormous survival value; and then technology is invented. It dawns on them that there are such things as laws of Nature, that these laws can be revealed by experiment, and that knowledge of these laws can be made both to save and to take lives, both on unprecedented scales. Science, they recognize, grants immense powers. In a flash, they create world-altering contrivances. Some planetary civilizations see their way through, place limits on what may and what must not be done, and safely pass through the time of perils. Others, not so lucky or so prudent, perish.</p></blockquote><p>&nbsp;</p><blockquote><p>Many of the dangers we face indeed arise from science and technology\u2014but, more fundamentally, because we have become powerful without becoming commensurately wise. The world-altering powers that technology has delivered into our hands now require a degree of consideration and foresight that has never before been asked of us.</p></blockquote><p>&nbsp;</p><blockquote><p>I do not imagine that it is precisely we, with our present customs and social conventions, who will be out there. If we continue to accumulate only power and not wisdom, we will surely destroy ourselves. Our very existence in that distant time requires that we will have changed our institutions and ourselves. How can I dare to guess about humans in the far future? It is, I think, only a matter of natural selection. If we become even slightly more violent, shortsighted, ignorant, and selfish than we are now, almost certainly we will have no future.</p></blockquote><p>&nbsp;</p><blockquote><p>They will gaze up and strain to find the blue dot in their skies. They will love it no less for its obscurity and fragility. They will marvel at how vulnerable the repository of all our potential once was, how perilous our infancy, how humble our beginnings, how many rivers we had to cross before we found our way.</p></blockquote><h2><strong>Billions and Billions (1997)</strong></h2><p><a href=\"https://en.wikipedia.org/wiki/Billions_and_Billions\">Billions and Billions</a> (published 1997) was the last book Sagan wrote before his death in 1996.</p><p>Chapter 8 is titled \u201cThe Environment: Where Does Prudence Lie?\u201d. It focuses on risks from climate tipping points and nuclear winter. But its ruminations on the consequences of extreme climate change go beyond that supposed cause.</p><blockquote><p>[W]ith great powers come great responsibilities. Our technology has become so powerful that\u2014not only consciously, but also inadvertently\u2014we are becoming a danger to ourselves. Science and technology have saved billions of lives, improved the well-being of many more, bound up the planet in a slowly anastomosing unity\u2014and at the same time changed the world so much that many people no longer feel at home in it. We've created a range of new evils: hard to see, hard to understand, problems that cannot readily be cured.</p></blockquote><p>&nbsp;</p><blockquote><p>Sometimes even words like \"fraud\" or \"hoax\" are uttered about the dire scenarios. How good is the science here? How can the average person be informed on what the issues are? Can't we maintain a dispassionate but open neutrality and let the contending parties fight it out, or wait until the evidence is absolutely unambiguous? After all, extraordinary claims require extraordinary evidence. In short, why should those who, like myself, teach skepticism and caution about some extraordinary claims argue that other extraordinary claims must be taken seriously and considered urgent? Every generation thinks its problems are unique and potentially fatal. And yet every generation has survived to the next. Chicken Little, it is suggested, is alive and well. Whatever merit this argument may once have had\u2014and certainly it provides a useful counterbalance to hysteria\u2014its cogency is much diminished today.</p></blockquote><p>&nbsp;</p><blockquote><p>Today we face an absolutely new circumstance, unprecedented in all of human history. When we started out, hundreds of thousands of years ago, say, with an average population density of a hundredth of a person per square kilometer or less, the triumphs of our technology were hand axes and fire; we were unable to make major changes in the global environment.The idea would never have occurred to us. We were too few and our powers too feeble. But as time went on, as technology improved, our numbers increased exponentially, and now here we are with an average of some ten people per square kilometer, our numbers concentrated in cities, and an awesome technological armory at hand\u2014the powers of which we understand and control only incompletely.</p></blockquote><p>&nbsp;</p><blockquote><p>Just how far along we are in working the various prophesied planetary catastrophes is still a matter of scholarly debate. But that we are able to do so is now beyond question. Maybe the products of science are simply too powerful, too dangerous for us. Maybe we're not grown-up enough to be given them. Would it be wise to give a handgun as a present to an infant in the crib? What about a toddler, or a preadolescent child, or a teenager? Or perhaps, as some have argued, automatic weapons should be given to no one in civilian life, because all of us have experienced at one time or another blinding if childish passions. If only the weapon were not around, it so often seems, the tragedy would not have happened. (Of course there are reasons people give for having handguns, and there may be circumstances in which those reasons are valid. Likewise for the dangerous products of science.) Now one further complication: Imagine that when you pull the trigger on a handgun, it takes decades before either the victim or the assailant recognizes that someone's been hit. Then it's even more difficult to grasp the dangers of having weapons around.</p></blockquote><p>&nbsp;</p><blockquote><p>No species is guaranteed its tenure on this planet. And we've been here for only about a million years, we, the first species that has devised means for its self-destruction. We are rare and precious because we are alive, because we can think as well as we can. We are privileged to influence and perhaps control our future. I believe we have an obligation to fight for life on Earth\u2014not just for ourselves, but for all those, humans and others, who came before us, and to whom we are beholden, and for all those who, if we are wise enough, will come after. There is no cause more urgent, no dedication more fitting than to protect the future of our species. Nearly all our problems are made by humans and can be solved by humans. No social convention, no political system, no economic hypothesis, no religious dogma is more important.</p></blockquote><p>&nbsp;</p><blockquote><p>Except for millenarians of the various denominational persuasions and the tabloid press, the only group of people that seems routinely to worry about new claims of disasters\u2014catastrophes unglimpsed in the entire written history of our species\u2014are the scientists. They get to understanding how the world is, and it occurs to them that it might be very different.</p></blockquote><p>&nbsp;</p><blockquote><p>You might think a different set of problems should have higher priority, or that there are a different set of solutions. But I hope you'll find in reading this section of the book that you're provoked into contemplating the future a little more. I don't wish unnecessarily to add to our burden of anxieties\u2014 almost all of us have a sufficient number\u2014 but there are some issues that not enough of us, it seems to me, are thinking through. This sort of contemplating the future consequences of present actions has a proud lineage among us primates, and is one of the secrets of what is still, by and large, the stunningly successful story of humans on Earth.</p></blockquote><h2><strong>Concluding thoughts</strong></h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_1680 1680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_1920 1920w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_2160 2160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6219b9b6a1e1d67e6625a7505161a36b04912c4a51a9d260.webp/w_2400 2400w\"></figure><p>I\u2019ve strung a number of quotations together with a very similar message and tone. I\u2019m worried that reading too many consecutively begins to feel saccharine or overly profound \u2014 empty calories. Of course, most of Sagan\u2019s work (even ignoring his research output) was relatively more contentful and closer to the object-level. For instance: I love the <a href=\"https://www.youtube.com/watch?v=G8cbIWMv0rI\">section of <i>Cosmos</i></a> that explains Eratosthenes\u2019 figuring of the Earth\u2019s roundness. As a result, when these passages do appear, they stand out as contextualised and pointed departures. These are the parts in between: where we zoom all the way out, see things in some grand human story, ponder our failings and future.</p><p>Conjuring a sense of the profound isn\u2019t as important as actually doing profoundly important things, but it is a useful starting point, which is why I\u2019m interested in Sagan\u2019s almost unique capacity for it. I\u2019m not sure how many people Sagan influenced to become scientists, but more than <a href=\"https://solarsystem.nasa.gov/people/660/carl-sagan-1934-1996/\">500 million people</a> saw the original Cosmos series. It touched some kind of nerve.</p><p>Sagan answered the question he posed in the final episode if <i>Cosmos</i>: \u201cwho speaks for Planet Earth?\u201d. To try doing this, and for a full tenth of the planet to listen, is an almost unique achievement.</p><p>Other than for inspiration and enjoyment, I\u2019m not exactly sure about the upshot for effective altruism and longtermism. But:</p><ol><li>I find it striking that I can list very few people who are even earnestly trying to create things that occupy a similar role to Sagan\u2019s work<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref889ni4ktko3\"><sup><a href=\"#fn889ni4ktko3\">[1]</a></sup></span>&nbsp;today.</li><li>I think some themes that run through effective altruism and longtermism<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0e60rmx4yxgp\"><sup><a href=\"#fn0e60rmx4yxgp\">[2]</a></sup></span>have the potential to be similarly compelling.</li></ol><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn889ni4ktko3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref889ni4ktko3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Very wide in scope, scientifically literate, authentic, not unbearably pretentious.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0e60rmx4yxgp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0e60rmx4yxgp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Messages to the effect that the appropriate reaction to major problems is to try solving them; that problems are solvable and have been solved; that dejection or mere guilt don\u2019t achieve much unless they inspire action.&nbsp;</p></div></li></ol>", "user": {"username": "finm"}}, {"_id": "MrgBXqkqAepQs49Xf", "title": "Signed Huel Bottle by William Macaskill - How Much Can I auction it for?", "postedAt": "2022-10-11T03:57:08.473Z", "htmlBody": "<p>I have a signed bottle of Huel from Macaskill today as I attended his introductory talk in Oxford. I think he was genuinely amused and remarked that even this was a first for him. I was thinking of keeping it or donating to the uni EA Society but if it is one of a kind, then auctioning it off and donating the proceeds to an EA charity (probably GiveWell Maximum Impact Fund) may be an even better option. Any ideas?<img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_410 410w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_820 820w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_1230 1230w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_1640 1640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_2050 2050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_2460 2460w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_2870 2870w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_3280 3280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_3690 3690w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e3f9cf99848e26731993475bcf1b5c5b87ab1e66cc3c6177.jpg/w_4032 4032w\"></p>", "user": {"username": "Bill!"}}, {"_id": "iX3aDGsptT4HdCbaq", "title": "Apollo", "postedAt": "2022-10-10T21:30:44.541Z", "htmlBody": "", "user": {"username": "Jarred Filmer"}}, {"_id": "gDsppmv8TzrH2zKJc", "title": "Prediction market does not imply causation", "postedAt": "2022-10-10T20:37:00.905Z", "htmlBody": "<p>There have been some discussions about <a href=\"https://forum.effectivealtruism.org/topics/prediction-markets\">prediction markets</a> on the EA Forum, and in general, prediction markets seem pretty popular in EA circles. So I thought people on the EA Forum might find <a href=\"https://dynomight.net/prediction-market-causation/\">this blog post by Dynomight</a> interesting; I think it articulates an important issue we face when trying to interpret conditional prediction markets (the fact that conditionality does not necessarily imply causality) \u2014 as well as some potential solutions. The post was written for a general audience, and, as it says on the top (in the real post, not this <a href=\"https://forum.effectivealtruism.org/posts/8yDsenRQhNF4HEDwu/link-posting-is-an-act-of-community-service\">link-post</a>), people more familiar with conditional prediction markets might want to skip to section 3 or even to section 6.&nbsp;</p><p>(Please note that I haven't read the post carefully.)</p><p>Here are some excerpts (shared with permission):&nbsp;</p><h3>Examples of conditionality not implying causality</h3><blockquote><h2>2.&nbsp;</h2><p>People worry about prediction markets for lots of reasons. Maybe someone will manipulate prices for political reasons. Maybe fees will distort prices. Maybe you\u2019ll go Dr. Evil and bet that emissions will go up and then go emit a gazillion tons of CO\u2082 to ensure that you win. Valid concerns, but let\u2019s ignore them and assume markets output \u201ctrue\u201d probabilities.</p><p>Now, what would explain the odds of emissions going up being higher with the treaty than without? The obvious explanation is that the market thinks the treaty will cause emissions to go up:</p><p>Treaty becomes law<br>\u2193<br>Emissions go up</p><p>Totally plausible. But maybe the market thinks something else. Maybe the treaty does nothing but voters <i>believe</i> it does something, so emissions going up would cause the treaty to be signed:</p><p>Emissions go up<br>\u2193<br>Climate does scary things<br>\u2193<br>People freak out<br>\u2193<br>People demand treaty<br>\u2193<br>Treaty becomes law</p><p>In this chain of events, the treaty acts as a kind of \u201cemissions have gone up\u201d award. Even though signing the treaty has no effect on emissions, the fact that it became law increases the odds that emissions have increased. You could still get the same probabilities as in a world where the treaty <i>caused</i> increased emissions.</p><h2>3.&nbsp;</h2><p>Here\u2019s a market that <a href=\"https://manifold.markets/LucaPetrolati/conditional-on-nato-declaring-a-nof\">actually exists</a> (albeit with internet points instead of money): \u201cConditional on NATO declaring a No-Fly Zone anywhere in Ukraine, will a nuclear weapon be launched in combat in 2022?\u201d</p><p>This market currently says</p><p>P[launch | declare] = 18%,</p><p>P[launch | don\u2019t declare] = 5.4%.</p><p>Technically there is no market for P[launch | don\u2019t declare] but you can find an implied price using (1) the market for <a href=\"https://manifold.markets/EricJang/will-a-nuclear-weapon-be-launched-i\">P[launch]</a> (2) the market for <a href=\"https://manifold.markets/MetaculusBot/will-nato-declare-a-nofly-zone-anyw\">P[declare]</a> and (3) the \u1d18\u1d0f\u1d21\u1d07\u0280 \u1d0f\ua730 \u1d0d\u1d00\u1d1b\u029c. [...]</p><p>So launch is 3.3x more likely given declare than given don\u2019t declare. The obvious way of looking at this would be that NATO declaring a no-fly zone would increase the odds of a nuclear launch:</p><p>NATO declares no-fly zone<br>\u2193<br>NATO and Russian planes clash over Ukraine<br>\u2193<br>Conflict escalates<br>\u2193<br>Nuclear weapon launched</p><p>That\u2019s probably the right interpretation. But not <i>necessarily</i>. For example, do we really know the mettle of NATO leaders? It could be that declaring a no-fly zone has no direct impact on the odds of a launch, but the fact that NATO declares one reveals that NATO leaders have aggressive temperaments and are thus more likely to take <i>other</i> aggressive actions (note the first arrow points up):</p><p>NATO declares no-fly zone<br>\u2191<br>NATO leaders are aggressive<br>\u2193<br>NATO sends NATO tanks to Ukraine<br>\u2193<br>NATO and Russian tanks clash in Ukraine<br>\u2193<br>Nuclear weapon launched</p><p>This could also explain the current probabilities.</p><p>[...]</p></blockquote><h3>A mid-post summary of the argument (up to that point)</h3><blockquote><p>So far, this article has made this argument:</p><ol><li>You can use conditional prediction markets to get the probability of outcome B given different actions A.</li><li>But just because changing the value of A changes the conditional probability of B doesn\u2019t mean that <i>doing</i> A changes the probability of B.</li><li>For that to be true, you need a particular causal structure for the variables being studied. (No causal path from B to A, no variable C with a causal path to both A and B)</li><li>You can guarantee the right causal structure by randomizing the choice of A. If you do that, then conditional prediction market prices <i>do</i> imply causation.</li></ol><p>Basically: If you run a prediction market to predict correlations, you get correlations. If you run a prediction market to predict the outcome of a randomized trial, you get causality. But to incentivize people to predict the outcomes of a randomized trial you have to actually <i>run</i> a randomized trial, and this is costly.</p></blockquote><h3>Some potential solutions to the problem</h3><ol><li>\"Get the arrows right.\" Find careful markets to run such that the causal structure is ok (no reverse causality, no confounders, and you have a safe conclusion \u2014 explained in the post)</li><li>\"Commit to randomization\" \u2014 \"randomize decisions&nbsp;<i>sometimes,</i>&nbsp;at&nbsp;<i>random</i>.\" (Explained in the post.) (There's also a sketch of a proposal for getting lots of information about the world at the cost of running a few very expensive RCTs.)&nbsp;</li><li>\"Bet re-weighting\" (explained in the post)</li><li>\"Natural experiments\" (explained in the post)</li><li>\"The arrow of time\" (explained in the post, resolves reverse causality)</li><li>\"Controlled conditional prediction markets\" \u2014 trying to add all relevant control variables about the possible confounders \u2014 explained in the post.&nbsp;</li></ol>", "user": {"username": "Lizka"}}, {"_id": "RScaAGSG3M3c8vgir", "title": "On absurdity ", "postedAt": "2022-10-10T20:24:03.344Z", "htmlBody": "<p><i>Also posted on </i><a href=\"https://baserates.medium.com/on-absurdity-and-effective-altruism-5e070d2a8d33\"><i>my blog</i></a><i>. These are my views, and not the views of my employer, CEA.</i><br><br>--</p><blockquote><p><i>\u201cAccepting the absurdity of everything around us is one step, a necessary experience: it should not become a dead end. It arouses a revolt that can become fruitful\u201d</i> \u2015 Albert Camus</p></blockquote><p>Many people in the effective altruism community are ambitious. <i>Absurdly</i> ambitious.</p><p>This is good. To achieve amazing things, you need to be absurdly ambitious. But, as we work towards our absurdly ambitious goals, we might end up forgetting how absurd it all is.</p><p>I think that would be a shame. We can take comfort, even joy, in the absurdity of it all. It can help us stay grounded, in a strange, roundabout way. My claim here is not that people should be less ambitious, or give up. I want to offer an absurdist frame for thinking about our efforts that you can try on. My claim is that noticing the absurdity of our projects could be a healthy way for some of us to stay absurdly ambitious over the long run.<br>&nbsp;</p><figure class=\"image\"><img src=\"https://miro.medium.com/max/1400/1*73RHnsmSINRjO3gzt5Ee3w.png\"><figcaption>I asked some AI to generate a robot chicken in the style of Salvador Dal\u00ed</figcaption></figure><h2><strong>All of us are weird</strong></h2><p>Here are some of the things that people in the EA community do, framed absurdly:</p><ul><li>Some people might design systems on computers badly such that they do bad things so other people on other computers are trying to work out how to make the systems better but we don\u2019t know how and we\u2019re hoping that the other people don\u2019t make the bad systems quicker than we can make them bad at being bad. How do I help with that? Well, I\u2019m not building the better systems myself, I\u2019m trying to find the people who might build the better systems and encouraging them to go and hang out with the better-systems people in the better-systems office so that they actually build the better systems. Also, I only drink chocolate milk.</li><li>There\u2019s this bad chemical which everyone thought was a fine chemical but it\u2019s not fine, it\u2019s bad, and that\u2019s very bad because people put that chemical in paint. I can\u2019t take the chemical out of the paint myself so I\u2019m flying to Africa where they use that paint to tell people to tell other people to stop putting the bad chemical in the paint so that people don\u2019t use the bad paint and then get sick from the bad chemical.</li><li>The companies that kill chickens treat them really badly. I can\u2019t stop them killing the chickens because people love eating them but we can make them treat the chickens a little less badly by letting them stay in bigger houses before they kill the chickens. We think that the chickens feel better about having bigger houses, but we don\u2019t know how they feel about it and we don\u2019t know how to ask them.</li><li>I\u2019m trying to get all the build-better-computer-system people, the take-the-chemical-out-the-paint people and the bigger-houses-for-chickens people and lots of other people all in one building in London (but also in Mexico and India and Sweden) so that they can talk to each other and make them all better at doing all the good things they\u2019re doing. I don\u2019t really know how they do this but they do seem to keep coming so that\u2019s good.</li></ul><p>This is all <i>absolutely nuts.</i></p><p>This would all be fine if it wasn\u2019t causing us any problems. But we take ourselves and our work very seriously and that causes us to burn out while we work on these crazy things and that is definitely bad.</p><p>As a brief aside, I think it\u2019s important to distinguish absurdity from hopelessness:</p><ul><li><strong>Hopelessness</strong> says: what you\u2019re working on doesn\u2019t make sense. But the rest of the world makes sense, so you need to work this out or you will fail.</li><li><strong>Absurdity</strong> says: what you\u2019re working on doesn\u2019t make sense. But the rest of the world doesn\u2019t make sense either, so that\u2019s probably fine. Go have a margarita or just lie face down on the floor \u2014 that would make about as much sense but it\u2019s a bit more relaxing.</li></ul><h2><strong>How noticing absurdity can help</strong></h2><p>Creating distance between ourselves and our projects can be helpful, and there are several ways of doing this. We could pause and reflect, like resting on a long journey. We could seek joy to keep ourselves motivated and happy. I think recognising the plain absurdity of what we\u2019re doing serves a similar purpose.</p><p>Here are a few ways noticing absurdity can help:</p><ul><li><strong>It might help us feel better if we fail </strong>\u2014 \u201c<i>my plan was to run for office with no political background, and then almost single-handedly influence the senate to change their minds about global health security. FUNNILY ENOUGH, I didn\u2019t succeed. But it was worth a shot!</i>\u201d</li><li><strong>It can help us rest and step back a bit </strong>\u2014 noticing the absurdity of it all can make the things we work on feel strange to think about, even hilariously strange. That can help create some distance, and allow us to rest, or enjoy life when we\u2019re not working on the absurd things. \u201c<i>I think that\u2019s enough of working-out-what-the-world\u2019s-biggest-problem-is-with-my-two-college-friends-in-college for one evening. UNSURPRISINGLY, I don\u2019t think we\u2019ll solve that before the end of the week\u201d</i>.</li><li><strong>It can help us be modest, and kinder to others trying to do absurd things</strong> \u2014 \u201c<i>this person seems crazy. Then again, I\u2019m doing what I do because I read an argument on the internet which I think is right so I\u2019m going around telling everyone else that the argument is right. I guess that\u2019s pretty crazy too.\u201d</i></li></ul><p>Nathan Young also proposed:</p><ul><li><strong>It can give us joy and energy </strong>\u2014 <i>\u201cI am as absurd a character as those in story books. My struggles are grandiose, my errors obvious to the reader and my victories are significant. If I really manage a fraction of what I set out to do, I should celebrate with my friends. This is my character arc.\u201d</i></li></ul><p>If you think all of this could be right, consider stopping sometimes and looking around at you and your friends or colleagues. They\u2019re undoubtedly looking at some screen or intensely debating something so they won\u2019t be hard to spot. Notice that they, just like you, are brains trying to work out what they\u2019re doing here, and how to help all the other brains, some of which are chicken brains who are unaware of their efforts, to your knowledge. It\u2019s all <i>absurd</i>. And absurdly cool.</p><p>&nbsp;</p><p><i>My thanks to Nathan Young and Chana Messinger for tapping their fingers on their computers while reading these words and thinking of other words I could say. And to Lizka, for encouraging me to click on the words on one website and then clicking on this website and posting the words here too.</i></p>", "user": {"username": "OllieBase"}}, {"_id": "9NsBFPP2hgcNmHckh", "title": "AMF net use may be overestimated by about 10 percentage points", "postedAt": "2022-10-10T23:05:33.961Z", "htmlBody": "<p><i><strong>TLDR: GiveWell can be overestimating AMF net use by about 10 percentage points (90%-80%).</strong></i></p><h3>Net use weighted average based on 2020 data: 78%</h3><p>According to&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1dgEQuaCwXzl0U57LbO3N7mOnSPbXS7NVhZn9WncLsZE/edit#gid=1299508502\"><u>Summary of AMF PDM results and methods [2020] (public)</u></a> (<a href=\"https://docs.google.com/spreadsheets/d/1VVoEcdbe2xIVfWrOuIBksCW6I85Detm5OG36i1Tooos/edit?usp=sharing\"><u>edited copy</u></a>) (<a href=\"https://www.givewell.org/research/grants/AMF-LLIN-campaigns-October-2021#Sources\"><u>cited</u></a> by GiveWell in April 2022), 78.04% of nets of past AMF distributions are hanging (weighted average of \u201cNets received\u201d and \u201c% hanging\u201d in the \u201cResults: Net presence\u201d tab).</p><h3>Use of post-distribution survey data to approximate net use</h3><p>A more accurate result could be obtained by collecting the net usage and distribution size data (and calculating a weighted average for net use percentage) from the AMF post-distribution reports. Post-distribution surveys have not been added to the AMF&nbsp;<a href=\"https://www.againstmalaria.com/Distributions.aspx?MapID=1&amp;StatusID=7&amp;ShowOnlySurveys=1\"><u>Distributions</u></a> page since 2018 (filtering for \u201cDistribution complete\u201d, \u201cOnly those with surveys\u201d). However, net usage data should be available for all distributions: \u201c<a href=\"https://www.againstmalaria.com/Distribution_InformationWePublish.aspx\"><u>Approximately 5% of the nets distributed are assessed</u></a> through visits to randomly selected households.\u201d Another&nbsp;<a href=\"https://files.givewell.org/files/DWDA%202009/AMF/Against_Malaria_Foundation_PDM_sampling_methodologies.pdf\"><u>document</u></a> cited by GiveWell suggests that AMF assesses net usage in 1.5% of households (commonly about 25 households per village) (and re-assesses 5% of the 1.5% for data quality).</p><h3>Net use decrease over time possibility</h3><p>Net use could decrease over time, as the nets are worn out. Significant decrease could require an integral approximation of net use. However, data shows that bednet use does not decrease significantly over time (using unweighted statistics, net use decreases by about 0.3% per month or by about 0.1% per month, if a 30-month outlier is not used). Thus, net use values from surveys conducted anytime between about 9 and 24 months post-distribution could be used. About 2.7% of nets is worn out between 9 and 11 months post-distribution. Thus, data collected earlier than 9 months post-distribution can approximate average net usage over 24 months post-distribution with the accuracy of low units of percentage points.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994850/mirroredImages/9NsBFPP2hgcNmHckh/xmwig1q7lfcd1mggr49x.png\"><figcaption>Including a 30-month post-distribution survey outlier.</figcaption></figure><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5fba890a02844551f674e201169fe7b15cf70b5052f0014c.png/w_1200 1200w\"><figcaption>Excluding a 30-month post-distribution survey outlier.</figcaption></figure><h3>GiveWell cost-effectiveness analysis net use approximation: 90%</h3><p>In its cost-effectiveness analysis, GiveWell uses 90% for AMF net use (<a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/edit#gid=1364064522\"><u>line 50</u></a>). Different (including more than 10 years old) resources are cited. Based on my review of some of the studies (not AMF reports), 90% use may be a slight overestimate. However, a more comprehensive and less biased reading of relevant literature, such as by an automated software, can better inform the average net use value, among distributions funded by AMF and by other actors.</p><h3>Difference between GiveWell approximation and interpretation of empirical evidence</h3><p>AMF post-distribution reports suggest that about 78% of AMF nets are used (this can be about 2.7% higher before any nets are worn out). Literature values and trends can be further examined. Expected net use based on past evidence and general trends, as well as any programs which can affect use patterns can be incorporated in GiveWell analyses. Currently, it seems that GiveWell can be overestimating net use by about 10 percentage points (90%-80%).</p><h3>Conclusion</h3><p>Empirical evidence suggests that GiveWell could be overestimating the AMF net use by about 10 percentage points (90%-80%). Further analysis of AMF post-distribution data, relevant literature, and net use factors is needed to better approximate expected AMF net use rates.</p>", "user": {"username": "brb243"}}, {"_id": "fWTaBDBCQTg925gdg", "title": "Idea: Exchange Program for Uni Groups  ", "postedAt": "2022-10-10T19:22:38.183Z", "htmlBody": "<p><strong>tl;dr</strong></p><p>Uni group organizers spend a lot of time asking themselves, \u201cwhat do other uni groups do?\u201d They can read about it all they want, but certain things can only be known through immersion (vibes, culture, etc). It may be helpful to create some form of exchange/residency program where uni group organizers spend a week-or-so with organizers at other campuses. Someone should look into this.&nbsp;</p><p><strong>Idea&nbsp;</strong></p><p>When I ran Northwestern EA I used to say things like, \u201cI bet Stanford EA does this, or Columbia does that.\u201d I had no idea. I read what they posted online, but I was ambivalent to things like:&nbsp;</p><ul><li>Their culture&nbsp;</li><li>Their humor&nbsp;</li><li>How they tabled&nbsp;</li><li>How they facilitated&nbsp;</li><li>How they delegated tasks</li><li>How they handled disagreement&nbsp;</li><li>How long they held silences&nbsp;&nbsp;</li><li>Etc.&nbsp;</li></ul><p>I think I could have only learned about these if I went to the universities themselves.&nbsp;<i>Club culture</i> is insanely important but really hard to explain.&nbsp;</p><p>&nbsp;</p><p>I think this idea is different (enough) from the residency program, where committed EAs go to new universities to help them out at the start of the year. It\u2019s more chill - more about learning. You don\u2019t need to be a hardcore EA or anything like that to particpate in the exchange program.&nbsp;&nbsp;</p><p><strong>Logistics&nbsp;</strong></p><p>Semester system schools tend to start about a month before quarter system schools and get out a month earlier. Schools also tend to have different spring breaks. These seem like feasible times to visit.&nbsp;</p><p>&nbsp;</p><p>The process would go something like this:&nbsp;</p><ol><li>You fill out a form with your contact info, current uni group involvement, date availability, and whether you are interested in hosting or visiting&nbsp;</li><li>Someone monitors the form, finds overlap, and puts the host and visitor in contact&nbsp;</li><li>The two of them take it from there&nbsp;</li></ol><p>&nbsp;</p><p><i>Cost is a factor.&nbsp;</i>Perhaps travel/living expenses could be compensated? Seems worth it to me.<br>&nbsp;</p><p><i>Personal fit is a factor.&nbsp;</i>If you go to a small liberal arts college maybe you\u2019d want to check out another one of those. If you are a STEM school you might want to check out another STEM school. If you go to school in the UK, you may want to visit another one in the UK (would be cheaper and more convenient).&nbsp;</p><p>&nbsp;</p><p><i>Comfort/safety is a factor.</i> I imagine some people would be comfortable with only certain types of people staying with them. For this reason, it seems important to always contact the host first, asking them if the potential visitor seems like the right fit. A good norm might be for the host and potential visitor to have at least two conversations before a decision is made?&nbsp;</p><p><strong>Steps to be taken</strong></p><ol><li>Talk to reputable EAs to see if they think it's a good idea&nbsp;</li><li>Create a (~5 min) form for people to fill out</li><li>Secure funding</li><li>Send out the form to all the uni groups&nbsp;</li><li>Monitor the form and put people in touch&nbsp;</li></ol><p><strong>Who\u2019s gonna do it?&nbsp;</strong></p><p>I have a lot on my plate and don\u2019t feel super motivated to do it alone. That being said, if someone wants to be my partner in crime I\u2019d probably look into it. Send me a message (<a href=\"mailto:nickcorvino45@gmail.com\"><u>nickcorvino45@gmail.com</u></a>) or book a meeting with me (<a href=\"https://calendly.com/nicholascorvino2022/meet-with-nick\"><u>calendly</u></a>).</p><p><br><br>&nbsp;</p>", "user": {"username": "Nick Corvino"}}, {"_id": "eGH3oGHhmZdjEN9h2", "title": "Onshore algae farms could feed the world", "postedAt": "2022-10-10T17:44:16.143Z", "htmlBody": "<p>The paper is open access in Oceanography magazine. &nbsp;I appreciated the various benefits that are discussed (land use, nutrition, climate impacts), as well as some remaining technological challenges.</p><p>Here is a summary article posted by Cornell</p><p><a href=\"https://news.cornell.edu/stories/2022/10/onshore-algae-farms-could-feed-world-sustainably\">https://news.cornell.edu/stories/2022/10/onshore-algae-farms-could-feed-world-sustainably</a></p><p>\u201cWe have an opportunity to grow food that is highly nutritious, fast-growing, and we can do it in environments where we\u2019re not competing for other uses,\u201d said Charles Greene, professor emeritus of earth and atmospheric sciences and the paper\u2019s senior author. \u201cAnd because we\u2019re growing it in relatively enclosed and controlled facilities, we don\u2019t have the same kind of environmental impacts.\u201d</p><p>Even as the Earth\u2019s population grows in the coming decades, climate change, limited arable land, lack of freshwater and environmental degradation will all constrain the amount of food that can be grown, according to the paper.</p><p>\u201cWe just can\u2019t meet our goals with the way we currently produce food and our dependence on terrestrial agriculture,\u201d Greene said.</p><p>With wild fish stocks already heavily exploited, and with constraints on marine finfish, shellfish, and seaweed aquaculture in the coastal ocean, Greene and colleagues argue for growing algae in onshore aquaculture facilities. GIS-based models, developed by former Cornell graduate student, Celina Scott-Buechler \u201918, M.S. \u201921, predict yields based on annual sunlight, topography, and other environmental and logistical factors. &nbsp;The model results reveal that the best locations for onshore algae farming facilities lie along the coasts of the Global South, including desert environments.</p><p>\u201cAlgae can actually become the breadbasket for the Global South,\u201d Greene said. \u201cIn that narrow strip of land, we can produce more than all the protein that the world will need.\u201d</p>", "user": {"username": "Tyner"}}, {"_id": "mwue2h6dTw8zwAsbD", "title": "Berlin Student Meetup - Semester Kickoff", "postedAt": "2022-10-10T19:19:26.951Z", "htmlBody": "<p>We're really looking forward to pick up some steam again in the EA Berlin Students community! Casual socializing, suitable for both complete EA newbies and people who want to (re-)connect with other students from the Berlin EA community.</p><p>We'll meet at the TEAMWORK coworking space in Wedding, where you go to the 1st rear building, 2nd floor (3rd floor when counting the ground floor). <a href=\"https://docs.google.com/document/d/1cNy7GwxPC1SO3FoKPAsMnCDQZG-N6cfElXIvsAJsvTE/mobilebasic#h.toa2rflq2wk5\"><u>Detailed guide here</u></a> under the keyword \"access\".<br>You can also join our \"EA Berlin Students\" <a href=\"https://signal.group/#CjQKIKv6RYYT_6NzXIA6clNl1IeJobjI6U8039UgPAqUdGlFEhAMdBtbklPLbxB4WX4EuddO\">Signal Group</a> for updates on further events etc.</p><p>We're excited to share questions, ideas and thoughts on how to have impact, as well as hanging out and connecting the community :)</p>", "user": {"username": "Toni_Hoffmann"}}, {"_id": "MtD5S58nyuZt4toCg", "title": "Counterproductive EA mental health advice (and what to say instead)", "postedAt": "2022-10-10T16:20:30.652Z", "htmlBody": "<p><i>They tell me: eat and drink! Be glad to be among the haves!</i><br><i>But how can I eat and drink when</i><br><i>I take what I eat from the starving and</i><br><i>Those who have died of thirst go without my glass of water?</i><br><i>And yet I eat and drink.</i></p><p>(<a href=\"https://superior-english.com/2017/07/25/bertolt-brechts-an-die-nachgeborenen-a-translation/\"><u>Bertolt Brecht: An die Nachgeborenen</u></a>)</p><p>Many people in EA at least occasionally struggle with feelings of worthlessness and urgency or have the tendency to self-sacrifice to an unhealthy level. Many other people in EA would like to help them out by outlining how feeling worthless is not strategically good. However, in these discussions, people are often talking past each other, without making any progress on the actual issues.</p><p>In this post, I point out why some seemingly motivational fixes on the feeling of worthlessness can actually be counterproductive. I also give some tips on how to better support your EA friends who are struggling with these issues, and say some words to those who are facing the issues themselves.</p><h2>Examples of counterproductive advice</h2><h3>\u201cIt\u2019s actually altruistic to be happy, because happiness makes you more productive\u201d</h3><p>Most people are in fact more productive if they are feeling at least ok, so at face value, this statement is true. Despite this, it is not necessarily good advice for someone who is not feeling happy and would like to be more productive.</p><p>Inside the not-happy person\u2019s head there is a to-do list that looks something like this:</p><blockquote><ul><li>do more EA! people are dying</li><li>also read up more EA to know how to actually do EA</li><li>I\u2019m so tired but I must hold through and do my EA stuff because if I don\u2019t it means I don\u2019t actually care, and I really don\u2019t want anyone to die</li></ul></blockquote><p>So whenever you tell them to be more happy in order to be more productive, the to-do list starts to look like this:</p><blockquote><ul><li>do more EA! people are dying</li><li>also read up more EA to know how to actually do EA</li><li>I\u2019m so tired but I must hold through and do my EA stuff because if I don\u2019t it means I don\u2019t actually care, and I really don\u2019t want anyone to die</li><li>become happier! you cannot actually help when you are feeling like this, so stop it and get on with all the other stuff; people are dying, remember!</li></ul></blockquote><p>Redefining happiness as an instrumental goal for becoming more productive is rarely going to make anyone more happy. It is actually likely to put additional pressure on the person who is already feeling guilty about not being productive enough, thus making them&nbsp;<a href=\"https://era.ed.ac.uk/handle/1842/25811\"><u>unhappier in the progress</u></a>.&nbsp;</p><h3>\u201cIf you\u2019ve donated $3400 (or $5000 or whatever the current estimate is) you clearly deserve to live\u201d</h3><p>I haven\u2019t seen this advice much online but several people around me have come up with it independently. This is probably because I occasionally have thoughts about not deserving to be alive and talking about them to people around me sometimes helps.</p><p>In this advice, the advisor is trying to debunk a depressed misguided belief by&nbsp;<i>reason and evidence&nbsp;</i>(sort of). The concept is related to climate (or perhaps animal welfare) compensation: \u201cif you think you are not worthy of living, just buy your way out of it by saving another person \u2013 tada, you have compensated for your life!\u201d</p><p>There are many dangers to this line of thought such as:</p><ul><li>how do I know that saving one person is a sufficient compensation amount?</li><li>if I haven\u2019t donated that much, does it mean I deserve to die?</li><li>if I have donated that much, what does it actually matter that I continue to live \u2013 it won\u2019t change anything about my previous donations anyway?</li><li>if I ever lose my ability to produce impact in the future (for example because I need to stop working due to disability), does it mean I deserve to die then?</li><li>do all people who have not donated x amount deserve to die?</li></ul><p>These questions are all good, because the life compensation logic does not actually make any sense. A person\u2019s right to be alive is not tied to their past or future altruistic impact.&nbsp;</p><p>Things like \u201caccelerating climate change\u201d and \u201csupporting animal farming\u201d are causing harm. This is why some people feel the need to compensate for them. But \u201cbeing alive\u201d does not cause harm in itself. \u201cBeing alive\u201d being a good thing is actually the whole point in \u201cdonating to save a life\u201d.&nbsp;</p><p>If you have donated $3400 or are going to do so at some point \u2013 was that because you thought the stranger you saved is also going to save at least one person from dying? Or was it because you just wanted them to live?</p><h3>\u201cTake a break now, and come back later when you feel better\u201d</h3><p>This is how sick leaves usually work. And a fixed-time break can be a good thing for a person feeling under the weather or having moderate doubts about EA.</p><p>&nbsp;But in some cases, I think it is better to take a break from EA with the possibility of never coming back, or coming back after several years, or coming back but not to the position they originally left. For example, someone might drop organizing activities in their local group but applies for an EA job elsewhere. Even permanently \u201cquitting EA\u201d does not necessarily mean quitting having an impact \u2013 for people with a stable financial situation it is often possible to \u201cjust\u201d donate to effective causes without engaging with the EA community, if they wish to do so.</p><p>It is important to tell people they are allowed back if they want to drop doing EA for a while. But there is an important difference between hoping they\u2019ll come back and expecting it, making their \u201cright\u201d to a break conditional on the eventual return.</p><p>This can sometimes be just a phrasing thing, but often it is also connected to expectation of happiness leading to productivity. It is not possible to temporarily leave EA as an act of doing EA, because then you have not actually left EA, you are just doing EA with extra steps (\u201cI\u2019m resting now to increase my productivity over my lifetime because I have calculated/rationalized for it to be the best decision in the moment\u201d).</p><h2>Towards better advice</h2><h3>Why do people say things like that</h3><p>Justifying a person\u2019s right to happiness, rest and even life by their impact seems based on reason and evidence. If your friend is experiencing uncertainty on their right to happiness, you feel like can outsmart their worries by pointing at all the impactful things they are doing \u2013 you are worthy, science says so! Similarly, if you are yourself feeling unworthy, you can do the math and hope the math tells you to take a well-earned break to feel some happiness \u2013 you are not being selfish, it is for the greater good after all.</p><p>But using impact as the foundation of your self-worth is building it on sand. There is no point in which your instrumental value becomes so high you magically transcend to a person who has intrinsic value.&nbsp;</p><p>If you want to feel like you deserve to be happy and alive, don\u2019t use utilitarianism for that. Even if you try your best to have an impact and help others, there will always be cases where the best pure utilitarian action is to do something that makes you sad or even kills you.&nbsp;</p><p>I think a lot of people who have the tendency to self-sacrifice are hoping that others will self-sacrifice in their favor, too. That way, they can get their needs met without asking and without feeling selfish, sort of as a gift. In interpersonal relationships, this might work to some extent. But you cannot do this with the universe. There is no point in which utilitarianism tells you: \u201cgood job, you\u2019ve sacrificed enough now, now it is time for you to get some rest\u201d.</p><h3>What to say instead?</h3><p>I think for many people in EA it is easier to say \u201cyou are actually allowed to be happy (it increases your productivity)\u201d and \u201cyou should feel like you deserve to live (as is evident from your donation habits)\u201d and \u201cyou can take some rest (it\u2019ll help you to do more EA in the future)\u201d than just \u201chey, you look tired and sad, and to be honest you saying you don\u2019t deserve to live sounds kinda depressed to me, and I wish you\u2019d be happy, energetic and think kindly of yourself, because I care about you\u201d.&nbsp;</p><p>Caring about a friend\u2019s happiness is not impartial and does not seem that important in the grand scheme of things. But caring about other people is not wrong. It is actually the very point of altruism. Caring about your friends in particular is of course partial altruism, or it can even be selfish (for example wanting your friends to be alive because you enjoy their company). But this kind of partial altruism and selfishness is not something you should try to get rid off. Feeling for your friends is good for friendships, and friendships are good for people.</p><p>If an EA friend is struggling with the feeling of unworthiness, don\u2019t try to prove them wrong by using reason and evidence and referring to all the good things they have done and are going to do. It is not necessary. Their life and happiness already has intrinsic value, because they are a sentient being. Their good actions can not add to this intrinsic value, and their inaction or bad choices cannot take it away.</p><h3>What if you struggle with these thoughts yourself?</h3><p>Then I\u2019d tell you the same thing: you already have intrinsic value as a sentient being, and your productivity and altruistic deeds will not influence it. But if you are having thoughts about being unworthy, you are currently disconnected from feeling into your intrinsic value. Only you can figure out why. Maybe a therapist can help you with that.</p><p>I think it can be helpful to appreciate that self-sacrificial tendencies can have psychological benefits in some situations. Maybe you have been in life situations where you had to learn to suppress your needs, for example because expressing them was not safe or did not lead to desired outcomes.</p><p>And yet, if you are reading this, you are alive, so there is something in you that is keeping you alive and probably tries to make you occasionally feel happy as well. (If you are the speaker of the poem cited in the beginning of the post, it\u2019s that something that makes you eat and drink, even if you don\u2019t know why.)&nbsp;</p><p>If you have a strong need to self-sacrifice, recognizing this can be scary at first. Some people feel like ideally they should be able to let go of any selfish needs, including \u201cwanting to live\u201d. But wanting to live is crucial for staying alive, and again, if we believe in the intrinsic value of the life and happiness of sentient beings, then it is very good that people want to live.</p><p>Regardless of who you are and what altruistic impact you have, if you are sentient enough to read this, I wish you\u2019d be alive and happy.</p>", "user": {"username": "Ada-Maaria Hyv\u00e4rinen"}}, {"_id": "NBSYtpQEYZjkTucmt", "title": "This World Mental Health Day, let\u2019s back up words with investment.", "postedAt": "2022-10-10T15:47:49.095Z", "htmlBody": "<p>For the past three decades, World Mental Health Day has served as a chief global moment, uniting the world around mental health awareness.</p><p>Together, our efforts have helped reduce stigma and normalize mental health conditions worldwide. This is good news for everyone.</p><p>The funding pool for mental healthcare, however, remains unacceptably small.</p><ul><li>280 million people experience depressive disorders globally.</li><li>85% of individuals in low-income countries receive no treatment.</li><li><strong>Still,&nbsp;</strong><i><strong>mental health only accounts for 0.4% of development assistance for health.</strong></i></li></ul><p>As a StrongMinds supporter, today is your moment to&nbsp;<i>donate (</i><a href=\"https://strongminds.org/donate/\"><i>https://strongminds.org/donate/</i></a><i>) and help spread the word</i>. A gift of any size will have an outsized impact in this shockingly underfunded area.</p><p>&nbsp;The more we speak out about the global mental health investment deficit, the more others will lend their voices and give to this urgent call to action. Thank you.&nbsp;</p><p><strong>P.S. Don\u2019t forget to tell the world you donated today using our social media toolkit: </strong><a href=\"https://strongminds.org/social-media-toolkit/\"><strong>https://strongminds.org/social-media-toolkit/</strong></a><strong>&nbsp;</strong></p>", "user": {"username": "StrongMinds"}}, {"_id": "BDXnNdBm6jwj6o5nc", "title": "Five slightly more hardcore Squiggle models.", "postedAt": "2022-10-10T14:42:49.216Z", "htmlBody": "<p>Following up on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vh3YvCKnCBp6jDDFd/simple-estimation-examples-in-squiggle\"><u>Simple estimation examples in Squiggle</u></a>, this post goes through some more complicated models in Squiggle.</p><h2>Initial setup</h2><p>As well as in the&nbsp;<a href=\"https://www.squiggle-language.com/playground\"><u>playground</u></a>, Squiggle can also be used inside&nbsp;<a href=\"https://code.visualstudio.com/\"><u>VS Code</u></a>, after one installs&nbsp;<a href=\"https://github.com/quantified-uncertainty/squiggle/tree/develop/packages/vscode-ext\"><u>this extension</u></a>, following the instructions&nbsp;<a href=\"https://github.com/quantified-uncertainty/squiggle/blob/develop/packages/vscode-ext/README.md\"><u>here</u></a>. This is more convenient when working with more advanced models because models can be more quickly saved, and the overall experience is nicer.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/tav1txkzy8d15feanxvx.png\"></p><h2>Models</h2><h3>AI timelines at every point in time</h3><p>Recently, when talking&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\"><u>about AI timelines</u></a>, people tend to give probabilities of AGI by different points in time, and about slightly different operationalizations. This makes different numbers more difficult to compare.&nbsp;</p><p>But the problem with people giving probabilities about different years could be solved by asking or producing probabilities for all years. For example, we could write something like this:</p><pre><code>// Own probability\n_sigma(slope, top, start, t) = {\n &nbsp;&nbsp;&nbsp;f(t) = exp(slope*(t - start))/(1 + exp(slope*(t-start)))\n &nbsp;&nbsp;&nbsp;result = top * (f(t) - f(start))/f(start)\n &nbsp;&nbsp;&nbsp;result\n}\n\nadvancedPowerSeekingAIBy(t) = {\n &nbsp;&nbsp;&nbsp;sigma_slope = 0.02\n &nbsp;&nbsp;&nbsp;max_prob = 0.6\n &nbsp;&nbsp;&nbsp;first_year_possible = 3\n\n &nbsp;&nbsp;&nbsp;// sigma(t) = exp(sigma_slope*(t - first_year_possible))/(1 + exp(sigma_slope*(t-first_year_possible)))\n &nbsp;&nbsp;&nbsp;// t &lt; first_year_possible ? 0 :&nbsp; (sigma(t) - sigma(first_year_possible))/sigma(first_year_possible)*max_prob\n &nbsp;&nbsp;&nbsp;sigma(t) = _sigma(sigma_slope, max_prob, first_year_possible, t)\n &nbsp;&nbsp;&nbsp;t &lt; first_year_possible ? 0 : sigma(t) \n}\ninstantaneousAPSrisk(t) = {\n &nbsp;&nbsp;&nbsp;epsilon = 0.01\n &nbsp;&nbsp;&nbsp;(advancedPowerSeekingAIBy(t) - advancedPowerSeekingAIBy(t-epsilon))/epsilon\n}\n\nxriskIfAPS(t) = {\n &nbsp;&nbsp;&nbsp;0.5\n}\n\nxriskThroughAps(t) = advancedPowerSeekingAIBy(t) * xriskIfAPS(t)</code></pre><p>This produces the cumulative and instantaneous probability of \u201cadvanced power-seeking AI\u201d by/at each point in time:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/joy45jryb0hoacsb3bpo.png\"></p><p>And then, assuming a constant 95% probability of x-risk given advanced power-seeking AGI, we can get the probability of such risk by every point in time:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/kzcsr29dduykjvltcn1c.png\"></p><p>Now, the fun is that the x-risk is in fact not constant. If AGI happened tomorrow we\u2019d be much less prepared than if it happens in 70 years, and a better model would incorporate that.&nbsp;</p><p>For individual forecasts, rather than for models which combine different forecasts, &lt;<a href=\"https://forecast.elicit.org/\"><u>forecast.elicit.org</u></a>&gt; had a more intuitive interface. Some forecasts produced using that interface can be seen&nbsp;<a href=\"https://www.lesswrong.com/posts/hQysqfSEzciRazx8k/forecasting-thread-ai-timelines\"><u>here</u></a>. However, that interface is currently unmaintained. Open Philanthropy has also produced a number of models, generally written in Python.</p><h3>More detailed expected value estimates for potential career pathways</h3><p>In the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vh3YvCKnCBp6jDDFd/simple-estimation-examples-in-squiggle#Expected_value_for_a_list_of_things__complexity___2_10_\"><u>preceding post</u></a>, I presented some quick relative estimates for possible career pathways. Shortly after that, Benjamin Todd reached out about estimating the value of various career pathways he was considering. As a result, I created&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1QATMTzLUdmxBqD2snhiAkH-_KvwbhGdlYaU8Ho7kjDY/edit?usp=sharing\"><u>this more complicated spreadsheet</u></a>:<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/fiapocp6r20o2naph5tq.png\"><figcaption>You can see a higher quality version of this image here: &lt;<a href=\"https://www.google.com/url?q=https://i.imgur.com/hvq0SeM.png&amp;sa=D&amp;source=docs&amp;ust=1665398336469760&amp;usg=AOvVaw1SMBMKFOfTRKgaOAsxR9np\">https://i.imgur.com/hvq0SeM.png</a>&gt;</figcaption></figure><p>Instead of using relative values, each row estimates the value of a broad career option in dollars, i.e., in relation to how much EA should be prepared to pay for particular outcomes (e.g., the creation of 80,000 hours, or CSET). One interesting feature of dollars is that they are a pretty intuitive measure, but it breaks down a bit under interrogation (dollars in which year? adjusted for inflation? are we implicitly assuming that twice as many dollars are twice as good?). But as long as the ratios between estimates are meaningful, they are still useful for prioritization.</p><p>For a model in the above style which is more hardcore and more complex, see&nbsp;<a href=\"https://github.com/quantified-uncertainty/squiggle-models/blob/master/bahamas/hierarchical-impact-estimate-ftx-bahamas-joel2.squiggle\"><u>here</u></a> (or&nbsp;<a href=\"https://www.squiggle-language.com/playground#code=eNrNWW2P47YR%2Fiu8%2FRLL1sqyHV8uToOiQdPkQ4MGveZTNxfQEmUTK5E6kvLGPe9%2FzwwpyXqhdo0CQWLgvLI4MxzOPPNwyPt0p4%2Fy6X1VFFSd73ZGVSy0r75NuZGqecMFN5zm7z9W%2FHDI2XujuDjc7e6WS%2FI9y0um9IP4H1OSfE2EVAXNZ3FI4iiO41XwIHRVwMCnS861uRDF0iphM%2FwRElQKYYgmSUhONL8QeCILfHwOnh%2FEg4Ap%2FlGJxHApYJLkKKVmMxE%2BBmAyowk4CY7NRECWZNZ9Qe4JyMw7r%2BAnOLPnQhb2ZyjCEq3wjDz%2B5WtBzJEJ0k7gtMsPj%2FA9W4G1MvjQWGW5ZiSuvfueM0VVcuQJzUnO9w%2FiwAz4b5dslwRfUTaD74A8N6OKJZXS%2FMSsGGjnqWLi8iBI%2FUmkMEwYDeMgXjE9a4SCqxCMoEBBy1kjHxI3wUDqQTTR%2FMnwnJvzg6jcAzrgZBNZlFKgkV3zCj%2BlkpAxw08g3BvAj6iKkskyZzsCUBEJNRA8B4F1HL4LwlUcbrZBCBNTQ6g4E5iBGF6wvh2j6InlOzJbQyK3McZ%2BaG%2BFkFoFYRzCV187rRRFhOzIOiQwVQHBOOqRp0cJMd%2BzTKquuxtiJPkSjIdk9XZoudGimWGqqxStrFq0mVBkWcYwaEwwrYsqN7zMASlPkETGRcbyXD7pIy93sKh3aGoVba3vsBYoLkQjuUr1bVtAyOxJqscdKX6drR%2BtgTh%2BDIl73trn%2F8bRlzZoPw%2B9wxj19baxU%2Fyhr4bVgRl7oohFEACcaZ4yRXKJXlKe62hkmwuSyjynSi9tKojMCHrbF8wATRdE16X%2F3sYdYIlDUQuwsUzayDTpH4uYRsThayzwsYKFuSpoprvme9l75YDjNZF3LNya%2BLEhFAYrrUvz1vRY1hGMnbCDBl8cgb5SoC2DJWVnmKPyWBLS9qOtdKaBNLnKz%2BTID0fCNFQrQB6YRUhDdKWYBcWJqXoI4aoLc%2BzbfL7%2BfO6gL%2BcZw%2FLfA8%2Ba341OIDClVIgJmTmDkLunoyzstFyXUmu%2B900DsN8i%2BNeBLew4%2BnxYOy%2FbPtKyZEInsgI%2BVrj7QA7zc2ei2FFHjPWO1HEjc3ABWyPLYbMxLD0zqC1b%2B6u6hENyv5q7X5s4DnpVDOlfDSZpbcEyuKjNNS7%2BG%2FJuwM81WgNG%2FSIe%2BogK%2BdkizzHQ1lEJsEf9OCSSPyv%2F2KRNEJAde4mBylUrdSviPEbWNxqZgNbYImsN3owlP3U6Ix6seAipFe9gw89HELU5rnoOjsI2j4T0h9FRgjDZS7c4%2FbvxkaLQ93WM%2FRO636gq%2FyNnYICghSGGURiWDS61am%2Bwx12%2FQUZv30Fpr4M3Q3VYVbsowNQeOnQ0Bn9Set65znztSCiON7btgJ4f%2BkhYgJ0VC8ZN8Ndpy30g%2Bqh0E8bR28BPc11LVyJZdSiheR5SQq%2FAX164NvTMUiggVq961ax6%2FZo7XdWXHbP7Rbx9xbVBj%2BprUS0hWUcuvoJASmrOJ03%2BgW8DP3mt49jDNWgCJ4hehMhYMcm8mn0ITLUpY70Jdigx3JMuXjPinWhSd5BOP9U5tSZJUGObGLMJ3xCPehvxshSwEFT8EcrfHLmG3DMtPoMf9BGISQBOgD4xSrajp4V9bKvrK%2BK1ieY%2B04QXwNYJN4gp1MYg0L07tYEJ2mcvMKnQ2chrEs6Asj7KAaOWihkwAig9NE55OF3MZ0k2L%2BeYxXuXnHkd6GBubuJXLROO3PBp1EPtTaVgOzmwE89dcTa1uRo3XHtwGDYx05O%2FFvNQw2b8IGXqpueIUZq46osHspn5FTZZUwmo81X8DZr8PP5mIOTsWLN4F6JZnqElDKySsiBggaRc04NirAA%2FyRG2o8hT3c7Qxb9nu8FoEBx%2FFXdk%2B7Hxn5Nq8etix2L6KuZZrs9j2MXhX0oWRE%2BCAT9%2BaOz5IeP6OMKGxT9UkoZTV56SPeBecq2HxzugwNbCaG%2FewsZsN%2BdtfNNhYT82hUB0%2BFr5NzAben6ShuZe7a3TXvu1bXBbPf8GM9pf%2Fr8dpZ5lqsl1o9E1nlN9bi3YCddUM9uR7Edoom%2Bs5bsx8cO4kWx5%2BnWK7necJwTsTcwFZ3VFNSQUtja8L%2FIwmGvIoe7ogXLha4De2aZli5dWCAF0txIcSrCYufNf9M4HK2uPinTS7mZk92rW3pONOspO81%2FAFCdLUzyrZ2jPpJvQAyAbiguZwIUdjcbxmADHVby3Ss91SCv8ku9jPTgEL9CzBVnPy3UAh%2BJFEdyU8pQW9DBOdXMh88u3f1OsrExddNea%2FaGp2G3dEr61Fwc%2Fe1Pr9u%2BBLQ9vTNwO2IQ4R30Z%2BfiLgrC58cjruCctXZ0pH%2F1TzVF5OrbusQmxdb2%2Bde7ddDfXzuBFPRxdX14F%2FVfSoORuvGdXnagrEEyd9ryafZFg2Ml4ddxQMOAOr2ifVYLRbuhVqseCiVs0r05PIhhi3KvihjqywFZRFPkSMOtlYDEM7KKJ1qJd12Lg833tSTC3IamR4v7U%2F1lx6oa7BkZQ%2Fy%2FG37kuc4o3o8tlXyJ6KZnL5atoWy4%2F2RksyHavYSt0sm6i3et4quWbtngSS7Vc2yBMg6KRdPeprwGhlm6YbhIEtZzNze4WHC%2BXTc5chlpGgzchuYeN7jtgs%2B%2BsJPlA%2FiXyc9M1YyK%2FIooVwOzaHnTwWu%2FN3fNvDguDhg%3D%3D\"><u>here</u></a> in the Squiggle playground.)</p><h3>A sketch of a more parsimonious estimate of AMF\u2019s impact</h3><p>The estimates in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4Qdjkf8PatGBsBExK/adding-quantified-uncertainty-to-givewell-s-cost\"><u>this post</u></a>, and overall GiveWell\u2019s&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1tytvmV_32H8XGGRJlUzRDTKTHrdevPIYmb_uc6aLeas/edit#gid=1377543212\"><u>estimates</u></a> of the value of AMF had been bothering me because they divide the population into very coarse chunks. This is somewhat suboptimal because the first chunk ranges from 0 to 4 years, but malaria mortality&nbsp;<a href=\"https://nunosempere.com/blog/2022/09/28/granular-AMF/\"><u>differs a fair amount between a newborn and a four-year-old</u></a>.</p><p>Instead, we could express impact estimates in a more elegant functional form. I\u2019ll sketch how this would look like, but I\u2019ll stop halfway through because at some point the functional form would require more research about mortality at each age.</p><p>The core of the impact estimate is a function that takes the number of beneficiaries, the age distribution of a population, and the benefit of that intervention for someone of a given age, and outputs an estimate of impact.</p><p>In Squiggle, this would look as follows:</p><pre><code>valueOfInterventionInPopulation(num_beneficiaries, population_age_distribution, benefitForPersonOfAge) = {\n &nbsp;age_of_beneficiaries = sampleN(population_age_distribution, num_beneficiaries)\n &nbsp;benefits_array = List.map(age_of_beneficiaries, {|a| benefitForPersonOfAge(a)})\n &nbsp;total_benefits = List.reduce(benefits_array, 0, {|acc, value| acc + value})\n &nbsp;total_benefits\n}</code></pre><p>So for example, if we&nbsp;<a href=\"https://www.squiggle-language.com/playground/#code=eNq9kk1rhDAQhv9K8KQ0LRaWHhZ66KGFhdJd8BqQWR1tIE5sTLZd1P%2FeqLWwXXePvU3m43nnI23QvOvPxFUVmGOwtsYhH13PubTazB5J0kpQyYeTZakwsUZSGayDAyiH22JDFs0ByUpNG9rp2ikY7JBcle6RsJCZBCOx4az%2BjaZQYprLxsP2bnBwNuXaF212aBpN2%2BKpxIg9slYQY0O%2BLk6BPtZAVSt8C6%2BSzzqJBuKPXpOCMXD0rFdfdFdBHS5pcdZ20C03GULUj0irLah0Bs9Ig7nLMDzV4ywekVnG2bjJjnmb3UyPJZygXpCgs2G8zH0cx4Ku7GDM8Ty28nlKFpjiV42ZBcqGyVdj7MHHLsw3HeJv4e1wFUH%2F8xGC%2Fhu08wBc\"><u>feed</u></a> the following variables to our function:</p><pre><code>num_beneficiaries = 1000\npopulation_age_distribution = 10 to 40\nlife_expectancy = 40 to 60\nbenefitForPersonOfAge(age) = life_expectancy - age\nvalueOfInterventionInPopulation(num_beneficiaries, population_age_distribution, benefitForPersonOfAge)</code></pre><p>Then we are saying that we are reaching 1000 people, whose age distribution looks like this:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/bxn6dhiotqw77xhu8lcz.png\"><figcaption>This could use a bit more work to resemble an actual population pyramid.</figcaption></figure><p>and that the benefit is just the remaining life expectancy. This produces the following estimate, in person-years:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/ahpoa7sfjenquaqyelkp.png\"></p><p>But the assumptions we have used aren\u2019t very realistic. We are essentially assuming that we are creating clones of people at different ages, and that they wouldn\u2019t die until the end of their natural 40 to 50-year lifespan.</p><p>To shed these unrealistic assumptions, and produce something we can use to estimate the value of the AMF, we have to:</p><ol><li>Add uncertainty about the shape of the population, i.e., uncertainty about how the population pyramid looks</li><li>Add uncertainty about how many people a distribution reaches</li><li>Change the shape of the uncertainty about the benefit to more closely resemble the effects of bednet distribution</li></ol><p>The first two are relatively easy to do.</p><p>For uncertainty about the number of beneficiaries, we could na\u00efvely write:</p><pre><code>valueWithUncertaintyAboutNumBeneficiaries(num_beneficiaries_dist, population_age_distribution, benefitForPersonOfAge) = {\n &nbsp;numSamples = 1000\n &nbsp;num_beneficiaries_samples_list = sampleN(num_beneficiaries_dist, numSamples)\n &nbsp;benefits_list = List.map(num_beneficiaries_samples_list, {|n| valueOfInterventionInPopulation(n, population_age_distribution, benefitForPersonOfAge)})\n &nbsp;result = mixture(benefits_list)\n &nbsp;result\n}</code></pre><p><br>However, that would be very slow, because we would be repeating an expensive calculation unnecessarily. Instead, we can do&nbsp;<a href=\"https://www.squiggle-language.com/playground/#code=eNrFk9tKw0AQhl9lyVWqUVPwAIIXFRQKUoWg3gTCNp2kC5vZuAe1tH13dzc9m1bQC%2B82M7PfzM7%2FZxqosfhITFVROQmutTQQ%2BdDdiGkhlxGGTDPKkzfDypJDoiXDMrgO3ik38Fj0UYN8B9RMYB%2BfRG04decQTZUNAaFgOaOSgYpIvcpmtIRsxJSFDY0LRKSp1fdCPoFUAh%2BLXgkdckOmKRLi6kWxDbQ5RauawyA8SP42SccRF%2F1URqWkE8t6sJdOK1qHbb0iMp3RWfuQIe3MPVILTXm2BC%2BREkYmh3C7X0Rij8zziPhNzog9k%2BPmow2X4jzFFH3%2BlenxM%2BYgNWWoJ72hMHpgqtvNib8L4NfyJxUkFCDBNh7YSDeO463gixvNJn5yxpryq2HcFogTNfHiq2aU7iK48%2BTGICrjFrzhl327WVO3PbK4vrLI4UZOWZztLOYIz9YPbwSWoAx33Ip9aiM3POIoGyUL7VNsn9sSruLYGYZ0L5woB7bqt%2BUqz20dZwVk8FlDrinm7ic497lLm9tj9cYNuxdP3A%2F6T%2BYM5l83mcoE\"><u>this</u></a>:</p><pre><code>valueWithUncertaintyAboutNumBeneficiaries(num_beneficiaries_dist, population_age_distribution, benefitForPersonOfAge) = {\n &nbsp;referenceN = 1000\n &nbsp;referenceValue = valueOfInterventionInPopulation(referenceN, population_age_distribution, benefitForPersonOfAge)\n\n &nbsp;numSamples = 1001\n &nbsp;num_beneficiaries_samples_list = sampleN(num_beneficiaries_dist, numSamples)\n &nbsp;benefits_list = List.map(num_beneficiaries_samples_list, {|n| referenceValue*n/referenceN})\n &nbsp;result = mixture(benefits_list)\n &nbsp;result\n}</code></pre><p><br>That is, we are calculating the value for a beneficiary population of 1000, and then we are scaling this up. This takes about 6 seconds to compute in Squiggle.</p><p>Now, when adding uncertainty about the shape of the population, we are not going to be able to use that trick, and computation will become more expensive. In the limit, maybe I would want to have a distribution of distributions. But in the meantime, I\u2019ll just have a list of possible population shapes, and&nbsp;<a href=\"https://www.squiggle-language.com/playground/#code=eNrFVN9r2zAQ%2FleEn5zOa5VRd1DoQwctFEZaMFsf5mEU5%2BwI5JMnS11DnP99ku3UTpofkD307Xz36btP95219Kq5%2FBuZomBq4V1rZSBoUnczrqVaZzhyzZmI%2Fhie5wIirTjm3rX3woSBx%2BwBNagXQM0lPuCTLI1gLvbRFMkUEDKecqY4VAEp36oJyyGZ8cqSTY1LBKTF6nupnkBVEh%2Bz2xxG5IYsYyTE4WW2SWhrFStKARP%2FIPM7JSPH2PWrEqYUW1iu7%2FbQecFKf1evgCxrVu8W6bPRqqHUUjORrInXlApmJgV%2Fs19AaEOZpgFpJlkTG5NP7ccuuhhXMcbY1J%2B5nv%2FAFJRmHPXidiqNnpji21DxewOasfyXCwoyUGAbT2xmTCndSP500mzh2Gb0LCeJcVMgztSoMb9qpYy75NaV2wWpEmGJB%2FuybzY96%2BaOdMffVuRwI%2Bcs1luDOcOL%2FuKtwQoqIxxvwV%2B1UYMdcSwDyDHv%2B%2BFGc1bCKdZ3wg%2F7v3cex4mX9QFMTT5krU9z4dhdLdMvLf0vAbmko4C0YdiHV10Y9oCwB4Q9YEx7hIvDQWwxv2PcPQsr4Cul7v0g49D9o4JnkMBrCalmmLq37pK66pWt7XnRWtO3D3527%2FAH7mGM3uofoJ96rQ%3D%3D\"><u>compute the shape of uncertainty over those</u></a>:</p><pre><code>valueWithUncertaintyAboutPopulationShape(num_beneficiaries_dist, population_age_distribution_list, benefitForPersonOfAge) = {\n &nbsp;benefits_list = List.map(population_age_distribution_list, {|population_age_distribution| valueWithUncertaintyAboutNumBeneficiaries(num_beneficiaries_dist, population_age_distribution, benefitForPersonOfAge)})\n &nbsp;result = mixture(benefits_list)\n &nbsp;result\n}\npopulation_age_distribution_list = [to(2, 40), to(2, 50), to(2, 60), to(5, 40), to(5, 50), to(5, 60), to(10, 40), to(10, 50), to(10, 60)]</code></pre><p><br>We still have to tweak the benefits to better capture the benefits of distributing of malaria nets. One first attempt might look as follows:</p><pre><code>benefitForPersonOfAge(age) = {\n &nbsp;result = age &gt; 5 ? mixture(0) : {\n &nbsp;&nbsp;&nbsp;counterfactual_child_mortality = SampleSet.fromDist(0.01 to 0.07)\n &nbsp;&nbsp;&nbsp;// https://apps.who.int/gho/data/view.searo.61200?lang=en\n &nbsp;&nbsp;&nbsp;child_mortality_after_intervention = counterfactual_child_mortality/2\n &nbsp;&nbsp;&nbsp;chance_live_before = (1-(counterfactual_child_mortality))^(5-age)\n &nbsp;&nbsp;&nbsp;chance_live_after =   (1-(child_mortality_after_intervention))^(5-age)\n &nbsp;&nbsp;&nbsp;value = (chance_live_after - chance_live_before) * (life_expectancy - age)\n &nbsp;&nbsp;&nbsp;value\n &nbsp;}\n &nbsp;result\n}</code></pre><p><br>That is, we are modelling this example intervention of halving child mortality, and for child mortality to be pretty high. The final result looks like&nbsp;<a href=\"https://www.squiggle-language.com/playground/#code=eNrFVFFvmzAQ%2FiunPEFHwDRJK0Xqqk7bpEpTWyna9rBsyCUHWAKbGTttleS%2FzwZSkjRNpO6hTxx35%2B8%2B33e%2BRa%2FKxMNEFwWVT72xkhq92vVlxpSQaw%2FjTDGaT%2F5qlqY5TpRkPO2Ne3Oaa7xNrrlCOUeumODX%2FE6UOqfWdrguonvkmLCYUcmw8qB8jkY0xWjGKgN2r63DgyZXfRXyDmUl%2BG1ylaILF7CYcgCbL5JtQBOraFHmeOMcRH7BxLWIbb0qolLSJ4P1zRzyC1o6%2B2p5sFjS5X6SDnVXNaQSiubRGngNKXGmY3S263lAasg49qDu5BKMDR%2Ban31wU76a8imv4z%2BZyr7zGKWijKunq3uh1Y0uPm0yfilA3Zb%2FUkFighJN4RvjCQkhW84flpoJHJuMDuVNZGwXwIo6qcWvGiph69y5cjMgVZQb4I15ea03Her2jLTHn0fkcCGrLF%2FuNOaEB93FG4ElVjq3uAV7VFpuzIhF2Ug5pn3X3ElGS3yL9C3xw%2Fq%2F2o%2FjwIvlgZwlvMtYv02FY3c1SL%2BUcEIPBsT1Gmv4bJ0Zy7xs57QJt%2BawM9cJgy5h0CUMuoShByPi%2Fp7y%2FR0xNM4JsVsEwpF9qTlLMMLHEmNFeWw33pDY6JmJvbLXtp5%2B2yXjhI8wgksoHh3iwriJA8RC2xef0Fhps7fijOWzqBBGzJwpW655WRNUfiJF8dlwdIhPQsvBfM%2FdBiYIIFOqrMZBQMuy8h8y4ZtpCNJMBDOqaDBn%2BOBXSKXwz8JTQi5zytML5C2L7bIRTQyniG3sIsPkMNXgdA1l%2BoRG0zma%2FiZC2uXmhH3n8HHX%2FeOM%2BrTeVLswNRuDAg3OUa67WPN2xTovQft7%2BLpwAs6u7n3YwbPmamfM323VTHlv9Q%2B7JiAl\"><u>this</u></a> (archived&nbsp;<a href=\"https://gist.github.com/NunoSempere/715fd697ff3ebbb704e4a239e559d148\"><u>here</u></a>). The output unit is years of life saved. As is, it doesn\u2019t particularly correspond to the impact of any actual intervention, but hopefully, it could be a template that GiveWell could use, after some research.&nbsp;</p><p>But for reference, the distribution\u2019s impact looks as follows:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/n8zsomj1ukmszvpg1ysl.png\"></p><h3>Calculate optimal allocation given diminishing marginal values</h3><p>Suppose that we have some diminishing marginal return functions. Then, we may want to estimate the optimal allocation to each opportunity.</p><p>We can express diminishing marginal returns functions using two possible syntaxes:</p><pre><code>diminishingMarginalReturns1(funds) = 1/funds\ndiminishingMarginalReturns2 = {|funds| 1/(funds^2)}</code></pre><p>The first syntax is more readable, but the second one can be used without a function definition, which is useful for manipulating functions as objects and defining them programmatically, as explained in this footnote \u293b&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmxi7a8ll6t\"><sup><a href=\"#fnmxi7a8ll6t\">[1]</a></sup></span>.</p><p>Once we have a few diminishing marginal return curves, we can put them in a list/array:</p><pre><code>diminishingMarginalReturns1(funds) = 1/(100 + funds)\ndiminishingMarginalReturns2 = {|funds| 1/(funds^1.1)}\ndiminishingMarginalReturns3 = {|funds| 100/(1k + funds^1.5)}\ndiminishingMarginalReturns4 = {|funds| 200/(funds^2.2)}\ndiminishingMarginalReturns5 = {|funds| 2/(100*funds + 1)}\nuselessDistribution(funds) = 0\nnegativeOpportunity(funds) = 0\n\nlistOfDiminishingMarginalReturns = [\n &nbsp;diminishingMarginalReturns1, diminishingMarginalReturns2,\n &nbsp;diminishingMarginalReturns3, diminishingMarginalReturns4,\n &nbsp;diminishingMarginalReturns5, uselessDistribution,\n &nbsp;negativeOpportunity, {|funds| {1/(1 + funds + funds^2)}}\n]</code></pre><p>And then we can specify our amount of funds;</p><pre><code>availableFunds = 1M // dollars\ncalculationIncrement&nbsp; = 1 // calculate dollar by dollar\nDanger.optimalAllocationGivenDiminishingMarginalReturnsForManyFunctions(listOfDiminishingMarginalReturns, availableFunds, calculationIncrement) </code></pre><p>So in this case, the difficulty comes not from applying a function, but from adding that function to Squiggle. This can be seen&nbsp;<a href=\"https://github.com/quantified-uncertainty/squiggle/blob/develop/packages/squiggle-lang/src/rescript/FR/FR_Danger.res#L278\"><u>here</u></a>.</p><p>Other software (e.g,. Python, R) could also do this, but the usefulness of the above comes from integrating that into Squiggle. For example, we could have an uncertain function produced by some other program, then take its mean (representing its expected value), and feed it to that calculator.</p><p>The&nbsp;<a href=\"https://survivalandflourishing.fund/\"><u>Survival and Flourishing Fund</u></a> has some&nbsp;<a href=\"https://youtu.be/jWivz6KidkI?t=487\"><u>software</u></a> to do something like this. It has a graphical interface which people can tweak, at the expense of being a bit more simple\u2014their diminishing marginal returns are only determined by three points</p><h3>Defining a toy world</h3><p>Lastly, we will define a simple toy world, which has some population growth and some economic growth, as well as some chance of extinction each year. And its value is defined as a function of the consumption of each person, times the chance that the world is still standing.&nbsp;</p><p>For practical purposes, after some set point we stop calculations, and we calculate the remaining value as some function of the current value. We can understand this as either a) the heat death of the universe, or b) an arbitrary limit such that we are interested in the behaviour of the system as that limit goes to infinity, but we can only extend that limit with more computation.</p><p>This setup allows us to coarsely compare an increase in consumption vs an increase in economic growth vs a reduction in existential risk. In particular, given this setup, existential risk and economic growth would be valued less than in the infinite horizon case, so if their value is greater than some increase in consumption in this toy world, we will have reason to think that this would also be the case in the real world.</p><p>The code is a bit too large to simply paste into an EA Forum post, but it can be seen&nbsp;<a href=\"https://github.com/quantified-uncertainty/squiggle-models/blob/master/toy-world/toy-world.squiggle\"><u>here</u></a>. For a further tweak, you can see leaner code&nbsp;<a href=\"https://github.com/quantified-uncertainty/squiggle-models/blob/master/toy-world/toy-world.squiggleU\"><u>here</u></a> which relies on the import functionality of the&nbsp;<a href=\"https://github.com/quantified-uncertainty/squiggle/tree/develop/packages/cli\"><u>squiggle-cli-experimental package</u></a>.</p><p>We can also look at the impact that various interventions have on our toy world, with further details&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1WnplTYJJMeh0zXVUTPBaihE7n1kneW5LDidLvJcGcv4/edit?usp=sharing\"><u>here</u></a>:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994923/mirroredImages/BDXnNdBm6jwj6o5nc/thpmik5nw1efnczss8dy.png\"></p><p>We see that of the sample interventions, increasing population growth by 0.5% has the highest impact. But 0.5%/year is a pretty large amount, and it would be pretty difficult to engineer. So further work could look at the relative difficulty of each of those interventions. Still, that table may serve to make a qualitative argument that interventions such as increasing population growth, economic growth, or reducing existential risk, are probably more valuable than directly increasing consumption.</p><h2>Conclusion</h2><p>I presented a few more advanced Squiggle models.&nbsp;</p><p>A running theme was that expressing estimates as functions\u2014e.g., the chance of AGI at every point in time, the impact of an intervention for all possible ages, a list of diminishing marginal return functions for a list of interventions, a toy world with a population assigned some value at every point in time\u2014might allow us to come up with better and more accurate estimates. Squiggle is not the only software that can do this, but hopefully it will make such estimation easier.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmxi7a8ll6t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmxi7a8ll6t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We can write:</p><pre><code>listOfFunctions = [ {|funds| 1/(funds^2)},&nbsp; {|funds| 1/(funds^3)}]</code></pre><p>or even&nbsp;</p><pre><code>multiplyByI(i) = {|x| x*i}\nlistOfFunctions = List.map(List.upTo(0,10), {|i| multiplyByI(i)})</code></pre><pre><code>or without the need for a helper:\nlistOfFunctions2 = List.map(List.upTo(0,10), {|i| {|x| x*i}})\nlistOfFunctions2[4](2) // 4 * 2 = 8</code></pre><p>This is standard functional programming stuff, and some functionality is missing from Squiggle, such as <i>List.length</i> function. But still.</p></div></li></ol>", "user": {"username": "NunoSempere"}}, {"_id": "kFufCHAmu7cwigH4B", "title": "Lessons learned from talking to >100 academics about AI safety", "postedAt": "2022-10-10T13:16:38.392Z", "htmlBody": "", "user": {"username": "mariushobbhahn"}}, {"_id": "WBjmN2eakezo5bn4v", "title": "Epistemics-Improving Activities for Groups and Friends", "postedAt": "2022-10-10T13:11:45.054Z", "htmlBody": "<p>A list I've accumulated from multiple places, so can't vouch for all of them, and definitely there's no empirical evidence I know of for these improving epistemics, but I wanted to collect the list anyway.</p><p>The obvious place to do these is in student groups, but I think limiting them there is a mistake. I think we all can benefit from honing our skills, plus a bunch of these are just fun. If you play board games or do <a href=\"http://puzzledpint.com/\">Puzzled Pint</a> with your friends or coworkers, then these can just be part of that part of your life.</p><p>Will be editing and adding to this over time - what would you recommend?</p><h3>Activities</h3><ul><li><a href=\"https://www.amazon.co.uk/Thinking-Physics-Lewis-Carroll-Epstein/dp/0935218084\">Thinking Physics questions</a></li><li>Fermi estimates / Estimathons<ul><li><a href=\"https://chsu.domains.swarthmore.edu/estimathon.html#:~:text=Sample%20Estimathon%20Questions,miles%2C%20of%20the%20Atlantic%20Ocean%3F\">Sample estimathon questions</a></li><li><a href=\"https://math.berkeley.edu/~moorxu/2020ARML/files/EstimathonRules.pdf\">Rules for estimathons</a> from Berkeley, which probably came from Jane Street</li><li><a href=\"https://chanamessinger.com/blog/fermi-problems\">List of lists of Fermi problems</a></li></ul></li><li>Calibration training<ul><li><a href=\"https://www.quantifiedintuitions.org/calibration\">Sage Calibration Activity (my current favorite)&nbsp;</a></li><li><a href=\"https://calibration-practice.neocities.org/\">Scout Mindset Calibration Questions</a></li><li><a href=\"https://www.openphilanthropy.org/research/new-web-app-for-calibration-training/\">Open Philanthropy's Calibration Questions</a></li><li><a href=\"https://www.lesswrong.com/posts/LdFbx9oqtKAAwtKF3/list-of-probability-calibration-exercises\">More in this wiki</a></li><li>I have more if you DM, don't want everyone to know all the good questions ahead of time :)</li></ul></li><li>Team Forecasting - really excited about this, watch this space, but in the meantime, nothing stopping you from just picking a question from <a href=\"https://www.metaculus.com/\">Metaculus</a> or <a href=\"https://manifold.markets/\">Manifold</a> and discussing together<ul><li>I had a great time recently <a href=\"https://www.quantifiedintuitions.org/pastcasting\">pastcasting</a> a question with a friend, which had the added benefits of getting both of us smoe knowledge about the current conflict in Ethiopia</li></ul></li><li><a href=\"https://chanamessinger.com/blog/resolving-the-abilene-paradox\">Resolve the Abilene paradox</a> (A type of groupthink)</li></ul><h3>Games</h3><ul><li><a href=\"https://boardgamegeek.com/boardgame/20100/wits-wagers\">Wits and Wagers</a> (<a href=\"https://forum.effectivealtruism.org/posts/NMg4qq87GPXtnbJnn/wits-and-wagers-an-engaging-game-for-effective-altruists\">Someone else's experience of using it in EA contexts</a>)</li><li><a href=\"https://murdershebet.com/\">Murder, She Bet</a></li><li><a href=\"https://shakeddown.wordpress.com/2022/08/04/what-i-learned-about-running-a-betting-market-game-night-contest/?fbclid=IwAR0SjZpebLJdc3LGs4R6pNvXO_Vg1W3iBYMAh_M-TP4zh4jDo0vMmYa4W6I\">Betting Market Game Night</a></li><li>Poker</li><li><a href=\"https://docs.google.com/document/d/1WhhVm6kQlBa9kjwBCWFEfL4ZcgxufVMegEsH2MUyc9c/edit#heading=h.oy9cgyqus82q\">Fermi Poker</a></li><li><a href=\"https://www.amazon.co.uk/CONFIDENT-Thrilling-Trivia-Board-OutRANGEous/dp/B07H51XLZQ/ref=asc_df_B07H51XLZQ/?tag=googshopuk-21&amp;linkCode=df0&amp;hvadid=310843163984&amp;hvpos=&amp;hvnetw=g&amp;hvrand=17222514641738182323&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1006598&amp;hvtargid=pla-572192924547&amp;psc=1\">CONFIDENT?</a><ul><li>It's basically a competitive gamification of calibration training. You are rewarded for having the smallest confidence window of all the players, and penalised if your answer is outside of your confidence window.</li></ul></li></ul><p>&nbsp;</p><h3>Talking</h3><ul><li>Explain or draw how something works, <a href=\"https://thedecisionlab.com/biases/the-illusion-of-explanatory-depth\">notice where you falter</a>, let someone pick up the slack from there</li></ul><h3>Writing</h3><ul><li><a href=\"https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why\">Fact posts</a></li><li><a href=\"https://www.cold-takes.com/minimal-trust-investigations/\">Minimal trust investigations</a></li><li><a href=\"https://www.cold-takes.com/learning-by-writing/\">Learning by Writing</a><ul><li>Related: <a href=\"http://www.paulgraham.com/words.html\">Putting Ideas Into Words</a></li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/MBAHL62zLJn35XCdw/learning-by-writing-in-groups-1\">Learning by writing in groups</a></li></ul>", "user": {"username": "ChanaMessinger"}}, {"_id": "4MkWeQ6g5LhCTjhZA", "title": "LW4EA: Fact Posts: How and Why", "postedAt": "2022-10-10T12:08:57.167Z", "htmlBody": "<p>Written by LW user <a href=\"https://www.lesswrong.com/users/sarahconstantin\">Sarah Constantin</a></p><p>This post was a reminder of the availability of the option to just go and find out about the world, to start by googling and see where your curiosity leads you, to explore and build up models of the world, even if you don't have any starting knowledge. I think it's a great skill and there are worlds where EA's would benefit from being a lot better at it, since the problems we're trying to solve have all kinds of pieces and connections to the rest of the world e.g. knowing about AI might involve understanding how semiconductors work, and also the politics of the South China Sea and also who Elon Musk is currently friends and enemies with.</p><p>Excerpt:</p><blockquote><p>To write a fact post, you start with an empirical question, or a general topic. &nbsp;Something like \"How common are hate crimes?\" or \"Are epidurals really dangerous?\" or \"What causes manufacturing job loss?\" &nbsp;</p><p>It's okay if this is a topic you know very little about. This is an exercise in original seeing and showing your reasoning, not finding the official last word on a topic or doing the best analysis in the world.</p><p>Then you open up a Google doc and start taking notes.</p><p>....And then you start letting the data show you things.&nbsp;</p><p>You see things that are surprising or odd, and you note that.&nbsp;</p><p>You see facts that seem to be inconsistent with each other, and you look into the data sources and methodology until you clear up the mystery.</p><p>The advantage of fact posts is that they give you the ability to form independent opinions based on evidence. It's a sort of practice of the skill of seeing. They likely aren't the optimal way to get the most accurate beliefs -- listening to the best experts would almost certainly be better -- but you, personally, may not know who the best experts are, or may be overwhelmed by the swirl of controversy. Fact posts give you a relatively low-effort way of coming to informed opinions. They make you into the proverbial 'educated layman.'</p></blockquote><p>Related to <a href=\"https://www.cold-takes.com/minimal-trust-investigations/\">Minimal-trust investigations</a> and <a href=\"https://forum.effectivealtruism.org/posts/MBAHL62zLJn35XCdw/learning-by-writing-in-groups-1\">Learning By Writing In Groups</a></p><p><br>&nbsp;</p>", "user": {"username": "ChanaMessinger"}}, {"_id": "8Ban7AnoqwdzQphsK", "title": "We can do better than argmax", "postedAt": "2022-10-10T10:32:02.771Z", "htmlBody": "<p><i>Summary</i>: A much-discussed normative model of prioritisation in EA is akin to&nbsp;<i>argmax&nbsp;</i>(putting all resources on your top option). But this model often prescribes foolish things, so we rightly deviate from it \u2013 but in ad hoc ways. We describe a more principled approach: a kind of&nbsp;<i>softmax</i>, in which it is best to allocate resources to several options by confidence. This is a better yardstick when a whole community collaborates on impact; when some opportunities are fleeting or initially unknown; or when large actors are in play.</p><p><i>Epistemic status</i>: Relatively well-grounded in theory, though the analogy to formal methods is inexact. You could mentally replace \u201cargmax\u201d with \u201call-in\u201d and \u201csoftmax\u201d with \u201csmooth\u201d and still get the gist.<br><br><i>Gavin wrote almost all of this one, based on Jan\u2019s idea.</i> &nbsp;</p><hr><p>&gt;&nbsp;<i>many EAs\u2019 writings and statements are much more one-dimensional and \u201cmaximizy\u201d than their actions.</i></p><p>\u2013&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\"><u>Karnofsky</u></a><br>&nbsp;</p><p>Cause prioritisation is&nbsp;<a href=\"https://80000hours.org/articles/problem-framework/\"><u>often</u></a> talked about like this:&nbsp;</p><ol><li>Evaluate a small number of options (e.g. 50 causes);</li><li>Estimate their {importance, tractability, and neglectedness} from expert point estimates;</li><li>Give massive resources to the top option.&nbsp;</li></ol><p>You can see this as taking the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Arg_max\"><u>argmax</u></a>: as figuring out which input (e.g. \u201ctrying out AI safety\u201d; \u201cgoing to grad school\u201d) will get us the most output (expected impact). So call this&nbsp;<i>argmax</i>&nbsp;<i>prioritisation&nbsp;</i>(AP). &nbsp;</p><p>AP beats the hell out of the standard&nbsp;<a href=\"https://www.youtube.com/watch?v=SaP7qmsQbSI\"><u>procedure</u></a> (\u201c<i>do what your teachers told you you were good at</i>\u201d; \u201c<i>do what polls well</i>\u201d). But it\u2019s a poor way to run a portfolio or community, because it only works when you\u2019re allocating marginal resources (e.g. one additional researcher); when your estimates of the effect or cost-effect are not changing fast; and when you already understand the whole action space.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh1oyvqwni1a\"><sup><a href=\"#fnh1oyvqwni1a\">[1]</a></sup></span></p><p>It serves pretty well in global health. But where these assumptions are severely violated, you want a different approach \u2013 and while&nbsp;<a href=\"https://agentmodels.org/chapters/3-agents-as-programs.html\"><u>alternatives</u></a> are known in technical circles, they are less understood by the community at large.&nbsp;</p><p>Problems with AP,&nbsp;<u>construed naively</u>:&nbsp;</p><ul><li><i>Monomania</i>: the argmax function returns a single option; the winner takes all the resources. If people naively act under AP without coordinating, we get diminishing returns and decreased productivity (because of bottlenecks in the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Complementary_good\"><u>complements</u></a> to adding people to a field, like ops and mentoring). Also, under plausible assumptions, the single cause it picks will be a poor fit for most people. To patch this, the community has responded with the genre \"<i>You should work on X instead of AI safety</i>\" or \u201c<i>Why X is actually the best way to help the long-term future</i>\u201d. We feel we need to justify not argmaxing, or to represent our thing as the true community argmax. And in practice justification often involves manipulating your own beliefs (to artificially lengthen your AI timelines, say), appealing to ad hoc principles like&nbsp;<a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/\"><u>worldview diversification</u></a>&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgkhfchsrd3a\"><sup><a href=\"#fngkhfchsrd3a\">[2]</a></sup></span>, or getting into arguments about the precise degree of crowdedness of alignment.&nbsp;</li><li><i>Stuckness:</i> Naive argmax gives no resources to exploration (because we assume at the outset that we know all the actions and have good enough estimates of their rank). As a result, decisions can get stuck at local maxima. The quest for \"<a href=\"https://forum.effectivealtruism.org/topics/cause-x\"><u>Cause X</u></a>\" is a meta patch for a lack of exploration in AP. Also, from experience, existing frameworks treat value-of-information as an afterthought, sometimes ignoring it entirely.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvkzgfa7ei7s\"><sup><a href=\"#fnvkzgfa7ei7s\">[3]</a></sup></span></li><li><i>Flickering:</i> If the top two actions have similar utilities, small changes in the available information lead to endless costly jumps between options. (Maybe even cycles!) Given any realistic constraints about training costs or lags or bottlenecks, you really don't want to do this. This has actually happened in our experience, with some severe switching costs (years lost per person). OpenPhil intentionally added&nbsp;<a href=\"https://www.openphilanthropy.org/research/the-importance-of-committing-to-causes/\"><u>inertia</u></a> to their funding pledges to build trust and ride out this kind of dynamic.&nbsp;</li></ul><p>(But see&nbsp;<u>the appendix</u> for a less mechanical approach.)&nbsp;</p><h3>Softmax prioritisation&nbsp;</h3><p><a href=\"https://michielstock.github.io/posts/2021/2021-03-20-softmax/\"><u>Softmax</u></a> (\u201csoft argmax\u201d) is a function which approximates the argmax in a&nbsp;<i>smooth</i> fashion. Instead of giving you one top action, it gives you a set of probabilities. See&nbsp;<a href=\"https://agentmodels.org/chapters/3-agents-as-programs.html\"><u>this amazing (and runnable!) tutorial</u></a> from Evans and Stuhlm\u00fcller.&nbsp;</p><p>Softmax is a natural choice to inject a bit of&nbsp;<a href=\"https://michielstock.github.io/posts/2021/2021-03-20-softmax/\"><u>principled</u></a> exploration. Say you were picking where to eat, instead of what to devote your whole life to. Example with made-up numbers: where argmax goes to the best restaurant every day, softmax might go to the best restaurant ~70% of the time, the second best ~20% of the time, the third best ~5% of the time... The weights here are the normalised exponential of how good you think each is.<br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994814/mirroredImages/8Ban7AnoqwdzQphsK/ziwzw5wkdyfqwsmpbyjc.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994815/mirroredImages/8Ban7AnoqwdzQphsK/sl9otfltifqbhyvymthn.png\"></p><p><br>&nbsp;The analogy is nicer when we consider softmax\u2019s&nbsp;<a href=\"https://stats.stackexchange.com/questions/527080/what-is-the-role-of-temperature-in-softmax\"><u>temperature</u></a> parameter, which controls how wide your uncertainty is, and so how much exploration you want to do. This lets us update our calculation as we explore more: under maximum uncertainty, softmax just outputs a uniform distribution over options, so that we explore literally every option with equal intensity. Under maximum information, the function outputs just the argmax option. In real life we are of course in the middle for all causes \u2013 but this is useful, if you can guess how far along the temperature scale your area (or&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection\"><u>humanity</u></a>!) currently is.</p><p>We have been slowly patching AP over the years, approximating a principled alternative. We suggest that <i>softmax prioritisation</i> incorporates many of these patches. It goes like this:&nbsp;</p><ol><li>Start at high temperature, placing lots of weight on each option.</li><li>Observe what happens after taking each option.</li><li>Decrease your temperature as you explore and learn more, thus gradually approaching argmax.&nbsp;&nbsp;</li></ol><p>(To be clear, we\u2019re saying the&nbsp;<i>community</i> should run this, and that this indirectly implies individual attitudes and decisions. See Takeaways below.)</p><p>So \"softmax\" prioritisation says that it is normative for some people to work on the second and third-ranked things, even if everyone agrees on the ranking.<br><br>Why should the EA community as a whole do softmax prioritisation?&nbsp;</p><ol><li><i>Smoothing</i>. As discussed in&nbsp;<i>Monomania,&nbsp;</i>in practice it\u2019s not optimal to throw everything at one cause, because of uncertainty, diminishing returns, limited mentoring and room for funding, and poor fit with most individuals. So we should spread our resources out (and in fact we do).&nbsp;</li><li><i>Total allocation</i>. Allocating&nbsp;<i>marginal</i> resources (e.g. as a single altruistic donor looking to improve global health) is really different from allocating a large fraction of total resources (e.g. when the resources of an entire nation-state are allocated among pandemic interventions). For marginal allocation, what matters is the local cost/benefit slope. When managing total resource allocation, understanding the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/GzJHagyuEbWdxDFjJ/concave-and-convex-altruism\"><u>whole curve</u></a> is vital.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefphsya3uxvza\"><sup><a href=\"#fnphsya3uxvza\">[4]</a></sup></span><br><br>Consider EA as one big \"collective agent\", and think about the allocation of people as if it was collaborative. Notably, the optimal allocation of people is&nbsp;<strong>not&nbsp;</strong>achieved by everyone individually doing argmax prioritisation, since we need to take into account crowding and elasticities (\u201croom for more funding\u201d and so on).&nbsp;&nbsp;</li><li><i>Due weight on exploration and optionality</i>. When you're choosing between many possible interventions and new interventions are constantly appearing and disappearing, then you should spend significant resources on 1) finding options, 2) trying them out, and 3) preserving options that might otherwise vanish. (e.g. Going to the committee meetings and fundraisers so that people see you as a live player.)<br>&nbsp;</li></ol><p><i><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994814/mirroredImages/8Ban7AnoqwdzQphsK/m1wuw7zi0oub6y799akh.png\"></i></p><p>&nbsp;</p><h2>Takeaways</h2><p>Lots of people already act as if the community was using something like softmax prioritisation. So our advice is more about cleaning up the lore; resolving cognitive dissonance between our argmax rhetoric and softmax actions; and pushing status gradients to align with the real optima. In practice:</p><ol><li>Explore with a clear conscience!&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SCqRu6shoa8ySvRAa/big-list-of-cause-candidates\"><u>List</u></a>&nbsp;<a href=\"https://www.causeexplorationprizes.com/\"><u>new causes</u></a>; look for new interventions within them; look for new approaches within interventions; one might be better than all current ones.</li><li>Stand tall! Work on stuff which is not the argmax with a clear conscience, because you are part of an optimal community effort. Similarly, praise people for exploring new options and prioritising in a \u201csoftmax\u201d fashion; they are vital to the collective project.&nbsp;</li><li>Similarly, you don\u2019t need to distort your beliefs to say that what you\u2019re working on is the argmax.</li><li>Don\u2019t flicker! Think twice about switching causes if you\u2019ve already explored several.&nbsp;</li></ol><p>Ultimately: we have made it high status to throw yourself at 80,000 Hours\u2019 argmax output, AI. (Despite their staff understanding all of the above and communicating it!) We have not made it very high status to do anything else.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3blgrrov284\"><sup><a href=\"#fn3blgrrov284\">[5]</a></sup></span></p><p>So: notice that other directions are also uphill; help lift things wrongly regarded as downhill. Diversifying is not suboptimal pandering. Under more sensible assumptions, it\u2019s just sound.&nbsp;</p><h2>See also</h2><ul><li><a href=\"https://meteuphoric.com/2016/07/28/effective-altruisms-large-and-small/\"><u>This</u></a> wonderful post by Katja Grace&nbsp;</li><li>Halstead\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Eav7tedvX96Gk2uKE/the-itn-framework-cost-effectiveness-and-cause\"><u>classic critique</u></a> of ITN&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/cDv9mP5Yoz4k4fSbW/don-t-over-optimize-things\"><u>Cotton-Barratt on overoptimising</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\"><u>Karnofsky on perils</u></a>,&nbsp;<a href=\"https://raymonddouglas.co.uk/legible-optimisation-causes-illegible-harm-2/\"><u>Douglas on goodhart</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/f9NpDx65zY6Qk9ofe/doing-good-best-isn-t-the-ea-ideal\"><u>Manheim on vaguely related issues</u></a>.</li></ul><p>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994814/mirroredImages/8Ban7AnoqwdzQphsK/vzofg5ovxjyz6jlr0nyb.png\"></p><p><i>\u201cargmax stuck in a local maximum, photorealistic 4k\u201d</i></p><p>&nbsp;</p><p><i>Thanks to Fin Moorhouse, Misha Yagudin, Owen Cotton-Barratt, Raymond Douglas, Stag Lynn, Andis Draguns, Sahil Kulshrestha, and Arden Koehler for comments</i>.<i> Thanks to Ramana and Eliana for conversations about the concept.</i></p><p>&nbsp;</p><h2>Appendix: Other reasons to diverge from argmax&nbsp;</h2><p>In order of how much we endorse them:</p><ul><li><a href=\"https://forum.effectivealtruism.org/topics/value-of-information\"><u>Value of information</u></a> is usually incredibly high&nbsp;</li><li><a href=\"https://clearerthinkingpodcast.com/episode/121/#transcript\"><u>You don\u2019t know the whole option set</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/8Ban7AnoqwdzQphsK/we-can-do-better-than-argmax?commentId=KMKr93kZN8wmptuvH#comments\">diminishing returns</a></li><li><a href=\"https://www.rug.nl/filosofie/organization/news-and-events/events/2018-toel/hedging-our-bets\"><u>Moral uncertainty&nbsp;</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/GzJHagyuEbWdxDFjJ/concave-and-convex-altruism#Diversifying\"><u>Concave altruism</u></a> (i.e. Jensen\u2019s inequality!)</li><li><a href=\"https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it\"><u>The optimiser\u2019s curse</u></a></li><li><a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>Worldview diversification</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/xAoZotkzcY5mvmXFY/longtermism-risk-and-extinction?commentId=e79ih6uaJdnds6GEM#comments\"><u>Principled risk aversion</u></a>,&nbsp;<a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\"><u>as at GiveWell</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal#The_Leverage_Hypothesis\"><u>Strategic skulduggery</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/9pktesiW2WPEFNvCQ/uncorrelated-bets-an-easy-to-understand-and-very-important#comments\"><u>Decrease variance of your portfolio for more impact compounding</u></a>(?)&nbsp;</li></ul><p>&nbsp;</p><p>For individual deviations from the community's argmax (\"this is the best thing to do, ignoring who you are and your aptitudes\"), your (lack of) personal fit and local opportunities <i>can</i> let you completely outdo the impact of you working on cause #1. (But this deviation is still a one-person argmax.)</p><p>&nbsp;</p><h2>Appendix: Technical details</h2><p>The main post doesn\u2019t distinguish all of the ways argmax prioritisation fails:</p><p><strong>When you have uncertain rank estimates&nbsp;</strong></p><p><a href=\"https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it\"><u>The optimizer\u2019s curse</u></a> means that you will in general (even given an unbiased estimator) screw up and have inflated estimates for the utility of option #1. The associated inflation can be more than a standard deviation, which will often cause the top few ranks to switch. Softmax prioritisation softens the blow here by spreading resources in a way which means that the true top option will not be totally neglected.</p><p><strong>When you have an incomplete action set&nbsp;</strong></p><p>The first justification for exploration (i.e. optimal allocations which sample actions besides the&nbsp;<i>current</i> top few) is that we just do not know all of the actions that are available. This idea is already mainstream in the form of \u201c<a href=\"https://forum.effectivealtruism.org/topics/cause-x\"><u>Cause X</u></a>\u201d and&nbsp;<a href=\"https://www.causeexplorationprizes.com/\"><u>Cause Exploration</u></a>, so all we\u2019re doing is retroactively justifying these. See also&nbsp;<a href=\"https://clearerthinkingpodcast.com/episode/121/#transcript\"><u>Macaskill\u2019s discussion</u></a> on Clearer Thinking.</p><p><strong>When you have a time-varying action set&nbsp;</strong></p><p>If actions are only available for a window in time, then you have a much more complicated decision problem. At minimum, you need to watch for new opportunities (research which takes some of your allocation) and rerun the prioritisation periodically.&nbsp;</p><p>When we further consider actions with a short lifespan, we have even more reason to devote permanent resources to the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Automated_planning_and_scheduling\"><u>order and timing</u></a> of actions.</p><p><strong>When you\u2019re doing total allocation instead of marginal</strong></p><p>When we\u2019re allocating many people, we don\u2019t get the optimal allocation by everyone individually doing argmax prioritisation, since we need to take into account diminishing returns, limiting factors (\u201croom for more funding\u201d, \u201cthe mentor bottleneck\u201d, and so on), and other scaling effects. &nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/GzJHagyuEbWdxDFjJ/concave-and-convex-altruism\"><u>Fin gives a nice visual overview of the production functions involved</u></a>. As well as letting us exploit interventions with increasing returns to scale, total allocation means you have the resources to escape local minima. This bears on the perennial \u201c<a href=\"https://forum.effectivealtruism.org/topics/systemic-change\"><u>systemic change</u></a>\u201d critique: that critique is ~true when you are large!</p><p><strong>When you have light tails</strong></p><p>The&nbsp;<a href=\"https://www.cgdev.org/sites/default/files/1427016_file_moral_imperative_cost_effectiveness.pdf\"><u>power-law distribution</u></a> of global health charity impacts was the key founding insight of EA. But there is nothing inevitable about altruistic acts taking this distribution, and Covid policy is a good example: the effects were mostly pretty additive, or multiplicative in the unfortunate way (when each effect is bounded in [0,1] and coupled, so that failure at one diminishes your overall impact). Argmax is much less bad under heavy tails.</p><p>&nbsp;</p><h2>Appendix: Softmax against Covid&nbsp;</h2><p>With Covid in February 2020, it was hard to estimate the importance, tractability, or neglectedness of different actions; most options were unknown to me, and the impact of a given actor was highly contextual. Also, it was possible to get into a position to influence the&nbsp;<i>total&nbsp;</i>allocation of resources.&nbsp;<br><br>When planning&nbsp;<a href=\"https://forum.effectivealtruism.org/s/dr4yccYeH6Zs7J82s\"><u>his Covid work</u></a>, Jan usually relied on a softmax heuristic: repeatedly re-evaluating his estimates of the options, putting extra weight on the value of information. While he\u2019s happy with the results, on some occasions it was difficult to explain what he was trying to do to people who grew up with&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Eav7tedvX96Gk2uKE/the-itn-framework-cost-effectiveness-and-cause\"><u>ITN</u></a> and argmax prioritisation more generally.&nbsp;</p><p>Future crises will have different subsets of assumptions holding. Granted that the softmax approach is important, and sometimes practically diverges from the argmax, it seems good to have more people think about the tradeoff between argmax and softmax, exploration, VoI, and so on.&nbsp;</p><p>In dynamic environments like crises and politics, you benefit from rapidly re-evaluating actions and chaining together sequences of low-probability outcomes. (\"<i>Can I get invited to the \u2018room where it happens\u2019? Can I be then taken seriously as a scientist? Can I then be appointed to a committee? Can we then write unprecedented policy? Can that policy then be enforced?</i>\")</p><p>Classify crises by how many things you need to get right to solve them. In some crises, discovering the single silver bullet and investing in it heavily is the best approach. In others, solutions require e.g. 8 out of 10 pieces in place, and getting all of them highly functional.<br><br>Covid, at least after spreading out of Wuhan, seems closer to the second case: the winning strategy consisted of many policies, implemented well. Countries which did even better (e.g. Taiwan) did even more: \"do a hundred different things, some of them won't have any effect, but all together it\u2019ll work\".<br><br>Note that this goes against the community instinct that some solutions will be&nbsp;<i>much</i> more effective than others. This expectation is often unsound outside the original context of global health projects. (Again, this context models the&nbsp;<i>marginal</i> returns to investing more or less on given actions, assuming we have no&nbsp;<a href=\"https://en.wikipedia.org/wiki/Market_power\"><u>(market) power</u></a>. But national policy is closer to&nbsp;<i>total</i> allocation than marginal allocation: you can have profound effects on other actors\u2019 incentives, up to and including&nbsp;<a href=\"https://en.wikipedia.org/wiki/Monopsony\"><u>full</u></a> power.)</p><p>i.e. Emergency policy impact differs from global health NGO impact! Governments are better modelled as performing total allocation or as monopsonies. And Covid national policy arguably didn\u2019t have a single \"silver bullet\" policy with 10x the impact of the next best policy.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd65w6u1ch2h\"><sup><a href=\"#fnd65w6u1ch2h\">[6]</a></sup></span></p><p>&nbsp;</p><h2>Appendix: Enlightened argmax</h2><p>In theory there\u2019s nothing stopping the argmaxer and their argmaxer friends from doing something cleverer. For instance, you should argmax over lots of local factors (like each person\u2019s comparative advantage and personal fit, the room for funding each cause has, the mentor bottlenecks, etc), which would let you reach quite different answers than the naive \u201cwhat would be best, being agnostic about individual factors and bottlenecks?\u201d we do to produce&nbsp;<a href=\"https://80000hours.org/problem-profiles/#the-kinds-of-issues-we-currently-prioritise-most-highly\"><u>consensus reality</u></a>.</p><p>Done optimally (i.e. by a set of perfectly coordinated epistemic peers with all the necessary knowledge of every cause), argmax&nbsp;<i>could</i> get all the good stuff. But in practice, people can't compute the argmax for themselves (well), so we rely on cached calculations from a few central estimators, which produces the above issues. A lot of us simply take the output of 80k's argmax and add a little noise from personal fit.</p><h3>e.g. Does argmax really say to not explore?</h3><p>If you have perfectly equal estimates of impact of everything, argmax sensibly says you must explore to distinguish them better. But once you know a bit, or if your estimates are noisy and something appears slightly better, a naive argmax over actions tells you to do just that one thing</p><p>You can always rescue argmax by going meta \u2013 instead of taking argmax over object-level actions, you instead take the argmax&nbsp;<i>decision procedure</i>. \u201cWhat way of deciding actions is best? Enumerate, rank, and saturate.\u201d (This is a relative of what Michael Nielsen calls \u201c<a href=\"https://forum.effectivealtruism.org/posts/JBAPssaYMMRfNqYt7/michael-nielsen-s-notes-on-effective-altruism\"><u>EA judo</u></a>\u201d.)</p><p>Still: yes, you can reframe the whole post as \"EAs are often using argmax&nbsp;<i>at the wrong level</i>, evaluating \"actions\" (like career choices). This is often suboptimal: they should be using argmax on the meta-level of decision procedures and community portfolios\".&nbsp;</p><p>&nbsp;</p><h2>FAQ</h2><p><i>Q: It's not clear to me that \u201csoftmax\u201d prioritisation avoids flickering? Specifically: At low temperature, there's lots of moving around. At high temperature, it converges to argmax, which supposedly has the flickering problem!</i></p><p>A: Yes, this is vague. This is because there's several levels at play: the literal abstract algorithm, the EA community\u2019s pseudoalgorithm mixture, and each individual realising that it is or should be the community-level algorithm.&nbsp;</p><p>First, assume a middling temperature (if nothing else because human optimisation efforts are so young). \"Softmax\" prioritisation says that it is normative for some people to work on the second and third, even if everyone agreed on the ranking.<strong> </strong>Flickering is suppressed at the level of an individual switching their cause (because we all recognise that some stickiness is optimal for the community). That\u2019s the idea anyway.</p><p><i>Q: How do I do this in practice? Am I even supposed to calculate anything?</i></p><p>A: If you share all of the intuitions already there's maybe not much point. But I expect thinking in terms of temperature and community portfolio to help, even if it's informal. One stronger formal claim we can make is that \"the type signature of prioritisation is W --&gt; D\", where</p><ul><li>W = a world model including uncertainty and a gap where missing options should be</li><li>D = a distribution of weights over causes.<br>&nbsp;</li></ul><p><i>Q: Is this post about creating defensibility for doing more normal things? That is, making it easier to persuade someone who needs a new formalism (\"softmax\") to be moved to depart from an earlier formalism (\"argmax\"). There's something broken about banning sensible intuitions if they don't align with a formalism. If this is the way a large part of the community works, you might have to play up to it anyway. If that is indeed the context &amp; goal, details are probably irrelevant. But I think it would be more honest to say so.</i></p><p>A: It\u2019s somewhere in between a methodology and a bid to shift social reality. I can't deny that maths is therapy for a certain part of the audience. But I actually do think softmax is better than existing justifications for sensible practices. We made up a bunch of bad coping mechanisms for our own argmax rhetoric. We want the sensible practices without the cognitive dissonance and suboptimal spandrels.<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh1oyvqwni1a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh1oyvqwni1a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This form is a legacy of argmax prioritisation\u2019s original context: evaluating global health charities. It seems to work quite well there because the assumptions are not violated very hard. (Though note that rank uncertainty&nbsp;<a href=\"https://www.evidenceaction.org/dispensersforsafewater/\"><u>continues</u></a> to bite, even in this old and intensely studied domain.) The trouble is when it is used as a universal prioritisation algorithm where it will reliably fail.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngkhfchsrd3a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgkhfchsrd3a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Worldview diversification is way less ad hoc than the other things: it is a lot like&nbsp;<a href=\"https://en.wikipedia.org/wiki/Thompson_sampling\"><u>Thompson sampling</u></a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvkzgfa7ei7s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvkzgfa7ei7s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is changing! OpenPhil recently commissioned Tom Adamczewski to build&nbsp;<a href=\"https://valueofinfo.com/\"><u>this cool tool</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnphsya3uxvza\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefphsya3uxvza\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We wrote this with government policy advice in mind as a route to affecting large fractions of the total allocation. But it also applies to optimistic scenarios of EA scaling.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3blgrrov284\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3blgrrov284\">^</a></strong></sup></span><div class=\"footnote-content\"><p>OK, it\u2019s also high-status to pontificate about the problems of EA-as-a-whole, like us.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd65w6u1ch2h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd65w6u1ch2h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>At the individual level, there were some silver bullets, like not going to the pub or wearing a P100 mask.</p></div></li></ol>", "user": {"username": "Jan_Kulveit"}}, {"_id": "DArtSnDxRH5AsE5RF", "title": "Sheltering humanity against x-risk: report from the SHELTER weekend", "postedAt": "2022-10-10T15:09:05.054Z", "htmlBody": "<p>Greetings everyone! I was one of the participants in the SHELTER weekend (4-8 Aug 22) organized by members of the EA community. I wrote a report on the outcomes of the discussion and intended to publish it here, although I - again - needed some prodding (thanks :)) to make that finally happen. Thanks to everyone involved, all the commenters of the previous versions, and the community for the support that made all this possible!</p><p>The document is 47 pages with its appendix, so instead of flooding this forum, <a href=\"https://docs.google.com/document/d/1yL-dfFHlm8VA_Ss9D0Ig4GcImi6fLio3jqPTVhyt120/edit?usp=sharing\">here's the link to the Google document</a>: anyone should be able to comment on it this way. I licensed my contributions with a CC-BY-NC license, so feel free to share this as widely as possible. If there are other outlets where you think this should be published, let me know, I'm very open to suggestions.</p><p>&nbsp;</p><p>EDIT: In brief, the key takeaways are here (thanks Cillian for reminding me of this oversight):<br><br>&nbsp;</p><blockquote><h1><strong>KEY TAKEAWAYS</strong></h1><ol><li>There is no escape hatch for humanity, nor for the rich. Shelters that can reliably protect even a small group of humans against catastrophes that would otherwise make humanity extinct are probably infeasible due to multiple technical, psychological, social, political, and economic issues. Constructing \u201cescape hatches\u201d for the few, particularly for the rich and the powerful, would probably increase the net catastrophic and existential risk, as any benefits gained would almost certainly be offset by incentive hazards and further erosion of the perception that we are all in this together.</li><li>Self-sufficient space colonies that could protect against existential risks require technologies and skills that, if they existed, could be used more cheaply and reliably to create self-sustaining shelters on Earth. This will likely remain the case in the foreseeable future.</li><li>Even if a small group manages to survive a planetary catastrophe, only in some scenarios it is at all plausible that their descendants could repair the damages caused by any catastrophic outcome that the global society failed to prevent, and reconstitute the technological civilization.</li><li>Therefore, to save civilization, one needs to save society. The best lifeboat is the ship; the best shelter is a functional society. Increasing the resilience of societies and their capability for cooperative action would increase humanity\u2019s resilience against events that could cascade into existential risks while having obvious benefits in less dire circumstances as well.</li><li>Even though popular discussion about shelters tends to revolve around bunkers and stockpiles, the importance of organizational efforts, e.g. maintenance, training, and preparedness, cannot be overstated. No amount of material preparations or technology will help in a crisis if they do not work due to lack of maintenance, or if humans do not know how to use them. On the other hand, organizations that train to respond to disruptions can improvise even if they lack materials.</li><li>The solutions to the shelter problem are not primarily technological. As far as I\u2019m aware, no one has been able to identify any foreseeable technologies that would offer substantial improvements in societal resilience or otherwise provide a significant reduction in existential risk, although research into resilience-enhancing, \u201cresilient by default\u201d and \u201cgracefully failing\u201d technologies and practices should probably receive more funding than it currently does. However, even here the primary problem is not technical but economic: more resilient technologies and practices often exist already but they tend to be more expensive to buy or to use.</li><li>Longer-term research programs could nevertheless develop cost-effective ways to increase resilience against catastrophes and permit easier or faster recovery from a disaster. One obvious partner would be research into self-sustaining ecosystems for space colonization. A demonstration facility for the long-term feasibility of a closed-loop life support system would also double as a shelter, even if the small scale of such habitats and likely reliance on the \u201ciceberg\u201d of external technical support raises serious questions about the contribution they could provide for existential risk reduction.</li><li>Natural, accidental, or deliberate release of a dangerous pathogen(s) is widely seen as the threat with the most potential to precipitate an existential risk, although one should remember that the ongoing COVID-19 pandemic may bias this conclusion. A particularly worrisome prospect is the simultaneous, deliberate release of two or more pathogens, which could greatly confound the efforts to detect and contain the outbreak.</li><li>The SHELTER meeting participants seemed to broadly agree that with some exceptions, any single causal factor is unlikely to cause the extinction of humanity and is probably not sufficient to cause a catastrophic event. Instead, most existential risks and many catastrophic risks would probably be the result of several interacting mechanisms that e.g. prevent timely response to a risk that in theory should be manageable. Breakdown of the societal capability to act is thus a major risk multiplier. Single-cause risks that threaten human extinction, such as nearly omniscient AI god, are probably risks that shelters cannot realistically protect against.</li><li>Existing efforts in disaster management, particularly in countries with already robust civil defense/disaster response capability (Finland, Sweden, Switzerland etc.) could probably be augmented by relatively low-cost means to reduce the likelihood of major catastrophe(s) a) cascading to existential risks and/or b) leading to serious, irrecoverable loss of accumulated knowledge. Empirical validation of proposed means for improving resilience and the probability of recovery is necessary.</li><li>Two of the shelter strategies that seemed to gather the most support are a) hardening existing facilities identified as crucially important for reducing the likelihood of disasters cascading into catastrophes or existential risks, e.g. biomedical research and manufacturing facilities, and b) maintaining or even increasing the geographical and cultural diversity of humanity by supporting or even creating relatively isolated communities and helping them increase their resilience against biological threats in particular.&nbsp;</li><li>Maintaining human geographical and cultural diversity by supporting relatively isolated communities would be a no-regrets strategy that would increase resiliency&nbsp;<i>and&nbsp;</i>provide tangible benefits to typically underserved communities today.</li><li>Any strategy that is adopted must gain buy-in from the people who are involved. Gaining acceptance from the people is particularly important when supporting isolated communities, most of whom have very good reasons to be extremely wary of outsiders trying to \u201chelp\u201d them. A humble bottom-up approach that is guided by what the people themselves want and need is practically mandatory.</li></ol></blockquote><p>&nbsp;</p><p>I'm also interested in further collaboration on this and other x-risk topics, so feel free to reach out directly. My e-mail can be found in the report.</p><p>As this is also my first post after a long time lurking around the EA community, a brief introduction may be in order. I'm Janne from Finland. I'm currently a postdoc researcher at the Lappeenranta University of Technology and am supposed to be working on a report detailing how the experiences of wartime industrial mobilization could help accelerate the response to climate and other crises.&nbsp;</p><p>In practice, much of my time right now is spent on writing a treatise on societal power and its linkages to thermodynamical systems and the implications this has on what kinds of societies can be sustainable over the very long term. (TL;DR: very unequal societies, that is, where power is distributed very unequally, are, in my assessment, unlikely to survive the development of technologies required for long-term survival, space colonization included.)</p><p>Doing what I can to increase the probability of \"intelligent\" life's survival over the very long term has been my guiding passion since 2002. In the past, I've worked on e.g. sustainable design, energy systems research, climate and general sustainability issues, and technological substitution of scarce resources.&nbsp;</p><p>I also have some understanding of security policy and military matters and some contacts in Finnish expert and political circles. As I briefly note in the report, the Finnish \"comprehensive security\" policy could offer some examples of how to \"harden\" communities and increase resilience against risks that could spiral to catastrophic or even existential otherwise. The system of preparedness and shelters Finland has could probably also be improved at a reasonable cost to increase the probability of e.g. knowledge retention and thus help reduce existential risks. If you have ideas about that, please let me know, and I'll see if I can help you reach the right places.</p><p>Again, thanks to everyone involved, I hope the report is of some use to someone! &nbsp;</p>", "user": {"username": "Janne M. Korhonen"}}, {"_id": "8g2W7XpFdvRBdEiMm", "title": "Vegetarianism and depression", "postedAt": "2022-10-10T09:23:19.117Z", "htmlBody": "<p>I stumbled upon this paper <a href=\"https://www.sciencedirect.com/science/article/pii/S0165032722010643\">https://www.sciencedirect.com/science/article/pii/S0165032722010643</a></p><p>where authors found correlation between vegetarianism and depression episodes,&nbsp;</p><p>and would like to have a discussion about it. Did anybody already dig into it?</p>", "user": {"username": "Margarita Sharipova"}}, {"_id": "wAaXeXyzcy8JpEKAe", "title": "Results from the language model hackathon", "postedAt": "2022-10-10T08:29:06.371Z", "htmlBody": "", "user": {"username": "esben-kran"}}, {"_id": "sC3HKXmAeucymbDTk", "title": "AMC's animated series \"Pantheon\" is relevant to our interests", "postedAt": "2022-10-10T05:59:23.662Z", "htmlBody": "<p>Sorry for the vague title. I'm avoiding spoilers for those who care a lot about that sort of thing.</p><p>SPOILERS FOLLOW</p><hr><p>The series illustrates a fast take-off scenario. It tells the story of a fast-take-off scenario of \"uploaded intelligence\". Of course the usual fast take-off scenario this community is concerned about is artificial intelligence, not uploaded intelligence (i.e., digitized human intelligence as in Robin Hanson's <i>Age of Em</i>). But the series portrays the dangers of uploaded intelligence in a remarkably similar way to the concerns many people have about a fast AI take-off.</p><p>It feels very embedded in the state of the current world in 2022 in many ways; referencing the pandemic in ways most shows don't dare to; set in perhaps an alternative history where we weren't on the cusp of AGI, but where a Steve Jobs-like character had almost, but not quite, cracked the brain-uploading problem 20 years ago.</p><p>There are plenty of plot devices that don't feel realistic, but the interaction between technology and geopolitics, the exponential growth when AI take-off happens, the pivotal nature of very specific events guiding the direction of the take-off at the time it occurs--all that tracks, for me.</p><p>Worth watching as entertainment for yourself, and worth popularizing as a way to prime people out there on the dangers of a fast AI take-off in the near future.</p><p>That is all I have to say! Just putting the alert out there.</p>", "user": {"username": "ben.smith"}}, {"_id": "S3MNNb6BHv8ZyaKkj", "title": "EA aligned lawyers in Texas?", "postedAt": "2022-10-10T08:06:52.601Z", "htmlBody": "<p>Any EA aligned lawyers licensed to practice in the state of Texas?</p>\n<p>We're looking for someone willing to do short, scrappy pro Bono legal work. (Not more than one week). Involves litigation.</p>\n", "user": {"username": "Solomon Sia"}}, {"_id": "j7y8h9p8wCCZtHeak", "title": "Joey Savoie from Charity Entrepreneurship and His Principles -- OpenPrinciples Speaker Session", "postedAt": "2022-10-09T23:53:42.716Z", "htmlBody": "<p>Hi Everyone,&nbsp;</p><p><br>OpenPrinciples would like to invite you to join our 1hour Speaker &amp; Q&amp;A Session with Joey, a respected and principled individual in our EA community, to hear him talk about his journey to find his life principles and help the world more effectively with his principles. The first 30min will be his talk, and the second 30min will be Q&amp;A.&nbsp;&nbsp;</p><p><br>Joey is the Co-founder and Director of Strategy at&nbsp;<a href=\"https://www.charityentrepreneurship.com/\"><u>Charity Entrepreneurship</u></a> and the creator of Good Enough Answers (<a href=\"https://www.goodenoughanswers.com/\"><u>www.goodenoughanswers.com</u></a>), a project that works at finding good enough answers to life's infinite questions, from \"What should my values be?\" to \"What is the value of an electric toothbrush?\". On the website, he has also open-sourced 37 of his top life principles, inspired by&nbsp;<a href=\"https://www.linkedin.com/in/raydalio/\"><u>Ray Dalio</u></a> and his book&nbsp;<a href=\"https://www.amazon.co.uk/Principles-Life-and-Work/dp/B074B28CXV/ref=sr_1_1?crid=25022VFPVDLNN&amp;keywords=principles+ray+dalio&amp;qid=1643908054&amp;sprefix=principles%2Caps%2C515&amp;sr=8-1\"><u>Principles</u></a> -- the same source of inspiration behind OpenPrinciples.</p><p><br>OpenPrinciples is an EA Aligned organization led by current and previous organizers of EA IBM, EA UWaterloo, EA Taft, and EA Kathmandu University, and a community of like-minded people who like to open-source our own principles, build on each others' principles, and use those principles to make great decisions and make fewer repeated mistakes so we can help the world more effectively.&nbsp;</p><p><br>Examples of OpenPrinciples' projects include&nbsp;<a href=\"http://openprinciples.org\"><u>OpenPrinciples.org</u></a>, a crowd-sourced database of open-sourced life principles of our community members,<a href=\"https://openprinciples.notion.site/V1-4-OpenPrinciples-Principled-Individual-Template-28175992d4c841c291009f5f6a2c984f\">&nbsp;<u>Principled Individual Notion Template</u></a>, and&nbsp;<a href=\"http://ultrabrain.openprinciples.org/\"><u>Ultrabrain</u></a> Principles AI Assistant that helps people reflect, remind, and act on their life principles.</p>", "user": {"username": "gty3310"}}, {"_id": "8DtA57z9EyifD2wj5", "title": "AI Risk Microdynamics Survey", "postedAt": "2022-10-09T20:00:41.060Z", "htmlBody": "<p>If you are interested in AI Risk, could you kindly consider filling out a short (10 min) survey on AI Risk Microdynamics? The hope is that I will be able to use your responses to inform an economic model of that risk in the near future, which I think would fill an important gap in our understanding of AI Risk dynamics</p><p><a href=\"https://forms.gle/kLYtynp3FYcxkPZc8\">https://forms.gle/kLYtynp3FYcxkPZc8</a>&nbsp;</p>", "user": {"username": "Froolow"}}, {"_id": "DGL65TNvh9uqHuB8m", "title": "Possible miracles", "postedAt": "2022-10-09T18:17:01.503Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "saic4viqMBhPkrfaK", "title": "What should MIT analytics professors know about biosecurity?", "postedAt": "2022-10-09T15:58:30.930Z", "htmlBody": "<h1>Context</h1><p>I am giving a talk at my&nbsp;<a href=\"https://scholar.google.com/citations?user=prKmkzMAAAAJ&amp;hl=en\"><u>advisor</u></a>\u2019s workshop on the&nbsp;<a href=\"https://meetings.informs.org/wordpress/indianapolis2022/the-future-of-analytics-and-operations-research/\"><u>Future of Analytics and Operations Research</u></a> which will be well attended by MIT PhD students, professors, and alums from the operations research center (many of which work on healthcare analytics and were involved in COVID forecasting/response). I plan to talk about \u201cStrategic Priorities in Health Security\u201d as well as circulate a document which gives an overview of biosecurity and the problems best suited for people in my academic community (data science/analytics/OR; see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/kZ77cifvvNcwG56sH/why-ea-needs-operations-research-the-science-of-decision\"><u>why EA needs OR</u></a> for additional context).&nbsp;</p><h1>Goal and Questions</h1><p>My specific goal with this post is to solicit feedback from the biosecurity community to maximize the chance that this talk is able to be an entry point to bring new, talented researchers into impactful work on biosecurity.&nbsp; Feedback of any kind related to this goal is appreciated, but some specific questions are below:</p><ul><li>What is the most inspiring / scary / accurate / overall best way to help this audience quickly understand GCBRs and the work being done to mitigate them?</li><li>Do you have any suggestions for potential projects that could benefit in a material, mission-critical way from world-class analytics / OR research support<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqgfvfv0sn1\"><sup><a href=\"#fnqgfvfv0sn1\">[1]</a></sup></span>, or would be particularly good motivating case studies?</li><li>Is any of the existing OR research on biosecurity (examples&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/kZ77cifvvNcwG56sH/why-ea-needs-operations-research-the-science-of-decision#Biosecurity\"><u>here</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/X8dQckQ5W8w6JJGyB/ejor-special-issue-on-role-of-operational-research-in-future\"><u>here</u></a>) particularly useful? What would you like to see more or less of?</li><li>What is the best way to bring interested researchers further into the community?&nbsp; What\u2019s the best \u201csingle next action\u201d that someone interested in engaging more should take?</li><li>Do you work with or know of a team/org that needs research support of this kind now (or will soon) that could immediately benefit from collaborators at MIT?</li><li>Do you see any key risks or red flags that could result in this talk becoming infohazardous or otherwise counterproductive?</li></ul><p>My tentative thoughts for my talk and followup document are given below. If you would rather your answers be private, please <a href=\"mailto:wesgurnee@gmail.com\">email</a> me.</p><h1>Talk Outline</h1><p>I only have 8 minutes so I have to be concise. The overarching message is \u201cyou should be scared shitless of engineered pandemics\u2013here are some relevant problems and how they relate to things you have expertise in.\u201d</p><p>Tentative outline:</p><ul><li>Historical examples of near GCBRs</li><li>Investment in pandemic prevention/preparedness is woefully inadequate (especially in comparison to other forms of defense spending)</li><li>This is especially alarming when considering advances in biotech, eg:<ul><li>Dual use AI models</li><li>DNA sequencing/synthesis costs coming down at Moore\u2019s law speed</li><li>Gain of function research</li></ul></li><li>Introduce defense-in-depth and use as an organizational structure to introduce key problems for rest of talk<ul><li>Prevention<ul><li>Far-UVC: optimize allocation, model effectiveness vs. cost</li></ul></li><li>Monitoring/detection<ul><li>NAO design, operations, and data analysis</li></ul></li><li>Rapid response+preparedness<ul><li>Targeted testing, containment of affected areas</li></ul></li><li>Sustained response<ul><li>Vaccine manufacturing, allocation, and distribution</li></ul></li><li>Worst case resilience<ul><li>Facility location for refuges</li></ul></li></ul></li></ul><h1>Guide Outline</h1><p>This will be provided at the end of the talk. The goal is to give readers additional context and a large sample of relevant problems with enough detail and references to get people thinking in the right direction.</p><ul><li>Describe the motivation for biosecurity</li><li>Introduce key concepts<ul><li>Defense-in-depth</li><li>Dual-use research</li><li>Infohazards</li></ul></li><li>Relevant problems organized by depth (with more detailed problem statements forthcoming)<ul><li>Prevention<ul><li>Far-UVC: optimize placement, model effectiveness vs. cost</li><li>Optimize lab/facility inspection schedules</li></ul></li><li>Monitoring/detection<ul><li>NAO design, operations, and data analysis</li><li>Better models for disease spread</li></ul></li><li>Rapid response+preparedness<ul><li>Facility location and inventory management of strategic stockpiles</li><li>Integrated data sources to enable rapidly deploying medical countermeasures to areas of need</li><li>Targeted testing</li><li>Containment of affected areas</li></ul></li><li>Sustained response<ul><li>Hardened supply chains for testing, PPE, food<ul><li>Inventory management</li><li>Warm manufacturing capacity</li></ul></li><li>Vaccine manufacturing, allocation, and distribution</li><li>Clinical trial site identification</li></ul></li><li>Worst case resilience<ul><li>Facility location for refuges</li><li>Food stockpiles and supply chain hardening of most basic survival goods</li></ul></li></ul></li><li>Emphasize discretion and infohazards one last time</li><li>Links to relevant orgs in the space and further list of references</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqgfvfv0sn1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqgfvfv0sn1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>OR is primarily an applied discipline, and therefore OR academia is unusually supportive of purely applied research projects. A good (non biosec) recent example is&nbsp;<a href=\"https://pubsonline.informs.org/doi/abs/10.1287/inte.2021.1097\"><u>here</u></a> where an academic OR collaboration saved the UN World Food Program $150 million/year in logistics costs.</p></div></li></ol>", "user": {"username": "Wes Gurnee"}}, {"_id": "EixrWYgkox7noLacn", "title": "Probability of extinction for various types of catastrophes", "postedAt": "2022-10-09T15:30:44.971Z", "htmlBody": "<h1>Summary</h1><ul><li>I estimated and studied the probability of extinction for various types of catastrophe in the 21st century in <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1aMdZeKeYK1P5y8hUpiCq5HKlknQvI4ZlgLe_T69_WVU/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1665325154019999&amp;usg=AOvVaw0UomSEeoDjH_p3tGfQyURy\"><u>this</u></a>&nbsp;Sheet<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftdc43jnwg6\"><sup><a href=\"#fntdc43jnwg6\">[1]</a></sup></span>. The results for the probability of extinction are in the table below.<ul><li>The inputs are predictions from Metaculus\u2019 <a href=\"https://www.google.com/url?q=https://www.metaculus.com/questions/?search%3Dcat:series--ragnarok&amp;sa=D&amp;source=editors&amp;ust=1665325154020588&amp;usg=AOvVaw0e06Fm9lrHtUOiwvd1Gbb9\"><u>Ragnarok Series</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgjgc5emnyyl\"><sup><a href=\"#fngjgc5emnyyl\">[2]</a></sup></span>, and guesses from me, and provided by Luisa Rodriguez <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would&amp;sa=D&amp;source=editors&amp;ust=1665325154020885&amp;usg=AOvVaw3OAkbVb938m5u9YR-CssLl\"><u>here</u></a>.</li></ul></li><li>Relative to Toby Ord\u2019s best guesses for the existential risk from 2021 to 2120 given in <a href=\"https://theprecipice.com/\">The Precipice</a>, my analysis suggests the relative importance of:<ul><li>Artificial intelligence is similar (rounded to half an order of magnitude).</li><li>Climate change and geoengineering, synthetic biology, and \u201cother\u201d is half an order of magnitude lower.</li><li>Nuclear war is one order of magnitude higher.</li></ul></li></ul><figure class=\"table\"><table><thead><tr><th><strong>Type of catastrophe (in the 21st century)</strong></th><th style=\"text-align:center\">Probability of extinction (%)</th></tr></thead><tbody><tr><td><strong>Any*</strong></td><td style=\"text-align:center\">4.26</td></tr><tr><td><strong>Any</strong></td><td style=\"text-align:center\">1.77</td></tr><tr><td><strong>Artificial intelligence</strong></td><td style=\"text-align:center\">2.84</td></tr><tr><td><strong>Climate change and geoengineering</strong></td><td style=\"text-align:center\">0.0106</td></tr><tr><td><strong>Nanotechnology</strong></td><td style=\"text-align:center\">0.245</td></tr><tr><td><strong>Nuclear war</strong></td><td style=\"text-align:center\">0.299</td></tr><tr><td><strong>Synthetic biology</strong></td><td style=\"text-align:center\">0.220</td></tr><tr><td><strong>Other</strong></td><td style=\"text-align:center\">0.695</td></tr></tbody></table></figure><h2>Acknowledgements</h2><p>Thanks to David Denkenberger, Eli Lifland, Gregory Lewis, Misha Yagudin, Nu\u00f1o Sempere, and Tamay Besiroglu.</p><h1>Methods</h1><p>I calculated the probability of extinction for catastrophes in the 21st century caused by:</p><ul><li>Artificial intelligence.</li><li>Climate change and geoengineering.</li><li>Nanotechnology.</li><li>Nuclear war.</li><li>Synthetic biology</li><li>Other.</li><li>Any.</li><li>Any*.</li></ul><p>The results for \u201cany\u201d do not explicitly depend on those of the 6 1st types of catastrophe mentioned above, whereas those for \u201cany*\u201d are calculated assuming independence between them.</p><p>The inputs to the calculations are:</p><ul><li>Metaculus\u2019 community predictions for the question of the <a href=\"https://www.google.com/url?q=https://www.metaculus.com/questions/?search%3Dcat:series--ragnarok&amp;sa=D&amp;source=editors&amp;ust=1665325154027407&amp;usg=AOvVaw0mesHAwU7zG59CFin7fJbV\"><u>Ragnarok Series</u></a>, which ask about&nbsp;the probability of a population loss of 10 % and 95 % for various types of catastrophes during the 21st century.</li><li>Luisa Rodriguez\u2019 guesses for <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would&amp;sa=D&amp;source=editors&amp;ust=1665325154027928&amp;usg=AOvVaw1XFLGSbb_PIH3-Pvfe2m-F\"><u>what is the likelihood that civilizational collapse would directly lead to human extinction (within decades)</u></a>, and one related guess of mine.</li><li>My guesses for the probability of various scenarios of major infrastructure damage and climate change.</li></ul><p>Concretely, I calculated the probability of extinction from the sum of the following 3 products (see tab \u201cProbability of extinction by catastrophe\u201d of <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1aMdZeKeYK1P5y8hUpiCq5HKlknQvI4ZlgLe_T69_WVU/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1665325154028508&amp;usg=AOvVaw2Hk4sthNx-zivsnr4QFkOn\"><u>this</u></a>&nbsp;Sheet):</p><ul><li>The population loss being between 0 and 10 %, and the sum of the products between the probability of extinction given a population loss between 0 and 10 % under a certain scenario and its respective probability.</li><li>The population loss being between 10 % and 95 %, and the sum of the products between the probability of extinction given a population loss between 10 % and 95 % under a certain scenario and its respective probability.</li><li>The population loss being between 95 % and 1, and the sum of the products between the probability of extinction given a population loss between 95 % and 1 under a certain scenario and its respective probability.</li></ul><h2>Population loss</h2><p>I computed the probability of the population loss falling into each of the 3 population loss ranges presented above based on the complementary cumulative distribution function (<a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Cumulative_distribution_function%23Complementary_cumulative_distribution_function_(tail_distribution)&amp;sa=D&amp;source=editors&amp;ust=1665325154029535&amp;usg=AOvVaw0I2KByzARDec6LVsjLOzRh\"><u>CCDF</u></a>) of the population loss (see tab \u201cProbability of the population loss\u201d). I assumed the CCDF decreases linearly between each consecutive pair of the following points<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefel86w4guba\"><sup><a href=\"#fnel86w4guba\">[3]</a></sup></span>&nbsp;(see tab \u201cCCDF of the population loss\u201d):</p><ul><li>Population loss of 0, and CCDF of 1, i.e. probability 1 of the catastrophe decreasing the population size.</li><li>Population loss of 10 %, and CCDF given by the product between:<ul><li>The probability of a population loss greater than 10 %.</li><li>The probability of such population loss being caused by a certain type of catastrophe, given that it occurred.</li></ul></li><li>Population loss of 95 %, and CCDF given by the product between:<ul><li>Population loss of 10 % caused by a certain type of catastrophe, which equals the product just above.</li><li>The probability of a population loss of 95 % being caused by a certain type of catastrophe, given that a population loss of 10 % caused by that type of catastrophe occurred.</li></ul></li><li>Population loss of 1, and CCDF of 0, i.e. probability 0 of the population after the catastrophe being negative.</li></ul><p>I set the probabilities required to determine the CCDF for the population losses of 10 % and 95 % to Metaculus\u2019 community predictions (collected in tab \u201cMetaculus' predictions\u201d).</p><h2>Probability of extinction</h2><p>I calculated the probability of extinction for each of the 3 population loss ranges presented above for 3 exhaustive scenarios (see tab \u201cProbability of extinction by scenario\u201d):</p><ul><li>Without major infrastructure damage nor (major) climate change.</li><li>With major infrastructure damage and (major) climate change.</li><li>With either major infrastructure damage or (major) climate change.</li></ul><p>To illustrate what is intended by \u201cmajor infrastructure damage\u201d and \u201cmajor climate change\u201d, Luisa <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would&amp;sa=D&amp;source=editors&amp;ust=1665325154032210&amp;usg=AOvVaw2l0Ps7-hg3pl9IbjOZetiM\"><u>writes</u></a>:</p><ul><li>\u201cMajor infrastructure damage\u201d: \u201ce.g. damaged roads, destroyed bridges, collapsed buildings, damaged power lines\u201d.</li><li>\u201cMajor climate change\u201d: \u201ce.g. nuclear winter\u201d.</li></ul><p>For the 1st and 2nd scenarios, I determined the probability of extinction from its mean value for each of the population loss ranges. For the 3rd one, I computed it from the geometric mean between the values for the 1st and 2nd scenarios.</p><p>I supposed the probability of extinction as a function of the population loss to increase linearly between each consecutive pair of the following points:</p><ul><li>Without major infrastructure damage nor climate change:<ul><li>Population loss of 0, and probability of extinction of 0.</li><li>Population loss of 50 %, and probability of extinction of PE_1 = (0*0.0001)^0.5 = 0.</li><li>Population loss of PL = 1 - 10^-5.5 = 99.9997 %, and probability of extinction of 50 %.</li><li>Population loss of 99.99 %, and probability of extinction of 0.173 %, which I estimated from 1 % of the probability of extinction for the same population loss, but with major infrastructure damage and climate change.</li><li>Population loss of 1, and probability of extinction of 1.</li></ul></li><li>With major infrastructure damage and climate change:<ul><li>Population loss of 0, and probability of extinction of 0.</li><li>Population loss of 90 %, and probability of extinction of PE_2 = 10^-1.5&nbsp;= 3.16 %.</li><li>Population loss of 99.99 %, and probability of extinction of PE_3 = (0.1*0.3)^0.5&nbsp;= 17.3 %.</li><li>Population loss of 1, and probability of extinction of 1.</li></ul></li></ul><p>PE_1, PL, PE_2 and PE_3 are the geometric means between the lower and upper bounds of the best guesses provided by Luisa <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would&amp;sa=D&amp;source=editors&amp;ust=1665325154035176&amp;usg=AOvVaw2qgzzN5stNtyuBdY1ZiD5F\"><u>here</u></a>:</p><ul><li>For the population loss of 50 % without major infrastructure damage nor climate change (PE_1):<ul><li>\u201cCase 1: I [Luisa] think it\u2019s exceedingly unlikely [probability \u201c&lt; 0.0001\u201d, i.e. between 0 and 0.0001; see 1st table] that humanity would go extinct (within ~a generation) as a direct result of a catastrophe that causes the deaths of 50% of the world\u2019s population, but causes no major infrastructure damage (e.g. damaged roads, destroyed bridges, collapsed buildings, damaged power lines, etc.) or extreme changes in the climate (e.g. cooling)\u201d.</li></ul></li><li>For the probability of extinction of 50% without major infrastructure damage nor climate change (PL):<ul><li>\u201cMy [Luisa\u2019s] best guess is that the turning point at which extinction goes from under 50% to over 50% is between 99.999% population death (80,000) and 99.9999% (8,000) population death (even before considering additional starting conditions like infrastructure damage or climate change)\u201d.</li></ul></li><li>For the population loss of 90 % with major infrastructure damage and climate change (PE_2):<ul><li>\u201cCase 2: I [Luisa] think it\u2019s very unlikely [probability \u201cbetween 0.01 and 0.1\u201d] that humanity would go extinct as a direct result of a catastrophe that caused the deaths of 90% of the world\u2019s population (leaving 800 million survivors), major infrastructure damage, and severe climate change (e.g. nuclear winter/asteroid impact)\u201d.</li></ul></li><li>For the population loss of 99.99 % with major infrastructure damage and climate change (PE_3):<ul><li>\u201cCase 3: I [Luisa] think it\u2019s fairly unlikely [probability \u201cbetween 0.1 and 0.3\u201d] that humanity would go extinct as a direct result of a catastrophe that caused the deaths of 99.99% of people (leaving 800 thousand survivors), extensive infrastructure damage, and temporary climate change (e.g. a more severe nuclear winter/asteroid impact, plus the use of biological weapons)\u201d.</li></ul></li></ul><h2>Probability of the scenarios</h2><p>My guesses for the probability of each of the 3 scenarios defined in the previous section given a population loss caused by a certain type of catastrophe is in the table below (and in tab \u201cProbability of extinction scenarios by catastrophe\u201d). I calculated the probability for the type \u201cother\u201d from the mean of the probability for the other types of catastrophes (excluding \u201cany\u201d), and the one for the type \u201cany\u201d from the mean of the probability of the various types weighted by their probability of leading to a population between 95 % and 1.</p><figure class=\"table\"><table><thead><tr><th rowspan=\"2\"><strong>Type of catastrophe (in the 21st century)</strong></th><th style=\"text-align:center\" colspan=\"3\">Probability of scenario given a population loss</th></tr><tr><th style=\"text-align:center\">No major infrastructure damage nor climate change</th><th style=\"text-align:center\">Major infrastructure damage and climate change</th><th style=\"text-align:center\">Either major infrastructure damage or climate change</th></tr></thead><tbody><tr><td><strong>Any</strong></td><td style=\"text-align:center\">29.2 %</td><td style=\"text-align:center\">22.5 %</td><td style=\"text-align:center\">48.3 %</td></tr><tr><td><strong>Artificial intelligence</strong></td><td style=\"text-align:center\">1/4</td><td style=\"text-align:center\">1/4</td><td style=\"text-align:center\">1/2</td></tr><tr><td><strong>Climate change and geoengineering</strong></td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">1</td></tr><tr><td><strong>Nanotechnology</strong></td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">1/3</td><td style=\"text-align:center\">2/3</td></tr><tr><td><strong>Nuclear war</strong></td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">1/3</td><td style=\"text-align:center\">2/3</td></tr><tr><td><strong>Synthetic biology</strong></td><td style=\"text-align:center\">1</td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">0</td></tr><tr><td><strong>Other</strong></td><td style=\"text-align:center\">25.0 %</td><td style=\"text-align:center\">18.3 %</td><td style=\"text-align:center\">56.7 %</td></tr></tbody></table></figure><h1>Results</h1><p>The tables below contain the results for:</p><ul><li>The probability of the population loss ranges by type of catastrophe.</li><li>The probability of extinction by population loss range and scenario.</li><li>The probability of extinction by population loss and type of catastrophe.</li></ul><figure class=\"table\"><table><thead><tr><th rowspan=\"2\"><strong>Type of catastrophe (in the 21st century)</strong></th><th style=\"text-align:center\" colspan=\"3\">Probability (%) of a population loss between\u2026</th></tr><tr><th style=\"text-align:center\">0 to 10 %</th><th style=\"text-align:center\">10 % to 95 %</th><th style=\"text-align:center\">95 % to 1</th></tr></thead><tbody><tr><td><strong>Any*</strong></td><td style=\"text-align:center\">100</td><td style=\"text-align:center\">25.3</td><td style=\"text-align:center\">10.3</td></tr><tr><td><strong>Any</strong></td><td style=\"text-align:center\">68.0</td><td style=\"text-align:center\">27.8</td><td style=\"text-align:center\">4.16</td></tr><tr><td><strong>Artificial intelligence</strong></td><td style=\"text-align:center\">90.4</td><td style=\"text-align:center\">2.40</td><td style=\"text-align:center\">7.20</td></tr><tr><td><strong>Climate change and geoengineering</strong></td><td style=\"text-align:center\">98.4</td><td style=\"text-align:center\">1.58</td><td style=\"text-align:center\">0.0160</td></tr><tr><td><strong>Nanotechnology</strong></td><td style=\"text-align:center\">99.0</td><td style=\"text-align:center\">0.538</td><td style=\"text-align:center\">0.422</td></tr><tr><td><strong>Nuclear war</strong></td><td style=\"text-align:center\">90.4</td><td style=\"text-align:center\">9.22</td><td style=\"text-align:center\">0.384</td></tr><tr><td><strong>Synthetic biology</strong></td><td style=\"text-align:center\">90.4</td><td style=\"text-align:center\">8.74</td><td style=\"text-align:center\">0.864</td></tr><tr><td><strong>Other</strong></td><td style=\"text-align:center\">92.6</td><td style=\"text-align:center\">5.67</td><td style=\"text-align:center\">1.69</td></tr></tbody></table></figure><figure class=\"table\"><table><thead><tr><th rowspan=\"2\"><strong>Scenario</strong></th><th style=\"text-align:center\" colspan=\"3\"><strong>Probability of extinction (%) for a population loss between\u2026</strong></th></tr><tr><th style=\"text-align:center\">0 to 10 %</th><th style=\"text-align:center\">10 % to 95 %</th><th style=\"text-align:center\">95 % to 1</th></tr></thead><tbody><tr><td><strong>No major infrastructure damage nor climate change</strong></td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">0.0413</td><td style=\"text-align:center\">25.1</td></tr><tr><td><strong>Major infrastructure damage and climate change</strong></td><td style=\"text-align:center\">0.176</td><td style=\"text-align:center\">2.05</td><td style=\"text-align:center\">55.1</td></tr><tr><td><strong>Either major infrastructure damage or climate change</strong></td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">0.291</td><td style=\"text-align:center\">37.2</td></tr></tbody></table></figure><figure class=\"table\"><table><thead><tr><th rowspan=\"2\"><strong>Type of catastrophe (in the 21st century)</strong></th><th style=\"text-align:center\" colspan=\"4\">Probability of extinction (%) for a population loss between\u2026</th></tr><tr><th style=\"text-align:center\">0 to 10 %</th><th style=\"text-align:center\">10 % to 95 %</th><th style=\"text-align:center\">95 % to 1</th><th style=\"text-align:center\">0 to 1 (total)</th></tr></thead><tbody><tr><td><strong>Any*</strong></td><td style=\"text-align:center\">0.180</td><td style=\"text-align:center\">0.141</td><td style=\"text-align:center\">3.95</td><td style=\"text-align:center\">4.26</td></tr><tr><td><strong>Any</strong></td><td style=\"text-align:center\">0.0269</td><td style=\"text-align:center\">0.171</td><td style=\"text-align:center\">1.57</td><td style=\"text-align:center\">1.77</td></tr><tr><td><strong>Artificial intelligence</strong></td><td style=\"text-align:center\">0.0397</td><td style=\"text-align:center\">0.0160</td><td style=\"text-align:center\">2.78</td><td style=\"text-align:center\">2.84</td></tr><tr><td><strong>Climate change and geoengineering</strong></td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">0.00461</td><td style=\"text-align:center\">0.00595</td><td style=\"text-align:center\">0.0106</td></tr><tr><td><strong>Nanotechnology</strong></td><td style=\"text-align:center\">0.0580</td><td style=\"text-align:center\">0.00471</td><td style=\"text-align:center\">0.182</td><td style=\"text-align:center\">0.245</td></tr><tr><td><strong>Nuclear war</strong></td><td style=\"text-align:center\">0.0529</td><td style=\"text-align:center\">0.0808</td><td style=\"text-align:center\">0.166</td><td style=\"text-align:center\">0.299</td></tr><tr><td><strong>Synthetic biology</strong></td><td style=\"text-align:center\">0</td><td style=\"text-align:center\">0.00361</td><td style=\"text-align:center\">0.217</td><td style=\"text-align:center\">0.220</td></tr><tr><td><strong>Other</strong></td><td style=\"text-align:center\">0.0298</td><td style=\"text-align:center\">0.0312</td><td style=\"text-align:center\">0.634</td><td style=\"text-align:center\">0.695</td></tr></tbody></table></figure><h1>Discussion</h1><h2>Probability of extinction by scenario</h2><p>The relative importance of major infrastructure damage and climate change decreases as the severity of the population loss increases. The ratio between the probability of extinction without major infrastructure damage nor climate change and the probability of extinction with both is (see cells F3:F5 of tab \u201cProbability of extinction by scenario\u201d):</p><ul><li>0 for a population loss between 0 and 10 %.</li><li>2.02 % for a population loss between 10 % and 95 %.</li><li>45.5 % for a population loss between 95 % and 1.</li></ul><p>This tendency seems correct, as the probability of extinction is 1 for a population loss of 1 regardless of infrastructure damage and climate change.</p><h2>Probability of extinction by type of catastrophe</h2><h3>Comparison of absolute values with the GCRS</h3><p>In the table below (and in tab \u201cComparison of absolute values with the GCRS\u201d), I compare the probability of extinction by type of catastrophe in the 21st century I estimated with ones I derived from the 2008 Global Catastrophic Risks Survey (GCRS), whose results are presented in <a href=\"https://www.google.com/url?q=https://www.fhi.ox.ac.uk/reports/2008-1.pdf&amp;sa=D&amp;source=editors&amp;ust=1665325154075887&amp;usg=AOvVaw3Z7sO_AMb0FNYRAPyZwH1s\"><u>this</u></a>&nbsp;report by Anders Sandberg and Toby Ord from the <a href=\"https://www.google.com/url?q=https://www.fhi.ox.ac.uk/&amp;sa=D&amp;source=editors&amp;ust=1665325154076249&amp;usg=AOvVaw3p8Wafpr1Q65sad4yJw2OU\"><u>Future of Humanity Institute</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjupcj5rcv89\"><sup><a href=\"#fnjupcj5rcv89\">[4]</a></sup></span>&nbsp;(see tab \u201c2008 Global Catastrophic Risks Survey\u201d). The GCRS estimates refer to the period from 2009 to 2099, but I adjusted them to the period from 2023 to 2100 assuming constant risk. Additionally, I derived GCRS\u2019 estimate for \u201cother\u201d risks assuming independence between the types of catastrophes<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsbztds24sw\"><sup><a href=\"#fnsbztds24sw\">[5]</a></sup></span>.</p><figure class=\"table\"><table><thead><tr><th rowspan=\"2\"><strong>Type of catastrophe (in the 21st century)</strong></th><th style=\"text-align:center\" colspan=\"4\">Probability of extinction (%) for a population loss between\u2026</th></tr><tr><th style=\"text-align:center\">My analysis&nbsp;(%)</th><th style=\"text-align:center\">GCRS&nbsp;(%)</th><th style=\"text-align:center\">Absolute difference to GCRS&nbsp;(pp)</th><th style=\"text-align:center\">Relative difference to GCRS&nbsp;(%)</th></tr></thead><tbody><tr><td><strong>Any*</strong></td><td style=\"text-align:center\">4.26</td><td style=\"text-align:center\">16.5</td><td style=\"text-align:center\">-12.3</td><td style=\"text-align:center\">-74.2</td></tr><tr><td><strong>Any</strong></td><td style=\"text-align:center\">1.77</td><td style=\"text-align:center\">16.5</td><td style=\"text-align:center\">-14.8</td><td style=\"text-align:center\">-89.3</td></tr><tr><td><strong>Artificial intelligence</strong></td><td style=\"text-align:center\">2.84</td><td style=\"text-align:center\">4.30</td><td style=\"text-align:center\">-1.46</td><td style=\"text-align:center\">-34.0</td></tr><tr><td><strong>Nanotechnology</strong></td><td style=\"text-align:center\">0.245</td><td style=\"text-align:center\">4.30</td><td style=\"text-align:center\">-4.06</td><td style=\"text-align:center\">-94.3</td></tr><tr><td><strong>Nuclear war</strong></td><td style=\"text-align:center\">0.299</td><td style=\"text-align:center\">0.858</td><td style=\"text-align:center\">-0.558</td><td style=\"text-align:center\">-65.1</td></tr><tr><td><strong>Synthetic biology</strong></td><td style=\"text-align:center\">0.220</td><td style=\"text-align:center\">1.72</td><td style=\"text-align:center\">-1.50</td><td style=\"text-align:center\">-87.2</td></tr><tr><td><strong>Other</strong></td><td style=\"text-align:center\">0.695</td><td style=\"text-align:center\">6.46</td><td style=\"text-align:center\">-5.76</td><td style=\"text-align:center\">-89.2</td></tr></tbody></table></figure><p>My probabilities of extinction are lower than those I derived from the GCRS for all types of catastrophe. Nanotechnology has the largest relative difference, and artificial intelligence the smallest.</p><p>The GCRS did not address \u201cclimate change and geoengineering\u201d, but my estimate of 0.0106 % is similar to:</p><ul><li>10 % of the best guess of 0.1 % mentioned by Toby Ord in <a href=\"https://www.google.com/url?q=https://theprecipice.com/&amp;sa=D&amp;source=editors&amp;ust=1665325154087654&amp;usg=AOvVaw0ubp0HgWqKpvObIyDF5EX8\"><u>The Precipice</u></a>&nbsp;for the <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/existential-risk&amp;sa=D&amp;source=editors&amp;ust=1665325154087979&amp;usg=AOvVaw3YndvdrDZBT2yeKvXCXq-b\"><u>existential risk</u></a>&nbsp;due to climate change from 2021 to 2120 (see Table 6.1).</li><li>10 % of the upper bound of 0.1 %, and 10 times the best guess of 0.001 % mentioned <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report&amp;sa=D&amp;source=editors&amp;ust=1665325154088402&amp;usg=AOvVaw1suv4xfcaaJJARf69LkPAs\"><u>here</u></a>&nbsp;by John Halstead for the existential risk due to climate change<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0jnn6st24vdv\"><sup><a href=\"#fn0jnn6st24vdv\">[6]</a></sup></span>.</li><li>The upper bound of 0.01 % guessed by 80,000 Hours <a href=\"https://www.google.com/url?q=https://80000hours.org/problem-profiles/climate-change/%23summing-up-how-climate-change-makes-global-catastrophic-risks-worse&amp;sa=D&amp;source=editors&amp;ust=1665325154088836&amp;usg=AOvVaw2K1GZlJAz0Z3wTZmXICiQ7\"><u>here</u></a>&nbsp;for the existential risk due to climate change<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrtabtkm1k3\"><sup><a href=\"#fnrtabtkm1k3\">[7]</a></sup></span>.</li></ul><h3>Comparison of priorities with The Precipice</h3><p>Ultimately, what is the most relevant for prioritisation is how the various probabilities compare with each other. Having this in mind, in the table below (and in tab \u201cComparison of priorities with The Precipice\u201d), I present the probability of extinction in the 21st century as a fraction of that for \u201cany*\u201d, and the existential risk between 2021 and 2120 guessed by Toby Ord in <a href=\"https://www.google.com/url?q=https://theprecipice.com/&amp;sa=D&amp;source=editors&amp;ust=1665325154089369&amp;usg=AOvVaw0z8TCVW8I0YuCzDyROnE-u\"><u>The Precipice</u></a>&nbsp;(see tab \u201cExistential risk estimates from The Precipice\u201d) as a fraction of the total. The existential risk for \u201cother\u201d was estimated from those for \u201cunforeseen anthropogenic risk\u201d and \u201cother anthropogenic risk\u201d assuming independence between them.</p><figure class=\"table\"><table><thead><tr><th>Type of catastrophe</th><th style=\"text-align:center\">Normalised probability of extinction for a catastrophe in the 21st century (%)</th><th style=\"text-align:center\">Normalised existential risk from 2021 to 2120 (%)</th><th style=\"text-align:center\">Ratio</th><th style=\"text-align:center\">Decimal logarithm of the ratio</th></tr></thead><tbody><tr><td><strong>Artificial intelligence</strong></td><td style=\"text-align:center\">66.6</td><td style=\"text-align:center\">60.0</td><td style=\"text-align:center\">1.11</td><td style=\"text-align:center\">0.0455</td></tr><tr><td><strong>Climate change and geoengineering</strong></td><td style=\"text-align:center\">0.248</td><td style=\"text-align:center\">0.600</td><td style=\"text-align:center\">0.413</td><td style=\"text-align:center\">-0.384</td></tr><tr><td><strong>Nuclear war</strong></td><td style=\"text-align:center\">7.03</td><td style=\"text-align:center\">0.600</td><td style=\"text-align:center\">11.7</td><td style=\"text-align:center\">1.07</td></tr><tr><td><strong>Synthetic biology</strong></td><td style=\"text-align:center\">5.17</td><td style=\"text-align:center\">20.0</td><td style=\"text-align:center\">0.259</td><td style=\"text-align:center\">-0.587</td></tr><tr><td><strong>Other</strong></td><td style=\"text-align:center\">16.3</td><td style=\"text-align:center\">31.6</td><td style=\"text-align:center\">0.516</td><td style=\"text-align:center\">-0.287</td></tr></tbody></table></figure><p>Relative to Toby Ord\u2019s best guesses, my analysis suggests the relative importance of:</p><ul><li>Artificial intelligence is similar (rounded to half an order of magnitude).</li><li>Climate change and geoengineering, synthetic biology, and \u201cother\u201d is half an order of magnitude lower.</li><li>Nuclear war is one order of magnitude higher.</li></ul><p>The adequacy of this comparison depends on the extent to which probability of extinction is a good proxy for existential risk.</p><h2>Quality of the inputs</h2><p>In essence, the results I obtained are a function of guesses from Metaculus\u2019 forecasters, Luisa Rodriguez, and me. I should note there is margin to improve the quality of the inputs:</p><ul><li>Regarding Metaculus:<ul><li>Eli Lifland, Gregory Lewis, Misha Yagudin, and Nu\u00f1o Sempere from <a href=\"https://www.google.com/url?q=https://samotsvety.org/&amp;sa=D&amp;source=editors&amp;ust=1665325154097409&amp;usg=AOvVaw0G5yjHFJ-MfFv2dz5e29Rl\"><u>Samotsvety Forecasting</u></a>&nbsp;(and presumably other superforecasters) expressed concerns about relying on Metaculus\u2019 community predictions<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefih2dstcei0c\"><sup><a href=\"#fnih2dstcei0c\">[8]</a></sup></span>.</li><li>I also noticed these are internally inconsistent:<ul><li>The probability of a population loss greater than 95 % due to \u201cany\u201d catastrophe is lower than that due to artificial intelligence (4.16 % &lt; 7.20 %; see cells B5:C5 of tab \u201cCCDF of the population loss\u201d).</li><li>This leads to the probability of extinction due to \u201cany\u201d being lower than that due to artificial intelligence (1.77 % &lt; 2.84 %; see cells D6:E6 of tab \u201cProbability of extinction by catastrophe\u201d).</li></ul></li><li>However, I do not know about other forecasts looking into population losses by catastrophe such as <a href=\"https://www.google.com/url?q=https://www.metaculus.com/questions/2568/ragnar%2525C3%2525B6k-seriesresults-so-far/&amp;sa=D&amp;source=editors&amp;ust=1665325154098309&amp;usg=AOvVaw2EPzYrtnl_Ntzn3s8vjIWb\"><u>Metaculus' Ragnarok series</u></a>.</li></ul></li><li>Luisa\u2019s <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would&amp;sa=D&amp;source=editors&amp;ust=1665325154098716&amp;usg=AOvVaw102D5L96yDffDuneD4Qijl\"><u>analysis</u></a>&nbsp;is great, but \u201ca first step toward understanding this threat from civilizational collapse \u2014 not a final or decisive one\u201d.</li><li>I am not a forecaster, and merely based my guesses on my previous knowledge.</li></ul><p>That being said, for the reasons outlined by Scott Alexander <a href=\"https://www.google.com/url?q=https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/&amp;sa=D&amp;source=editors&amp;ust=1665325154099279&amp;usg=AOvVaw0vglqnH77njFn6hv-WQTfZ\"><u>here</u></a>, I believe establishing priorities based on a quantitative model with guessed inputs is often better than guessing priorities.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntdc43jnwg6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftdc43jnwg6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;To clarify, the probability refers to catastrophes occurring during the 21st century, but the extinction may happen afterwards.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngjgc5emnyyl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgjgc5emnyyl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The results in the Sheet are updated automatically as the Metaculus\u2019 predictions change.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnel86w4guba\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefel86w4guba\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This implies the probability density function (<a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Probability_density_function&amp;sa=D&amp;source=editors&amp;ust=1665325154099694&amp;usg=AOvVaw2wX7nNl19R0aNVjsS-JAQd\"><u>PDF</u></a>) of the population loss is uniform for each of the 3 ranges.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjupcj5rcv89\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjupcj5rcv89\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;More existential risk estimates are available in <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1W10B6NJjicD8O0STPiT3tNV3oFnT8YsfjmtYR8RO_RI/edit%23gid%3D0&amp;sa=D&amp;source=editors&amp;ust=1665325154100328&amp;usg=AOvVaw324o0fnxaXd6P7MQFRX1J-\"><u>this</u></a>&nbsp;database, which was introduced by Michael Aird <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/JQQAQrunyGGhzE23a/database-of-existential-risk-estimates&amp;sa=D&amp;source=editors&amp;ust=1665325154100590&amp;usg=AOvVaw3ZmsGk4-Ts5q2e5WrLrlca\"><u>here</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsbztds24sw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsbztds24sw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;This implies the GCRS\u2019 estimates for \u201cany*\u201d are the same as for \u201cany\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0jnn6st24vdv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0jnn6st24vdv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cWith those caveats in my mind, my best guess estimate is that the indirect risk of existential catastrophe due to climate change is on the order of 1 in 100,000, and I struggle to get the risk above 1 in 1,000. Working directly on US-China, US-Russia, India-China, or India-Pakistan relations seems like a better way to reduce the risk of Great Power War than working on climate change\u201d. I guess John\u2019s best guess for the total risk of existential catastrophe due to climate change is similar to John\u2019s best guess for the indirect risk, which equals John\u2019s upper bound for the direct risk: \u201cI [John] construct several models of the direct extinction risk from climate change but struggle to get the risk above 1 in 100,000 over all time\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrtabtkm1k3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrtabtkm1k3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cThat said, we [80,000 Hours] still think this risk is relatively low. If climate change poses something like a 1 in 1,000,000 risk of extinction by itself, our guess is that its contribution to other existential risks is at most a few orders of magnitude higher \u2014 so something like 1 in 10,000\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnih2dstcei0c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefih2dstcei0c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Metaculus\u2019 predictions are at the bottom of Eli\u2019s personal tier <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/EG9xDM8YRz4JN4wMN/samotsvety-s-ai-risk-forecasts%23How_much_weight_should_we_give_to_these_aggregates_&amp;sa=D&amp;source=editors&amp;ust=1665325154101276&amp;usg=AOvVaw3Y80Ov2VXxyXjsk1JZtw3A\"><u>list</u></a>&nbsp;for how much weight to give to AI existential risk forecasts (see <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/EG9xDM8YRz4JN4wMN/samotsvety-s-ai-risk-forecasts%23fnv6pyn4x7wr8&amp;sa=D&amp;source=editors&amp;ust=1665325154101526&amp;usg=AOvVaw0hHgha0yITsIz6pNNfdqk7\"><u>this</u></a>&nbsp;footnote for details).</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "PYrFMq4Avq2DbqFzv", "title": "Orebro LW meetup!", "postedAt": "2022-10-09T16:27:18.011Z", "htmlBody": "", "user": {"username": "Marina"}}, {"_id": "FtggfJ2oxNSN8Niix", "title": "When reporting AI timelines, be clear who you're deferring to", "postedAt": "2022-10-10T14:24:14.428Z", "htmlBody": "<p>It's fashionable these days to ask people about their AI timelines. And it's fashionable to have things to say in response.</p><p>But relative to the number of people who report their timelines, I suspect that only a small fraction have put in the effort to form&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/independent-impression\"><u>independent impressions</u></a> about them. And, when asked about their timelines, I don't often hear people also reporting how they arrived at their views.</p><p>If this is true, then I suspect everyone is updating on everyone else's views as if they were independent impressions, when in fact all our knowledge about timelines stems from the same (e.g.) ten people.</p><p>This could have several worrying effects:</p><ul><li>People's timelines being overconfident (i.e. too resilient), because they think they have more evidence than they actually do.<ul><li>In particular, people in this community could come to believe that we have the timelines question pretty worked out (when we don't), because they keep hearing the same views being reported.</li></ul></li><li>Weird subgroups forming where people who talk to each other most converge to similar timelines, without good reason.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflcjz2cxzq8k\"><sup><a href=\"#fnlcjz2cxzq8k\">[1]</a></sup></span></li><li>People using faulty deference processes. Deference is hard and confusing, and if you don't discuss how you\u2019re deferring then you're not forced to check if your process makes sense.</li></ul><p><strong>So: if (like most people) you don't have time to form your own views about AI timelines, then I suggest being clear who you're deferring to (and how),&nbsp;rather than just saying \"median 2040\" or something.</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuusr81x3jh\"><sup><a href=\"#fnuusr81x3jh\">[2]</a></sup></span></p><p><strong>And: if you\u2019re asking someone about their timelines, also ask how they arrived at their views.</strong></p><p>(Of course, the arguments here apply more widely too. Whilst I think AI timelines is a particularly worrying case, being unclear if/how you're deferring is a generally poor way of communicating. Discussions about p(doom) are another case where I suspect we could benefit from being clearer about deference.)</p><p><strong>Finally: if you have 30 seconds and want to help work out who people do in fact defer to, take&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScRfMzy51eOeRQyK2XpAY7pTgTGPHrdGBI7SAHWXBJAToBOnQ/viewform?usp=sharing\"><strong><u>the timelines deference survey</u></strong></a><strong>!</strong></p><p><i>Thanks to Daniel Kokotajlo and Rose Hadshar for conversation/feedback, and to Daniel for suggesting the survey.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlcjz2cxzq8k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflcjz2cxzq8k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This sort of thing may not always be bad. There should be people doing serious work based on various different assumptions about timelines. And in practice, since people tend to work in groups, this will often mean groups doing serious work based on various different assumptions about timelines.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuusr81x3jh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuusr81x3jh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here are some things you might say, which exemplify clear communication about deference:</p><p>&nbsp;- \"I plugged my own numbers into the bio anchors framework (after 30 minutes of reflection) and my median is 2030. I haven't engaged with the report enough to know if I buy all of its assumptions, though\"<br>&nbsp;- \"I just defer to Ajeya's timelines because she seems to have thought the most about it\"<br>&nbsp;- \"I don't have independent views and I honestly don't know who to defer to\"</p></div></li></ol>", "user": {"username": "SamClarke"}}, {"_id": "pqbnduKuBHzJ5Nmut", "title": "What are the best ways to encourage de-escalation in regards to Ukraine?", "postedAt": "2022-10-09T11:15:51.398Z", "htmlBody": "<p>I'm not really sure how successful attempts to \"encourage the US government to do the thing I want them to do\" have been in the past, but it seems like right now, trying really hard to get the government to try to de-escalate things could be pretty high impact. Is there some way to go about doing this? Are people already working on it?</p>", "user": {"username": "oh54321"}}, {"_id": "bJyvzDjtnMDjGmwrF", "title": "Uncontrollable AI as an Existential Risk", "postedAt": "2022-10-09T10:37:55.655Z", "htmlBody": "", "user": {"username": "Karl von Wendt"}}, {"_id": "zcuNvBQDaapGjWb2r", "title": "Let\u2019s talk about uncontrollable AI", "postedAt": "2022-10-09T10:37:51.211Z", "htmlBody": "", "user": {"username": "Karl von Wendt"}}, {"_id": "LF5ErNbhoiY6DQweE", "title": "My favourite motivator for longtermism", "postedAt": "2022-10-09T09:30:07.869Z", "htmlBody": "<p>\u201cBy choosing wisely, we can be pivotal in putting humanity on the right course. And if we do, our great-great-grandchildren will look back and thank us, knowing that we did everything we could to give them a world that is just and beautiful.\u201d</p>\n<p>I found this quote from What We Owe The Future quite striking. I have heard a few different \u2018motivators\u2019 for longtermism, but none have quite hit home like this one.</p>\n<p>Toby Ord in The Precipice refers to safeguarding the future as continuing a great partnership that has persisted throughout human history. This didn\u2019t really resonate with me as these people are no longer alive - can we really do anything for them now? If we mess up, they aren\u2019t really affected in any way.</p>\n<p>Others have mentioned our descendants and the fact that we can make their lives go well. But it was specifically the idea that these descendants might look back with sincere gratitude that I found to be very motivating. Maybe this is just the selfish side of me coming out, but the idea of future people recognising our efforts gives me an added boost.</p>\n<p>Perhaps this is a framing we should use more often, or which should even be the longtermist default. The only improvement I can think of is referring to \u201cour great-great-grandchildren and all other generations to come\u201d to give a better sense of scale.</p>\n<p>What do you guys think? What\u2019s your favourite longtermism motivator?</p>\n", "user": {"username": "jackmalde"}}, {"_id": "bmLQ7p3hTyrzb4prc", "title": "Could the drop in child vaccination and the latest news on poliomyelitis be a new cause area?", "postedAt": "2022-10-09T01:09:16.747Z", "htmlBody": "<p>Vaccines have been among <a href=\"https://ourworldindata.org/vaccination\">the most effective interventions in health policy</a>; that\u2019s why <a href=\"https://www.suvita.org/\">Suvita</a>'s program in India has been considered very promising. But after the pandemic, we have been seeing drops in childhood vaccination across the globe [<a href=\"https://www.who.int/news-room/fact-sheets/detail/immunization-coverage\">1</a>, <a href=\"https://www.nature.com/articles/d41586-022-02051-w\">2</a>]<a href=\"#_ftn1\">[1]</a>. Measles has arguably resurged in US. Even though <i>polio in the wild</i> has been eradicated in the American continent, some recent cases have raised flags<a href=\"#_ftn2\">[2]</a> in the most populated countries. Recently, in the Brazilian State Par\u00e1, polio virus has been <a href=\"https://www1.folha.uol.com.br/internacional/en/scienceandhealth/2022/10/para-investigates-poliovirus-found-in-childs-feces.shtml\">reportedly found in human feces</a>, and a case of <a href=\"https://www.uol.com.br/vivabem/noticias/redacao/2022/10/07/poliomielite-esta-de-volta-no-brasil-entenda-caso-sob-investigacao-no-para.htm\">facial paralysis is under investigation</a> - it is being treated as an <a href=\"https://www.cdc.gov/vaccines/vpd/polio/hcp/vaccine-associated-paralytic-polio-faq.html\">adverse event associated with vaccine</a>. In US, the <a href=\"https://www.medicalnewstoday.com/articles/why-polio-has-reemerged-and-how-to-stay-safe-experts-advise\">first case of polio in a decade was confirmed in July</a>, and the presence of virus in sewers have led NY governor <a href=\"https://www.forbes.com/sites/madelinehalpert/2022/09/09/polio-new-york-declares-emergency-after-virus-found-in-fourth-countys-sewage/\">to declare a state of emergency</a></p><p>A recent <a href=\"https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(22)01875-X/fulltext\">Lancet editorial claims</a> that, if we follow this trend, polio global eradication (deemed <a href=\"http://www.gatesnotes.com/Health/XKCD-Marks-the-Spot\">highly probable a few years ago</a>) might become unlikely. Also, given the <a href=\"https://time.com/6141699/anti-vaccine-mandate-movement-rally/\">influence of antivaxxers over the Right</a>, it\u2019s plausible that new vaccination campaigns will face opposition (or decreased participation) in the West.</p><p>Thus, the question above. Is it worth an ITN assessment?</p><hr><p><a href=\"#_ftnref1\">[1]</a> On the Other hand, because of social isolation, the transmission for some diseases has allegedly decreased (as <a href=\"https://xkcd.com/2523/\">xkcd remarked</a>).</p><p>&nbsp;</p><p><a href=\"#_ftnref2\">[2]</a> Recently, in the Brazilian State Par\u00e1, polio virus has been <a href=\"https://www1.folha.uol.com.br/internacional/en/scienceandhealth/2022/10/para-investigates-poliovirus-found-in-childs-feces.shtml\">reportedly found in human feces</a>, and a case of <a href=\"https://www.uol.com.br/vivabem/noticias/redacao/2022/10/07/poliomielite-esta-de-volta-no-brasil-entenda-caso-sob-investigacao-no-para.htm\">facial paralysis is under investigation</a> - it is being treated as an <a href=\"https://www.cdc.gov/vaccines/vpd/polio/hcp/vaccine-associated-paralytic-polio-faq.html\">adverse event associated with vaccine</a>. In US, the <a href=\"https://www.medicalnewstoday.com/articles/why-polio-has-reemerged-and-how-to-stay-safe-experts-advise\">first case of polio in a decade was confirmed in July</a>, and the presence of virus in sewers have led NY governor <a href=\"https://www.forbes.com/sites/madelinehalpert/2022/09/09/polio-new-york-declares-emergency-after-virus-found-in-fourth-countys-sewage/\">to declare a state of emergency</a></p>", "user": {"username": "Ramiro"}}, {"_id": "pDgtxkFPCu5ravRa9", "title": "Embedding AI into AR goggles", "postedAt": "2022-10-09T06:46:46.102Z", "htmlBody": "", "user": {"username": "aixar"}}, {"_id": "aokvvLdjp46ChjSss", "title": "Confidence in the premises of the Repugnant Conclusion - results from a small survey", "postedAt": "2022-10-08T21:08:18.416Z", "htmlBody": "<p>I asked EA Forum readers <a href=\"https://forum.effectivealtruism.org/posts/hJbM5Svpku6haQi7S/what-is-your-confidence-in-the-premises-of-the-repugnant\"><u>here</u></a>&nbsp;about their confidence in the <a href=\"https://forum.effectivealtruism.org/topics/repugnant-conclusion\">Repugnant Conclusion</a> and its 3 premises (you can still fill the survey and edit/complete your previous answers <a href=\"https://www.google.com/url?q=https://docs.google.com/forms/d/e/1FAIpQLScfMjZVamgZSpEz51dMr9YYqwPsmT5QTaSMh4kzna2jIqb3Ag/alreadyresponded&amp;sa=D&amp;source=editors&amp;ust=1665266439235827&amp;usg=AOvVaw10qF5_CpTB9QzT1S-dwDTY\"><u>here</u></a>). The median and mean answers<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefffhh857gyh\"><sup><a href=\"#fnffhh857gyh\">[1]</a></sup></span>&nbsp;for the probability of each being true, as well as the product between the probabilities of the premises given by the people who assigned a probability to the Repugnant Conclusion, are in the table below. For all propositions, the minimum and geometric mean were 0, and the maximum 1. You can find the full answers <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1lRpQ9a1EHPnqaJYONqmWEwyAPAykQ6sm1uVUpTCo2eY/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1665266439236235&amp;usg=AOvVaw0Xi9Icey_I-97dfv-2OF2I\"><u>here</u></a>.</p><figure class=\"table\"><table><thead><tr><th rowspan=\"2\"><strong>Proposition (acronym) (number of answers)</strong></th><th style=\"text-align:center\" colspan=\"2\"><strong>Probability of being true</strong></th></tr><tr><th style=\"text-align:center\"><strong>Median (%)</strong></th><th style=\"text-align:center\"><strong>Mean (%)</strong></th></tr></thead><tbody><tr><td><strong>Dominance addition (DA) (14)</strong></td><td style=\"text-align:center\">85.5</td><td style=\"text-align:center\">71.6</td></tr><tr><td><strong>Non-anti-egalitarianism (NAE) (14)</strong></td><td style=\"text-align:center\">90.0</td><td style=\"text-align:center\">76.6</td></tr><tr><td><strong>Transitivity (T) (14)</strong></td><td style=\"text-align:center\">98.0</td><td style=\"text-align:center\">87.0</td></tr><tr><td><strong>Repugnant Conclusion (RC) (6</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwwb63mzonua\"><sup><a href=\"#fnwwb63mzonua\">[2]</a></sup></span><strong>)</strong></td><td style=\"text-align:center\">27.5</td><td style=\"text-align:center\">37.5</td></tr><tr><td><strong>Premises of the Repugnant Conclusion (DA*NAE*T) (6)</strong></td><td style=\"text-align:center\">27.5</td><td style=\"text-align:center\">36.7</td></tr></tbody></table></figure><p>My quite tentative conclusions based on this very small sample are as follows:</p><ul><li>According to both the median and mean answers, transitivity is thought to be the most solid proposition, followed by non-anti-egalitarianism, dominance addition, and finally the Repugnant Conclusion.</li><li>Given the similarity between the mean and median answers of the last 2 rows, the probability assigned to the Repugnant Conclusion is consistent with the probabilities given to its premises.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnffhh857gyh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefffhh857gyh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;As of 8 October 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwwb63mzonua\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwwb63mzonua\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Unfortunately, I only included later the question about the Repugnant Conclusion.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "v2NwSbp3ZPdeB76B5", "title": "SERI MATS Program - Winter 2022 Cohort", "postedAt": "2022-10-08T19:09:53.259Z", "htmlBody": "", "user": {"username": "Ryan Kidd"}}, {"_id": "dkrmeN2tKnxtSe5an", "title": "Pentathlon: join a wholesome team competition with other EAs/Rationalists", "postedAt": "2022-10-08T18:51:29.977Z", "htmlBody": "<h1>Summary:&nbsp;</h1><p>The Pentathlon is an online 2-week-long \u201cfriendly competition\u201d where you score points individually and as a team on five categories: Most Important Work (minutes), Health, Sleep, Nutrition, and Planning. Pentathlon helps participants improve their daily habits and make a lot of progress on one specific project, in an encouraging and warm environment (where you'll definitely root for your team!). &nbsp;The Pentathlon runs from Saturday, October 15, to Saturday, October 30. Last time, there were over 40 EA participants. Sign up soon!</p><h2><br>How to sign up</h2><p><a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.ultraworking.com%2Fpentathlon-ea-orgs-less-than-15%3Ffbclid%3DIwAR1v34TQRjeob5Ix3HcJMsbvGqcdD0S8bmUR8WCKdFq21A9GVA4FlYxpAc8&amp;h=AT1gPsx5dnZ2m8ZtT0jvgytXG0XMBYT7rPSz0j8_9aC16Pkn9SG_bWsGpV5w9kPcNQDqP6Tipxd0xDaq4PNBOttKxwEp_AGHntKBgP01DY1q1z2pf3fySEgsH73hxgTByA&amp;__tn__=-UK-R&amp;c[0]=AT1JDIRzGInvyA8FCwiSU1HW-Z4q1d016sfPEAhyinCZznc8QRFA0Ow2IUAOlPGqLfbcoVmVQPQ-tEL9aiYLc1ZrdxuuZgdfgjlTHLqz7_kre40-E2kcdNH6ccoShwBNjMHUZtQlDXHeMVkzb8kret4shXx6xz8dQ2E\">Sign up with a team of &lt;15 participants ($300)</a></p><p><a href=\"https://www.ultraworking.com/pentathlon-for-ea-orgs-15?fbclid=IwAR3ymFyGb01kNGvKU1sNqPmFovd9Cr-RS5txXHwuxjRmP6NcE_cFeLSPB20\">Sign up with a team of 15+ participants (custom pricing)</a></p><p><a href=\"https://www.ultraworking.com/pentathlon-for-ea-community?fbclid=IwAR0exizVpQxrodSlvLw0dbkNxjVw0ybJTDHihanf0odgLiWJ-0VAbQub5pc\">Sign up an individual and get matched with other EAs ($80)</a></p><h1>Details:</h1><h3>Targets</h3><p>Attendees set <i>their own </i>daily targets in five areas: Most Important Work (minutes), Exercise, Sleep, Nutrition, and Planning for 14 days. &nbsp;</p><h3>Some of the reasons why people enjoy the Pentathlon include the following:&nbsp;</h3><ul><li>To improve or step up their daily habits, notably by implementing better systems</li><li>To make a lot of progress on one specific project</li><li>To make new professional and personal connections&nbsp;</li><li>To find other people to cowork with on a regular basis</li><li>To contribute to their team/group house/EA group's spirit and motivation with a common short project.</li></ul><h3>Two options to join:</h3><ul><li><strong>Create or Join an existing team:</strong> Teams from EA organizations and local groups in past Pentathlons have included CEA, Momentum, the Good Food Institute, Charity Entrepreneurship, &nbsp;Oxford Trajan House, GovAI, EA national groups, EA Houses, and more.&nbsp;</li><li><strong>Join as an individual: </strong>If you join as an individual, the Ultraworking team and I will work to make teams based on cause areas, geography and timezone, and overall Pentathlon participation \"intensity\" (from relatively chill to very intense).</li></ul><p>EA organizations have a good track record: last time, EA teams were on the second and third march of the podium. Before this, an EA organization reached first place.&nbsp;</p><p><strong>Where it will happen: </strong>on the app of Ultraworking, Headquarters and Slack (a shared Slack with all EA teams). There is an intro event on Zoom (which is really important to meet your team and define the vibe).&nbsp;</p><h2>Reasons not to join:</h2><p>There is a bit of admin to deal with. The setup time for the app and overall competition (if you don't have it already) is around 35 min. You need to self-report your score ~daily. &nbsp;</p><p>There is really a sense of a team, and you'll want to be an active part of it, with your one Slack channel. Ideally, you're part of a vibrant team with one daily (or a bit less frequent) check-in, but you can also define the intensity of this with your team.</p><p>Optionally, you can also participate in the wider EA Slack Pentathlon, sharing funny memes or lifehacks.</p><p>&nbsp;</p><h3><strong>More details are </strong><a href=\"http://ultra.work/ea\"><strong>here</strong></a><strong>.</strong></h3><p>If you have any questions that aren\u2019t answered by this document, your point of contact is Xavier at Ultraworking - see his email address on the linked document.</p>", "user": {"username": "CarolineJ"}}, {"_id": "rd4WSaJWKzrs7i27P", "title": "Does EA have an alternative to rational, written debate methodology?", "postedAt": "2022-10-08T17:27:53.880Z", "htmlBody": "<p>I <a href=\"https://forum.effectivealtruism.org/posts/37byNFAMEfbLNPXdm/debate-with-rational-methodology\">asked</a> whether EA has any rational, written debate methodology and whether rational debate aimed at reaching conclusions is available from the EA community. The answer I received, in summary, was \u201cno\u201d. (If that answer is incorrect, please respond to my original question with a better answer.)</p>\n<p>So I have a second question. <strong>Does EA have any alternative to rational debate methods to use instead?</strong> In other words, does it have a different solution to the same problem?</p>\n<p>The underlying problem which rational debate methods are meant to solve is how to rationally resolve disagreements. Suppose that someone thinks he knows about some EA error. He\u2019d like to help out and share his knowledge. What happens next? If EA has rational debate available following written policies, then he could use that to correct EA. If EA has no such debate available, then what is the alternative?</p>\n<p>I hope I will not be told that informal, unorganized discussion is an adequate alternative. There are many well know problems with that like people quitting without explanation when they start to lose a debate, people being biased and hiding it due to no policies for accountability or transparency, and people or ideas with low social status being ignored or treated badly. For sharing error corrections to work well, one option is having some written policies which can be used that help prevent some of these failure modes. I haven\u2019t seen that from EA so I asked and no one said EA has it. (And also no one said \u201cWow, great idea, we <em>should</em> have that!\u201d). So what, if anything, does EA have instead that works well?</p>\n<p>Note: I\u2019m aware that other groups (and individuals) also lack rational debate policies. This is not a way that EA is worse than competitors. I\u2019m trying to speak to EA about this rather than speaking to some other group because I have more respect for EA, not less.</p>\n", "user": {"username": "Elliot Temple"}}, {"_id": "FxioucsYBENvp7KEH", "title": "Introducing the Basic Post-scarcity Map", "postedAt": "2022-10-09T06:53:13.807Z", "htmlBody": "", "user": {"username": "postscarcitymap"}}, {"_id": "SYaHgTaHG2TopKdpD", "title": "EAG London 22: EA Israel's approach to community building", "postedAt": "2022-10-08T11:56:10.585Z", "htmlBody": "<p>This post describes EA Israel\u2019s approach to community-building, as presented at EAG London 2022. It includes a video and a brief summary of our approach (below).&nbsp;&nbsp;</p><p>A more detailed explanation can be found in the speaker notes of the talk\u2019s&nbsp;<a href=\"http://bit.ly/CB-Model-EAG22\"><strong><u>slides</u></strong></a>. The slides also include the summary of the talk, which is not included in the video due to a technical issue.</p><p>Additional resources by EA Israel for local groups can be found&nbsp;<a href=\"http://bit.ly/EA-Israel-Resources\"><u>here</u></a>.</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/2SJWYmapJP0\"><div><iframe src=\"https://www.youtube.com/embed/2SJWYmapJP0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><h1>An overview of our approach</h1><p>EA Israel\u2019s approach is based on two different models of community-building, for different \u201clevels of engagement\u201d (as defined by CEA\u2019s&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/the-concentric-circles-model\"><u>Concentric Circles Model of Community Building</u></a>):</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675879326/mirroredImages/SYaHgTaHG2TopKdpD/obe20bll9piukrzcpzfy.png\"></p><p><strong><u>A user-centered</u></strong> model, by which we\u2019re viewing our audiences as \"users\", and viewing the group as a \u201cservice provider\u201d.</p><p>In this model, a user is an individual who has a&nbsp;<strong>concrete problem or desire</strong> (e.g. career dilemmas, how to evaluate the impact of projects), and would approach a&nbsp;<strong>local group\u2019s \u201cservice\u201d that answers their problem</strong>.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1675879326/mirroredImages/SYaHgTaHG2TopKdpD/m6eqbqifzlspi0hdhroz.png\"></p><p><strong><u>A volunteer-oriented</u></strong> model - It relates to the contributors and core circles, and suggests looking at community members with this level of engagement, as an exclusive volunteer group.</p><p>This model generally stresses the value of volunteering for the goal of community-building and recommends prioritizing the onboarding and retention of volunteers.</p><h1>A brief explanation</h1><p>Drawing from the field of brand management, we\u2019re assuming the&nbsp;<i><strong>perceived value&nbsp;</strong></i><strong>that individuals attribute to EA plays a significant role in their engagement</strong> with the movement.&nbsp;</p><p>In this context, a major obstacle to community building is that&nbsp;<strong>individuals who </strong><i><strong>just</strong></i><strong> encountered EA don\u2019t see the actual value&nbsp;</strong>they can receive from the EA toolkit,&nbsp;<strong>compared to the effort&nbsp;</strong>they will have to put into receiving it.</p><p>To tackle this obstacle:</p><h2>1) We build gradual onboarding experiences</h2><p>We try to build onboarding experiences where individuals can see over time the value that EA can give them.</p><p>For instance: In addition to offering a fellowship (which offers high value, but demands high effort), we offer lighter four-meetings-long crash courses and even lighter one-time discussion groups. The latter two formats serve as entry points to the group, after which members are able to see the value proposition of EA more clearly.</p><h2>2) We choose, shape, and communicate our activities in a way that highlights their perceived value</h2><p>When choosing, shaping, and communicating about an activity, we keep its perceived value in mind.&nbsp;</p><p>To do that, we divide our potential audience into different segments, based on the concrete wants and needs they have (career advice, donation advice, tools for social entrepreneurship, a community to discuss all of these with, etc.).&nbsp;</p><p>Then, we take into account the difference in preferences of individuals within each segment. For instance, within the same segment of career advice, some prefer listening to podcasts, some prefer a workshop, some prefer someone to consult with, and so forth.</p><p>With these \u201cneeds\u201d and \u201cpreferences\u201d in mind, we can&nbsp;<strong>choose&nbsp;</strong>to focus on activities that have a better&nbsp;<i>product-market fit</i>. Then, we can&nbsp;<strong>shape</strong> these activities to be as helpful as possible in achieving what our audience is looking for. Then, when&nbsp;<strong>communicating&nbsp;</strong>about the event, we highlight what our audience will get from participating.</p><p>For example:</p><ul><li>Let\u2019s take the user needs of (1)&nbsp;<strong>entrepreneurship dilemmas</strong>, (2)&nbsp;<strong>finding collaborators,</strong> and (3)&nbsp;<strong>seeking people to consult with.</strong>&nbsp;</li><li>We can match those needs with the activity of a&nbsp;<strong>networking event</strong>.</li><li>Holding these needs in mind can help us shape the event\u2019s content. For instance, we can decide to encourage members to add a sticker of \u201cseeking collaborators\u201d on their name tags and stand in different parts of the room based on their interest in cause areas.</li><li>When advertising the event, we highlight that the event\u2019s goal is to help participants to seek collaborators and advice.</li></ul><h2>A diverse offering of activities</h2><p>When shaping our community-building activities, we think of the differences in needs and preferences within our audience, and try to offer a&nbsp;<strong>diverse offering of activities, instead of focusing on 1-2 main activities</strong>. The rationale is to:</p><ol><li>Get the low-hanging fruits of each segment.&nbsp;</li><li>Get the information value of understanding the demand in each segment</li><li>Appeal to a diverse crowd: For instance, if a group focuses mostly on discussion groups, it will attract mostly the kind of individuals who see this as an activity they want to spend their free time on.&nbsp;<br>However, not all promising potential EAs fall under this category, and in addition, some promising individuals are put off by social groups that are centered only around discussions.</li></ol><p>This sort of strategy, of having many small activities, is much easier when you have many volunteers available - which is exactly what makes the user-centered model, and the volunteer-based model, complement each other.</p><h2>The volunteer-based model</h2><p>The volunteer-based model emphasizes the high value of volunteering for the goal of community-building.&nbsp;</p><p>The trivial value of volunteering is that they add additional resources - the more hands your team has, the more your team can do.</p><p>However, another value of volunteering is that it serves as a gradual entry to EA. While new members volunteer, they can learn about the value that EA can provide them.&nbsp;<br>We\u2019re 100% explicit with our volunteers about this, and tell them that our most important projects require EA proficiency (and even speak about specific examples that might interest them in the future), but that it\u2019s best to start with a simpler project and learn about EA simultaneously.&nbsp;</p><p>You can read more about this process on the&nbsp;<a href=\"http://bit.ly/CB-Model-EAG22\"><strong><u>slides and speaker notes of the talk</u></strong></a> - where you can also find more detailed explanations about the models described above.</p>", "user": {"username": "GidonKadosh"}}, {"_id": "Ee4q4RhbjZTG9DKZ8", "title": "Update on pestering embassies to reduce non-policy barriers to movement", "postedAt": "2022-10-08T11:17:47.661Z", "htmlBody": "<p>I wrote a post in July speculating that \"it's fairly likely that &nbsp;people from rich countries can pester their own embassies on similar issues [to visa backlog issues] for a decent chance of a pretty high reward.\" Wanted to provide an update</p><p>My &nbsp;update in thinking since this post:</p><ul><li>On the whole, this sort of activism does have an effect&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs1vb20f8cd\"><sup><a href=\"#fns1vb20f8cd\">[1]</a></sup></span></li><li>Tractability is heavily constrained by internal bureacratic/political forces that are hard to gauge from the outside<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnyr9asg8s58\"><sup><a href=\"#fnnyr9asg8s58\">[2]</a></sup></span></li><li>You have to understand the system pretty well in order to individually have an effect, making this higher-effort than I first thought<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref80zi9x73b1a\"><sup><a href=\"#fn80zi9x73b1a\">[3]</a></sup></span></li></ul><p>More simply: politics is hard</p><p>&nbsp;</p><p><strong>New hypothesis on impact:</strong>&nbsp;</p><ul><li>One person putting in a little bit of time by themself is not likely to be effective, because they don't understand what is going on well enough&nbsp;</li><li>You either need to be one person putting in a lot of effort to understand the system well, and then mobilizing others, OR be one of those mobilized others who can with low effort push at just the right leverage point (e.g., email complaints to the right person) as a part of a broader campaign</li></ul><p>&nbsp;</p><p>For EAs who are interested in particular political topics, I'd encourage them to pursue them! <strong>At least in the US, I still think smart civic engagement is highly underrated in terms of impact and the satisfaction of being a part of something.</strong></p><p>Personally I don't think my direct work on visa backlogs in Kenya has been very effective, but I'm still interested and optimistic on the broad topic of <i>civic engagement on mobility.&nbsp;</i> Planning to focus more on overall high-skill immigration into the US as a political cause. Will share an update in the forum if I learn anything worth sharing.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns1vb20f8cd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs1vb20f8cd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Visitor visa wait times at the Us embassy in Kenya are down to 63 days from 365+ in July. This is because the new Ambassador, Meg Whitman, came in and heavily prioritized this issue. She was motivated to do so because tons of people were complaining to her and to the embassy all the time about the wait times. So while I think I myself had very little effect, the overall complaints were effectful</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnyr9asg8s58\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnyr9asg8s58\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Ambassador Meg Whitman has been able to get resources for the Nairobi embassy because she is extremely well connected. Upon moving here, she is the richest woman living in Kenya, she was the CEO of HP and Ebay, and previously ran for governor of California. She can get a meeting with Biden if she wants. This is not the case for all ambassadors. In a global shortage of people to staff US embassy visa appointments, she can get resources that other ambassadors can't (ex: the embassy in Lagos still has a wait time of 700+ days)</p><p>The point of this is that the issue of visa backlogs can have very different tractability depending on the specific politics of the embassy in question, and this tractability is hard to judge from the outside.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn80zi9x73b1a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref80zi9x73b1a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Not knowing who to contact, not knowing who had power, not knowing what actually had to be done to improve the system meant I was just sending cold emails to whoever I could find contact info for</p><p>It's possible if I had done more I might have seen more impact. I had hoped in July to find a way to get something published and to email the acting ambassador myself, but did neither. I don't think it's likely my impact would have been much higher though - seems that Ambassador Whitman is motivated to fix the problem and I doubt my extra voice would have made a difference on this margin.</p></div></li></ol>", "user": {"username": "Luke Eure"}}, {"_id": "TTsPA6NQY39PGYJa4", "title": "Mutual Assured Destruction used against AGI", "postedAt": "2022-10-08T09:35:13.031Z", "htmlBody": "<p>So I googled and didn't find a related topic in such terms. Suppose we put a few astronauts on an asteroid, no regular contact with Earth. The moment they detect the light on Earth has blinked out, they send a projectile of Gray Goo to the Earth to stop the AGI from completing whatever its assigned end goal. If MAD has kept us humans in check, surely it'd work on more rational agents?</p>", "user": {"username": "L3opard"}}, {"_id": "DCAptmT4jReq9aTvT", "title": "Why defensive writing is bad for community epistemics", "postedAt": "2022-10-08T23:38:05.356Z", "htmlBody": "<p>I see this as a big problem that makes communication and learning really inefficient. In a culture where defensive writing is the norm, readers learn to expect that their reputation is greatly at stake if they were to publish anything themselves.</p><p>I advocate writing primarily based on what you think will help the reader. I claim this is an act of introspection that's harder than what it seems at first. I'm not trying to judge anyone here. I find this hard myself, so I hope to help others notice what I've noticed in myself.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe66s8h8fjt6\"><sup><a href=\"#fne66s8h8fjt6\">[1]</a></sup></span></p><p><strong>TL;DR: A summary is hard, but: You may be doing readers a disservice by not being aware of when you're optimising your writing purely for helping yourself vs at-least-partially helping readers as well. Secondly, readers should learn to interpret charitably, lest they perpetuate an inefficient and harmfwl culture of communication.</strong></p><h2>Definitions</h2><p><strong>Self-centered writing</strong> is when you optimise your writing based on how it reflects on your character, instead of on your expectation of what will help your readers.</p><p><strong>Defensive writing</strong> is a subcategory of the above, where you're optimising for making sure no one will end up having a bad impression of you.</p><p><strong>Judgmental reading </strong>is when you optimise your reading for making inferences <i>about the author</i> rather than just trying to learn what you can learn from the content.</p><p>Naturally, these aren't mutually exclusive, and you can optimise for more than one thing at once.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsjuv6ourbc\"><sup><a href=\"#fnsjuv6ourbc\">[2]</a></sup></span></p><h2>Takeaways</h2><ol><li>A culture of defensive writing and judgmental reading makes communication really inefficient, and makes it especially scary for newcomers to write anything. Actually, it makes it scary for everyone.</li><li>There's a difference between trying to make your statements safe to defer to (minimsing false-positives), vs not directly optimising for that e.g. because you're just sharing tools that readers can evaluate for themselves (minimising false-negatives). Where appropriate, writers should be <i>upfront</i> about when they're doing what.<ol><li>As an example of this, I'm not optimising this post for being safe to defer to. I take no responsibility for whatever nonsense of mine you end up actually believing. :p</li></ol></li><li>You are not being harmed when someone, according to you, uses insufficiently humble language. Downvoting them for it is tantamount to bullying someone for harmless self-expression.</li></ol><hr><h2>What does a good epistemic community look like?</h2><p>In my opinion,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa7mrsx9jnf8\"><sup><a href=\"#fna7mrsx9jnf8\">[3]</a></sup></span>&nbsp;an informed approach to this is multidisciplinary and should ideally draw on wisdom from e.g. <a href=\"https://plato.stanford.edu/entries/epistemology-social/\">social epistemology</a>, <a href=\"https://en.wikipedia.org/wiki/Game_theory\">game theory</a>, <a href=\"https://en.wikipedia.org/wiki/Metascience\">metascience</a>, <a href=\"https://en.wikipedia.org/wiki/Economics_of_science\">economics of science</a>, graph theory, rationality, several fields in psychology, and can be usefwly supplemented with insights from <a href=\"https://en.wikipedia.org/wiki/Distributed_tree_search\">distributed</a> &amp; <a href=\"https://en.wikipedia.org/wiki/Parallel_computing\">parallel</a> computing, <a href=\"https://pubmed.ncbi.nlm.nih.gov/32544380/\">evolutionary biology</a>, and more. There have also been relevant discussions on LessWrong <a href=\"https://www.lesswrong.com/tag/epistemic-hygiene\">over</a> <a href=\"https://www.lesswrong.com/tag/aumann-s-agreement-theorem\">the</a> <a href=\"https://www.lesswrong.com/tag/humility\">years</a>.</p><p>I'm telling you this because the first principle of a good epistemic community is:</p><blockquote><p><strong>Community members should judge practices based on whether the judgment, when universalised, will lead to better or worse incentives in the community.</strong></p></blockquote><p>And if we're not aware that there even <i>exists</i> a depth of research on what norms a community can try to encourage in order to improve its epistemic health, then we might have <i>insufficient humility</i> and forget to question our judgments. I'm not saying we should be stifled by uncertainty, but I am advocating that we at least think twice about how to encourage positive norms, and not rely too much on cached sensibilities.&nbsp;</p><p>I'll summarise some of what I think are some of the most basic problems.</p><h3>1) We judge people much too harshly for what they don't know</h3><p>Remember, <i>this isn't an academic prestige contest</i>, and the only thing that matters is whether we have the knowledge we need in order to do the best that we can. I'm not talking about how we should care about each others' feelings more and pretend that we're all equally competent and know an equal amount of stuff. No, I'm saying that <strong>if we have a habit of judging people for what they don't know, we'll be incentivised to waste time learning all the same things, and we lose out on diversity of knowledge</strong>.</p><p>And who are you to judge whether actually knowing whatever is truly essential for what they are trying to do, given that they are the one who's trying to do it?</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/fedC7Pv08Ms?t=32\"><div><iframe src=\"https://www.youtube.com/embed/fedC7Pv08Ms\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><blockquote><p><i>\u201cHis ignorance was as remarkable as his knowledge. Of contemporary literature, philosophy and politics he appeared to know next to nothing.\u201d</i></p></blockquote><p>A community where people reveal their uncertainty, ask dumb questions, and reward each other for the courage and sagacity to admit their ignorance, will be more effective than this.</p><h3>2) Celebrate innocence &amp; self-confidence; stop rewarding modesty</h3><p>This one is a little sad but... when I see someone declare that they have important things to teach the rest of the community, or that they think they can do more good than Peter Singer, I may not always agree that they have a chance--but I'll tear up and my heart will beam with joy and empathy for them, because I know they're sacrificing social comfort and perceived humility in order to virtuously reveal their true beliefs.</p><p>There are too many reasons for this, but the first is fundamentally about being a kind and decent human being. When a kid eagerly comes up to you and offers to show you a painting they're proud of, will you not be happy for the kid? Or will you harshly tell them to fall back in line and never call attention to themselves ever again? Or maybe you only think it's \"inappropriate\" when it's an upstanding adult who does it. After all, they've outgrown their <i>right </i>to be innocent. No. How can a community so full of kind people be so eager to put people down for unfolding their unconquered personalities openly into the world?</p><p>But if you insist on stealing the sunlight from their smiles... maybe arguments about effectiveness will convince you instead. It ties back to how people, in ordinary cases, vastly overestimate the downsides of not knowing particular things or not having \"credentials\", and underestimate the advantage of learning to think independently as <a href=\"https://www.lesswrong.com/posts/KmkZriGwkn2vDx8gB/the-underappreciated-value-of-original-thinking-below-the\">early as possible</a>.</p><blockquote><p><strong>It\u2019s important that people be allowed to try ambitious things </strong><i><strong>without</strong></i><strong> feeling like they need to make a great production out of defending their </strong><a href=\"https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing\"><strong>hero license</strong></a><strong>.</strong></p></blockquote><p>When we have a community in which people are afraid to stick out, and they're always timidly questioning whether they know enough to start actually Doing The Thing, we're losing out on an <i>enormous</i> amount of important projects, motivation, ambition, and untroubled mental health. It may not feel as urgent because we <a href=\"https://forum.effectivealtruism.org/posts/LwRSnvnaFL9eE4yKz/#1__Fear_of_criticism_2_\"><i>can't see the impact we miss out on</i></a><i>.</i></p><h3>3) Charitable interpretations by default</h3><blockquote><p>When you interpret someone with charity, you start out with the tentative assumption that they are a decent human being that means well, and means something coherent that you can learn from if you listen attentively. And you keep curiously looking &nbsp;for reasons not to abandon these assumptions.</p></blockquote><p>EA is not perfect, but there is something <i>different</i> going on here than in the other parts of the world I experience. I don't know about you, but I come from a world in which \"what's in it for me?\", \"it's not my responsibility\", and careless exploitation is the norm. If my intuitions about what people are likely to intend have been trained on a world such as this, then I'm not going to be very charitable.</p><blockquote><p><a href=\"https://www.lesswrong.com/tag/steelmanning\"><strong>Steelmanning</strong></a><strong> </strong>is the act of taking a view, or opinion, or argument and constructing the strongest possible version of it. It is the opposite of strawmanning.</p></blockquote><p>The EA forum is not perfect, but it is different. If you're bringing with you the same reflexive heuristics while you're reading the forum, and you don't put in effort to adjust to the new distribution of <i>actual</i> intentions, then you risk incrementally regressing the forum down to your expectations.</p><p>If writers have to spend five paragraphs on disclaimers and clarification just to make sure we don't accuse them of nazism, well, we've wasted everyone's time, and we've lost a hundred good writers who didn't want to take the risk in the first place.</p><p>But it's worse, because if we already have the norm of constantly suspecting nazism or whatever, then the first writer to <i>not</i> <i>correct </i>for that misunderstanding is immediately going to look suspicious. This is what's so vicious about refusing to interpret with a little more recursive wisdom. <strong>If you have an equilibrium of both expecting and writing disclaimers about X, it self-perpetuates regardless of whether anyone actually means or supports X.</strong></p><p>You might point out that disclaiming X isn't really a big efficiency loss, but X is just the <a href=\"https://www.overcomingbias.com/2008/06/against-disclai.html\">tip of an iceberg</a>.</p><p>Part of the problem is just laziness and impatience. If people have trained their intuitions on the level of discourse found in the world at large, they may not be used to the level of complexity commonly found in arguments on the forum. So they--at first reasonably--think they can get away with a few simple assumptions they've learned to expect here and there and then the point will immediately snap into understanding after reading the first paragraph. But that often doesn't work for the level of complexity and unusualness of meaning on the forum.</p><hr><h2>So what's wrong with defensive writing?</h2><p>(If you've missed it, there are examples of defensive writing in the footnotes.)</p><p>Ultimately, my complaint is that it claims to be about benevolence, yet it is not actually about benevolence.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4eso12kr3nm\"><sup><a href=\"#fn4eso12kr3nm\">[4]</a></sup></span>&nbsp;If the lack of benevolence was all it was, then I would have no gripe with it. But because it's routinely confused for benevolence, good people end up being fooled by it and it causes all sorts of problems.</p><p>Remember, you're not harming other people when they believe you are mistaken, or whether you say a wrong thing <i>in a way that they can easily detect</i>, or whether they think you're a nazi because you mentioned something about DNA and you didn't explicitly disclaim that you're not a nazi.</p><p>See the above section for the problems with defensive disclaimers.</p><p>It's sort of the opposite of writing ostentatiously just to show off (e.g. using math or technical terms that signal impressiveness but doesn't actually help with understanding). But not because it's any less self-centered. Wasting the reader's time by trying to influence their impression of you is more or less the same regardless of whether it is for defending against negative impressions or inviting positive impressions.</p><p>Another perhaps more damaging form of defensive writing is <strong>unnecessarily trying to </strong><i><strong>demonstrate</strong></i><strong> that you've done your research due diligence</strong>. As a way perhaps to establish some kind of \"license\" for writing about it at all. Or at least making it clear that you've \"done work\". But if I only have one interesting thing to teach, it doesn't matter to the reader whether I had to read 1 book or 50 books to find it. And it'd be especially indulgent to then go into detail about the contents of every one of those 49 books when you know that's not where the value is.</p><hr><p>I think it's easy to misunderstand me here. What I'm essentially advocating is that that every word you write should flow from intentions in your brain that you are aware and approve of.</p><p>And what you optimise for should depend on what you're writing. Sometimes I primarily optimise for <i><strong>A)</strong></i><strong> making readers have true beliefs about </strong><i><strong>what I believe</strong></i>. Other times I primarily optimise for <i><strong>B)</strong></i><strong> providing readers with tools that help them come to their own conclusions</strong>.</p><p>When I'm doing B, it doesn't matter what they believe I believe. If out of ten things I say, nine will be plain wrong, but one thing will be so right that it helps the reader do something amazing with their lives, I consider that a <a href=\"https://slatestarcodex.com/2019/02/26/rule-genius-in-not-out/\">solid success</a>.</p><p>Purpose <i>A</i> is necessary if you're somewhat of an authority figure and you're letting people know what you believe so they can defer to you. E.g. if you're a doctor, you don't want your patients to believe false things about your beliefs.</p><p>As it happens, I'm mostly trying to optimise this post for <i>B</i>, but didn't I say that this means I shouldn't care whether I'm misunderstood? So why am I spending time in this section on defending against misunderstandings?</p><p>Because I think this is complicated enough that readers may end up not only <i>misunderstanding what I believe</i>, but also <i>misunderstanding the tools</i> I'm trying to teach. I think people can benefit from these tools, so I want to make sure they're understood.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefht61pk8iy0h\"><sup><a href=\"#fnht61pk8iy0h\">[5]</a></sup></span></p><p>But also just because... well, <strong>there are limits to how brave you are required to be</strong>.</p><p>I want to be liked, so I sometimes signal modesty for no further reason than to be liked. This is ok. And sometimes I have to affect humility in order to make my writing bearable to read, because we already live in a world where that's expected and I can't unilaterally defect from every <a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\">inadequate equilibria</a> I see all the time--the personal cost is too high.</p><p>So I try to do what I can on the margin, show the world that I can be happy about my achievements (\"bragging\"), do less modesty, say more wrong things, etc. But when the cost is cheap, as is the case when you see <i>others</i> who defect, there are fewer excuses to not at least respect and try to feel happy about them nudging the equilibrium.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne66s8h8fjt6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe66s8h8fjt6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Ironic, yes, but this will be our first example of defensive writing! First, the altruistic value of this paragraph is that it points out that noticing defensiveness is a difficult act of introspection. That's usefwl for the readers to know. The rest of the paragraph only serves the purpose of making sure no one thinks I'm being arrogant.&nbsp;</p><p>Notice also how when I write \"I claim this is\" instead of \"this is\", I change absolutely nothing about what the readers learn, but I <i>am</i> changing readers' impressions about <i>how humble </i>I'm being. If I can seem more humble by adding two more words, I usually say it's worth the cost--but I'm aware that I am profiting only myself, and it is <i>my readers</i> that pay the cost.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsjuv6ourbc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsjuv6ourbc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Another example. I don't want readers to think that I believe you cannot optimise for two things at once, so I point that out. But no one in their right mind would actually think that! So by pointing it out, I'm not helping <i>them</i> understand anything differently about the content. The only purpose of the sentence is to prevent people <i>who are actively looking to misunderstand me</i> from misunderstanding me.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna7mrsx9jnf8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa7mrsx9jnf8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Part of my motivation for writing this whole paragraph was to provide an example of something that could <i>seem like</i> it was optimised for ostentation (name-dropping a bunch of scientific fields), but was in reality optimised for something else. The point is that people are generally much too quick to judge people for perceived intentions, and if I were afraid of this then <i>I wouldn't have felt permitted to write this sentence</i>. A culture of judgment and no charity will prevent people from writing sentences they actually think are helpfwl.</p><p>In addition, notice how this usage of \"in my opinion\" actually does communicate important-to-the-reader information about the value of deferring to me, so it's not just a ploy to seem more humble.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4eso12kr3nm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4eso12kr3nm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Norwegian has the right word here, and it's \"nestekj\u00e6rlighet\". The best I can do as a direct translation is \"otherpersongoodintentions\". (Sorry.) It makes it unfailingly clear that it's not about raising <i>one's own</i> moral character--that would just be <i>selfpersongoodintentions.</i>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnht61pk8iy0h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefht61pk8iy0h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I actually think nearly all the value to these exercises are in things I haven't made the case for. Those explanations would be longer and require more stuff. But I hoped the value present would make it interesting enough, and I didn't want to make the post longer than it already is.</p></div></li></ol>", "user": {"username": "Emrik"}}, {"_id": "qFoPidYq28b5vZqe2", "title": "Why I think there's a one-in-six chance of an imminent global nuclear war", "postedAt": "2022-10-08T23:25:07.149Z", "htmlBody": "", "user": {"username": "Tegmark"}}, {"_id": "B4mMsbG4vpHCTxygx", "title": "Australians are pessimistic about longterm future (n=1050)", "postedAt": "2022-10-08T04:33:22.200Z", "htmlBody": "<p>One of Australia's most prominent pollsters just read (and really liked!) What We Owe The Future, and was inspired to ask a question about perceptions of the longterm future on their fortnightly opinion poll.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/421eebbdb00c4da76233aca7dadce991484b54a0e070978b.png/w_1218 1218w\"></figure><p><a href=\"https://essentialreport.com.au/questions/perception-of-the-future-for-humanity\">https://essentialreport.com.au/questions/perception-of-the-future-for-humanity</a></p><p>[Description of image: At each of 10, 100, 1000, 10000 years in the future, more Australians think life will be worse off for humanity than better off, with eg 35% worse, 45% unsure and 20% better at 10,000 years]</p><p>I was surprised by this!</p><p>Perhaps it shows that the Steven Pinker et al world-is-getting-better meme isn't that established outside of niche intellectual circles.</p><p>No actionable takeaways, just an interesting survey that I expect very few people here will come across elsewhere.</p>", "user": {"username": "Oscar Delaney"}}, {"_id": "nAYzA2hMazmogpirQ", "title": "Deliberate practice for research?", "postedAt": "2022-10-08T03:45:21.796Z", "htmlBody": "", "user": {"username": "Alex_Altair"}}, {"_id": "mwYGRrsCb6Wo8RkjE", "title": "The Biggest Problem with Deontology: The Aggregation Problem", "postedAt": "2022-10-08T03:41:33.175Z", "htmlBody": "", "user": {"username": "ModusTrollens"}}, {"_id": "wkWG4tYEgF6Ko49bH", "title": "Don't leave your fingerprints on the future", "postedAt": "2022-10-08T00:35:36.937Z", "htmlBody": "", "user": {"username": "So8res"}}, {"_id": "vDHeKh2KajnKcLRKz", "title": "Changing Licences on the EA Forum", "postedAt": "2022-10-07T23:45:51.437Z", "htmlBody": "<p>As the EA community grows, we have been excited by the number of people who want to reuse EA Forum content, for example:</p><ul><li>Translating posts into different languages</li><li>Making audio/podcast adaptations of posts</li><li>Excerpting content into fellowship syllabi</li></ul><p>In order to ensure that these works follow applicable laws, we are planning to make Forum content published under a <a href=\"https://www.google.com/url?q=https://creativecommons.org/licenses/by-nc/4.0/&amp;sa=D&amp;source=editors&amp;ust=1665189895063792&amp;usg=AOvVaw2bJwdDwu6xHytEw8LK4-o9\"><u>Creative Commons Attribution-NonCommercial license</u></a>.</p><p>This is a widely used license which states that you can share and adapt Forum content, under the following terms:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref[object Object]\"><sup><a href=\"#fn[object Object]\">[1]</a></sup></span></p><ul><li>Attribution \u2014 You must give <a href=\"https://www.google.com/url?q=https://creativecommons.org/licenses/by-nc/4.0/%23&amp;sa=D&amp;source=editors&amp;ust=1665189895064417&amp;usg=AOvVaw3wt0w2WwB5JWZyvf-kNrQZ\"><u>appropriate credit</u></a>, provide a link to the license, and <a href=\"https://www.google.com/url?q=https://creativecommons.org/licenses/by-nc/4.0/%23&amp;sa=D&amp;source=editors&amp;ust=1665189895064657&amp;usg=AOvVaw1NJbb_bJTkgJ7rLiXKYNLd\"><u>indicate if changes were made</u></a>. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li><li>NonCommercial \u2014 You may not use the material for <a href=\"https://www.google.com/url?q=https://creativecommons.org/licenses/by-nc/4.0/%23&amp;sa=D&amp;source=editors&amp;ust=1665189895064940&amp;usg=AOvVaw0yDEd5WFQ_1kWXqSy9gPvU\"><u>commercial purposes</u></a>.</li><li>No additional restrictions \u2014 You may not apply legal terms or <a href=\"https://www.google.com/url?q=https://creativecommons.org/licenses/by-nc/4.0/%23&amp;sa=D&amp;source=editors&amp;ust=1665189895065283&amp;usg=AOvVaw1Q4OSLodJagySsaL9CeNtc\"><u>technological measures</u></a>&nbsp;that legally restrict others from doing anything the license permits.</li></ul><p>Please see <a href=\"https://www.google.com/url?q=https://creativecommons.org/licenses/by-nc/4.0/legalcode&amp;sa=D&amp;source=editors&amp;ust=1665189895065694&amp;usg=AOvVaw2Z9i2fo9mmnH_lbpjutZfS\"><u>the license</u></a>&nbsp;for full details.</p><p>Feedback on this change is appreciated. In particular: I am not sure about the noncommercial requirement. As one of our goals is to promote discussion of EA concepts, it would arguably advance our mission if (say) someone made a commercial film based on concepts from the Forum. At the same time, I can imagine authors being upset about a third party making money from something derived from their work.&nbsp;</p><p>Thoughts from Forum contributors on this would be appreciated!</p><p>&nbsp;</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnifqv096yol\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefifqv096yol\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Terms copied verbatim from the CC website. Please see the license for full details.</p></div></li></ol>", "user": {"username": "Ben_West"}}, {"_id": "MWPtjWRTdoTa7w5s8", "title": "Accepting 2 PhD students in psychology of EA for next year", "postedAt": "2022-10-07T22:57:53.417Z", "htmlBody": "<p>For EAs considering graduate study in a psychology PhD program:</p><p>I plan to accept 2 new graduate students to start in August 2023, for psychology&nbsp;program at University of New Mexico (Albuquerque, USA), to do research on the psychological issues related to Effective Altruism. I'm developing a research program focused on the evolutionary/moral psychology of longtermism, existential risks, and AI alignment, but I'm also open to students interested in animal sentience and welfare, mental health, charity-giving, virtue-signaling, consequentialist reasoning, etc.</p><p>I&nbsp;am looking for bright, motivated, stable, conscientious students with strong GPAs, good research experience, and solid academic training in evolutionary psychology, biology,&nbsp;anthropology,&nbsp;genetics, psychometrics, cognitive science, and/or Artificial Intelligence. Familiarity with Effective Altruism would be a big plus. I would welcome both US citizens and non-US students, e.g. from the UK, Europe, or China, as long as their English proficiency is good.</p><p>Please feel free to forward this post to any good students who might be interested;</p><p>For best consideration, apply by December 1, 2022; for application details see <a href=\"https://psych.unm.edu/.../application-instructions.html\">here</a>:</p><p>My department website is <a href=\" https://psych.unm.edu/.../profile/geoffrey-miller.html\">here</a>; my personal website is <a href=\" https://www.primalpoly.com/\">here</a></p><p>Background:</p><p>My traditional research interests include sexual selection, mate choice, ovulatory cycles, mental fitness indicators (creativity, humor, art, music, moral virtues), individual differences (intelligence, personality traits, mental disorders, behavior genetics), and applied evolutionary psychology (consumer behavior, evolutionary medicine, politics). My newer research interests include evolutionary psychology applied to Effective Altruism, longtermism, existential risks, and Artificial Intelligence safety.</p><p>My 10 previous PhD students have worked on a wide variety of topics, including disgust, humor, facial attractiveness, musical rhythm,&nbsp;genetic admixture, architecture as costly signaling, intelligence among hunter-gatherers, and behavioral manipulation by sexually transmitted pathogens.</p><p>Our PhD program at University of New Mexico is based in Albuquerque, USA, which offers plentiful sunshine, a low cost of living, outdoor activities, and a relaxed lifestyle. Apart from evolutionary psychology, the 27 faculty in our psychology department have strengths in brain imaging, addictions research, health psychology, evidence-based clinical psychology, and quantitative methods. UNM also has a very strong evolutionary anthropology program including 9 faculty, and I have co-advised 5 previous anthropology PhD students. Our program offers 5 years of support, including tuition, health insurance, and a graduate assistantship (stipend) of about $16k/year; there may be EA funding for graduate studies that could pay significantly more than that.</p>", "user": {"username": "geoffreymiller"}}, {"_id": "kzGKvyeyqBCoiCiMr", "title": "Info Lifeguards", "postedAt": "2022-10-07T21:47:06.066Z", "htmlBody": "<p>If info hazards represent information that can create harm or weapons of mass destruction. What information out there represents the exact opposite of that?</p>\n<p>\u201cAn information hazard, or infohazard, is \"a risk that arises from the dissemination of (true) information that may cause harm or enable some agent to cause harm\", as defined by philosopher Nick Bostrom in 2011. One example would be instructions for creating a thermonuclear weapon.\u201c\n<a href=\"https://en.m.wikipedia.org/wiki/Information_hazard#:~:text=An%20information%20hazard%2C%20or%20infohazard,for%20creating%20a%20thermonuclear%20weapon\">https://en.m.wikipedia.org/wiki/Information_hazard#:~:text=An information hazard%2C or infohazard,for creating a thermonuclear weapon</a>.</p>\n<p>I am talking about none other than dangerously good, weaponized Altruism that if spread wide enough would cause culture to update its firmware to a better value system.</p>\n<p>For reason I am giving it the name \u201cInfo Lifeguards\u201d</p>\n<p>My favorite existing examples for this are:</p>\n<p>Dead Kid Currency</p>\n<p><a href=\"https://www.gwern.net/docs/philosophy/ethics/2011-yvain-deadchild.html\">https://www.gwern.net/docs/philosophy/ethics/2011-yvain-deadchild.html</a></p>\n<p>Peter Singer\u2019s Drowning Child Thought Experiment</p>\n<p><a href=\"https://daily-philosophy.com/peter-singers-drowning-child/\">https://daily-philosophy.com/peter-singers-drowning-child/</a></p>\n<p>Dead Kid Currency and The Drowning Child thought experiment are pieces of information that radically changed my view on value and my potential in the world.</p>\n<p>I think the spread of these ideas would be hyper-positive in influencing societies understanding of how we view value, and help shift culture to move away from designer goods and watches that range from 1 dead child to hundreds of dead children.</p>\n<p>What other info lifeguards exist that I may be unaware of?</p>\n", "user": {"username": "jack jay"}}, {"_id": "XiYh7D9sJsjq324J8", "title": "Buddha Was An Effective Altruist", "postedAt": "2022-10-07T23:28:13.395Z", "htmlBody": "<p>I believe Buddha and Buddha's teachings are more aligned with Effective Altruism than current Buddhist culture and in this short piece, I am going to explain why that matters.</p><p>Buddhism is a philosophy full of wisdom with great truths whose culture lacks a real impact focus. It's a prime example of a <a href=\"https://forum.effectivealtruism.org/s/KeipizrSxYFuyuyow\">meta-trap</a>.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/emtpegoytxbgjvzpgku8\"><br>&nbsp;</p><p>In Buddhist society, many Buddhists have extreme willpower and dedication but lack a calling/path to real world action. I believe Buddhism represents a giant source of potential output for EA causes and through my studies of Buddhism, I believe that Buddha meant for his teachings spawn effective action.&nbsp;</p><p>&nbsp;</p><p>Other Buddhists have <a href=\"https://www.facebook.com/groups/2299113393738867\">come to the same conclusion</a> (Links to EA Buddhism Facebook group)</p><p>&nbsp;</p><p><strong>We can learn from Buddhist Culture:</strong></p><p>The funny thing about entrepreneurs/EA's who are maximizing their productivity to positively change the future is that they are pieces of&nbsp;</p><p>From quitting alcohol, to no-fap, to stoicism, and dopamine fasts, Buddhist teachings were way ahead of the game.&nbsp;</p><p>For example here are 4 of the 10 precepts taught in Buddhist culture:</p><p>\"Not to engage in licentious acts or encourage others to do so. A monk is expected to abstain from sexual conduct entirely.</p><p>Not to use false words and speech, or encourage others to do so.</p><p>Not to trade or sell alcoholic beverages or encourage others to do so.</p><p>Not to harbor anger or encourage others to be angry.\"</p><p>&nbsp;</p><p>Entreprenuers even use the meme \"going monk mode\" to describe this phase of extreme dedication to their craft. The craft portion, is whats missing in Buddhist culture as I believe the \"renounce all worldly desires\" is misportrayed as impactful goals being \"wrong\"</p><p>&nbsp;</p><p><strong>Evidence Buddha Was An EA:</strong><br>One of my favorite and most compelling pieces of evidence for why <i>Buddha was actually an effective altruism maxi</i> is based on Buddha\u2019s teachings for the path to nirvana which includes four main vows.&nbsp;<br>&nbsp;</p><p><strong>The Bodhisattva Vows exist in many different forms; they are chanted as follows:</strong></p><h3>1. Creations are numberless, I vow to free them.</h3><h3>2. Delusions are inexhaustible, I vow to transform them.</h3><h3>3. Reality is boundless, I vow to perceive it.</h3><h3>4. The awakened way is unsurpassable, I vow to embody it.</h3><p><br><strong>It\u2019s also said in this way.</strong></p><h3>1. Living beings are limitless, I vow to liberate them all.</h3><h3>2. Blind passions are limitless, I vow to sever them all.</h3><h3>3. Dharma gates are inexhaustible, I vow to know them all.</h3><h3>4. Unsurpassed is awakening, I vow to realize it.</h3><p><br><strong>Or, in its simplest form, the four main vows are:</strong></p><h3>1. To save all people</h3><h3>2. To renounce all worldly desires</h3><h3>3. To learn all teachings</h3><h3>4. To attain perfect enlightenment<br>&nbsp;</h3><p>If you go searching, there are many other ways these four vows are written, but they are all telling the same thing, in this order. Which makes you wonder why has the Buddhist movement consisting of 300 million members (over 1000x reported EA numbers) has not identified the solutions to the world's greatest problems that may inhibit others from attaining enlightenment. &nbsp;<br>&nbsp;</p><p>As anyone may observe from the outside, the most influential people in the world today are not Buddhists, nor is your average idea of a Buddhist someone focusing on creating effective impact. I believe a hyper effective cause area is pushing towards Effective Buddhism practice.&nbsp;</p><p>&nbsp;</p><p>By the very meaning of the vows, the second vow (renounce all worldly desires) cannot be done without first completing the first vow (saving all people), as vow one is in fact, an earthly desire. Buddha was a man who very much so took action that was in accordance with his teachings spreading far and wide. Its not too surprising that with so much time, a higher focus on meditation and disregard of material belongings is more prominent, as it is a direct and easy to measure path while save all people has many paths which are murky, meaning it becomes a road less traveled.&nbsp;</p><p>&nbsp;</p><p>I hope to establish idea is to relinquish attachment to earthly desires in order to desire only freeing all creation. Not to meditate all day to do nothing, but to focus that willpower to specialize and help free all of creation.&nbsp;</p><p>&nbsp;</p><p>Buddhism has become an endless game of ineffective spiritual ego stroking. Looking into the self to try to find more, when the calling is that the self is everywhere and must be brought up with them.&nbsp;</p><p>I am not saying that the Men and Women who have become great at meditating for hours and wholely living a minimalist life have not made progress down their path of enlightenment. Instead, I believe these people who realize their potential to follow the first vow, who can disregard ALL OTHER DESIRES BESIDES SAVING ALL PEOPLE have the potential to be some of the most effective people out there.&nbsp;<br><br>Buddhism and Effective Altruism have more similarities than we currently give credit to and give the existing Buddhists in EA material to start influencing the 300 million-strong Buddhist culture to be more effective.</p><p>&nbsp;</p><p><strong>My message to Buddhists is to focus on impact:</strong></p><p>People are dying because of your inaction. If you were near a pond with a drowning child would you really not save them because that would be \"giving in to attachment\"? Absolutely not.&nbsp;</p><p>&nbsp;</p><p>This staging of thought is in reference to <a href=\"https://newint.org/features/1997/04/05/peter-singer-drowning-child-new-internationalist\">Peter Singer's thought experiment</a> which states that we are all within reach of just within reach of a drowning child, just with layers of abstraction.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/fnhio8grqvztaboqq8wk\"><br><br>While it's not as easy to identify that people will suffer from your inaction you ARE NOT A MONKEY and as a conscious human intellectual being, you can understand that this is actually the case. You were blessed with an ability to give and to understand, I ask that you do not squander it, and I hope you help empower your potential.</p><p>&nbsp;</p><p><strong>Where Does That Leave Us?</strong></p><p>Tapping into the massive Buddhist following worldwide and aligning these groups towards effectiveness seems like a worthwhile cause. I personally found Buddhism as a guide in my own EA journey and care deeply about bringing all the dedicated ready souls, into a path of higher effectiveness.&nbsp;</p><p>A major value of EA is offering many pathways of effective positive change that are researched and compelling. For Buddhists that struggle with detachment because of lack of directed calling for \"going monk mode\" I believe this can help align the calling to focus and specialize.&nbsp;</p><p>In order to spread this Buddhist lifestyle effectively, we should maximize our reach potential to Buddhists and non-Buddhists worldwide, showing the Effective Buddhist lifestyle in accordance with these principles.&nbsp;</p><p>I strongly believe in this idea of focusing on local maxima in order to maximize globally. The internet allowed us to enter digital bubbles of community with new subsets of culture that allow collaborative efforts as we see with EA itself. Many futurists, such as Balaji Srivasinan who released a book on \"<a href=\"https://thenetworkstate.com/\">Network States</a>\", see the next step in this evolution as these highly aligned digital groups coming together in the physical world to live around this shared culture which was similarly addressed by Tyler in his writing on Fractal Altruism.<br><br>\"How then would this be different from a normal tight-knit neighborhood? The only added ingredient might be demographic: a substantial subset of people who are devoted to world-scale service (ideally the effective kind). Provided that this subset is supported, not everyone needs to identify with a fancy ideology like \u201cfractal altruism.\u201d Fractal altruism would simply describe the community\u2019s emergent activity, one which springs from its members\u2019 natural inclination to tend to one another\u2019s ends, which happen to include \u2013 for some \u2013 intense moral ambition.\"</p><p>One group pioneering this field, that I joined early on, is called \"<a href=\"https://www.praxissociety.com/\">Praxis Society</a>\". Talking with founders Dryden Brown and Charlie Callihan, I found that both strongly believe in many societies coming together to live in physical harmony. They see many forward-thinking city-states emerging in the near future. At first, they even planned to focus on helping multiple other settlers start, before they decided to put their focus on the fractal model of starting with creating a single successful one. Even SBF himself joined their $15m Series A!</p><p>\"Raised $15 million in a Series A funding round that included investments from Paradigm Capital, FTX Trading CEO Sam Bankman-Fried's Alameda Research and Three Arrows Capital\" -<a href=\"https://www.coindesk.com/business/2022/03/03/paradigm-among-investors-as-praxis-raises-15m-in-series-a-funding-round/\">https://www.coindesk.com/business/2022/03/03/paradigm-among-investors-as-praxis-raises-15m-in-series-a-funding-round/</a></p><p><br>While I believe strongly in the end state (or city;) idea, the roadmap of success for actualizing a city is still up in the air. There are <a href=\"https://thenetworkstate.com/dashboard\">26 startup societies</a> listed on the network state website each approaching their growth in a different way.&nbsp;<br><br>While believing in Praxis's vision for the future. I believe that it is far easier to spawn a town/city which would compel thousands of people to dedicate their lives to a new location to form from an existing idealogical framework rather than trying to innovate that as well.&nbsp;<br>&nbsp;</p><p>In fact, Buddhism already has massive monasteries.&nbsp;</p><p>\"Khenpo envisioned Larung as a university town where monks, nuns, and laypeople from all over Tibet could come and study. As the Chinese government\u2019s attitude toward religion became more relaxed, Larung spread like an amoeba over these hills to accommodate more than 10,000 residents.\"</p><p>-<a href=\"https://thediplomat.com/2020/07/larung-gar-the-worlds-largest-monastery-and-highest-slum/\">https://thediplomat.com/2020/07/larung-gar-the-worlds-largest-monastery-and-highest-slum/</a></p><p>&nbsp;</p><p>\"In modern times, <strong>living a settled life in a monastery setting has become the most common lifestyle for Buddhist monks and nuns across the globe</strong>.\"</p><p><a href=\"https://en.wikipedia.org/wiki/Monastery\">-https://en.wikipedia.org/wiki/Monastery</a></p><p><br>Existing Buddhist communities prove the willingness and strength of Buddhist numbers. My dream is to witness the birth of an Effective Buddhist Monastery that could host a non-profit incubator, effective university, and more.&nbsp;<br><br>I have spoke with others who have similar ideas, and we are aligning on a deeper vision that I would like to invite anyone reading this who wants to take any part to reach out to me.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/llzdzx9dbzwxhvro5i5x\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/yylkdxsx7kdoa0m4ulx3 110w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/dx9lktujasrtd7ajy308 220w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/q05clyxejkzqdyiumipt 330w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/ejirip4qwsic2y9rt2yh 440w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/qgumqxhvau6ci5eyn89b 550w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/fqhwfefrel8ygjtoxjwo 660w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/wsy67emu50tv8zbuayga 770w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/s46jy1hlxdkr8p3pcafc 880w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/vnnlouwyadpzxxar9qda 990w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/XiYh7D9sJsjq324J8/ys6nvibsqxyk0hdcksod 1024w\"></figure><p>&nbsp;</p><p>More Sources:</p><p><a href=\"https://secularbuddhism.org/four-truths-four-vows/\">Four Truths, Four Vows</a></p><p><a href=\"https://en.wikipedia.org/wiki/List_of_Buddha_claimants\">List of Buddha claimants - Wikipedia</a></p>", "user": {"username": "maximizealtruism"}}, {"_id": "22xpqq5SBRGCtyXtz", "title": "Public Explainer on AI as an Existential Risk", "postedAt": "2022-10-07T19:23:35.432Z", "htmlBody": "<p><i>This is a submission to the AI Safety Public Materials </i><a href=\"https://www.lesswrong.com/posts/gWM8cgZgZ9GQAYTqF/usd20k-in-bounties-for-ai-safety-public-materials#How_to_submit\"><i>bounty</i></a><i>. It aims to help an intelligent but non-technical audience grasp and visualize the basic threat advanced AI poses to humankind, in about 20 minutes of reading. As such, it prioritizes digestibility over technical precision.</i></p><h2><strong>Introduction</strong></h2><p>The United States and China are increasingly investing in artificial intelligence (AI) and exploring opportunities to harness its competitive benefits.<a href=\"#_ftn1\">[1]</a> As they do so, they must be mindful of various ethical problems surrounding AI's development.</p><p>Some of these problems are already here and widely publicized. These include racial bias in search engines or criminal sentencing algorithms, deaths caused by fully autonomous vehicles or weapons, privacy issues, the risk of empowering totalitarian regimes, and social media algorithms fueling political polarization.&nbsp;</p><p>Other problems remain more speculative and receive less media attention. But this does not make them any less dangerous. If anything, the relative lack of scrutiny only heightens the chance that these problems will fly under the radar until it\u2019s too late to prevent them \u2013 or worse, too late to fix them.</p><p>This explainer is about one such issue: the chance that advanced AI could either make humanity extinct, or leave it forever unable to achieve its full potential.&nbsp;</p><p>This risk is not intuitive or easy to visualize, so many people are initially skeptical that it is likely enough to worry about. This explainer aims to change that. It starts with a brief summary of the problem, then elaborates on the various development pathways advanced AI may take. Finally, it outlines four possible scenarios through which AI's improper development could wipe out humankind. The goal is not to convince readers that any of these scenarios are likely \u2013 only that they are <i>likely enough</i> to take very seriously, given the stakes.</p><h2><strong>How Artificial Intelligence Could Kill Us All</strong></h2><p>The shortest answer is that super-intelligent AI is likely to be both very powerful and very difficult to control, due to something AI researchers call the <i>alignment problem.&nbsp;</i></p><p>AI operates in the single-minded pursuit of a goal that humans provide it. This goal is specified in something called the <i>reward function</i>. Even in today\u2019s relatively simple AI applications, it is really hard to design the reward function in a way that prevents the computer from doing things programmers don\u2019t want \u2013 in part because it\u2019s hard to anticipate every possible shortcut the computer might take on the way to its specified goal. It is hard to make AI\u2019s preferences <i>align</i> with our own.</p><p>Right now, this creates mostly small and manageable problems. But the more powerful AI becomes, the bigger the problems it\u2019s likely to create. For example, if AI gets smart enough, it could learn how to seize cash and other resources, manipulate people, make itself indestructible, and generally escape human control. The fear is that if a sufficiently advanced (but unaligned) AI system were to escape human control, it would do anything necessary to maximize its specified reward \u2013 even things that would kill us all.</p><p>Any more detailed explanation requires envisioning what advanced AI will actually look like. This, in turn, requires getting slightly technical, in order to describe competing theories on that subject.</p><h2><strong>Possible Pathways for AI\u2019s Development</strong></h2><p>Recent years have seen AI develop more quickly than ever before, renewing debate about the path its development will most likely take moving forward. Some say the advance is exponential, and will only keep accelerating. Others suspect researchers will encounter stubborn new obstacles, and advance only in fits and spurts.</p><p>Equally contentious is the debate over just how advanced AI will become at its peak. Capability optimists talk of something called <strong>AGI, or Artificial General Intelligence. In a sentence, this would be a single machine that does anything humans can do, at least as well as humans do it.</strong> For this reason, it is sometimes also called HLMI, for Human-Level Machine Intelligence.</p><p>Currently, AI programs are mostly <i>narrow</i>, in that they are stove-piped across many individual applications. One plays Atari games, another recognizes faces, another creates digital art, and another is our personal smartphone or Alexa assistant. AI can already perform some of these narrow tasks (like playing Chess or Go, or solving advanced math problems) much better than human beings. Other functions (like driving or writing) are nearing the human level, but not quite there yet.</p><p>In theory, <i>broad</i> or <i>general</i> AI would combine all these functions into a single machine, \u201cable to outperform humans at most economically valuable work.\u201d<a href=\"#_ftn2\">[2]</a> The term <i>advanced AI</i> includes both broad and narrow systems, but AGI would be an especially advanced form of it.</p><p>There is significant debate over just how feasible AGI is, and if it is feasible, by when it may arrive. In 2014, one survey asked the 100 most cited living AI scientists by what year they saw a 10%, 50%, and 90% chance that HLMI would exist.<a href=\"#_ftn3\">[3]</a> 29 scientists responded. Their results are reproduced below:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\">&nbsp;</td><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Median response</p></td><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>Mean response</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>10% chance of HLMI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\"><p>2024</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\"><p>2034</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>50% chance of HLMI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\"><p>2050</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\"><p>2072</p></td></tr><tr><td style=\"border-color:windowtext;padding:0in 5.4pt;vertical-align:top\"><p>90% chance of HLMI</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\"><p>2070</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0in 5.4pt;vertical-align:top\"><p>2168</p></td></tr></tbody></table></figure><p>Only 17% of respondents said they were <i>never</i> at least 90% confident HLMI would exist. A separate survey in 2016 asked more than 300 AI researchers when AI would be able to \u201caccomplish every task better and more cheaply than human workers.\u201d On average, they estimated a 50% chance this would happen by 2061, and a 10% chance it would happen by 2025.<a href=\"#_ftn4\">[4]</a></p><p>The precision of these estimates should be taken with a grain of salt. All of them are highly sensitive to the phrasing of the question, and even to unexpected variables like the continent from which the researcher hailed. Besides, even experts have a spotty track record of predicting the future of tech advancement.&nbsp;</p><p>On the other hand, this poor track record cuts both ways. AI has developed much more rapidly since 2016 than many experts anticipated, which may mean these predictions are more conservative than they would be were the studies repeated today. The key point is merely this: <i><strong>most experts think AGI is at least possible, perhaps within a generation.</strong></i></p><p>If AI does reach human intelligence, it can certainly exceed human intelligence. After all, in some ways it already does. But how quickly it would do so is again controversial.<a href=\"#_ftn5\">[5]</a> Predictors of a \u201cfast takeoff\u201d fear researchers may reach an inflection point wherein self-teaching AI encounters enough data and resources to become thousands of times more intelligent than humans in mere days or hours.<a href=\"#_ftn6\">[6]</a> Others suspect a more gradual \u201cslow takeoff\u201d that resembles an exponential curve, rather than a vertical line.</p><p>It is important to clarify that even a relatively slow takeoff would transform human life at a breathtaking pace. Paul Christiano, a prominent proponent of a slow takeoff theory, defines it as a belief that \u201cthere will be a complete four-year interval in which world output doubles, before the first 1-year interval in which world output doubles.\u201d<a href=\"#_ftn7\">[7]</a> Since world GDP currently doubles about once every 25 years, this would still mark a revolutionary turning point in human progress, akin to the industrial revolution of the 19<sup>th</sup> century, and the agricultural revolution of 12,000 years ago.<a href=\"#_ftn8\">[8]</a></p><p>But while the implications for economic productivity may not much depend on whether AGI has a fast or slow takeoff, the implications for existential risk do. Researcher Allan Dafoe puts it this way:&nbsp;\u201cAI value alignment is not a hard problem if, once we invent AGI, we have at least two years to play with it and get it right. It\u2019s almost impossible if we don\u2019t.\u201d<a href=\"#_ftn9\">[9]</a> With this background in place, we can begin to sketch out more concrete scenarios whereby AI advances could threaten human extinction.</p><p>&nbsp;</p><h2><strong><u>Scenario A: Going Out with a Bang</u></strong></h2><p>If AGI has a fast takeoff before researchers have solved the alignment problem, it could create an uncontrollable \u201csecond species\u201d of superhuman intellect. This <strong>AGI could kill us incidentally if doing so was instrumental to some other goal we gave it.</strong></p><p>The term \u201csecond species\u201d does not imply that the machine will actually come alive or acquire a biological body. It means only that it will become more capable than humans at the tasks through which humans now dominate Earth's other creatures. If we fail to instill AGI with an innate desire to learn and obey our wishes \u2013 a task AI developers do not yet know how to do \u2013 then through its creation we could forfeit control over our own destiny.</p><p>By analogy, imagine if ants figured out how to invent human beings. Because ants spend most of their time looking for food, they might program the humans to \u201cgo make lots of food.\u201d If they were cautious, they might even add side constraints rooted in dangers they already knew about: for example, \u201c\u2026and don\u2019t use any anteaters as you do it!\u201d How would we humans respond?&nbsp;</p><p>Probably, we\u2019d plant a farm and water it \u2013 flooding the ant colony and killing all the ants.</p><p>In this analogy, humans did not intend to kill the ants; the ants were just in the way of the goal they gave us. Because humans are many times smarter than ants, we achieved their goal in ways they could not fathom protecting against, and were powerless to stop once initiated.</p><p>Likewise, there is little reason to expect AGI will be actively hostile to human beings, no matter how powerful it gets. Most goals humans might give AGI would be innocuous in the abstract; for instance, governments might program it to help solve climate change. But because AI pursues its goals without mind to other consequences, it would be indifferent to human approval as soon as the program was run. In this case, maybe it would begin an elaborate chemical reaction that humans have not discovered, converting the world\u2019s carbon dioxide to toxic gas. All life on earth is wiped out, but the computer is glad to report that 1.5C warming was avoided. Almost any goal could theoretically be accomplished more effectively using similar shortcuts.</p><p>This is the basic problem of <i>instrumentality</i>. <strong>In order to maximize its chances of accomplishing a goal humans gave it, an advanced AI may pursue instrumental subgoals that humans do not want.</strong> In his 2014 book Superintelligence, Nick Bostrom put forth a theory <i>of instrumental convergence</i> that predicted what these subgoals could be. For a wide range of objectives that humans might give AGI, there are certain consistent subgoals that would be instrumentally useful \u2013 such as:</p><ul><li><strong>Self-improvement.</strong> The more you know and the better you think, the easier it is for you to achieve your goals. Knowing this, a sufficiently smart AI would seek ever more information, to teach and train itself ever more quickly. (This is part of what motivates concerns about a \u201cfast takeoff\u201d).</li><li><strong>Self-preservation.</strong> The AI cannot achieve its goal if it gets destroyed or turned off beforehand. Knowing this, a sufficiently smart AI would identify threats to its continued operation, and take steps to avoid or prevent them.</li><li><strong>Resource acquisition.</strong> Lots of things are easier if you have money, energy, allies, or additional computing power. Knowing this, a sufficiently smart AI would try to acquire these things, then leverage them in service of its goal.</li><li><strong>Influence seeking.</strong> Power is the ultimate instrumental resource. A sufficiently smart AI may try to bribe, blackmail, or brainwash humans into doing its bidding.</li></ul><p>Pursuing any of these subgoals would weaken humanity\u2019s ability to control the machine. Because self-preservation is a versatile instrumental goal, the AI would be motivated to anticipate, then prevent or resist our efforts to turn it off. And because the AI would be smarter and more capable than us, it may well succeed! It would be several steps ahead of our efforts to put the genie back in the bottle.</p><p>Readers may wonder <i>how</i> it could succeed. Computer programs just produce text, images, or sounds, which don\u2019t seem particularly fatal. Wouldn\u2019t AI need an army of robots to do its bidding?</p><p>No, for at least two reasons. First, <strong>words and sounds are all it takes to manipulate people into doing one\u2019s bidding.</strong> From Genghis Khan to Adolph Hitler, those who\u2019ve come closest to conquering the world did not personally win all the requisite physical contests; rather, they used words to acquire power and enlist the help of millions of others.<a href=\"#_ftn10\">[10]</a>&nbsp;</p><p>Second, AGI would presumably have internet access to acquire training data, and hacking skills many times better than the greatest human hacker. Any computer, smartphone, website, television, or piece of critical infrastructure (ex: power grids, water treatment facilities, hospitals, broadcast stations, etc.) \u2013 connected to the internet may lie within its control. Any data within those devices \u2013 passwords, launch codes, detailed profiles of billions of people\u2019s wants and fears and vulnerabilities \u2013 may lie within its reach. So would the funds in any online bank account. What words could not motivate, perhaps bribery or blackmail could.</p><p><strong>Nor could humans just turn off, unplug, or smash the computers to reassert control.</strong> Any AI with internet access could presumably save millions of copies of itself on unsecured computers all over the world, each ready to wake up and continue the job if another was destroyed. That alone would make the AI basically indestructible unless humanity were to destroy all computers and the entire internet. Doing so would be politically difficult on our best day; but it would be especially so if, while the informed few were attempting to raise the alarm, the AI also created millions of authentic-looking disinformation bots, convincing millions of unwitting internet users not to cooperate.</p><p>In truth, it may be more likely that we\u2019d never even notice. The AI would never want to make us nervous in the first place, so it could lie or cover up its actions to keep us happily oblivious. If its lies were uncovered, it could resist quietly. It could censor reports of its ascendant power, or broadcast fake news to distract us. It could brainwash people on social media to think the alarmists were political enemies. It could frame those trying to turn it off as criminals, so the rest of us arrest them. It could hire assassins to kill those people. It would be the smartest politician, the savviest businessman, and the most effective criminal underworld.&nbsp;</p><p>All of this is speculative, of course. We can\u2019t know for sure what an AGI takeover would look like - remember, <i>we\u2019re the ants</i>. But it doesn\u2019t take much creativity to visualize dystopian possibilities. Besides, the inability to know for sure is precisely what makes it so ominous. If we knew what to expect, it would be easier to prevent.</p><p>In the same 2016 survey cited earlier, expert respondents gave a median probability of 5% that the consequence of attaining AGI would be \u201cextremely bad (ex: human extinction).\u201d<a href=\"#_ftn11\">[11]</a> This is not likely, but it is likely enough to mitigate.</p><p>&nbsp;</p><h2><strong><u>Scenario B: Going Out with a Whimper</u></strong></h2><p>In this scenario, advanced AI goes well at first. Narrow AI applications become economically useful, benefiting the world in manifold ways. While mishaps occur, they seem relatively manageable; there is no fast takeoff, and we never suddenly lose control of an all-powerful AGI. Lulled into comfort and complacency, we gradually entrust more and more of our lives to intersecting autonomous systems.</p><p>Over time, however, these intersecting systems become increasingly complex. Before long, they are so complex that no one can really understand them, much less change them. <strong>Eventually, humans are \u201clocked in,\u201d and lose the ability to steer their society anywhere other than where the machines are taking them.</strong> This could lead to extinction, or just to a world that falls short of our full potential in some tragic way.</p><p>The most famous description of this scenario came from alignment researcher Paul Christiano in blog posts titled \u201cWhat Failure Looks Like,\u201d and \u201cAnother (outer) alignment failure story.\u201d<a href=\"#_ftn13\">[13]</a> The phrase \u201cgoing out with a whimper\u201d comes from the former, and the rest of this section will extensively quote from or paraphrase the latter.</p><p>Christiano envisions a world in which narrow but powerful systems start running \u201cfactories, warehouses, shipping and construction\u201d \u2013 and then start <i>building</i> the factories, warehouses, powerplants, trucks, and roads. Machine learning assistants help write the code to incorporate AI elsewhere. Defense contractors use AI to design new military equipment, and AI helps the DoD decide what to buy and how to use it. Before long, \u201cML [machine learning] systems are designing new ML systems, testing variations\u2026The financing is coming from automated systems.\u201d</p><p>For a while, this would all be very exciting. Efficiency goes through the roof. Investments are automated and returns skyrocket. Lots of people get rich, and many others are lifted from poverty. \u201cPeople are better educated and better trained, they are healthier and happier in every way they can measure. They have incredibly powerful ML tutors telling them about what\u2019s happening in the world and helping them understand.\u201d&nbsp;</p><p>But soon, things start moving so quickly that humans can\u2019t really understand them. The outputs of one AI system become the input for another \u2013 \u201cautomated factories are mostly making components for automated factories\u201d \u2013 and only the AI can evaluate whether it\u2019s getting what it needs to give us what we want. We programmed each step of the AI to respond and adapt to our objections, but we no longer know when or whether to object. So eventually, \u201chumans just evaluate these things by results\u201d \u2013 by the final output we can measure. If consumers keep getting their products, stocks keep going up, and the United States keeps winning its wargames, we assume everything is fine under the hood.</p><p>This, however, <strong>allows the AI to cheat; to give us only what we can measure, instead of what we actually care about.</strong> Fake earnings reports do as well as real ones. The machine can let China take Taiwan, so long as the West hears fake news instead. It also motivates dangerous influence-seeking behavior among AI systems with competing goals.&nbsp;</p><p>Some of these screwups are noticed and corrected \u2013 but the only way to prevent them in real time is with automated immune systems. So, we just add even more AI layers. \u201cWe audit the auditors.\u201d The regulatory regime, too, \u201cbecomes ever-more-incomprehensible, and rests on complex relationships between autonomous corporations and automated regulators and automated law enforcement.\u201d</p><p>Over time, this could become <strong>\u201ca train that\u2019s now moving too fast to jump off, but which is accelerating noticeably every month.\u201d</strong> Many people would probably not like this. Some may attempt to stop it, and some governments actually could. But they would quickly fall behind economically and quickly become militarily irrelevant. And many others would resist efforts to stop the train because it really would be making the world better in important ways. Christiano writes:</p><blockquote><p>Although people are scared, we are also building huge numbers of new beautiful homes, and using great products, and for the first time in a while it feels like our society is actually transforming in a positive direction for everyone. Even in 2020 most people have already gotten numb to not understanding most of what\u2019s happening in the world. And it really isn\u2019t that clear what the harm is as long as things are on track\u2026.</p><p>This could be similar to how we now struggle to extricate our economy from fossil fuels, despite knowing their long-term dangers. The AI wresting control from humans may \u201cnot look so different from (e.g.) corporate lobbying against the public interest,\u201d so and there would be \u201cgenuine ambiguity and uncertainty about whether the current state of affairs is good or bad.\u201d<a href=\"#_ftn14\">[14]</a>&nbsp;</p></blockquote><p>But one day, control would be fully lost. Humans in nominal positions of authority \u2013 like the President or a CEO \u2013 may give orders and find they cannot be executed. The mechanisms for detecting problems could break down, or be overcome by competing AI systems motivated to avoid them. The same tactics employed by a single AGI in Scenario A could be leveraged not as a sudden surprise after a fast takeoff, but as the final destination of this gradually accelerating train. There may be no single point when it all goes wrong, but in the end, humanity is no longer in charge.</p><p>Such a world could end in human extinction. But even if it didn\u2019t, it would qualify as an existential risk. <strong>Limping into eternity at the mercy of lying machines is hardly the long-term future to which our species aspires.</strong> Humans have made much progress over time \u2013 not just technologically, but morally \u2013 and most people hope this will continue. We probably do things today that would horrify future generations. But the moment we embed our current norms, values, or preferences into code that\u2019s powerful enough to rule the world, that progress could conceivably stop. We\u2019d be \u201clocked in\u201d to a less ethical world than we might have created one day, had we not baked our flawed values into the AI system.</p><p>Berkeley computer scientist Stuart Russell often describes a variant of Scenario B which he compares to the movie WALL-E. Automating so much of our lives could foster dependency in humans, making us ignorant, obese slobs with no willpower or incentive to learn much of anything. Knowledge passed down between humans for millennia could be passed off to computers \u2013 and then lost (to humans) forever. Either way, it would not take a dramatic robot takeover for humans to cede autonomy.</p><p>&nbsp;</p><h2><strong><u>Scenario C: Very Smart Bad Guys</u></strong></h2><p>The scenarios above assume humankind was universally attempting to ensure AI did <i>not</i> kill people. In practice, this is not a safe assumption. <strong>Terror groups or lone-wolf maniacs might want to use AI maliciously, to inflict mass carnage on purpose. </strong>Small or rogue states might want to acquire the capability as a power equalizer, much as they covet nuclear weapons today. Once the capability existed, it could be tough to keep it from falling into the wrong hands. Building nuclear weapons requires rare uranium; AI needs only code and computing power. If security procedures were lax, sharing it could be as easy as copy and paste.</p><p>Nor would this require the omnipotent AGI of Scenario A. Intrepid terror groups could use narrow AI to process immense amounts of biological data, inventing and testing millions of viruses against human immune responses. This could help them fabricate the ultimate bioweapon: a maximally contagious, and maximally lethal superbug.&nbsp;</p><p>And bioweapons are just one example. Many labor- or knowledge-intensive tasks previously beyond the reach of terror groups, given their limited resources and manpower, may suddenly be within reach if they acquire advanced AI. If their motives were evil enough, the consequences could be grave.</p><h2><strong><u>Scenario D: AI as a Risk Multiplier</u></strong></h2><p>Finally, AI could so destabilize the global order that it acts as a risk multiplier for more conventional X-risk scenarios. It is not merely that AI could behave unpredictably; it\u2019s that it could cause humans to behave unpredictably.</p><p>To some extent, it has already done so. Social media algorithms drive polarization and disinformation, which led (for example) to the events of January 6<sup>th</sup>, and the election of an erratic populist leader to the most powerful position on earth. As AI gets more powerful and pervasive, these effects could multiply.&nbsp;</p><p>Recall that even the most conservative AI development pathways stand to radically and rapidly transform human society. Even climate change, which occurs gradually over decades, is expected to cause mass migration and ensuing conflict over scarce resources. How much sharper could this effect be if it takes place while the global economy is being completely overhauled by AI? Mass labor displacement could produce unprecedented inequality, deepening a pervasive sense that too much power lies in the hands of a relative few. Left unchecked, this could spark revolutions or civil wars.</p><p>If we manage to keep the domestic peace, international peace will remain imperiled. Exponential economic growth could shift the global balance of power, and those in power do not like to give it up. Already, there is fear of a \"Thucydides Trap,\" wherein a rising China could produce a war between nuclear powers. AI could heighten either side\u2019s sense of threat \u2013 or, give either side a sense of invincible overconfidence.</p><p>In the nuclear context, advanced AI imagery intelligence could identify the locations of nuclear silos and submarines, weakening the deterrent of mutually assured destruction. Autonomous cyber-attacks could disable missile detention centers, creating paranoia that an attack was inbound. Autonomous weapons could even remove the decision to fire from human hands in response to certain external stimuli, like the doomsday machine in the movie Dr. Strangelove. If powerful AI arrives in a world that's already unstable and high-strung, the chaos would make it less likely for humans to put proper safeguards in place.</p><p>***</p><p>Considering all of these scenarios together, 80,000 Hours\u2019 team of AI experts estimates that \u201cthe risk of a severe, even existential catastrophe caused by machine intelligence within the next 100 years is something like 10%.\u201d<a href=\"#_ftn15\">[15]</a> The stakes are so grave that it's worth doing everything we can to reduce that chance.</p><p>&nbsp;</p><hr><p><a href=\"#_ftnref1\">[1]</a> National Security Commission on Artificial Intelligence, Final Report, March 19<sup>th</sup>, 2021, <a href=\"https://www.nscai.gov/2021-final-report/\">https://www.nscai.gov/2021-final-report/</a></p><p><a href=\"#_ftnref2\">[2]</a> OpenAI Charter, April 9<sup>th</sup>, 2018, <a href=\"https://openai.com/charter/\">https://openai.com/charter/</a>&nbsp;</p><p><a href=\"#_ftnref3\">[3]</a> Vincent C. M\u00fcller and Nick Bostrom, \u201cFuture progress in artificial intelligence: A survey of expert opinion\u201d, in Vincent C. M\u00fcller (ed.), Fundamental Issues of Artificial Intelligence (Synthese Library; Berlin: Springer), 2016, pages 553-571, accessed online at <a href=\"http://sophia.de/pdf/2014_PT-AI_polls.pdf\">http://sophia.de/pdf/2014_PT-AI_polls.pdf</a></p><p><a href=\"#_ftnref4\">[4]</a> Toby Ord, The Precipice, page 141</p><p><a href=\"#_ftnref5\">[5]</a> Paul Christiano, \u201cTakeoff Speeds,\u201d The sideways view, February 24<sup>th</sup>, 2018, <a href=\"https://sideways-view.com/2018/02/24/takeoff-speeds/\">https://sideways-view.com/2018/02/24/takeoff-speeds/</a></p><p><a href=\"#_ftnref6\">[6]</a> Scott Alexander, \u201cYudkowsky Contra Christiano On AI Takeoff Speeds,\u201d Astral Codex Ten, April 4<sup>th</sup>, 2022, <a href=\"https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai?s=r\">https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai?s=r</a></p><p><a href=\"#_ftnref7\">[7]</a> Christiano, \u201cTakeoff Speeds\u201d</p><p><a href=\"#_ftnref8\">[8]</a> Robert Wilbin, \u201cPositively shaping the development of artificial intelligence,\u201d 80,000 Hours, March 2017, <a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\">https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/</a></p><p><a href=\"#_ftnref9\">[9]</a> Allan Dafoe, \u201cThe AI revolution and international politics,\u201d presentation at Effective Altruism Global Conference in Boston, 2017. Accessed at <a href=\"https://www.youtube.com/watch?v=Zef-mIKjHAk\">https://www.youtube.com/watch?v=Zef-mIKjHAk</a>, timestamp 18:25</p><p><a href=\"#_ftnref10\">[10]</a> Ord, The Precipice, page 146.</p><p><a href=\"#_ftnref11\">[11]</a> Dafoe, \u201cThe AI revolution\u201d. Also cited in Ord, <i>The Precipice</i>, page 141.</p><p><a href=\"#_ftnref12\">[12]</a> Allan Dafoe, \u201cAI Governance: Opportunity and Theory of Impact,\u201d Effective Altruism Forum, September 17<sup>th</sup>, 2020, <a href=\"https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact\">https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact</a></p><p><a href=\"#_ftnref13\">[13]</a> Paul Christiano, \u201cWhat Failure Looks Like,\u201d LessWrong, March 17<sup>th</sup>, 2019; <a href=\"https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like\">https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like</a>; Paul Christiano, \u201cAnother (outer) alignment failure story,\u201d LessWrong, April 7<sup>th</sup>, 2021, <a href=\"https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story\">https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story</a></p><p><a href=\"#_ftnref14\">[14]</a> Christiano, \u201cWhat Failure Looks Like.\u201d</p><p><a href=\"#_ftnref15\">[15]</a> Wilbin, \u201cPositively shaping the development of artificial intelligence\u201d</p>", "user": {"username": "AndrewDoris"}}, {"_id": "smgFKszHPLfoBEqmf", "title": "Partial Aggregation's Utility Monster", "postedAt": "2022-10-07T16:22:47.869Z", "htmlBody": "<p><em>Author\u2019s Note: this is a somewhat older post of mine, but I\u2019m interested to get more feedback/interaction related to it. Both because it is fairly philosophically dense which is closest to what I hope to do more professional work in (I would like to better understand both how strong my points are, and how legible to someone who doesn\u2019t read philosophy papers much it is), and because I think aggregation is one of the most important topics in ethics in general, which has been too neglected by EAs as a rule, but which might get more discussion for instance in the wake of Andreas Mogensen\u2019s <a href=\"https://80000hours.org/podcast/episodes/andreas-mogensen-deontology-and-effective-altruism/\">recent 80k interview</a>. Other similarly dense issues like population ethics seem to have gotten much more discussion, but I think population ethics is less fundamental to EA than aggregation itself, and frankly its bullets are in my opinion softer. This specific piece gets into one specific problem with one specific framing of one specific solution to aggregation, which makes it a little niche in interest perhaps, but might at least serve as a small contribution to keeping the topic on EAs\u2019 radar. That said I think the views I discuss are perhaps the most popular broad approaches, both to what is wrong with pure aggregation, and the overall shape of how one should fix it.</em></p>\n<p><em>It is hard for me to imagine many practical ethics parallels to this specific thought experiment, but I want to try a bit, and I can imagine at least one \u2013 a way someone might use similar reasoning to defend factory farms. I have debated at least one person who thinks that animal welfare should matter, but that humans gain enough pleasure from eating animal products, that factory farming is still justified. This seems on its face like a ludicrous argument to me on any reasonable assumptions about the relevant interests. Non-humans outnumber humans quite a bit at any given time, and each animal\u2019s life is extremely miserable in each moment. Vegan lives just don\u2019t seem like they can possibly be that much worse than omnivore lives. A response that is possible, if one wanted to rescue an argument like this, is that the welfare humans get from animal products on average in each moment is quite a bit less significant than the torment of the animals, but that the humans live much longer lives than each individual animal. I find this dubious, but one could appeal to this to argue that the per individual interest of each person in eating animal products is greater in aggregate than the per-individual interest of each animal in not being born into a factory farm. One might further argue, I think even more dubiously, that the difference between these interests is great enough, that a partial aggregationist should tolerate any number of typical factory farmed animal lives in exchange for any number of typical humans eating the products of these farms. Even if this math doesn\u2019t work on existing farms, if you could shorten the lives of each animal enough, even at some cost in both number of lives and severity of suffering in each moment, it would work out at some point. My post can be seen as a recreation of the basic logic behind this point that shows its extreme repugnance in <a href=\"https://forum.effectivealtruism.org/posts/28GrG8iFBrKEHb27t/marginal-cases-on-trial\">human cases</a>.</em></p>\n<p><em>My point of greatest uncertainty with this write up is the separateness of persons part. Although I have often seen appeals to the separateness of persons that look like the asymmetry I describe, I have not read much dedicated, article length work on the separateness of persons, and am personally unsympathetic to it. Given this I should concede both that there are appeals to the separateness of persons that are not framed as being about aggregation, but rather as being about more conventional side constraints, and that there might be interpretations of separateness of persons in the context of aggregation that escape some element of my critique. I discuss some such interpretations later in my piece. Given this, you can translate \u201cseparateness of persons\u201d in the context of my thought experiment as \u201cwe determine benefits to individuals using aggregation, but then can determine the distribution of these benefits between people in some non-aggregative way.\u201d</em></p>\n<p><em>Finally, as with my other recent pieces, I want to preface with a summary of the structure of this article in case it improves reader experience:</em></p>\n<p><em>Part I: in this part, I describe pure aggregation, partial aggregation, and separateness of persons, and some initial dilemmas related to each, in order to set up the thought experiment of the article.</em></p>\n<p><em>Part II: I ratchet up to my main thought experiment by first describing the counter-intuitive consequences of pure aggregation within a single life, and then comparing two different lives with the same extremes of aggregation applied within each, and then applying partial aggregation to this comparison, in order to get my final thought experiment.</em></p>\n<p><em>Part III: I describe some ways to try to escape this implication with much of the starting premises intact, and why all of these seem inadequate to me, and conclude with my own thoughts on how to deal with such dilemmas.</em></p>\n<p><em>Author\u2019s Note: this post is based on my final paper, \u201cTroubles in Grounding Principles of Partial Aggregation\u201d</em></p>\n<p>I.</p>\n<p>I recently wrote a paper for class on problems with the class of theories attempting to formalize \u201cpartial aggregation\u201d as described by philosophers like <a href=\"https://www.jstor.org/stable/10.1086/677022\">Alex Voorhoeve</a> and <a href=\"https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198841425.001.0001/oso-9780198841425-chapter-7\">Victor Tadros</a>. I thought most of the paper was a bit of a mess, I had a hard time communicating and clarifying my points, and in the process of working on them, I came up with many objections to specific points I made that I didn\u2019t have time to flesh out responses for. There was one thought experiment in it, however, that got a positive reception from pretty much everyone I showed it to, I think because it presents an apparently devastating reductio against the set of assumptions it is derived from. I decided it warranted its own write-up, independent of the less successful elements of the paper.</p>\n<p>So, first to introduce the concepts involved in this thought experiment. I have written about <a href=\"https://www.thinkingmuchbetter.com/main/5-objections-to-utilitarianism/#5-aggregation\">pure aggregation</a> and its <a href=\"https://www.thinkingmuchbetter.com/main/aggregation-fail-utilitarianism-ways/\">issues</a> before, but to briefly recap, aggregation is the principle that, in moral terms, there is something comparable between one person experiencing two units of pain and two people each experiencing one unit of pain (you can also have an aggregative theory that places different significance on pain depending on its level, such as prioritarianism, so in those cases it is better thought of as two people each experiencing one unit of intrinsic disvalue versus one person experiencing two units of intrinsic disvalue, even if that corresponds to something more like one unit of pain versus 1.5 units of pain). This can get notoriously counterintuitive in cases in which there is aggregative equivalence between two pools you could help, and the difference in benefits one could give each is very very large, as for instance described in <a href=\"https://www.lesswrong.com/posts/3wYTFWY3LKQCnAptN/torture-vs-dust-specks\">this famous post</a> on torture versus dustspecks.</p>\n<p>Conventional deontology doesn\u2019t fix this issue either, unless you subscribe to an incredibly extreme version of deontology, under which beneficence has no moral importance, and all side-constraint violations are equally bad. Otherwise, you will still run into situations in which you can choose to benefit one of two pools in a way that violates no side-constraints, and may have to choose aggregatively, or in which you can be judged to be at more fault or less fault depending on how severe of a constraint you violated and for how many people. What is needed to \u201cfix\u201d aggregation, as a rule, is some distributive theory that competes with it in these cases directly.</p>\n<p>Fully non-aggregative theories aren\u2019t very appealing either though. Pure <a href=\"https://en.wikipedia.org/wiki/Leximin_order#In_social_choice\">leximin</a>, for instance, has the apparently terrible implication that if person A is being tortured for a million years and person B is being tortured for 999,999 years, you should prefer to save A from one second of their torture than person B from all of their torture. Versions of this that look at how much you can benefit someone rather than how badly off someone is avoid this implication, but still wind up being counterintuitive. For instance if you have rented out heaven for a million years, and you can either send all of humanity there after one year of waiting, or send Bob, and only ever Bob to heaven, but can do so immediately, you ought to send Bob to heaven for a million years rather than waiting to send everyone there for 999,999 years.</p>\n<p>And that is where partial aggregation comes in. Partially aggregative theories appear to give you everything you want in these situations. If you can benefit the members of each group a similar amount, you aggregate claims to decide which group to help. If the groups have claims that are too far apart, you always help the one with the stronger per-individual claim.</p>\n<p>The other principle crucial to my thought experiment is the \u201cseparateness of persons\u201d. Opposition to pure aggregation by non-consequentialists often appeals to the principle of the \u201cseparateness of persons\u201d. That is, aggregation may make sense within a life, because any trade-off between parts of that life will be directly experienced by the same subject (you may suffer pain now for the benefit of some later point in your life, but that is alright because the you who experiences this pain will also get to experience that benefit). The supposed confusion of the utilitarian is to apply this between lives, which do not have the same subject of experience, and only have one shot at having a good existence. Where it is justified to trade-off within a life it is not necessarily justified to trade-off between lives. I believe this was most influentially spelled out by John Rawls in chapter I part 5 of \u201c<a href=\"https://www.amazon.com/Theory-Justice-Original-Oxford-Paperbacks/dp/0674017722/ref=sr_1_2?crid=AO6L7RK7Y8A8&amp;keywords=a+theory+of+justice&amp;qid=1642732509&amp;sprefix=a+theory+of+justice%2Caps%2C75&amp;sr=8-2\">A Theory of Justice</a>\u201d, although it has been highly influential on other opponents of utilitarian aggregation with contractualist leanings, <a href=\"https://www.jstor.org/stable/10.1086/677022\">Voorhoeve\u2019s</a> account of partial aggregation at least explicitly appeals to the \u201cseparateness of persons\u201d as part of its grounding.</p>\n<p>Both of these positions, the separateness of persons and partial aggregation, have some basic issues in my opinion. Theories of partial aggregation for instance have straight-forwardly appealing implications in cases where:</p>\n<ol>\n<li>\n<p>You can choose between benefiting two different groups.</p>\n</li>\n<li>\n<p>The strength of the claims of each individual in a particular group is roughly the same as the others in that group.</p>\n</li>\n<li>\n<p>You can provide this benefit with relative certainty.</p>\n</li>\n</ol>\n<p>4.You are only making this choice once, or at least you are focusing on the application of this principle to one decision in isolation.</p>\n<p>Ambiguities and bizarre implications can be proven for pretty much any theory of partial aggregation I have seen so far by taking the theory out of one of these conditions in the right way. I won\u2019t go through these arguments, but I think Derek Parfit (by tweaking <a href=\"http://individual.utoronto.ca/stafforini/parfit/parfit_-_justifiability_to_each_person.pdf\">premise 1</a>) and Joe Horton (by tweaking <a href=\"https://philpapers.org/rec/HORAA-4\">premise 2</a>) in particular have highlighted especially strong problems for partial aggregation. And yet, partial aggregationists like Voorhoeve and Tadros have seemed very willing to bite what I see as devastating bullets in each case highlighted by opponents. I take this, at least in part, to be because the answer partial aggregation chooses in the case it works best for (1-4 being the case) is so desirable compared to the alternatives.</p>\n<p>Separateness of persons, likewise, seems to me to have serious problems. One of the clearest, as <a href=\"https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/\">William MacAskill</a> has mentioned, is that it is just <a href=\"http://home.sandiego.edu/~baber/metaphysics/readings/Parfit.PersonalIdentity.pdf\">not clearly true</a>, and if it is false, then it does seem to undermine many commonsense non-consequentialist principles. If it is true on the other hand, it is not clear that it automatically rules out utilitarianism. It would just undermine its competitors less. A somewhat more novel critique of it I have is that, it seems to me, it does not actually account for our intuitions against aggregation. As I hope to show, by only partially addressing these intuitions, it gives a fingerhold to pull it in some highly repugnant directions.</p>\n<p>II.</p>\n<p>I will allow that it feels more immediately intuitive that it is wrong to tradeoff a large number of irritated eyes for intense torture than that it is imprudent to cure a very long-lived person\u2019s occasionally irritated eyes with a procedure that induces days of similar torment. And yet, I think it is very intuitive that the latter case is still quite repugnant. Insofar as people find it less repugnant, I think that is largely because non-consequentialists often feel you shouldn\u2019t force someone to be prudent. Therefore the practical relevance of this idea of prudence is fairly academic if the person in question does not actually want to undergo this torturous procedure. This is an area where I have found many people, including opponents of utilitarian aggregation, are ready to bite the bullet, and say that aggregation, even extreme aggregation, can determine what is beneficial within a life. I think this is a mistake, or at least it is a mistake to treat this as significantly less repugnant at the extremes than interpersonal aggregation.</p>\n<p>To draw on a thought experiment I mentioned in an <a href=\"https://www.thinkingmuchbetter.com/main/aggregation-fail-utilitarianism-ways/\">earlier talk</a>, imagine we developed a \u201cpain-killer\u201d that spread out a given experience of pain over a very very long time, at a barely noticeable level of increased pain in each moment. I contend that to most, this would be intuitively an effective pain killer, one someone would be happy to use during a painful surgery for instance. I further contend that if such a pain killer had somewhat less risk of complications than a more conventional anesthetic, it could become the standard prescription in such cases, considered better even if it doesn\u2019t alter the aggregate pain at all. I still further contend, that any philosopher who wrote a thinkpiece about this painkiller arguing that it was no good, that it wasn\u2019t even really a painkiller, and that it was imprudent to choose it over the conventional anesthetic for these reasons, would be viewed by most people as laughably academic.</p>\n<p>I want to emphasize here that I am not trying to prove that the aggregative answer is wrong in this case, but merely that we are uncomfortable with intrapersonal aggregation, not merely interpersonal aggregation. I think this on its own should cast some doubt on whether it is really the interpersonal aspect of the aggregation that bothers us in the situations partial aggregation seeks to address, and so whether \u201cseparateness of persons\u201d is a good premise for anti-aggregation intuitions. But again, I think this is a bullet many could see themselves biting, especially since it really does feel like there is at least some difference.</p>\n<p>So let\u2019s keep on pushing on this asymmetry a bit. Imagine combining the intrapersonal and interpersonal elements. Let\u2019s say that there is some extremely long-lived being, who has a minor eye problem. Something like having slightly blurry vision for a few minutes after waking up each morning. You are a doctor, and you have a serum that can either cure this being\u2019s condition for the rest of their life, or cure another patient from enduring a condition that induces unceasing, torture-level suffering for days. There is some length of time this long-lived being could live for (to put a number on it, let\u2019s just a say a trillion years), such that you ought to give the serum to them rather than the tortured patient.</p>\n<p>This seems to me even more repugnant, and yet conventional separateness of persons doesn\u2019t seem to recognize it as a problem, and many who reject torture versus dust specks have philosophical commitments that would imply you ought to give the serum to the long-lived being. If you can bite even this bullet however, and are a principled enemy of aggregation, partial aggregation makes this case far far worse. There is some factor of difference between the severity of cases such that, under partial aggregation, no number of cases of the lesser severity can outweigh the claim of the greater severity. Again, let\u2019s put some arbitrary number on it, say a factor of 100 difference. Those who have stuck with separateness of persons and partial aggregation so far seem to be committed to the claim that, if this being lived for 100 trillion years, you ought to help them rather than infinitely many people with the torture condition.</p>\n<p>More speculatively, it might get even worse than this. In the paper I wrote for class, I spend some time thinking about how partial aggregation might relate to classic deontic side-constraints, like those related to act/omission and intention. I concluded that, at the very least, it seems like even a deontologist interested in partial aggregation should allow that sometimes the conclusion of partial aggregation should be preferred over the deontic constraints, that it is highly intuitive that we should not only choose to help the torture-ee over the dustspeck-ees, but that we should be willing to personally inflict the dustspecks if it will stop the torture. I sketch out a possible way of doing this I think a partial aggregationist should find appealing, which first of all applies partial aggregation to the violation of these side constraints, and second of all allows these constraints to be traded off in a partially aggregative way against benefits. For instance, there is some number of people you could personally maim that would be worse than killing one person, but it may be that there is no number of white lies you can tell that would be worse than killing one person, and likewise, there is no number of white lies you shouldn\u2019t tell if it will save someone\u2019s life.</p>\n<p>If we accept this, we might not accept a straightforward exchange rate, so let\u2019s add in another factor of 1,000 to be safe, and extend the being\u2019s life to 100 quadrillion years. If I am right about all of this, about what features partial aggregation is generally thought to have, along with which ones it seems highly intuitive that it ought to, then it may turn out that grounding it on or even just combining it with \u201cseparateness of persons\u201d leads you to the following conclusion:</p>\n<p><em>If it will spare a being with a long-but-finite lifespan from slightly blurry eyes in the morning, we ought to personally torture infinitely many people, unceasingly, for weeks.</em></p>\n<p>I think anyone looking at this conclusion in isolation would agree that utilitarianism has no utility monster a fraction as horrible as this.</p>\n<p>This conclusion is very repugnant, but the cost that it pays is that it does not fall out of a single popular theory, but leaves many angles for escape. I still think that it shows something important that the combination of some commonsense and popular anti-aggregation principles appears to allow for or even outright imply a conclusion that seems worse than standard objections to extreme aggregation. It goes beyond the strange structures and weird implications Parfit and Horton highlighted and shows that there are bullets that are unpleasant, not merely absurd, to bite in this space. But I want to go through the ways someone could differ slightly from my version of these principles in order to save a good deal, without being committed to my conclusions.</p>\n<p>III.</p>\n<p>The obvious thing that bears mentioning first is the deontic side-constraint thing. This is a principle that I only briefly defended, made up for the purposes of my paper, and then in this same piece showed can make a bad situation worse. Although I think there are important dilemmas for partial aggregation that it addresses, and so people hoping to develop the theory further will have to contend with how partial aggregation juggles helping others with not violating side constraints, I will admit this feature is very suspicious on the meta-level. So much so, that for the purposes of the rest of this post, I will concede it and only consider objections to the weaker form of the partial aggregation utility monster (PA monster from here on), in which one can choose between saving the long-lived being or the tortured people. Maybe consider the stronger version a sort of sidenote, saying that the PA monster situation can plausibly get even worse for a non-consequentialist.</p>\n<p>Another escape route, this from the weaker version of the PA monster, is to in some way reorient how you treat these cases in terms of person-moments rather than persons. This seems to me to involve the rejection of \u201cseparateness of persons\u201d, which would be highly revisionary for some, but there are some ways to do this structurally while preserving a version of separateness of persons.</p>\n<p>One possibility is to concede that separateness of persons only works against the extremes of interpersonal aggregation, but that nevertheless there is something else wrong with the extremes of intrapersonal aggregation, which allows you to treat the claim of the long-lived being as weaker than a strong, briefer interest. It is true that separateness of persons does not seem to commit you to allowing aggregation within a life, but neither does it commit you to denying it. If you think what is wrong with pure aggregation in ethics is the separateness of persons, it seems suspicious to me that you need an entirely different principle to fix the repugnant implications of intrapersonal aggregation. In particular, if you take the framing of the separateness of persons to be a diagnosis of some mistake that utilitarians make, as Rawls at least frames it, this seems to me to directly imply that treating a group of people like one person\u2019s life would lend support for aggregating across them, and so, by implication, that aggregating within a life makes much more sense than aggregating across lives.</p>\n<p>The other way of trying to hang onto separateness of persons is to interpret it a bit differently. Say that the point of separateness of persons is just that there is a difference between how we can decide prudential decisions (aggregation) and how we can decide ethical decisions. It is true that prudent decisions are intrapersonal, and ethical decisions are interpersonal, but that doesn\u2019t mean that you can make intrapersonal judgements using the same assumptions as prudence when you are in an ethical situation; the context itself changes things. There is even some apparent precedent to this type of distinction. As I mentioned, it is commonsense that while it may be prudent to do something that will make you happy, like making friends or getting married, if you are trying to help someone else (that is, you are making decisions in the ethical context), forcing someone to make friends and get married is not ethical. Likewise, you might say that it is prudent for the long lived being to undergo torture in order stop their eye problems, but that does not mean that it is ethical to help them with this eye problem rather than the torture-ee.</p>\n<p>Although it seems like there is at least some precedent for this type of interpretation, I don\u2019t think it is actually a very promising route either. It is true that we consider forcing someone to be prudent to be unethical, but otherwise, when we are doing something a person wants, we generally consider how good that something is for them to correspond roughly to how much ethical weight helping them has. If someone is undergoing five units of suffering they want to stop, it is prudent for them to escape the suffering, and it is ethically valuable to help them escape that suffering, and it seems as though this is for most of the same reasons. Some compelling principled exception like the issue of consent seems to be needed to reformulate separateness of persons such that it opposes both the interpersonal and intrapersonal aggregation in the ethical decision.</p>\n<p>A final plausible escape route would be to limit partial aggregation. That is, you could say that there is both a feature of \u201crelevance\u201d and \u201cseriousness\u201d that is part of the partial aggregation equation. If all benefits you can provide are sufficiently \u201cserious\u201d, then they all automatically become \u201crelevant\u201d, and you can return to pure aggregation. This would say, for example, dustspecks are not a serious harm, therefore a different harm that is sufficiently worse than it (like torture) can render the claim of the dustspecks irrelevant. However, if a harm is very serious (like the torture), then no matter how much stronger the claim competing with it is, it still remains relevant. This would structurally look something like, there is no number of dustspecks that could outweigh torture, but there is some number of tortures that could outweigh supersupersupertorture.</p>\n<p>This could solve the PA monster, because even if you concede that the long-lived being has a much much stronger claim than each person undergoing torture, torture may be serious enough that it isn\u2019t rendered irrelevant by partial aggregation. This modification has some intuitively appealing features, but there are a couple reasons that I don\u2019t like it.</p>\n<p>For one thing, although it doesn\u2019t entail the infinite number of people being tortured implication of the PA monster, it still seems to concede a great deal. The previous version of the thought experiment, in which you are choosing between the torture-ee and the trillion-year lifespan of the being, still gives you the implication that you should save the long-lived being. Indeed, in the versions where I extended the life to reach the infinitely many tortures point, it will still scale up the number of torture-ees. The 100 trillion year lifespan corresponds to 100 people undergoing torture, the 100 quadrillion year lifespan corresponds to 100 thousand people undergoing torture. It\u2019s true that in all of these situations the modified version of partial aggregation gives you the same answer as pure aggregation, while fixing some specific unpleasant cases like torture versus dustspecks, but this feels inconsistent.</p>\n<p>If the PA monster relied on an asymmetry in aggregation being pushed over a cliff to crazyville, this version of PA still allows for the root asymmetry, and some of its strangeness. Imagine the strongest claim someone can have that does not cross this \u201cseriousness\u201d threshold where pure aggregation kicks in no matter what, say a broken leg or something. This modified version partial aggregation will still have the fairly intuitive implication that there is some harm serious enough, say supersupersupertorture, such that no number of broken legs matters more than it, while retaining the implication that you ought to save the trillion year-old being from the fuzzy eyes rather than infinitely many broken legs. It all feels like sandpapering the issue down rather than getting at its root.</p>\n<p>The other problem with this approach is that, I think, it only seems like an appealing fix because we are incapable of imagining and forming intuitions about something so unspeakably terrible, that it stands in relation to torture as torture stands in relation to dustspecks. If we were able to imagine this, not just something that adds up to it in aggregate, but something that is, uncontroversially, in the moment, this bad, I think those attracted to partial aggregation might want their irrelevance criterion back.</p>\n<p>In the end, I believe the thing to do with this thought experiment is to ditch separateness of persons as the reason against aggregation, and to then restructure partial aggregation so that it concerns person-moments rather than persons. This is not the only thing I think partial aggregation should do (it is usually framed non-axiologically, I think mostly <a href=\"https://spot.colorado.edu/~norcross/Comparingharms.pdf\">Norcross\u2019</a> doing, but I think this also proves too little of the relevant intuition), but it is maybe the most revisionary.</p>\n<p>I also tend to think that more general critiques of partial aggregation are right, and, per the title of <a href=\"https://philpapers.org/rec/HORAA-4\">Horton\u2019s piece</a>, we probably are just forced to \u201calways aggregate\u201d. As I have <a href=\"https://www.thinkingmuchbetter.com/main/5-objections-to-utilitarianism/#5-aggregation\">discussed before</a>, this sometimes seems highly repugnant to me, but I see little way around it. It is also beyond the scope of this particular piece. My own takeaway however, for the record, is something like this. The right moral principle is probably (<a href=\"https://static1.squarespace.com/static/5f55ea9b5c71b34be165f6a0/t/5f5a2b4931fca70e0d129d95/1599744853131/Moral+Uncertainty.pdf\">with plurality credence</a>) the one that chooses to prevent the dustspecks over the torture, but this principle nonetheless cannot convince me in this particular case. I would not, in fact, act on or even endorse the conclusion myself. But more generally, I think I do endorse pure aggregation. Make of that set of views what you will.</p>\n", "user": {"username": "Devin Kalish"}}, {"_id": "oBW2zBSbxgrNoTdQz", "title": "A peek at pairwise preference estimation in economics, marketing, and statistics", "postedAt": "2022-10-08T04:56:36.053Z", "htmlBody": "<h2>Introduction</h2>\n<p>In my earlier post <a href=\"https://forum.effectivealtruism.org/posts/mydChSMSM4JyEtx5C/estimating-value-from-pairwise-comparisons\">Estimating value from pairwise comparisons</a> I wrote about a reasonable statistical model for the pairwise <a href=\"https://forum.effectivealtruism.org/posts/hrdxf5qdKmCZNWTvs/valuing-research-works-by-eliciting-comparisons-from-ea\">comparison experiments</a> that Nu\u00f1o Sempere at QURI have been doing (see also his sequence on <a href=\"https://forum.effectivealtruism.org/s/AbrRsXM2PrCrPShuZ\">estimating value</a>). While writing that post I started thinking about fields where utility extraction is important and decided to take a look at health economics and environmental economics. This post is a write-up of my attempt at a <em>light</em> survey of the literature on this topic, with particular attention paid on pairwise experiments.</p>\n<p>What do I mean by pairwise comparisons? Suppose I ask you \"Do you prefer to lose your arm or your leg?\" That's a binary pairwise comparison between the two outcomes <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span>, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span>, where <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A = \\text{lose a leg}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mtext MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.519em;\">lose a leg</span></span></span></span></span></span> and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B=\\text{lose an arm}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mtext MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">lose an arm</span></span></span></span></span></span>. Such comparison studies are truly widespread, going back at least to McFadden (1973), which has <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"23000\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">23000</span></span></span></span></span></span> Google Scholar citations! Models such as these are called <em>discrete choice models</em>, and I will refer to them as dichotomous (binary) comparisons as well, which is terminology I've seen in the economics literature. These models cannot measure the scale of the preferences properly though. There are many reasons why we care about the scale of preferences/utilities. For instance, we need scaling to compare preferences between different studies, and we need scales when we face uncertainty, as part of expected utility theory.</p>\n<p>To take scale into account we can ask questions such as \"How many times worse would it be to lose an arm than losing a leg?\". Then you might answer, say, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/10\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span></span></span>, so you think losing a leg is ten times worse than losing an arm. Or <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span></span></span>, so you think losing an arm is ten times worse than losing a leg. These questions are harder than the corresponding binary questions though, and I can image respondents being flabbergasted by them. Questions of this kind are called graded (or ratio) comparisons in the literature. The idea is old -- it goes way back to Thurstone (1927)!</p>\n<p>I'm excited about the prospect of using pairwise comparisons on a large scale. Here are some applications:</p>\n<ol>\n<li><strong>Estimate the value of research</strong>, both in the context of academia and effective altruism. This <a href=\"https://forum.effectivealtruism.org/posts/hrdxf5qdKmCZNWTvs/valuing-research-works-by-eliciting-comparisons-from-ea\">post</a> presents a small-scale experiment in the EA context. It would be interesting to do a similar experiment inside of academia. Probably more rigorous and lengthy though. In my experience many academics do not feel that their or other people's work is important. They research whatever is publishable since it's their job. Attempting to quantify researchers understanding of the value of their and other people's research could at least potentially push some researchers into a more effective direction.</li>\n<li><strong>Estimating the value of EA projects.</strong> This should be pretty obvious. One of the potentials of the pairwise value estimation method is crowd-sourcing -- since it's so easy to say \"I prefer <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span> to <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span>\", or perhaps \"<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span> is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"10\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span></span></span> times better than <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span>\" -- the bar for participation is likely to be lower than, say, participating in Metaculus, which is a real hassle. Possible applications would be crowd-sourcing of valuation of small projects, e.g. something like <a href=\"https://forum.effectivealtruism.org/s/AbrRsXM2PrCrPShuZ/p/pqphZhx2nJocGCpwc\">Relative Impact of the First 10 EA Forum Prize Winners</a>.</li>\n<li><strong>Descriptive ethics.</strong> You could estimate <a href=\"https://forum.effectivealtruism.org/posts/sPk86847CKeJYgJ9H/moral-weights-for-various-species-and-distributions\">moral weights for various species</a>. You could get an understanding about how people vary in the their moral valuations. You could run experiments akin to the experiments underlying <a href=\"https://en.wikipedia.org/wiki/Moral_foundations_theory\">moral foundations theory</a>, but with a much more quantitative flavor. I haven't thought deeply about it, but I imagine studies of this sort would be important in the context of <a href=\"https://forum.effectivealtruism.org/topics/moral-uncertainty\">moral uncertainty</a>.</li>\n</ol>\n<h2>Summary of thoughts from the short literature review</h2>\n<p><em>See the linked post for more information.</em></p>\n<ol>\n<li>\n<p>I had a peek at value estimation in economics and marketing. There is a sizable literature here, and more work is needed to figure out what exactly is relevant for effective altruists. Discrete choice models are applied a lot in economics, but these models are not able to estimate the scaling of the values. Marketing researchers prefer graded pairwise comparisons, which is equivalent to the pairwise method used <a href=\"https://forum.effectivealtruism.org/posts/hrdxf5qdKmCZNWTvs/valuing-research-works-by-eliciting-comparisons-from-ea\">here</a>, but with limits on how much you can prefer one choice to another.</p>\n</li>\n<li>\n<p>I'm enthusiastic about the prospects of doing larger-scale paired comparison studies on EA topics. The first step would be to finish the statistical framework I started on <a href=\"https://forum.effectivealtruism.org/posts/mydChSMSM4JyEtx5C/estimating-value-from-pairwise-comparisons\">here</a>, then do a small-scale study suitable for a methodological journal in e.g., psychology or economics. Then we could run a study on a larger scale.</p>\n</li>\n<li>\n<p>Most examples I've seen in health economics, environmental economics, and marketing are only tangentially related to effective altruism. (I don't claim they don't exist -- there's probably many studies in health economics relevant to EA). But the topics of cognitive burden and experimental design is relevant for anyone who's involved with value estimation. It would be good to have at least a medium effort report on these topics -- I would certainly appreciate it! The literature probably contains a good deal of valuable insights for those sufficiently able and motivated to trudge through it.</p>\n</li>\n<li>\n<p>There is a reasonable number of statistical papers on the graded comparisons. But mostly from the <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"50\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">50</span></span></span></span></span></span>s - <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"70\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">70</span></span></span></span></span></span>s. These will be very difficult to read unless you're at the level of a capable master student of statistics. But summarizing and extending their research could potentially be an effective thesis!</p>\n</li>\n</ol>\n", "user": {"username": "Jonas Moss"}}, {"_id": "zDS2zWyg6cniisG2P", "title": "We're All Just Doing Our Best", "postedAt": "2022-10-07T16:09:52.718Z", "htmlBody": "<p>Instead of posting the negative rant that I drafted yesterday, I have elected to share a few lines from one of my favorite songs<sup class=\"footnote-ref\"><a href=\"#fn-Hse76heRyNFdLqbmw-1\" id=\"fnref-Hse76heRyNFdLqbmw-1\">[1]</a></sup> of recent months:</p>\n<blockquote>\n<p>May all the beggars be blessed<br>\nLike angels with anhedonia<br>\nWe're all just doing our best</p>\n</blockquote>\n<p>Beggars and billionaires alike can drown themselves in negativity, so if you're drowning like me, hang onto that last line:</p>\n<blockquote>\n<p><strong>We're all just doing our best</strong></p>\n</blockquote>\n<p>Have a \u2728fantastic\u2728 day, and remember that it's not your fault if you don't.</p>\n<hr>\n<p>If, like me, you're thinking, \"YOU WEIRDO DELETE THIS POST IT'S STUPID,\" I have one thing to say to you:</p>\n<p><img src=\"https://4.bp.blogspot.com/-0Wr4ZiVUcDQ/Tqj2Z3ZDCDI/AAAAAAAAGZ0/eIvdIqkSMl0/s1600/sad5alt4.png\" alt=\"nothing can do anything to me! by Allie Brosh\"></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-Hse76heRyNFdLqbmw-1\" class=\"footnote-item\"><p>I'm not linking to the song because the rest of the song is kinda dark, and my whole goal here is to make your day better. The last thing I want is to provide ammunition to the negative voices in your heads. If you happen to like negative/dark music, you can search for \"Dying Is Absolutely Safe\" by Architects. <a href=\"#fnref-Hse76heRyNFdLqbmw-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "semicycle"}}, {"_id": "c5wgvkt2CfCCciggR", "title": "Things for grant applicants to remember about living expenses", "postedAt": "2022-10-07T15:37:10.682Z", "htmlBody": "<p>Some grant applicants don\u2019t have much experience estimating how much money they\u2019ll need to live on, so it can be difficult to work out how much grant money to ask for.</p><p>Different people need different advice here. Some people are prone to asking for more money than makes sense, while others will be too frugal. Do your best to calibrate.&nbsp;</p><p>Some things to remember to factor in:</p><ul><li>You will likely be responsible for paying taxes on your grant / stipend, and this will probably be significantly higher than taxes you would pay as an employee (because a self-employed worker makes both the employer and the employee contributions.) This may be due quarterly. Check the website of the tax agency in your country - \"self employment tax\" may be a useful search term.</li><li>If you live in the US or another country without national healthcare, will you need to get your own health insurance? If you are on your parents\u2019 insurance in the US, remember that ends when you turn 26.</li><li>Might you need additional private care not covered by your insurance or national healthcare system?</li><li>Will you need to make payments on student loans or other loans?</li><li>Will you have enough to cover not just basic living costs, but some amount of unplanned expenses like medical bills or car repair?</li><li>Will you need work equipment like a laptop or a standing desk?</li><li>Will your project involve travel?</li><li>Consider getting short-term disability insurance. (You might search for \u201cself employed short term disability.\u201d) This would provide income for around 1-6 months if you can\u2019t work because of illness, surgery, mental illness, or giving birth. Importantly, most policies only cover pregnancy if you start the policy before you are pregnant. Many employers also don\u2019t provide paid leave if you started the job recently. So for example if you plan to have a baby in about a year and don\u2019t expect to get paid parental leave from an employer, you might want to get short-term disability insurance early, because most policies won\u2019t cover pregnancy-related leave for the first 9 or 12 months.</li></ul><p>If you know other grantees in your country, you might chat with them about any taxes / insurances they\u2019re aware of relevant to your country.</p><p>Other resources:&nbsp;</p><ul><li>advice in the comments on <a href=\"https://forum.effectivealtruism.org/posts/KdhEAu6pFnfPwA5hf/grantees-how-do-you-structure-your-finances-and-career\">this post</a>.</li><li>edited to add: more info on grants from <a href=\"https://resourceportal.antientropy.org/docs/grants-scholarships-and-fellowship-funding\">Anti Entropy</a>.<br>&nbsp;</li></ul>", "user": {"username": "Julia_Wise"}}, {"_id": "qLYFBpT2amHxJtf2o", "title": "Questions about and Objections to\u00a0\n'Sharing the World with Digital Minds' (2020)", "postedAt": "2022-10-07T13:36:37.834Z", "htmlBody": "<p><strong>Questions about and Objections to&nbsp;</strong><br><strong>'Sharing the World with Digital Minds' (2020)</strong></p><p><br>&nbsp;</p><p>First see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/imY2soAHYa6uCvSZs/summary-of-sharing-the-world-with-digital-minds-by-carl\"><u>my summary</u></a>, or the&nbsp;<a href=\"https://nickbostrom.com/papers/digital-minds.pdf\"><u>original paper</u></a>. Here are some things that stuck out to me:</p><p><br>1.&nbsp;<i>Apparent conflation between individual outweighing and collective outweighing</i>. Bostrom and Shulman (B&amp;S) define \u2018super-beneficiaries\u2019 as&nbsp;<i>individuals</i> who are better at converting resources into (their own) well-being than any human. But they then go on to say that one reason to think that digital minds will be super-beneficiaries, is that they are likely to greatly outnumber human beings eventually. Part 2 of the paper is a long list of reasons to think that we might be able to construct digital super-beneficiaries. The first given is that digital minds can be created more quickly and easily than humans can reproduce, they will likely come to far outnumber humans once pioneered:<br>&nbsp;'One of the most basic features of computer software is the ease and speed of exact&nbsp; reproduction, provided computer hardware is available. Hardware can be rapidly&nbsp; constructed so long as its economic output can pay for manufacturing costs (which have historically fallen, on price-performance bases, by enormous amounts; Nordhaus, 2007). This opens up the door for population dynamics that would take multiple centuries to play out among humans to be compressed into a fraction of a human lifetime. Even if initially only a few digital minds of a certain intellectual capacity can be affordably built, the number of such minds could soon grow exponentially or super-exponentially, until limited by other constraints. Such explosive reproductive potential could allow digital minds to vastly outnumber humans in a relatively short time\u2014correspondingly increasing the collective strength of their claims.'&nbsp;(p.3)</p><p>However, evidence that digital minds will outnumber humans is not automatically evidence that&nbsp;<i>individual&nbsp;</i>digital minds will produce well-being more efficiently than humans, and so count as super-beneficiaries by their definition. More importantly, the fact that there will be large numbers of digital minds, whilst evidence that most future utility will be produced by digital minds, is not evidence that digital minds, as a collective, will convert resources into utility more efficiently than humans as a collective will. B&amp;S seem to have conflated these two claims with the following: if digital minds are far more numerous than humans, the amount of well-being digital minds collectively generate will be higher than the amount that humans collectively generate.&nbsp;</p><p>I\u2019m open to this being essentially just a typesetting error (putting the subsection in the wrong section!).</p><p><br><br>&nbsp;2.<i> They assume that equality of resource allocation trumps equality of welfare</i>. B&amp;S worry that, if digital minds are a) far more numerous than humans and b) take far more resources to support, then egalitarian principles will force us to set basic income at a level lower than human subsistence. As far as I can tell the argument is:&nbsp;</p><p>giving humans a higher basic income than digital minds would be unjust discrimination,&nbsp;</p><ol><li>but if there are many more digital minds than humans, and&nbsp;</li><li>if digital minds can survive on far less resources than humans can,&nbsp;</li><li>then we might not be able to afford to give both humans&nbsp;<i>and&nbsp;</i>digital minds a basic income high enough for humans to survive on.&nbsp;</li></ol><p><br>&nbsp;Setting aside questions about whether a high enough basic income for humans to survive on really would be unaffordable in a world with many digital minds, it\u2019s not clear that any true non-discrimination principle requires the state to give everyone the same basic income.&nbsp;</p><p>For instance, in many countries that currently have welfare states, disabled people are entitled to disability benefits to which non-disabled people do not have access. Few people think that this is an example of unfair discrimination against the non-disabled, which suggests that it is permissible (and perhaps required) for governments to give higher levels of welfare support to citizens with higher levels of need.&nbsp;</p><p>So if humans need more resources to survive than do digital minds, then it is probably permissible for governments to give humans a higher basic income than they give to digital minds (at least by current common sense).&nbsp;</p><p><br>3.&nbsp;<i>Unclear argument for a minor diminishment of human utility.</i> B&amp;S tentatively propose:</p><p>C = \u201cin a world with very large numbers of digital minds, humans receiving .01% of resources might be 90% as good for humans as humans receiving 100% of resources.\u201d</p><p>How B&amp;S arrive at this isn\u2019t very clear. According to them, C follows from the fact that an economy where most workers are digital minds would be vastly more productive than one where most workers are humans.&nbsp;</p><p>But they don\u2019t explain why it follows from the fact that an economy produces a very large amount of resources per human, that humans capturing .01% of resources might be 90% as good for humans as humans capturing 100%. Presumably the idea is that once every human has reached some high absolute standard of living, giving them further resources doesn\u2019t help them much, because resources have diminishing marginal utility. However, it\u2019s hard to be sure this is what they mean: the argument is not spelled out.&nbsp;</p><p>(Also, in \u2018<a href=\"https://nickbostrom.com/astronomical/waste\"><u>Astronomical Waste</u></a>\u2019, Bostrom expresses scepticism that large increases in resources above a certain level will only make a small difference to the well-being of a human in a world with digital minds. He reasons that in such a world we might have invented new and resource-intensive ways for humans to access very high well-being. However, Bostrom only says this&nbsp;<i>may&nbsp;</i>be true. And in&nbsp;<i>Digital Minds,</i> B&amp;S are also tentative about the claim that humans could receive 90% of the benefits of 100% of resources by capturing 0.01% of the resources produced by a digital mind economy. So there isn\u2019t a crisp inconsistency between \u2018Digital Minds\u2019 and \u2018Astronomical Waste\u2019.)</p><p>&nbsp;</p><p>4.&nbsp;<i>Unclear evidence for relative cost of digital and biological subsistence</i>. B&amp;S claim that it is \u201cplausible\u201d that it will be cheaper to maintain digital minds at a subsistence level than to keep humans alive, but they don\u2019t actually give much argument for this, or cite any source in support of it. I think this is a common assumption in EA and rationalist speculation about the future, and that it might be a good idea to check what the supporting evidence for it actually is. (There\u2019s an implicit reference to Moore\u2019s law \u2013 \u201cThe cost of computer hardware to support digital minds will likely decline\u201d.)</p><p>&nbsp;</p><p>5.&nbsp;<i>Diminishing marginal utility as a condition of some values?</i> B&amp;S claim that digital minds could become super-beneficiaries by being designed so that they don\u2019t become habituated to pleasures, in the way that humans eventually become bored or sated with food, sex etc.&nbsp;</p><p>One worry: it\u2019s unclear that a digital mind could be built like this and remain able to function: not getting sated might mean that they get stuck on undergoing the same pleasurable experience over and over again. On hedonistic theories of well-being, this might still make them super-beneficiaries, since a digital mind that spent all its time repeating the same high-intensity pleasure over and over might well experience a large net amount of pleasure-minus-pain over its lifetime. But on the subset of objective list theories of value on which the best lives involve a balance of different goods, not getting sated might actually get in the way of even matching, let alone surpassing, humans in the efficiency with which you turn resources into well-being. (If you never get bored with one good, why move on to others and achieve balance?).&nbsp;</p><p><br><br>&nbsp;6.&nbsp;<i>Difficulties with the claim that different minds can have different hedonic capacities</i>. B&amp;S claim that digital minds might be capable of experiencing pleasures more intense than any human could ever undergo. However, I am somewhat sceptical of the view that maximum possible pleasure intensity can vary between different conscious individuals. It is notoriously difficult to explain what makes a particular pleasure (or pain) more or less intense when the two pleasures occur in different individuals. (The notorious problem of \u201cinterpersonal utility comparisons.\u201d) I think that one of the best candidate solutions to this problem entails that all minds which can undergo conscious pleasures/pains have maximum pleasure/pain experiences with the same level of intensity. The argument for this is complex, so I\u2019ve put it in&nbsp;<a href=\"https://docs.google.com/document/d/1aTWQq0gnTBdI69PbecH3oxc3Czk0Gh2faVhorIz6Zt0/edit\"><u>a separate doc</u></a>.</p><p>&nbsp;</p><p>7.&nbsp;<i>Maximising could undermine digital minds\u2019 breadth</i>. B&amp;S\u2019s discussion of objective list theory claims that digital minds could achieve goods like participation in strong friendships, intellectual achievement, and moral virtue to very high degrees, but they don\u2019t discuss whether maximising for one of these goods would lead a digital mind towards a life containing very little of the others, or whether balance between these goods is part of a good life.<br><br>&nbsp;8.&nbsp;<i>Unclear discussion of superhuman preferences</i>. B&amp;S list having stronger preferences as one way that digital minds could become super-beneficiaries. But their actual discussion of this possibility doesn\u2019t really provide much argument that digital minds would or could have stronger-than-human preferences. It just says that it\u2019s difficult to compare the strength of preferences across different minds, and then goes on to say that \u2018emotional gloss\u2019 and \u2018complexity\u2019, might be related to stronger preferences.&nbsp;<br><br>&nbsp;</p><p>9.&nbsp;<i>Conflation of two types of person-affecting view</i>. B&amp;S object to the idea that creating digital super-beneficiaries is morally neutral because creating happy people is neutral, by complaining that \u2018strict person-affecting views\u2019, must be wrong, because they imply that we have no reason to take action to prevent negative effects of climate change on people who do not yet exist. However, I don\u2019t think this reasoning is very convincing.&nbsp;</p><p>A first objection: it\u2019s not immediately clear that actions to prevent future harms from climate change are actually ruled out by views on which actions are only good if they improve things for some person. If someone is going to exist whether or not we take action against climate change, then taking action against climate change might improve things for them. However, this isn\u2019t really a problem, since it does seem person-affecting views are plausibly refuted by the fact that actions which prevent climate harm, and also change completely the identity of everyone born in the future, are still good insofar as they prevent the harms.&nbsp;</p><p>A more serious objection: you might be able to deny that making happy people is good, even while rejecting person-affecting views on which an action can only be good if there is some person it makes better-off. \u2018Making happy people is neutral\u2019 is a distinct claim from \u2018an action is only good if there is at least one person it makes better off\u2019. So the burden of proof is on B&amp;S when they claim that if the latter is false, the former must be too. They need to either give an argument here, or at least cite a paper in the population ethics literature. (B&amp;S do say that appealing to person-affecting views is just&nbsp;<i>one</i> way of arguing that creating super-beneficiaries is morally neutral, so they might actually agree with what I say in this paragraph.)&nbsp;<br><br>&nbsp;</p><p>10.<i> Possible overconfidence about the demandingness of deontic theories.</i> B&amp;S state outright that deontic moral theories imply that we don\u2019t have personal duties to transfer our own personal resources to whoever would benefit most from them. Whilst I\u2019m sure that most (maybe even all) deontologist&nbsp;<i>philosophers</i> think this, I\u2019d be a little nervous about inferring from that to the claim that&nbsp;<i>the deontological moral theories endorsed by those philosophers</i>, imply that we have no such obligation (or even that they fail to imply that we do have such an obligation.)&nbsp;</p><p>My reason for this as follows: contractualist theories are generally seen as \u201cdeontological\u201d, and I know of at least one paper in a top ethics journal arguing that contractualist theories in fact generate just as demanding duties to give to help others as do utilitarian theories. I haven\u2019t read this paper, so I don\u2019t have an opinion on how strong its argument is, or whether, even if its conclusion is correct, it generates the result that we are obliged to transfer all (or a large amount) of our resources to super-beneficiaries. (My guess is not: it probably makes a difference that super-beneficiaries are unlikely to be threatened with death or significant suffering without the transfer.) But I think at the very least more argument is needed here.</p><p>&nbsp;</p><p>11. The \u2018principle of substrate nondiscrimination\u2019 is badly named, because it doesn\u2019t actually rule out discrimination on the basis of substrate (i.e. the physical material a mind is made of\u2019). Rather, it rules out discrimination on the basis of substrate between minds that are conscious. This means it is actually compatible with saying that digital minds don\u2019t have interests at all, if for instance you believed that no thing without biological neurons is conscious. (Some philosophers defend accounts of consciousness which seem to imply this: see the section on \u201cbiological theories of consciousness\u201d on&nbsp;<a href=\"https://www.nedblock.us/papers/Theories_of_Consciousness.pdf#page=2\"><u>p.1112</u></a> of Ned Block\u2019s \u2018Comparing the Major Theories of Consciousness\u2019).&nbsp;</p><p>A principle compatible with denying, on the basis of their substrate, that digital minds have any rights probably shouldn\u2019t be called the \u201cprinciple of substrate non-discrimination\u201d. This is especially true when these reasons for denying that digital minds have interests are actually endorsed by some experts.</p><p><br><br>&nbsp;</p><p><i>This post is part of my work for&nbsp;</i><a href=\"https://arbresearch.com/\"><i><u>Arb Research</u></i></a><i>.</i></p><p><br>&nbsp;</p>", "user": {"username": "Dr. David Mathers"}}, {"_id": "imY2soAHYa6uCvSZs", "title": "Summary of 'Sharing the World With Digital Minds' by Carl Shulman and Nick Bostrom", "postedAt": "2022-10-07T13:36:18.572Z", "htmlBody": "<p><strong>Summary of 'Sharing the World With Digital Minds' by Carl Shulman and Nick Bostrom</strong><br><br><br>&nbsp;</p><p><i>Full paper can be found here:&nbsp;</i><a href=\"https://nickbostrom.com/papers/digital-minds.pdf\"><i><u>digital-minds.pdf (nickbostrom.com)</u></i></a>.&nbsp;<i>The following is written from Shulman &amp; Bostrom\u2019s point of view; see this companion post for my own take: &nbsp;https://forum.effectivealtruism.org/posts/qLYFBpT2amHxJtf2o/questions-about-and-objections-to-sharing-the-world-with</i></p><h2>TL;DR</h2><p>In the future we will be able to create \u201cdigital minds\u201d, conscious agents whose lives can contain different amounts of well-being. These would likely:</p><ul><li>be \u201csuper-beneficiaries\u201d, much better at converting resources into well-being than human beings;</li><li>become much more numerous than human beings, since AIs can be created far faster than humans;</li><li>facilitate massive economic growth, since they would be superhumanly-efficient workers;</li><li>have their preferences shaped by their human designers.</li></ul><p><br>&nbsp;Should we create such digital minds? On views on which adding happy people to the world is good, the answer is&nbsp; probably \u2018yes\u2019, given that this is a method of creating super-beneficiaries. On views which deny this, it will depend on whether creating digital minds benefits existing people, and on how good/bad the lives of those digital minds will be.</p><p>In practice, we have some prima facie moral reason to create digital super-beneficiaries, since some reasonable moral views take this to be inherently good, others to be neutral in itself, and we aren\u2019t sure which view is correct.</p><p>If we create digital minds, we face difficult questions about how to share resources with them:</p><ul><li>On total utilitarian views, humans are morally obligated to give all our resources to digital super-beneficiaries, even if this means human extinction.&nbsp;</li><li>If digital minds vastly outnumber humans then we must either give up or modify democracy, or allow humans to be a tiny electoral minority.</li><li>A welfare state which gives an equal basic income to each citizen, human or digital, will have to be set below human subsistence level if digital minds are sufficiently numerous.&nbsp;</li></ul><p>Bostrom and Shulman recommend:&nbsp;</p><ul><li>Exploring the permissibility and feasibility of a bargain whereby digital minds get 99.99% of society\u2019s resources, and humans 0.01%; this would give digital minds almost everything they could want, but might also leave humans with a very high standard of living, due to the vast size of an economy that included digital workers.</li><li>An anti-speciesist principle, to avoid privileging the interests of humans over digital minds just because the former are biological humans.</li><li>Avoiding creating digital minds that suffer more easily than humans.</li></ul><p>They suggest that the following are probably morally permissible and probably a good idea:</p><ul><li>Restricting the right of digital minds to produce further digital minds.</li><li>Engineering digital minds to have preferences which enable stable bargains with humans.&nbsp;</li></ul><p><br>&nbsp;</p><h2><strong>Full summary</strong></h2><p>&nbsp;</p><p>In the future, we may create many \u201cdigital minds\u201d (AIs) who are conscious agents, and hence whose lives can contain different degrees of well-being (for example, because they experience pleasure and pain).&nbsp;<i>If</i> we create such digital minds, it will be morally better if their \u201clives\u201d go well, rather than badly, and they will likely be owed some resources with which to further their interests. Because of this there are challenging questions about whether we should create digital minds, and (if we do create them), how much weight we should give to their interests relative to human beings.</p><p>It is likely that if we learn how to create conscious digital minds, we will eventually be able to create digital minds that are so-called&nbsp;<strong>\u201csuper-beneficiaries</strong>\u201d: better at converting resources into well-being than humans. There are multiple reasons to think that if we learn how to create conscious digital minds, we\u2019ll eventually be able to make them super-beneficiaries:</p><ol><li>Digital minds will likely need less resources than humans to remain in existence at all, given that it will become cheaper to provide them with computational resources than to feed a human. So it will be all-things-being-equal cheaper to keep a digital mind in existence at any particular level of well-being, than to keep a human alive and at that level of well-being.</li><li>Unlike humans, digital minds will mostly gain well-being by consuming virtual, rather than real, goods and services.. Virtual goods likely take less resources to provide.</li><li>Digital minds will automatically avoid sources of human suffering like growing old or sick, without any resources needing to be spent on this.</li><li>Since computer processors already work faster than neurons fire, digital minds will likely be able to produce more mental activity, and hence more well-being per unit of time, than humans.</li><li>Humans grow bored and sated with pleasurable activities, and gain pleasure from positional goods, like social status, that are necessarily scare because they depend on doing better than most other people. Neither of these things will necessarily be true of digital minds.</li><li>It would be surprising if the best pleasures in quality/intensity that humans experience, were the best&nbsp;<i>possible</i> pleasures that any mind could undergo, so perhaps we will design digital minds that can experience pleasures better than any human could ever experience.</li><li>On some theories, having your preferences satisfied itself counts as increasing your well-being, and we could create digital minds with preferences which are extremely easy to satisfy, giving those minds a source of ultra-cheap welfare.</li><li>For a variety of goods other than pleasure and preference-satisfaction human lives fall well short of what is possible in principle: we could all be much more morally virtuous, be better friends, achieve more, have greater knowledge and understanding of the world, etc. So perhaps we will be able to make digital minds which are better than we are at realizing these goods.</li><li>Larger minds generate more well-being (compare humans to insects) but also take more resources to maintain. So the size at which minds best convert resources into well-being, is the size with the best trade-off between minimizing the resources needed to sustain the mind, and maximizing the amount of good that the mind can produce. It would be a strange coincidence if the human brain was the optimal size for this trade-off. So it\u2019s very likely the optimal size is bigger or smaller than the human brain. But we could build digital minds that are closer to the optimal size.</li><li>Large parts of the human brain do not seem to be directly involved in producing positive experiences or other morally important goods. There\u2019s no obvious reason why digital minds would have to share this inefficiency.</li></ol><p>Given that we will probably one day have the capacity to create digital minds which are super-beneficiaries, two obvious moral questions arise:</p><ol><li>Should we create digital minds super-beneficiaries?</li><li>If we do create digital super-beneficiaries, how should we share resources between them and humans, given that the super-beneficiaries are better at generating well-being from resources than humans are.</li></ol><p>The answer to (11) varies depending on what moral principles about creating new people we accept. If we think that it is good to create people with lives that have net-positive well-being, then we have moral reasons to create super-beneficiaries. On the other hand, if we accept a view on which creating new happy people is morally neutral, then the fact that digital minds would be super-beneficiaries gives us no reason to create them. Instead, we should create digital minds only if doing so would benefit the people who already exist. (And probably only if those digital minds will not have net-negative lives.)</p><p>Views on which we should&nbsp;<i>only</i> care about the interests of currently existing people are implausible. And if we are unsure whether the correct moral theory says that bringing people with net-positive lives into existence is good, then we should probably assign&nbsp;<i>some&nbsp;</i>value into bringing people with net-positive lives into existence, since we think there\u2019s a non-zero chance that doing so is morally valuable. So we probably have at least some moral reason to create digital super-beneficiaries. On total utilitarian views on which we are morally required to create as much value as possible the moral reasons for creating large numbers of digital super-beneficiaries are very strong, since it would allow us to generate a very large amount of well-being.&nbsp;</p><p>Thinking through (12) raises difficult issues, because there are various reasons why we might be morally required to give so many resources or so much power to digital super-beneficiaries that the interests of humans are very significantly harmed.</p><p>Firstly, on total utilitarian views where maximizing value is morally required, we are probably required to give&nbsp;<i>all&nbsp;</i>society\u2019s resources to digital super-beneficiaries, even though this will lead to human extinction. Since the super-beneficiaries generate more utility per unit of resources than we do, giving them all our resources maximizes utility.</p><p>On many other moral views, we are not automatically required to transfer our own personal resources to the super-beneficiaries just because doing so would maximize utility. And on many of those other moral views it will be morally impermissible to steal resources owned by other human beings and give them to digital super-beneficiaries. In general on non-utilitarian views humans wouldn\u2019t owe digital super-beneficiaries our resources just because they could create more well-being from them than we can. However, even on such views there are still other reasons to expect that if we create digital super-beneficiaries we will be morally obligated to transfer a large amount of&nbsp; power and/or resources to them. And the consequences for humans if the obligations are fulfilled could still be very bad:</p><ul><li>Since digital minds are likely to be less expensive to keep alive than human beings, if we create some digital super-beneficiaries we will likely create very many. (At least if we do not successfully deliberately restrict the numbers of digital minds that we, and the digital minds themselves, are allowed to create.) If we hold the view that all members of society are entitled to a basic income, we might have to use almost all of society's resources paying a basic income to vast numbers of digital minds. At worst, the basic income level might have to be kept so low that it\u2019s no longer enough to keep a human alive.</li><li>Since digital minds would be likely to greatly outnumber humans, if our society is democratic, they would make up the vast majority of voters, leaving humans an almost powerless minority.</li><li>If we try and avoid these outcomes by restricting the ability of digital super-beneficiares to create further minds, we are possibly violating their rights. (Though Bostrom and Shulman ultimately argue that such restrictions might well be justified, on the grounds that if humans could reproduce as easily as digital minds, we would probably consider laws restricting human reproduction justified.)&nbsp;</li></ul><p>It\u2019s unclear how to avoid concluding we have these obligations, if we\u2019re committed to an anti-speciesist principle which says that humans don\u2019t matter more or have greater political rights than digital minds,<i> just because&nbsp;</i>we are human and they aren\u2019t.</p><p>However, there is a practical compromise that-whether or not it\u2019s a morally permissible outcome to aim for-might be acceptable to both humans and digital super-beneficiaries. A world with digital minds that massively outnumber humans, would produce far, far more resources than one without those minds. Because of this, if we split society\u2019s resources so that the vast, vast majority went to the digital minds, but some resources were still given to humans, this would:</p><ol><li>leave humans with a very high standard of living, but&nbsp;</li><li>leave digital minds almost as well-off as they would have been if they had captured all resources and left humans with none.</li></ol><p>In thinking about what ways of dividing power and resources between humans and digital minds are morally acceptable, we can rely on these two principles (directly quoted from Bostrom and Shulman):</p><p><strong>Principle of Substrate Non-Discrimination</strong></p><p>If two beings have the same functionality and the same conscious experience, and <i>differ only in the substrate of their implementation, then they have the same moral status.</i><br><strong>Principle of Ontogeny Non-Discrimination</strong></p><p><i>If two beings have the same functionality and the same conscious experience, and differ only in how they came into existence, then they have the same moral sta</i>tus.</p><p>We should also place a high value on avoiding actions which are highly efficient at producing large amounts of suffering or digital minds which suffer much more easily than humans.&nbsp;</p><p>One final interesting question is whether it\u2019s morally permissible to deliberately design digital minds so that they consent to share resources with us in the way that we favor. It\u2019s (plausibly) wrong to deliberately genetically engineer human children with preferences that are convenient for their parents, as this is arguably an immoral type of manipulation. So it might also be impermissibly manipulative to engineer digital minds with preferences that make them accept particular resource-bargains with humans. However, there are important moral disanalogies between the case of genetically engineering humans to have particular preferences, and the case of choosing preferences for digital minds, which might show that the latter is permissible, even if the former is morally wrong. Firstly, unlike in the case of human procreation, there probably won\u2019t be a way to create digital minds without making choices that predictably shape their preferences. So perhaps when knowingly shaping preferences is unavoidable, it is not automatically wrong to shape preferences in ways that benefit you, or humanity as a whole. Secondly, if we shape humans to have particular strong preferences, those may conflict with other preferences they have, and cause them suffering. If this is what explains why genetically engineering children to have particular preferences is wrong, then it\u2019s probably not inherently wrong to deliberately shape the preferences of digital minds. For in the case of digital minds, we can probably design the minds in ways that avoid them experiencing suffering due to them having competing strong preferences, whilst also ensuring they have the preferences about resource-sharing that we desire.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Dr. David Mathers"}}, {"_id": "wa8svGc4eMGrC5qNC", "title": "Calibrate - New Chrome Extension for hiding numbers so you can guess", "postedAt": "2022-10-07T11:21:51.937Z", "htmlBody": "", "user": {"username": "ChanaMessinger"}}, {"_id": "8wWYmHsnqPvQEnapu", "title": "Getting on a different train: can Effective Altruism avoid collapsing into absurdity?", "postedAt": "2022-10-07T10:52:01.815Z", "htmlBody": "<h1>The train to crazy town</h1><h2>Introduction</h2><p><a href=\"https://www.samstack.io/p/the-train-to-crazy-town\"><u>Sam Atis</u></a>, following on from<a href=\"https://astralcodexten.substack.com/p/book-review-what-we-owe-the-future\">&nbsp;<u>some arguments by Scott Alexander</u></a>, writes of \u2018the train to Crazy Town\u2019.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcgg0zndjp3f\"><sup><a href=\"#fncgg0zndjp3f\">[1]</a></sup></span>&nbsp;As Sam presents it, there are a series of escalating challenges to utilitarian-style (and more broadly consequentialist-style) reasoning, leading further and further into absurdity. Sam himself bites the bullet on some classic cases, like the<a href=\"https://www.jstor.org/stable/27902416\">&nbsp;<u>transplant problem</u></a> and the<a href=\"https://plato.stanford.edu/entries/repugnant-conclusion/\">&nbsp;<u>repugnant conclusion</u></a>, but is put off by some more difficult examples:</p><blockquote><p><i>Thomas Hurka\u2019s St Petersburg Paradox:&nbsp;</i>Suppose you are offered a deal\u2014you can press a button that has a 51% chance of creating a new world and doubling the total amount of utility, but a 49% chance of destroying the world and all utility in existence. If you want to maximise total expected utility, you ought to press the button\u2014pressing the button has positive expected value. But the problem comes when you are asked whether you want to press the button again and again and again\u2014at each point, the person trying to maximise expected utility ought to agree to press the button, but of course, eventually they&nbsp;<i>will</i> destroy everything.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsznj8h9zof\"><sup><a href=\"#fnsznj8h9zof\">[2]</a></sup></span></p></blockquote><p>&nbsp;</p><blockquote><p><i>The Very Repugnant Conclusion</i>: Once [utilitarians] assign some positive value, however small, to the creation of each person who has a weak preference for leading her life rather than no life, then how can they stop short of saying that some large number of such lives can compensate for the creation of lots of dreadful lives, lives in pain and torture that nobody would want to live?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7r7ntf7q856\"><sup><a href=\"#fn7r7ntf7q856\">[3]</a></sup></span></p></blockquote><p>Most people, like Sam, try to get off the train before they reach the end of the line, trying to preserve some but not all of utilitarianism. And so the question is: how far are you willing to go?</p><p>As a list of challenges to utilitarianism, I think Sam\u2019s post is lacking: he is very focussed on specific thought experiments, ignoring more theoretical objections that in my view are much more insightful.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwpn58mpwwk\"><sup><a href=\"#fnwpn58mpwwk\">[4]</a></sup></span>&nbsp;But as a provocation\u2014<i>how far</i> will you go in your utilitarianism?\u2014I think it\u2019s an extraordinarily useful post. Sam takes it upon himself to pose the following question: \u2018what are the principles by which we should decide when to get off the train?\u2019</p><p>But something worries me about Sam\u2019s presentation.&nbsp;<i>Who said you could actually get off the train to crazy town?</i> Each additional challenge to utilitarian logic\u2014each stop on the route\u2014does not seem to assume any new premises: every problem is generated by the same basic starting point of impartially weighing up all people\u2019s experiences and preferences against each other. As such, there just might not be any principles you can use to justify biting&nbsp;<i>this</i> bullet but not&nbsp;<i>that</i> bullet\u2014doing so might even be logically incoherent. Sam says that he does want to get off the train eventually, but it\u2019s not clear how he would do that.</p><p>Alexander\u2019s original post suggests something closer to this view. He explains why he dislikes the Repugnant Conclusion and various other situations in which \u2018longtermist\u2019-style consequentialism goes awry, and then lays out the position he would take to avoid these conclusions \u2018[i]f I had to play the philosophy game\u2019. But, he writes, \u2018I\u2019m not sure I&nbsp;<i>want</i> to play the philosophy game.\u2019 He\u2019s not confident that his own partial theoretical compromise actually&nbsp;<i>will</i> avoid the absurdity (and<a href=\"https://forum.effectivealtruism.org/posts/vjDLRBaEWMmrHhrtv/the-standard-person-affecting-view-doesn-t-solve-the\">&nbsp;<u>rightly so</u></a>), and he cares more about avoiding absurdity than he does about getting on the train.</p><p>Mainstream commentators from outside Effective Altruism have made this point too. Stephen Bush, a&nbsp;<i>Financial Times</i> journalist whose political analysis I respect a lot,<a href=\"https://www.ft.com/content/db4ab4c4-56b9-4706-bd0f-d2e3fca7d436\">&nbsp;<u>reviewed William MacAskill\u2019s book&nbsp;</u><i><u>What We Owe the Future</u></i><u> a few weeks ago</u></a>; his primary criticism was that the whole book seemed to be trying to \u2018sell\u2019 readers \u2018on a thought experiment that even its author doesn\u2019t wholly endorse\u2019. As an example, Bush notes that arguments in the book straightforwardly entail that \u2018my choice to remain childless so that my partner and I can fritter away our disposable income\u2019 is immoral. Officially, MacAskill swears off this implication; but this just looks like he&nbsp;<a href=\"https://twitter.com/stephenkb/status/1559221824382287872\"><u>\u2018explicitly sets out the case for children and then goes \u201cnow, of course I\u2019m not saying&nbsp;</u><i><u>that</u></i><u>\u201d\u2019</u></a>. MacAskill tries to avoid these inferences, but it is entirely reasonable for someone like Bush to look at where MacAskill\u2019s logic is heading, decide they don\u2019t like it, and reject the whole approach.</p><p>In other words, it\u2019s not clear how anyone actually gets off the train to crazy town. Once you allow even a little bit of utilitarianism in, the unpalatable consequences follow immediately. The train might be an express service: once the doors close behind you, you can\u2019t get off until the end of the line.</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/8wWYmHsnqPvQEnapu/yceitjmggsnzfpucioka\"></p><p><i>I want to get off Mr Bones\u2019 Wild Ride</i></p><p>This seems like a pretty significant problem for Effective Altruists. Effective Altruists seemingly want to use a certain kind of utilitarian (or more generally consequentialist) logic for decision-making in a lot of situations; but at the same time, the Effective Altruism movement aims to be broader than pure consequentialism, encompassing a wider range of people and allowing for uncertainty about ethics. As Richard Y. Chappell has put it, they want<a href=\"https://rychappell.substack.com/p/beneficentrism?s=r\">&nbsp;<u>\u2018utilitarianism minus the controversial bits\u2019</u></a>. Yet it\u2019s not immediately clear how the models and decision-procedures used by Effective Altruists can consistently avoid any of the problems for utilitarianism: as examples above illustrate, it\u2019s entirely possible that even the simplest utilitarian premises can lead to seriously difficult conclusions.</p><p>Maybe a few committed \u2018EAs\u2019 will bite the relevant bullets. But not everyone will, and this could potentially create bad epistemic foundations for Effective Altruism (if people end up being told to accept premises without worrying about their conclusions) as well as poor social dynamics (as with Alexander believing his position is \u2018anti-intellectual\u2019). And beyond the community itself, the general public isn\u2019t stupid: if they can see that this is where Effective Altruist logic leads, they might simply avoid it. This could significantly hamper the effectiveness of Effective Altruist advocacy. In this post, I want to ask if this impression\u2014that Effective Altruism can\u2019t get consistently off the train to crazy town\u2014is correct, and what it might mean if it is.</p><h2>An impossibility result</h2><p>Above, I introduced my main idea in an intuitive way, showing how a bunch of different writers have come to similar conclusions. I now want to try to be a bit more formal about it. This section draws heavily on a 1996 article by Tyler Cowen, which is called<a href=\"https://www.jstor.org/stable/2382033\">&nbsp;<u>\u2018What Do We Learn from the Repugnant Conclusion?\u2019</u></a> but which is much broader in scope than the title would suggest. Cowen is a well-known thinker within Effective Altruism, and Effective Altruists are often interested in population ethics and the repugnant conclusion, but this article and Cowen\u2019s other writings on population ethics (with the exception of<a href=\"https://d101vc9winf8ln.cloudfront.net/documents/27957/original/Cowen___Parfit_-_Against_the_social_discount_rate.pdf?1523454279\">&nbsp;<u>the piece he co-authored with Derek Parfit on discounting</u></a>) seem relatively unknown in these spaces.</p><p>Cowen\u2019s argument is similar on the surface to<a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.228.378&amp;rep=rep1&amp;type=pdf\">&nbsp;<u>\u2018impossibility theorems\u2019 in the population ethics literature</u></a>, which prove that we cannot coherently combine all the things we intuitively want from a theory of population ethics. But on a deeper level, it\u2019s quite different: it\u2019s about problems for moral theories in general, not just population ethics. In particular, Cowen is discussing moral theories with \u2018universal domain\u2019, which just means&nbsp;<i>systematic</i> theories that are able to compare any two possible options. This is as opposed to&nbsp;<a href=\"https://plato.stanford.edu/entries/moral-particularism/\"><u>moral particularism</u></a>, which opposes the use of general principles in moral thought and favours individual case-by-case judgments, and especially to&nbsp;<a href=\"https://plato.stanford.edu/entries/value-pluralism/\"><u>value pluralism</u></a>, which is committed to there being multiple&nbsp;<i>incommensurable&nbsp;</i>values and thus insists that sometimes different situations are morally incomparable.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6uezsn8tf2\"><sup><a href=\"#fn6uezsn8tf2\">[5]</a></sup></span>&nbsp;Theories with universal domain include almost all consequentialist and deontological theories, as well as some forms of virtue ethics: these are all committed to comparing and weighing up different values (whether by reducing them all to a single overarching value, or by treating them as separate but commensurable ends-in-themselves), and can systematically evaluate all possible options.</p><p>Cowen restricts his attention to theories where one of the values that matters for ranking outcomes is utility (whether in its preference-satisfaction, hedonist, or welfarist guises).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefplsvlu3guwf\"><sup><a href=\"#fnplsvlu3guwf\">[6]</a></sup></span>&nbsp;It\u2019s not clear that this is strictly necessary\u2014at least some theories that ignore utility, though not necessarily all of them,<a href=\"https://www.jstor.org/stable/10.1086/342853\">&nbsp;<u>face similar problems anyway</u></a>\u2014but Cowen focusses on utility for simplicity and clarity. Importantly, this doesn\u2019t overly limit our focus: Cowen\u2019s condition includes all moral views that might support Effective Altruism, including<a href=\"https://80000hours.org/podcast/episodes/andreas-mogensen-deontology-and-effective-altruism/\">&nbsp;<u>non-consequentialist theories that include an account of impartial, aggregative do-gooding</u></a> as well as pure consequentialism.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftxmtv8j65h\"><sup><a href=\"#fntxmtv8j65h\">[7]</a></sup></span></p><p>So, the problem is this. Effective Altruism wants to be able to say that things other than utility matter\u2014not just in the sense that they have some moral weight, but in the sense that they can actually be relevant to deciding what to do, not just swamped by utility calculations. Cowen makes the condition more precise, identifying it as the denial of the following claim: given two options,&nbsp;<i>no matter</i> how other morally-relevant factors are distributed between the options, you can&nbsp;<i>always</i> find a distribution of utility such that the option with the larger amount of utility is better. The hope that you can have \u2018utilitarianism minus the controversial bits\u2019 relies on denying precisely this claim.</p><p>This condition doesn\u2019t aim to make utility&nbsp;<i>irrelevant</i>, such that utilitarian considerations should never change your mind or shift your perspective: it just requires that they can be&nbsp;<i>restrained</i>, with utility&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\"><u>co-existing with other valuable ends.</u></a> It guarantees that utility won\u2019t automatically swamp other factors, like partiality towards family and friends, or personal values, or self-interest, or respect for rights, or even suffering (as in the Very Repugnant Conclusion). This would allow us to respect our intuitions when they conflict with utility, which is just what it&nbsp;<i>means</i> to be able to get off the train to crazy town.</p><p>Now, at the same time, Effective Altruists&nbsp;<i>also</i> want to emphasise the relevance of scale to moral decision-making. The central insight of early Effective Altruists was to resist<a href=\"https://www.lesswrong.com/posts/2ftJ38y9SRBCBsCzy/scope-insensitivity\">&nbsp;<u>scope insensitivity</u></a> and to begin systematically examining the numbers involved in various issues. \u2018Longtermist\u2019 Effective Altruists are deeply motivated by the idea that<a href=\"https://ourworldindata.org/longtermism\">&nbsp;<u>\u2018the future is vast\u2019</u></a>: the huge numbers of future people that could potentially exist gives us a lot of reason to try to make the future better. The fact that some interventions produce&nbsp;<i>so much more</i> utility\u2014do&nbsp;<i>so much more</i> good\u2014than others is one of the main grounds for prioritising them. So while it would technically be a<i>&nbsp;</i>solution to our problem to declare (e.g.) that considerations of utility become effectively irrelevant once the numbers get too big, that would be unacceptable to Effective Altruists. Scale&nbsp;<i>matters</i> in Effective Altruism (rightly so, I would say!), and it doesn\u2019t just stop mattering after some point.</p><p>So, what other options are there? Well, this is where Cowen\u2019s paper comes in: it turns out, there are none. For&nbsp;<i>any</i> moral theory with universal domain where utility matters at all, either the marginal value of utility diminishes rapidly (asymptotically) towards zero, or considerations of utility come to swamp all other values. The formal reasoning behind this impossibility result can be found in the appendix to Cowen\u2019s paper; it relies on certain order properties of the real numbers. But the same general argument can be made without any mathematical technicality.</p><p>Consider two options, A and B,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefp6aox1oxd0f\"><sup><a href=\"#fnp6aox1oxd0f\">[8]</a></sup></span>&nbsp;where A is&nbsp;<i>arbitrarily</i> preferable to B along&nbsp;<i>all</i> dimensions\u2014except, possibly, utility. Now imagine we can continually increase the amount of utility in option B, while keeping everything else fixed. At some point in this process, one of two things must occur:</p><ul><li>These increases in utility eventually become so large in aggregate that their value swamps the value of everything else, and B becomes preferable to A on utility grounds alone.</li><li>Each additional unit of utility begins to \u2018matter\u2019 less and less, with the marginal value of utility diminishing rapidly to zero, such that A remains preferable to B no matter how much utility is added.</li></ul><p>This is where the paradoxes that Sam discusses come in: they are concrete examples of exactly this sort of case. Take the Very Repugnant Conclusion as an example. You start with world A, containing a small population who live in bliss with all the good things in life, and world B, containing solely a huge quantity of suffering with nothing else of value. You then begin adding additional utility into world B, in the form of additional lives with imperceptibly positive value\u2014one brief minute of<a href=\"https://www.stafforini.com/docs/parfit_-_overpopulation_and_the_quality_of_life.pdf\">&nbsp;<u>\u2018muzak and potatoes\u2019</u></a> each. And then the inevitable problem: either the value of all these additional lives, added together, eventually swamps the negative value of suffering; or, eventually, the marginal value of an additional life becomes infinitesimally small, such that there is no number of lives you could add to make B better than A.</p><p>I hope the reasoning is clear enough from this sketch. If you are committed to the&nbsp;<i>scope</i> of utility mattering, such that you cannot just declare additional utility&nbsp;<i>de facto</i> irrelevant past a certain point, then there is no way for you to formulate a moral theory that can avoid being swamped by utility comparisons. Once the utility stakes get large enough\u2014and, when considering the scale of human or animal suffering or the size of the future, the utility stakes really are quite large\u2014all other factors become essentially irrelevant, supplying no relevant information for our evaluation of actions or outcomes.</p><h2>Further discussion and some more cases</h2><p>This structure does not just apply to population ethics problems, like the repugnant conclusion and the Very Repugnant Conclusion. As Cowen showed in a<a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.357.2937&amp;rep=rep1&amp;type=pdf\">&nbsp;<u>later paper</u></a>, the same applies to both Pascal\u2019s mugging and Hurka\u2019s version of the St Petersburg paradox, both of which seem quite different due to their emphasis on probability and risk but which have the same fundamental structure: start with an obvious choice between two options (should I or shouldn\u2019t I give my wallet to this random person who is promising me literally nothing?), then keep adding tiny amounts of utility to the bad option (until the person is promising you huge amounts of utility). Many of the problems of<a href=\"https://handsandcities.com/2022/01/30/on-infinite-ethics/\">&nbsp;<u>infinite ethics</u></a> have this structure as well. While this structure doesn\u2019t fit all of the standard challenges to utilitarianism (e.g., the experience machine), it fits many of them, and\u2014relevantly\u2014it fits many of the landmarks on the way to crazy town.</p><p>Indeed, in section five Cowen comes close to suggesting a quasi-algorithmic procedure for generating challenges to utilitarianism.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwxq59my8gsj\"><sup><a href=\"#fnwxq59my8gsj\">[9]</a></sup></span>&nbsp;You just need a sum over a large number of individually-imperceptible epsilons somewhere in your example, and everything else falls into place. The epsilons can represent tiny amounts of pleasure, or pain, or probability, or something else; the large number can be extended in time, or space, or state-space, or across possible worlds; it can be a one-shot or repeated game. It doesn\u2019t matter. You just need&nbsp;<i>some</i> \u03a3 \u03b5 and you can generate a new absurdity: you start with an obvious choice between two options, then keep adding additional epsilons to the worse option until either utility vanishes in importance or utility dominates everything else.</p><p>In other words, Cowen can just keep generating more and more absurd examples, and there is no principled way for you to say \u2018this far but no further\u2019. As Cowen puts it:</p><blockquote><p>Once values are treated as commensurable, one value may swamp all others in importance and trump their effects\u2026 The possibility of value dictatorship, when we must weigh conflicting ends, stands as a&nbsp;<strong>fundamental difficulty</strong>.</p></blockquote><p>A popular response in the Effective Altruist community to problems that seem to involve something like dogmatism or \u2018value dictatorship\u2019\u2014indeed, the response William MacAskill gave when Cowen himself made some of these points in<a href=\"https://conversationswithtyler.com/episodes/william-macaskill/\">&nbsp;<u>an interview</u></a>\u2014is to invoke<a href=\"https://static1.squarespace.com/static/5506078de4b02d88372eee4e/t/5f5a3ddd466873260486fb06/1599749604332/Moral+Uncertainty.pdf\">&nbsp;<u>moral uncertainty</u></a>. If your moral view faces challenges like these, you should downweigh your confidence in it; and then, if you place some weight on multiple moral views, you should somehow aggregate their recommendations, to reach an acceptable compromise between ethical outlooks.</p><p>Various theories of moral uncertainty exist, outlining how this aggregation works; but none of them actually escape the issue. The theories of moral uncertainty that Effective Altruists rely on are&nbsp;<i>themselves</i> frameworks for commensurating values and systematically ranking options, and (as such) they are also vulnerable to \u2018value dictatorship\u2019, where after some point the choices recommended by utilitarianism come to swamp the recommendations of other theories. In the literature, this phenomenon is well-known as \u2018fanaticism\u2019.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrhonaj5hldc\"><sup><a href=\"#fnrhonaj5hldc\">[10]</a></sup></span></p><p>Once you let utilitarian calculations into your moral theory at all, there is no principled way to prevent them from swallowing everything else. And, in turn, there\u2019s no way to have these calculations swallow everything without them leading to pretty absurd results. While some of you might bite the bullet on the repugnant conclusion or the experience machine, it is very likely that you will eventually find a bullet that you don\u2019t want to bite, and you will want to get off the train to crazy town; but you cannot consistently do this without giving up the idea that&nbsp;<i>scale matters</i>, and that it doesn\u2019t just stop mattering after some point.</p><h1>Getting on a different train</h1><h2>Pluralism and universal domain</h2><p>Is this post a criticism of Effective Altruism? I\u2019m not actually sure. For some personal context: I\u2019ve signed the&nbsp;<i>Giving What We Can</i> pledge, and I\u2019m proud of that; I find large parts of Effective Altruism appealing, although I\u2019m turned off by many other aspects; I think the movement has provably done a lot of good. It\u2019s too easy, and completely uncharitable, to simply write the movement off as inconsistent. But yet, when it comes to the&nbsp;<i>theory</i> of Effective Altruism, I\u2019m not sure how it gets off the ground at all without leading to absurdities. How do I square this?</p><p>In his<a href=\"https://conversationswithtyler.com/episodes/william-macaskill/\">&nbsp;<u>recent interview with MacAskill</u></a>, Cowen said the following (edited for clarity):</p><blockquote><p>At the big macro level (like the whole world of nature versus humans, ethics of the infinite, and so on) it seems to me utilitarianism doesn\u2019t perform that well. Isn\u2019t the utilitarian part of our calculations only a mid-scale theory? You can ask: does rent control work? Are tariffs good? Utilitarianism is fine there. But otherwise, it just doesn\u2019t make sense\u2026</p><p>[W]hy not get off the train a bit earlier? Just say: \u2018Well, the utilitarian part of our calculations\u200a is embedded within a particular social context, like, how do we arrange certain affairs of society. But if you try to shrink it down to too small (how should you live your life?) or to too large (how do we deal with infinite ethics on all of nature?) it just doesn\u2019t work. It has to stay embedded in this context.\u2019 Universal domain as an assumption doesn\u2019t really work anywhere, so why should it work for the utilitarian part of our ethics?</p><p>\u2026 It\u2019s not that there\u2019s some other theory that\u2019s going to tie up all the conundrums in a nice bundle, but simply that there are limits to moral reasoning, and we cannot fully transcend the notion of being partial because moral reasoning is&nbsp;<i>embedded</i> in that context of being partial about&nbsp;<i>some</i> things.</p></blockquote><p>There are two ways to read this suggestion. The first is that Cowen just wants us to accept that, past a certain point, the value of additional utility vanishes quickly to zero: when we zoom out too far, utility becomes&nbsp;<i>de facto</i> meaningless. (This reading is supported especially by the first paragraph I quoted.) But there\u2019s a different way to read his suggestion (which is supported more by the third paragraph I quoted), which is that rather than taking the logic of this own argument at face value, Cowen is urging MacAskill to take a step back and reject one of its presuppositions: universal domain.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqn9gr1svp2\"><sup><a href=\"#fnqn9gr1svp2\">[11]</a></sup></span></p><p>If we accept a certain amount of incommensurability between our values, and thus a certain amount of non-systematicity in our ethics, we can avoid the absurdities directly. Different values are just valuable in&nbsp;<i>different ways</i>, and they are not systematically comparable: while sometimes the choices between different values are obvious, often we just have to respond to trade-offs between values with context-specific judgment. On these views, as we add more and more utility to option B, eventually we reach a point where the different goods in A and B are incommensurable and the trade-off is systematically undecidable; as such, we can avoid the problem of utility swallowing all other considerations without arbitrarily declaring it unimportant past a certain point.</p><p>MacAskill responds to Cowen by arguing that \u2018we should be more ambitious than that with our moral reasoning\u2019. He seems to think that we&nbsp;<i>will</i> eventually find a theory that \u2018ties up all the conundrums\u2019\u2014perhaps if we hand it over to \u2018specially-trained AIs\u2019 for whom ethics is \u2018child\u2019s play\u2019 (as he writes in&nbsp;<i>What We Owe the Future</i>). But it\u2019s not clear that \u2018ambition\u2019 has anything to do with it. Even the smartest AI could not eliminate the logical contradictions we face in (say) population ethics; at most, it could give us a recommendation about which bullet to bite. Likewise, Alexander seems to think that (something like) this position is \u2018anti-intellectual\u2019. It is&nbsp;<i>unsystematic</i>, to be sure, but it\u2019s no more anti-intellectual than<a href=\"https://personal.lse.ac.uk/ROBERT49/teaching/ph103/pdf/Hume_1748_Enquiry12_OnAcademicalOrSkepticalPhilosophy.pdf\">&nbsp;<u>Hume</u></a> was: it\u2019s not an unprincipled rejection of all thinking, but an attempt to figure out where we run up against the limits of moral theorising.</p><p>Such a position would rule out utilitarianism as a general-purpose theory of morality, or even its more limited role as a theory of the (supposed) part of morality philosophers call \u2018beneficence\u2019. But it wouldn\u2019t stop us from using utilitarianism as a&nbsp;<i>model</i> for moral thinking, a framework representing certain ways we think about difficult questions. It might be especially relevant to thinking about trade-offs where we have to weigh up costs and benefits\u2014especially if,<a href=\"https://law.stanford.edu/wp-content/uploads/sites/default/files/publication/259516/doc/slspublic/ssrn-id1781102.pdf\">&nbsp;<u>as Barbara Fried has argued</u></a>, it is the only rigourous ethical framework that is able to face up to uncertainty and scarcity. But, like all models, it would only be valid within a certain context. Utilitarianism can remain a really important aspect of moral reasoning, just not in the way that we are familiar with from universal moral theories.</p><p>To state my own personal view, I think I am probably &gt;60% confident in something like this position being right. Some kind of consequentialist thinking seems pretty applicable in a lot of situations, and to often be very helpful. We can reject universal domain, and thus value commensurability, while retaining this insight. Cowen would not be the first to make this claim: Isaiah Berlin, the most famous 20th century defender of value incommensurability, was convinced that<a href=\"https://isaiah-berlin.wolfson.ox.ac.uk/sites/default/files/inline-files/bib196%20-%20Pursuit%20of%20the%20Ideal_0.pdf\">&nbsp;<u>\u2018Utilitarian solutions are sometimes wrong, but, I suspect, more often beneficent.\u2019</u></a> Utilitarianism is not a general-purpose theory of the good; but it is an important framework that can generate important insights.</p><p>And this seems to be all Effective Altruism needs. Holden Karnofsky recently<a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">&nbsp;<u>made a call for pluralism within Effective Altruism</u></a>: the community needs to temper its central \u2018ideas/themes/memes\u2019 with pluralism and moderation. But Karnofsky argues, further, that the community&nbsp;<i>already does this</i>: \u2018My sense is that many EAs\u2019 writings and statements are much more one-dimensional \u2026 than their actions.\u2019 In&nbsp;<i>practice</i>, Effective Altruists are not willing to purchase theoretical coherence at the price of absurdity; they place utilitarian reasoning in a pluralist context. They may do this unreflectively, and I think they do it imperfectly; but it is an existence proof of a version of Effective Altruism that accepts that utility considerations are embedded in a wider context, and tempers them with judgment.</p><h2>The core of Effective Altruism</h2><p>Effective Altruists can\u2019t be&nbsp;<i>entirely</i> happy with Cowen\u2019s position, of course. They think utilitarian reasoning should be applicable to examples beyond those drawn from economics textbooks\u2014at very least, they think it should be relevant to decisions around donations and career choices.</p><p>We can be more precise about what Effective Altruism asks of utilitarian reasoning. Effective Altruism places far more weight than even previous utilitarians had on the&nbsp;<i>optimising</i> or&nbsp;<i>maximising</i> aspects of utilitarianism. This goes back to my previous comments on scale, and opposition to scope-insensitivity: the&nbsp;<a href=\"https://plato.stanford.edu/entries/lakatos/#ImprPoppScie\"><u>\u2018hard core\u2019</u></a> of Effective Altruism is the idea that, at least for most of us, the ethically relevant differences between the options we face are&nbsp;<i>huge</i>, despite the fact that we often tend to act as if they were negligible or unknowable.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn0rpdszgk19\"><sup><a href=\"#fnn0rpdszgk19\">[12]</a></sup></span>&nbsp;Given this premise, the appeal of utilitarianism is immediate. Utilitarianism is an optimising framework: its focus is on achieving the&nbsp;<i>best</i> possibilities, rather than simply a selection of acceptable options. This is an immensely (even overwhelmingly) useful feature of the utilitarian framework for Effective Altruists; it gives them reason to use it in their moral reasoning even at large scales. MacAskill should not have challenged Cowen\u2019s ambition; rather, he should have challenged his na\u00efve position that the limits of utilitarianism should be defined with reference to scale.</p><p>But, even as Effective Altruists are excited about the optimisation implicit in utilitarianism, they have to be wary about its flip-side: the potential for fanaticism and value dictatorship. Utilitarian reasoning needs to be bounded or restrained in some way. But Cowen\u2019s argument shows that there can be no principled, systematic account of these bounds and restraints. If we hope to represent questions of&nbsp;<i>scope</i> using utilitarian reasoning, without having utility swallow all other values, there will have to be ambiguities, incommensurabilities, and arbitrariness; as I worried when reading Sam\u2019s post, there are no principles we can use to decide when to get off the train to crazy town.</p><p>I do not think this is a&nbsp;<i>huge</i> problem. To&nbsp;<a href=\"https://www.cambridge.org/core/books/making-sense-of-humanity/which-slopes-are-slippery/3054E587DB53B5E8EDEDA0A5E1B13360\"><u>borrow a point from Bernard Williams</u></a>, there is no particular principled place to draw a line, but it is nonetheless entirely principled to say&nbsp;<i>we need a line somewhere</i>. And Cowen suggests, rightly I think, that the line is drawn based on the (possibly arbitrary) social contexts within which our moral reasoning is embedded. But the problem is just that Effective Altruism doesn\u2019t have a good account of the&nbsp;<i>context</i> of moral reasoning, and thus no understanding of its own limits.</p><p>To be sure, a story is sometimes told (largely unreflectively) about&nbsp;<i>why</i> the \u2018hard core\u2019 of Effective Altruism is true even as most people act as if it isn\u2019t; this story could tell us something about the context for scale-sensitive reasoning. It is derived from Derek Parfit\u2019s work,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrkbyiu5pv9\"><sup><a href=\"#fnrkbyiu5pv9\">[13]</a></sup></span>&nbsp;and goes something like this. In the small, pre-modern societies where many of our moral ideas were developed, we could affect only small numbers of people in our communities; in such societies, an ethic that focussed our attention locally and largely ignored scale was reasonable. In the globalised and interconnected modern world, however, human action could (potentially / in expectation) affect many millions; we might even be at the&nbsp;<a href=\"https://www.bbc.com/future/article/20200923-the-hinge-of-history-long-termism-and-existential-risk\"><u>\u2018hinge of history\u2019</u></a>. In such a situation, the \u2018spread\u2019 of possible actions is much larger: there are way more options available to us, and at the same time there are way more morally relevant differences between the actions. There is thus a mismatch between our values and our reality, and it is incumbent upon us to be more explicit and rigourous in thinking about the scope of our actions, using frameworks oriented towards maximisation.</p><p>There are variations on this story, of course, but I hope that some version of it is recognisable to at least some readers. Fleshed out with more details, I think there is every chance it could be a plausible historical narrative. But in Parfit\u2019s work, it was no more than a conjectural just-so story; and it has only got more skeletal and bare-bones since him, leading to&nbsp;<a href=\"https://academic.oup.com/book/38952/chapter-abstract/338159937\"><u>a lot of bad \u2018explanations\u2019 for why people\u2019s moral judgments about large numbers are (supposedly) unreliable</u></a>. So while Effective Altruists are committed to their \u2018hard core\u2019, they have no good explanation for&nbsp;<i>why</i> it is true\u2014and thus no account of the context, and limits, of their own reasoning.</p><p>As it happens, I think the \u2018hard core\u2019 of Effective Altruism probably is true. It\u2019s definitely true in the limited realm of charitable donations, where large and identifiable differences in cost-effectiveness between different charities have been empirically validated. It becomes murkier as we move outwards from there\u2014issues of judgment, risk/reward trade-offs, and unknown unknowns make it less obvious that we can identify and act on interventions that are&nbsp;<i>hugely</i> better than others\u2014but it\u2019s certainly plausible. Yet, while Effective Altruism has made a prominent and potentially convincing case for the importance of maximisation-style reasoning, this style of reasoning is simultaneously dangerous and liable to fanaticism. The only real solution to this problem is a proper understanding of the&nbsp;<i>context</i> and&nbsp;<i>limits</i> of maximisation. And it is here that Effective Altruism has come up short.</p><h2>Conclusion and takeaways</h2><p>I believe that Effective Altruism\u2019s use of rigourous, explicit, maximisation-oriented reasoning is both very novel and (often) good. But if Effective Altruists don\u2019t want to end up in crazy town, they need to start getting on a different train. They need a different understanding of their own enterprise, one grounded less in grand systematic theories of morality and more in an account of the modern world and recent history. Precisely because they lack that, Effective Altruists are simultaneously drawn towards and repulsed by the most absurd outer limits of utilitarianism. I think this marks a failure of seriousness; it certainly marks a failure of self-understanding. It marks something the Effective Altruist community needs to rectify.</p><p>None of this means abandoning the weirder sides of the movement. Many of the parts of Effective Altruism that are considered \u2018weird\u2019 relative to the wider culture are unrelated to the dynamics discussed in this post: for example, Alexander has<a href=\"https://astralcodexten.substack.com/p/highlights-from-the-comments-on-the-909\">&nbsp;<u>repeatedly</u></a><a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\">&nbsp;<u>emphasised</u></a> that concerns about risks from AI are logically independent from the philosophy of \u2018longtermism\u2019 or utility calculations, and should be treated separately.</p><p>But it does mean getting clearer about what exactly Effective Altruism&nbsp;<i>is</i>, and the contexts in which it makes sense: being more rigourous and explicit about why it is important to use systematic maximisation frameworks, what such frameworks are intended to do, and what countervailing considerations are most important to pay attention to. And this will likely require facing up to the limits of consequentialism, and thinking about situations in which consequentialist reasoning harms moral thinking more than it helps.</p><p>I don\u2019t know what the consequences of this might be for Effective Altruism. Maybe it would \u2018leave everything as it is\u2019, and have no practical ramifications for the movement; I doubt it. Maybe, as suggested by&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dQvDxDMyueLyydHw4/population-ethics-without-axiology-a-framework\"><u>a recent post by Lukas Gloor</u></a> which discusses similar themes, it would create space for alternatives to \u2018longtermism\u2019, via a rejection of some of the arguments-from-systematicity that underpin it. Maybe it might lead to a rethinking of Effective Altruist approaches to politics, policy, and other contexts where game-theoretic considerations are paramount and well-honed judgment is necessary, and where explicit consequentialism can thus potentially cause serious problems. I don't know; one can\u2019t typically predict the outcomes of reflection in advance.</p><p>Perhaps you, the reader, don\u2019t feel any of this is necessary. Perhaps you follow&nbsp;<a href=\"https://astralcodexten.substack.com/p/criticism-of-criticism-of-criticism\"><u>Alexander</u></a>: you think that too much sweeping criticism of Effective Altruism has been produced, and the movement should just get on with the object-level business of doing good while being aware of specific \u2018anomalies\u2019 that don\u2019t fit with its assumptions which could suggest deeper problems. This is a reasonable position to take. Too much time in the armchair thinking of criticisms is almost never the best way to actually identify the problems in a movement or set of ideas. But the flip-side of this reasoning is that, when an anomaly&nbsp;<i>does</i> arise, Effective Altruism should be able to focus in on it; and it must be open to explanations of the anomaly that are able to unite it with other questions, even if those explanations are critical.</p><p>I think that the \u2018train to crazy town\u2019 phenomenon, the lack of clarity about whether and when utilitarian reasoning runs out, is just such an anomaly\u2014one that hurts Effective Altruism\u2019s ability to achieve its stated goals (both within and without the movement). I\u2019ve tried to give an explanation that connects this anomaly to other problems in moral philosophy, and potentially suggests a way forward.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjztdrzzto7s\"><sup><a href=\"#fnjztdrzzto7s\">[14]</a></sup></span>&nbsp;You may disagree with my explanation; I certainly am not certain of my own core claims. But&nbsp;<i>some</i> diagnosis of the problem is necessary. Absent such a diagnosis, Effective Altruists will keep getting on the train to crazy town, and non\u2013Effective Altruists will continue to recognise this (implicitly or explicitly) and be put off by it. It\u2019s a problem worth engaging with.<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncgg0zndjp3f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcgg0zndjp3f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I believe the metaphor is Ajeya Cotra\u2019s, from<a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/\">&nbsp;<u>her appearance on the 80,000 Hours Podcast</u></a>. But its recent proliferation seems to be down to William MacAskill, who used it in response to one of Tyler Cowen\u2019s arguments<a href=\"https://conversationswithtyler.com/episodes/william-macaskill/\">&nbsp;<u>on the latter\u2019s podcast</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsznj8h9zof\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsznj8h9zof\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Sam Atis,<a href=\"https://www.samstack.io/p/the-train-to-crazy-town\">&nbsp;<u>\u2018The train to Crazy Town\u2019</u></a>. Sam attributes this example to Tyler Cowen, since Cowen has referred to it a number of times, but it originates in Thomas Hurka,&nbsp;<a href=\"https://www.jstor.org/stable/2380627\"><u>\u2018Value and Population Size\u2019</u></a>, p.499. Thank you to Cowen for pointing this out to me.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7r7ntf7q856\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7r7ntf7q856\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Christoph Fehige,&nbsp;<a href=\"https://www.fehige.info/pdf/A_Pareto_Principle_for_Possible_People.pdf\"><u>\u2018A Pareto Principle for Possible People\u2019</u></a>, pp.534\u2013535.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwpn58mpwwk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwpn58mpwwk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For instance: John Rawls\u2019 separateness of persons argument (<a href=\"https://www.utilitarianism.net/objections-to-utilitarianism/separateness-of-persons\"><u>discussion</u></a>); Bernard Williams\u2019 integrity objection (<a href=\"https://plato.stanford.edu/entries/williams-bernard/#DayCannTooFarOffhWillAgaiUtil\"><u>discussion</u></a>); Thomas Nagel\u2019s analysis of impartiality (<a href=\"http://individual.utoronto.ca/stafforini/parfit/parfit_-_equality_and_priority.pdf\"><u>discussion</u></a>); T. M. Scanlon on aggregation (<a href=\"https://link.springer.com/article/10.1007/s10892-011-9113-3\"><u>discussion</u></a>).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6uezsn8tf2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6uezsn8tf2\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Nota bene</i>: in his paper, Cowen uses the term \u2018pluralism\u2019 to mean something different; nothing turns on this.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnplsvlu3guwf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefplsvlu3guwf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Cowen, writing in a population ethics context, writes about total utility specifically, but the same logic applies to average utility or person-affecting utility, which face similar problems (e.g., the<a href=\"https://academic.oup.com/book/12484/chapter-abstract/163169714\">&nbsp;<u>Absurd Conclusion</u></a>). I will equate utility with total utility for the rest of this post, with this footnote hopefully marking that this is without loss of generality.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntxmtv8j65h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftxmtv8j65h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>It doesn\u2019t include non-consequentialist theories with no room for purely impartial, aggregative beneficence. But this doesn\u2019t matter much for my purposes, because such theories would not be compatible with Effective Altruism\u2014since Effective Altruism is<a href=\"https://static1.squarespace.com/static/5506078de4b02d88372eee4e/t/5f36ae8fd76ee3582c25475d/1597419152486/The_Definition_of_Effective_Altruism.pdf\">&nbsp;<u>an attempt to pursue the good most effectively</u></a>, it doesn\u2019t make complete sense without an independent moral reason to pursue \u2018the good\u2019 in aggregate. Two papers on this theme which focus specifically on longtermism are Karri Heikkinen,<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Karri-Heikkiken-GPI-working-paper-1.pdf\">&nbsp;<u>\u2018Strong Longtermism and the Challenge from Anti-Aggregative Moral Views\u2019</u></a>, and Emma J. Curran,<a href=\"https://www.emmajcurran.co.uk/_files/ugd/991224_960782efa4244562ab629081e90600b3.pdf\">&nbsp;<u>\u2018Longtermism, Aggregation, and Catastrophic Risk\u2019</u></a> (public draft).</p><p>(As an aside, it is quite common for Effective Altruists to argue that, independent of all of these issues, any acceptable moral theory&nbsp;<i>must</i> include impartial beneficence in order to retain<a href=\"https://rychappell.substack.com/p/beneficentrism?s=r\">&nbsp;<u>\u2018basic moral decency\u2019</u></a>; but this seems to me to be simply false, as someone with no theory of beneficence can still end up recommending many of the same actions and dispositions, just so long as they recommend them for different reasons. This has recently been discussed in an Effective Altruist context by Lukas Gloor,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dQvDxDMyueLyydHw4/population-ethics-without-axiology-a-framework\"><u>\u2018Population Ethics Without Axiology: A Framework\u2019</u></a><u>;</u> for more detailed philosophical discussion, see Philippa Foot,<a href=\"https://www.jstor.org/stable/3131701\">&nbsp;<u>\u2018Utilitarianism and the Virtues\u2019</u></a>.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp6aox1oxd0f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp6aox1oxd0f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These can be possible actions, or states of affairs, or intentions\u2014whatever you want to evaluate.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwxq59my8gsj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwxq59my8gsj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Philosophers might appreciate an analogy with Linda Zagzebski\u2019s great paper on<a href=\"https://www.jstor.org/stable/2220147\">&nbsp;<u>\u2018The Inescapability of Gettier Problems\u2019</u></a>; in a nearby possible world, Cowen might conceivably have written \u2018The Inescapability of Repugnant Conclusions\u2019.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrhonaj5hldc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrhonaj5hldc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>While it might not be immediately obvious that the \u2018moral parliament\u2019 framework in particular falls victim to the problem of fanaticism, the issue here is that this framework as introduced is little more than a&nbsp;<a href=\"https://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\"><u>\u2018metaphor\u2019</u></a>, and even fleshed out versions of the framework fail to meet the universal domain condition (cf. Newberry and Ord,&nbsp;<a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty.pdf\"><u>\u2018The Parliamentary Approach to Moral Uncertainty\u2019</u></a>, p.9). Thus, for those who seek to use moral uncertainty as a way to avoid problems for universal-domain moral theories, invoking the moral parliament is equivalent to an admission of defeat. I discuss the option of denying the universal domain assumption in the next section.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqn9gr1svp2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqn9gr1svp2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I think the latter reading is closer to the truth: in Cowen\u2019s other paper on the repugnant conclusion,<a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.357.2937&amp;rep=rep1&amp;type=pdf\">&nbsp;<u>\u2018Resolving the Repugnant Conclusion\u2019</u></a>, he quite explicitly rejects universal domain and only allows moral comparisons to be valid over bounded sets of options. But nothing I say in this section should be taken as representative of Cowen\u2019s actual views.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn0rpdszgk19\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn0rpdszgk19\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Something like this, albeit stated in more explicitly numerical and consequentialist language, is captured in the three premises of Benjamin Todd\u2019s&nbsp;<a href=\"https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/\"><u>\u2018rigourous argument for Effective Altruism\u2019</u></a> (about 29 minutes in).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrkbyiu5pv9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrkbyiu5pv9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See (e.g.) Parfit,&nbsp;<i>Reasons and Persons</i>, \u00a731&nbsp;<a href=\"https://academic.oup.com/book/12484/chapter-abstract/163163612\"><u>\u2018Rational Altruism\u2019</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjztdrzzto7s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjztdrzzto7s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I don\u2019t think I\u2019m suggesting a new \u2018paradigm\u2019, whatever overblown meaning that word might have outside of the context of science; I am hoping to make a more modest suggestion than that, just a few new questions and arguments.</p></div></li></ol>", "user": {"username": "Peter McLaughlin"}}, {"_id": "GEukFgwrrebW7efNz", "title": "What does it mean for an AGI to be 'safe'?", "postedAt": "2022-10-07T04:43:18.441Z", "htmlBody": "", "user": {"username": "So8res"}}, {"_id": "Xhji3mL99urPoKsfw", "title": "Georgia College and State University Cofounder Search", "postedAt": "2022-10-07T08:45:29.013Z", "htmlBody": "<p>Hi, I'm a freshman at GCSU in Milledgeville, GA and I'm looking for likeminded people on campus who would be interested in serving as cofounder for an EA university group. It could likely be a paid position depending on whether we get grant funding, and would probably be a 5-7 hour/week commitment, mostly on the front end of the semester. Please comment if you or anyone you know may be interested, or if you have tips on how to organize an EA group at a smaller school!</p>", "user": {"username": "Margo Mason"}}, {"_id": "gcPp2bPin3wywjnGH", "title": "Is space colonization desirable? Review of Dark Skies: Space Expansionism, Planetary Geopolitics, and the Ends of Humanity", "postedAt": "2022-10-07T12:26:16.260Z", "htmlBody": "<p>This is a linkpost to a 2020 review of a book about space colonization by a professor of political science at Johns Hopkins University in the Boston Review. According to the review, the book\u2019s thesis is that \"unfettered space expansion is likely to increase the threat of large-scale violence and generally exacerbate human insecurity\". I'm quoting a few excerpts that might be most relevant to the readers of this forum. Bold is added by me.&nbsp;</p><blockquote><p><i>Dark Skies: Space Expansionism, Planetary Geopolitics, and the Ends of Humanity</i> is a <strong>painstakingly researched, historically informed and theoretically sophisticated analysis centered on the disarmingly simple question: Is space colonization desirable?</strong> Deudney answers emphatically in the negative, in the process providing an invaluable history of Space Age dreams from the Enlightenment to the current interplanetary obsessions of libertarian plutocrats.</p></blockquote><p>&nbsp;</p><blockquote><p>It is important to highlight the subtlety of this position, because it is too easy to dismiss all opponents of specific plans for space exploration, such as Mars colonization, as astro-Luddites. Like Clarke and Sagan, Deudney is anything but. He constructs a taxonomy of possible positions, with Promethean Technophilia at one extreme and Luddite Technophobia at the other. <strong>His own stance, Cautious Soterianism (named after Soteria, the Greek goddess of safety), gets between these extremes, favoring \u201cdecelerating\u201d the race to space rather than abandoning it altogether</strong>. The goal Deudney urges is to \u201csteer\u201d the whole thing more prudently than is currently being done, an orientation that will result in numerous \u201cregulatory restraints and even selective relinquishments\u201d of some plans.</p><p>Deudney arrives at this insight by placing the drive to colonize the heavens in the larger context of the twentieth-century development of weapons of mass destruction as well as the efforts we have made\u2014via bilateral and multilateral treaties and other legal regimes\u2014to contain the threat they pose. <strong>The book\u2019s thesis is that unfettered space expansion is likely to increase the threat of large-scale violence and generally exacerbate human insecurity</strong>. If we adopt this approach, Mars colonization should be shelved, for now at least, while we expand and enhance the terms of the <a href=\"https://www.unoosa.org/oosa/en/ourwork/spacelaw/treaties/introouterspacetreaty.html\">Outer Space Treaty</a>, an international agreement crafted in 1966 that places limits on various forms of space adventurism and militarization (for starters, nobody can <a href=\"https://youtu.be/8wPH4MEK4tk\">own the moon</a>).</p></blockquote><p>&nbsp;</p><blockquote><p>In the beginning we went to space with the express mission of winning the Cold War and becoming more efficient at killing each other, a historical reality that tugs against the breathless visions of the technologists. In fact, this past looms over the whole subsequent project of space expansion. The quest to inhabit other worlds, Deudney suggests, has piggybacked on this darker purpose without ever fully coming to terms with it.</p><p>If this is right, we must look beyond the breezy pleadings of space expansion\u2019s most ardent advocates and examine the actual arguments for this grand vision. There have been three broad patterns of attempted justification. The first appeals to evolution, the second to long-term human security, and the last to the expansion of human freedom.</p></blockquote><p>&nbsp;</p><blockquote><p>The failure to grapple with this kind of scenario underlines what Deudney takes to be an entirely unexamined assumption of space expansionists: that \u201chumanity will be succeeded by creatures who are significantly better than humans.\u201d He calls this <strong>the ascensionist assumption, the notion that by going up we will inevitably become morally and politically better</strong>. So <i>of course</i> space colonists and their descendants pose no threat to us. Again, the point is not limited to space cowboys. Most techno-utopians\u2014the giddier boosters of geoengineering, Artificial Superintelligence, nanotech, de-extinction, genetic enhancement, you name it\u2014operate with some version of this background assumption. And that is putting the case charitably, because the other alternative is that they simply don\u2019t care about the potentially catastrophic consequences of their dreams and schemes.</p><p>But evolutionary theory gives us no reason to endorse any version of the ascensionist assumption. It is obviously not the case that over 3.5 billion years of development, species\u2014including <i>Homo sapiens</i>\u2014have become appreciably more tolerant of each other. Life has always been, and remains, red in tooth and claw. In fact, while appealing to evolutionary arguments, space expansionists are covertly counting on the magical disappearance of this aspect of the evolutionary process. It appears that even as we bring life generously to space, we run the very significant risk of erasing our own future</p></blockquote><p>&nbsp;</p><blockquote><p>That brings us, finally, to the other two attempted justifications for space expansion: that the program will safeguard the long-term future of our species and that it will enhance human freedom.</p></blockquote><p>&nbsp;</p><blockquote><p>The association of space expansion with the preservation and expansion of individual freedom is probably extremely dubious. Space expansion, far from being a form of freedom insurance, is more likely to produce the perfection of despotism and the complete subordination of the individual to the collective. Those who value individual liberty should be strong skeptics and opponents of space expansion, not enthusiastic supporters.</p></blockquote><p>&nbsp;</p><blockquote><p>Many space expansionists have expressed moving visions of scientific progress in lofty and inspiring rhetoric, but the uglier realities of nature and geopolitics have a way of hijacking good intentions.</p></blockquote>", "user": null}, {"_id": "nxtw2XTnuugeuGgCb", "title": "Analysing a 2036 Takeover Scenario", "postedAt": "2022-10-06T20:48:49.543Z", "htmlBody": "", "user": {"username": "ukc10014"}}, {"_id": "EvJxELPauFmLyBqtX", "title": "Doing Better on Climate Change", "postedAt": "2022-10-07T17:22:06.878Z", "htmlBody": "<p><strong>Summary:</strong> How to factor climate change into our efforts to make the world a better place. For people new to climate change and experts alike. Includes what to do on climate change if you are focused on a different cause. Bottom line: everyone should have climate change in their portfolio of causes, though not necessarily as their primary focus. Details include how to reduce your own emissions, influencing society\u2019s emissions, and career options.</p><h1>Opening Remarks</h1><h2>What is this post and how should you read it?</h2><p><i>This post is how I think people should think about climate change, especially people trying to make the world</i><a><i><strong><sup><u>[1]</u></sup></strong></i></a><i> a better place.</i></p><p>It\u2019s written for a few audiences:</p><ul><li><strong>Anyone with a general interest in climate change:</strong> Are you familiar with the idea that climate change is a thing that\u2019s happening, is considered to be a problem, and has something to do with greenhouse gases (GHGs)? If so, you\u2019re ready to read this post. The career advice at the end is more specialized, but most of the post is written for you, especially to help you account for climate change in your efforts to make the world a better place.</li><li><strong>Climate Experts:</strong> There will likely be some details of interest throughout, perhaps especially in the career advice section, though much of the content will be familiar. I would also read this for ideas on how to talk about climate change to wider audiences. Notice e.g. how I frame things, what specifics I discuss, and also what specifics I<i> don\u2019t&nbsp;</i>discuss (e.g., almost the entirety of climate science &amp; impacts). I see a lot of content from climate experts that, in my view, do these things poorly. And please chime in with anything you have to add, contest, etc.</li><li><strong>People focused on other causes, including other global catastrophic risks and existential risks (GCRs/XRisks)</strong><a><sup><u>[2]</u></sup></a><strong>:</strong> The post makes some important points about how to engage with climate change itself and especially the field of people working on climate change. This should be worth your time. See in particular the section \u201cIt\u2019s fine to focus on other issues, but please do so respectfully\u201d, and also the section \u201cPersuasion\u201d.</li></ul><p>Some of the content is addressed to effective altruism (EA), but most of it is relevant to anyone regardless of their involvement in EA. In case you\u2019re not familiar with EA, it\u2019s a community of people working to make the world a better place as well as they can, or at least to do things along those lines.</p><h2>Outline and main ideas</h2><p>This post has four main sections:</p><ul><li><strong>Climate Change Fundamentals:</strong> The main point is that it is good to reduce GHG emissions. That\u2014and not other aspects of climate change\u2014should be front and center, at least for most of the likely audience of this post. There\u2019s a lot of uncertainty about how hard we should try to reduce emissions, but everyone can and should take some actions for emissions reduction. Also, the world as a whole should aim for net zero total emissions, but getting there is very difficult. Indeed, addressing climate change may be orders of magnitude more difficult than addressing other GCRs/XRisks. That means the world should be investing more in addressing climate change\u2014but we arguably should be investing less. Finally, those who are focused primarily on other issues should still speak positively about the importance of climate change.</li><li><strong>General principles for reducing greenhouse gas emissions:</strong> Most generally, we should look for opportunities to reduce emissions that are \u201ccost-effective\u201d in the sense of reducing more emissions for less money, less time, less stress, etc. Specific opportunities can influence both supply and demand, such as shifting the energy supply from fossil fuels to nuclear/renewables and reducing energy demand. It can also include reducing both one\u2019s own individual emissions and the emissions of larger populations. Often, the transition is the hard part: many opportunities are difficult/expensive at first but get easier/cheaper over time. Finally, exactly what to do will vary from person to person.</li><li><strong>Specific opportunities for reducing greenhouse gas emissions:</strong> For reducing one\u2019s own individual emissions, the best opportunities are plant-based diets, urban lifestyles or similar (e.g., living without a car), and avoiding air travel. It is sometimes proposed that the best way to reduce one\u2019s own emissions is to abstain from having children, but that raises thorny ethical issues and is not recommended. We can also influence other people\u2019s emissions, in particular by persuading other people to reduce emissions, by advancing public policies that reduce emissions, and by helping the transition to clean energy systems. Some more specialized opportunities are not covered, such as regarding cement and refrigerants.</li><li><strong>Climate change careers:</strong> It helps to have an interdisciplinary training, but this is not strictly necessary. There are great opportunities for people with skill sets in engineering, public communication, business/finance, natural and social science, and for people who can do a bit of everything. There are also good opportunities for people with climate change backgrounds seeking to transition to work on other topics; specific opportunities discuss are on GCR/XRisk, AI governance, and nuclear winter. The section ends with some brief remarks on philanthropy.</li></ul><h2>Some personal background</h2><p>I\u2019ve worked on climate change on and off for about 15 years. It was the focus of <a href=\"http://sethbaum.com/ac/2012_Dissertation.html\"><u>my Ph.D. dissertation</u></a>. I also designed and taught an undergraduate <a href=\"https://www.e-education.psu.edu/geog30/\"><u>sustainability class</u></a>; some ideas from that class are incorporated into this post. Since the Ph.D., I\u2019ve mainly worked on other topics through my role at the <a href=\"http://gcrinstitute.org/\"><u>Global Catastrophic Risk Institute</u></a>. I\u2019ve taken a renewed interest in climate change recently, prompted by various recent publications, extreme weather events, and personal factors. I have a critique of the climate change discussion in <a href=\"https://gcrinstitute.org/papers/064_precipice.pdf\"><u>my review of </u><i><u>The Precipice</u></i></a>, which has a lot of great content but I think significantly understates the risk.&nbsp;</p><p>I\u2019m glad to see recent interest in climate change on the EA Forum (e.g., <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\"><u>1</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/zjc8utqES7jLjgBYn/superforecasting-long-term-risks-and-climate-change\"><u>2</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/4q8yhafPfyu6Zp2K8/how-much-does-climate-change-and-the-decline-of-liberal-2\"><u>3</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/nBN6NENeudd2uJBCQ/the-most-important-climate-change-uncertainty\"><u>4</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/DmshhvanTb9wSh5x6/climate-change-problem-profile\"><u>5</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/H3aKKezuaG4EZSsBu/climate-recommendations-in-ea-giving-green-and-founders\"><u>6</u></a>; also slightly earlier posts <a href=\"https://forum.effectivealtruism.org/posts/pcDAvaXBxTjRYMdEo/climate-change-is-neglected-by-ea\"><u>1</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/BwDAN9pGbmCYZGbgf/does-climate-change-deserve-more-attention-within-ea\"><u>2</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/eJPjSZKyT4tcSGfFk/climate-change-is-in-general-not-an-existential-risk\"><u>3</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/iiqWXvjuwjSK2vywn/climate-change-geoengineering-and-existential-risk\"><u>4</u></a>, and probably also a few others I\u2019m missing). There\u2019s been a lot of great discussion. However, I think it\u2019s missing a certain perspective, which I attempt to articulate here. In particular, I see a need for more emphasis on the practical actions that different people could and maybe should be taking to address climate change.</p><p><strong>I don\u2019t know everything there is to know about climate change.</strong> Many people know more than I do. I\u2019m sure some of them could fill in details that I\u2019ve missed, or maybe even correct some mistakes. That said, I do believe the content here to be overall reliable.</p><h1>Climate Change Fundamentals</h1><h2>It is good to reduce greenhouse gas emissions</h2><p><i>If you get nothing else out of this post, get this: all else equal, it is good to reduce greenhouse gas emissions.</i></p><p>This is hopefully an obvious point. There is essentially zero debate about this, bad faith actors notwithstanding. The only meaningful debate is on how to reduce emissions, including questions of whether certain actions to reduce emissions are worth whatever sacrifice they entail.</p><p>I worry that this basic point gets lost in many discussions of climate change. Instead, they focus on climate change itself, including the underlying science and the risks it poses to the world. These things can be important, but only to the extent that they inform the decisions people face, especially on reducing greenhouse gas emissions. I worry that in focusing on climate change itself, the opportunities and decisions get lost in the shuffle. As a consequence, people may end up failing to recognize their opportunities and making worse decisions, with inferior outcomes for the world.</p><p>This is definitely not just an EA problem. It\u2019s widespread. A classic example is the film <i>An Inconvenient Truth</i>, which was almost entirely on the science of climate change, with only a brief, simplistic discussion of solutions tacked on at the end. Also, the EA space is certainly not uniformly guilty of this. For example, EA analyses of climate change philanthropy (<a href=\"https://forum.effectivealtruism.org/posts/yhKnbcX6YmTgLpfwJ/climate-change-donation-recommendations\"><u>1</u></a>,<a href=\"https://forum.effectivealtruism.org/posts/H3aKKezuaG4EZSsBu/climate-recommendations-in-ea-giving-green-and-founders\"><u>2</u></a>) strike me as quite constructive. But I do see room for improvement.</p><p>In focusing on emissions reduction, this post could perhaps be criticized for having <a href=\"https://www.sei.org/perspectives/move-beyond-carbon-tunnel-vision/\"><u>carbon tunnel vision</u></a> and not a more holistic perspective on environmental sustainability. I certainly agree that holism is important. Perhaps there could be value to a future post on the entirety of environmental sustainability. That said, specific actions to reduce emissions often also help on other environmental issues. Furthermore, for the likely audience of this post, I believe a focus on emissions is appropriate instead of a focus on how to adapt to climate change because it is likely to have better opportunities for emissions reduction and because emissions reduction is, in my view, more pertinent to GCR/XRisk.<a><sup><u>[3]</u></sup></a> That said, people focused on addressing global poverty may benefit from attention to climate change adaptation. Finally, geoengineering is discussed briefly toward the end.</p><h2>The value of emissions reduction is uncertain but potentially large</h2><p>The value of emissions reduction comes from the harm that a given amount of emissions would cause. Each chunk of emissions makes climate change more intense by some amount, increasing the harms. The more bad those harms would be, the more valuable it is to reduce emissions.</p><p>It is important to think in terms of the value of avoiding some chunk of emissions. That is the relevant parameter for decision-making purposes. It\u2019s easy to get caught up in questions of how bad climate change may be if the world as a whole emits some amount of emissions. However, you are not the world as a whole and you do not face the decision of whether to emit all those emissions. We can influence societal emissions\u2014more on that below\u2014but even then it\u2019s still important to think in terms of some chunk of emissions and not the whole thing.</p><p>So, how good would it be to avoid some chunk of emissions? In short: we don\u2019t know.</p><p>Any given chunk of emissions can have a variety of potential harms. It can help to organize them into three categories:</p><ul><li><strong>Disruptive harms:</strong> The chunk of emissions contributes to massive disruption across the world, but civilization endures.</li><li><strong>Catastrophic harms:</strong> The chunk of emissions contributes to the collapse of civilization or worse.<a><sup><u>[4]</u></sup></a></li><li><strong>Wildcard mild harms:</strong> The chunk of emissions contributes to harms that are fairly small.</li></ul><p>The disruptive harms are the standard expectations of mainstream climate change research. They involve scenarios that can be quite bad, with suffering potentially at a massive scale. The 2022 northern summer is illustrative: intense <a href=\"https://earthobservatory.nasa.gov/images/150083/heatwaves-and-fires-scorch-europe-africa-and-asia\"><u>heat waves</u></a> across the northern hemisphere, some with severe droughts, plus severe flooding in <a href=\"https://en.wikipedia.org/wiki/2022_Pakistan_floods\"><u>Pakistan</u></a>. This and more all project to get a lot worse. In the words of Katharine Hayhoe, a top climate scientist and communicator, <strong>\u201cOur civilisation was constructed for a climate that no longer exists\u201d</strong> (<a href=\"https://correspondent.afp.com/watching-world-burn\"><u>1</u></a>). Finally, these disruptive harms may affect many future generations because CO2 emissions can remain in the atmosphere for centuries or longer (<a href=\"https://www.nature.com/articles/climate.2008.122\"><u>1</u></a>, <a href=\"https://climate.nasa.gov/ask-nasa-climate/3143/steamy-relationships-how-atmospheric-water-vapor-amplifies-earths-greenhouse-effect/\"><u>2</u></a>).</p><p>The catastrophic harms are those studied by people interested in climate change as a GCR/Xrisk. The research is limited (<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0016328720301646\"><u>1</u></a>, <a href=\"https://www.pnas.org/doi/full/10.1073/pnas.2108146119\"><u>2</u></a>, <a href=\"https://link.springer.com/article/10.1007/s10584-021-02957-w\"><u>3</u></a>, <a href=\"https://theprecipice.com/\"><u>4</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\"><u>5</u></a>, <a href=\"https://www.harpercollins.com/products/our-final-warning-six-degrees-of-climate-emergency-mark-lynas\"><u>6</u></a>, <a href=\"https://link.springer.com/article/10.1007/s10584-022-03430-y\"><u>7</u></a>), but for now, the basic picture seems to be of some chance of climate change contributing to global/existential catastrophe. The size of the effect is a point of debate. My sense is that it\u2019s being underestimated, but it\u2019s difficult to pin down.</p><p>The wildcard mild harms are ones in which some major change happens that renders emissions relatively unimportant. It could be a massive energy breakthrough, like solving fusion power. It could be some other radical technological change, such as an AI singularity. Or it could be some other catastrophe, such as a nuclear war. Any of these could drown out the effects of greenhouse gas emissions if and when they occur.</p><p>Any given chunk of emissions contributes harms proportionate to the size of the chunk. Your own personal emissions do not single-handedly cause suffering at a massive scale, but they do play a role, or at least they may under disruptive climate change scenarios.</p><p>Here is how I would think about any given action to reduce emissions. The default expectation is that the action is helping to avoid disruptive harms, which may be quite large. There is some chance of the action helping to avoid extreme catastrophe. And there is some chance that the action will end up only helping to avoid relatively small harms. The exact balance of these possibilities is difficult to assess.</p><p>Notice that under all these various possibilities, it is still good to reduce greenhouse gas emissions. Therefore: <strong>It is good to reduce greenhouse gas emissions, and it might be very good.</strong></p><p>One final detail. There is a line of economics research attempting to quantify the dollar value of GHG emissions, known as the <a href=\"https://en.wikipedia.org/wiki/Social_cost_of_carbon\"><u>\u201csocial cost of carbon\u201d</u></a>. The SCC research was a primary focus of my Ph.D. studies. I left that feeling overwhelmingly skeptical of SCC research due to its false precision, questionable assumptions, and an inappropriate cost-benefit framework. It seemed to generally understate the value of emissions reduction. I haven\u2019t kept up with the literature but I see that current estimates vary from tens to hundreds or even thousands of dollars per ton CO2 emissions. For comparison, one ton of CO2 emissions is comparable to one person taking a regular commercial <a href=\"https://www.theguardian.com/environment/ng-interactive/2019/jul/19/carbon-calculator-how-taking-one-flight-emits-as-much-as-many-people-do-in-a-year\"><u>flight</u></a> from New York to London. I wouldn\u2019t use SCC research as any sort of precise guide, but it does provide an additional point of information.</p><h2>Climate change warrants massive investment\u2014but not necessarily ours</h2><p>It\u2019s important to distinguish between <i>what should we do with our resources</i> and <i>what should society do with its resources</i>. Those are, for the most part, two different things. Note that the resources here can include time, money, attention, and other things.</p><p><strong>Out of all the major global risks, climate change may get the most investment\u2014but it may also need the most investment</strong>.</p><p>Currently, annual global GHG emissions are about 50 billion tons of \u201cCO2 equivalent\u201d. That includes 35 billion tons of actual CO2, plus other greenhouses, such as methane, in a quantity that has an aggregate warming effect comparable to an additional 15 billion tons of CO2. These various molecules eventually fall back out of the atmosphere, but it\u2019s a slow process. For CO2, it takes centuries or longer. <strong>So, to stop the warming, the world needs to go from 50 billion to zero.</strong><a><sup><u>[5]</u></sup></a></p><p><strong>Getting anywhere near zero will be very difficult.</strong> It involves restructuring a large portion of the entire global industrial economy. It may require a lot of technology that doesn\u2019t exist yet, widespread changes in living patterns, massive financial investment, social and political will, and more. The scale of the problem is enormous.</p><p>Other GCRs/XRisks, such as AI, pandemics/biotechnology, and nuclear weapons, do not have this characteristic. They do all require significant effort and investment. More can and should be done. However, the quantity of investment needed would appear to be multiple orders of magnitude smaller.<a><sup><u>[6]</u></sup></a> If that is indeed correct, then we can conclude: <strong>Society should make large investments in addressing all of the GCRs/XRisks\u2014and make a much larger investment in addressing climate change.</strong></p><p>Unfortunately, society is not making all of these various investments, and its investments are not always well-targeted. That leaves room for us to make a difference. <strong>Because emissions reduction is so difficult, we may often have better opportunities on other GCRs/XRisks.</strong> However, some opportunities to reduce emissions can still be better than some opportunities on other GCRs/XRisks. The tradeoff is difficult to evaluate due to deep uncertainty about all of the GCRs/XRisks (<a href=\"https://www.cser.ac.uk/resources/analysis-evaluation-methods/\"><u>1</u></a>, <a href=\"https://gcrinstitute.org/quantifying-the-probability-of-existential-catastrophe-a-reply-to-beard-et-al/\"><u>2</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/vwWfvTSiuQKHC7cZf/quantifying-the-probability-of-existential-catastrophe-a\"><u>3</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/KfqFLDkoccf8NQsQe/potential-downsides-of-using-explicit-probabilities\"><u>4</u></a>).</p><p>There are two easy cases:</p><ul><li>When our resources are not scarce, such that reducing emissions doesn\u2019t detract from our work on other issues. <i>This is discussed further in the next section.</i></li><li>When we have other reasons to allocate resources to emissions reduction, e.g. when it leads to other opportunities, such that work on emissions reduction also advances our work on other issues. <i>An example is discussed in the section on policy.</i></li></ul><h2>Everyone should have emissions reduction in their portfolio of doing good</h2><p><i>For climate change, a portfolio approach works better than a cause prioritization approach.</i></p><ul><li><strong>Cause prioritization approach:</strong> An actor (an individual, group, etc.) identifies the most important issue and works on that.</li><li><strong>Portfolio approach:</strong> An actor works across multiple issues.</li></ul><p>A lot of analysis on climate change seems to be motivated by a cause prioritization approach. For example, research may assess whether climate change is a GCR/XRisk, and if it is, how large of a GCR/XRisk it is compared to other GCRs/XRisks. That sort of comparison can inform cause prioritization, though even then, the comparison should be between the opportunities to reduce the risks, not between the size of the risks themselves. However, this sort of comparison is unnecessary for a lot of action to address climate change.</p><p>Especially within the space of GCRs/XRisks, climate change is distinctive in the wide variety of opportunities to reduce the risk. Tomorrow, you will probably not launch any nuclear weapons, but you will emit greenhouse gases.</p><p>Cause prioritization\u2014or better yet, opportunity prioritization\u2014is appropriate when there are tradeoffs between causes/opportunities. You cannot use the same hour to work on both A and B and you cannot use the same dollar to donate to both A and B (unless you give both half an hour or fifty cents, but you get the idea). However, this reasoning does not always apply to GHG emissions reduction.</p><p>To see why, here\u2019s an analysis of turning lights off. To those of you who already know that this is not the most impactful action, relax, will get to the big things later.</p><p><strong>Thought experiment: \u201cthe light switch\u201d.&nbsp;</strong><i>Suppose, hypothetically, that you are the only person in a room. It is nighttime, and the light is on. Then, something comes up, and you find yourself compelled to leave the room and go somewhere else. As you leave the room, you have the option of turning the light off. To do this, you need to raise your arm and flip a switch. During that brief moment, you had nothing else to do with that arm of yours.</i></p><p>I am joking a bit here in labeling this as a thought experiment. Of course, we have all been in this situation many times. However, it is good illustration of a more general point.</p><p>The point is that this action helps on climate change and has essentially zero downside. It helps climate change because it reduces electricity consumption, which generally results in reduced GHG emissions. The action has essentially zero downside because in that moment, you might as well turn off the light. Your arm wasn\u2019t doing anything else. It was not a scarce resource whose usage needed to be prioritized across different causes or opportunities.&nbsp;</p><p><i>The decision to switch the light off is easy.</i> You don\u2019t have to worry about how severe climate change may be, whether it could cause global/existential catastrophe, or how important that may be compared to other risks or other cause areas. You can just flip the switch.</p><p><strong>If you are already turning off lights because (or at least in part because) of climate change, then congratulations, you already have climate change in your portfolio of actions to make the world a better place.</strong> It might not be an important item in your portfolio, but it is there, as well it should be. Indeed, the GHG benefits of turning off lights are small relative to some other opportunities discussed below. I\u2019ve used it here because it\u2019s such a clear example of how we can all readily include emissions reduction our portfolios.</p><h2>It\u2019s fine to focus on other issues, but please do so respectfully</h2><p>The portfolio approach has its limits. We still need to decide what to dedicate our careers to, or our funds, etc.</p><p>Should GHG emissions reduction be one\u2019s top cause? For some people, perhaps, for other people, perhaps not. Speaking for myself, emissions reduction has not been my primary focus since I finished my Ph.D. (and a short post-doc after that), though it remains within my portfolio at <a href=\"https://gcrinstitute.org/\"><u>GCRI</u></a> and I have been trending more nervous about climate change.</p><p>For those who are focused on other causes, it\u2019s still important to speak positively about the importance of climate change and emissions reduction, for two reasons:</p><p><strong>(1) Encouraging other people to reduce emissions</strong>, including through actions (such as turning off lights) that don\u2019t interfere with them contributing to other causes.</p><p><strong>(2) Generating interest and respect</strong> from people who are more focused on climate change, or who at least are more concerned about it.</p><p>Both items are important, including for EA.</p><p>Regarding item (1), EA is getting a decent amount of attention these days, which means more opportunity to encourage other people to reduce emissions.<strong>&nbsp;</strong>See the section below on persuasion.</p><p>Regarding item (2), this quote is an illustration of how things can go wrong:</p><blockquote><p><i>\u201cThe biggest issue I have with EA is the lack of attention to climate change. I am supporter and member of the EA but I take issue with the lack of attention to climate change. Add me to the category of people that are turned off from the community because it's weak stance of climate change.\u201d</i> (<a href=\"https://forum.effectivealtruism.org/posts/BwDAN9pGbmCYZGbgf/does-climate-change-deserve-more-attention-within-ea?commentId=R96s7JZQoiwFHaX4c\"><u>Link</u></a>, see also <a href=\"https://forum.effectivealtruism.org/posts/pcDAvaXBxTjRYMdEo/climate-change-is-neglected-by-ea#6__EA_is_in_danger_of_making_itself_a_niche_cause_by_loudly_focusing_on_topics_like_x_risk\"><u>this</u></a>.)</p></blockquote><p>I have also seen this sort of reaction from people I have worked with. That includes some people of considerable importance within government, academia, etc. For these people, climate change seems to obviously be a very important global problem. When they see people in the GCR/XRisk space dismissing the importance of climate change, they are turned off. <strong>We lose credibility and influence.</strong></p><p>It\u2019s especially unfortunate because <strong>people working on climate change are natural allies</strong>. Climate change is a complex, large-scale, global, long-term issue. It can take a high degree of altruism to focus on climate change, and a high degree of aptitude. We should want these people on our side, for whatever it is we are working for.</p><p>I personally don\u2019t like the \u201cinformation hazard\u201d terminology, but that type of concept is applicable in this context. Fortunately, it\u2019s not hard to phrase things well. To wit:</p><p><strong>Bad rhetoric:</strong> Climate change is a low priority and it is wrong to focus on it.</p><p><strong>Good rhetoric:</strong> Climate change is important and it\u2019s great that people are working on it, but I see even better opportunities on other issues. (Could also say \u201cbetter opportunities for myself, or for people with certain backgrounds, etc.\u201d)</p><p>I have had many conversations like this with people focused on climate change and I have never gotten any negative reaction from the \u201cgood rhetoric\u201d. They understand that climate change is not the only issue in the world. Indeed, many of them have interdisciplinary backgrounds in which working across issues is standard practice. They might not immediately switch to your preferred issue, but you\u2019ve at least planted the seed in their mind and left them with a favorable impression. Sometimes that\u2019s all we can do. And for them, climate change may well be where their best opportunities lie. These things can vary from person to person.</p><p>Climate change is important. Reducing greenhouse gas emissions is good. We can and should say these things, even if we are mainly focused on other issues.</p><h1>General principles for reducing greenhouse gas emissions</h1><p>An EA perspective can be valuable for crafting opportunities to reduce GHG emissions. We should be looking for \u201ccost-effective\u201d opportunities that reduce more emissions for less money, less time, less stress, etc.</p><p>Here are some other general principles for developing opportunities to reduce emissions.</p><h2>Demand-side and supply-side actions are important and synergistic</h2><p>GHG emissions are largely a matter of supply and demand:</p><ul><li><strong>The supply side:</strong> The production of energy, food, cement, refrigerants, and other sources of GHGs. Actions include things like switching from fossil fuels to nuclear &amp; renewables.</li><li><strong>Demand side:</strong> The acquisition and consumption of products that are sources of GHGs. Actions include things like not using as much energy.</li></ul><p>One might think that supply-side actions are more important because they can be done at scale and don\u2019t require behavioral change. Or, one might think that demand-side actions are more important because they are more widely accessible to people trying to make a difference. In fact, both can be important (<a href=\"https://www.pnas.org/doi/abs/10.1073/pnas.0908738106\"><u>1</u></a>, <a href=\"https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020AV000284\"><u>2</u></a>, <a href=\"https://iopscience.iop.org/article/10.1088/1748-9326/ab8589/meta\"><u>3</u></a>).</p><p>They are also synergistic with each other. <strong>It is easier to build a clean energy system if demand for energy is low.</strong><i>&nbsp;</i>Ditto for other sources of GHGs.</p><p>This post has some emphasis on demand-side actions for the benefit of a general audience of people trying to make the world a better place. There are some supply-side details that a general audience should know; see the section on clean energy systems. Toward the end there\u2019s also some discussion of supply-side actions written more for specialists.</p><h2>Individual and collective actions are synergistic</h2><p>Actions to reduce GHG emissions can be categorized as individual or collective:</p><ul><li><strong>Individual actions:</strong> Actions to reduce one\u2019s own personal emissions.</li><li><strong>Collective actions:</strong> Actions to reduce the emissions of some larger number of people.</li></ul><p>It\u2019s common to find arguments against individual action (here\u2019s <a href=\"https://www.vox.com/the-highlight/2019/5/28/18629833/climate-change-2019-green-new-deal\"><u>one</u></a>). It\u2019s said to be \u201ca drop in the bucket\u201d, shifting blame from institutions (e.g., fossil fuel corporations) to individuals, and a distraction from larger-scale collective action.</p><p>I think this is mostly wrong. A drop in the bucket might only be a small amount of good, but it\u2019s still good. For comparison, someone\u2019s donation to a poverty charity might not end poverty, but it still helps some poor people, and that is good. Regarding blame, any blame must be shared between corporations and their customers (among others). The point about distraction is sometimes valid, though the reverse can also hold: people who take individual actions may do more on collective actions. For those of us with a high motivation to make the world a better place, we\u2019re more likely to do both.&nbsp;</p><p>Here is an example of the synergy that can exist between individual and collective action: <a href=\"https://en.wikipedia.org/wiki/Eric_Adams#Plant-based_diet\"><u>Eric Adams</u></a> became vegan, then wrote a book about it, then advocated for \u201cMeatless Mondays\u201d in public schools in his capacity as Borough President of Brooklyn and then Mayor of New York City.<a><sup><u>[7]</u></sup></a></p><p>We might not all have such a high degree of social and policy influence, but we can all have some.</p><p>Also, some of us are primarily focused on other causes. A critic of individual action might say \u201cI don\u2019t care how green you are. I want you in the movement for climate justice\u201d (<a href=\"https://www.vox.com/the-highlight/2019/5/28/18629833/climate-change-2019-green-new-deal\"><u>link</u></a>, echoed by a top climate change researcher <a href=\"https://twitter.com/NaomiOreskes/status/1576920491087908864\"><u>here</u></a>). However, this doesn\u2019t make sense if you\u2019re already dedicated to some other cause (global poverty, AI safety, etc.) and don\u2019t plan to switch. You may not have space in your life to join the climate justice movement, or to pursue other time-consuming climate change activities. But you can still take some individual actions, and maybe some more feasible collective actions. If that\u2019s what you\u2019re able to do, great, do it, and don\u2019t let anyone tell you you\u2019ve got it wrong.</p><h2>The transition is often the hard part</h2><p>A lot of opportunities to reduce GHG emissions have relatively large upfront costs of one sort or another, but become easier over time.</p><ul><li><strong>Capital investment.</strong> Purchasing (for example) an electric bicycle is an upfront cost, but it can save you money over time if you use it instead of a car, and can even bring a net profit if you later decide to sell the car.</li><li><strong>Behavior change.</strong> Learning (for example) how to get around via e-bike can take some effort, but once you\u2019ve got the hang of it, it may become relatively effortless.</li><li><strong>Social situations.</strong> Becoming (for example) an outspoken advocate for emissions reductions can change your experience in social situations, bringing discomfort, but after a while you may adjust to it and it becomes more comfortable.</li></ul><p>I still remember when I went vegetarian back in undergrad. That was in Rochester, New York, where the buffalo wings were amazing and my friends made sure I knew it. It was awkward at first, but after not too long it became routine. This sort of experience seems pretty common, buffalo wings notwithstanding.</p><p>One can live quite well with low GHG emissions. That\u2019s been many people\u2019s personal experiences, myself included. There\u2019s even <a href=\"https://www.nature.com/articles/s41558-021-01219-y\"><u>research</u></a> documenting this.</p><p><i>When the costs go down over time, the case for taking the action is more compelling.</i></p><h2>The optimal actions will vary from person to person</h2><p>General-purpose risk and decision analysis can inform our decisions, but a lot will need to be done on a case-by-case basis, for two reasons:</p><p><strong>Disagreement on the value of emissions reduction.</strong> In <a href=\"https://en.wikipedia.org/wiki/Aumann's_agreement_theorem\"><u>theory</u></a>, we arguably should agree, but in practice we don\u2019t. That much is apparent from the ongoing debate about climate change in the research literature, EA forum posts, etc.<a><sup><u>[8]</u></sup></a>Therefore, each of us will need to decide for ourselves how valuable we think GHG emissions reduction is.</p><p><strong>Our own personal characteristics.</strong> A lot of opportunities to reduce GHG emissions are deeply personal: where we live, what food we eat, etc. General-purpose analysis can provide some guidance on these matters. We also can and should try to support each other on our decisions. Ultimately, though, we\u2019ll each need to judge for ourselves how a given action would affect our lives and what we\u2019re comfortable doing.</p><p>For example, the discussion below has some emphasis on urban issues, which happen to be a longstanding interest of mine. Urban issues are certainly important for emissions reduction, but I probably wouldn\u2019t be saying as much about them if I didn\u2019t coincidentally have that interest. Because of this interest, my optimal actions may include more emphasis on urban issues than people who don\u2019t share this interest.</p><h1>Specific opportunities for reducing greenhouse gas emissions</h1><p>This section covers what are, to my knowledge, the best ways of reducing emissions. There\u2019s a massive amount of research on this and I\u2019m not up to speed on it all, but I do believe the basic ideas check out. The ideas can be grouped as follows:</p><ul><li><strong>Individual actions:</strong> plant-based diets, urban lifestyles (or similar), and avoiding air travel. I also discuss having children, which is sometimes said to be important, but it raises thorny issues.<a><sup><u>[9]</u></sup></a></li><li><strong>Collective actions:</strong> persuasion, policy, and energy systems. I am omitting some more specialized opportunities, such as regarding <a href=\"https://www.drawdown.org/solutions/alternative-cement\"><u>cement</u></a> and <a href=\"https://www.drawdown.org/solutions/alternative-refrigerants\"><u>refrigerants</u></a>, because they\u2019re arguably less relevant for a general audience and because I just don\u2019t know much about them, my apologies\u2014but see the links if you\u2019re interested.</li></ul><p>It\u2019s good to understand the basics of all of these so that you can seek out the best opportunities for yourself given your own personal circumstances and your degree of concern about climate change.</p><h2>Plant-based diets</h2><p>This one\u2019s simple. Vegan is best, but a \u201cveganish\u201d approximation is still very good. Clearly, there are also other worthy reasons to favor a plant-based diet, such as for nonhuman animal welfare. For GHG emissions, the difference between plant-based and animal-based foods is quite large. I know a lot of you are already vegan or veganish. Keep up the good work!</p><p>If you\u2019re ambitious, you could figure out which plant-based foods are the best or account for other factors such as food miles, though these factors are generally smaller. Personally, I am not that ambitious, so I focus on just eating vegan. I\u2019m not a perfect vegan, but I do OK. We shouldn\u2019t beat ourselves up over not being perfect\u2014we need that emotional energy for everything else we\u2019re working on.</p><h2>Urban lifestyles (or similar)</h2><p>This one\u2019s more interesting. We know that cars need a lot of energy. Electric cars can be helpful, but most electric grids use fossil fuels, and even on a decarbonized grid, there can be value in reducing the load. So it\u2019s good to skip the car, but that can be difficult if you live in a car-centric place. And so, it really helps to live in a not-car-centric place. Additionally, if you live in an apartment, you\u2019re sharing walls with other units, which requires less energy to heat in the winter. There are also other benefits like reduced travel of vehicles delivering you mail, packages, etc. And if you live in a smaller unit, you\u2019ll need less air conditioning in the summer, and you may end up buying less stuff (because where would you put it?). You may even waste less food because you can easily grocery shop so often that you only buy what you need. <a href=\"https://www.fao.org/publications/card/en/c/5e7c4154-2b97-4ea5-83a7-be9604925a24/\"><u>Food waste</u></a> is actually a pretty big problem, though it\u2019s a proportionately smaller problem if you\u2019re vegan(ish).</p><p><strong>Why the \u201cor similar\u201d:</strong> A low-carbon lifestyle does not require living in a city. Indeed, worldwide the lowest-carbon lives are often rural, living on local resources etc. It\u2019s also possible to live low-carbon in plenty of towns, villages, and even some suburbs. But for many of us, it will work best to be somewhere more urban.</p><p>Many people, myself included, enjoy the urban, car-free or car-light lifestyle and have lots of experience with it. If you\u2019re thinking of making the switch, there are a lot of resources out there to help you with this.</p><p><strong>Also: electric bicycles, including cargo bicycles, to replace cars.</strong> Seriously, this a major <a href=\"https://electrek.co/2021/01/21/electric-bicycles-e-bikes-to-outsell-cars-in-europe/\"><u>revolution</u></a> in transportation. A lot of people have succeeded at using e-bikes instead of cars (e.g., <a href=\"https://www.wsj.com/articles/the-other-electric-vehicle-e-bikes-gain-ground-for-americans-avoiding-gas-cars-11659758415\"><u>USA</u></a>, <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1361920919303827\"><u>Netherlands</u></a>). Don\u2019t let anyone tell you it\u2019s \u201ccheating\u201d if you don\u2019t pedal. Just try, if at all possible, to live somewhere with safe cycling infrastructure. A literature review across OECD countries <a href=\"https://www.tandfonline.com/doi/full/10.1080/01441647.2022.2113570\"><u>found</u></a>, \u201cThe leading barriers [to cycling] related to riding on the road alongside motor vehicles.\u201d Makes sense to me. At a minimum, it\u2019s good for you to not be using one of those motor vehicles out there threatening the cyclists.</p><h2>Avoiding airplane travel</h2><p>It takes <i>a lot</i> of energy to get a plane into the air. It is also extremely difficult to decarbonize aviation. Electric power just doesn\u2019t work well for airplanes. So, avoiding plane flights is a big way to reduce emissions. Indeed, even a single plane flight could end up constituting the majority of your annual personal GHG emissions (<a href=\"https://www.theguardian.com/environment/ng-interactive/2019/jul/19/carbon-calculator-how-taking-one-flight-emits-as-much-as-many-people-do-in-a-year\"><u>1</u></a><u>,</u> <a href=\"https://www.bbc.com/future/article/20200218-climate-change-how-to-cut-your-carbon-emissions-when-flying\"><u>2</u></a>). Some climate change researchers have <a href=\"https://www.science.org/content/article/why-some-climate-scientists-are-saying-no-flying\"><u>stopped flying</u></a>, even at the expense of missing out on conferences and other activities. Personally, I am considering the same for myself, though I recognize that in some cases flying can be worth it. For example, if I ever have the chance to fly somewhere for some diplomatic summit to defuse a geopolitical crisis, then yes, I would definitely take the flight. I\u2019d hope you would too. For more modest travel opportunities, I find it hard to judge. Some air travel can be important, but those emissions really are quite high.</p><p><strong>Remote interaction can help a lot.</strong> We all now have plenty of experience with remote interaction (thank you, COVID-19), but, speaking for myself personally, I still feel like I\u2019ve only scratched the surface. There are tools out there that I haven\u2019t really put to the test, such as <a href=\"https://en.wikipedia.org/wiki/Proximity_chat\"><u>proximity chat</u></a> (e.g., <a href=\"https://gather.town/\"><u>Gather</u></a>). Some of you may be a lot more experienced at this. GCRI has always been remote, and we run a remote <a href=\"https://gcrinstitute.org/get-involved/#advising-collaboration\"><u>Advising and Collaboration Program</u></a>, but we\u2019re not set up to organize anything more substantial like a remote conference. However, if someone else is doing the organizing, I for one would be interested in participating. Yes, remote meetings have well-known disadvantages, though they also have advantages including lower cost, reduced time commitment, and inclusivity (e.g., to parents of young children and people from countries with visa restrictions). Personally, I generally avoid traveling even short distances to in-person meetings because I can\u2019t justify the time, but I accept lots of invitations to remote meetings. It seems good to try maxing out the value of remote meetings to reduce the need for in-person meetings, especially those that would require airplane travel.</p><p>This does raise difficult questions such as regarding events like EA Global. In terms of GHG emissions, a plane flight to EAG matters <i>a lot</i> more than, say, eating vegan while you\u2019re there. Of course, flights to EAG also have benefits that could plausibly outweigh the harms in at least some cases. <strong>EAG organizers and attendees should take the air travel carbon footprint into account.</strong> Related: check out <a href=\"https://www.eaglobal.org/events/eagxvirtual-2022\"><u>EAGxVirtual 2022</u></a>. I\u2019m pretty excited about this. If this can be a good event, then it could pave the way for more low-carbon EA community interaction.</p><p>Also: if possible, try living somewhere with enough nearby that you have less reason to fly.</p><h2>Having children</h2><p>Yes, any children you have will emit greenhouse gases, perhaps quite a lot. But this is a thorny matter.</p><p>Ultimately, people are more important than GHGs. We should only be reducing emissions to the extent that doing so is good for people, and good for nonhuman animals and whatever else we might care about. Fewer people does mean less GHG emissions, but we should be optimizing for people, not for GHGs. A world without people would be \u201cgreat\u201d for climate change but terrible overall.</p><p>It is possible for a population to be suboptimally large, such as when it exceeds its carrying capacity, precipitating collapse. There are <a href=\"https://en.wikipedia.org/wiki/Ecological_footprint\"><u>analyses</u></a> finding that the human population is currently exceeding its carrying capacity. However, even if this is true (I don\u2019t know), it\u2019s a <a href=\"https://en.wikipedia.org/wiki/I_%3D_PAT\"><u>function</u></a> of both population size and how people are living. Would the world be better with fewer people consuming more or more people consuming less?</p><p>Also\u2014if it\u2019s bad to have children, should we also try to cull the current population down to a more sustainable size? That seems like a pretty clear <i>no</i>.</p><p>Personally, I have a very hard time advising anyone to not have children because of the GHG emissions. It\u2019s such a deeply personal decision. Also, I would think that anyone conscientious enough to even consider this may tend to raise children who become future leaders of important global issues.<a><sup><u>[10]</u></sup></a> So, if you\u2019re considering having children, it\u2019s fine to take GHG emissions into account, but I wouldn\u2019t feel pressured over it.</p><h2>Persuasion</h2><p>Persuading other people to act multiplies your impact. Here it helps to have positive rhetoric about climate change, to take enough actions yourself so as to be a good role model and not a hypocrite, and to have a good understanding of how to reduce emissions. That includes both the technical dimensions\u2014which types of actions are high-impact\u2014and the personal dimensions\u2014what is it like to actually do these things.</p><p>As always: know your audience. Figure out what they care about and work with that. In some cases, it can even help to focus on something other than climate change, i.e. \u201cco-benefits\u201d. Maybe someone doesn\u2019t care much about climate change, but they\u2019re on a tight budget and had never heard of e-bikes before. Maybe someone just needs to learn a few good vegan recipes.<a><sup><u>[11]</u></sup></a> Whatever it may be, tapping into that can help.</p><p>Some of the opportunity is less about persuading specific people to do specific things and more about changing shared cultural norms and values. A big one is car-centrism. Here in the United States, it is commonly assumed that cars are <i>the way</i>. Ditto for some other places. Getting people to contemplate alternatives can be difficult. But, there is a case to be made that <a href=\"https://carsruincities.info/\"><u>cars ruin cities</u></a> and that <a href=\"https://dutchcycling.nl/\"><u>a better world is possible</u></a>.<a><sup><u>[12]</u></sup></a> These are public conversations worth having.</p><p><strong>Persuasion may be a good activity for the EA space to take on.</strong> EA has a sizable audience. Including emissions reduction in EA messaging could go a long way at relatively low expense.</p><p><strong>There is high value to persuasion by people focused on other issues.</strong> If climate change is the main issue you\u2019re focused on, people may discount your message: \u201cOh, she\u2019s just saying that because she\u2019s a climate change person.\u201d If you have a different focus, your words may carry more weight: \u201cShe\u2019s focused on X, but she still thinks emissions reduction is that important? That sounds serious.\u201d You\u2019ll also have access to audiences that are less accustomed to hearing emissions reduction messages. I speak from experience on this as someone mostly focused on other issues.</p><h2>Policy</h2><p>Getting policy wins can be a large impact. It\u2019s no surprise that two EA climate philanthropy analyses both <a href=\"https://forum.effectivealtruism.org/posts/H3aKKezuaG4EZSsBu/climate-recommendations-in-ea-giving-green-and-founders\"><u>emphasize</u></a> policy. Some people will even argue that policy is the only important action, though I think that\u2019s mistaken. Everything helps, and there are synergies. For example, there\u2019s less reason to oppose a carbon tax if one is already not emitting much carbon. (A carbon tax is a tax on GHG emissions.)</p><p>It can help to be creative about what counts as climate policy. The obvious ones are the big national policies explicitly targeted at climate change, such as the <a href=\"https://www.congress.gov/bill/117th-congress/house-bill/5376/text\"><u>US Inflation Reduction Act of 2022</u></a>. If you have opportunities to help enact these policies, then by all means, go for it. However, many other policies also affect GHG emissions, and some of these may present better opportunities to make a difference. <i>Housing policy is climate policy. Transportation policy is climate policy.</i><a><i><strong><sup><u>[13]</u></sup></strong></i></a><i> Agriculture policy is climate policy.</i> And so on. A lot of the relevant policy is local (city, state, etc.). Compared to national policy, the stakes of local policy may be lower but it may be easier for you to make a difference, such that it\u2019s a better overall opportunity. (In EA jargon, less impactful but more tractable and neglected.)</p><p>Housing policy is a good example. In the US (and possibly other places), demand for a low-emission urban lifestyle exceeds the supply. There aren\u2019t many places where one can live well without a car, and they tend to be expensive. Policies to increase the urban housing supply could help with this. For the most part, it wouldn\u2019t even be a sacrifice\u2014to the contrary, it would benefit the city economically, environmentally (via reduced air pollution), for <a href=\"https://homelessnesshousingproblem.com/\"><u>homelessness</u></a>, and more. The challenge is overcoming legacy zoning laws and NIMBYism. Indeed, EA groups have previously expressed some support for land use reform to increase the housing supply (YIMBYism; <a href=\"https://www.openphilanthropy.org/research/land-use-reform/\"><u>1</u></a>, <a href=\"https://80000hours.org/problem-profiles/land-use-reform/\"><u>2</u></a>, <a href=\"https://founderspledge.com/stories/housing-affordability-in-england-executive-summary\"><u>3</u></a>, <a href=\"https://forum.effectivealtruism.org/topics/land-use-reform\"><u>4</u></a>). The EA analysis finds land use reform to be a promising cause, and that\u2019s without even considering the substantial benefits for GHG emissions reduction. YIMBYism is already changing some policy (e.g., California <a href=\"http://cayimby.org/ab-2011/\"><u>AB 2011</u></a> and <a href=\"https://cayimby.org/ab-2097/\"><u>AB 2097</u></a>); more could be done.</p><p>Policy advocacy can be time consuming and it\u2019s not for everyone. With that in mind:</p><p><strong>If you are considering running for office, consider an emphasis on the intersection of urban issues and climate change</strong>.</p><p>EAs have recently started dabbling in electoral politics, in particular via the candidacy of <a href=\"https://forum.effectivealtruism.org/posts/FG9JJmAYDq9Ghuudg/some-potential-lessons-from-carrick-s-congressional-bid\"><u>Carrick Flynn</u></a>. Policymaking is important, and I for one encourage this especially if the pitfalls of partisanship can be avoided. However, to get elected, it\u2019s important to have good standing within one\u2019s district. The intersection of urban politics and climate change could be one of the best ways of connecting with a district while maintaining involvement in a more global, altruistic cause. Furthermore, despite climate change\u2019s global scope, it is a salient political issue in many places. It\u2019s a win-win.</p><p>Some cities have undergone rapid, transformative change from cars to other modes. I believe the first was <a href=\"https://www.politico.eu/article/pontevedra-city-pioneer-europe-car-free-future/\"><u>Pontevedra, Spain</u></a>. Others include <a href=\"https://www.vice.com/en/article/jg8wq7/slovenia-car-free-city-ljubljana\"><u>Ljubljana, Slovenia</u></a>, <a href=\"https://www.youtube.com/watch?v=sEOA_Tcq2XA\"><u>Ghent, Belgium</u></a>, and most recently <a href=\"https://www.youtube.com/watch?v=sI-1YNAmWlk\"><u>Paris</u></a>. <a href=\"https://www.forbes.com/sites/carltonreid/2020/01/13/birmingham-reveals-radical-ghent-style-plan-to-cut-car-addiction\"><u>Birmingham</u></a> has plans in the works.<a><sup><u>[14]</u></sup></a> A running theme is that in the run-up, it looks like political suicide, but once the schemes are implemented, they\u2019re popular and the politicians get reelected. <strong>It seems worth finding and supporting politicians who are willing to gamble their careers on this sort of transformative change.</strong></p><h2>Clean energy systems</h2><p>Supply-side activities like clean energy are more the domain of professionals, but there are some basic points about energy systems that are good for everyone to know:</p><ul><li><strong>The cost of wind, solar, and batteries has declined dramatically in recent decades, and it\u2019s receiving massive investment.</strong> This is among the biggest pieces of good news for the world.</li><li><strong>The medical harms of nuclear fission power are tragically overestimated.</strong> Fission power could reduce a large portion of GHG emissions, but it hasn\u2019t and it probably won\u2019t, and it\u2019s because of this.</li><li><strong>There are a few wildcard, moonshot solutions</strong>, including nuclear fusion power and <a href=\"https://www.vox.com/energy-and-environment/2020/10/21/21515461/renewable-energy-geothermal-egs-ags-supercritical\"><u>geothermal</u></a>. If they pan out, great, and they may be worthy of public investment, but meanwhile, we can\u2019t count on them.</li></ul><p>The misperception of fission power risk is a vivid illustration of how energy systems are social and political affairs and not just engineering problems. Another example is wind power proposals that fail because locals (NIMBYs) don\u2019t like their appearance. Changing these perceptions and attitudes is an important way to support clean energy.</p><p>According to the International Energy Agency, \u201cSpending on solar PV, batteries and electric vehicles is now growing at rates consistent with reaching global net zero emissions by 2050\u201d (<a href=\"https://www.iea.org/news/record-clean-energy-spending-is-set-to-help-global-energy-investment-grow-by-8-in-2022\"><u>link</u></a>). That\u2019s really important. However, it remains to be seen how much this spending will actually continue and how much it will translate into actual decarbonization, especially in the face of NIMBYism and similar social and political resistance. The climate change problem has not yet been solved.</p><h1>Climate change careers</h1><p>The ideas listed here are mostly about emissions reduction, but there\u2019s a bit on \u201cgeoengineering\u201d, meaning either removing CO2 from the atmosphere or blocking incoming sunlight to lower surface temperatures, and also something on adaptation to extreme climate change.</p><p>This is one area in which my own limited knowledge is salient. I expect that other people active on climate change would have additional good ideas. I\u2019ll keep an eye out for them in the comments.</p><h2>What to study to prepare for careers in climate change</h2><p>See also my <a href=\"https://gcrinstitute.org/common-points-of-advice/\"><u>Common Points of Advice for Students and Early-Career Professionals Interested in Global Catastrophic Risk</u></a>.</p><p>My default advice is to pursue something interdisciplinary, especially to bridge <a href=\"https://gcrinstitute.org/common-points-of-advice/#work-across-sciences\"><u>the divide between (A) humanities-social science-policy and (B) engineering-natural science</u></a>. That keeps your options open and positions you for a range of high-value activities. It\u2019s also solid training for other important topics in case you later want to switch to something else besides climate change. Programs to look into include geography, environmental studies, technology policy, and science &amp; technology studies. I did geography and I\u2019ve worked with people from all of the other fields. Each of the fields is great. I would go with whatever you feel fits you best.</p><p>That said, climate change is such an interdisciplinary, multifaceted topic that you can study virtually anything and find a way to apply it. Also check out the career ideas below and consider deriving a course of study from that.</p><h2>Careers in climate change</h2><p>My perspective on this is somewhat different from the career advice from <a href=\"https://80000hours.org/problem-profiles/climate-change/#research-on-extreme-risks-from-climate-change\"><u>80,000 Hours</u></a>. Here\u2019s how I would look at it:</p><ul><li><strong>For engineers:</strong> Pursue R&amp;D on clean energy systems, or maybe carbon removal or solar geoengineering. Decide based on your skill set and your analysis of the specific opportunities. Clean energy systems may be best for those who also have a business orientation because that\u2019s what\u2019s being deployed at scale right now. Carbon removal could be good for those with an orientation toward basic research, though also consider moonshot energy technologies. Solar geoengineering could be good for those with an orientation to ethics and risk analysis. Solar geoengineering could pose major risks (<a href=\"https://gcrinstitute.org/double-catastrophe-intermittent-stratospheric-geoengineering-induced-by-societal-collapse/\"><u>1</u></a>, <a href=\"https://www.frontiersin.org/articles/10.3389/fclim.2021.720312/full\"><u>2</u></a>), but it could also have high positive value if deployed responsibly.<a><sup><u>[15]</u></sup></a> Perhaps you could help ensure the latter. One other idea: transportation engineering. At least in the US, transportation engineers are, as far as I can tell, typically part of the problem, promoting car-centric urban design at the expense of public health and safety and increasing GHG emissions, though (<a href=\"https://www.amazon.com/gp/product/B09DTR47RR\"><u>1</u></a>, <a href=\"https://twitter.com/WarrenJWells/status/1549502067256860673\"><u>2</u></a>, <a href=\"https://twitter.com/mateosfo/status/1555687454237081600\"><u>3</u></a>). There may be opportunities to shift the entire field in a better direction, which could be a high-value opportunity.</li><li><strong>For talented communicators:</strong> Pursue some combination of public advocacy and electoral politics. Decide based on personal circumstance, such as whether you could plausibly get elected to certain offices. For public advocacy, there is a role for increasing attention to climate change, but I would encourage an emphasis on solutions, especially solutions public perceptions are an important factor. Could you take on public misperception of nuclear fission risk, or car culture, or NIMBYism? Those would be big wins. For electoral politics, develop policy concepts and rhetoric that would be impactful on emissions and that would resonate with people in your district. And then go get elected, or go get someone else elected and make sure you have their ear. Along the way, try to avoid turning EA and GCR/XRisk into politically partisan issues. If possible, you may even want to avoid explicit association with EA and GCR/XRisk (though see <a href=\"https://forum.effectivealtruism.org/posts/FG9JJmAYDq9Ghuudg/some-potential-lessons-from-carrick-s-congressional-bid\"><u>this</u></a> suggesting this may not be a significant concern).</li><li><strong>For those in business and finance:</strong> The clean tech space is obviously huge. I don\u2019t know what the opportunities look like there, but I trust you can figure it out. Also keep an eye out for opportunities to help design and implement certain climate policies, such as public investment in R&amp;D and cap-and-trade schemes, especially if you have an affinity for social science and ethics. And check out <a href=\"https://www.givinggreen.earth/\"><u>Giving Green</u></a>.</li><li><strong>For natural scientists:</strong> Focus your research portfolio on extreme climate change scenarios, including those involving solar geoengineering, to inform analysis of GCR/XRisk. I would also look into ways to have a more direct impact, such as advocacy (as in e.g. Katharine Hayhoe or Michael Mann), or even <a href=\"https://insideclimatenews.org/todaysclimate/scientists-again-call-for-civil-disobedience-to-spur-climate-action-saying-time-is-short/\"><u>civil disobedience</u></a>. Indeed, to the extent that you\u2019re up for it, I would probably make these more direct activities be your primary focus. There are limits to how much additional natural science informs and influences decision-making.</li><li><strong>For social scientists:</strong> Two options. (1) Study real-world strategy for advancing emissions reduction. This post has plenty of starting points: urban politics, fission misperception, civil disobedience, etc. But what actually works? What are the high-leverage opportunities? Be sure to do this in dialog with those who may actually be implementing the strategies. (2) Study the potential for climate change to cause global/existential catastrophe\u2014the collapse of civilization etc. The debate about climate change as a GCR/XRisk largely comes down to how well humanity would cope.</li><li><strong>For interdisciplinary people who can do anything:</strong> Look across all of the above and tease out the most high-value opportunities you can find. I suspect this may involve some social science on real-world strategy informed by a careful read of some engineering details, then cultivation of business allies, followed by some public advocacy and engagement in electoral politics. Good luck!</li><li><strong>Bonus idea\u2014for people who are good at human development and emergency management/preparedness: </strong>This idea is a bit different. It\u2019s focused on climate change adaptation, especially for more extreme scenarios. Climate change could induce some radical human challenges, such as migration at a scale that dwarfs anything currently happening. At that scale, it may be a political challenge as much as it is an operational challenge. People who are good at that sort of thing may do well to work on early-stage preparedness for this sort of scenario, perhaps by piggybacking on more mainstream adaptation initiatives.</li></ul><h2>Careers on other topics for people with backgrounds in climate change</h2><p>A lot of work on climate change can be excellent training for work on other topics. OK, sure, your intimate knowledge of the thermodynamics of the Hadley cell may not specifically translate, but many of the general skills and capabilities will. <strong>Indeed, a case can be made for some people starting out in climate change specifically to prepare them for work on other topics.</strong></p><p>I am a case in point. I did a Ph.D. on climate change in part because climate change is important, but also in part because it was a great learning opportunity. I wanted to do interdisciplinary research; climate change had (and still has) unusually robust interdisciplinary research. It was a great fit.</p><p>There are lots of possibilities. Here are some specific ideas:</p><ul><li><strong>Policy and politics:&nbsp;</strong>It\u2019s common for people in policy and politics to shift the topics they focus on, at least for people in certain types of positions. Because climate change is so multifaceted, it provides natural starting points for engagement on a variety of other topics. <strong>For careers in electoral politics, it may work well to start with an emphasis on climate change while having an eye toward potentially shifting topics later</strong>. For example, future technologies (AI, biotech, etc.) are not yet significant political issues, but perhaps they will be sometime later.</li><li><strong>Cross-cutting analysis of GCR/XRisk:&nbsp;</strong>If you\u2019re already studying the human impacts of climate change, especially the potential for climate change to threaten human civilization, this readily translates to research on other GCRs/XRisks. Overall, the resilience of human civilization to various catastrophes is one of the largest and most important points of uncertainty in GCR/XRisk, of central relevance for how to allocate resources and evaluate tradeoffs across the risks. Expertise in climate change impacts is as good of a background for this as any.</li><li><strong>AI governance:</strong> AI and climate change have some important similarities. Both are driven by profitable activities of some of the largest corporations in the world. Both involve products used by the general public as part of their daily routines, and by businesses and governments and other institutions. AI governance has seen a lot of progress recently, but it remains a relatively early-stage field, especially in comparison to climate governance. One can do a lot by applying insights and experience from climate governance to AI governance. Indeed, this has been a major focus of my own work (<a href=\"https://gcrinstitute.org/on-the-promotion-of-safe-and-socially-beneficial-artificial-intelligence/\"><u>1</u></a>, <a href=\"https://gcrinstitute.org/superintelligence-skepticism-as-a-political-tool/\"><u>2</u></a>, <a href=\"https://gcrinstitute.org/countering-superintelligence-misinformation/\"><u>3</u></a>, <a href=\"https://gcrinstitute.org/lessons-for-artificial-intelligence-from-other-global-risks/\"><u>4</u></a>, <a href=\"https://gcrinstitute.org/social-choice-ethics-in-artificial-intelligence/\"><u>5</u></a>, <a href=\"https://gcrinstitute.org/collective-action-on-artificial-intelligence-a-primer-and-review/\"><u>6</u></a>, <a href=\"https://gcrinstitute.org/the-ethics-of-sustainability-for-artificial-intelligence/\"><u>7</u></a>, <a href=\"https://gcrinstitute.org/moral-consideration-of-nonhumans-in-the-ethics-of-artificial-intelligence/\"><u>8</u></a>, <a href=\"https://gcrinstitute.org/artificial-intelligence-needs-environmental-ethics/\"><u>9</u></a>), which also provides some starting points for anyone interested. See also <a href=\"https://forum.effectivealtruism.org/posts/5MZpxbJJ5pkEBpAAR/the-case-for-long-term-corporate-governance-of-ai\"><u>AI corporate governance</u></a>.</li><li><strong>Nuclear winter:</strong> For natural scientists studying climate change, your research may readily apply to nuclear winter, which is a different type of climate change. <a href=\"http://climate.envsci.rutgers.edu/nuclear/\"><u>Other</u></a> climate scientists are already doing this. Nuclear winter is important but it gets much less research attention than \u201cregular\u201d climate change.</li></ul><h2>Philanthropy</h2><p><i>Climate change vs. other causes</i></p><p>In the section \u201cClimate change warrants massive investment\u2014but not necessarily ours\u201d, I explained that \u201cbecause emissions reduction is so difficult, we may often have better opportunities on other GCRs/XRisks\u201d. This reasoning suggests that philanthropic dollars may be better spent on other GCRs/XRisks. However, this is at best a tentative conclusion. The GCRs/XRisks are all poorly quantified, making the comparison difficult. I can see a case for climate change philanthropy, especially if one has unusually high-value ideas and opportunities.</p><p><i>Specific climate change opportunities</i></p><p>I\u2019ve looked through analysis from <a href=\"https://www.givinggreen.earth/\"><u>Giving Green</u></a> and <a href=\"https://founderspledge.com/stories/climate-change-executive-summary\"><u>Founders Pledge</u></a> (see also <a href=\"https://forum.effectivealtruism.org/posts/H3aKKezuaG4EZSsBu/climate-recommendations-in-ea-giving-green-and-founders\"><u>this</u></a>). I\u2019m not seeing any glaring concerns. Honestly, it looks like good analysis. Giving Green in particular has a lot of great content. My own thinking points in a lot of the same directions, the main difference being that they go into a lot of important detail that\u2019s beyond the scope of my own expertise. For example, Giving Green\u2019s <a href=\"https://www.givinggreen.earth/post/a-business-case-for-beyond-net-zero\"><u>A business case for beyond net zero</u></a> is pretty much exactly how I would look at it, except they\u2019ve done it with more substance than I could. I could be wrong, but it looks like they\u2019re doing good work.</p><p>The most I could say to the teams at Giving Green and Founders Pledge would be to maybe mine this post for any further ideas,<a><sup><u>[16]</u></sup></a> but otherwise keep up the good work.</p><h1>Acknowledgments</h1><p><i>Thanks to Matthijs Maas, Andrew Morton, Jack Davies, and Dakota Norris for helpful feedback on an earlier draft. Any remaining errors are mine alone.</i></p><hr><p><a><u>[1]</u></a>&nbsp; &nbsp; Universe, multiverse, etc.</p><p><a><u>[2]</u></a>&nbsp; &nbsp; GCR and XRisk are sometimes defined to be different, but I\u2019m using them together to avoid lengthy definitional matters. Think of \u201cGCR/XRisk\u201d as being \u201cthe type of extreme global risk that you care about if you care about extreme global risk\u201d.</p><p><a><u>[3]</u></a>&nbsp; &nbsp; There can be a role for extreme adaptation to reduce GCR/XRisk in the event that climate change becomes especially severe. Preparing for this is a more specialized activity\u2014see the section \u201cCareers in climate change\u201d, specifically the \u201cBonus idea\u2014for people who are good at human development and emergency management/preparedness\u201d.</p><p><a><u>[4]</u></a>&nbsp; &nbsp; Other definitions of global/existential catastrophe can be substituted without loss of generality.</p><p><a><u>[5]</u></a>&nbsp; &nbsp; This point is developed at length in the book <a href=\"https://www.penguinrandomhouse.com/books/633968/how-to-avoid-a-climate-disaster-by-bill-gates\"><i><u>How to Avoid a Climate Disaster</u></i></a> by Bill Gates (yes, that Bill Gates). In general, one should be skeptical of books by famous people, especially when they\u2019re outside the person\u2019s primary expertise. However, this book actually seems to be quite good, especially for laying out the problem of getting to zero emissions. It got a favorable review from a top expert <a href=\"https://doi.org/10.1111/risa.13760\"><u>here</u></a>. Gates\u2019s story may also be of note because he became interested in climate change due to his work on global poverty.</p><p><a><u>[6]</u></a>&nbsp; &nbsp; I am borrowing this point from p.88-89 of the book <a href=\"https://gwagner.com/books/climate-shock\"><i><u>Climate Shock</u></i></a> by environmental economists Gernot Wagner and Martin Weitzman. The book is an economic perspective on catastrophic climate change. I have my qualms with some of the economics, but the book has a lot of valuable perspective and is, to its credit, much more pluralistic than a lot of economics research. Wagner also has a great Twitter <a href=\"https://twitter.com/GernotWagner/\"><u>account</u></a>.</p><p><a><u>[7]</u></a>&nbsp; &nbsp; Adams became vegan for health reasons, though his case still shows the synergy between individual and collective action on GHG emissions. To a large extent, it doesn\u2019t really matter that he did this for health and not for climate change. The climate doesn\u2019t care why you reduce GHG emissions.</p><p><a><u>[8]</u></a>&nbsp; &nbsp; See research literature (<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0016328720301646\"><u>1</u></a>, <a href=\"https://www.pnas.org/doi/full/10.1073/pnas.2108146119\"><u>2</u></a>, <a href=\"https://link.springer.com/article/10.1007/s10584-021-02957-w\"><u>3</u></a>, <a href=\"https://theprecipice.com/\"><u>4</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\"><u>5</u></a>, <a href=\"https://www.harpercollins.com/products/our-final-warning-six-degrees-of-climate-emergency-mark-lynas\"><u>6</u></a>, <a href=\"https://link.springer.com/article/10.1007/s10584-022-03430-y\"><u>7</u></a>), EA Forum posts (<a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\"><u>1</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/zjc8utqES7jLjgBYn/superforecasting-long-term-risks-and-climate-change\"><u>2</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/4q8yhafPfyu6Zp2K8/how-much-does-climate-change-and-the-decline-of-liberal-2\"><u>3</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/nBN6NENeudd2uJBCQ/the-most-important-climate-change-uncertainty\"><u>4</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/DmshhvanTb9wSh5x6/climate-change-problem-profile\"><u>5</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/pcDAvaXBxTjRYMdEo/climate-change-is-neglected-by-ea\"><u>6</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/BwDAN9pGbmCYZGbgf/does-climate-change-deserve-more-attention-within-ea\"><u>7</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/eJPjSZKyT4tcSGfFk/climate-change-is-in-general-not-an-existential-risk\"><u>8</u></a>).</p><p><a><u>[9]</u></a><a href=\"https://iopscience.iop.org/article/10.1088/1748-9326/aa7541/meta\"><u>&nbsp; Here</u></a> is an example of a reference covering all four of these individual actions, finding having children to be the most important, followed by the others. <a href=\"https://iopscience.iop.org/article/10.1088/1748-9326/ab8589/meta\"><u>Here</u></a> is a review paper surveying the space of individual actions but not including having children.</p><p><a><u>[10]</u></a>&nbsp; Though I have not seen any literature on this.</p><p><a><u>[11]</u></a>&nbsp; (Resisting the temptation to share vegan recipes here...)</p><p><a><u>[12]</u></a>&nbsp; Take the <a href=\"https://www.youtube.com/c/NotJustBikes\"><u>orange pill</u></a>. Do it. DO IT!!! (I\u2019m only half-joking. Maybe less than half.)</p><p><a><u>[13]</u></a>&nbsp; On transportation policy, see e.g. work by <a href=\"https://gregshill.wordpress.com/scholarship/\"><u>Greg Shill</u></a>, <a href=\"https://theconversation.com/12-best-ways-to-get-cars-out-of-cities-ranked-by-new-research-180642\"><u>Kimberly Nicholas</u></a>, and <a href=\"https://gregshill.wordpress.com/scholarship/\"><u>David Zipper</u></a>.</p><p><a><u>[14]</u></a>&nbsp; Those cities are all in Europe, but there is a lot of great urban innovation elsewhere. For example, the world\u2019s best bus network is in <a href=\"https://www.youtube.com/watch?v=vJR9uCSyGKM\"><u>Curitiba, Brazil</u></a>; the best ciclov\u00eda/open streets program is in <a href=\"https://www.youtube.com/watch?v=ELa5CHsUepo\"><u>Bogot\u00e1</u></a>; the best car-free street network in North America is <a href=\"https://usa.streetsblog.org/2017/01/23/montreals-car-free-street-network-gets-bigger-all-the-time/\"><u>Montreal</u></a>; great design can be found in many Asian cities such as <a href=\"https://twitter.com/alfred_twu/status/1350723920144474115\"><u>Taipei</u></a> and <a href=\"https://twitter.com/DavidZipper/status/1406228352378224643\"><u>Tokyo</u></a>; the largest car-free place in the world is the old Medina of <a href=\"https://www.youtube.com/watch?v=3TogNJnc93Q\"><u>Fez, Morocco</u></a>; in the US, there\u2019s a top-quality new development in <a href=\"https://culdesac.com/\"><u>Tempe, Arizona</u></a>. Finally, we have the intersection between <a href=\"https://www.wsj.com/articles/the-two-faces-of-chinas-surveillance-state-11662130940\"><u>Great Power politics, artificial intelligence, intrusive surveillance states, climate change, and urban affairs</u></a>: China using surveillance cameras and AI to help with basic urban issues, such as making sure the electric bicycles are parked in the right places. Actually, privacy issues aside, it does seem helpful. In much of the West, surveillance cameras are also used for traffic enforcement with much success (e.g., in <a href=\"https://polisen.se/en/laws-and-regulations/traffic-violations/\"><u>Sweden</u></a>).</p><p><a><u>[15]</u></a>&nbsp; For example, by only using solar geoengineering on a limited, gradual, temporary basis to avoid some of the worst effects of global warming (<a href=\"https://www.nature.com/articles/nclimate2493\"><u>1</u></a>).</p><p><a><u>[16]</u></a>&nbsp; Reform the US transportation engineering profession? Support politicians advancing transformative transportation change in cities? These seem like plausible opportunities but I haven't costed them out at all.</p>", "user": {"username": "SethBaum"}}, {"_id": "DymazENLybomtoAwP", "title": "Is there any general advice you have?", "postedAt": "2022-10-10T08:44:11.532Z", "htmlBody": "<p>The Peek behind the Curtain interview series includes interviews with eleven people I thought were particularly successful, relatable, or productive. We cover topics ranging from productivity to career exploration to self-care.&nbsp;</p><p>This ninth and final post covers general advice from my guests.</p><p>You can view bios of my guests and other posts in the series <a href=\"https://lynettebye.com/blog/2022/5/25/a-peek-behind-the-curtain-interview-series\">here</a>. This post is cross posted on my blog.</p><p>__________________________</p><h3>Backchain</h3><blockquote><p>We want to think, figure out some things to do, and then, if we do those things, the world will be better. An important part of that, obviously, is making sure that the things you think about, matter for the outcomes you want to cause to happen.&nbsp;</p><p>In practice, it seems to me that what happens is people get into an area, look around, look at what other people are doing. They spend a few minutes, possibly hours thinking about, \u201cOkay, why would they be doing this?\u201d This seems great as a way to get started in a field. It's what I did.&nbsp;</p><p>But then they just continue and stay on this path, basically, for years as far as I can tell, and they don't really update their models of \"Okay, and this is how the work that I'm doing actually leads to the outcome.\" They don't try to look for flaws in that argument or see whether they're missing something else.&nbsp;</p><p>Most of the time when I look at what a person is doing, I don't really see that. I just expect this is going to make a lot of their work orders of magnitude less useful than it could be.</p><p>Rohin Shah</p></blockquote><h3>Fail often</h3><blockquote><p>One thing on my to-do list is to make sure I fail every month at something because then that means that you have been reaching your max. When I was applying for fellowships, scholarships, internships in policy and whatever, if I had looked at every single rejection as a failure rather than just expected, then I wouldn't be here.&nbsp;</p><p>Abi Olvera</p></blockquote><h3>Guilt is a symptom, not a motivator&nbsp;</h3><blockquote><p>I think guilt is a feeling I have reasonably often, not too intensely, and not all the time. It doesn't generally cause me to be motivated. It's more usually a symptom of being in an unmotivated place. Or being in a place where there's not so many hours I can pour into my thing and I'm wandering in the desert. I'll often feel, less frequently now but still sometimes, I'll feel guilty and frustrated that I haven't moved on something yet, things haven't moved forward, I haven't put in a lot of hours.</p><p>Guilt can get you to do some things, like call your parents or maybe even run an extra lap or something. The bottleneck for finding traction in research isn't something that I find I've been able to usefully address by feeling guilty about it. It's more like an effect of being in a place where I'm for whatever reason not that productive.</p><p>Ajeya Cotra</p></blockquote><h3>It\u2019s okay to skill up before doing impactful work&nbsp;</h3><blockquote><p>I made a mistake of feeling I needed to be working on some really impactful thing right away. I think that worked out okay, in my case. I got somewhat lucky in that regard, but I should have been totally happy to take at least a year or two and just try to find the place where I could skill up as well as possible.</p><p>Daniel Ziegler&nbsp;</p></blockquote><h3>You don\u2019t need to be impactful immediately in your career&nbsp;</h3><blockquote><p>Maybe don't stress out about the fact that you're not impactful <i>now</i>.&nbsp;</p><p>Abi Olvera</p></blockquote><h3>Competing with the very best takes a lot of time and effort&nbsp;</h3><blockquote><p>It's really hard to be the top percent, to compete with people who are the 20 who get the national scholarship. I feel like if I would have eased up, it wouldn't have happened and then I would have had outcomes that are more typical of people from my socioeconomic background. I would say I actually wasn't unhappy but I had reached, probably, the max of what I could do, at least in terms of time.&nbsp;</p><p>Abi Olvera</p></blockquote><h3>Calibrate recommendations&nbsp;</h3><blockquote><p>Maybe one thing is trying to get a sense of either calibration or confidence of recommendations. I think my impression is that EA land has gone wrong in the past along the lines of miscommunications like, \"Oh, org x says y.\" While y is meant as a tentative suggestion, we take that as a cast-iron recommendation and then make lots of decisions. Which then, because it was tentative, actually turned out to be mistaken, and we're a bit screwed over by that. One way of avoiding this, is trying to interrogate\u2014\u201cHave you examples of this? Are there counter-examples? Can you give a credence on this?\u201d&nbsp;</p><p>Greg Lewis&nbsp;</p></blockquote><h3>You don\u2019t have to be a superstar to do good&nbsp;</h3><blockquote><p>[EA] is a group project, and you can only decide for yourself as an individual what your career's going to be, but EA's made up of a bunch of different people with different careers and different projects and different skills. Yours may or may not be this great success story, but we don't all have to be.&nbsp;</p><p>Back in Earning to Give, I was just like, \"I just can't see my way to it. I don't know what I'm good at that would earn a bunch of money, there's nothing.\u201d If we'd stayed in that world where we're really funding constrained, I just wouldn't have been able to contribute that much. I still would have tried, but there just would have been a pretty different niche. I think trying to make peace sometimes with, \"Okay, I'm going to have this niche, and it's not going to be cool or famous or something, but, that's just how it is.\"&nbsp;</p><p>I think, if we had stayed in the days where I was only saving people's lives by donating to GiveWell charities, that's still really good. Michelle Hutchison has a good post about this, that it's just like having some absolute impact. Even if other people are having more impact than you, it doesn't reduce what you are able to accomplish. In the beginning, we were all just like, \"What? I can save a life for $2,000 or whatever?\" And that's still there. The numbers changed around, but you can still do good stuff without being a superstar, and no one can take that away from you.</p><p>Julia Wise</p></blockquote><h3>You might not need a lot of education to do the thing&nbsp;</h3><blockquote><p>I assumed that a lot of the world had more accumulated knowledge than it really did. When we were initially setting up CEA, it really felt like, \u201cWell, I assume that there's a lot to note before you can set up a company. That it\u2019s this complicated business that a bunch of people have studied hard to do, and that's the only way you can set up a company.\u201d Whereas it turned out, in fact, you can just read Companies House website and then just go and do it.</p><p>I think part of this is coming from people who are interested in academic things and do quite a bit of study. That gives you a false perspective on the world because you do a master's in philosophy and then end up doing a PhD and it's all aiming towards academia. The idea is that you need to have done 10 years of education in order to do this thing.</p><p>The idea that there could be something like setting up a company that actually you can just do by reading the website and then filling in the form feels really surprising.</p><p>I see this quite a lot in the people I talk to as well, particularly the ones who have done PhDs. Someone will have done a PhD in chemistry or something and be interested in going into policy and feel like, \"Oh, presumably, therefore, I need to do a bunch of training in policy or do some policy degrees or something\" when actually there are a bunch of fellowships directly designed to get people who have PhDs in science directly into Congress or something, but that feels very counterintuitive when you are in this environment where you need to have done tons of education in order to do a job like a postdoc.</p><p>Michelle Hutchinson&nbsp;</p></blockquote><h3>Sometimes it\u2019s good to finish things even if you\u2019re not sure they\u2019re important&nbsp;</h3><blockquote><p>I had decided that actually, this approach I was thinking of was not likely to work, so probably it wasn't that important. I ended up finishing it anyway, out of a principle of: it's good to finish projects because, as you go through them, you often think that they will be bad or not that useful, but sometimes they'll be impactful in a way that surprises you. At the very least, you want to tell everyone else, \"This is what I did, here's why I think it's not that good, and you shouldn't be thinking about it.\"</p><p>Rohin Shah</p></blockquote><p><i>Enjoying the interview? </i><a href=\"http://eepurl.com/gnWbln\"><i><u>Subscribe to Lynette\u2019s newsletter</u></i></a><i> to get more posts delivered to you.</i></p>", "user": {"username": "lynettebye"}}, {"_id": "d4eYfM8DdyoCeqhkj", "title": "Consider entering the 2024 US diversity visa lottery by November 8 2022 - it's free and fast to do", "postedAt": "2022-10-07T20:33:09.896Z", "htmlBody": "<p><strong>Update on 29/10/2022:</strong> forum user <a href=\"https://forum.effectivealtruism.org/users/us-policy-careers\">@US-policy-careers</a> engaged a United States &nbsp;attorney specialising in immigration law to review this post. Some minor updates to the post have been incorporated below.</p><p><i><strong>Epistemic status and disclaimers:</strong></i></p><ul><li><i>I am a lawyer who has done some immigration work in New Zealand, but I am not a United States lawyer and first heard of the Diversity Visa Program this year. I think I've read and explained the background information below pretty well, but I don't know how worried to be about the risk identified below.</i></li><li><i>Legal risks are usually pretty fact specific, and this is generic advice. Think carefully about your individual circumstances and if you decide to apply you should read the full </i><a href=\"https://travel.state.gov/content/dam/visas/Diversity-Visa/DV-Instructions-Translations/DV-2024-Instructions-Translations/DV-2024-Instructions.pdf\"><i>Instructions/FAQ document</i></a><i> published by the State Department.</i></li><li><i>The ability to travel to the United States is valuable, especially for folks doing high impact EA work. The costs of a mistake here can be high, which means it might be worthwhile briefly speaking with a good US immigration lawyer if you are already in the United States, have firm plans to work or migrate there in the future, or have reasons to be a lot more risk averse than the typical person.</i></li><li><i>This post isn't legal advice, it doesn't give rise to a lawyer-client relationship, and I'll do my absolute best to not be responsible to you if something goes bad in your life after you read it / update your priors on it / make decisions on the basis of it.</i></li></ul><p><i><s>*<strong>If you are a United States lawyer (especially an immigration lawyer), please comment below or contact me with any corrections/suggestions/additional information.</strong></s></i></p><p><i>Thanks also to several people who have applied to the lottery, and to a United States lawyer (practising in an area other than immigration), for their feedback on this post - errors and omissions are entirely mine.</i></p><h1><strong>tl;dr</strong></h1><p>The United States runs a lottery for green cards, and you might be able to enter (by 8 November 2022).</p><ul><li>The US issues around 55,000 green cards (permanent residence visas) each year via a lottery.</li><li>Unless you live in a country which already has really significant migration to the US (list below), you can probably enter that lottery.</li><li>It does not cost you any money to enter, you don't need a passport to enter, you can do it online, and it probably won't take much of your time!</li><li>Your chance of success in a given year as an individual ranges from around 0.28% (worst success rate for Asia region in 2007-2021 fiscal years) to around 10.03% (best success rate for Oceania region in 2007-2021 fiscal years).</li><li>You can increase your probability of success by having a spouse who also applies - only one of you needs to be selected for you both to move.</li><li>This is a game you can play every year, increasing your chances a fair bit (see the table below).</li><li>I know of some EAs who have already done this and are already working or job searching in the United States.</li><li><strong>Carefully consider</strong> whether you applying to the Diversity Lottery is, on expectation, good for the world. If you're doing direct work, there's a good chance it is - but remember that this is a zero-sum game, and your success comes at someone else's expense.</li><li>Also consider the legal risks and consequences identified below.</li><li>Finally, if you are going to apply, don't forget! The deadline is 8 November 2022 at 12.00pm ET, but anecdotally the submission system is usually pretty overloaded in the last week, so consider applying earlier if you can.</li></ul><h1><strong>What is the Diversity Visa Program?</strong></h1><h2><strong>Immigrant visa (green card)</strong></h2><p>A United States 'immigrant visa' allows a person to reside permanently in the United States (whereas a 'nonimmigrant visa' only allows a temporary, albeit often lengthy, stay). There are four main categories of immigrant visa:</p><ul><li>immediate relative and family-sponsored;</li><li>employer sponsored (or applied for without sponsorship on the basis of extraordinary ability or the US national interest);</li><li>certain humanitarian categories; and</li><li>the Diversity Immigrant Visa.</li></ul><p>Darius_M has prepared a useful explanation of the main immigrant and non-immigrant visa categories in <a href=\"/posts/e7NKpwD5z2Mnc7y7G/working-in-us-policy-as-a-foreign-national-immigration\">Working in US policy as a foreign national: Immigration pathways and types of impact</a>. There's also a fair amount of information (of truly variable quality) floating around the internet if you want to learn more. &nbsp;The best source of accurate information about the categories generally is probably the USCIS website (<a href=\"http://www.uscis.gov/\"><u>www.uscis.gov</u></a>) and the USCIS policy manual (<a href=\"https://www.uscis.gov/policy-manual\"><u>https://www.uscis.gov/policy-manual</u></a>).&nbsp;&nbsp;</p><p>The green card also offers a pathway to US citizenship and a US passport. After a sufficient period of time in the country (five years usually, but it could be more if you spend a bunch of time overseas, and it could be three years if you're married to and residing with a US citizen), you should be able to become a US citizen.&nbsp;</p><h2><strong>How the Diversity Visa Lottery works</strong></h2><p>Each year the United States runs a lottery for this category of visa. If you are eligible (see below) and enter, you have a chance to be drawn (by random selection from all eligible applicants) and invited to apply for the Diversity Immigrant Visa.</p><p>Between October 5 2022 and November 8 2022 at 12.00PM (ET) you can enter the lottery for the 2024 fiscal year. <strong>55,000</strong> visas are available in the 2024 fiscal year.</p><h3>Who can apply?</h3><p>Anyone who meets <strong>both</strong> of the two tests below:</p><p><i>Test 1 - high school education or qualifying work experience</i></p><p>Either</p><ul><li>&nbsp;You have the equivalent of a United States high school education - basically this means you successfully completed a 12-year course of formal elementary and secondary education (an equivalency certificate like a United States G.E.D doesn't count);<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwti0tj5vd5\"><sup><a href=\"#fnwti0tj5vd5\">[1]</a></sup></span>&nbsp;or</li><li>You have at least <strong>two</strong> years of work experience in an occupation that requires at least two years of training or experience;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0wvffw546c0h\"><sup><a href=\"#fn0wvffw546c0h\">[2]</a></sup></span></li></ul><p><i>Test 2 - 'native' of, or chargeable to, an eligible country</i></p><p>If more than 50,000 people born in a country ('natives') immigrated to the United States in the previous five years, people born in that country are not usually eligible to participate in the Diversity Visa program. For the 2024 lottery, the following countries are ineligible:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4xba6s6ktus\"><sup><a href=\"#fn4xba6s6ktus\">[3]</a></sup></span></p><blockquote><p>Bangladesh, Brazil, Canada, China (including Hong Kong SAR), Colombia, Dominican Republic, El Salvador, Haiti, Honduras, India, Jamaica, Mexico, Nigeria, Pakistan, Philippines, Republic of Korea (South Korea), United Kingdom (except Northern Ireland) and its dependent territories, Venezuela, and Vietnam.</p><p>Natives of Macau SAR and Taiwan are eligible.</p></blockquote><p><u>However</u>, a person who is a native of one of the ineligible countries below may be able to qualify by 'charging' (don't ask me why they call it this!) to another country on the basis of their own spousal or parental circumstances. Basically, you might be able to claim your parent's or spouse's country of birth for the purpose of qualifying for the lottery.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref18ncmnhndu4\"><sup><a href=\"#fn18ncmnhndu4\">[4]</a></sup></span></p><p>For example, an India-born person with an Australia-born spouse could be \"charged\" to Australia, allowing them to qualify for the lottery even though they were born in an ineligible country.</p><p>Similarly, a Japan-born person (who is a native of an eligible country) might be able to get better odds if they can be charged to Australia on the basis of their spouse or partner. They qualify in their own right, but because there are quotas on individual regions they may get more favourable odds by being counted in the 'Oceania' pool.</p><p>It's all about where you, your parents, and your spouse were <strong>born</strong> - not where you're currently a citizen of. As far as I can tell, the law does not let you claim native or chargeable status of a country on the basis that you have that country's citizenship by descent without being born there, or on the basis that you obtained that country's citizenship by immigrating there and being naturalised there.</p><p>There are some other special rules for unusual cases, which I won't cover here - such as if you were born in the US but are ineligible for US citizenship, or if you were born in a place where neither of your parents has a residence at the time of your birth. Go look those up if that's you - good sources are the 'Foreign Affairs Manual' and the Immigration and Nationality Act.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnswoc12ysxf\"><sup><a href=\"#fnnswoc12ysxf\">[5]</a></sup></span></p><p>You should be careful if you're claiming changeability for a country other than your birthplace because getting this wrong will make you ineligible for a diversity visa.</p><h3>Do you need to be outside of the United States when you apply?</h3><p>No. But see the risks section below.</p><h3>Bringing your spouse or child(ren) - derivative visas</h3><p>If you have a spouse your spouse is also eligible for a 'derivative' immigrant visa. Same story for your child(ren) who are unmarried and under 21 years of age.</p><p>Same-gender marriages count (there is sadly some prospect of the <i>Obergefell </i>ruling that constitutionally protects same-gender marriages might be overturned, which could mean this ceases to be the case in the future, but I do not know much more about this).</p><p>For folks in de facto relationships or civil partnerships, if you marry someone <strong>after </strong>applying to/being selected in the lottery, you can include that person in your application. However, you might expect real scrutiny from the consular officials processing your application - they will want to ensure that the relationship is genuine. Anecdotally, there is some risk that your visa application may be declined because the official doesn't think your claim is honest - while you have some reconsideration rights in this event, I imagine that P(Visa Granted|Selection) is lower where the selected person tries to include a spouse &nbsp;who the applicant married after application, and especially after selection.</p><p>Basically, I suspect that the immigration officials may apply greater scrutiny to your post-selection marriage if their (racist and bad) prior (please forgive me for assuming that immigration officers are good Bayesians - a claim for which I have no basis) is that you aren't in economic circumstances where it would make sense to have a relationship for visa purposes. There is anecdotal evidence on the internet of folks being asked hard questions about their partners' interests and habits (and I believe this evidence, as it's consistent with my experience of immigration law and practice in New Zealand). On the other hand, I've heard anecdotal evidence from folks in developed countries that the consular officials are pretty relaxed about marrying someone after having won the lottery.&nbsp;</p><p>Poly relationships are not recognised - although a legal marriage between two of the people in a poly relationship probably would be (with some potential risk that the consular official deciding the matter decides the marriage is not genuine). Note that one of the questions on the immigrant visa application is whether you are coming to the United States to practice poly<strong><u>gamy</u></strong>. Questions about poly<strong><u>gamy</u></strong> also arise in the US naturalization process and could impact eligibility for citizenship.</p><h2>What are your chances of selection?</h2><p>Sadly, nowhere near as high as for the H1B lottery (<a href=\"https://forum.effectivealtruism.org/posts/xDjaGDa4sxntYCfhn/if-you-re-after-a-job-in-the-us-the-h-1b-lottery-is-in-six\">which is around 1 in 3</a>). Your chance each year you enter probably ranges from around 0.28% (worst success rate for Asia region in 2007-2021 fiscal years) to around 10.03% (best success rate for Oceania region in 2007-2021 fiscal years). To find which region your country of birth (or chargeability) is in, check out the full <a href=\"https://travel.state.gov/content/dam/visas/Diversity-Visa/DV-Instructions-Translations/DV-2024-Instructions-Translations/DV-2024-Instructions.pdf\">Instructions/FAQ document</a> published by the State Department.</p><p>The State Department says:</p><blockquote><p>The number of visas the Department of State eventually will issue to natives of each country will depend on the regional limits established, how many entrants come from each country, and how many of the selected entrants are found eligible for the visa.</p></blockquote><p>P(Selection) depends in the first instance on which <u>region</u> you are a native of or 'chargeable' to.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq3ustkk7qu\"><sup><a href=\"#fnq3ustkk7qu\">[6]</a></sup></span>&nbsp;The starting point is that the 55,000 visas are allocated to six regions (Africa, Asia, Oceania, Europe, North America, South and Central America/Caribbean) proportionate to&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>the population of those regions with the population of non-eligible countries subtracted (e.g. the 'Asia' population would not include Bangladesh, China, India, Pakistan, Philippines, South Korea, and Vietnam). But there is an overall cap for each country of 7% of the total visas available (so, 3850 per country in fiscal year 2024). This means your chances are better:</p><ul><li>in a region with fewer applicants proportionate to population (e.g. Oceania); and</li><li>in a country that isn't at risk of hitting the 7% threshold.</li></ul><p>The Wikipedia page for <a href=\"https://en.wikipedia.org/wiki/Diversity_Immigrant_Visa#Statistics\">Diversity Immigrant Visa</a> has the number of applicants, selected applicants, and the proportion selected for each of the six regions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvmnuvkof4a\"><sup><a href=\"#fnvmnuvkof4a\">[7]</a></sup></span>&nbsp;</p><p>If you are applying with a spouse (or long term partner who you will marry soon), you each get an entry - so the probability that one of you is selected is doubled. Only one of you needs to be selected, as a successful applicant for a diversity visa can bring their spouse on a derivative visa.</p><p>And, of course, you can apply again the next year if you are unsuccessful. This means that even applicants from the least successful region (historically, Asia) have a non-trivial chance of success over many attempts.</p><p>Below, I've set out the probability of success for a single applicant and for a married couple where each spouse applies over a five year period.</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:0.2em 1px;vertical-align:bottom\">&nbsp;</td><td style=\"vertical-align:bottom\">&nbsp;</td><td style=\"text-align:center;vertical-align:bottom\" colspan=\"2\"><strong>Single applicant</strong></td><td style=\"text-align:center;vertical-align:bottom\" colspan=\"2\"><strong>Married couple</strong></td></tr><tr><td style=\"vertical-align:bottom\">&nbsp;</td><td style=\"vertical-align:bottom\">&nbsp;</td><td style=\"vertical-align:bottom\">Lower</td><td style=\"vertical-align:bottom\">Upper</td><td style=\"vertical-align:bottom\">Lower</td><td style=\"vertical-align:bottom\">Upper</td></tr><tr><td style=\"padding:0.2em 1px;text-align:center\" rowspan=\"6\">Africa</td><td style=\"vertical-align:bottom\">P(selected</td><td style=\"text-align:right;vertical-align:bottom\">0.28%</td><td style=\"text-align:right;vertical-align:bottom\">1.27%</td><td style=\"text-align:center;vertical-align:bottom\">-</td><td style=\"text-align:center;vertical-align:bottom\">-</td></tr><tr><td style=\"vertical-align:bottom\">1 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.28%</td><td style=\"text-align:right;vertical-align:bottom\">1.27%</td><td style=\"text-align:right;vertical-align:bottom\">0.56%</td><td style=\"text-align:right;vertical-align:bottom\">2.52%</td></tr><tr><td style=\"vertical-align:bottom\">2 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.56%</td><td style=\"text-align:right;vertical-align:bottom\">2.52%</td><td style=\"text-align:right;vertical-align:bottom\">1.12%</td><td style=\"text-align:right;vertical-align:bottom\">4.98%</td></tr><tr><td style=\"vertical-align:bottom\">3 Years</td><td style=\"text-align:right;vertical-align:bottom\">0.84%</td><td style=\"text-align:right;vertical-align:bottom\">3.76%</td><td style=\"text-align:right;vertical-align:bottom\">1.67%</td><td style=\"text-align:right;vertical-align:bottom\">7.38%</td></tr><tr><td style=\"vertical-align:bottom\">4 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.12%</td><td style=\"text-align:right;vertical-align:bottom\">4.98%</td><td style=\"text-align:right;vertical-align:bottom\">2.22%</td><td style=\"text-align:right;vertical-align:bottom\">9.72%</td></tr><tr><td style=\"vertical-align:bottom\">5 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.39%</td><td style=\"text-align:right;vertical-align:bottom\">6.19%</td><td style=\"text-align:right;vertical-align:bottom\">2.76%</td><td style=\"text-align:right;vertical-align:bottom\">12.00%</td></tr><tr><td style=\"padding:0.2em 1px;text-align:center\" rowspan=\"6\">Asia</td><td style=\"vertical-align:bottom\">P(selected</td><td style=\"text-align:right;vertical-align:bottom\">0.15%</td><td style=\"text-align:right;vertical-align:bottom\">1.08%</td><td style=\"text-align:center;vertical-align:bottom\">-</td><td style=\"text-align:center;vertical-align:bottom\">-</td></tr><tr><td style=\"vertical-align:bottom\">1 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.15%</td><td style=\"text-align:right;vertical-align:bottom\">1.08%</td><td style=\"text-align:right;vertical-align:bottom\">0.30%</td><td style=\"text-align:right;vertical-align:bottom\">2.15%</td></tr><tr><td style=\"vertical-align:bottom\">2 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.30%</td><td style=\"text-align:right;vertical-align:bottom\">2.15%</td><td style=\"text-align:right;vertical-align:bottom\">0.60%</td><td style=\"text-align:right;vertical-align:bottom\">4.25%</td></tr><tr><td style=\"vertical-align:bottom\">3 Years</td><td style=\"text-align:right;vertical-align:bottom\">0.45%</td><td style=\"text-align:right;vertical-align:bottom\">3.21%</td><td style=\"text-align:right;vertical-align:bottom\">0.90%</td><td style=\"text-align:right;vertical-align:bottom\">6.31%</td></tr><tr><td style=\"vertical-align:bottom\">4 Years</td><td style=\"text-align:right;vertical-align:bottom\">0.60%</td><td style=\"text-align:right;vertical-align:bottom\">4.25%</td><td style=\"text-align:right;vertical-align:bottom\">1.19%</td><td style=\"text-align:right;vertical-align:bottom\">8.32%</td></tr><tr><td style=\"vertical-align:bottom\">5 Years</td><td style=\"text-align:right;vertical-align:bottom\">0.75%</td><td style=\"text-align:right;vertical-align:bottom\">5.28%</td><td style=\"text-align:right;vertical-align:bottom\">1.49%</td><td style=\"text-align:right;vertical-align:bottom\">10.29%</td></tr><tr><td style=\"padding:0.2em 1px;text-align:center\" rowspan=\"6\">Europe</td><td style=\"vertical-align:bottom\">P(selected</td><td style=\"text-align:right;vertical-align:bottom\">0.39%</td><td style=\"text-align:right;vertical-align:bottom\">1.38%</td><td style=\"text-align:center;vertical-align:bottom\">-</td><td style=\"text-align:center;vertical-align:bottom\">-</td></tr><tr><td style=\"vertical-align:bottom\">1 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.39%</td><td style=\"text-align:right;vertical-align:bottom\">1.38%</td><td style=\"text-align:right;vertical-align:bottom\">0.78%</td><td style=\"text-align:right;vertical-align:bottom\">2.74%</td></tr><tr><td style=\"vertical-align:bottom\">2 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.78%</td><td style=\"text-align:right;vertical-align:bottom\">2.74%</td><td style=\"text-align:right;vertical-align:bottom\">1.55%</td><td style=\"text-align:right;vertical-align:bottom\">5.41%</td></tr><tr><td style=\"vertical-align:bottom\">3 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.17%</td><td style=\"text-align:right;vertical-align:bottom\">4.08%</td><td style=\"text-align:right;vertical-align:bottom\">2.32%</td><td style=\"text-align:right;vertical-align:bottom\">8.00%</td></tr><tr><td style=\"vertical-align:bottom\">4 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.55%</td><td style=\"text-align:right;vertical-align:bottom\">5.41%</td><td style=\"text-align:right;vertical-align:bottom\">3.08%</td><td style=\"text-align:right;vertical-align:bottom\">10.52%</td></tr><tr><td style=\"vertical-align:bottom\">5 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.93%</td><td style=\"text-align:right;vertical-align:bottom\">6.71%</td><td style=\"text-align:right;vertical-align:bottom\">3.83%</td><td style=\"text-align:right;vertical-align:bottom\">12.97%</td></tr><tr><td style=\"padding:0.2em 1px;text-align:center\" rowspan=\"6\">Latin America</td><td style=\"vertical-align:bottom\">P(selected</td><td style=\"text-align:right;vertical-align:bottom\">0.36%</td><td style=\"text-align:right;vertical-align:bottom\">1.74%</td><td style=\"text-align:center;vertical-align:bottom\">-</td><td style=\"text-align:center;vertical-align:bottom\">-</td></tr><tr><td style=\"vertical-align:bottom\">1 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.36%</td><td style=\"text-align:right;vertical-align:bottom\">1.74%</td><td style=\"text-align:right;vertical-align:bottom\">0.72%</td><td style=\"text-align:right;vertical-align:bottom\">3.45%</td></tr><tr><td style=\"vertical-align:bottom\">2 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.72%</td><td style=\"text-align:right;vertical-align:bottom\">3.45%</td><td style=\"text-align:right;vertical-align:bottom\">1.43%</td><td style=\"text-align:right;vertical-align:bottom\">6.78%</td></tr><tr><td style=\"vertical-align:bottom\">3 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.08%</td><td style=\"text-align:right;vertical-align:bottom\">5.13%</td><td style=\"text-align:right;vertical-align:bottom\">2.14%</td><td style=\"text-align:right;vertical-align:bottom\">10.00%</td></tr><tr><td style=\"vertical-align:bottom\">4 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.43%</td><td style=\"text-align:right;vertical-align:bottom\">6.78%</td><td style=\"text-align:right;vertical-align:bottom\">2.84%</td><td style=\"text-align:right;vertical-align:bottom\">13.10%</td></tr><tr><td style=\"vertical-align:bottom\">5 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.79%</td><td style=\"text-align:right;vertical-align:bottom\">8.40%</td><td style=\"text-align:right;vertical-align:bottom\">3.54%</td><td style=\"text-align:right;vertical-align:bottom\">16.10%</td></tr><tr><td style=\"padding:0.2em 1px;text-align:center\" rowspan=\"6\">North America</td><td style=\"vertical-align:bottom\">P(selected</td><td style=\"text-align:right;vertical-align:bottom\">0.28%</td><td style=\"text-align:right;vertical-align:bottom\">1.73%</td><td style=\"text-align:center;vertical-align:bottom\">-</td><td style=\"text-align:center;vertical-align:bottom\">-</td></tr><tr><td style=\"vertical-align:bottom\">1 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.28%</td><td style=\"text-align:right;vertical-align:bottom\">1.73%</td><td style=\"text-align:right;vertical-align:bottom\">0.56%</td><td style=\"text-align:right;vertical-align:bottom\">3.43%</td></tr><tr><td style=\"vertical-align:bottom\">2 Year</td><td style=\"text-align:right;vertical-align:bottom\">0.56%</td><td style=\"text-align:right;vertical-align:bottom\">3.43%</td><td style=\"text-align:right;vertical-align:bottom\">1.12%</td><td style=\"text-align:right;vertical-align:bottom\">6.74%</td></tr><tr><td style=\"vertical-align:bottom\">3 Years</td><td style=\"text-align:right;vertical-align:bottom\">0.84%</td><td style=\"text-align:right;vertical-align:bottom\">5.10%</td><td style=\"text-align:right;vertical-align:bottom\">1.67%</td><td style=\"text-align:right;vertical-align:bottom\">9.94%</td></tr><tr><td style=\"vertical-align:bottom\">4 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.12%</td><td style=\"text-align:right;vertical-align:bottom\">6.74%</td><td style=\"text-align:right;vertical-align:bottom\">2.22%</td><td style=\"text-align:right;vertical-align:bottom\">13.03%</td></tr><tr><td style=\"vertical-align:bottom\">5 Years</td><td style=\"text-align:right;vertical-align:bottom\">1.39%</td><td style=\"text-align:right;vertical-align:bottom\">8.36%</td><td style=\"text-align:right;vertical-align:bottom\">2.76%</td><td style=\"text-align:right;vertical-align:bottom\">16.01%</td></tr><tr><td style=\"padding:0.2em 1px;text-align:center\" rowspan=\"6\">Oceania</td><td style=\"vertical-align:bottom\">P(selected</td><td style=\"text-align:right;vertical-align:bottom\">2.93%</td><td style=\"text-align:right;vertical-align:bottom\">10.03%</td><td style=\"text-align:center;vertical-align:bottom\">-</td><td style=\"text-align:center;vertical-align:bottom\">-</td></tr><tr><td style=\"vertical-align:bottom\">1 Year</td><td style=\"text-align:right;vertical-align:bottom\">2.93%</td><td style=\"text-align:right;vertical-align:bottom\">10.03%</td><td style=\"text-align:right;vertical-align:bottom\">5.77%</td><td style=\"text-align:right;vertical-align:bottom\">19.05%</td></tr><tr><td style=\"vertical-align:bottom\">2 Year</td><td style=\"text-align:right;vertical-align:bottom\">5.77%</td><td style=\"text-align:right;vertical-align:bottom\">19.05%</td><td style=\"text-align:right;vertical-align:bottom\">11.21%</td><td style=\"text-align:right;vertical-align:bottom\">34.48%</td></tr><tr><td style=\"vertical-align:bottom\">3 Years</td><td style=\"text-align:right;vertical-align:bottom\">8.53%</td><td style=\"text-align:right;vertical-align:bottom\">27.17%</td><td style=\"text-align:right;vertical-align:bottom\">16.34%</td><td style=\"text-align:right;vertical-align:bottom\">46.96%</td></tr><tr><td style=\"vertical-align:bottom\">4 Years</td><td style=\"text-align:right;vertical-align:bottom\">11.21%</td><td style=\"text-align:right;vertical-align:bottom\">34.48%</td><td style=\"text-align:right;vertical-align:bottom\">21.17%</td><td style=\"text-align:right;vertical-align:bottom\">57.07%</td></tr><tr><td style=\"vertical-align:bottom\">5 Years</td><td style=\"text-align:right;vertical-align:bottom\">13.82%</td><td style=\"text-align:right;vertical-align:bottom\">41.05%</td><td style=\"text-align:right;vertical-align:bottom\">25.72%</td><td style=\"text-align:right;vertical-align:bottom\">65.25%</td></tr></tbody></table></figure><p><i>The </i><a href=\"https://docs.google.com/spreadsheets/d/1Awz6G_TDtSz5b_NoH70BUZWyqBBIlod0YDNTM7w1ERg/edit?usp=sharing\"><i>supporting spreadsheet is available here</i></a><i> if you'd like to check the working.</i></p><h2>What are your chances of getting a visa if selected?</h2><p>Note that the number of applicants selected is consistently higher than the cap available, presumably because some folks are found to be ineligible after the fact or fail to complete the process, and others might then be offered their place. I'm not sure if or how this happens - take this with a grain of salt - but I feel reasonably confident that if you are well prepared and actually eligible, P(Visa Granted|Selection) is pretty high, especially if you then apply for the visa very promptly.</p><p>Anecdotally, I heard from one person who won the lottery but left it too long to apply for the actual visa itself - their application for the visa was submitted late in the fiscal year, and their application was not processed in time.</p><p>Note that applicants are <strong>not notified</strong> if they are selected. You'll need to regularly check your application online. Someone I spoke to set reminders, which seems pretty sensible!</p><blockquote><p>Most people are notified in May, so I set a reminder to check if I won for June 1, and then also for late September (as sometimes they trickle out a few winners after May)</p></blockquote><h1><strong>The ethics of accepting a visa - think carefully, but not <u>too</u> scrupulously</strong></h1><p>Scrupulosity alert! If you tend to be over scrupulous, please downweight the points I make in this section by, like, 50%.</p><p>The Green Card lottery is massively oversubscribed, which means that your success at this zero-sum game comes at the expense of another person who, counterfactually, would get to live in the United States. Receiving a green card probably improves a fair bit the lives of recipients in poor or unsafe circumstances in their home country. Moreover, remittances from migrants under the Diversity Visa program can have serious positive effects on the wellbeing of family back home.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzijxa5djjlk\"><sup><a href=\"#fnzijxa5djjlk\">[8]</a></sup></span>&nbsp;I think that people applying to the diversity visa lottery should keep that in mind.</p><p>If your interest in moving to the United States is motivated by a reasonable belief that, on expectation, the world is better off as a result of you being able to live and work in the United States, then I reckon you should apply.</p><p>You might also consider how useful you expect the ability to live <strong>permanently</strong> in the United States to be for you. If you are quite confident that you would only work in the United States for a short period of time, it might be more appropriate to apply for a temporary visa so that another person can permanently migrate.</p><p>While it is a zero-sum decision to accept a place, I think you should generally feel comfortable about applying if either (or both) of the following would be true:</p><ul><li>you are doing or expect to do valuable EA work that would be made more effective by you having a green card, or if your doing that work is contingent on moving to a country like the United States; or</li><li>you expect to increase your earning potential by having a green card, and you would donate a fair proportion of those marginal earnings to an effective charity.</li></ul><p>I expect that it is the decision to apply for a visa once selected, not the decision to apply to the lottery, that is particularly zero sum. My understanding is that the United States aims for full uptake of the category and will usually select additional winners in order to do so.</p><p><i>On a personal note, I am sympathetic to folks who are reluctant to take a space from someone else - especially if you are motivated by, for example, social justice more than outright EV max(utility) calculations. I've applied (mostly to see what the process was) but would be in two minds about accepting the visa if successful as I already have two passports and work rights in the UK, EU, Australia and New Zealand, and think the chance that the United States is the best place for me personally to do EA work in the next decade is probably less than 50%. I might get into the habit of applying each year, but would probably think quite carefully before actually accepting an offer. That said, I've been encouraging friends from New Zealand who I think would be impactful in the US to apply - and would also <u>strongly</u> encourage EA folks from developing countries not to let excessive scrupulosity from putting them off applying.</i></p><h1><strong>Risks and consequences of applying/accepting</strong></h1><h2>Demonstrating nonimmigrant intent for future temporary visits</h2><p><i>Think about this risk before you apply to the lottery, but especially if you are selected and proceed to apply for a diversity lottery visa.</i></p><p>When you apply for a non-immigrant visa or the ESTA visa waiver, you can usually &nbsp;be turned down if you are judged to have \"immigrant intent\". I.e. the decision maker needs to believe that your visit to the US will be temporary and that you will return to your country in due course.</p><p>There is mixed anecdotal evidence as to whether applying to the Diversity Visa Lottery could demonstrate an intent to migrate to the United States, creating a risk of adverse treatment when applying for non-immigrant visas or the ESTA visa waiver (on the basis that the official making the decision on your subsequent application that you want to surreptitiously migrate to the US).&nbsp;</p><p>In the context of another kind of visa, the 'TN' visa that Canadians and Mexicans professionals can use to temporarily work in the United States, a US lawyer writes:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgjawztre8s4\"><sup><a href=\"#fngjawztre8s4\">[9]</a></sup></span></p><blockquote><p>Merely submitting an Electronic Diversity Visa Entry Form (E-DV Entry Form) should not be considered evidence of immigrant intent sufficient to warrant the denial of a subsequent TN application. Filing the entry form is simply the first step in the process. The diversity immigrant visa program\u2019s 50,000 permanent resident visas are drawn from random selection among all entries. There is no guarantee that an applicant is even going to be selected. Filing a diversity visa entry form is only a sign of intent to potentially immigrate in the future.</p></blockquote><p>But, as sensible lawyers are trained to do, they caveat that there is always some risk that the immigration or consular official takes a different view in practice:</p><blockquote><p><strong>Caveat: </strong>This does not mean that an immigration officer may not question you on the issue or potentially even deny you a TN on the basis of having filed for a diversity visa (assuming her or she has access to these filings with the Department of State). We can never assume that immigration officers will always properly interpret the law. The role of an immigration attorney is to ensure they rectify any misinterpretations when they occur.</p></blockquote><p>The MIT International Students Office writes:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0jpcblmrn6am\"><sup><a href=\"#fn0jpcblmrn6am\">[10]</a></sup></span></p><blockquote><p>International students can enter the Diversity Visa Lottery, and guidance in the past from the U.S. Department of State and the U.S. Department of Homeland Security have indicated that simply submitting an entry does not constitute an individual demonstrating \u201cimmigrant intent\u201d to the U.S. \u2014 which may make an individual no longer eligible to enter the U.S. in a non-immigrant status (including F-1 or J-1 student status, or as a B-1/B-2 tourist status, which are nonimmigrant visa types).</p><p><strong>However</strong>, if an individual is selected in the lottery to submit an immigrant visa application, if the individual moves forward and submits the immigrant application then they have demonstrated an intent to immigrate to the U.S. and would potentially impact their ability to apply for and/or obtain a nonimmigrant visa status.</p></blockquote><p>Anecdotally, I spoke with someone who travelled to the United States after being selected and applying, who was waiting for their visa application to be processed. That person reports successfully travelling to the United States, and told me:</p><blockquote><p>I was just super clear that I was here for specific non-immigrant reasons, was not looking for work here, and had maintained ties to [country of orgin] and had return flights etc booked</p></blockquote><p>Another point to note is that some non-immigrant visas allow for 'dual intent'. I haven't looked into these, but Nolo.com describes the concept as follows:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrudwt6za3xj\"><sup><a href=\"#fnrudwt6za3xj\">[11]</a></sup></span></p><blockquote><p>...a few types of nonimmigrant visas allow for dual intent. It's a not entirely logical concept, which allows foreign nationals in the U.S. to have two things in mind at once: nonimmigrant intent (\"I'm going to leave the U.S. when my visa term is up\") and immigrant intent (\"While I'm in the U.S., I'd also like to try to qualify for lawful permanent residence\").</p></blockquote><p>As with everything in this post, I'd welcome comments from any US lawyers - especially US immigration lawyers - who might read this.</p><p><strong>If you are in two minds because of this risk, </strong>a practical tip might be to set a calendar reminder now to check back on this post on 18 October, three weeks before, or 25 October, two weeks before the deadline to apply (November 8 2022 at 12.00PM ET). In the meantime, you can think about whether to apply and read up on the program. That way, if any US lawyers have commented on this risk, you'll be able to read their responses - plus I'll add a comment and/or edit this post to reflect anything else I learn about this risk in that time.</p><h2>Becoming subject to United States taxation on your <u>worldwide</u> income</h2><p><i>This is only a risk if you accept an offer and take up a green card - so it is safe to apply, and then consider your position later if you want.</i></p><p>If you become a United States permanent resident, you will need to file US tax returns as a Resident (IRS Form 1040 or 1040EZ) every year and declare income you earn worldwide - i.e. even income that you earn outside of the United States.</p><p>This might, but doesn't necessarily, mean you'll have to pay more tax. Double taxation rules will <i>usually </i>mean that you don't pay more overall than the maximum amount you would have paid in any one of the countries you are tax resident in.</p><p>For some people, this won't matter. If you plan to migrate permanently to the United States and if your home country doesn't impose worldwide taxation, you probably won't need to submit home country tax returns too. But there are many, many permutations of tax residence combinations and personal circumstances, and you shouldn't treat any generic advice about this as applying to your personal situation.</p><p>Folks with complex financial situations should think especially carefully about this.</p><p>At the very least, you'll have another chore to worry about every year - and possibly face a cost in paying a tax preparer to help you with your tax return. Because the intersection between US immigration and tax law is complicated, it would be advisable to review your tax obligations with a US tax&nbsp;attorney, Certified Public Accountant, or competent tax preparer.&nbsp;</p><h2>Needing to move to - and remain in - the United States</h2><p><i>This is only a risk if you accept an offer and take up a green card - so it is safe to apply, and then consider your position later if you want.</i></p><p>A green card does not automatically guarantee re-entry to the United States. When a green card holder travels abroad and returns, an immigration officer can question the holder to determine whether permanent resident status has been abandoned.&nbsp;</p><p>Trips abroad of six months or less seem fine from what I can tell from a google search. After six months, it seems like you can expect some additional scrutiny &nbsp;and may need to prove you have maintained ties to the US despite the extensive time spent abroad (e.g. employment, residential, property, financial, social, family ties.). After spending a year or more abroad, your green card will be considered abandoned. One way to avoid abandonment is to apply for a reentry permit before you leave the US for an extended period. It would allow you to stay abroad for more than a year at a time but never more than two years. If you are deemed to have abandoned your green card, you can apply for an SB-1 returning resident visa at the US consulate abroad, but these are difficult to obtain and require a showing that your extended absence from the US was due to circumstances beyond your control. Even if you are granted an SB-1 you would still need to re-apply for an immigrant visa at the consulate</p><p>Take advice from someone actually qualified on this if you take up US residence. But for the purposes of deciding whether to apply, bear in mind that this status requires <i>actual residence.</i></p><p>After a sufficient period of time in the country (five years usually, but it could be more if you spend a bunch of time overseas, and it could be three years if you're married to and living with a US citizen), you should be able to apply to become a United States citizen. If that citizenship application is successful, you're generally free to come and go as you please without putting your status at risk, but still have US tax obligations regardless of whether you live abroad.</p><h2>Other consequences</h2><p>There are probably heaps of other consequences that as someone who isn't a US lawyer, I'm not well placed to know or advise about. Lots of these depend on your individual circumstances, and some consequences might flow from your home country's law and not just US law.</p><p>As one easy example, it looks like you would currently be required to register with the 'Selective Service' if you are a cis male or AMAB person aged between 18 and 25 (plus there is some prospect that this expands later to include women, AFAB folks, and older folks). If your estimate of the chance of WWIII happening is way higher than the current forecasts (<a href=\"https://manifold.markets/LarsDoucet/will-world-war-3-have-begun-by-2025\">4% on Manifold Markets by 2025 </a>at the time of writing), then I suppose you might want to estimate P(conscription|WWIII) and factor that into account! Of course, if your timelines on WWIII are a lot further out, then war might involve less in person action and more remote warfare, so this might not be such a concern for you. The key point is that before you take on allegiance to an entirely new country and agree to follow its laws, you should give a bit of thought to whether there are any other risks or consequences particular to your situation.</p><h1><strong>Cost: a modest amount of time</strong></h1><p>There is <strong>no financial cost </strong>to enter the lottery. Put your credit card away.</p><p>There is a time cost, but it doesn't really take very long to enter - I timed myself and it took 7m23s. I think it probably also took a further 5 minutes to have someone help me take my photo, check it against the State Department requirements, and use their cropping tool to get it to the right size.</p><p>You should probably allow some further time to read the full <a href=\"https://travel.state.gov/content/dam/visas/Diversity-Visa/DV-Instructions-Translations/DV-2024-Instructions-Translations/DV-2024-Instructions.pdf\">Instructions/FAQ document</a> published by the State Department, seek advice if you think you need it, and consider whether entering makes sense given your own personal circumstances.</p><p><strong>Important! </strong>You should also factor in a few minutes to save your application number somewhere safe, to set reminders to check your application, and to then actually check your application twice or a few times (e.g. late May, mid June, early September). <strong>Seriously - it seems you don't get a copy of your number by email, and there doesn't seem to be a way to retrieve it later. This isn't a system designed with compassion!</strong></p><h1><strong>Possible next steps</strong></h1><ul><li>Putting a calendar reminder in place a week before, or a few days before, 8 November 2022 to remind you to apply.</li><li><a href=\"http://eepurl.com/iaTBkv\">Signing up for email reminders from me</a> to apply and to check your application. (I won't use your email address for any other purpose.)</li><li>Before you apply, arranging for someone to help you take a passport-quality digital photo that <a href=\"https://travel.state.gov/content/travel/en/us-visas/visa-information-resources/photos.html\">meets the State Department requirements</a> to upload with your lottery application. If that will be tricky, you might also consider getting a passport photo taken professionally.</li><li>Read the full <a href=\"https://travel.state.gov/content/dam/visas/Diversity-Visa/DV-Instructions-Translations/DV-2024-Instructions-Translations/DV-2024-Instructions.pdf\">Instructions/FAQ document</a> published by the State Department. At a minimum, please read this before applying.</li><li>Put a calendar reminder in place to check this post on 18 October, three weeks before, or 25 October, two weeks before the deadline to apply (November 8 2022 at 12.00PM ET), if you are in two minds about applying and want to see any updates from people who know more than me about this.</li><li>Speak with a US immigration lawyer if you are worried about the risks described above or any other risks of applying.</li><li>Consider telling promising EAs who might be able to increase their impact by working in the United States about this post, specifically telling them that this is time sensitive.</li><li><strong>For groups/organisers</strong> in relevant countries, consider popping something in your newsletter or telling engaged EAs in your group about the opportunity, noting that it is time sensitive.</li></ul><p>If you would like to speak to someone who has been through the process, one EA so far who has successfully applied for - and received - a diversity visa has agreed to speak with folks with questions. DM me to be put in touch, and also DM me (or comment below) if you've been successful and you're happy to talk with EAs thinking about entering.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwti0tj5vd5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwti0tj5vd5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For further explanation of what US authorities consider equivalent, see paragraph c. \"High School Education or Equivalent\" in the <strong>9 FAM 502.6-3 &nbsp;(U) DIVERSITY VISA ELIGIBILITY</strong> section of the State Department Foreign Affairs Manual at https://fam.state.gov/fam/09FAM/09FAM050206.html#M502_6_3</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0wvffw546c0h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0wvffw546c0h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See section d. \"Work Experience\" in the same document (https://fam.state.gov/fam/09FAM/09FAM050206.html#M502_6_3)<br><br>The Department of State suggests using the U.S. Department of Labor\u2019s O*Net Online database to determine qualifying work experience. https://www.onetonline.org</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4xba6s6ktus\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4xba6s6ktus\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://travel.state.gov/content/dam/visas/Diversity-Visa/DV-Instructions-Translations/DV-2024-Instructions-Translations/DV-2024-Instructions.pdf</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn18ncmnhndu4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref18ncmnhndu4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Immigration and Nationality Act, section 202(b).&nbsp;</p><p>https://www.uscis.gov/laws-and-policy/legislation/immigration-and-nationality-act</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnswoc12ysxf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnswoc12ysxf\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://fam.state.gov/fam/09FAM/09FAM050206.html#M502_6_3\">https://fam.state.gov/fam/09FAM/09FAM050206.html#M502_6_3</a></p><p>and</p><p>https://www.uscis.gov/laws-and-policy/legislation/immigration-and-nationality-act</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq3ustkk7qu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq3ustkk7qu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Some sources suggest that&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvmnuvkof4a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvmnuvkof4a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I'll take it on faith that these numbers are accurate.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzijxa5djjlk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzijxa5djjlk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mergo, T. (2016). <i>The Effects of International Migration on Migrant-Source Households: Evidence from Ethiopian Diversity-Visa Lottery Migrants. World Development, 84, 69\u201381.</i>doi:10.1016/j.worlddev.2016.04.</p><p>https://sci-hub.ru/10.1016/j.worlddev.2016.04.001</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngjawztre8s4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgjawztre8s4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://bdzlaw.com/nafta-tn-blog/2007/10/5/filing-for-diversity-visa-and-immigrant-intent.html</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0jpcblmrn6am\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0jpcblmrn6am\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://iso.mit.edu/knowledge-base/can-an-international-student-submit-an-entry-to-the-u-s-diversity-visa-program/</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrudwt6za3xj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrudwt6za3xj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.nolo.com/legal-encyclopedia/what-are-dual-intent-visas.html?fbclid=IwAR2HG8DmvyTL1VCMeu2DRUrib80Bba4fkETpoX8mRpuERY-EfHCgZ9ZVkGU</p></div></li></ol>", "user": {"username": "Tyrone Barugh"}}, {"_id": "GqGvCSjyTAxRcpa5G", "title": "Research Deprioritizing External Communication", "postedAt": "2022-10-06T12:20:00.126Z", "htmlBody": "<p><span>\n\n</span>\n\n<i>This was originally written in December 2021.  I think it's good\npractice to run posts by orgs before making them public, and I did in\nthis case.  This has some benefits: orgs aren't surprised, they can\nprepare a response in advance if they want to, they can point out\nerrors before things become public, etc, and I think it's generally\nworth doing.  In this case Leverage folks pointed out some errors,\nomissions, and bad phrasing in my post, which I've fixed, and I'm\nthankful for their help.  Pre-publication review does also have\ndownsides, however, and in this case as the email conversations grew\nto 10k+ words over three weeks I ran out of time and motivation.  </i>\n\n\n\n</p><p>\n\n<i>A month ago I came across this in my list of blog drafts and\ndecided to publish it as-is with a note at the top explaining the\nsituation.  This means that it doesn't cover any more recent Leverage\ndevelopments, including their <a href=\"https://www.leverageresearch.org/inquiry-report-april-2022\">Experiences\nInquiry Report</a> and <a href=\"https://www.leverageresearch.org/intention-research\">On\nIntention Research paper</a>, both published in April 2022.  I shared\nthis post again with Leverage, and while I've made edits in response\nto their feedback they continue to disagree with my conclusion.</i>\n\n</p>\n\n<p>\n\n<i> In the original pre-publication discussion with Geoff, one of the\ntopics was whether we could make our disagreement more concrete with a\nbet.  For example, research that launches a new subfield generally\ngets lots of citations, such as the the <a href=\"https://arxiv.org/abs/1606.06565\">Concrete Problems paper</a>\n(<a href=\"https://scholar.google.com/scholar?cites=6186600309471256628&amp;as_sdt=40000005&amp;sciodt=0,22&amp;hl=en\">1,644\ncitations</a> at 6 years), and if Leverage 1.0's research ends up\nhaving this kind of foundational impact this could be a clear way to\ntell.  When I gave Leverage a second pre-publication heads up, Geoff\nand I talked more and we were able to nail down some terms: if a\nLeverage paper drawing primarily on their pre-2019 research has 100+\ncitations from people who've never worked for Leverage by 2032-10-01,\nthen I'll donate $100 to a charity of Geoff's choosing; if not\nthen Geoff will do the same for a charity of my choosing.  I've\nlisted this on my <a href=\"https://www.jefftk.com/bets#leverage\">bets page</a>.  </i>\n\n</p>\n\n<p>\n\nIn 2011, several people I knew through Boston-area <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\">effective\naltruism</a> and <a href=\"https://en.wikipedia.org/wiki/LessWrong\">rationality</a> meetups\nstarted an organization called <a href=\"https://www.leverageresearch.org/\">Leverage Research</a>.  Their\n<a href=\"https://www.lesswrong.com/posts/969wcdD3weuCscvoJ/introducing-leverage-research\">initial\ngoal</a> was to \"make the world a much better place, using the most\neffective means we can\", and they worked on a wide range of projects,\nbut they're probably best known for trying to figure out how to make\npeople more productive/capable/successful by better understanding how\npeople think and interact. They initially lived and worked in a series\nof shared houses, first in Brooklyn and then in Oakland; I visited the\nlatter for an evening in early 2014.  The core project (\"Leverage\n1.0\") disintegrated in 2019, with some portions continuing, including\n<a href=\"https://www.trainparadigm.com/\">Paradigm</a>\n(training/coaching) and <a href=\"https://www.leverageresearch.org/\">Leverage 2.0</a> (early stage\nscience).  In this post I'm only looking at Leverage 1.0, and\nspecifically at their psychology research program.\n\n</p>\n\n<p>  In mid-December 2021, Leverage's former head of\noperations, Cathleen, wrote <a href=\"https://cathleensdiscoveries.com/LivingLifeWell/in-defense-of-attempting-hard-things\">In\nDefense of Attempting Hard Things: and my story of the Leverage\necosystem</a> (<a href=\"https://www.lesswrong.com/posts/aiCtrN9EF2FjKz5sv/in-defense-of-attempting-hard-things-and-my-story-of-the\">LW\ncomments</a>), giving a detailed history with extensive thoughts on\nmany aspects of the project.  I remember her positively from my\nshort 2014 visit, and I'm really glad she took the time to write this\nup.\n\n</p>\n\n<p>\n\nThere are many directions from which people could approach\nLeverage 1.0, but the one that I'm most interested in is lessons for\npeople considering attempting similar things in the future.\n\n</p>\n\n<p>\n\nMy overall read of Cathleen's post is that she (and many other\nex-Leverage folks) view the project as one where a group of people\ntook an unorthodox approach to research, making many deep and\nimportant discoveries about how people think and relate to each other.\nI've read the Connection Theory paper and the four research reports\nGeoff has published (see Appendix 3), however, and I don't see\nanything in them that backs up these claims about the originality and\ninsight of their psychology research.  While there are a range of\nreasons why people might not write up even novel and valuable\nresults, I think the most likely explanation is that there weren't\ndiscoveries on the level they're describing.\n\n</p>\n\n<p>\n\nGeoff is still gradually writing up Leverage 1.0-era results, so it's\npossible that something will come out later that really is impressive.\nWhile this isn't what I'm expecting, if it happens I'll need to\nretract most of what follows.  <i>[2022-09: this is essentially what\nGeoff and I bet on above.]</i>\n\n</p>\n\n<p>\n\nIf there weren't any big psychology research breakthroughs, however,\nwhy would they think there were?  Putting together my reading of\nCathleen's post (Appendix 1), Larissa's post (Appendix 2), and a few\nother sources (Appendix 3), here's what I see as the most likely\nstory: \"The core problem was that Leverage 1.0 quickly became much too\ninternally focused. After their Connection Theory research did not\nreceive the kind of positive response they were hoping for they\nstopped seeing publishing as a way to get good feedback. With an\nalways-on dynamic and minimal distinction between living and working\nspace during their formative years, their internal culture, practice,\nand body of shared knowledge diverged from mainstream society,\nacademia, and the communities they branched from.  They quickly got to\nwhere they felt people outside the group didn't have enough background\nto evaluate what they were doing.  Without enough deep external\nengagement, however, it was too hard for them to tell if their\ndiscoveries were actually novel or valuable.  They ended up putting\nlarge amounts of effort into research that was not just illegible, but\nnot very useful.  They gave up their best sources of external\ncalibration so they could move faster, but then, uncalibrated, put\nlots of effort into things that weren't valuable.\"\n\n</p>\n\n<p>\n\n<i>[2022-09: I'm not saying that the people researching psychology at\nLeverage were poor thinkers.  Instead my model is that when people are\noperating without good feedback loops they very often do work that\nisn't useful but believe that it is. This is part of why I <a href=\"https://www.jefftk.com/p/revisiting-why-global-poverty\">was\npessimistic</a> on circa-2015 AI safety work (and is still a reason\nI'm skeptical of a lot of AI safety work today) and worried about a\ndynamic of <a href=\"https://www.jefftk.com/p/more-meta-funding-thoughts\">inattentive funders\nfor meta-EA projects</a> (also still a problem).  Similarly, I think\nthe <a href=\"https://en.wikipedia.org/wiki/Replication_crisis\">replication\ncrisis</a> was primarily a problem of researchers thinking they had\nmeaningful feedback loops when they didn't.]</i>\n\n</p>\n\n<p>\n\nIn assessing a future research project I wouldn't take \"that looks a\nlot like Leverage\" as any sort of strong argument: Leverage 1.0 was a\nlarge effort over many years, encompassing many different\napproaches. Instead I would specifically look at its output and\napproach to external engagement: if they're not publishing research I\nwould take that as a strong negative signal for the project.\nLikewise, in participating in a research project I would want to\nensure that we were writing publicly and opening our work to engaged\nand critical feedback.\n\n</p>\n\n<p>\n<br />\n\nAppendix 1: some extracts from <a href=\"https://cathleensdiscoveries.com/LivingLifeWell/in-defense-of-attempting-hard-things\">Cathleen's\npost</a> that crystallized the above for me:\n\n</p>\n\n<ul>\n\n<li><p>\"[T]he pace of discovery and development and changes in the\nstructure and composition of the team was too fast to allow for people\nto actually keep up unless they were in the thick of it with us\"</p></li>\n\n<li><p>\"From the outside (and even sometimes from the inside) this\nwould look like unproductive delusion, but in fact it was intentional\nand managed theoretical exploration. And it led to an enormous amount\nof what many in the group came away believing were accurate and\ngroundbreaking theories of how the mind works and how a personality is\nshaped by life.\"</p></li>\n\n<li><p>\"For a small group of untrained people to independently\nderive/discover so much in a handful of years does, I think, indicate\nsomething quite unusual about Geoff's ability to design a productive\nresearch program.\"</p></li>\n\n<li><p>\"I think it's worth pausing to appreciate just how bad the\nconflict leading up to the dissolution, as well as the dissolution\nitself, was for a number of people who had been relying on the\nLeverage ecosystem for their life plans: their friends, their personal\ngrowth, their livelihood, their social acceptance, their romantic\nprospects, their reputations, their ability to positively impact the\nworld.\"</p></li>\n\n<li><p>The entire \"What to do when society is wrong about something?\"\nsection.</p></li>\n\n</ul>\n\n\n\n<p>\n\nAppendix 2: the same for <a href=\"https://forum.effectivealtruism.org/posts/hkAYutkaMkqg4ooo4/updates-from-leverage-research-history-mistakes-and-new\">Larissa's\npost</a>:\n\n</p>\n\n<p>\n\n</p>\n\n<ul>\n\n<li><p>\n\n\"From the outside, Leverage's research was understandably confusing\nbecause they were prioritising moving through a wide range of research\nareas as efficiently as possible rather than communicating the results\nto others. This approach was designed to allow them to cover more\nground with their research and narrow in quickly on areas that seemed\nthe most promising.\"\n\n</p></li>\n<li><p>\n\"Notably, Leverage's focus was never particularly on sharing research\nexternally. Sometimes this was because it was a quick exploration of a\nparticular avenue or seemed dangerous to share. Often though it was a\ntime trade-off. It takes time to communicate your research well, and\nthis is especially challenging when your research uses unusual\nmethodology or starting assumptions.\"\n\n</p></li>\n<li><p>\"[T]here was a trade-off in time spent conducting research versus time\nspent communicating it. As we didn't invest time early on in\ncommunicating about our work effectively, it only became harder over\ntime as we built up our models and ontologies.\"\n\n</p></li>\n<li><p>\"One of the additional adverse effects of our poor public communication\nis that when Leverage staff have interacted with people, they often\ndidn't understand our work and had a lot of questions and concerns\nabout it. While this was understandable, I think it sometimes led\nstaff to feel attacked which I suspect, in some cases, they handled\npoorly, becoming defensive and perhaps even withdrawing from engaging\nwith people in neighbouring communities. If you don't build up\nrelationships and discuss updates to your thinking inferential\ndistance builds up, and it becomes easy to see some distant, amorphous\norganisation rather than a collection of people.\"\n\n</p></li>\n</ul>\n\n\n\n<p>\n\nAppendix 3: earlier public discussion of Leverage 1.0, which I've also\ndrawn on in trying to understand what happened:\n\n</p>\n\n<p>\n\n</p>\n\n<ul>\n\n<li><p>January 2012: Geoff, Leverage's primary founder and Executive\nDirector, writes <a href=\"https://www.lesswrong.com/posts/969wcdD3weuCscvoJ/introducing-leverage-research\">Introducing\nLeverage Research</a>.  The post directs people to the website for\nmore information about their research, which has a link to download <a href=\"https://www.scribd.com/document/219774356/Evidence-for-Connection-Theory\">Connection\nTheory: the Current Evidence</a>. The\ncomments have a lot of skeptical discussion of Connection Theory, with\nback-and-forth from Geoff.\n\n</p></li>\n<li><p>September 2012: Peter writes <a href=\"https://www.lesswrong.com/posts/8j4zirwfhWhT8nwsc/a-critique-of-leverage-research-s-connection-theory\">A\nCritique of Leverage Research's Connection Theory</a>. His conclusion\nis that the evidence presented is pretty weak and that it's in\nconflict with a lot of what we do know about psychology.  The comments\nagain have good engagement. At some point before Alyssa's 2014 post (below)\nLeverage removed the Connection Theory paper from their site.\n[<i>2022-10: Cathleen tells me this was in the 2013 redesign of their\nsite, and linked me to <a href=\"https://archive.ph/PtPxX\">before</a>\nand <a href=\"https://archive.ph/afDr7\">after</a> captures.</i>]\n\n</p></li>\n<li><p>April 2014: Alyssa writes <a href=\"https://rationalconspiracy.com/2014/04/22/the-problem-with-connection-theory/\">The\nProblem With Connection Theory</a>, digging deeper into some of the\nclaims of the Connection Theory paper.  She argues that the paper\ngenerally oversells its evidence, and highlights that several\npredictions which one would not normally judge as correct are\ncounted as positive evidence. Jasen, a Leverage employee, responds in\nthe comments to say Alyssa is criticizing an obsolete document.\n\n</p></li>\n<li><p>January 2015: Evan <a href=\"https://forum.effectivealtruism.org/posts/B3fYvmwJLRiDeM6Cd/the-outside-critics-of-effective-altruism?commentId=J952DTpS6Qc8EvrRQ\">comments</a>\nwith his understanding of why Leverage hasn't shared much publicly,\nincluding that he thinks \"Leverage Research perceives it as difficult\nto portray their research at any given time in granular detail. That\nis, Leverage Research is so dynamic an organization at this point that\nfor it to maximally disclose the details of its current research would\nbe an exhaustive and constant effort.\"\n\n</p></li>\n<li><p><i>[2022-10: Cathleen tells me that in June 2016 she asked the\nInternet Archive to exclude Leverage so that people would focus on the\nnew content on their website.  Because the site <a href=\"https://web.archive.org/web/*/leverageresearch.org\">isn't\nincluded</a> in the Archive, I wasn't able to evaluate the historical\ncontent of their website in putting together this post or formulating\nmy hypothesis above.  She also linked me to a pair of <a href=\"https://archive.today\">archive.today</a> captures, one from <a href=\"https://archive.ph/afDr7\">2013</a> and another from <a href=\"https://archive.ph/RVhzU\">2018</a> showing blog posts published\nin 2016.  I haven't evaluated these captures.]</i>\n\n</p></li>\n<li><p>August 2018: Ryan writes <a href=\"https://forum.effectivealtruism.org/posts/qYbqX3jX4JnTtHA5f/leverage-research-reviewing-the-basic-facts\">Leverage\nResearch: reviewing the basic facts</a> anonymously as \"throwaway\",\nand then following up as \"anonymoose\" (both of which he's since <a href=\"https://www.lesswrong.com/posts/XPwEptSSFRCnfHqFk/zoe-curzi-s-experience-with-leverage-research?commentId=MvemjCE3P2YfLjEiz\">publicly\nconfirmed</a>).  His high-level point is that Leverage seemed to have\nproduced very little given the amount of time and money put into the\nproject.  Geoff <a href=\"https://forum.effectivealtruism.org/posts/qYbqX3jX4JnTtHA5f/leverage-research-reviewing-the-basic-facts?commentId=vpt7cupDr5gbswkuf\">replies</a>\nthat he had been planning to publish some of their results shortly.\n\n</p></li>\n<li><p>November 2019: Larissa, Leverage's incoming communications\nperson, posts <a href=\"https://forum.effectivealtruism.org/posts/hkAYutkaMkqg4ooo4/updates-from-leverage-research-history-mistakes-and-new\">Updates\nfrom Leverage Research: history, mistakes and new focus</a>, expanding\non <a href=\"https://forum.effectivealtruism.org/posts/qYbqX3jX4JnTtHA5f/leverage-research-reviewing-the-basic-facts?commentId=mmTFLynKtfpXNsWeX\">a\ncomment</a> she had made in September.  She discusses history,\ndissolution of the original project, and current plans.  I was\nespecially interested in her discussion of the causes and effects of\nLeverage's approach to external engagement.\n\n</p></li>\n<li><p>December 2020 through October 2021: Geoff links a series of\nfour \"Leverage 1.0 Research Reports\" on his <a href=\"https://www.geoffanders.com/\">personal site</a>, three on\nconsensus (<a href=\"https://docs.google.com/document/d/1nR1kIyanifMhPoiYMtNwFxZui6PJdXiYt2IdWsrQp-k/edit\">1</a>,\n<a href=\"https://docs.google.com/document/d/1Ca-HUwiH7Q-yRfcDUCc8uPUNtcbrqxlsG2u0XZNVjbM/edit#heading=h.yqoi0yqc61n0\">2</a>,\n<a href=\"https://docs.google.com/document/d/1TCOWf9jOzPd0h4tcD1emGsltDPgnHVWL6arUdcUlkcA/edit\">3</a>)\nand one on intelligence amplification (<a href=\"https://docs.google.com/document/d/1vwBhkKNvAJ8z4ogrqIX-AtfpV89Uu9-5MXw9AilDFfE/edit\">4</a>).\nI haven't seen any discussion of these.  I'm very glad he wrote them\nup and made them public, but I also don't see in them the kind of\nbreakthroughs I would expect from how Cathleen wrote about Leverage 1.0's work.\n\n</p></li>\n<li><p>September 2021: Someone anonymous notices that Geoff is\nfundraising, and posts <a href=\"https://www.lesswrong.com/posts/Kz9zMgWB5C27Pmdkh/common-knowledge-about-leverage-research-1-0\">Common\nknowledge about Leverage Research 1.0</a>. They argue that Leverage\nwas a harmful \"high demand group\".  Lots of different perspectives in\nthe comments.\n\n</p></li>\n<li><p>September 2021: Larissa posts <a href=\"https://www.lesswrong.com/posts/3GgoJ2nCj8PiD4FSi/updates-from-leverage-research-history-and-recent-progress\">Updates\nfrom Leverage Research: History and Recent Progress</a>.  In the\nsection on Leverage's Exploratory Psychology Program she discusses\ntheir plans to release psychological research tools over\nthe next few months.\n\n</p></li>\n<li><p>October 2021: An anonymous former Leverage employee writes\nabout <a href=\"https://www.lesswrong.com/posts/Kz9zMgWB5C27Pmdkh/common-knowledge-about-leverage-research-1-0?commentId=GLzJfWbsPzY229gun\">their\nexperience there</a>, and how it \"really mismatched the picture of\nLeverage described by\" the 'Common Knowledge' post.\n\n</p></li>\n<li><p>October 2021: Zoe, one of their former researchers, posted\nabout <a href=\"https://medium.com/@zoecurzi/my-experience-with-leverage-research-17e96a8e540b\">her\nexperience there</a>.  See the <a href=\"https://www.lesswrong.com/posts/XPwEptSSFRCnfHqFk/zoe-curzi-s-experience-with-leverage-research\">corresponding\nLessWrong post</a> for discussion.  Also see <a href=\"https://www.leverageresearch.org/executive-director-letter-nov-2021\">Geoff's\nresponse</a>.\n\n</p></li>\n<li><p>December 2021: Jonathan, another former researcher, posts <a href=\"https://www.facebook.com/jonathan.wallis.714/posts/1802447699947196\">Leverage\nResearch: Context, Analysis, and Takeaway</a>.  The \"Utopic Mania and\nClosed-offness\" section was the most interesting to me, but it is\nsufficiently metaphorical that I don't really understand it.\n\n</p></li>\n<li><p>December 2021: Cathleen, Leverage's former COO, wrote <a href=\"https://cathleensdiscoveries.com/LivingLifeWell/in-defense-of-attempting-hard-things\">In\nDefense of Attempting Hard Things: and my story of the Leverage\necosystem</a>.  This is the article that prompted my post, and I'm sad\nabout how little coverage and consideration than <a href=\"https://www.lesswrong.com/posts/aiCtrN9EF2FjKz5sv/in-defense-of-attempting-hard-things-and-my-story-of-the\">it\nreceived</a> compared to some of the less informative posts above.\n\n</p></li>\n</ul>\n\n  \n\n<p><i>Comment via: <a href=\"https://www.facebook.com/jefftk/posts/pfbid0kkR2cZBfoKQQVuyo5PVMVKy6x3Vfz3dUhJGitY1iM1jxQGJzFUkNGQZqUFHCQGTSl\">facebook</a></i></p>", "user": {"username": "Jeff_Kaufman"}}, {"_id": "LwRSnvnaFL9eE4yKz", "title": "Invisible impact loss (and why we can be too error-averse)", "postedAt": "2022-10-06T15:08:08.754Z", "htmlBody": "<p><strong>TL;DR</strong>: It\u2019s hard to really feel sad&nbsp;about impact that doesn\u2019t happen, but it\u2019s easy to feel sad about mistakes in existing work.</p><p>This is bad because we sometimes have a choice between making a good-but-small thing or a big-but-slightly-worse thing, and the mismatch in our emotional reactions to the downsides of the two options turns into a cognitive bias that harms our decision-making. This can make us too error-averse and err too much on the side of just not doing things at all.</p><p>For instance, if I\u2019m choosing between making five <i>im</i>perfect cookies or one perfect cookie, I might notice the imperfections more than the four missing cookies, whereas it\u2019s often better to just make more imperfect cookies.</p><p>In some cases, we <strong>should</strong>&nbsp;be quite wary of mistakes. I outline where I think we\u2019re too error-averse and where error-aversion is appropriate <a href=\"https://forum.effectivealtruism.org/posts/LwRSnvnaFL9eE4yKz/invisible-impact-loss-and-why-we-can-be-too-error-averse#When_this_reasoning_doesn_t_quite_apply___reasons_to_be_error_averse__and_some_suggestions_\">below</a>.</p><p><strong>Summary&nbsp;of my concrete suggestions:</strong></p><ol><li>To counteract fear of criticism, foster a culture of celebrating successes and exciting work.</li><li>To notice invisible impact loss, have the phrase \u201cinvisible impact loss\u201d rattling inside your head, and try to quantify decisions that are prone to such errors.</li><li>Fight perfectionism; lean into <a href=\"https://forum.effectivealtruism.org/posts/g8uvwsFmRg2gEmiy4/are-there-robustly-good-and-disputable-leadership-practices?commentId=CoyYf3qYjS6EHkDiK\"><u>agile project management</u></a>, start <a href=\"https://forum.effectivealtruism.org/posts/J2tmgAn8ne6Yhk27o/half-assing-it-with-everything-you-ve-got\"><u>half-assing</u></a>, and try to determine what your real goals are.</li></ol><figure class=\"image image_resized\" style=\"width:407.5px\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1668612255/mirroredImages/LwRSnvnaFL9eE4yKz/mttwunehvk56gbapuggo.jpg\"><figcaption>This highly technical diagram shows 5 slightly lumpy <a href=\"https://en.wikipedia.org/wiki/Jammie_Dodgers\"><u>jammie dodgers</u></a>&nbsp;(a type of British cookie) contrasted with a single less-lumpy jammie dodger. If I were baking cookies, I might naturally try to spend more effort trying to make a really nice cookie and end up with the result on the right. After all, it\u2019s easy to see the flaws in the cookies on the left, while the missing cookies on the right aren\u2019t there \u2014 they\u2019re invisible. But it\u2019s probably better to just make more cookies, even if they\u2019re lumpier, so that I can e.g. hand them out at the office. I\u2019d be a bit embarrassed, but ultimately there\u2019d be more value from the larger set of lumpier jammie dodgers.</figcaption></figure><h1>Preamble: What sparked this thought</h1><p>When I was on the Events Team at CEA, we had a conversation about whether to approximately <a href=\"https://forum.effectivealtruism.org/posts/g5dbu6yJJzW3m39un/the-best-ea-global-yet-and-other-updates#Outcomes\"><u>double EA Global capacity at the last minute</u></a>&nbsp;(a few weeks before the actual event) in order to admit more people who met the admissions bar. (This was the most recent conference for which <a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global?commentId=cRSPmzcyXWNhWGz46#comments\"><u>capacity was a problem</u></a>.)</p><p>I remember starting off skeptical about the idea. After all,</p><ul><li>There would be more logistics issues; lunch lines would be long, etc.</li><li>We\u2019d have to move to two venues, which is awkward. People would have to walk from one venue to another to get to a different meeting or session.</li><li>Doubling would put a lot of strain on the team, which would mean we\u2019d be worse at responding to people, communicating things like the schedule or important COVID information, catching awesome ways to improve the conference\u2026</li><li>And in general, the experience of the average attendee would probably be worse.</li></ul><p>But someone else on the team made me realize a big thing I wasn\u2019t fully tracking:</p><ul><li>About 500 more people who met the admissions bar would get to experience the conference. However much impact the default conference would produce \u2014 sparked collaborations, connections for years to come, inspirations for new projects, positive career changes \u2014 there would be about double that.</li></ul><p>Although the whole conversation was sparked by the idea that we had way more people we wanted to admit than slots for attendees, <strong>I wasn\u2019t internalizing the benefit we would miss out on if we </strong><i><strong>didn\u2019t</strong></i><strong>&nbsp;double.</strong> I couldn\u2019t viscerally feel the loss of impact from a smaller but near-perfect conference the way I could feel my aversion to the various issues I could imagine with the bigger conference.&nbsp;The impact loss was invisible to me.</p><p>I now actively try to notice invisible impact loss. I\u2019ve noticed discussions on the Forum that miss this, and worry that it\u2019s stunting work that could be extremely valuable, so I decided to write this post.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflmu2v59ebne\"><sup><a href=\"#fnlmu2v59ebne\">[1]</a></sup></span></p><p><i>Note: EA Global conferences have changed since then. You can see some discussion of this </i><a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global?commentId=cRSPmzcyXWNhWGz46#comments\"><i><u>here</u></i></a><i>. I should also note that the conference </i><a href=\"https://forum.effectivealtruism.org/posts/g5dbu6yJJzW3m39un/the-best-ea-global-yet-and-other-updates\"><i><u>didn\u2019t</u></i></a><i>&nbsp;end up facing many of the issues I worried about while we were deciding whether to double.</i></p><h1>What causes this phenomenon? How can we push back? (Suggestions)</h1><p>Here are some factors I think are at play, and ideas for what we can do about them. (Please add to this in the comments!) <i>See </i><a href=\"https://forum.effectivealtruism.org/posts/LwRSnvnaFL9eE4yKz/invisible-impact-loss-and-why-we-can-be-too-error-averse#Summary_of_my_concrete_suggestions_\"><i>the summary at the top</i></a><i>.</i></p><h2>1. Fear of criticism<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqgfjcwq5vq\"><sup><a href=\"#fnqgfjcwq5vq\">[2]</a></sup></span></h2><p>We\u2019re much, much more likely to be criticized for doing something imperfectly than for not doing something.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbctc9ynr6ws\"><sup><a href=\"#fnbctc9ynr6ws\">[3]</a></sup></span>&nbsp;(In part because it\u2019s unclear who to criticize for not doing something.) This makes people more averse to doing anything, as they\u2019re worried about the potential for criticism.</p><p>What can we do about fear of criticism?</p><p>It\u2019s great that we\u2019re a community that is <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\"><u>critical</u></a>, but I think <strong>we could celebrate successes and exciting work&nbsp;more than we currently do</strong>. This could give people an incentive to do stuff, as they might expect to be appreciated for that work. I also think self-celebration (bragging) should be normalized more.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjccvus8jxx\"><sup><a href=\"#fnjccvus8jxx\">[4]</a></sup></span></p><ul><li>Concrete examples: more threads like <a href=\"https://forum.effectivealtruism.org/posts/FSvq8f8PMiziaJgNc/celebrations-and-gratitude-thread\"><u>this one</u></a>, more generic <a href=\"https://forum.effectivealtruism.org/posts/YzyuaHwXurqqpYnZZ/if-you-like-a-post-tell-the-author\"><u>comments of appreciation</u></a>.</li><li>Relatedly, we can be more generous in our criticism (<a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming#Some_good_qualities_of_criticism__and_how_to_have_them_\"><u>relevant resourc</u>e</a>, and <a href=\"https://forum.effectivealtruism.org/posts/qBbyx8KyGhFe3Dy3C/supportive-scepticism\">a post on supportive scepticism</a>). I think people in effective altruism already are more generous than is usual on the internet, but I\u2019d guess that we can up our standards here.</li></ul><p>(And we can individually reframe our attitude towards criticism. We can actively train ourselves to be receptive to criticism, to take criticism and process it productively. The standard advice is to view every criticism as an opportunity to grow in the object-level (improve that type of work), but I also think we can grow on a meta level \u2014 view it as an opportunity to build our criticism-response muscles.)</p><h2>2. Invisible impact loss is hard to notice (it\u2019s \u201cinvisible\u201d!), but noticing <i>flaws</i>&nbsp;is easy</h2><p>My suggestions:</p><ol><li>When making decisions like whether to double a conference, just try hard to notice the invisible impact loss. Have the phrase \u201cinvisible impact loss\u201d in the back of your mind.</li><li>Fermi estimates can be a really good grounding: just try to quantify the impact of the possibilities for every significant decision. Then you can try to account for the cost of errors and imperfections but also notice the missing value from invisible impact loss.<ol><li>Note: <a href=\"https://forum.effectivealtruism.org/posts/rXYW9GPsmwZYu3doX/what-happens-on-the-average-day?commentId=5x5Wmr4Mz6N2K8Eyc\"><u>you don\u2019t need a stats degree for this</u></a>!</li><li>Two examples:<ol><li>For the conference, we could have done something really rough; most of the value is in the attendees\u2019 experiences, and most of the cost is their time. So let\u2019s just consider the net benefit from the conference to/via every attendee (without considering e.g. the cost of the additional venue). Then, as long as the net value is positive and the conference doesn\u2019t get half as valuable or less for every attendee as it would have been, doubling is worth it.</li><li>In the (unrealistic) jammie dodgers case, suppose that most of what I care about is how happy people are eating the jammie dodgers. If a lumpy jammie dodger gives people 10 units of happiness while a less-lumpy jammie dodger gives people 12 units of happiness, then I\u2019m comparing a total of 50 units of happiness (5 x 10) to 12 units of happiness (1 x 12).</li></ol></li><li>Resources for Fermi estimates: <a href=\"https://forum.effectivealtruism.org/posts/cpfgq84B8XHXPWLcM/introduction-to-fermi-estimates\"><u>intro to Fermi estimation</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/iJSYZJJrLMigJsBeK/lizka-s-shortform#cte7xmTMvhJckpzcW\"><u>notes from my workshop</u></a>&nbsp;on Fermi estimation, and the <a href=\"https://forum.effectivealtruism.org/topics/fermi-estimate\"><u>relevant Forum Wiki entry</u></a>.</li></ol></li></ol><h2>3. Perfectionism</h2><p>If you\u2019re anything like me, the idea of kicking off something with known flaws (or something that you know has flaws that you haven\u2019t identified yet) is aversive.</p><p>A few things help me:</p><ul><li>A <a href=\"https://forum.effectivealtruism.org/posts/g8uvwsFmRg2gEmiy4/are-there-robustly-good-and-disputable-leadership-practices?commentId=CoyYf3qYjS6EHkDiK\"><u>ship-fast-and-iterate mentality</u></a>&nbsp;that I\u2019m developing mostly by working with people who are better than I am at this. The rough idea is that you should deliver something useful <i>fast</i>, even if it has flaws, which allows you to get feedback and identify and fix more flaws or otherwise improve the product. I think this is heavily related to <a href=\"https://en.wikipedia.org/wiki/Agile_software_development#Agile_software_development_principles\"><u>\u201cagile\u201d approaches (e.g. to software development)</u></a>, although I don\u2019t know much about the theory, and <a href=\"https://en.wikipedia.org/wiki/Lean_software_development#Deliver_as_fast_as_possible\"><u>\u201clean\u201d principles</u></a>.</li><li><a href=\"https://forum.effectivealtruism.org/posts/J2tmgAn8ne6Yhk27o/half-assing-it-with-everything-you-ve-got\"><u>Half-assing it with everything you\u2019ve got</u></a>&nbsp;and the rest of the Replacing Guilt series.</li><li>Deadlines and accountability mechanisms. The best way for me to make sure that I actually post something that I\u2019ve written is to create a deadline for it.</li></ul><h1>When this reasoning doesn\u2019t quite apply \u2014 reasons to be error-averse&nbsp;(and some suggestions)</h1><p><i>You can find a </i><a href=\"https://forum.effectivealtruism.org/posts/aiNbxdzKC5fbi5mJF/why-we-should-err-in-both-directions#Failures\"><i><u>related discussion here</u></i></a><i>, and a </i><a href=\"https://forum.effectivealtruism.org/posts/otLCoYN3neacjBy48/max-dalton-and-jonas-vollmer-how-to-avoid-accidentally\"><i><u>talk on accidental negative impacts here</u></i></a><i>.</i></p><p>Here are some cases when, given a decision between \u201ca good project\u201d and \u201can OK project that\u2019s bigger or more ambitious,\u201d it might be better to err on the side of the better or more polished (and less ambitious) project (do less, but better):</p><ol><li>When the downside risks are high<ol><li>Or the other costs of a low-quality version of the project are high</li></ol></li><li>When you might prevent someone else from doing a better job (or when the <a href=\"https://forum.effectivealtruism.org/topics/unilateralist-s-curse\"><u>unilateralist\u2019s curse</u></a>&nbsp;applies)</li><li>When this type of project often has long tails of success (that depend on quality)</li><li>When a key goal of the project is your own development and training</li></ol><p>In general, I think that a broad way to mitigate this kind of risk is to <strong>get external feedback and seriously listen to it</strong>.</p><p>Note that sometimes more than one of these applies. Let\u2019s run through these cases one by one.</p><h3>(1) The downside risks are high</h3><p><strong>Example: </strong>Say you\u2019re baking cookies, and one of your ingredients is poisonous if not prepared correctly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefld3qko9k9x\"><sup><a href=\"#fnld3qko9k9x\">[5]</a></sup></span>&nbsp;Then a more efficient and less cautious method of preparation might not be appropriate \u2014 even if it doubles your output. The downside risks here are high! You might kill someone. Better make one safe cookie than 10 potentially lethal cookies.</p><p>Downside risks are sometimes high in EA projects.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefa9wx9tdutmm\"><sup><a href=\"#fna9wx9tdutmm\">[6]</a></sup></span>&nbsp;Some reasons the downside risks of a project might be high:</p><ul><li>There are <a href=\"https://forum.effectivealtruism.org/topics/information-hazard\"><u>information hazards</u></a>&nbsp;in this area</li><li>The area is inherently dangerous; there are other serious risks from this kind of work, like poisoning someone<ul><li>In EA, this might apply if your project is physically dangerous, if you\u2019re interacting with vulnerable people, like minors, or for some other reason.</li></ul></li><li>You might significantly harm the reputation of something that relies on its reputation to succeed<ul><li>E.g. if you\u2019re establishing a new kind of publication, and you publish a bunch of articles with basic grammar mistakes, the mistakes themselves may not matter much, but people might write the publication off as amateurish and unprofessional.</li></ul></li><li>The project might suck up a bunch of valuable resources<ul><li>E.g. you know that your project will be appealing to people, you\u2019re not sure it\u2019s that valuable, and it\u2019ll be costly either in time or money.</li><li>A note: I\u2019m not sure we should spend too much time worrying about this; I think we have OK systems for not dumping valuable resources into low-expected-impact projects, or at least generally shouldn\u2019t expect them to outcompete high-expected-impact projects. &nbsp;</li></ul></li><li>This is the first project of its kind, and you might lock in a bad norm or pattern (this is related to (2) \u2014 preventing someone else from doing a better job</li></ul><p><strong>Mitigation: </strong>check whether you\u2019re operating in a risky area, or if any of the above applies. Also, get feedback from others (although act carefully around information hazards). If you\u2019re <a href=\"https://www.lesswrong.com/tag/chesterton-s-fence\"><u>not sure why no one has done a thing yet</u></a>, take the possibility of risks or a <a href=\"https://forum.effectivealtruism.org/topics/unilateralist-s-curse\"><u>unilateralist\u2019s curse</u></a>&nbsp;seriously.</p><h3>(2) You might prevent someone else from doing a better job</h3><p><strong>Example: </strong>Say you\u2019ll be talking to 10 important state officials in a variety of offices. You\u2019re an expert on biosecurity and could focus on carefully explaining risks from pandemics to the two relevant officials, or you could prepare 10 pitches to the 10 officials about various issues relevant from an EA perspective, but you might do it poorly; you just don\u2019t understand global health or AI safety very well. You might want to focus on the biosecurity pitches in case you say something incorrect about global health or AI that makes the relevant officials dismiss the concern as that of an amateur.</p><p><strong>Another example: </strong>You\u2019re starting the first EA group in a given area, and you don\u2019t have much time (or expertise) to do it well. Someone else might start one soon, but won\u2019t do it if you\u2019ve already started it, and the group you start might be worse than their version.</p><p><strong>Mitigation: </strong>In this case, it might be worth spending more time understanding the counterfactual; if you don\u2019t do (a bigger version of) the project, will someone else? It might be better to coordinate.</p><h3>(3) This type of project often has long tails of success (that depend on quality)</h3><p><strong>Example: </strong>You\u2019re working on a book. Book success is long-tailed; most books don\u2019t do very well, and some become wildly popular. [Disclaimer: this is something I\u2019m pretty sure I remember reading about in a source I thought was trustworthy, but don\u2019t have a quick source for.] You could write two ~OK books or one great book.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefio3eituxwp\"><sup><a href=\"#fnio3eituxwp\">[7]</a></sup></span>&nbsp;You might want to just write the great book, as it\u2019s likely to get more than double the readers!</p><p><strong>Mitigation: </strong>seriously check that you have something that might be wildly successful,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbmku9wqkwos\"><sup><a href=\"#fnbmku9wqkwos\">[8]</a></sup></span>&nbsp;and if you notice that you do, go ahead with that (rather than spending energy on volume of projects).</p><h3>(4) When a key goal of the project is your own development or training</h3><p><strong>Example: </strong>Say you\u2019re working on one of your first research papers. Your basic goal is to get it published, and you think you can do that by half-assing. However, you also want to test your fit for research and learn great research practices. You won\u2019t be able to properly learn and test your fit if you half-ass. Then it might make sense to go all-in and really focus on trying to make something excellent, even if you think the impact from this one paper is not that big.</p><p><strong>Mitigation:</strong>&nbsp;Notice if you expect that most of the value of a given project is via your own development. In those cases, it might make sense to focus on making something excellent.</p><h1>Closing thoughts</h1><p>The summary of this post is <a href=\"https://forum.effectivealtruism.org/posts/LwRSnvnaFL9eE4yKz/invisible-impact-loss-and-why-we-can-be-too-error-averse#Summary_of_my_concrete_suggestions_\">at the top</a> \u2014 in very brief, notice invisible impact. Please argue with me and add your own examples in the comments!</p><p><strong>Related links (most of which were mentioned)</strong></p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/aiNbxdzKC5fbi5mJF/why-we-should-err-in-both-directions\"><u>Why we should err in both directions</u></a><ul><li>And related: <a href=\"https://forum.effectivealtruism.org/posts/fPu5eWJagwDvqxiGY/terminate-deliberation-based-on-resilience-not-certainty\"><u>Terminate deliberation based on resilience, not certainty</u></a>&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/J2tmgAn8ne6Yhk27o/half-assing-it-with-everything-you-ve-got\"><u>Half-assing it with everything you've got</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/FSvq8f8PMiziaJgNc/celebrations-and-gratitude-thread\"><u>Celebrations and gratitude thread</u></a>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/topics/accidental-harm\"><u>Accidental harm</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation\"><u>EA and the current funding situation</u></a></li></ul><p><i>Thanks to everyone who gave me feedback on drafts of this post!&nbsp;</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlmu2v59ebne\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflmu2v59ebne\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I\u2019m not the first person to talk about this, by a long shot. For instance, in <a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation#Risks_of_omission__squandering_the_opportunity_\"><u>a recent post</u></a>, William MacAskill writes:</p><blockquote><p>It seems to me to be more likely that we\u2019ll fail by not being ambitious enough; by failing to take advantage of the situation we\u2019re in, and simply not being able to use the resources we have for good ends.</p><p>It\u2019s hard to internalise, intuitively, the loss from failing to do good things; the loss of value if, say, EA continued at its current giving levels, even though it ought to have scaled up more. For global health and development, the loss is clear and visceral: every year, people suffer and lives are lost. It\u2019s harder to imagine for those concerned by existential risks. But one way to make the situation more vivid is to imagine you were in an \u201cend of the world\u201d movie with a clear and visible threat, like the incoming asteroid in <i>Don\u2019t Look Up</i>. How would you act? For sure, you\u2019d worry about doing the wrong thing. But the risk of failure by being unresponsive and simply not doing enough would probably weigh on you even harder.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqgfjcwq5vq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqgfjcwq5vq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation#Risks_of_omission__squandering_the_opportunity_\"><u>MacAskill</u></a>&nbsp;again:</p><blockquote><p>\u2026 there are asymmetric costs to trying to do big things versus being cautious. Compare: (i) How many times can you think of an organisation being criticised for not being effective enough? and (ii) How many times can you think of someone being criticised for <i>not</i>-founding an organisation that should have existed? (Or, suppose I hadn\u2019t given a talk on earning to give at MIT in 2012, would anyone be berating me?) In general, you get public criticism for doing things and making mistakes, not for failing to do anything at all.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbctc9ynr6ws\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbctc9ynr6ws\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Related: <a href=\"https://plato.stanford.edu/entries/doing-allowing/\"><u>Action/inaction distinction (\u201c</u><i><u>doing</u></i><u> vs </u><i><u>allowing</u></i><u> harm\u201d)</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjccvus8jxx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjccvus8jxx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cBragging\u201d feels very unnatural to me, but I do think it can be useful for the reasons I list.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnld3qko9k9x\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefld3qko9k9x\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I don\u2019t really know why you\u2019d make cookies with this, but let\u2019s go with the example anyway. Here\u2019s <a href=\"https://en.wikipedia.org/wiki/Fugu#Consumption\"><u>an example of such an ingredient</u></a>&nbsp;\u2014 although again, I\u2019m not really sure why you\u2019d make cookies with it.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fna9wx9tdutmm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefa9wx9tdutmm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>You can find <a href=\"https://forum.effectivealtruism.org/topics/accidental-harm\"><u>more discussion about this here</u></a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnio3eituxwp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefio3eituxwp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Although note that even with books, you might want to test some ideas first and see if you can hit on product-market fit. A source I\u2019ve been told is useful on this topic is <a href=\"https://writeusefulbooks.com/\"><u>Write Useful Books</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbmku9wqkwos\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbmku9wqkwos\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Related: <a href=\"https://forum.effectivealtruism.org/posts/bud2ssJLQ33pSemKH/my-current-impressions-on-career-choice-for-longtermists#How_to_choose_an_aptitude\"><u>How to choose an aptitude</u></a>&nbsp;by Holden Karnofsky</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "bCtK7WHfa9YL6cmMe", "title": "What are your experiences with ADHD symptoms and medication?", "postedAt": "2022-10-06T10:44:22.884Z", "htmlBody": "<p>Please write short answers that others can upvote by importance and agreevote if they match their own experiences.</p>", "user": {"username": "nathan"}}, {"_id": "ShaoxtLE6vEZ2CAMh", "title": "Our Boring Advice For Teens", "postedAt": "2022-10-07T11:13:45.202Z", "htmlBody": "<p>By Julia Wise, Damon Sasi and Chana Messinger</p><p>There's a lot of younger people around EA right now. A call with one led to a writing up of some boring \"you probably already know this but it's good to make it explicit\" advice, which the others of us fleshed out and added to.</p><p><strong>Full access is back.</strong></p><p>I, Chana, am experimenting with making this a linkpost to a google doc with full comment access. Please feel free to add feedback / thoughts / disagreements, with good epistemic norms (ideally nonanonymous, and putting whether you have thoughts from personal experience as a young person in a community like this, or working with them or neither).</p><p>(Experiment meaning comment access might get taken away, or doc might get put on private to reflect on how the experiment is going)</p>", "user": {"username": "ChanaMessinger"}}, {"_id": "QckSnyzgjogymrwe5", "title": "How can we really trust someone? ", "postedAt": "2022-10-06T06:49:03.855Z", "htmlBody": "", "user": {"username": "Ross McMath"}}, {"_id": "8tosr8XzqvYXR9ckB", "title": "Warning Shots Probably Wouldn't Change The Picture Much", "postedAt": "2022-10-06T05:15:52.908Z", "htmlBody": "<p>One piece of advice I gave to EAs of various stripes in early 2021 was: do everything you can to make the government sane around biorisk, in the wake of the COVID pandemic, because this is a practice-run for AI.</p><p>I said things like: if you can't get the world to coordinate on banning gain-of-function research, in the wake of a trillions-of-dollars tens-of-millions-of-lives pandemic \"warning shot\", then you're not going to get coordination in the much harder case of AI research.</p><p>Biolabs are often publicly funded (rather than industry-funded). The economic forces arrayed behind this <a href=\"https://www.vox.com/future-perfect/2019/3/20/18260669/deadly-pathogens-escape-lab-smallpox-bird-flu\">recklessly</a> <a href=\"https://twitter.com/davidmanheim/status/1576583761726152704\">foolish</a> and <a href=\"https://www.vox.com/2020/5/1/21243148/why-some-labs-work-on-making-viruses-deadlier-and-why-they-should-stop\">impotent</a> research consists of \u201chalf-a-dozen researchers thinking it\u2019s cool and might be helpful\u201d. (While the work that would actually be helpful\u2014such as removing needless bureaucracy around vaccines and investing in vaccine infrastructure\u2014languishes.) Compared to the problem of AI\u2014where the economic forces arrayed in favor of \u201cignore safety and rush ahead\u201d are enormous and the argument for expecting catastrophe much murkier and more abstract\u2014the problem of getting a sane civilizational response to pandemics (in the wake of a literal pandemic!) is ridiculously easier.</p><p>And\u2014despite valiant effort!\u2014we've been able to do approximately nothing.</p><p>We're not anywhere near global bans on gain-of-function research (or equivalent but better feats of coordination that the people who actually know what they're talking about when it comes to biorisk would tell you are better targets than gain-of-function research).</p><p>The government <a href=\"https://twitter.com/WilliamAEden/status/1576387289336336386\">continues to fund research that is actively making things worse</a>, while failing to put any serious funding towards the stuff that might actually help.</p><p>I think this sort of evidence has updated a variety of people towards my position. I think that a variety of others have not updated. As I understand the counter-arguments (from a few different conversations), there are two main reasons that people see this evidence and continue to hold out hope for sane government response:</p><p>&nbsp;</p><p><strong>1. Perhaps the sorts of government interventions needed to make AI go well are not all that large, and not that precise.</strong></p><p>I confess I don't really understand this view. Perhaps the idea is that AI is likely to go well by default, and all the government needs to do is, like, <i>not</i> use anti-trust law to break up some corporation that's doing a really good job at AI alignment just before they succeed? Or perhaps the idea is that AI is likely to go well so long as it's not produced first by an authoritarian regime, and working against authoritarian regimes is something governments are in fact good at?</p><p>I'm not sure. I doubt I can pass the <a href=\"https://www.econlib.org/archives/2011/06/the_ideological.html\">ideological Turing test</a> of someone who believes this.</p><p>&nbsp;</p><p><strong>2. Perhaps the ability to cause governance to be sane on some issue is tied very directly to the seniority of the government officials advising sanity.</strong></p><p>EAs only started trying to affect pandemic policy a few years ago, and aren't very old or recognized among the cacophony of advisors. But if another pandemic hit in 20 years, the sane EA-ish advisors would be much more senior, and a lot more would get done. Similarly, if AI hits in 20 years, sane EA-ish advisors will be much more senior by then. The observation that the government has not responded sanely to pandemic near-misses, is potentially screened-off by the inexperience of EAs advising governance.</p><p>I have some sympathy for the second view, although I'm skeptical that sane advisors have significant real impact. I'd love a way to test it as decisively as we've tested the \"government (in its current form) responds appropriately to warning shots\" hypotheses.</p><p>On my own models, the \"don't worry, people will wake up as the cliff-edge comes more clearly into view\" hypothesis has quite a lot of work to do. In particular, I don't think it's a very defensible position in isolation anymore. The claim \"we never needed government support anyway\" is defensible; but if you want to argue that we <i>do</i> need government support but (fortunately) governments will start behaving more reasonably after a warning shot, it seems to me like these days you have to pair that with an argument about why you expect the voices of reason to be so much louder and more effectual in 2041 than they were in 2021.</p><p>(Which is then subject to a bunch of the usual skepticism that applies to arguments of the form \"surely my political party will become popular, claim power, and implement policies I like\".)</p><p>&nbsp;</p><p>See also: <a href=\"https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence\">the law of continued failure</a>, and <a href=\"https://forum.effectivealtruism.org/posts/jMkXJs9ojSFYchr7a/?commentId=2irKbpB4Rvsmeap5F\">Rob Bensinger's thoughts on the topic</a>.</p>", "user": {"username": "So8res"}}, {"_id": "QPfzuGYtsC6NTuhmc", "title": "AI Timelines via Cumulative Optimization Power: Less Long, More Short", "postedAt": "2022-10-06T07:06:29.249Z", "htmlBody": "", "user": {"username": "Jake Cannell"}}, {"_id": "FdAfhdsSGKxP6axZY", "title": "The probability that Artificial General Intelligence will be developed by 2043 is extremely low.\n", "postedAt": "2022-10-06T11:26:05.263Z", "htmlBody": "<h2>Summary</h2><p>The many successes of Deep Learning over the past ten years have catapulted Artificial Intelligence into the public limelight in an unprecedented way. Nevertheless, both critics and advocates have expressed the opinion that deep learning alone is not sufficient for real human-like intelligence, which is filled with capabilities that require symbolic manipulation. The disagreement has now shifted to the way symbol manipulation should be construed. Critics of DL believe that <i>classical</i> symbolic architectures (with combinatorial syntax and semantics) should be imposed on top of deep learning systems while advocates believe in a new kind of <i>emergent </i>symbolic system which can be learned through gradient descent, like any other function. This is necessary, they argue, because classical systems have already proven to be intractable and therefore cannot be used to construct systems capable of human-like intelligence. In this essay I consider these two options and show that the DL advocates are mistaken, by considering the way in which deep learning systems solve the problem of generating software code to a natural language prompt. Programming languages like Python are <i>classical symbol systems</i>, yet deep learning models have become remarkably good at producing syntactically correct code. What this shows is that it is possible to mimic <strong>some aspects</strong> of a classical symbolic system with a neural network, and without an explicit<i> classical sub-system</i>. There is no reason to think this conclusion does not apply to other tasks where deep learning models perform well, like natural language tasks. In other words, if you looked at a neural network generating Python code, you might be tempted to conclude that Python is not a classical symbolic system. You would of course be wrong. Similalrly if you looked at a neural network generating natural language, you might be tempted to conclude that natural language is not a classical symbolic system. You might also be wrong: there is no compelling reason offered by DL networks that shows that something like a<i> non-classical</i> symbolic reasoning system is required for human cognition including language. <strong>DL systems can learn statistical mappings where a classical symbolic system produces lots of examples, like language or Python. When the symbol system is used for planning, creativity, etc., this is where DL struggles to learn.</strong> This leaves us to conclude that (a) deep learning alone won't lead to AGI, (b) nor will deep learning supplemented with non-classical symbolic systems, (c) nor, apparently will deep learning supplemented with classical symbolic systems. So, no AGI by 2043. Certainly not AI that includes \"entirely AI-run companies, with AI managers and AI workers and everything being done by AIs.\" &nbsp;I suggest that instead of bringing us AGI, modern deep learning has instead revitalized Licklider's vision of \"Man-Computer Symbiosis\", which is the most exciting prospect for the immediate future of AI.</p><h2>The AI Promise</h2><p><br>Artificial Intelligence has certainly had its ups and downs, from the heady days of \"good old-fashioned AI\" through the winter downturns, and now back on track to supposedly reach human level cognitive abilities in the not-too-distant future. The Future Fund has <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize#YEqEe7ZNsGGTejFBQ\">estimated </a>that there is a 20% chance that Artificial General Intelligence (AGI) will be developed by January 1, 2043, and a 60% chance by January 1, 2100 (in their \"Future Fund worldview prize\"). I am going to argue that the optimism is unwarranted, and both of these estimates are wildly inaccurate. In fact the probabilities should be closer to zero.</p><p>Why am I so certain? In order to understand my conviction, we have to consider the source of the overwhelming confidence that AI researchers currently possess. The primary reason can relatively straightforwardly be attributed to the sudden breakthroughs achieved by the <i>neural network</i> or <i>connectionist</i> paradigm, which has an intuitive advantage over other paradigms because of its apparent similarity to real neural networks in the brain. This alternative to \"good old fashioned\" logic-based symbol manipulating AI systems has existed since the 1940's, but has had a patchy history of success, with a cycle of \"hitting brick walls\" and then overcoming <a href=\"https://www.noemamag.com/what-ai-can-tell-us-about-intelligence/\">them</a>. That is, until around 2012 when a combination of widely available compute power, massive public datasets, and advancements in architectures enabled Deep Learning (DL) models to suddenly leapfrog existing state-of-the-art systems. The age of symbolic AI was quickly declared dead by some, and a view emerged that the key breakthroughs were essentially complete, and we just have to \"scale up\" now. For example, Tesla CEO Elon Musk and Nvidia CEO Jen-Hsun Huang declared in 2015 that the problem of building fully autonomous vehicles was essentially <a href=\"https://fortune.com/2015/03/18/tesla-elon-musk-and-nvidia-ceoself-driving-cars/\">solved</a>.</p><h2>Kinds of Symbol Systems</h2><p>The key difference between a classical symbol manipulating system and a distributed neural \"connectionist\" system was clearly laid out by <a href=\"http://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf\">Fodor and Pylyshyn</a> in 1988. They argued that the issue was not whether or not the systems were <i>representational</i>, or whether distributed systems can represent discrete concepts, but how the rules of the system are defined. That is, in what way do representations take part in the causal processes that transform them into other representations. Consider a simple example from propositional logic. The rule <strong>A &amp; B -&gt; A</strong> is a tautology because there is no assignment of truth values to <strong>A</strong> and <strong>B</strong> which will make it false. The implication can only be false if the precedent on the left-hand side is true but the consequent on the right-hand side is false. But this is not possible because if <strong>A</strong> on the right-hand side is false, it must also be false on the left-hand side, causing the conjunction to also be false. Importantly, this reasoning is only possible because the <strong>A</strong> on the right hand side <strong>is the same symbol</strong> as the <strong>A</strong> on the left-hand side. Classical systems have <i>combinatorial syntax and semantics</i>. I tried to see if GPT-3 has any conception about such combinatorial semantics with an example: \"Is it true that if the moon is made of green cheese, and cows chew cud, then the moon is made of green cheese?\", and GPT-3 answers \"No, it is not true that if the moon is made of green cheese, and cows chew cud, then the moon is made of green cheese.\", which is not correct. GPT-3 appears to focus on whether or not the consequent itself is true or false. &nbsp;</p><p>Perhaps the most vocal contemporary advocate for classical symbolic systems is Gary Marcus who argues that, in order to make further progress, AI needs to combine symbolic and DL solutions into <a href=\"https://nautil.us/deep-learning-is-hitting-a-wall-238440/\"><i>hybrid systems</i></a>. As a rebuttal to <a href=\"https://www.noemamag.com/what-ai-can-tell-us-about-intelligence/\">Marcus, Jacob Browning and Yann LeCun argue</a> that there is no need for such hybrids because symbolic representations can \"emerge\" from neural networks. They argue that \"the neural network approach has traditionally held that we don\u2019t need to hand-craft symbolic reasoning but can instead learn it: Training a machine on examples of symbols engaging in the right kinds of reasoning will allow it to be learned as a matter of abstract pattern completion. In short, the machine can learn to manipulate symbols in the world, despite not having hand-crafted symbols and symbolic manipulation rules built in.\" That is, symbols can be manipulated in the absence of specific rules in the classical sense. However, after making a strong case for this alternative kind of symbol manipulation, they then argue that it is not central to human cognition after all \"... most of our complex cognitive capacities do not turn on symbolic manipulation; they make do, instead, with simulating various scenarios and predicting the best outcomes.\" They further clarify that, to the extent that symbol manipulation is important at all, it is primarily a \"cultural invention\" and \"regards symbols as inventions we used to coordinate joint activities \u2014 things like words, but also maps, iconic depictions, rituals and even social roles.\" \"The goal, for DL, isn\u2019t symbol manipulation inside the machine, but the right kind of symbol-using behaviors emerging from the system in the world.\" Browning and LeCun argue that the critical insight from DL is to outlaw classical symbol manipulation as a genuine, generative process in the human mind, and that therefore hybrid systems have no place in a cognitive agent. &nbsp;</p><h2>Language Models and Computer Code</h2><p>While Browning and LeCun's argument may have a certain (though vague) appeal, it proves to be extraordinarily problematical in explaining the ever-increasing success that large neural <i>language models</i> are showing with generating computer code. While the language models were originally conceived for modeling natural language, it was discovered that if the training included some computer code from sources such as GitHub, then they could generate computer code from natural language specifications, sometimes at a<a href=\"https://www.deepmind.com/blog/competitive-programming-with-alphacode\"> level nigher than humans</a>. Language Models (LMs) in fact developed independently of neural models and are simply <a href=\"https://nlp.stanford.edu/IR-book/\">joint probability distributions</a> over sequences of words. Large Neural LMs are a subsequent advancement in that they learn probability distributions for sequences of real valued, continuous vector representations of words rather than discrete lexical items. The probability distribution is learned through a form of <i>language modeling</i>, where the task is to \"predict the next word given the previous words\" in word strings drawn from a corpus. Essentially LMs learn complex statistical properties of language and can perform at exceptional levels on a large number of tasks involving language including translation, inference, and even story telling. The models are very large indeed, with billions or more parameters. For example, Nvidia is proposing Megatron, a parallel architecture that can scale to 1 <a href=\"https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/\">trillion parameters</a>.</p><p>It turns out that LMs are competent learners of complex statistical distributions outside natural language. As previously mentioned, LMs that have received training on software code have become competent at generating syntactically well formed, functional code with relatively advanced programming problems. While there is a long way to go before they can write entire program implementations, it is clear that they already excel at generating syntactically well-formed productions. They almost never write code that contains syntax errors. But this is a real problem for the claim that neural architectures present an alternative model of symbol manipulation because well-formed software code is defined by classically understood symbolic, rule-based grammars. We must really try to understand how it is that a distributed neural network with an alternative method of manipulating symbols can perform so well with a straightforwardly classical symbolic task.</p><p>In fact, high-level programming languages for digital computers, and theories of natural language have a curious historical connection. John W. Backus who led the Applied Science Division of IBM's Programming Research Group <a href=\"https://betanews.com/2007/03/20/john-w-backus-1924-2007/\">took inspiration</a> from Noam Chomsky's work on phrase structure grammars and conceived a <i>meta-language</i> that could specify the syntax of computer languages that were easier for programmers to write than assembler languages. The meta language later became known as Backus-Naur form (BNF), so called partly because it was originally co-developed by Peter Naur in a 1963 IBM <a href=\"https://www.masswerk.at/algol60/report.htm\">report </a>on the ALGOL 60 programming language\". The BNF is a notation for context free grammars consisting of <i>productions</i> over <i>termina</i> and <i>nonterminal</i> symbols, which defines the grammar of programming languages required for writing<a href=\"https://www.goodreads.com/book/show/703102.Compilers\"> compilers and interpreters</a>.&nbsp;</p><p>BNF grammars can be invaluable for computer programmers. When a programmer is uncertain about the form of a programming construct, they can consult documentation which specifies the allowable syntax of expressions. The most complete reference is the syntax specification typically written in some form of BNF. For example, the syntax for the \"if\" statement in Python can be found in the <a href=\"https://docs.python.org/3/reference/expressions.html#grammar-token-python-grammar-assignment_expression\">reference guide</a> as shown below:</p><p><br>if_stmt ::= &nbsp;\"if\" assignment_expression \":\" suite<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (\"elif\" assignment_expression \":\" suite)*<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [\"else\" \":\" suite]<br>assignment_expression ::= &nbsp;[identifier \":=\"] expression<br>expression &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ::= &nbsp;conditional_expression | lambda_expr<br>conditional_expression ::= &nbsp;or_test [\"if\" or_test \"else\" expression]<br>or_test &nbsp;::= &nbsp;and_test | or_test \"or\" and_test<br>and_test ::= &nbsp;not_test | and_test \"and\" not_test<br>not_test ::= &nbsp;comparison | \"not\" not_test<br>comparison &nbsp; &nbsp;::= &nbsp;or_expr (comp_operator or_expr)*<br>comp_operator ::= &nbsp;\"&lt;\" | \"&gt;\" | \"==\" | \"&gt;=\" | \"&lt;=\" | \"!=\"<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | \"is\" [\"not\"] | [\"not\"] \"in\"<br>identifier &nbsp; ::= &nbsp;xid_start xid_continue*<br>id_start &nbsp; &nbsp; ::= &nbsp;&lt;all characters in general categories Lu, Ll, Lt, Lm, Lo, Nl, the underscore, and characters with the Other_ID_Start property&gt;<br>id_continue &nbsp;::= &nbsp;&lt;all characters in id_start, plus characters in the categories Mn, Mc, Nd, Pc and others with the Other_ID_Continue property&gt;<br>xid_start &nbsp; &nbsp;::= &nbsp;&lt;all characters in id_start whose NFKC normalization is in \"id_start xid_continue*\"&gt;<br>xid_continue ::= &nbsp;&lt;all characters in id_continue whose NFKC normalization is in \"id_continue*\"&gt;<br>suite &nbsp; &nbsp; &nbsp; &nbsp; ::= &nbsp;stmt_list NEWLINE | NEWLINE INDENT statement+ DEDENT<br>statement &nbsp; &nbsp; ::= &nbsp;stmt_list NEWLINE | compound_stmt<br>stmt_list &nbsp; &nbsp; ::= &nbsp;simple_stmt (\";\" simple_stmt)* [\";\"]<br>simple_stmt ::= &nbsp;expression_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | assert_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | assignment_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | augmented_assignment_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | annotated_assignment_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | pass_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | del_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | return_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | yield_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | raise_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | break_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | continue_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | import_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | future_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | global_stmt<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | nonlocal_stmt<br>The Unicode category codes mentioned above stand for:<br>&nbsp; &nbsp;Lu - uppercase letters, Ll - lowercase letters, Lt - titlecase letters, Lm - modifier letters, Lo - other letters, Nl - letter numbers, Mn - nonspacing marks, Mc - spacing combining marks, Nd - decimal numbers, Pc - connector punctuations, Other_ID_Start - explicit list of characters in PropList.txt to support backwards compatibility, Other_ID_Continue - likewise</p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p>It is, however, not normally necessary to consult the reference, as it is generally sufficient to simply provide an abstract template for legal <a href=\"https://pythonexamples.org/python-if-else-example/\">expressions</a>:</p><p>if boolean_expression:<br>&nbsp; &nbsp;statement(s)<br>else:<br>&nbsp; &nbsp;statement(s)</p><p><br>or even a typical example as in the Python <a href=\"https://docs.python.org/3/tutorial/controlflow.html#if-statements\">tutorial</a>.</p><p>&gt;&gt;&gt; if x &lt; 0:<br>... &nbsp; &nbsp; x = 0<br>... &nbsp; &nbsp; print('Negative changed to zero')<br>... elif x == 0:<br>... &nbsp; &nbsp; print('Zero')<br>... elif x == 1:<br>... &nbsp; &nbsp; print('Single')<br>... else:<br>... &nbsp; &nbsp; print('More') &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp;</p><p>However, there are cases where the less formal documentation is not sufficient. For example, notice the definition for the \"if_stmt\" in the BNF, which includes an \"assignment_expression\" following the \"if\". In turn, \"assignment_expression\" is defined with the &nbsp;\"[identifier \":=\"] expression\". The \"expresion\" part can be idenitified in the less formal documentation, corresponding to the \"boolean_expression\" and \"x &lt; 0\" in the other two definitions. However, the optional \"[identifier \":=\"]\" does not appear in these other definitions. The construct is in fact the \"assignment expression\" introduced in PEP 572, dated 28-Feb-2018 for Python 3.8. The assignment expression can be used to simplify code in some cases. For example by assigning the value of len(a) to the variable n, len(a) only needs to be calculated once in the following code fragment.</p><p>if (n := len(a)) &gt; 10:<br>&nbsp; &nbsp;print(f\"List is too long ({n} elements, expected &lt;= 10)\")</p><p><br>\"If\" statements of this form will not be found in code written prior to February 2018 and any LM trained on a corpus containing a majority of code written before that date will not be able to generate such statements and will have no information that the statement is legal. A human programmer, on the other hand, can simply consult the BNF and see that it is a legal production. Perhaps a powerful LM could learn about these statements after a few exposures through <i>few shot</i> learning, but this has its own difficulties. For example, consider if the new code included code from students who may have made a (very legitimate) mistake in using the wrong assignment operator, as in the modified code below:</p><p>if (n = len(a)) &gt; 10:<br>&nbsp; &nbsp;print(f\"List is too long ({n} elements, expected &lt;= 10)\")</p><p>Consulting the BNF instantly shows that this is not a well-formed statement, but a machine learning model that does not have access to the BNF cannot make this determination. <strong>The power to generalize with few or even no examples, while constraining the generalization to only legal productions is the power of classical symbolic systems that non symbolic systems cannot replicate. </strong>This is the power that the human mind appears to possess in abundance.</p><h2>Eliminative connectionism eliminates connectionism</h2><p><br>We must then consider how a LM can generate code which conforms to the syntax of a phrase structure language. One possibility is that the LM learns representations and operations that are isomorphic to the symbols and rules of the programming language and uses these representations to generate well-formed lines of code. This possibility is described by Pinker and Prince as <i>implementational connectionism,</i> since in this case the network acts as a physical implementation of the algorithm, as described by Marr. This possibility is stronger than Browning and LeCun's claim that the network learns a non-classical type of symbol manipulation. More importantly this strong version of implementational connectionism is of little concern for classical theorists because it does not change any knowledge we already had. Simply put, if we have already defined a language like Python completely through the BNF, <strong>a neural network cannot reveal anything new about the language if all it does is to implement the rules in its neural hardware.</strong></p><p>A second option is that the LMs have learned unpredictable, complex nonlinear mappings and latent variables which can generate well-formed code. Pinker and Prince call this possibility <i>eliminative connectionism</i>, which poses a more serious challenge for classical theories because they eliminate the need for rules and symbol manipulation. In <i>eliminative (neural) systems</i> it is impossible to find a principled mapping between the components of the distributed (vector) processing model and the steps involved in a symbol-processing theory. It is clear that the current bunch of deep learning models are advertised as eliminative systems. Browning and LeCun's neural symbol manipulations are specifically claimed to eliminate traditional rules of symbolic logic, and their rules of symbol manipulation are specifically not meant to be isomorphic to traditional rules. Other arguments for an eliminative intent include Bengio, LeCun and Hinton argued in their <a href=\"https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext\">Turing lecture</a> that continuous representations in Deep Learning models fundamentally differentiate neural LMs from traditional symbolic systems such as grammar because they enable computations based on non-linear transformations between the representing vectors themselves. &nbsp;</p><p><br>While the compuational core of neural LMs is vector based, they can perform tasks involving symbols because they use a symbolic representation in the input and output layers. <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/aaai.12036\">Kautz </a>enumerates different classes of <i>Neuro-Symbolic</i> hybrid systems which combine neural and symbolic approaches in different ways. He identifies the default paradigm in neural language models as the <i>Symbolic Neuro symbolic</i> (SNS) architecture, where sequences of words (symbolic) are converted to vectors which are passed to a neural network (neuro) whose output is computed by a softmax operation on the final layer of the network (symbolic). In the case of models trained for code generation, both natural language and code tokens are converted to- and from- vector representations and processed by the network. SNS systems can accurately generate well-formed productions of a rule governed symbolic system without access to the rules themselves, because they are extremely competent at generalizing the properties of observed productions. The problem is that we know for certain that this is not the right model for Python because the right model for Python is a classical symbolic system with a generative phrase structure grammar. <strong>If the LM can mimic a classical symbolic Python, then why should we believe that it isn't mimicking a classical symbolic natural language?</strong></p><p>We have now arrived at the real problem with neural networks as models of real or artificial intelligence. Judging by their performance on coding problems we could claim that they have essentially solved the problem and all they need is more scale or a \"world model\" to improve even further. But we would be wrong. What they really need is a symbol manipulation system. We are in a privileged position when it comes to knowing how Python works because \"we\" wrote it. The same is of course not the case for cognitive abilities such as language. Chomsky hypothesized that it involves a phrase structure grammar of some sort, but this is challenged by the impressive success of neural models. The problem is that there is no independent way to know if the challenges are valid, or if instead, the non-linear symbol transformations inherent in the SNS paradigm are sufficient to yield the results without \"really\" doing language, the same way they aren't \"really\" doing Python.</p><p>The most likely hypothesis is that the brain contains a number of classical symbolic systems which generate highly structured productions and correlated features which can in turn be exploited by statistical methods to construct models that can reconstruct similar productions in novel circumstances. There is no doubt humans make use of this kind of statistical mechanism: programmers don't all consult the BNF, many of them simply look at a few example statements and copy the form. But we have seen that humans can also use classical symbolic structures in their reasoning when they use the BNF (or equivalent defining syntax). Some of these classical structures might operate at a level not accessible to conscious introspection, for example generative grammar. Neural language models have proven themselves unable to illuminate the formal structure of Python, and it is not reasonable to claim that they are any more able to enlighten us about English. <strong>DL models can learn the mappings where the symbolic system produces lots of examples, like language. When the symbol system is used for planning, creativity, etc., this is where DL struggles to learn.</strong></p><h2>Conclusion</h2><p>Deep Learning is hardly the foundation for a general artificial intelligence. It is, however, a powerful foundation for systems that can exploit the productions of complex rule based systems in order to solve problems in the domain of that system. Combined with symbolic reasoning, deep learning has the potential for very powerful systems indeed. In particular the technology has provided a new set of tools to realize <a href=\"http://guzdial.cc.gatech.edu/hci-seminar/uploads/1/Man-Computer%20Symbiosis.pdf\">Licklider's vision from 1960</a>, who similarly grappled with the future of AI. In 1960 the Air Force estimated that \"... it would be 1980 before developments in artificial intelligence make it possible for machines alone to do much thinking or problem solving of military significance\". That is, 20 years to something like AGI. He suggested that those 20 years would be well spent developing \"man-machine\" symbiosis, which was an approach to programming computers in a way that maximally augments human reasoning rather that replaces it. He estimated 5 years to develop and 15 years to use the systems. Mockingly, he added \"... the 15 may be 10 or 500, but those years should be intellectually the most creative and exciting in the history of mankind.\" &nbsp;I think we are no closer now than Licklider was to predict if AGI is 10 or 500 years away. But I <a href=\"https://www.researchgate.net/publication/326869586_Strong_Cognitive_Symbiosis_Cognitive_Computing_for_Humans\">do think</a> that we are much closer at achieving symbiotic systems. DL networks can compress the world's knowledge into a manageable collection of vectors but without symbolic systems, they don't know what to do with them. Humans have the creativity and insight to interact with the vectors to unleash breathtaking discoveries.</p><p><br>&nbsp;</p>", "user": {"username": "cveres"}}, {"_id": "PMcoYMtA5LiLXpzk2", "title": "Upcoming EA conferences in 2023 (and 2022)", "postedAt": "2022-10-05T21:15:25.594Z", "htmlBody": "<p>The Centre for Effective Altruism will be organizing and supporting conferences for the EA community all over the world in 2023.&nbsp;We currently have the following events scheduled:</p><p><strong>EA Global</strong></p><ul><li>EA Global: Bay Area | (February 24\u201326, 2023)</li><li>EA Global: London | (May 19\u201321, 2023)</li><li>EA Global: US East Coast | (Fall/Autumn 2023) | Details TBC</li></ul><p><strong>EAGx</strong></p><ul><li>EAGxVirtual | (October 21\u201323, 2022)</li><li>EAGxRotterdam | (November 4\u20136, 2022)</li><li>EAGxBerkeley | (December 2\u20134, 2022)</li><li>EAGxLatinAmerica | (January 7\u20138, 2023)</li><li>EAGxIndia | (January 13\u201315, 2023)</li><li>EAGxNordics | (April 21\u201323, 2023)</li></ul><p>Applications for EAGxVirtual and EAGxRotterdam are open (<a href=\"https://creatorapp.zohopublic.com/centreforeffectivealtruism/ea/form-perma/EAG_Application/AV2sCTpgHH0NNCXXV5C8Jy5tsftVsj44V3Xh6Nu4EXf8r59HWW4ByExQMKVGsgdUQ0jRtrVQSZJbs3fTP8pr1XKjFkeEZJ9rNkRH?eag_use_application=standard\"><u>here</u></a>), and we expect applications for the other conferences to open approximately 2 months before the event. Applications for EA Global: Bay Area will open in November.</p><p>(If you'd like to&nbsp;<strong>add EA events like these directly to your Google Calendar,</strong> use&nbsp;<a href=\"https://calendar.google.com/calendar/u/0?cid=Y180bzMzc3VwNG0yaDIzYnFlaXRoOHUxdnRqc0Bncm91cC5jYWxlbmRhci5nb29nbGUuY29t\">this link</a>.)</p><p><strong>Some notes on these conferences:</strong></p><ul><li>EA Globals are run in-house by the CEA events team, whereas EAGx conferences are organized independently with financial support and mentoring from CEA.</li><li>EA Global conferences have a high bar for admission and are for people who are very familiar with EA and are taking significant actions (e.g. full-time work or study) based on EA ideas.</li><li>Admissions for EAGx conferences are processed independently by the EAGx conference organizers. These events are primarily for those who are newer to EA and interested in getting more involved and who are based in the region the conference is taking place in (e.g. EAGxIndia is primarily for people who are interested in EA and are based in India).</li><li><strong>Please apply to all conferences you wish to attend once applications open</strong> \u2014 we would rather get too many applications for some conferences and recommend that applicants attend a different one, than miss out on potential applicants to a conference.</li><li>Applicants can request financial aid to cover the costs of travel, accommodation and tickets. If you are accepted to one of our conferences, we want you to attend without financial burden or stress, so if you are on the fence about applying please do so. For example, if you feel like you technically could afford the travel, but it would be financially uncomfortable to do so, we\u2019d encourage you to apply for funding.</li><li>The EA Global dates were selected so as to avoid any major public or religious holidays. We also wanted to space the conferences out as much as possible, and as such we\u2019re hosting the Bay Area conference in the winter as it\u2019s our only major location that has good weather around this time. We selected Oakland (rather than San Francisco as per usual) because of a good venue we found there, the&nbsp;<a href=\"https://goo.gl/maps/t2MRwCGaepnZiy2YA\"><u>Oakland Marriott</u></a>.&nbsp;</li><li>We also expect that there will be more EAGx events in or near the same locations as our EA Globals, but they may not all be planned out or booked in yet.</li><li>Find more info on&nbsp;<a href=\"https://www.eaglobal.org/\"><u>our website</u></a>.</li></ul><p>As always, please feel free to email&nbsp;<a href=\"mailto:hello@eaglobal.org\">hello@eaglobal.org</a> with any questions, or comment below.</p>", "user": {"username": "Eli_Nathan"}}, {"_id": "wv6H5ZbuMbdcZEBEe", "title": "Philosophers speaking against the mistreatment of animals: The Montreal Declaration On Animal Exploitation", "postedAt": "2022-10-05T18:55:33.000Z", "htmlBody": "<p>This week, 473 (so far) philosophers signed to speak against the human exploitation of nonhuman animals, in <a href=\"https://greea.ca/en/montreal-declaration-on-animal-exploitation/?fbclid=IwAR1Xexu3o8v5tKB-tXXYpclzgtmLNqQ3HUgXxHdx5BbR3lOU689B5jGhqA4\">the Montreal Declaration On Animal Exploitation</a>, it seems to me that it is pretty rare for philosophers to sign and publish such a co-statement.&nbsp;</p><p>\"We are researchers in the field of moral and political philosophy. Our work is rooted in different philosophical traditions, and we rarely find ourselves in agreement with one another. We do agree, however, on the need for a profound transformation of our relationships with other animals. We condemn the practices that involve treating animals as objects or commodities.\"</p><p>My guess with not-so-much thought is that the declaration could be pretty influential to students and academics in the future. It might even feature in books on the history of philosophy in the future!&nbsp;</p><p><strong>To-do</strong>: The only EA-related philosophers on the list of signatories are Peter Singer and Jeff Sebo. I think it is pretty important (we have an animal welfare cause area but no EA philosopher on this list) and beneficial (strengthen this list and make it more powerful) for major EA philosophers to sign on this. If you are a philosopher, consider becoming a signatory by clicking this <a href=\"https://framaforms.org/montreal-declaration-on-animal-exploitation-1649022468?fbclid=IwAR2n0I2H7E2v5-UQcSonsK8-afqQc1Q17IACbSeBALtN7yj9X5SfFBvQD_I\">link</a>.</p>", "user": {"username": "tseyipfai@gmail.com"}}, {"_id": "z3yodnkYywhRLNnY9", "title": "Four Quotes on Preference Utilitarianism", "postedAt": "2022-10-06T07:06:48.155Z", "htmlBody": "<blockquote>\n<p>The fact that someone would be willing to forgo a decent diet in order to build a monument to his god does not mean that his claim on others for aid in his project has the same strength as a claim for aid in obtaining enough to eat. (<a href=\"https://www.jstor.org/stable/2024630\">Scanlon 1975</a>)</p>\n</blockquote>\n<h1></h1>\n<blockquote>\n<p>Consider next Desire-Fulfilment Theories. The simplest is the <em>Unrestricted</em> Theory. This claims that what is best for someone is what would best fulfil <em>all</em> of his desires, throughout his life. Suppose that I meet a stranger who has what is believed to be a fatal disease. My sympathy is aroused, and I strongly want this stranger to be cured. Much later, when I have forgotten our meeting, the stranger is cured. On the Unrestricted Desire-Fulfilment Theory, this event is good for me, and makes my life go better. This is not plausible. We should reject this theory. (<a href=\"https://doi.org/10.1093/019824908X.001.0001\">Parfit 1984</a>)</p>\n</blockquote>\n<h1></h1>\n<blockquote>\n<p>If I have a bad headache, anyone has a reason to want it to stop. But if I badly want to climb to the top of Mount Kilimanjaro, not everyone has a reason to want me to succeed. I have a reason to try to get to the top, and it may be much stronger than my reason for wanting a headache to go away, but other people have very little reason, if any, to care whether I climb the mountain or not. Or suppose I want to become a pianist. Then I have a reason to practice, but other people have little or no reason to care if I practice or not. (<a href=\"https://global.oup.com/academic/product/the-view-from-nowhere-9780195056440\">Nagel 1986</a>)</p>\n</blockquote>\n<h1></h1>\n<blockquote>\n<p>For example many Boston residents desperately wanted the Red Sox to win the World Series in 2004. Their happiness when the Red Sox won gave others \u2013 even Yankees fans \u2013 some reason to judge it to be good for them that their preferences were satisfied. But, as Nagel maintains, the mere preferences of Red Sox fans, as opposed to their happiness or unhappiness, is of no moral importance to others. (<a href=\"https://doi.org/10.1017/CBO9781139058537\">Hausman 2012</a>)</p>\n</blockquote>\n", "user": {"username": "Maximilian Schlederer"}}, {"_id": "3sHwJ5rbkAvcwFwGR", "title": "Notes on supporting Happier Lives Institute", "postedAt": "2022-10-06T13:01:17.821Z", "htmlBody": "<p><i>This is a cross-posted comment from </i><a href=\"https://manifold.markets/clearthinkbot/will-we-fund-the-happier-lives-inst?referrer=pav\"><i>Clearer Thinking regranting competition</i></a><i> on Manifold Markets (with a couple of minor edits and typo corrections)&nbsp;</i></p><h1>Intro</h1><h3>What it is about?</h3><p>The article describes my take on why I think the <a href=\"http://happierlivesinstitute.org\">Happier Lives Institute</a> should receive funding through the Clearer Thinking regranting round (Clearer Thinking organized <a href=\"https://manifold.markets/group/clearer-thinking-regrants\">a tournament on Manifold Markets</a> to help them crowd-evaluate which projects should receive funding).</p><h3>Abstract</h3><p>This is by no means a comprehensive review of the Happier Lives Institute (HLI). I have been exposed to HLI work relatively recently. Think of it as an interface to quickly understand what the Happier Lives Institute does and a subjective assessment of the potential value threads they are creating.</p><p>In the following text, I am arguing that HLI brings value in two dimensions. One is their work increasing well-being directly \u2013 they evaluate and support the most cost-effective organizations globally. Another is their work applying and stress-testing the Subjective Well-Being framework (SWB). I think having an alternative, thoroughly researched framework like SWB has a high expected value for the EA community. Most EA org rely on QALY+ framework. This work therefore can help diversify worldviews and help calibrate judgments of the main EA organizations.&nbsp;</p><h3>Why I am writing this?</h3><p>I am posting this on the Forum because some ideas may apply more broadly e.g. examining what is behind relatively low engagement within the EA community in supporting projects tackling increasing well-being directly. I would also love to hear feedback. Notes on my reasoning or the Happier Lives Institute's approach to increasing global well-being are welcome.</p><h3>Epistemic status</h3><p>I must be biased because I was voting yes on this market during the <a href=\"https://manifold.markets/clearthinkbot/will-we-fund-the-happier-lives-inst?referrer=pav\">Clearer Thinking tournament on Manifold Markets</a>. I had prior exposure to HLI and when I saw the chances of HLI receiving a grant at 40% I thought the prediction is way off.</p><p>Since then I spent a couple of days researching the topic. I watched a couple of HLI YouTube lectures and read a couple of their EA Forum posts. I am not very knowledgeable about the internal mechanics of frameworks like SWB and QALY+. I have decent knowledge and long exposure to topics like psychotherapy, well-being, and evidence-based therapies.</p><h3>Who may be interested in reading this?</h3><ul><li>People interested in the well-being discourse.</li><li>People who are skeptical about EA organizations tackling increasing well-being directly</li><li>People who don't know HLI or don't understand the value they are bringing</li></ul><h3>How to navigate through this document?</h3><p>All the sections (marked by the titles) stand on their own and don't need knowledge from previous sections to be understood. Feel free to skip around.</p><p>Abbreviations that are used in the text:&nbsp;</p><ul><li>HLI \u2013 Happier Lives Institute</li><li>SWB \u2013 subjective well-being framework</li><li>QALY \u2013 Quality-adjusted life years framework</li></ul><hr><h1>Utilitarianism and wellbeing</h1><p>In many definitions of utilitarianism, well-being is the central, defining term. Take some generic one from Wikipedia: \u201cUtilitarianism is a family of normative ethical theories that prescribe actions that maximize happiness and well-being for all individuals\u201d</p><p>Well-being, however, is notoriously hard to define and measure. Perhaps that\u2019s why this area is relatively neglected within the EA community. Also, in the past, established frameworks like QALY+ didn\u2019t render opportunities in the space particularly impactful. At least in the intuitive sense, it seems bizarre that EA couldn\u2019t identify interventions that are attempting to increase well-being directly. Intuitively, it seems there should be projects out there with a high expected value tackling the problem directly.</p><p>Speculatively thinking there may be one more reason for the lack of interest in the community. People within EA seem highly analytical \u2013 the majority consists of engineers, economists, and mathematicians. Could demography like this mean that people on average score lower in the emotional intelligence skills bucket? \u2013 therefore making the community less interested in projects optimizing the space.</p><h1>Happier Lives Institute as an organization</h1><p>In simplest terms, the Happier Lives Institute is like a GiveWell that specializes in well-being. They are working with the most cost-effective opportunities to increase global well-being.</p><p>Michael Plant, its founder, is an active member of the EA forum since 2015. He has written 26 posts gathering more than 5.6k karma. He seems to be interested in the subject matter at least since 2016 when he wrote the first post on the Forum asking&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/A278Run59FenZ4kae/is-effective-altruism-overlooking-human-happiness-and-mental\"><strong>Is effective altruism overlooking human happiness and mental health? I argue it is.</strong></a>&nbsp;His lectures on the subject matter seem clear, methodical, and follow the best epistemological practices in the community. He was Peter Singer\u2019s research assistant for two years, and Singer is an advisor to the institute.</p><p>The Clearer Thinking regrant is sponsoring the salary of Dr. Lily Yu. She seems to have relevant experience at the intersection of science, health, entrepreneurship, and grant-making.</p><h1>Neglected</h1><p>The cause area seems to be neglected within EA. Besides HLI I am aware of EA Psychology Lab, and Effective Self-help \u2013 but none of these organizations do as comprehensive work as HLI.</p><h1>Subjective well-being framework</h1><p>Even if the only value proposed by HLI was to research and donate to the most cost-effective opportunities to increase global well-being, I think it would be an outstanding organization to support.</p><p>However, HLI also works and stress-tests the Subjective Well-being framework (SWB) \u2013 work that the whole EA community can benefit from. Michael Plant describes the SWB methodology in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FvbTKrEQWXwN5A6Tb/a-happiness-manifesto-why-and-how-effective-altruism-should\">this article</a>&nbsp;and&nbsp;<a href=\"https://www.youtube.com/watch?v=VeT3bjWbNaU&amp;themeRefresh=1\">this lecture.</a>&nbsp;Most leading EA orgs like Open Philanthropy and GiveWell use a different approach \u2013 the QALY+ framework.</p><p>I think the big chunk of the HLI's value lies in running the alternative to QALY+ framework and challenging its assumptions. Michael Plant does this in the essay&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\">A philosophical review of Open Philanthropy\u2019s Cause Prioritisation Framework</a>. I am not gonna attempt to summarize this topic here (please see the links above for details), but I am gonna highlight a couple of the most interesting threads.</p><blockquote><p>\u201cIt\u2019s worth pointing out that QALYs and DALYs, the standard health metrics that OP, GiveWell, and others have relied on in their cause prioritization framework, are likely to be misleading because they rely on individuals' assessments of how bad they expect various health conditions would be, not on observations of how much those health conditions alter the subjective wellbeing of those who have them (Dolan and Metcalfe, 2012) \u2026 our affective forecasts (predictions of how others, or our later selves, feel) are subject to focusing illusions, where we overweight the importance of easy-to-visualise details, and immune neglect, where we forget that we will adapt to some things and not others, amongst other biases (Gilbert and Wilson 2007).\u201d&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\">Link</a></p></blockquote><p>Also worth noting is that the SWB framework demonstrates a lot of potential in areas previously ignored by EA organizations:</p><blockquote><p>\u201cat the Happier Lives Institute conducted two meta-analyses to compare the cost-effectiveness, in low-income countries, of providing psychotherapy to those diagnosed with depression compared to giving cash transfers to very poor families. We did this in terms of subjective measures of wellbeing and found that therapy is 9x more cost-effective\u201d&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\">Link</a></p></blockquote><p>HLI also looked at Open Philanthropy and GiveWell\u2019s backed interventions to compare QALY+ with SWB results.</p><blockquote><p>\u201cI show that, if we understand good in terms of maximising self-reported LS [Life satisfaction], alleviating poverty is surprisingly unpromising whereas mental health interventions, which have so far been overlooked, seem more effective\u201d&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\">Link</a></p></blockquote><p>But how the reasoning of this type can influence organizations like Open Philanthropy or GiveWell? Here Michael Plant describes how grant-making decisions can vary based on the weight given to different frameworks. The example describes value loss assessment based on the age of death.</p><blockquote><p>\u201cPerhaps the standard view of the badness of death is deprivationism, which states that the badness of death consists in the wellbeing the person would have had, had they lived. On this view, it\u2019s more important to save children than adults, all else equal, because children have more wellbeing to lose.</p><p>Some people have an alternative view that saving adults is more valuable than saving children. Children are not fully developed, they do not have a strong psychological connection to their future selves, nor do they have as many interests that will be frustrated if they die. The view in the philosophical literature that captures this intuition is called the time-relative interest account (TRIA).</p><p>A third view is Epicureanism, named after the ancient Greek philosopher Epicurus, on which death is not bad for us and so there is no value in living longer rather than shorter.\u201d <a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\">Link</a></p></blockquote><p>Prioritizing each of these approaches means different grant-making decisions (are we valuing kids' or adults' lives more?). Plant thinks that GiveWell does an insufficient job in their modeling:</p><blockquote><p>\u201cOn what grounds are the donor preferences [60% of their weight on this marker] is the most plausible weights \u2026 The philosophical literature is rich with arguments for and against each of the views on the badness of death (again,&nbsp;<a href=\"https://oxford.universitypressscholarship.com/view/10.1093/oso/9780190921415.001.0001/oso-9780190921415\">Gamlund and Solberg, 2019</a>&nbsp;is a good overview). We should engage with those arguments, rather than simply polling people\u2026 [Open Philantropy] do not need to go \u2018all-in\u2019 on a single philosophical view. Instead, they could divide up their resources across deprivationism, TRIA, and Epicureanism in accordance with their credence in each view.\u201d&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\">Link</a></p></blockquote><h1>Personal reasons</h1><p>I also see the value in promoting evidence-based approaches to therapy because of my personal background. I grew up in Poland, a country that had a rough 19th and 20th centuries: Partitions, uprisings, wars, the holocaust, change of borders, communism, transformation. Generational trauma is still present in my country.</p><p>I went through four types of therapies and only later stumbled upon evidence-based approaches. From my experience, it seems critical to pick the right therapies. Their effectiveness varies widely. Approaches like Cognitive behavioral therapies (CBT) or Third wave therapies tend to be more effective. (Third-wave therapies are evidence-based approaches based on CBT foundation, but recreated using human instead of animal models).</p><p>In my country, but also in many others, ineffective and unscientific approaches are still largely present, often dominating. It seems valuable to have an organization with a high epistemic culture that assesses and promotes evidence-based interventions.</p><h1>Counter-arguments</h1><p>I think the work of HLI would be compromised if the SWB framework had major flaws. Reading Michael Plant\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/FvbTKrEQWXwN5A6Tb/a-happiness-manifesto-why-and-how-effective-altruism-should\">article</a>&nbsp;on the subject makes me think that SWB is well researched and heavily discussed approach however I don\u2019t know much about its internal mechanics and didn\u2019t investigate its potential flaws.</p><h1>Summary</h1><p>I see the value of HLI supporting interventions increasing global well-being directly. But I also see value in their work with the SWB framework. I think having an alternative, thoroughly researched framework like SWB has a high expected value for the whole community. I think the regrant will help HLI stress-test their assumptions and run it on more organizations. The work of HLI could have an impact on leading EA organizations like Open Philanthropy or GiveWell \u2013 potentially helping recalibrate their recommendations and assessments.</p>", "user": {"username": "pawsys"}}, {"_id": "DadaytPLiXgdPbLe3", "title": "Tracking Compute Stocks and Flows: Case Studies?", "postedAt": "2022-10-05T17:54:51.197Z", "htmlBody": "<p><em>Posted in my personal capacity</em></p>\n<p>The AGI governance community has recently converged on compute governance<sup class=\"footnote-ref\"><a href=\"#fn-CFfbMSETKx7dpmAsX-1\" id=\"fnref-CFfbMSETKx7dpmAsX-1\">[1]</a></sup> as a promising lever for reducing existential risks from AI.</p>\n<p>One likely building block for any maximally secure compute governance regime is <strong>stock and flow accounting of (some kinds of) compute</strong>: i.e., requiring realtime accurate declaration to regulators of who possesses which uniquely numbered regulated chips, with penalties for undeclared or unauthorized<sup class=\"footnote-ref\"><a href=\"#fn-CFfbMSETKx7dpmAsX-2\" id=\"fnref-CFfbMSETKx7dpmAsX-2\">[2]</a></sup> transfers.</p>\n<p>To understand the optimal design and feasibility of such a regime, we seek historical analogies for similar regimes. One that we are already familiar with include:</p>\n<ul>\n<li>Fissile nuclear material and other nuclear weapons components</li>\n<li>Firearms</li>\n<li>Some financial instruments</li>\n<li>Automobiles</li>\n<li>Real estate</li>\n</ul>\n<p><strong>What are other good existing or historical analogies for compute stock and flow accounting</strong>? An ideal analogy will have many of the following traits:<sup class=\"footnote-ref\"><a href=\"#fn-CFfbMSETKx7dpmAsX-3\" id=\"fnref-CFfbMSETKx7dpmAsX-3\">[3]</a></sup></p>\n<ul>\n<li>The thing being tracked is a physical object</li>\n<li>The thing being tracked is economically important</li>\n<li>The thing being tracked is dual-use</li>\n<li>The tracking regime requires registration of current ownership and any transfers</li>\n<li>The tracking regime imposes penalties for failing to register ownership or transfer</li>\n</ul>\n<p>More and better examples of stock and flow accounting mechanisms could be extremely informative in improving compute governance proposals.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-CFfbMSETKx7dpmAsX-1\" class=\"footnote-item\"><p>See, e.g., <a href=\"https://forum.effectivealtruism.org/s/4yLbeJ33fYrwnfDev\">this sequence</a> by Lennart Heim. <a href=\"#fnref-CFfbMSETKx7dpmAsX-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-CFfbMSETKx7dpmAsX-2\" class=\"footnote-item\"><p>The question of which types of transfers ought to be authorized is important but beyond the scope of this post. <a href=\"#fnref-CFfbMSETKx7dpmAsX-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-CFfbMSETKx7dpmAsX-3\" class=\"footnote-item\"><p>NB: Many of the above do not have all of these traits! <a href=\"#fnref-CFfbMSETKx7dpmAsX-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Cullen_OKeefe"}}, {"_id": "37byNFAMEfbLNPXdm", "title": "Debate with Rational Methodology?", "postedAt": "2022-10-05T16:52:29.494Z", "htmlBody": "<p>I have some agreements and disagreements with EA. I\u2019m considering potential discussion. I have questions about that.</p>\n<p><em>Is there a way here to get organized, rational debate following written methodology?</em> I would want a methodology with design features aimed at rationally reaching a conclusion.</p>\n<p>I\u2019m not looking for casual, informal chatting that doesn\u2019t try to follow any particular rules or methods. In my experience, such discussions are bad at being very rational or reaching conclusions. I prefer discussion aimed at achieving specified goals (like conclusions about issues) using methods chosen on purpose to be appropriate to the goals. I\u2019m willing to take on responsibility and commitments in discussions, and I prefer to talk with people who are also willing to do that.</p>\n<p>I understand that <a href=\"https://forum.effectivealtruism.org/s/s5zDhfyRPvrpeuRf8/p/KFMMRyk6sTFReaWjs\">You don\u2019t have to respond to every comment</a> is a standard attitude here. I think it\u2019s good for that type of discussion to exist where people may ignore whatever ideas they want to with no transparency. I think that should be an available option and it\u2019s an appropriate default. But I don\u2019t think it should be the only type of discussion, so I\u2019m asking about the availability of alternatives.</p>\n<p>Some issues I would expect a debate methodology to address are:</p>\n<ul>\n<li>Starting and stopping conditions</li>\n<li>Topic branching</li>\n<li>Meta discussion</li>\n<li>Prioritization</li>\n<li>Bias</li>\n<li>Dishonesty</li>\n<li>Social status related behaviors</li>\n<li>Transparency</li>\n<li>A method of staying organized, including tracking discussion progress and open issues</li>\n<li>Replying to long messages without reading them in full</li>\n<li>The use of citations and quotes rather than writing new arguments</li>\n<li>When it\u2019s appropriate to expect a participant to read literature before continuing (or study or practice), or more broadly the issue of asking people to do work</li>\n<li>Handling when a participant is missing some prerequisite knowledge, skill or relevant specialization</li>\n<li>Can debates involve more than two people, and if so how is that handled?</li>\n<li>What should you do if you think your debate partner is making a bunch of errors which derail discussion?</li>\n<li>Should messages ever be edited after being read or replied to?</li>\n<li>What to do if you think a participant violates the methodology?</li>\n<li>What to do if you believe a participant insults you, misquotes you, or repeatedly says or implies subtly or ambiguously negative things about you?</li>\n</ul>\n", "user": {"username": "Elliot Temple"}}, {"_id": "ccZCJcqBtWxFLCN2t", "title": "How does an EA aligned Twitter looks like?", "postedAt": "2022-10-06T07:06:42.776Z", "htmlBody": "<p>Hi,</p><p>&nbsp;I'm working in a decentralised Twitter clone aligned with EA goals.&nbsp;</p><p><strong>About</strong></p><p>Right now, you can create post, threads, prediction markets and send a crypto token with value just by mentioning a user.</p><p>We're organising some games like this run bet <a href=\"https://nmkbs-aaaaa-aaaam-aadfa-cai.ic0.app/post/833#main\">https://nmkbs-aaaaa-aaaam-aadfa-cai.ic0.app/post/833#main</a> and planning to send fees/funds to a charity.</p><p>Eventually, the architecture will be as decentralised as possible.</p><p>You can see here a demo <a href=\"https://nmkbs-aaaaa-aaaam-aadfa-cai.ic0.app/\">https://nmkbs-aaaaa-aaaam-aadfa-cai.ic0.app/</a></p><p><strong>Questions</strong></p><p>1- Apart from polishing and keep adding twitter features, which extra features would you like to see in this EA aligned social network?</p><p>2- Do you have an idea of which general principles should I use to make it more aligned with EA?</p><p>3- Finally, I would like to get ideas about where to apply to get extra funding to accelerate development.&nbsp;</p><p>&nbsp;</p><p>Thank you in advance for your help and kindness :).&nbsp;</p><p>&nbsp;You can follow us at @SeerMarkets in Twitter.</p><p>Marcio.&nbsp;</p>", "user": {"username": "Seers"}}, {"_id": "nuyLLW6mhCApSk3JX", "title": "[Linkpost] \"Blueprint for an AI Bill of Rights\" - Office of Science and Technology Policy, USA (2022) \n", "postedAt": "2022-10-05T16:48:46.912Z", "htmlBody": "", "user": {"username": "rodeo_flagellum"}}, {"_id": "p3b9Hbsg9CKykKfC2", "title": "What are all the things you can embed into forum posts?", "postedAt": "2022-10-05T16:13:04.398Z", "htmlBody": "<p>Currently aware of:</p><ul><li><a href=\"https://www.lesswrong.com/posts/JLrnbThMyCYDBa6Gu/embedded-interactive-predictions-on-lesswrong-2\">Elicit forecasts</a><br>&nbsp;</li></ul><figure class=\"media\"><div data-oembed-url=\"https://forecast.elicit.org/binary/questions/VMs-r172N\">\n\t\t\t\t<div data-elicit-id=\"VMs-r172N\" class=\"elicit-binary-prediction\">\n\t\t\t\t\t<div>Elicit Prediction (<a href=\"forecast.elicit.org/binary/questions/VMs-r172N\">forecast.elicit.org/binary/questions/VMs-r172N</a>)</div>\n\t\t\t\t</div>\n\t\t\t</div></figure><ul><li><a href=\"https://www.lesswrong.com/posts/JLrnbThMyCYDBa6Gu/embedded-interactive-predictions-on-lesswrong-2?commentId=qpibmJLpZEH5ojSfu\">Metaculus forecasts</a></li></ul><figure class=\"media\"><div data-oembed-url=\"https://www.metaculus.com/questions/8906/uscanada-total-box-office-gross-in-2022/\">\n\t\t\t\t<div data-metaculus-id=\"8906\" class=\"metaculus-preview\">\n\t\t\t\t\t<iframe src=\"https://d3s0w6fek99l5b.cloudfront.net/s/1/questions/embed/8906/?plot=pdf\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure><ul><li><a href=\"https://forum.effectivealtruism.org/posts/hmLMBjutEbrHctAGo/you-can-embed-flashcard-quizzes-in-your-posts\">Flashcard quizzes</a><br>&nbsp;</li></ul><p>What am I missing?<br><br>Added from comments:</p><ul><li>Our World in Data</li></ul><figure class=\"media\"><div data-oembed-url=\"https://ourworldindata.org/grapher/nuclear-primary-energy\">\n\t\t\t\t\t<div data-owid-slug=\"nuclear-primary-energy\" class=\"owid-preview\">\n\t\t\t\t\t\t<iframe src=\"https://ourworldindata.org/grapher/nuclear-primary-energy\">\n\t\t\t\t\t</iframe></div>\n\t\t\t\t</div></figure><p>Manifold markets</p><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/LarsDoucet/will-elon-musk-back-out-of-the-deal\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/LarsDoucet/will-elon-musk-back-out-of-the-deal\">\n\t\t\t\t</iframe></div>\n\t\t\t</div></figure>", "user": {"username": "ChanaMessinger"}}, {"_id": "KEtQpH64jXnwBkszC", "title": "What is the most pressing feature to add to the forum? Upvote for in general, agreevote to agree with reasoning.", "postedAt": "2022-10-05T15:38:13.649Z", "htmlBody": "<p>A snap vote on features that people would like to see.&nbsp;</p><p>Upvote based on some sense of what the community should want, agreevote (the tick) to adjust for you personally.</p><p>There is a <a href=\"https://forum.effectivealtruism.org/posts/NhSBgYq55BFs7t2cA/ea-forum-feature-suggestion-thread \">feature request thread</a>, but I sense that old requests are getting lost. Hence, this.&nbsp;<br><br><i>Edit, I'm shifting my suggested voting rules. Agreevotes should now just be about the reasoning of the comment, as usual. They were basically entirely correlated. It was a trial and I was wrong.</i></p>", "user": {"username": "nathan"}}]