[{"_id": "SFuBsSgGnhXYzHnH7", "postedAt": "2024-01-04T14:13:04.896Z", "postId": "FDZc3bgywDnNmSHjv", "htmlBody": "<p><strong>Executive summary</strong>: This post suggests backup plans if AI systems become misaligned, as well as ideas for making AI systems more cooperative.</p><p><strong>Key points</strong>:</p><ol><li>We could study AI generalization to influence properties like lack of spitefulness, even if not full alignment.</li><li>Some properties, like lack of spite, may lead misaligned AIs to cooperate more with humans or other AIs.</li><li>We could implement \"surrogate goals\" in AI systems as harmless placeholders that threats could target instead of original goals.</li><li>Negotiation-assist AI could help resolve complex situations with many parties and options.</li><li>Acausal decision theory suggests learning too much could be risky; we may want caution before expanding knowledge of distant civilizations.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]