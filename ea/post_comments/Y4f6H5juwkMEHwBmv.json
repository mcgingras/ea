[{"_id": "fyiwiGsbTtN3Kases", "postedAt": "2022-11-13T19:58:39.103Z", "postId": "Y4f6H5juwkMEHwBmv", "htmlBody": "<p>Most of the things that are being pursued as longtermist interventions only require caring about our grandchildren, or maybe great grandchildren, which well within scope of even many ethical frameworks that care about preferences but not future lives. The rest of the interventions potentially require caring about the next, say, 1,000 years - which still doesn't require anything like the actual longtermist assumptions. (Anything further out than that isn't really going to be amenable to the types of actions we're taking anyways.)<br><br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "hKH7RTwfbSaHAWGmq", "postedAt": "2022-11-13T21:16:02.165Z", "postId": "Y4f6H5juwkMEHwBmv", "htmlBody": "<blockquote><p>Averting extinction makes sense in near termist ethical frameworks (8 billion people dying is very bad), but extinction is not the only category of existential risk, and it's the only one that can readily be justified within neartermist frameworks.</p></blockquote><p>Doesn't this ignore the impacts of averting extinction on almost all moral patients in the near term, i.e. nonhuman animals, farmed and wild? Why think those are good or outweighed by the positive impacts on humans?</p>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "spQY8igdcsB3x7Prp", "postedAt": "2022-11-13T21:33:36.175Z", "postId": "Y4f6H5juwkMEHwBmv", "htmlBody": "<p>Sorry, I'm just very human centric in my moral thinking. Considering non human moral patients requires deliberate effort, and it's not something that readily comes to mind.</p><p>&nbsp;</p><p>That said, while I do grant non human animals some consideration in some moral decision making, I don't particularly care for them here:</p><p>I'd destroy the rest of the biosphere in a heartbeat so that humanity may flourish among the stars.</p>", "parentCommentId": "hKH7RTwfbSaHAWGmq", "user": {"username": "Dragon God"}}, {"_id": "XWu7rxDyTt96qc8K5", "postedAt": "2022-11-14T00:28:50.109Z", "postId": "Y4f6H5juwkMEHwBmv", "htmlBody": "<p>It is not clear to me that taking action on non-extinction x-risks would be in conflict with neartermist goals:</p><p>Value lockin -&gt; like an AI singleton locking in a scenario that would not be optimal for longtermist goals? Isn't that akin to the alignment problem, and so directly intertwined with extinction risk?</p><p>Irreversible technological regression -&gt; wouldn't this be incredibly bad for present humans and so coincide with neartermist goals?</p><p>Any discrete event that prevents us from reaching technological maturity -&gt; wouldn't this essentially translate to reducing extinction risk as well as ensuring we have the freedom and wealth to pursue technological advancement, thus coinciding with neartermist goals?</p><p>Am I missing something?</p>", "parentCommentId": null, "user": {"username": "Kevin_Cornbob"}}, {"_id": "GKKJwqFFvHc74qdPa", "postedAt": "2022-11-14T11:32:50.712Z", "postId": "Y4f6H5juwkMEHwBmv", "htmlBody": "<p>I think it's a question of priorities. Yes, irreversible technological regression would be incredibly bad for present humans, but so would lots of other things that deserve a lot of attention from a neartermist perspective. However, once you start assigning non-trivial importance to the long-term future, things like this start looking <i>incredibly incredibly </i>bad and so get bumped up the priority list.</p><p>Also value lock-in could theoretically be caused by a totalitarian human regime with extremely high long-term stability.</p><p>I'd add s-risks as another longtermist priority not covered by either neartermist priorities or a focus on mitigating extinction risks (although one could argue that most s-risks are intimately entwined with AI alignment).</p>", "parentCommentId": "XWu7rxDyTt96qc8K5", "user": null}, {"_id": "PwptbqXKjuLvEa7KE", "postedAt": "2022-11-16T04:02:44.241Z", "postId": "Y4f6H5juwkMEHwBmv", "htmlBody": "<p>You seem to be not considering global catastrophic risk. This would generally not cause extinction, but could cause collapse of civilization from which we may not recover. And even if we do recover, we may end up losing significant fractions of long-term value. And it even if there's not a collapse of civilization, it could make global totalitarianism more likely, or worse values could end up in AI. At least some of these could be considered existential risk in the sense that much of the long-term value is lost. And yet preventing or mitigating them can generally be <a href=\"https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/\">justified </a>based on saving lives in the present generation.</p>", "parentCommentId": null, "user": {"username": "Denkenberger"}}]