[{"_id": "Jw3NJRndyroxHMbF7", "postedAt": "2023-05-24T21:23:29.748Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<blockquote><p>The reference classes I look at generate a prior for AGI control over current human resources anywhere between 5% and 60% (mean of ~16-26%).</p></blockquote><p>&nbsp;</p><p>Thanks for this Zach. I found it quite thought provoking, especially the quoted sentence.&nbsp;</p><p>&nbsp;</p><p>Based on your model, &nbsp;AGI controlling human resources is much more likely to occur than extinction. Given that, what events do you think we should be worried about with losing autonomy over resources (and potentially institutions) and are you more concerned about that after this work?&nbsp;</p>", "parentCommentId": null, "user": {"username": "benleo"}}, {"_id": "bhcAbAauAteNuE7mg", "postedAt": "2023-05-24T21:53:33.839Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<p>I take 5%-60% as an estimate of how much of human civilization's future value will depend on what AI systems do, but it does not necessarily exclude human autonomy. If humans determine what AI systems do with the resources they acquire and the actions they take, then AI could be extremely important, and humans would still retain autonomy.</p><p>I don't think this really left me more or less concerned about losing autonomy over resources. It does feel like this exercise made it starker that there's a large chance of AI reshaping the world beyond human extinction. It's not clear how much of that means the loss of human autonomy. I'm inclined to think in rough, nebulous terms that AI will erode human autonomy over 10% of our future, taking 10% as a sort of midpoint between the extinction likelihood and the degree of AI influence over our future. I think my previous views would have been in that ballpark.</p><p>The exercise did lead me to think the importance of AI is higher than I previously did and the likelihood of extinction per se is lower (though my final beliefs place all these probabilities higher than the priors in the report).</p>", "parentCommentId": "Jw3NJRndyroxHMbF7", "user": {"username": "zdgroff"}}, {"_id": "KAeah6mqEFbm5xzah", "postedAt": "2023-05-30T21:48:35.771Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<p>What do you think of the human subspecies base rate? In some ways, I think of the position of being human with the arrival of the AI species to be more similar to being a non-<i>sapiens</i> human with the arrival of <i>homo sapiens, </i>than e.g. being wooly mammooths or lions when humans arrived. In particular, I think of the relevant ecological \"niches\" as closer, and we may conjecture that AIs will have more resource competition with humans than humans with elephants.</p>", "parentCommentId": null, "user": {"username": "Linch"}}, {"_id": "6y8mbqJxkDDRpntNX", "postedAt": "2023-05-31T00:49:06.390Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<p>Yeah, this is an interesting one. I'd basically agree with what you say here. I looked into it and came away thinking (a) it's very unclear what the actual base rate is, but (b) it seems like it probably roughly resembles the general species one I have here. Given (b), I bumped up how much weight I put on the species reference class, but I did not include the human subspecies as a reference class here given (a).</p><p>From my exploration, it looked like there had been loose claims about many of them going extinct because of <i>Homo sapiens</i>, but it seemed like this was probably not true in the relevant sense of \"extinct\" except possibly in the cases of Neanderthals and <i>Homo floresiensis</i>. By relevant sense of \"extinct\", I mean dying off/ceasing to reproduce rather than interbreeding. This seems to be the best paper on the topic, concluding that climate change drove most of the extinctions: <a href=\"https://www.sciencedirect.com/science/article/pii/S2590332220304760\">https://www.sciencedirect.com/science/article/pii/S2590332220304760</a></p><p>As that paper says, <i>Homo sapiens</i> may have contributed to the extinction of the Neanderthals. I found suggestions in the case of <i>Homo floresiensis</i> to be pretty rough. So my take was that there was one species in the Homo genus that <i>might</i> have gone extinct because of Homo sapiens out of ~18 or so. That looks pretty similar to the means I take away from species extinctions (0.5-6%), but I felt it was too unclear to put a number on that gave added value.</p>", "parentCommentId": "KAeah6mqEFbm5xzah", "user": {"username": "zdgroff"}}, {"_id": "7c9BqQpnxXrjoBwom", "postedAt": "2023-05-31T05:54:13.143Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<p>Thanks for the reply! I appreciate it and will think further.</p><blockquote><p>This seems to be the best paper on the topic, concluding that climate change drove most of the extinctions: <a href=\"https://www.sciencedirect.com/science/article/pii/S2590332220304760\">https://www.sciencedirect.com/science/article/pii/S2590332220304760</a></p></blockquote><p>To confirm, you find the climate change extinction hypotheses very credible here? I know very little about the topic except I vaguely recall that some scholars also advanced climate change as the hypothesis for the megafauna extinctions but these days it's generally considered substantially less credible than human origin.</p>", "parentCommentId": "6y8mbqJxkDDRpntNX", "user": {"username": "Linch"}}, {"_id": "q3no2ekNeJ4Q9XtdT", "postedAt": "2023-06-04T19:56:25.896Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<p>From what I can tell, the climate change one seems like the one with the most support in the literature. I'm not sure how much the consensus in favor of the human cause of megafauna extinctions (which I buy) generalizes to the extinction of other species in the <i>Homo</i> genus. Most of the <i>Homo</i> extinctions happened much earlier than the megafauna ones. But it could be\u2014I have not given much thought to whether this consensus generalizes.</p><p>The other thing is that \"extinction\" sometimes happened in the sense that the species interbred with the larger population of <i>Homo sapiens</i>, and I would not count that as the relevant sort of extinction here.</p>", "parentCommentId": "7c9BqQpnxXrjoBwom", "user": {"username": "zdgroff"}}, {"_id": "E7ZFz9cwvoKktcFzZ", "postedAt": "2023-06-13T07:36:29.584Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<p>Interesting analysis, thanks. I'm a bit wary of it leading to a false sense of security around AGI though. Your \"reasons not to believe\", such as -</p><blockquote><p>- Biological causes of extinction may differ from AGI-related causes.<br>- Intelligent species, including humans may be qualitatively different from superhuman AGI.<br>- Since AGI is agential, it is likely more damaging than accidental risks.</p></blockquote><p>- are overpowering imo. AGI would be unprecedented in that it would threaten the entirety of carbon-based life (e.g. a superintelligent AI might remove the oxygen from the atmosphere to prevent the corrosion of it's machinery, or star-lift the Sun for energy).</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "Gooag5aDwNm3bJJCo", "postedAt": "2024-02-27T07:27:33.926Z", "postId": "MDNcMLQfxg2n9qXEZ", "htmlBody": "<p>[Separating out this paragraph into a new comment as I'm guessing it's what lead to the downvotes, and I'd quite like the point of the parent paragraph to stand alone. Not sure if anyone will see this now though.]<br><br>I think it's imperative to get the leaders of AGI companies to realise that they are in a <a href=\"https://www.youtube.com/watch?v=VcVfceTsD0A&amp;t=2895s&amp;ab_channel=LexFridman\">suicide</a> <a href=\"https://forum.effectivealtruism.org/posts/hZWuMuLDhi6wknAu5/the-sad-trade-off-between-ai-misuse-and-ai-x-risk-concerns?commentId=SE4spSQopEbY4XzFb\">race</a> (and that AGI will likely kill <a href=\"https://forum.effectivealtruism.org/posts/hZWuMuLDhi6wknAu5/the-sad-trade-off-between-ai-misuse-and-ai-x-risk-concerns?commentId=ckwuKDdGmdvLcuNmh\">them</a> <a href=\"https://forum.effectivealtruism.org/posts/hZWuMuLDhi6wknAu5/the-sad-trade-off-between-ai-misuse-and-ai-x-risk-concerns?commentId=QTqshby5swkgXazfK\">too</a>). The <a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom\">default</a> outcome of AGI is doom. For extinction risk at the 1% level, it seems reasonable (even though it's still 80M lives in expectation) to pull the trigger on AGI for a 99% chance of utopia. This is totally wrong-headed and is arguably contributing massively to current x-risk.</p>", "parentCommentId": "E7ZFz9cwvoKktcFzZ", "user": {"username": "Greg_Colbourn"}}]