[{"_id": "cE62Qtf7wNkCYPyML", "postedAt": "2022-10-31T15:18:22.851Z", "postId": "Nb2HnrqG4nkjCqmRg", "htmlBody": "<p>This is really nice, major kudos for a) the result, b) pulling the research colaboration off. Also keen to see people build upon these, e.g., with the optimizer's curse.</p>", "parentCommentId": null, "user": {"username": "NunoSempere"}}, {"_id": "3556nmwPS8mvsjwGT", "postedAt": "2022-10-31T15:46:38.156Z", "postId": "Nb2HnrqG4nkjCqmRg", "htmlBody": "<p>Thanks for your entry!</p>\n", "parentCommentId": null, "user": {"username": "GiveWell"}}, {"_id": "6fsXJnbo2npCKcKe2", "postedAt": "2023-04-02T18:07:54.336Z", "postId": "Nb2HnrqG4nkjCqmRg", "htmlBody": "<p>I see a potential 'buried headline' here. It seems as though Helen Keller  more or less 'dominates' the other charities!</p>\n<ul>\n<li>even its bottom 10th percentile is above the 50th percentile for all the others, and</li>\n<li>is  above the 75th percentile of all but New Incentives (and the latter has extremely wide intervals).</li>\n</ul>\n<p>Stepping outside the model itself, how confident are you in this result?</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "TaD4cZE7MKdNzcNuy", "postedAt": "2023-04-02T21:55:36.893Z", "postId": "Nb2HnrqG4nkjCqmRg", "htmlBody": "<p>The VOI simulation calculation discussion you give (\"This expectation of value obtainable in the informed state) is super-interesting to me, not sure I've seen this laid out before, I love it!.</p>\n<p>Maybe you could add 1 or 2 more lines or footnotes to make it a bit clearer; I had to think through 'yeah these distributions represent our beliefs over the probability of each actual value for each charity ... so sampling from these reflects what we can expect to learn.' Maybe it just took me a moment because I didn't start out as a Bayesian.</p>\n<p>Also, I might state the VOI as</p>\n<blockquote>\n<p>In the context of <em>choosing among discrete alternatives</em>, information is more valuable when:</p>\n</blockquote>\n<blockquote>\n<p>It is more likely to change our mind <em>about the best alternative</em><br>\n<em>the value of the alternatives we are choosing among is likely to be larger</em>\nThe environment is high-stakes, implying a greater potential for producing value or harm, given the resources devoted to it.</p>\n</blockquote>\n<p>I would want to disambiguate this so as to not be confused with a case where the context is high-stakes, but we are trying to choose between 'purple bednets and pink bednets' (nearly equivalent value)</p>\n<p>What do you think?</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "fBN9Kki2d4x9ioP67", "postedAt": "2023-04-02T21:58:28.181Z", "postId": "Nb2HnrqG4nkjCqmRg", "htmlBody": "<p>Is there any basis for Hubbard's 'spend 1/10th of the max' rule of thumb? It seems arbitrary. Is there any evidence or information-theoretic justification for this?</p>\n<p>On a related note, one could make a case that the 'benefit of research for informing Givewell's decisions' might understate the global benefits, if the research informs other research and decisionmaking outside of GW.</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "w7CpTBag8p5Eufwyv", "postedAt": "2023-04-02T22:01:07.501Z", "postId": "Nb2HnrqG4nkjCqmRg", "htmlBody": "<blockquote>\n<p>we use our uncertainty quantification to calculate the Expected Value of Perfect Information on all GiveWell Top Charities, and find that the cost would outweigh the benefits if GiveWell were to spend more than 11.25% of their budget on research for this information.</p>\n</blockquote>\n<p>Small point: I would replace 'budget' with 'funds they expect to influence'; considering the time horizon for this is difficult though, it probably requires a model of how the benefit of the research evolves over time, and some sort of explore/exploit modeling;</p>\n<p>Finger-exercise: If we think GW influences 500 million per year over the next 10 years, or 5 billion in total, 11% is 550 million, 1% 55 million. ... or 5.5 million per year. (But as I said in the other comment, I don't know where that '1/10th rule' came from).</p>\n<p>I guess this assumes that all of the money they shift are allocated to the single top-EV charity.</p>\n<p><a href=\"https://www.givingwhatwecan.org/charities/givewell-unrestricted-fund\">https://www.givingwhatwecan.org/charities/givewell-unrestricted-fund</a> suggests their operating expenses are about $11 million per year. But:</p>\n<ul>\n<li>that's not all research obviously</li>\n<li>on the other hand, there are many others doing research in this space</li>\n<li>on the other hand,  there are benefits outside GW</li>\n</ul>\n<p>But of course this doesn't count the VOI for the arrival of new charities, which could justify a greater expense.</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "xmDLveSMjaA6xbbnQ", "postedAt": "2023-04-02T22:23:53.257Z", "postId": "Nb2HnrqG4nkjCqmRg", "htmlBody": "<blockquote>\n<p>To get estimates comparable to each other, GiveWell CEAs use many variables that are the same across different charities. These include moral variables such as the discount rate and the moral weights, as well as a number or adjustment factors.</p>\n</blockquote>\n<blockquote>\n<p>Because each of our CEAs was built in its separate notebook. Variables that should have more correlated uncertainty do not correlate.</p>\n</blockquote>\n<p>Correlated uncertainty driven by <em>using the same parameters across models</em>: I'd love to see this, I expect it will effect the VOI calculations.</p>\n<p>Also could be a good gateway towards the more difficult correlated uncertainty: the correlation between parameters <em>within</em> a model. I expect some of these to be highly correlated, and this might matter a lot. In fact, in the fistula model, some of these might largely be different takes on the same uncertain underlying variable? Here one might need to report on the VOI for something like a 'cluster of variables'?</p>\n<p>But this is more challenging; I guess it would probably start with some calibrated judgment/educated guessing on the correlation parameter.</p>\n<p>I think maybe the issue of correlated uncertainty is the biggest limitation to bringing uncertainty into the models. If the uncertainties are 'super correlated in the same direction' then the naive 'worst case/best case' for everything is closer to being correct. If the uncertainties are 'super negatively correlated' then we may need to worry less about uncertainty, or perhaps worry about the largest of the uncertainties.</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}]