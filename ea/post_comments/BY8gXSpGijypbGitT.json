[{"_id": "K6776G3yFxGnCaKvK", "postedAt": "2018-02-21T00:23:26.655Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I thought this piece was good. I agree that MCE work is likely quite high impact - perhaps around the same level as X-risk work - and that it has been generally ignored by EAs. I also agree that it would be good for there to be more MCE work going forward. Here's my 2 cents:</p>\n<p>You seem to be saying that AIA is a technical problem and MCE is a social problem. While I think there is something to this, I think there are very important technical and social sides to both of these. Much of the work related to AIA so far has been about raising awareness about the problem (eg the book Superintelligence), and this is more a social solution than a technical one. Also, avoiding a technological race for AGI seems important for AIA, and this also is more a social problem than a technical one.</p>\n<p>For MCE, the 2 best things I can imagine (that I think are plausible) are both technical in nature. First, I expect clean meat will lead to the moral circle expanding more to animals. I really don't see any vegan social movement succeeding in ending factory farming anywhere near as much as I expect clean meat to. Second, I'd imagine that a mature science of consciousness would increase MCE significantly. Many people don't think animals are conscious, and almost no one thinks anything besides animals can be conscious. How would we even know if an AI was conscious, and if so, if it was experiencing joy or suffering? The only way would be if we develop theories of consciousness that we have high confidence in. But right now we're very limited in studying consciousness, because our tools at interfacing with the brain are crude. Advanced neurotechnologies could change that - they could allow us to potentially test hypotheses about consciousness. Again, developing these technologies would be a technical problem. </p>\n<p>Of course, these are just the first ideas that come into my mind, and there very well may be social solutions that could do more than the technical solutions I mentioned, but I don't think we should rule out the potential role of technical solutions, either.</p>\n", "parentCommentId": null, "user": {"username": "Daniel_Eth"}}, {"_id": "jrzXLtbJyeyfMmLdh", "postedAt": "2018-02-21T00:55:20.253Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for the comment! A few of my thoughts on this:</p>\n<blockquote>\n<p>Presumably we want some people working on both of these problems, some people have skills more suited to one than the other, and some people are just going to be more passionate about one than the other.</p>\n</blockquote>\n<p>If one is convinced non-extinction civilization is net positive, this seems true and important. Sorry if I framed the post too much as one or the other for the whole community.</p>\n<blockquote>\n<p>Much of the work related to AIA so far has been about raising awareness about the problem (eg the book Superintelligence), and this is more a social solution than a technical one.</p>\n</blockquote>\n<p>Maybe. My impression from people working on AIA is that they see it as mostly technical, and indeed they think much of the social work has been net negative. Perhaps not <em>Superintelligence</em>, but at least the work that's been done to get media coverage and widespread attention without the technical attention to detail of Bostrom's book.</p>\n<p>I think the more important social work (from a pro-AIA perspective) is about convincing AI decision-makers to use the technical results of AIA research, but my impression is that AIA proponents still think getting those technical results is probably the more important projects.</p>\n<p>There's also social work in coordinating the AIA community.</p>\n<blockquote>\n<p>First, I expect clean meat will lead to the moral circle expanding more to animals. I really don't see any vegan social movement succeeding in ending factory farming anywhere near as much as I expect clean meat to. </p>\n</blockquote>\n<p>Sure, though one big issue with technology is that it seems like we can do far less to steer its direction than we can do with social change. Clean meat tech research probably just helps us get clean meat sooner instead of making the tech progress happen when it wouldn't otherwise. The <em>direction</em> of the far future (e.g. whether clean meat is ever adopted, whether the moral circle expands to artificial sentience) probably matters a lot more than the <em>speed</em> at which it arrives.</p>\n<p>Of course, this gets very complicated very quickly, as we consider things like value lock-in. Sentience Institute has a bit of basic sketching on the topic on <a href=\"https://www.sentienceinstitute.org/foundational-questions-summaries#social-change-vs.-food-technology\">this page.</a></p>\n<blockquote>\n<p>Second, I'd imagine that a mature science of consciousness would increase MCE significantly. Many people don't think animals are conscious, and almost no one thinks anything besides animals can be conscious</p>\n</blockquote>\n<p>I disagree that &quot;many people don't think animals are conscious.&quot; I almost exclusively hear that view in from the rationalist/LessWrong community. A recent survey suggested that <a href=\"https://www.sentienceinstitute.org/animal-farming-attitudes-survey-2017\">87.3% of US adults</a> agree with the statement, &quot;Farmed animals have roughly the same ability to feel pain and discomfort as humans,&quot; and presumably even more think they have at least some ability.</p>\n<blockquote>\n<p>Advanced neurotechnologies could change that - they could allow us to potentially test hypotheses about consciousness.</p>\n</blockquote>\n<p>I'm fairly skeptical of this personally, partly because I don't think there's a fact of the matter when it comes to whether a being is conscious. I think Brian Tomasik has <a href=\"http://reducing-suffering.org/dissolving-confusion-about-consciousness/\">written eloquently</a> on this. (I know this is an unfortunate view for an animal advocate like me, but it seems to have the best evidence favoring it.)</p>\n", "parentCommentId": "K6776G3yFxGnCaKvK", "user": {"username": "Jacy"}}, {"_id": "mqaX6eTMPLw4ZvDZd", "postedAt": "2018-02-21T01:35:34.423Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>On this topic, I similarly do still believe there\u2019s a higher likelihood of creating hedonium; I just have more skepticism about it than I think is often assumed by EAs.</p>\n</blockquote>\n<p>This is the main reason I think the far future is high EV. I think we should be focusing on p(Hedonium) and p(Delorium) more than anything else. I'm skeptical that, from a hedonistic utilitarian perspective, byproducts of civilization could come close to matching the expected value from deliberately tiling the universe (potentially multiverse) with consciousness optimized for pleasure or pain. If p(H)&gt;p(D), the future of humanity is very likely positive EV.</p>\n", "parentCommentId": null, "user": {"username": "Michael_S"}}, {"_id": "LdYK5NEuCAWyyMPDB", "postedAt": "2018-02-21T02:52:20.917Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for writing this, I thought it was a good article. And thanks to Greg for funding it.</p>\n<p>My pushback would be on the cooperation and coordination point. It seems that a lot of other people, with other moral values, could make a very similar argument: that they need to promote their values now, as the stakes as very high with possible upcoming value lock-in. To people with those values, these arguments should seem roughly as important as the above argument is to you.</p>\n<ul>\n<li>Christians could argue that, if the singularity is approaching, it is vitally important that we ensure the universe won't be filled with sinners who will go to hell.</li>\n<li>Egalitarians could argue that, if the singularity is approaching, it is vitally important that we ensure the universe won't be filled with wider and wider diversities of wealth.</li>\n<li>Libertarians could argue that, if the singularity is approaching, it is vitally important that we ensure the universe won't be filled with property rights violations.</li>\n<li>Naturalists could argue that, if the singularity is approaching, it is vitally important that we ensure the beauty of nature won't be bespoiled all over the universe. </li>\n<li>Nationalists could argue that, if the singularity is approaching, it is vitally important that we ensure the universe will be filled with people who respect the flag.</li>\n</ul>\n<p>But it seems that it would be very bad if everyone took this advice literally. We would all end up spending a lot of time and effort on propaganda, which would probably be great for advertising companies but not much else, as so much of it is zero sum. Even though it might make sense, by their values, for expanding-moral-circle people and pro-abortion people to have a big propaganda war over whether foetuses deserve moral consideration, it seems plausible we'd be better off if they both decided to spend the money on anti-malaria bednets.</p>\n<p>In contrast, preventing the extinction of humanity seems to occupy a privileged position - not exactly comparable with the above agendas, though I can't exactly cache out why it seems this way to me. Perhaps to devout Confucians a pre-occupation with preventing extinction seems to be just another distraction from the important task of expressing filial piety \u2013 though I doubt this. </p>\n<p>(Moral Realists, of course, could argue that the situation is not really symmetric, because promoting the true values is distinctly different from promoting any other values.)</p>\n", "parentCommentId": null, "user": {"username": "Larks"}}, {"_id": "QM3SJSaxHWH3GKKCA", "postedAt": "2018-02-21T04:13:56.166Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Yeah, I think that's basically right. I think moral circle expansion (MCE) is closer to your list items than extinction risk reduction (ERR) is because MCE mostly competes in the values space, while ERR mostly competes in the technology space.</p>\n<p>However, MCE is competing in a narrower space than just values. It's in the MC space, which is just the space of advocacy on what our moral circle should look like. So I think it's fairly distinct from the list items in that sense, though you could still say they're in the same space because all advocacy competes for news coverage, ad buys, recruiting advocacy-oriented people, etc. (Technology projects could also compete for these things, though there are separations, e.g. journalists with a social beat versus journalists with a tech beat.)</p>\n<p>I think the comparably narrow space of ERR is ER, which also includes people who don't want extinction risk reduced (or even want it increased), such as some hardcore environmentalists, antinatalists, and negative utilitarians.</p>\n<p>I think these are legitimate cooperation/coordination perspectives, and it's not really clear to me how they add up. But in general, I think this matters mostly in situations where you actually can coordinate. For example, in the US general election when Democrats and Republicans come together and agree not to give to their respective campaigns (in exchange for their counterpart also not doing so). Or if there were anti-MCE EAs with whom MCE EAs could coordinate (which I think is basically what you're saying with &quot;we'd be better off if they both decided to spend the money on anti-malaria bednets&quot;).</p>\n", "parentCommentId": "LdYK5NEuCAWyyMPDB", "user": {"username": "Jacy"}}, {"_id": "9AbameRNfNEKgA5Qt", "postedAt": "2018-02-21T05:43:21.387Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>The lists were interesting on how they allude to the different psychology and motivations each EA has between the two camps. I hope someday I can have a civil discussion with someone not directly benefiting from AIA (such as being involved in the research). Aside, I have a friend who's crazy about futurism, 2045 Initiative/propaganda, and in love with everything Musk says on Twitter. </p>\n<p>&quot;The idea of 'helping the worst off' is appealing.&quot; Why wouldn't it be? Copenhagen Consensus.</p>\n<p>&quot;Their reaction when they look about extinction risk or AI safety is nonsensical&quot;, imaginary and completely unknown--zero tractability. No evidence to go off of since such technology does not exist. Why give to CS grad students? It's like trying to fund a mission to Mars, not priority. It's like funding time travel safety research, non sequitur. </p>\n<p>&quot;They are generally an unhappy person.&quot; I just had to laugh and compare how one interested in AI safety matched up. A neo-Freudian Jung/MBTI type of deal.  Almost like zodiac signs. Although, the Minnesota Multiphasic Personality Inventory (MMPI) is rigorous--so who am I to judge this informal inventory.</p>\n<p>Anyway, I simply do not see that individual action or donation to AIA research has measurable outcomes. We're talking about Strong AI here--it doesn't even exist! Not that it couldn't though. In the future, even the medium-term future, general standards of living could be significantly improved. Synthetic meat on a production scale is a much more realistic research area (or even anti-malaria mosquitoes)  instead of making a fuss about imaginary-theoretical events. We're at a unique sliver in time where it is extremely practical to help lessen the suffering of humans and animals in the near and medium-term future. (I.e., we have rapid transportation and instant information transfer.)</p>\n", "parentCommentId": null, "user": {"username": "adamaero"}}, {"_id": "HLbMShjdNM2CKKyEL", "postedAt": "2018-02-21T07:36:21.727Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>But it seems that it would be very bad if everyone took this advice literally. </p>\n</blockquote>\n<p>Fortunately, not everyone does take this advice literally :).</p>\n<p>This is very similar to the tragedy of the commons. If everyone acts out of their own self motivated interests, then everyone will be worse off. However, the situation as you described does not fully reflect reality because none of the groups you mentioned are actually trying to influence AI researchers at the moment. Therefore, MCE has a decisive advantage. Of course, this is always subject to change.</p>\n<blockquote>\n<p>In contrast, preventing the extinction of humanity seems to occupy a privileged position</p>\n</blockquote>\n<p>I find that it is often the case that people will dismiss any specific moral recommendation for AI except this one. Personally I don't see a reason to think that there are certain universal principles of minimal alignment. You may argue that human extinction is something that almost everyone agrees is bad -- but now the principle of minimal alignment has shifted to &quot;have the AI prevent things that almost everyone agrees is bad&quot; which is another privileged moral judgement that I see no intrinsic reason to hold. </p>\n<p>In truth, I see no neutral assumptions to ground AI alignment theory in. I think this is made even more difficult because even relatively small differences in moral theory from the point of view of information theoretic descriptions of moral values can lead to drastically different outcomes. However, I do find hope in moral compromise. </p>\n", "parentCommentId": "LdYK5NEuCAWyyMPDB", "user": {"username": "Matthew_Barnett"}}, {"_id": "qp9aJ7GNnoHnqN2BR", "postedAt": "2018-02-21T07:47:37.425Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Just because an event is theoretical doesn't mean that it won't occur. An asteroid hitting the Earth is theoretical, but something I think you might realize is quite real when it impacts.</p>\n<p>Some say that superintelligence doesn't have precedence, but I think that's overlooking a key fact. The rise of homo sapiens has radically altered the world -- and all signs point toward intelligence as the cause. We think at the moment that intelligence is just a matter of information processing, and therefore, there should be a way that it could be done by our own computers some day, if only we figured out the right algorithms to implement.</p>\n<p>If we learn that superintelligence is impossible, that means our current most descriptive scientific theories are wrong, and we will have learned something new. That's because that would indicate that humans are somehow cosmically special, or at least have hit the ceiling for general intelligence. On the flipside, if we create superintelligence, none of our current theories of how the world operates must be wrong. </p>\n<p>That's why it's important to take seriously. Because the best evidence we have available tells us that it's possible, not that it's impossible. </p>\n", "parentCommentId": "9AbameRNfNEKgA5Qt", "user": {"username": "Matthew_Barnett"}}, {"_id": "xNn9JmgnFMaqfPyp3", "postedAt": "2018-02-21T08:01:40.744Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>A very interesting and engaging article indeed.</p>\n<p>I agree that people often underestimate the value of strategic value spreading. Oftentimes, proposed moral models that AI agents will follow have some lingering narrowness to them, even when they attempt to apply the broadest of moral principles. For instance, in Chapter 14 of Superintelligence, Bostrom highlights his common good principle:</p>\n<blockquote>\n<p>Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals. </p>\n</blockquote>\n<p>Clearly, even something as broad as that can be controversial. Specifically, it doesn't speak at all about any non-human interests except insofar as humans express widely held beliefs to protect them.</p>\n<p>I think one thing to add is that AIA researchers who hold more traditional moral beliefs (as opposed to wide moral circles and transhumanist beliefs) are probably less likely to believe that moral value spreading is worth much. The reason for this is obvious: if everyone around you holds, more or less, the same values that you do, then why change anyone's mind? This may explain why many people dismiss the activity you proposed. </p>\n", "parentCommentId": null, "user": {"username": "Matthew_Barnett"}}, {"_id": "2ijtsX5sQRY64XemS", "postedAt": "2018-02-21T09:43:55.001Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thank you for this piece. I enjoyed reading it and I'm glad that we're seeing more people being explicit about their cause-prioritization decisions and opening up discussion on this crucially important issue. </p>\n<p>I know that it's a weak consideration, but I hadn't, before I read this, considered the argument for the scale of values spreading being larger than the scale of AI alignment (perhaps because, as you pointed out, the numbers involved in both are huge) so thanks for bringing that up.  </p>\n<p>I'm in agreement with Michael_S that hedonium and delorium should be the most important considerations when we're estimating the value of the far-future, and from my perspective the higher probability of hedonium likely does make the far-future robustly positive, despite the valid points you bring up. This doesn't necessarily mean that we should focus on AIA over MCE (I don't), but it does make it more likely that we should. </p>\n<p>Another useful contribution, though others may disagree, was the biases section: the biases that could potentially favour AIA did resonate with me, and they are useful to keep in mind.</p>\n", "parentCommentId": null, "user": {"username": "Vidur_Kapur"}}, {"_id": "YwWiiCuip2wA9nkvW", "postedAt": "2018-02-21T13:02:23.831Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>I'm fairly skeptical of this personally, partly because I don't think there's a fact of the matter when it comes to whether a being is conscious.</p>\n</blockquote>\n<p>I would guess that increasing understanding of cognitive science would generally increase people's moral circles if only because people would think more about these kinds of questions. Of course, understanding cognitive science is no guarantee that you'll conclude that animals matter, as we can see from people like Dennett, Yudkowsky, Peter Carruthers, etc.</p>\n", "parentCommentId": "jrzXLtbJyeyfMmLdh", "user": {"username": "Brian_Tomasik"}}, {"_id": "RhcRAetAM8Ev3b7e2", "postedAt": "2018-02-21T13:50:57.015Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Agreed.</p>\n", "parentCommentId": "YwWiiCuip2wA9nkvW", "user": {"username": "Jacy"}}, {"_id": "MnRHMwdFEmDK436FL", "postedAt": "2018-02-21T13:54:31.961Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>That makes sense. If I were convinced hedonium/dolorium dominated to a very large degree, and that hedonium was as good as dolorium is bad, I would probably think the far future was at least moderately +EV.</p>\n", "parentCommentId": "2ijtsX5sQRY64XemS", "user": {"username": "Jacy"}}, {"_id": "L3a74fKzjtDPhXh3X", "postedAt": "2018-02-21T15:31:26.959Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I thought this was very interesting, thanks for writing up. Two comments</p>\n<ol>\n<li><p>It was useful to have a list of reasons why you think the EV of the future could be around zero, but it still found it quite vague/hard to imagine - why exactly would more powerful minds be mistreating less powerful minds? etc. - so I'd would have liked to see that sketched in slightly more depth.</p>\n</li>\n<li><p>It's not obvious to me it's correct/charitable to draw the neglectedness of MCE so narrowly. Can't we conceive of a huge ammount of moral philosophy, and well as social activism, both new and old, as MCE? Isn't all EA outreach an indirect form of MCE? </p>\n</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "GTcTkvnZMrhiCnfyJ", "postedAt": "2018-02-21T16:38:47.981Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I'm sympathetic to both of those points personally.</p>\n<p>1) I considered that, and in addition to time constraints, I know others haven't written on this because there's a big concern of talking about it making it more likely to happen. I err more towards sharing it despite this concern, but I'm pretty uncertain. Even the detail of this post was more than several people wanted me to include.</p>\n<p>But mostly, I'm just limited on time.</p>\n<p>2) That's reasonable. I think all of these boundaries are fairly arbitrary; we just need to try to use the same standards across cause areas, e.g. considering only work with this as its explicit focus. Theoretically, since Neglectedness is basically just a heuristic to estimate how much low-hanging fruit there is, we're aiming at &quot;The space of work that might take such low-hanging fruit away.&quot; In this sense, Neglectedness could vary widely. E.g. there's limited room for advocating (e.g. passing out leaflets, giving lectures) directly to AI researchers, but this isn't affected much by advocacy towards the general population.</p>\n<p>I do think moral philosophy that leads to expanding moral circles (e.g. writing papers supportive of utiltiarianism), moral-circle-focused social activism (e.g. anti-racism, not as much something like campaigning for increased arts funding that seems fairly orthogonal to MCE), and EA outreach (in the sense that the A of EA means a wide moral circle) are MCE in the broadest somewhat-useful definition.</p>\n<p><a href=\"https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/\">Caspar's blog post</a> is a pretty good read on the nuances of defining/utilizing Neglectedness.</p>\n", "parentCommentId": "L3a74fKzjtDPhXh3X", "user": {"username": "Jacy"}}, {"_id": "aFkCAcbYTCiijxzCK", "postedAt": "2018-02-21T20:52:48.111Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>This post is extremely valuable - thank you! You have caused me to reexamine my views about the expected value of the far future.</p>\n<p>What do you think are the best levers for expanding the moral circle, besides donating to SI? Is there anything else outside of conventional EAA?</p>\n", "parentCommentId": null, "user": {"username": "avacyn"}}, {"_id": "qdQJQSChrPN4vYcbH", "postedAt": "2018-02-21T22:09:37.908Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks! That's very kind of you.</p>\n<p>I'm pretty uncertain about the best levers, and I think research can help a lot with that. Tentatively, I do think that MCE ends up aligning fairly well with conventional EAA (perhaps it should be unsurprising that the most important levers to push on for near-term values are also most important for long-term values, though it depends on how narrowly you're drawing the lines).</p>\n<p>A few exceptions to that:</p>\n<ul>\n<li><p>Digital sentience probably matters the most in the long run. There are good reasons to be skeptical we should be advocating for this now (e.g. it's quite outside of the mainstream so it might be hard to actually get attention and change minds; it'd probably be hard to get funding for this sort of advocacy (indeed that's one big reason SI started with farmed animal advocacy)), but I'm pretty compelled by the general claim, &quot;If you think X value is what matters most in the long-term, your default approach should be working on X directly.&quot;\nAdvocating for digital sentience is of course neglected territory, but Sentience Institute, the Nonhuman Rights Project, and Animal Ethics have all worked on it. People for the Ethical Treatment of Reinforcement Learners has been the only dedicated organization AFAIK, and I'm not sure what their status is or if they've ever paid full-time or part-time staff.</p>\n</li>\n<li><p>I think views on value lock-in matter a lot because of how they affect food tech (e.g. supporting The Good Food Institute). I place significant weight on this and a few other things (see <a href=\"https://www.sentienceinstitute.org/foundational-questions-summaries#social-change-vs.-food-technology\">this section</a> of an SI page) that make me think GFI is actually a pretty good bet, despite my concern that technology progresses monotonically.</p>\n</li>\n<li><p>Because what might matter most is society's general concern for weird/small minds, we should be more sympathetic to indirect antispeciesism work like that done by Animal Ethics and the fundamental rights work of the Nonhuman Rights Project. From a near-term perspective, I don't think these look very good because I don't think we'll see fundamental rights be a big reducer of factory farm suffering.</p>\n</li>\n<li><p>This is a less-refined view of mine, but I'm less focused than I used to be on wild animal suffering. It just seems to cost a lot of weirdness points, and naturogenic suffering doesn't seem nearly as important as anthropogenic suffering in the far future. Factory farm suffering seems a lot more similar to far future dystopias than does wild animal suffering, despite WAS dominating utility calculations for the next, say, 50 years.</p>\n</li>\n</ul>\n<p>I could talk more about this if you'd like, especially if you're facing specific decisions like where exactly to donate in 2018 or what sort of job you're looking for with your skillset.</p>\n", "parentCommentId": "aFkCAcbYTCiijxzCK", "user": {"username": "Jacy"}}, {"_id": "Fw53moPFXdwMbrZay", "postedAt": "2018-02-21T22:24:11.366Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thank you for writing this post. An evergreen difficulty that applies to discussing topics of such a broad scope is the large number of matters that are relevant, difficult to judge, and where one's judgement (whatever it may be) can be reasonably challenged. I hope to offer a crisper summary of why I am not persuaded. </p>\n<p>I understand from this the primary motivation of MCE is avoiding AI-based dystopias, with the implied causal chain being along the lines of, \u201cIf we ensure the humans generating the AI have a broader circle of moral concern, the resulting post-human civilization is less likely to include dystopic scenarios involving great multitudes of suffering sentiences.\u201d</p>\n<p>There are two considerations that speak against this being a greater priority than AI alignment research: 1) Back-chaining from AI dystopias leaves relatively few occasions where MCE would make a crucial difference. 2) The current portfolio of \u2018EA-based\u2019 MCE is poorly addressed to averting AI-based dystopias.</p>\n<p>Re. 1): MCE may prove neither necessary nor sufficient for ensuring AI goes well. On one hand, AI designers, even if speciesist themselves, might nonetheless provide the right apparatus for value learning such that resulting AI will not propagate the moral mistakes of its creators. On the other, even if the AI-designers have the desired broad moral circle, they may have other crucial moral faults (maybe parochial in other respects, maybe selfish, maybe insufficiently reflective, maybe some mistaken particular moral judgements, maybe naive approaches to cooperation or population ethics, and so on) - even if they do not, there are manifold ways in the wider environment (e.g. arms races), or in terms of technical implementation, that may incur disaster.</p>\n<p>It seems clear to me that, <em>pro tanto</em>, the less speciesist the AI-designer, the better the AI. Yet for this issue to be of such fundamental importance to be comparable to AI safety research generally, the implication is of an implausible doctrine of \u2018AI immaculate conception\u2019: only by ensuring we ourselves are free from sin can we conceive an AI which will not err in a morally important way.</p>\n<p>Re 2): As Plant <a href=\"http://effective-altruism.com/ea/1l0/why_i_prioritize_moral_circle_expansion_over/dck\">notes</a>, MCE does not arise from animal causes alone: global poverty, climate change also act to extend moral circles, as well as propagating other valuable moral norms. Looking at things the other way, one should expect the animal causes found most valuable from the perspective of avoiding AI-based dystopia to diverge considerably from those picked on face-value animal welfare. Companion animal causes are far inferior from the latter perspective, but unclear on the former if this a good way of fostering concern for animals; if the crucial thing is for AI-creators not to be speciest over the general population, targeted interventions like \u2018Start a petting zoo at Deepmind\u2019 look better than broader ones, like the abolition of factory farming. </p>\n<p>The upshot is that, even if there are some particularly high yield interventions in animal welfare from the far future perspective, this should be fairly far removed from typical EAA activity directed towards having the greatest near-term impact on animals. If this post heralds a pivot of Sentience Institute to directions pretty orthogonal to the principal component of effective animal advocacy, this would be welcome indeed.</p>\n<p>Notwithstanding the above, the approach outlined above has a role to play in some ideal \u2018far future portfolio\u2019, and it may be reasonable for some people to prioritise work on this area, if only for reasons of comparative advantage. Yet I aver it should remain a fairly junior member of this portfolio compared to AI-safety work. </p>\n", "parentCommentId": null, "user": {"username": "Gregory_Lewis"}}, {"_id": "spvMxDuu3w5zTJ73Q", "postedAt": "2018-02-21T23:47:25.384Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Those considerations make sense. I don't have much more to add for/against than what I said in the post.</p>\n<p>On the comparison between different MCE strategies, I'm pretty uncertain which are best. The main reasons I currently favor farmed animal advocacy over your examples (global poverty, environmentalism, and companion animals) are that (1) farmed animal advocacy is far more neglected, (2) farmed animal advocacy is far more similar to potential far future dystopias, mainly just because it involves vast numbers of sentient beings who are largely ignored by most of society. I'm not relatively very worried about, for example, far future dystopias where dog-and-cat-like-beings (e.g. small, entertaining AIs kept around for companionship) are suffering in vast numbers. And environmentalism is typically advocating for non-sentient beings, which I think is quite different than MCE for sentient beings.</p>\n<p>I think the better competitors to farmed animal advocacy are advocating broadly for antispeciesism/fundamental rights (e.g. Nonhuman Rights Project) and advocating specifically for digital sentience (e.g. a larger, more sophisticated version of People for the Ethical Treatment of Reinforcement Learners). There are good arguments against these, however, such as that it would be quite difficult for an eager EA to get much traction with a new digital sentience nonprofit. (We considered founding Sentience Institute with a focus on digital sentience. This was a big reason we didn't.) Whereas given the current excitement in the farmed animal space (e.g. the coming release of &quot;clean meat,&quot; real meat grown without animal slaughter), the farmed animal space seems like a fantastic place for gaining traction.</p>\n<p>I'm currently not very excited about &quot;Start a petting zoo at Deepmind&quot; (or similar direct outreach strategies) because it seems like it would produce a ton of backlash because it seems too adversarial and aggressive. There are additional considerations for/against (e.g. I worry that it'd be difficult to push a niche demographic like AI researchers very far away from the rest of society, at least the rest of their social circles; I also have the same traction concern I have with advocating for digital sentience), but this one just seems quite damning.</p>\n<blockquote>\n<p>The upshot is that, even if there are some particularly high yield interventions in animal welfare from the far future perspective, this should be fairly far removed from typical EAA activity directed towards having the greatest near-term impact on animals. If this post heralds a pivot of Sentience Institute to directions pretty orthogonal to the principal component of effective animal advocacy, this would be welcome indeed.</p>\n</blockquote>\n<p>I agree this is a valid argument, but given the other arguments (e.g. those above), I still think it's usually right for EAAs to focus on farmed animal advocacy, including Sentience Institute at least for the next year or two.</p>\n<p>(FYI for readers, Gregory and I also discussed these things before the post was published when he gave feedback on the draft. So our comments might seem a little rehearsed.)</p>\n", "parentCommentId": "Fw53moPFXdwMbrZay", "user": {"username": "Jacy"}}, {"_id": "eEcKJSzb8T92eDujf", "postedAt": "2018-02-22T02:36:22.645Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>@Matthew_Barnett \nAs a senior electrical engineering student, proficient in a variety of programming languages, I do think and believe that AI is important to think about and discuss. The theoretical threat of a malevolent strong AI would be immense. But that does not mean one has cause or a valid reason to support CS grad students financially.</p>\n<p>A large, significant, asteroid collision with Earth would also be quite devastating. Yet, to fund and support aerospace grads does not follow. Perhaps I really mean this: AI safety is an Earning to Give non sequitur. </p>\n<p>Lastly, again, there is no evidence or results. Effective Altruism is about being beneficent instead of merely benevolent (meaning well). In other words, making decisions off well researched initiatives (e.g., bed nets). Since strong AI does not exist, it does not make sense to support though E2G. (I'm not saying it will never exist; that is unknown.) Of course, there are medium-term (systematic change) with results that more or less rely on historical-type empiricism--but that's still some type of evidence. For poverty we have RCTs and developmental economics. For AI safety [<em>something</em>?]. For animal suffering we have proof that less miserable conditions can become a reality. </p>\n", "parentCommentId": null, "user": {"username": "adamaero"}}, {"_id": "yTCCnu3jFyi7Zyn9S", "postedAt": "2018-02-22T02:56:12.391Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>Second, I'd imagine that a mature science of consciousness would increase MCE significantly. Many people don't think animals are conscious, and almost no one thinks anything besides animals can be conscious. How would we even know if an AI was conscious, and if so, if it was experiencing joy or suffering? The only way would be if we develop theories of consciousness that we have high confidence in. But right now we're very limited in studying consciousness, because our tools at interfacing with the brain are crude. Advanced neurotechnologies could change that - they could allow us to potentially test hypotheses about consciousness. Again, developing these technologies would be a technical problem.</p>\n</blockquote>\n<p>I think that's right. Specifically, I would advocate consciousness research as a foundation for principled moral circle expansion. I.e., if we do consciousness research correctly, the equations themselves will tell us how conscious insects are, whether algorithms can suffer, how much moral weight we should give animals, and so on.</p>\n<p>On the other hand, if there is no fact of the matter as to what is conscious, we're headed toward a very weird, very contentious future of conflicting/incompatible moral circles, with no 'ground truth' or shared principles to arbitrate disputes.</p>\n<p>Edit: I'd also like to thank Jacy for posting this- I find it a notable contribution to the space, and clearly a product of a lot of hard work and deep thought.</p>\n", "parentCommentId": "K6776G3yFxGnCaKvK", "user": {"username": "MikeJohnson"}}, {"_id": "HZ47ZrwXv8nhAoARx", "postedAt": "2018-02-22T05:07:43.057Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I don't think anyone here is suggesting supporting random CS grads financially. Although, they might endorse something like that indirectly by funding AI alignment research, which tends to attract CS grads. </p>\n<p>I agree that simply because an asteroid collision would be devastating, it does not follow that we should necessarily focus on that work in particular. However, there are variables which I think you might be overlooking.</p>\n<p>The reason why people are concerned with AI alignment is not necessarily because of the scope of the issue, but also the urgency and tractability of the problem. The urgency of the problem comes from the idea that advanced AI will probably be developed this century. The tractability of the problem comes from the idea that there exists a set of goals that we could in theory put into an AI goals that are congruent with ours -- you might want to read up on the Orthogonality Thesis. </p>\n<p>Furthermore, it is dangerous to assume that we should judge the effectiveness of certain activities merely based on prior evidence or results. There are some activities which are just infeasible to give post hoc judgements about -- and this issue is one of them. The inherent nature of the problem is that we will probably only get about one chance to develop superintelligence -- because if we fail, then we will all probably die or otherwise be permanently unable to alter its goals. </p>\n<p>To give you an analogy, few would agree that because climate change is an unprecedented threat, it therefore follows that we should wait until <em>after</em> the damage has been done to assess the best ways of mitigating it. Unfortunately for issues that have global scope, it doesn't look like we get a redo if things start going badly. </p>\n<p>If you want to learn more about the research, I recommend reading Superintelligence by Nick Bostrom. The vast majority of AI alignment researchers are <em>not</em> worried about malevolent AI despite your statement. I mean this is in the kindest way possible, but if you really want to be sure that you're on the right side of a debate, it's worth understanding the best arguments against your position, not the worst. </p>\n", "parentCommentId": "eEcKJSzb8T92eDujf", "user": {"username": "Matthew_Barnett"}}, {"_id": "5QHTcjbMqkqYFDPWE", "postedAt": "2018-02-22T05:47:33.741Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for funding this research. Notes:</p>\n<ul>\n<li><p>Ostensibly it seems like much of Sentience Institute's (SI) current research is focused on identifying those MCE strategies which historically have turned out to be more effective among the strategies which have been tried. I think SI as an organization is based on the experience of EA as a movement in having significant success with MCE in a relatively short period of time. Successfully spreading the meme of effective giving; increasing concern for the far future in notable ways; and corporate animal welfare campaigns are all dramatic achievements for a young social movement like EA. While these aren't on the scale of shaping MCE over the course of the far future, these achievements makes it seem more possible EA and allied movements can have an outsized impact by pursuing neglected strategies for values-spreading. </p>\n</li>\n<li><p>On terminology, to say the focus is on non-human animals, or even moral patients which typically come to mind when describing 'animal-like' minds, i.e., familiar vertebrates is inaccurate. &quot;Sentient being&quot;, &quot;moral patient&quot; or &quot;non-human agents/beings&quot; are terms which are inclusive of non-human animals, and other types of potential moral patients posited. Admittedly these aren't catchy terms.</p>\n</li>\n</ul>\n", "parentCommentId": "Fw53moPFXdwMbrZay", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "ZYcsdxGxgZjj9zcgR", "postedAt": "2018-02-22T05:59:39.892Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>I think one thing to add is that AIA researchers who hold more traditional moral beliefs (as opposed to wide moral circles and transhumanist beliefs) are probably less likely to believe that moral value spreading is worth much.</p>\n</blockquote>\n<p>Historically it doesn't seem to be true. As AIA becomes more mainstream, it'll be attracting a wider diversity of people, which may induce a form of common grounding and normalization of the values in the community. We should be looking for opportunities to collect data on this in the future to see how attitudes within AIA change. Of course this could lead to attempts to directly influence the proportionate representation of different values within EA. That'd be prone to all the hazards of an internal tug of war pointed out in other comments on this post. Because the vast majority of the EA movement focused on the impact of advanced AI on the far future are relatively coordinated and with sufficiently similar goals there isn't much risk of internal fraction in the near future. I think organizations from MIRI to FRI are also averse to growing AIA in ways which drive the trajectory of the field away from what EA currently values. </p>\n", "parentCommentId": "xNn9JmgnFMaqfPyp3", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "xS5pMw2vqXZaR2axu", "postedAt": "2018-02-22T06:12:18.095Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>As EA as a movement has grown so far, the community appears to converge upon a rationalization process whereby most of us have realized what is centrally morally important is the experiences of well-being of a relatively wide breadth of moral patients, and the relatively equal moral weight assigned to well-being of each moral patient. The difference between SI and those who focus on AIA is primarily their differing estimates of the expected value of far-future in terms of average or total well-being. Among the examples you provided, it seems some worldviews are more amenable to the rationalization process which lends itself to consequentialism and EA. Many community members were egalitarians and libertarians who find common cause now in trying to figure out if to focus on AIA or MCE. I think your point is important in that ultimately advocating for this type of values spreading could be bad. However what appears to be an extreme amount of diversity could end up looking less fraught in a competition among values as divergent worldviews converge on similar goals. </p>\n<p>Since different types of worldviews, like any amenable to aggregate consequentialist frameworks, can collate around a single goal of something like MCE. The relevance of your point, then, would hinge upon how universal MCE really is or can be across worldviews, relative to other types of values, such that it wouldn't clash with many worldviews in a values-spreading contest. This is a matter of debate I haven't thought of. It seems an important way to frame solutions to the challenge to Jacy's point you raise.</p>\n", "parentCommentId": "LdYK5NEuCAWyyMPDB", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "fawndMuCnaWLhEycB", "postedAt": "2018-02-22T12:31:49.974Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>The main reasons I currently favor farmed animal advocacy over your examples (global poverty, environmentalism, and companion animals) are that (1) farmed animal advocacy is far more neglected, (2) farmed animal advocacy is far more similar to potential far future dystopias, mainly just because it involves vast numbers of sentient beings who are largely ignored by most of society. </p>\n</blockquote>\n<p>Wild animal advocacy is far more neglected than farmed animal advocacy, and it involves even larger numbers of sentient beings ignored by most of society. If the superiority of farmed animal advocacy over global poverty along these two dimensions is a sufficient reason for not working on global poverty, why isn't the superiority of wild animal advocacy over farmed animal advocacy along those same dimensions not also a sufficient reason for not working on farmed animal advocacy?</p>\n", "parentCommentId": "spvMxDuu3w5zTJ73Q", "user": {"username": "Pablo_Stafforini"}}, {"_id": "tE4c5CtszgMKGjqtq", "postedAt": "2018-02-22T14:53:57.524Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I personally don't think WAS is as similar to the most plausible far future dystopias, so I've been prioritizing it less even over just the past couple of years. I don't expect far future dystopias to involve as much naturogenic (nature-caused) suffering, though of course it's possible (e.g. if humans create large numbers of sentient beings in a simulation, but then let the simulation run on its own for a while, then the simulation could come to be viewed as naturogenic-ish and those attitudes could become more relevant).</p>\n<p>I think if one wants something very neglected, digital sentience advocacy is basically across-the-board better than WAS advocacy.</p>\n<p>That being said, I'm highly uncertain here and these reasons aren't overwhelming (e.g. WAS advocacy pushes on more than just the &quot;care about naturogenic suffering&quot; lever), so I think WAS advocacy is still, in Gregory's words, an important part of the 'far future portfolio.' And often one can work on it while working on other things, e.g. I think Animal Charity Evaluators' WAS content (e.g. ]guest blog post by Oscar Horta](<a href=\"https://animalcharityevaluators.org/blog/why-the-situation-of-animals-in-the-wild-should-concern-us/)\">https://animalcharityevaluators.org/blog/why-the-situation-of-animals-in-the-wild-should-concern-us/)</a>) has helped them be more well-rounded as an organization, and didn't directly trade off with their farmed animal content.</p>\n", "parentCommentId": "fawndMuCnaWLhEycB", "user": {"username": "Jacy"}}, {"_id": "s749PcmhJ5FrQ33La", "postedAt": "2018-02-22T18:49:53.603Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>AI designers, even if speciesist themselves, might nonetheless provide the right apparatus for value learning such that resulting AI will not propagate the moral mistakes of its creators</p>\n</blockquote>\n<p>This is something I also struggle with in understanding the post. it seems like we need:</p>\n<ol>\n<li>AI creators can be convinced to expand their moral circle</li>\n<li>Despite (1), they do not wish to be convinced to expand their moral circle</li>\n<li>The AI follows this second desire to not be convinced to expand their moral circle</li>\n</ol>\n<p>I imagine this happening with certain religious things; e.g. I could imagine someone saying &quot;I wish to think the Bible is true even if I could be convinced that the Bible is false&quot;. </p>\n<p>But it seems relatively implausible with regards to MCE?</p>\n<p>Particularly given that AI safety talks a lot about things like <a href=\"https://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">CEV</a>, it is unclear to me whether there is really a strong trade-off between MCE and AIA.</p>\n<p>(Note: Jacy and I discussed this via email and didn't really come to a consensus, so there's a good chance I am just misunderstanding his argument.)</p>\n", "parentCommentId": "Fw53moPFXdwMbrZay", "user": {"username": "Ben_West"}}, {"_id": "LDmYhHr3ACpkhhzc5", "postedAt": "2018-02-22T19:02:31.546Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Hm, yeah, I don't think I fully understand you here either, and this seems somewhat different than what we discussed via email.</p>\n<p>My concern is with (2) in your list. &quot;[T]hey do not wish to be convinced to expand their moral circle&quot; is extremely ambiguous to me. Presumably you mean they -- without MCE advocacy being done -- wouldn't put in wide-MC* values or values that lead to wide-MC into an aligned AI. But I think it's being conflated with, &quot;they actively oppose&quot; or &quot;they would answer 'no' if asked, 'Do you think your values are wrong when it comes to which moral beings deserve moral consideration?'&quot;</p>\n<p>I think they don't actively oppose it, they would mostly answer &quot;no&quot; to that question, and it's very uncertain if they will put the wide-MC-leading values into an aligned AI. I don't think CEV or similar reflection processes reliably lead to wide moral circles. I think they can still be heavily influenced by their initial set-up (e.g. what the values of humanity when reflection begins).</p>\n<p>This leads me to think that you only need (2) to be true in a very weak sense for MCE to matter. I think it's quite plausible that this is the case.</p>\n<p>*Wide-MC meaning an extremely wide moral circle, e.g. includes insects, small/weird digital minds.</p>\n", "parentCommentId": "s749PcmhJ5FrQ33La", "user": {"username": "Jacy"}}, {"_id": "C7edEqMsbo7oWbEr5", "postedAt": "2018-02-22T19:46:55.863Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thank you for providing an abstract for your article. I found it very helpful.</p>\n<p>(and I wish more authors here would do so as well)</p>\n", "parentCommentId": null, "user": {"username": "oge"}}, {"_id": "z6LvhRwi8ouivjagQ", "postedAt": "2018-02-22T21:23:57.496Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I tend to think of moral values as being pretty contingent and pretty arbitrary, such that what values you start with makes a big difference to what values you end up with even on reflection. People may &quot;imprint&quot; on the values they receive from their culture to a greater or lesser degree.</p>\n<p>I'm also skeptical that sophisticated philosophical-type reflection will have significant influence over posthuman values compared with more ordinary political/economic forces. I suppose philosophers have sometimes had big influences on human politics (religions, Marxism, the Enlightenment), though not necessarily in a clean &quot;carefully consider lots of philosophical arguments and pick the best ones&quot; kind of way.</p>\n", "parentCommentId": "s749PcmhJ5FrQ33La", "user": {"username": "Brian_Tomasik"}}, {"_id": "tEo48cpLiif97S3C8", "postedAt": "2018-02-22T22:31:06.988Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Please, <em>what</em> AIA organizations? MIRI? And do not worry about offending me. I do not intend to offend. If I do/did though my tone or however, I am sorry. </p>\n<p>That being said, I wish you would've examined the actual claims I presented. I did not claim AI researchers are worried about a malevolent AI. I am not against researchers; research in robotics, industrial PLCs, nanotech, whatever--are fields in their own right. It is donating my income, as an individual that I take offense. People can fund whatever they want: A new planetary wing at a museum, research in robotics, research in CS, research in CS philosophy. </p>\n<p>Although, Earning to Give does not follow. Thinking about and discussing the risks of strong AI does make sense, and we both seem to agree it is important. The CS grad students being supported, however, what makes them different from a random CS grad? Just because they claim to be researching AIA? Following the money, there is not a clear answer on which CS grad students are receiving it. <strong>Low or zero transparency.</strong> MIRI or no? Am I missing some public information?</p>\n<p>Second, what do you define as advanced AI? Before, I said strong AI. Is that what you mean? Is there some sort of AI in between? I'm not aware. This is crucially where I split with AI safety. The theory is an idea of a belief about the far future. To claim that we're close to developing strong AI is unfounded to me. What in this century is so close to strong AI? Neural networks do not seem to be (from my light research).</p>\n<p>I do not believe climate change is as simple to define a &quot;before&quot; and &quot;after.&quot; Perhaps a large rogue solar flair or the Yellowstone supervolcano. Or perhaps even a time travel analogy would suffice ~ time travel safety research. <strong>There is no tractability/solvability.</strong> [Blank] cannot be defined because it doesn't exist; unfounded and unknown phenomena cannot be solved. Climate change exists. It is a very real reality. It has solvability. A belief in an idea about the future is a poor reason for claiming some sort of tractability for funding. Strong AI safety (singularity safety) has &quot;solvability&quot; for thinking about and discussing--but, again, it does not follow that one should give monetarily. I feel like I'm beating a dead horse with this point.</p>\n<p>For the book recommendation, I looked into it. I'd rather read about morality/ethics directly or further delve into better learning Java, Python, Logix5000, LabVIEW, etc. </p>\n<p><a href=\"https://www.goodreads.com/review/show/2285250665?book_show_action=true&amp;from_review_page=1\">SE for</a></p>\n<p><a href=\"https://www.goodreads.com/review/show/1160979492?book_show_action=true&amp;from_review_page=1\">SE against</a></p>\n", "parentCommentId": "HZ47ZrwXv8nhAoARx", "user": {"username": "adamaero"}}, {"_id": "zEh5We37gjzRgTEvC", "postedAt": "2018-02-22T23:03:10.503Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Random thought: (factory farm) animal welfare issues will likely eventually be solved by cultured (lab grown) meat when it becomes cheaper than growing actual animals. This may take a few decades, but social change might take even longer. The article even suggests technical issues may be easier to solve, so why not focus more on that (rather than on MCE)?</p>\n", "parentCommentId": null, "user": {"username": "ateabug"}}, {"_id": "BAZz7L5qh8tuwPLTH", "postedAt": "2018-02-23T05:56:55.537Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>Please, what AIA organizations? MIRI?</p>\n</blockquote>\n<p>Yes, MIRI is one. FHI is another.</p>\n<blockquote>\n<p>That being said, I wish you would've examined the actual claims I presented. I did not claim AI researchers are worried about a malevolent AI.</p>\n</blockquote>\n<p>You did, however, say &quot;The theoretical threat of a malevolent strong AI would be immense. But that does not mean one has cause or a valid reason to support CS grad students financially.&quot; I assumed you meant that you believed someone was giving an argument along the lines of &quot;since malevolent AI is possible, then we should support CS grads.&quot; If that is not what you meant, then I don't see the relevance of mentioning malevolent AI.</p>\n<p>Since you also stated that you had an issue with me not being charitable, I would reciprocate likewise. I agree that we should be charitable to each other's opinions.</p>\n<p>Having truthful views is not about winning debate. It's about making sure that you hold good beliefs for good reasons, end of story. I encourage you to imagine this conversation not as a way to convince me that I'm wrong -- but more of a case study about what the current arguments are, and whether they are valid. In the end, you don't get points for winning an argument. You get points for actually holding correct views. </p>\n<p>Therefore, it's good to make sure that your beliefs actually hold weight under scrutiny. Not in a, &quot;you can't find the flaw after 10 minutes of self-sabotaged thinking&quot; sort of way, but in a very deep understanding sort of way. </p>\n<blockquote>\n<p>It is donating my income, as an individual that I take offense. People can fund whatever they want: A new planetary wing at a museum, research in robotics, research in CS, research in CS philosophy.</p>\n</blockquote>\n<p>I agree people <em>can</em> fund whatever they want. It's important to make a distinction between normative questions and factual ones. It's true that people can fund whatever project they like; however, it's also true that some projects have a high value from an impersonal utilitarian perspective. It is this latter category that I care about, which is why I want to find projects with particular high value. I believe that existential risk mitigation and AI alignment is among these projects, although I fully admit that I may be mistaken. </p>\n<blockquote>\n<p>Although, Earning to Give does not follow. Thinking about and discussing the risks of strong AI does make sense, and we both seem to agree it is important. </p>\n</blockquote>\n<p>If you agree that thinking about something is valuable, why not also agree that funding that thing is valuable. It seems you think that the field should just get a certain threshold of funding that allows certain people to think about the problem just enough -- but not too much. I don't a reason to believe that the field of AI alignment has reached that critical threshold. On the contrary, I believe the field is far from it at the moment. </p>\n<blockquote>\n<p>Following the money, there is not a clear answer on which CS grad students are receiving it. Low or zero transparency. MIRI or no? Am I missing some public information?</p>\n</blockquote>\n<p>I suppose when you make a donation to MIRI, it's true that you can't be certain about how they spend that money (although I might be wrong about this, I haven't actually donated to MIRI). Generally though, funding an organization is about whether you think that their mission is neglected, and whether you think that further money would make a marginal impact in their cause area. This is no different than any other charity that EA aligned people endorse.</p>\n<blockquote>\n<p>Second, what do you define as advanced AI? Before, I said strong AI. Is that what you mean? Is there some sort of AI in between? I'm not aware. </p>\n</blockquote>\n<p>It might be confusing that there are all these terms for AI. To <a href=\"http://lesswrong.com/lw/nu/taboo_your_words/\">taboo</a> the words &quot;advanced AI&quot;, &quot;strong AI&quot;, &quot;AGI&quot; or others -- what I am worried about is an information processing system that can achieve broad success in cognitive tasks in a way that rivals or surpasses humans. I hope that makes it clear. </p>\n<blockquote>\n<p>This is crucially where I split with AI safety. The theory is an idea of a belief about the far future. To claim that we're close to developing strong AI is unfounded to me.</p>\n</blockquote>\n<p>I'm not quite clear what you mean here. If you mean we are worried about AI in the far future, fine. But then in the next sentence you say that we're worried about being close to strong AI. How can we simultaneously believe both. <em>If</em> AI is near then I care about the near-term future. <em>If</em> AI is not near, then I care about the long-term future. I do not claim either, however. I think it is important consideration even if it's a long way off. </p>\n<blockquote>\n<p>Neural networks do not seem to be (from my light research).</p>\n</blockquote>\n<p>This is what I'm referring to when I talk about how important it is to really, truly understand something before developing an informed opinion about it. If you admit that you have only done light research, how can you be confident that you are right. Doing a bit of research might give you an edge for debate purposes, but we are talking about the future of life on Earth here. We <em>really</em> need to know the answers to these questions. </p>\n<blockquote>\n<p>Perhaps a large rogue solar flair or the Yellowstone supervolcano. Or perhaps even a time travel analogy would suffice ~ time travel safety research. There is no tractability/solvability. </p>\n</blockquote>\n<p>Lumping all existential risks in a single category and then asserting that there's no tractability is a simplified approach. First what we need is the probability of any given existential risk occurring. For instance, if scientists discovered that the Yellowstone supervolcano was probably about to erupt sometime in the next few centuries, I'd definitely agree we should do research in that area, and we should fund that research as well. In fact, some <a href=\"http://effective-altruism.com/ea/1jp/paper_interventions_that_may_prevent_or_mollify/\">research is being done in that area</a> and I'm happy that it's being done. </p>\n<blockquote>\n<p>A belief in an idea about the future is a poor reason for claiming some sort of tractability for funding.</p>\n</blockquote>\n<p>I'd agree with you if it was an idea asserted without evidence or reason. But there's a whole load of arguments about why it is a tractable field, and how we can do things now -- yes right now -- about making the future safer. Ignorance of these arguments does not mean they do not exist.</p>\n<p>Remember, ask yourself first what is true. Then form your opinion. Do not go the other way.</p>\n", "parentCommentId": "tEo48cpLiif97S3C8", "user": {"username": "Matthew_Barnett"}}, {"_id": "YoDeWfPuoiYnjBWFu", "postedAt": "2018-02-23T16:20:42.917Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I just took it as an assumption in this post that we're focusing on the far future, since I think basically all the theoretical arguments for/against that have been made elsewhere. <a href=\"https://ea-foundation.org/blog/the-importance-of-the-far-future/\">Here's a good article on it.</a> I personally mostly focus on the far future, though not overwhelmingly so. I'm at something like 80% far future, 20% near-term considerations for my cause prioritization decisions.</p>\n<blockquote>\n<p>This may take a few decades, but social change might take even longer.</p>\n</blockquote>\n<p>To clarify, the post isn't talking about ending factory farming. And I don't think anyone in the EA community thinks we should try to end factory farming without technology as an important component. Though I think there are good reasons for EAs to focus on the social change component, e.g. there is less for-profit interest in that component (most of the tech money is from for-profit companies, so it's less neglected in this sense).</p>\n", "parentCommentId": "zEh5We37gjzRgTEvC", "user": {"username": "Jacy"}}, {"_id": "rWmfkvyG43G3yTwKm", "postedAt": "2018-02-23T19:50:20.941Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Isn't hedonium inherently as good as dolorium is bad? If it's not, can't we just normalize and then treat them as the same? I don't understand the point of saying there will be more hedonium than dolorium in the future, but the dolorium will matter more. They're vague and made-up quantities, so can't we just set it so that &quot;more hedonium than dolorium&quot; implies &quot;more good than bad&quot;?</p>\n", "parentCommentId": "MnRHMwdFEmDK436FL", "user": {"username": "zdgroff"}}, {"_id": "BtFpWtYEqGMz8DdK8", "postedAt": "2018-02-23T20:09:36.066Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>He defines hedonium/dolorium as the maximum positive/negative utility you can generate with a certain amount of energy:</p>\n<p>&quot;For example, I think a given amount of dolorium/dystopia (say, the amount that can be created with 100 joules of energy) is far larger in absolute moral expected value than hedonium/utopia made with the same resources.&quot;</p>\n", "parentCommentId": "rWmfkvyG43G3yTwKm", "user": {"username": "MetricSulfateFive"}}, {"_id": "9WSuBufet4ToME4Xt", "postedAt": "2018-02-23T20:36:18.162Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p>I think there\u2019s a significant[8] chance that the moral circle will fail to expand to reach all sentient beings, such as artificial/small/weird minds (e.g. a sophisticated computer program used to mine asteroids, but one that doesn\u2019t have the normal features of sentient minds like facial expressions). In other words, I think there\u2019s a significant chance that powerful beings in the far future will have low willingness to pay for the welfare of many of the small/weird minds in the future.[9]</p>\n</blockquote>\n<blockquote>\n<p>I think it\u2019s likely that the powerful beings in the far future (analogous to humans as the powerful beings on Earth in 2018) will use large numbers of less powerful sentient beings</p>\n</blockquote>\n<p>So I'm curious for your thoughts.  I see this concern about &quot;incidental suffering of worker-agents&quot; stated frequently, which may be likely in many future scenarios.  <em>However, it doesn't seem to be a crucial consideration</em>, specifically <em>because</em> I care about small/weird minds with non-complex experiences (your first consideration).</p>\n<p>Caring about small minds seems to imply that &quot;Opportunity Cost/Lost Risks&quot; are the dominate consideration - if small minds have moral value comparable to large minds, then the largest-EV risk is <em>not optimizing for small minds</em> and wasting resources thrown at large minds with complex/expensive experiences (or thrown at something even less efficient, like biological beings, any non-total-consequentialist view, etc).  This would you lose you many orders of magnitude of optimized happiness, and this loss would be worse than the other scenarios' aggregate incidental suffering.\n  Even if this inefficient moral position merely reduced optimized happiness by 10% - far less than an order of magnitude - this would dominate incidental suffering, even if the incidental suffering scenarios were significantly more probable.  And even if you very heavily weight suffering compared to happiness, my math still suggests this conclusion survives by a significant margin).</p>\n<p>Also note that Moral Circle Expansion is relevant conditional on solving the alignment problem, so we're in the set of worlds where the alignment problem was actually solved in some way (humanity's values are somewhat intact).  So, the risk is that whatever-we're-optimizing-the-future-for is far less efficient than ideal hedonium could have been, because we're wasting it on complex minds, experiences that require lots of material input, or other not-efficiently-value-creating things.  &quot;Oh, what might have been&quot;, etc.  Note this still says values spreading might be very important, but I think this version has a slightly different flavor that implies somewhat different actions.  Thoughts?</p>\n", "parentCommentId": null, "user": {"username": "nonn"}}, {"_id": "vY4qyj8rkZ2cic6CS", "postedAt": "2018-02-23T21:44:58.663Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I am not trying to &quot;win&quot; anything. I am stating why MIRI is not transparent, and does not deal in scalable issues. As an individual, Earning to Give, it does not follow to fund such things under the guise of Effective Altruism. Existential risk is important to think about and discuss as individuals. However, funding CS grad students <a href=\"https://rationalwiki.org/wiki/Effective_altruism#Mosquito_nets_versus_AI_risk\">does not make sense in the light of Effective Altruism</a>.</p>\n<p>Funding does not increase &quot;thinking.&quot; The whole point of EA is to not give blindly. For example, giving food aid, although meaning well, can have a very negative effect (i.e., the crowding out effect on the local market). Nonmaleficence should be one's initial position in regards to funding. </p>\n<p>Lastly, no I rarely accept something as true first. I do not first accept the null hypothesis. &quot;But there's a whole load of arguments about why it is a tractable field&quot;--What are they? Again, none of the actual arguments were examined: How is MIRI going about tractable/solvable issues? Who of MIRI is getting the funds? How is time travel safety not as relevant as AI safety?</p>\n", "parentCommentId": "BAZz7L5qh8tuwPLTH", "user": {"username": "adamaero"}}, {"_id": "9mwDJfRS9YM6toFhC", "postedAt": "2018-02-24T01:07:19.715Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for writing it. </p>\n<p>Here are my reasons for the belief wild animal/small minds/... suffering agenda is based mostly on errors and uncertainties. Some of the uncertainties should warrant research effort, but I do not believe the current state of knowledge justifies prioritization ofany kind of advocacy or value spreading.</p>\n<p>1] The endeavour seems to be based on extrapolating intuitive models far outside the scope for which we have data. The whole suffering calculus is based on extrapolating the concept of suffering far away from the domain for which we have data from human experience. </p>\n<p>2] Big part of it seems arbitrary. When expanding the moral circle toward small computational processes and simple systems, why not expand it toward large computational processes and complex systems? E.g. we can think about the DNA based evolution as about large computational/optimization process - suddenly &quot;wild animal suffering&quot; has a purpose and traditional environmnet and biodiversity protection efforts make sense.</p>\n<p>(Similarly we could argue much &quot;human utility&quot; is in the larger system structure above individual humans)</p>\n<p>3] We do not know how to measure and aggregate utility of mind states. Like, really don't know. E.g. it sems to me completely plausible the utility of 10 people reaching some highly joyful mindstates is the dominanat contribution over all human and animal minds.</p>\n<p>4] Part of the reasoning usually seems contradictory. If the human cognitive processes are in the priviledged position of creating meaning in this universe ... well, then they are in the priviledged postion, and there _is_ a categorical difference between humans and other minds. If they are not in the priviledged positions, how it comes humans should impose their ideas about meaning on other agents?</p>\n<p>5] MCE efforts directed toward AI researchers with the intent of influencing values of some powerful AI may increase x-risk. E.g. if the AI is not &quot;speciist&quot; and gives the same weight to satysfing prefrences of all humans and all chicken, the chicken would outnumber humans. </p>\n", "parentCommentId": null, "user": {"username": "Jan_Kulveit"}}, {"_id": "a2w6YnKrNiGnTkXS5", "postedAt": "2018-02-24T11:28:05.058Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for this discussion, which I find quite interesting. I think the <em>effectiveness</em> and <em>efficiency</em> of funding research projects concerning risks of AI is a largely neglected topic. I've posted some concerns on this below an older thread on MIRI: <a href=\"http://effective-altruism.com/ea/14c/why_im_donating_to_miri_this_year/dce\">http://effective-altruism.com/ea/14c/why_im_donating_to_miri_this_year/dce</a></p>\n<ul>\n<li>the primary problem being the lack of transparency on the side of Open Phil. concerning the evaluative criteria used in their decision to award MIRI with an extremely huge grant.</li>\n</ul>\n", "parentCommentId": "vY4qyj8rkZ2cic6CS", "user": {"username": "Dunja"}}, {"_id": "cWi4c4NEjQiX8oMg4", "postedAt": "2018-02-24T16:56:26.179Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>You raise some good points. (The following reply doesn't necessarily reflect Jacy's views.)</p>\n<p>I think the answers to a lot of these issues are somewhat arbitrary matters of moral intuition. (As you said, &quot;Big part of it seems arbitrary.&quot;) However, in a sense, this makes MCE more important rather than less, because it means expanded moral circles are not an inevitable result of better understanding consciousness/etc. For example, Yudkowsky's stance on consciousness is a reasonable one that is not based on a mistaken understanding of present-day neuroscience (as far as I know), yet some feel that Yudkowsky's view about moral patienthood isn't wide enough for their moral tastes.</p>\n<p>Another possible reply (that would sound better in a political speech than the previous reply) could be that MCE aims to spark discussion about these hard questions of what kinds of minds matter, without claiming to have all the answers. I personally maintain significant moral uncertainty regarding how much I care about what kinds of minds, and I'm happy to learn about other people's moral intuitions on these things because my own intuitions aren't settled.</p>\n<blockquote>\n<p>E.g. we can think about the DNA based evolution as about large computational/optimization process - suddenly &quot;wild animal suffering&quot; has a purpose and traditional environmnet and biodiversity protection efforts make sense.</p>\n</blockquote>\n<p>Or <a href=\"http://briantomasik.com/suffering-focused-ecocentrism/\">if we take</a> a suffering-focused approach to these large systems, then this could provide a further argument against environmentalism. :)</p>\n<blockquote>\n<p>If the human cognitive processes are in the priviledged position of creating meaning in this universe ... well, then they are in the priviledged postion, and there is a categorical difference between humans and other minds.</p>\n</blockquote>\n<p>I selfishly consider my moral viewpoint to be &quot;privileged&quot; (in the sense that I prefer it to other people's moral viewpoints), but this viewpoint can have in its content the desire to give substantial moral weight to non-human (and human-but-not-me) minds.</p>\n", "parentCommentId": "9mwDJfRS9YM6toFhC", "user": {"username": "Brian_Tomasik"}}, {"_id": "k6pGk5aj45Z9DAhWF", "postedAt": "2018-02-25T00:12:38.810Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>My current position is that the amount of pleasure/suffering that conscious entities will experience in a far-future technological civilization will not be well-defined. Some arguments for this:</p>\n<ol>\n<li><p>Generally utility functions or reward functions are invariant under affine transformations (with suitable rescaling for the learning rate for reward functions). Therefore they cannot be compared between different intelligent agents as a measure of pleasure.</p>\n</li>\n<li><p>The clean separation of our civilization into many different individuals is an artifact of how evolution operates. I don't expect far future civilization to have a similar division of its internal processes into agents. Therefore the method of counting conscious entities with different levels of pleasure is inapplicable.</p>\n</li>\n<li><p>Theoretical computer science gives many ways to embed one computational process within another so that it is unclear whether or how many times the inner process &quot;occurs&quot;, such as running identical copies of the same program, using a quantum computer to run the same program with many inputs in superposition, and homomorphic encryption. Similar methods we don't know about will likely be discovered in the future.</p>\n</li>\n<li><p>Our notions of pleasure and suffering are mostly defined extensionally with examples from the present and the past. I see no reason that such an extensionally-derived concept to have a natural definition that applies to extremely different situations. Uncharitably, it seems like the main reason people assume this is a sort of wishful thinking due to their normal moral reasoning breaking down if they allow pleasure/suffering to be undefined.</p>\n</li>\n</ol>\n<p>I'm currently uncertain about how to make decisions relating to the far future in light of the above arguments. My current favorite position is to try to understand the far future well enough until I find something I have strong moral intuitions about.</p>\n", "parentCommentId": null, "user": {"username": "itaibn"}}, {"_id": "M8W7CkKMSovrcYNKu", "postedAt": "2018-02-25T00:57:22.798Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for this post.  Some scattered thoughts:</p>\n<blockquote>\n<p>The main risk for AIA seems to be that the technical research done to better understand how to build an aligned AI will increase AI capabilities generally, meaning it\u2019s also easier for humanity to produce an unaligned AI.</p>\n</blockquote>\n<p>This doesn't seem like a big consideration to me.  Even if unfriendly AI comes sooner by an entire decade, this matters little on a <a href=\"https://waitbutwhy.com/2013/08/putting-time-in-perspective.html\">cosmic timescale</a>.  An argument I find more compelling: If we plot the expected utility of an AGI as a function of the amount of effort put into aligning it, there might be a &quot;valley of bad alignment&quot; that is worse than no attempt at alignment at all.  (A paperclip maximizer will quickly kill us and not generate much long-term suffering, whereas an AI that understands the importance of human survival but doesn't understand any other values will imprison us for all eternity.  Something like that.)</p>\n<p>I'd like to know more about why people think that our moral circles have expanded.  I suspect activism plays a <a href=\"https://en.wikipedia.org/wiki/Great_man_theory\">smaller role than you think</a>.  Steven Pinker talks about possible reasons for declining violence in his book <em>The Better Angels of Our Nature</em>.  I'm guessing this is highly related to moral circle expansion.</p>\n<p>One theory I haven't seen elsewhere is that self-interest plays a big role in moral circle expansion.  Consider the example of slavery.  The BBC <a href=\"http://www.bbc.co.uk/history/british/abolition/royal_navy_article_01.shtml\">writes</a>:</p>\n<blockquote>\n<p>It becomes clear that humanitarianism and imperial muscling were able bedfellows...</p>\n</blockquote>\n<blockquote>\n<p>One can be certain that the high ideals of abolition and the promotion of legitimate trade were equally matched by economic and territorial ambitions, impulses which brought forward partition and colonial rule in Africa in the late 19th century.</p>\n</blockquote>\n<p>You'll note that the villains of the slave story are the slavers--people with an interest in slavery.  The heroes seem to have been Britons who would not lose much if slavery was outlawed (though I guess boycotting sugar would go against their self-interest?)  Similarly, I think I remember reading that poor northern whites were motivated to fight in the US Civil War because they were worried their labor would be displaced by slave labor.</p>\n<p>According to this story, the expanding circle is a side effect of the world growing wealthier.  As lower levels of Maslow's hierarchy are met, people care more about humanitarian issues.  (I'm assuming that genetic relatedness predicts where on the hierarchy another being falls.)  Conquest is less common now because it's more profitable to control a multinational company than control lots of territory.  Slavery is less common because unskilled laborers are less of an asset &amp; more of a liability, and it's hard to coerce skilled labor.  Violence has declined because <a href=\"http://www.overcomingbias.com/2009/09/this-is-the-dream-time.html\">sub-replacement fertility</a> means we're no longer in a zero-sum competition for resources.  (Note that the bloodiest war in recent memory happened in the Democratic Republic of Congo, a country where women average six children each--<a href=\"https://www.economist.com/news/briefing/21737021-president-joseph-kabila-seventh-year-five-year-term-he-struggling-hold\">source</a>.  Congo has a lot of mineral wealth, which seems to incentivize conflict.  Probably this wealth doesn't diminish as much in the presence of conflict as much as e.g. manufacturing wealth would.)</p>\n<p>I suppose a quick test for the Maslow's hierarchy story is to check whether wealthy people are more likely to be vegan (controlling for meat calories being as expensive as non-meat calories).</p>\n<p>I don't think everyone is completely self-interested all the time, but I think people are self-interested enough that it makes sense for activists to apply leverage strategically.</p>\n<p>Re: a computer program used to mine asteroids, I'd expect <a href=\"http://lesswrong.com/lw/x4/nonperson_predicates/\">certain</a> AI alignment work to be useful here.  If we understand AI algorithms more deeply, an asteroid miner can be simpler and less likely sentient.  Contrast with the scenario where AI progress is slow, brain emulations come before AGI, and the asteroid miner is piloted using an emulation of someone's brain.</p>\n<p>I'm not comfortable relying on innate human goodness to deal with moral dilemmas.  I'd rather eliminate incentives for immoral behavior.  In the presence of bad incentives, I worry about activism backfiring as people come up with rationalizations for their immoral behavior.  See e.g. biblical justifications for slavery in the antebellum south.  Instead of seeing the EA movement as something that will sweep the globe and make everyone altruistic, I'm more inclined to see it as a team of special forces working to adjust the incentives that everyone else operates under in order to create good outcomes as a side effect of everyone else working towards their incentives.</p>\n<p>Another moral circle expansion story involves <a href=\"https://psmag.com/social-justice/bugs-like-made-germ-theory-democracy-beliefs-73958\">improved hygiene</a>.  <a href=\"https://www.vox.com/2015/5/27/8660249/gates-flu-pandemic\">See also</a>.</p>\n<p>Singer and Pinker talk a lot about the importance of reason and empathy to the expanding moral circle.  This might be achieved through better online discussion platforms, widespread adoption of meditation, etc.</p>\n<p>Anyway, I think that if we take a broad view of moral circle expansion, the best way to achieve it might be some unexpected thing: improving the happiness of voters who control nuclear weapons, helping workers deal with technological job displacement, and so on.  IMO, more EAs should work on <a href=\"https://www.economist.com/news/leaders/21735586-how-shifts-technology-and-geopolitics-are-renewing-threat-growing-danger\">world</a> <a href=\"https://www.economist.com/news/special-report/21735480-great-powers-seem-have-little-appetite-full-scale-war-there-room\">peace</a>.</p>\n", "parentCommentId": null, "user": {"username": "John_Maxwell_IV"}}, {"_id": "AvPw49pXgaFCtS87g", "postedAt": "2018-02-25T05:30:04.149Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Possibly the biggest unknown in ethics is whether bits matter, or whether atoms matter.</p>\n<p>If you assume bits matter, then I think this naturally leads into a concept cluster where speaking about utility functions, preference satisfaction, complexity of value, etc, makes sense. You also get a lot of weird unresolved thought-experiments like homomorphic encryption.</p>\n<p>If you assume atoms matter, I think this subtly but unavoidably leads to a very different concept cluster-- qualia turns out to be a natural kind instead of a leaky reification, for instance. Talking about the 'unity of value thesis' makes more sense than talking about the 'complexity of value thesis'.</p>\n<p>TL;DR: I think you're right that if we assume computationalism/functionalism is true, then pleasure and suffering are inherently ill-defined, not crisp. They do seem well-definable if we assume physicalism is true, though.</p>\n", "parentCommentId": "k6pGk5aj45Z9DAhWF", "user": {"username": "MikeJohnson"}}, {"_id": "NncuJW7Bz7rg8Ft2F", "postedAt": "2018-02-25T12:13:38.751Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for reminding me that I was implicitly assuming computationalism. Nonetheless, I don't think physicalism substantially affects the situation. My arguments #2 and #4 stand unaffected; you have not backed up your claim that qualia is a natural kind under physicalism. While it's true that physicalism gives clear answers for the value of two identical systems or a system simulated with homomorphic encryption, it may still be possible to have quantum computations involving physically instantiated conscious beings, by isolating the physical environment of this being and running the <a href=\"https://en.wikipedia.org/wiki/CPT_symmetry\">CPT reversal</a> of this physical system after an output has been extracted to maintain coherence. Finally, physicalism adds its own questions, namely, given a bunch of physical systems that all appear to have behavior that appears to be conscious, which ones are actually conscious and which are not. If I understood you correctly, physicalism as a statement about consciousness is primary a negative statement, &quot;the computational behavior of a system is not sufficient to determine what sort of conscious activity occurs there&quot;, which doesn't by itself tell you what sort of conscious activity occurs.</p>\n", "parentCommentId": "AvPw49pXgaFCtS87g", "user": {"username": "itaibn"}}, {"_id": "AHvuTaJkbkbGEsz29", "postedAt": "2018-02-25T16:57:42.891Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Impressive article - I especially liked the biases section. I would recommend doing a quantitative model of cost effectiveness comparing to AIA, as I have done for <a href=\"http://effective-altruism.com/ea/1g9/should_we_be_spending_no_less_on_alternate_foods/\">global agricultural catastrophes</a>, especially because neglectedness is hard to define in your case.</p>\n", "parentCommentId": null, "user": {"username": "Denkenberger"}}, {"_id": "EBDrh7q2KjmX6k6kf", "postedAt": "2018-02-25T20:46:38.823Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>It seems to me your #2 and #4 still imply computationalism and/or are speaking about a straw man version of physicalism. Different physical theories will address your CPT reversal objection differently, but it seems pretty trivial to me.</p>\n<blockquote>\n<p>If I understood you correctly, physicalism as a statement about consciousness is primary a negative statement, &quot;the computational behavior of a system is not sufficient to determine what sort of conscious activity occurs there&quot;, which doesn't by itself tell you what sort of conscious activity occurs.</p>\n</blockquote>\n<p>I would generally agree, but would personally phrase this differently; rather, as noted <a href=\"http://effective-altruism.com/ea/1cn/why_i_think_the_foundational_research_institute/\">here</a>, there is no objective fact-of-the-matter as to what the 'computational behavior' of a system <em>is</em>. I.e., no way to objectively derive what computations a physical system is performing. In terms of a positive statement about physicalism &amp; qualia, I'm assuming something on the order of dual-aspect monism / neutral monism. And yes insofar as a formal theory of consciousness which has broad predictive power would depart from folk intuition, I'd definitely go with the formal theory.</p>\n", "parentCommentId": "NncuJW7Bz7rg8Ft2F", "user": {"username": "MikeJohnson"}}, {"_id": "dmJPeXsguRtjAs34t", "postedAt": "2018-02-26T01:50:05.743Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for the link. I didn't think to look at what other posts you have published and now I understand your position better.</p>\n<p>As I now see it, there two critical questions for distinguishing the different positions on the table:</p>\n<ol>\n<li>Does our intuitive notion of pleasure/suffering have objective precisely defined fundamental concept underlying it?</li>\n<li>In practice, is it a useful approach to look for computational structures exhibiting pleasure/suffering in the distant future as a means to judge possible outcomes?</li>\n</ol>\n<p>Brian Tomasik answers these questions &quot;No/Yes&quot;, and a supporter of the Sentience Institute would probably answer &quot;Yes&quot; to the second question. Your answers are &quot;Yes/No&quot;, and so you prefer to work on finding the underlying theory for pleasure/suffering. My answers are &quot;No/No&quot;, and am at a loss.</p>\n<p>I see two reasons why a person might think that pleasure/pain of conscious entities is a solid enough concept to answer &quot;Yes&quot; to either of these questions (not counting conservative opinions over what futures are possible for question 2). The first is a confusion caused by subtle implicit assumptions in the way we talk about consciousness, which makes a sort of conscious experience from which includes in it pleasure and pain seem more ontologically basic than it really is. I won't elaborate on this in this comment, but for now you can round me as an eliminativist.</p>\n<p>The second is what I was calling &quot;a sort wishful thinking&quot; in argument #4: These people have moral intuitions that tell them to care about others' pleasure and pain, which implies not fooling themselves about how much pleasure and pain others experience. On the other hand, there are many situations where their intuition does not give them a clear answer, but also tells them that picking an answer arbitrarily is like fooling themselves. They resolve this tension by telling themselves, &quot;there is a 'correct answer' to this dilemma, but I don't know what it is. I should act to best approximate this 'correct answer' with the information I have.&quot; People then treat these &quot;correct answers&quot; like other things they are ignorant about, and in particular imagine that a scientific theory might be able to answer these questions in the same way science answered other things we used to be ignorant about.</p>\n<p>However, this expectation infers something external, the existence of a certain kind of scientific theory, from evidence that is internal, their own cognitive tensions. This seems fallacious to me.</p>\n", "parentCommentId": "EBDrh7q2KjmX6k6kf", "user": {"username": "itaibn"}}, {"_id": "JbssgoY74eZbox6dc", "postedAt": "2018-02-26T03:35:03.691Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks, this is helpful. My general position on your two questions is indeed &quot;Yes/No&quot;.</p>\n<p>The question of 'what are reality's natural kinds?' is admittedly complex and there's always room for skepticism. That said, I'd suggest the following alternatives to your framing:</p>\n<ul>\n<li><p>Whether the existence of qualia itself is 'crisp' seems prior to whether pain/pleasure are. I call this the <a href=\"https://qualiaresearchinstitute.org/2018/02/17/the-problems-of-consciousness-a-taxonomy/\">'real problem'</a> of consciousness.</p>\n</li>\n<li><p>I'm generally a little uneasy with discussing pain/pleasure in technically precise contexts- I prefer 'emotional valence'.</p>\n</li>\n<li><p>Another reframe to consider is to disregard talk about pain/pleasure, and instead focus on whether <em>value</em> is well-defined on physical systems (i.e. the subject of Tegmark's worry <a href=\"https://arxiv.org/abs/1409.0813\">here</a>). Conflation of emotional valence &amp; moral value can then be split off as a subargument.</p>\n</li>\n</ul>\n<p>Generally speaking, I think if one accepts that it's possible in principle to talk about qualia in a way that 'carves reality at the joints', it's not much of a stretch to assume that emotional valence is one such natural kind (arguably the '<em>c. elegans</em> of qualia'). I don't think we're logically forced to assume this, but I think it's <em>prima facie</em> plausible, and paired with some of our other work it gives us a handhold for approaching qualia in a scientific/predictive/falsifiable way.</p>\n<p>Essentially, QRI has used this approach to bootstrap <a href=\"https://qualiacomputing.com/2017/06/18/quantifying-bliss-talk-summary/\">the world's first method for quantifying emotional valence in humans from first principles, based on fMRI scans</a>. (It also should work for most non-human animals; it's just harder to validate in that case.) We haven't yet done the legwork on connecting future empirical results here back to the computationalism vs physicalism debate, but it's on our list.</p>\n<p>TL;DR: If consciousness is a 'crisp' thing with discoverable structure, we should be able to build/predict useful things with this that cannot be built/predicted otherwise, similar to how discovering the structure of electromagnetism let us build/predict useful things we could not have otherwise. This is probably the best route to solve these metaphysical disagreements.</p>\n", "parentCommentId": "dmJPeXsguRtjAs34t", "user": {"username": "MikeJohnson"}}, {"_id": "3xNpdvWxfAhzCTbnM", "postedAt": "2018-02-26T03:50:12.437Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<blockquote>\n<p> I don't think CEV or similar reflection processes reliably lead to wide moral circles. I think they can still be heavily influenced by their initial set-up (e.g. what the values of humanity when reflection begins).</p>\n</blockquote>\n<p>Why do you think this is the case?\nDo you think there is an alternative reflection process (either implemented by an AI, by a human society, or combination of both) that could be defined that would reliably lead to wide moral circles? Do you have any thoughts on what would it look like?</p>\n<p>If we go through some kind of reflection process to determine our values, I would much rather have a reflection process that wasn't dependent on whether or not MCE occurred before hand, and I think not leading to a wide moral circle should be considered a serious bug in any definition of a reflection process. It seems to me that working on producing this would be a plausible alternative or at least parallel path to directly performing MCE.</p>\n", "parentCommentId": "LDmYhHr3ACpkhhzc5", "user": {"username": "William_S"}}, {"_id": "EAKpmHxtBjf4tsAe4", "postedAt": "2018-02-26T07:23:25.883Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I think that there's an inevitable tradeoff between wanting a reflection process to have certain properties and worries about this violating goal preservation for at least some people. <a href=\"https://foundational-research.org/reply-thomas-metzingers-baan-thought-experiment/\">This blogpost</a> is not about MCE directly, but if you think of &quot;BAAN thought experiment&quot; as &quot;we do moral reflection and the outcome is such a wide circle that most people think it is extremely counterintuitive&quot; then the reasoning in large parts of the blogpost should apply perfectly to the discussion here. </p>\n<p>That is not to say that trying to fine tune reflection processes is pointless: I think it's very important to think about what our desiderata should be for a CEV-like reflection process. I'm just saying that there will be tradeoffs between certain commonly mentioned desiderata that people don't realize are there because they think there is such a thing as &quot;genuinely free and open-ended deliberation.&quot;</p>\n", "parentCommentId": "3xNpdvWxfAhzCTbnM", "user": {"username": "Lukas_Gloor"}}, {"_id": "HKSsW8xMYuKCBZXGk", "postedAt": "2018-02-27T01:01:56.364Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>But humanity/AI is likely to expand to other planets. Won't those planets need to have complex ecosystems that could involve a lot of suffering? Or do you think it will all be done with some fancy tech that'll be too different from today's wildlife for it to be relevant? It's true that those ecosystems would (mostly?) be non-naturogenic but I'm not that sure that people would care about them, it'd still be animals/diseases/hunger.etc. hurting animals. Maybe it'd be easier to engineer an ecosystem without predation and diseases but that is a non-trivial assumption and suffering could then arise in other ways. </p>\n<p>Also, some humans want to spread life to other planets for its own sake and relatively few people need to want that to cause a lot of suffering if no one works on preventing it.</p>\n<p>This could be less relevant if you think that most of the expected value comes from simulations that won't involve ecosystems.</p>\n", "parentCommentId": "tE4c5CtszgMKGjqtq", "user": {"username": "saulius"}}, {"_id": "AWzYxiwg3gYhGcjN2", "postedAt": "2018-02-27T17:29:35.668Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks for commenting, Lukas. I think Lukas, Brian Tomasik, and others affiliated with FRI have thought more about this, and I basically defer to their views here, especially because I haven't heard any reasonable people disagree with this particular point. Namely, I agree with Lukas that there seems to be an inevitable tradeoff here.</p>\n", "parentCommentId": "EAKpmHxtBjf4tsAe4", "user": {"username": "Jacy"}}, {"_id": "nnM7M9hgwdrEjYtS4", "postedAt": "2018-02-27T17:32:07.211Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I'd qualify this by adding that the philosophical-type reflection seems to lead in expectation to more moral value (positive or negative, e.g. hedonium or dolorium) than other forces, despite overall having less influence than those other forces.</p>\n", "parentCommentId": "z6LvhRwi8ouivjagQ", "user": {"username": "Jacy"}}, {"_id": "ZcPyvR86qm79PsfGv", "postedAt": "2018-02-27T17:35:03.070Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Yes, terraforming is a big way in which close-to-WAS scenarios could arise. I do think it's smaller in expectation than digital environments that develop on their own and thus are close-to-WAS.</p>\n<p>I don't think terraforming would be done very differently than today's wildlife, e.g. done without predation and diseases.</p>\n<p>Ultimately I still think the digital, not-close-to-WAS scenarios seem much larger in expectation.</p>\n", "parentCommentId": "HKSsW8xMYuKCBZXGk", "user": {"username": "Jacy"}}, {"_id": "6r9DyiCSkbRou5gbs", "postedAt": "2018-02-27T17:36:12.463Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Exactly. Let me know if this doesn't resolve things, zdgroff.</p>\n", "parentCommentId": "BtFpWtYEqGMz8DdK8", "user": {"username": "Jacy"}}, {"_id": "dQHczoTREkYBgQwvv", "postedAt": "2018-02-27T23:05:45.290Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>It wasn't clear to me from your comment, but based on your link I am presuming that by &quot;crisp&quot; you mean &quot;amenable to generalizable scientific theories&quot; (rather than &quot;ontologically basic&quot;). I was using &quot;pleasure/pain&quot; as a catch-all term and would not mind substituting &quot;emotional valence&quot;.</p>\n<p>It's worth emphasizing that just because a particular feature is crisp does not imply that it generalizes to any particular domain in any particular way. For example, a single ice crystalline has a set of directions in which the molecular bonds are oriented which is the same throughout the crystal, and this surely qualifies as a &quot;crisp&quot; feature. Nonetheless, when the ice melts, this feature becomes undefined -- no direction is distinguished from any other direction in water. When figuring out whether a concept from one domain extends to a new domain, to posit that there's a crisp theory describing the concept does not answer this question without any information on what that theory looks like.</p>\n<p>So even if there existed a theory describing qualia and emotional valence as it exists on Earth, it need not extend to being able to describe every physically possible arrangement of matter, and I see no reason to expect it to. Since a far future civilization will be likely to approach the physical limits of matter in many ways, we should not assume that it is not one such arrangement of matter where the notion of qualia is inapplicable.</p>\n", "parentCommentId": "JbssgoY74eZbox6dc", "user": {"username": "itaibn"}}, {"_id": "Hhh2aXFNToQCgQD79", "postedAt": "2018-02-28T18:05:47.988Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>This is an important point and seems to hinge on the notion of reference, or the question of how language works in different contexts. The following may or may not be new to you, but trying to be explicit here helps me think through the argument.</p>\n<p>Mostly, words gain meaning from contextual embedding- i.e. they\u2019re meaningful as nodes in a larger network. Wittgenstein observed that often, philosophical confusion stems from taking a perfectly good word and trying to use it outside its natural remit. His famous example is the question, \u201cwhat time is it on the sun?\u201d. As you note, maybe notions about emotional valence are similar- trying to \u2018universalize\u2019 valence may be like trying to universalize time-zones, an improper move.</p>\n<p>But there\u2019s another notable theory of meaning, where parts of language gain meaning through deep structural correspondence with reality. Much of physics fits this description, for instance, and it\u2019s <em>not</em> a type error to universalize the notion of the electromagnetic force (or electroweak force, or whatever the fundamental unification turns out to be). I am essentially asserting that qualia is like this- that we can find universal principles for qualia that are equally and exactly true in humans, dogs, dinosaurs, aliens, conscious AIs, etc. When I note I\u2019m a physicalist, I intend to inherit many of the semantic properties of physics, how meaning in physics \u2018works\u2019.</p>\n<p>I suspect all conscious experiences have an emotional valence, in much the same way all particles have a charge or spin. I.e. it\u2019s well-defined across all physical possibilities.</p>\n", "parentCommentId": "dQHczoTREkYBgQwvv", "user": {"username": "MikeJohnson"}}, {"_id": "wNeTQFxbwKqWwE8zT", "postedAt": "2018-03-02T00:23:36.996Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Do you think we should move the conversation to private messages? I don't want to clutter a discussion thread that's mostly on a different topic, and I'm not sure whether the average reader of the comments benefits or is distracted by long conversations on a narrow subtopic.</p>\n<p>Your comment appears to be just reframing the point I just made in your own words, and then affirming that you believe that the notion of qualia generalizes to all possible arrangements of matter. This doesn't answer the question, why do you believe this?</p>\n<p>By the way, although there is no evidence for this, it is commonly speculated by physicists that the laws of physics allow multiple metastable vacuum states, and the observable universe only occupies one such vacuum, and near different vacua there different fields and forces. If this is true then the electromagnetic field and other parts of the Standard Model are not much different from my earlier example of the alignment of an ice crystal. One reason this view is considered plausible is simply the fact that it's possible: It's not considered so unusual for a quantum field theory to have multiple vacuum states, and if the entire observable universe is close to one vacuum then none of our experiments give us any evidence on what other vacuum states are like or whether they exist.</p>\n<p>This example is meant to illustrate a broader point: I think that making a binary distinction between contextual concepts and universal concepts is oversimplified. Rather, here's how I would put it: Many phenomena generalize beyond the context in which they were originally observed. Taking advantage of this, physicists deliberate seek out the phenomena that generalize as far as possible, and over history broadened their grasp  very far. Nonetheless, they avoid thinking about any concept as &quot;universal&quot;, and often when they do think a concept generalizes they have a specific explanation for why it should, while if there's a clear alternative to the concept generalizing they keep an open mind.</p>\n<p>So again: Why do you think that qualia and emotional valence generalize to all possible arrangements of matter?</p>\n", "parentCommentId": "Hhh2aXFNToQCgQD79", "user": {"username": "itaibn"}}, {"_id": "WZh8p5Gk873KZw9RT", "postedAt": "2018-03-02T05:15:49.080Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>EA forum threads auto-hide so I\u2019m not too worried about clutter.</p>\n<p>I don\u2019t think you\u2019re fully accounting for the difference in my two models of meaning. And, I think the objections you raise to consciousness being well-defined would also apply to physics being well-defined, so your arguments seem to prove too much.</p>\n<p>To attempt to address your specific question, I find the hypothesis that \u2018qualia (and emotional valence) are well-defined across all arrangements of matter\u2019 convincing because (1) it seems to me the alternative is not coherent (as I noted in the piece on computationalism I linked for you) and (2) it seems generative and to lead to novel and plausible predictions I think will be proven true (as noted in the linked piece on quantifying bliss and also in Principia Qualia).</p>\n<p>All the details and sub arguments can be found in those links.</p>\n<p>Will be traveling until Tuesday; probably with spotty internet access until then.</p>\n", "parentCommentId": "wNeTQFxbwKqWwE8zT", "user": {"username": "MikeJohnson"}}, {"_id": "WDfGhNPqNSc9BSo8F", "postedAt": "2018-03-10T23:38:30.260Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I haven't responded to you for so long firstly because I felt like we got to the point in the discussion where it's difficult to get across anything new and I wanted to be attentive to what I say, and then because after a while without writing anything I became disinclined from continuing. The conversation may close soon.</p>\n<p>Some quick points:</p>\n<ul>\n<li><p>My whole point in my previous comment is that the conceptual structure of physics is not what you make it out to be, and so your analogy to physics is invalid. If you want to say that my arguments against consciousness apply equally well to physics you will need to explain the analogy.</p>\n</li>\n<li><p>My views on consciousness that I mentioned earlier but did not elaborate on are becoming more relevant. It would be a good idea for me to explain them in more detail.</p>\n</li>\n<li><p>I read your linked piece on quantifying bliss and I am unimpressed. I concur with the last paragraph of <a href=\"http://effective-altruism.com/ea/1cn/why_i_think_the_foundational_research_institute/bjy\">this comment</a>.</p>\n</li>\n</ul>\n", "parentCommentId": "WZh8p5Gk873KZw9RT", "user": {"username": "itaibn"}}, {"_id": "qYm9v22uLSXqpw8nu", "postedAt": "2018-04-13T15:48:41.421Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Thanks very much for writing this, and thanks to Greg for funding it! I think this is a really important discussion. Some slightly rambling thoughts below.</p>\n<p>We can think about 3 ways of improving the EV of the far future:</p>\n<p>1: Changing incentive structures experienced by powerful agents in the future (e.g. avoiding arms races, power struggles, selection pressures)</p>\n<p>2: \n a) Changing the moral compass of powerful agents in the future in specific directions (e.g. MCE). </p>\n<p> b) Indirect ways to improve the moral compass of powerful agents in the future (e.g. philosophy research, education, intelligence/empathy enhancement)</p>\n<p>All of these are influenced both by strategies such as activism, improving institutions, and improving education, as well as by AIA. I am inclined to think of AIA as a particularly high-leverage point at which we can have influence on these. </p>\n<p>However, these are issues are widely encountered. Consider 2b: we have to decide how to educate the next generation of humans, and they may well end up with ethical beliefs that are different from ours, so we must judge how much to try and influence or constrain them, and how much to accept that the changes are actually progress. This is similar to the problem of defining CEV: we have some vague idea of the direction in which better values lie (more empathy, more wisdom, more knowledge), but we can't say exactly what the values should be. For this intervention, working on AIA may be more important than activism because it has more leverage - it is likely to be more tractable and have greater influence on the future than the more diffuse ways in we can push on education and intergenerational moral progress.</p>\n<p>This framework also suggests that MCE is just one example of a collection of similar interventions. MCE involves pushing for a fairly specific belief and behaviour change on a principle that's fairly uncontroversial. You could also imagine similar interventions - for instance, helping people reduce unwanted aggressive or sadistic behaviour. We could call this something like 'uncontroversial moral progress': helping individuals and civilisation to live by their values more. (on a side note: sometimes I think of this as the minimal core of EA: trying to live according to your best guess of what\u2019s right)</p>\n<p>The choice between working on 2a and 2b depends, among other things, on your level of moral uncertainty. </p>\n<p>I am inclined to think that AIA is the best way to work on 1 and 2b, as it is a particularly high-leverage intervention point to shape the power structures and moral beliefs that exist in the future. It gives us more of a clean slate to design a good system, rather than having to work within a faulty system. </p>\n<p>I would really like to see more work on MCE and other examples of 'uncontroversial moral progress'. Historical case studies of value changes seem like a good starting point, as well as actually testing the tractability of changing people's behaviour. </p>\n<p>I also really appreciated your perspective on different transformative AI scenarios, as I\u2019m worried I\u2019m thinking about it in an overly narrow way. </p>\n", "parentCommentId": null, "user": {"username": "ElizabethBarnes"}}, {"_id": "qDZvkWqaunXjh2EXj", "postedAt": "2018-04-13T15:48:50.800Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>You say</p>\n<blockquote>\n<p>I think a given amount of dolorium/dystopia (say, the amount that can be created with 100 joules of energy) is far larger in absolute moral expected value than hedonium/utopia made with the same resources</p>\n</blockquote>\n<p>Could you elaborate more on why this is the case? I would tend to think that a prior would be that they're equal, and then you update on the fact that they seem to be asymmetrical, and try to work out why that is the case, and whether those factors will apply in future. They could be fundamentally asymmetrical, or evolutionary pressures may tend to create minds with these asymmetries. The arguments I've heard for why are:</p>\n<ul>\n<li>The worst thing that can happen to an animal, in terms of genetic success, is much worse than the best thing. </li>\n</ul>\n<p>This isn't entirely clear to me: I can imagine a large genetic win such as securing a large harem could be comparable to the genetic loss of dying, and many animals will in fact risk death for this. This seems particularly true considering that dying leaving no offspring doesn't make your contribution to the gene pool zero, just that it's only via your relatives. </p>\n<ul>\n<li>There is selection against strong positive experiences in a way that there isn't against strong negative experiences.</li>\n</ul>\n<p>The argument here is, I think, that strong positive experiences will likely result in the animal sticking in the blissful state, and neglecting to feed, sleep, etc, whereas strong negative experiences will just result in the animal avoiding a particular state, which is less maladaptive. This argument seems stronger to me but still not entirely satisfying - it seems to be quite sensitive to how you define states.</p>\n", "parentCommentId": null, "user": {"username": "ElizabethBarnes"}}, {"_id": "2jGi2s387LcKi6wzF", "postedAt": "2018-12-25T23:24:45.258Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Next to the counterpoints mentioned by Gregory Lewis, I think there is an additional reason why MCE seems less effective than more targeted interventions to improve the quality of the long-term future: Gains from trade between humans with different values become easier to implement as the reach of technology increases. As long as a non-trivial fraction of humans end up caring about animal wellbeing or digital minds, it seems likely it would be cheap for other coalitions to offer trades. So whether 10% of future people end up with an expanded moral circle or 100% may not make much of a difference to the outcome: It will be reasonably good either way if people reap the gains from trade. </p><p>One might object that it is unlikely that humans would be able to cooperate efficiently, given that we don&#x27;t see this type of cooperation happening today. However, I think it&#x27;s reasonable to assume that staying in control of technological progress beyond the AGI transition requires a degree of wisdom and foresight that is very far away from where most societal groups are at today. And if humans <em>do</em> stay in control, then finding a good solution for value disagreements may be the easier problem, or at worst similarly hard. So it feels to me that most likely, we either we get a future that goes badly for reasons related to lack of coordination and sophistication in the pre-AGI stage, or we get a future where humans set things up wisely enough to actually design an outcome that is nice (or at least not amongst the<em> 10</em>% of worst outcomes) by the lights of nearly everyone.</p><p>Brian Tomasik made the point that conditional on human values staying in control, we may be very unlikely to get something like broad moral reflection. Instead, values could be determined by a very small group of individuals who happened to be in power by the time AGI arrives (as opposed to individuals ending up there because they were unusually foresighted and also morally motivated). This feels possible too, but it seems to not be the likely default to me because I suspect that you&#x27;d need to <em>necessarily</em> increase your philosophical sophistication in order to stay in control of AGI, and that probably gives you more pleasant outcomes (correlational claim). Iterated amplification for instance, as an approach to AI alignment, has several uses for humans: Humans are not only where the resulting values come from, but they&#x27;re also in charge of keeping the bootstrapping process on track and corrigible. And as <a href=\"https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition\">this post on factored cognition</a> illustrates, this requires sophistication to set up. So if that&#x27;s the bar that AGI creators need to pass before they can determine how &quot;human values&quot; are to be extrapolated, maybe we shouldn&#x27;t be too pessimistic about the outcome. It seems kind of unlikely that someone would go through all of that only to be like &quot;I&#x27;m going to implement my personal best guess about what matters to me, with little further reflection, and no other humans get a say here.&quot; Similarly, it also feels unlikely that people would go through with all that and not find a way to make subparts of the population <em>reasonably</em> content about how sentient subroutines are going to be used. </p><p>Now, I feel a bit confused about the feasibility of AI alignment if you were to do it somewhat sloppily and with lower standards. I <em>think</em> that there&#x27;s a spectrum from &quot;it just wouldn&#x27;t work at all and not be competitive&quot; (and then people would have to try some other approach) to &quot;it would produce a capable AGI but it would be vulnerable to failure modes like adversarial exploits or optimization daemons, and so it would end up with not human values&quot;. These failure modes, to the very small degree I currently understand them, sound like they would <em>not</em> be sensitive to whether the human whose approval you tried to approximate had an expanded moral circle or not. I might be wrong about that. If people mostly want sophisticated alignment procedures because they care about preserving the option for philosophical reflection, rather than because they <em>also</em> think that you simply run into large failure modes otherwise, then it seems like (conditional on <em>some</em> kind of value alignment) whether we get an outcome with broad moral reflection is not so clear. If it&#x27;s technically easier to build value-aligned AI with very parochial values, then MCE could make a relevant difference to these non-reflection outcomes. </p><p>But all in all my argument is that it&#x27;s somewhat strange to assume that a group of people could succeed at building an AGI optimized for its creators&#x27; values, without having to put in so much thinking about how to get this outcome right that they&#x27;d almost <em>can&#x27;t help</em> but become reasonably philosophically sophisticated in the process. And sure, philosophically sophisticated people can still have fairly strange values by your own lights, but it seems like there&#x27;s more convergence. Plus I&#x27;d at least be optimistic about their propensity to strive towards positive-sum outcomes, given how little scarcity you&#x27;d have if the transition does go well. </p><p>Of course, maybe value-alignment is going to work very differently from what people currently think. The main way I&#x27;d criticize my above points is that they&#x27;re based on heavy-handed inside-view thinking about how difficult I (and others I&#x27;m updating towards) expect the AGI transition to be. If AGI will be more like the Industrial Revolution rather than something that is even more difficult to stay remotely in control of, or if some other technology proves to be more consequential than AGI, then my argument has less force. I mainly see this as yet another reason to caveat that the ex ante plausible-seeming position that MCE can have a strong impact on AGI outcomes starts to feel more and more conjunctive the more you zoom in and try to identify concrete pathways.</p>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "XbA93cCoyJAd9w2sp", "postedAt": "2018-12-26T22:52:02.741Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I agree with this. It seems like the world where Moral Circle Expansion is useful is the world where:</p><p>The creators of AI are philosophically sophisticated (or persuadable) enough to expand their moral circle if they are exposed to the right arguments or work is put into persuading them.</p><p>They are not philosophically sophisticated enough to realize the arguments for expanding the moral circle on their own (seems plausible).</p><p>They are not philosophically sophisticated enough to realize that they might want to consider a distribution of arguments that they could have faced and could have persuaded them about what is morally right, and design AI with this in mind (ie CEV), or with the goal of achieving a period of reflection where they can sort out the sort of arguments that they would want to consider.</p><p>I think I&#x27;d prefer pushing on point 3, as it also encompasses a bunch of other potential philosophical mistakes that AI creators could make.</p>", "parentCommentId": "2jGi2s387LcKi6wzF", "user": {"username": "William_S"}}, {"_id": "KRdLQm4rDEpu5RH8F", "postedAt": "2018-12-28T01:08:25.481Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Interesting points. :) I think there could be substantial differences in policy between 10% support and 100% support for MCE depending on the costs of appeasing this faction and how passionate it is. Or between 1% and 10% support for MCE applied to more fringe entities.</p><blockquote>philosophically sophisticated people can still have fairly strange values by your own lights, but it seems like there&#x27;s more convergence.</blockquote><p>I&#x27;m not sure if sophistication increases convergence. :) If anything, people who think more about philosophy tend to diverge more and more from commonsense moral assumptions.</p><p>Yudkowsky and I seem to share the same metaphysics of consciousness and have both thought about the topic in depth, yet we occupy almost antipodal positions on the question of how many entities we consider moral patients. I tend to assume that one&#x27;s starting points matter a lot for what views one ends up with.</p>", "parentCommentId": "2jGi2s387LcKi6wzF", "user": {"username": "Brian_Tomasik"}}, {"_id": "wiw8h5Fi5MLK9kMHG", "postedAt": "2021-07-18T04:09:40.398Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>In Stuart Russell's <em>Human Compatible</em> (2019), he advocates for AGI to follow preference utilitarianism, maximally satisfying the values of humans. As for animal interests, he seems to think that they are sufficiently represented since he writes that they will be valued by the AI insofar as humans care about them. Reading this from Stuart Russell shifted me toward thinking that moral circle expansion probably does matter for the long-term future. It seems quite plausible (likely?) that AGI will follow this kind of value function which does not directly care about animals rather than broadly anti-speciesist values, since AI researchers are not generally anti-speciesists. In this case, moral circle expansion across the general population would be essential.</p>\n<p>(Another factor is that Russell's reward modeling depends on receiving feedback occasionally from humans to learn their preferences, which is much more difficult to do with animals. Thus, under an approach similar to reward modeling, AGI developers probably won't bother to directly include animal preferences, when that involves all the extra work of figuring out how to get the AI to discern animal preferences. And how many AI researchers want to risk, say, mosquito interests overwhelming human interests?)</p>\n<p>In comparison,  if an AGI was planned to only care about the interests of people in, say, Western countries, that would instantly be widely decried as racist (at least in today's Western societies) and likely not be developed. So while moral circle expansion encompasses caring about people in other countries, I'm less concerned that large groups of humans will not have their interests represented in the AGI's values than I am about nonhuman animals.</p>\n<p>It may be more cost-effective to have targeted approach of increasing anti-speciesism among AI researchers and doing anti-speciesist AI alignment philosophy/research (e.g., more details on how AI following preference utilitarianism can also intrinsically care about animal preferences, accounting for preferences of digital sentience given the problem that they can easily replicate and dominate preference calculations), but anti-speciesism among the general population still seems to be an important component of reducing risk of having a bad far future.</p>\n", "parentCommentId": "Fw53moPFXdwMbrZay", "user": {"username": "michaelchen"}}, {"_id": "yZrhwATtAqJs3zG3Z", "postedAt": "2021-12-02T20:33:01.615Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>I come back to this post quite frequently when considering whether to prioritize MCE (via animal advocacy) or AI safety. It seems that these two cause areas often attract quite different people with quite different objectives, so this post is unique in its attempt to compare the two based on the same long-term considerations.&nbsp;</p><p>I especially like the discussion of bias. Although some might find the whole discussion a bit <i>ad hominem</i>, I think people in EA should take seriously the worry that certain features common in the EA community (e.g., an attraction towards abstract puzzles) might bias us towards particular cause areas.</p><p>I recommend this post for anyone interested in thinking more broadly about longtermism.</p>", "parentCommentId": null, "user": {"username": "seanrichardson@outlook.com"}}, {"_id": "mEEvnEjNNMzbrxvHB", "postedAt": "2022-01-19T22:45:22.738Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>While the central thesis to expand one\u2019s moral circles can be well-enjoyed by the community, this post is not selling it well. This is exemplified by the \u201cOne might be biased towards AIA if\u2026\u201d section, which makes assumptions about individuals who focus on AI alignment. Further, while the post includes a section on cooperation, it discourages it. [Edit: <i>Prima facie</i>,] the post does not invite critical discussion. Thus, <s>I would not recommend this post to any readers interested in moral circles expansion, AI alignment, or cooperation.</s> Thus, I would <strong>recommend this post to readers interested in moral circles expansion, AI alignment, and cooperation, as long as they are interested in a vibrant discourse.</strong></p>", "parentCommentId": null, "user": {"username": "brb243"}}, {"_id": "WWgo6or9ebvRo9m4q", "postedAt": "2022-01-20T06:08:53.289Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Do you think there's a better way to discuss biases that might push people to one cause or another? Or that we shouldn't talk about such potential biases at all?</p>\n<p>What do you mean by this post discouraging cooperation?</p>\n<p>What do you expect an invitation for critical discussion to look like? I usually take that to be basically implicit when something is posted to the EA Forum, unless the author states otherwise.</p>\n", "parentCommentId": "mEEvnEjNNMzbrxvHB", "user": {"username": "MichaelStJules"}}, {"_id": "gj8JHMZmbNNAeAJmL", "postedAt": "2022-01-20T13:30:05.934Z", "postId": "BY8gXSpGijypbGitT", "htmlBody": "<p>Hm, 1) how do you define a bias? What is your <a href=\"https://www.alignmentforum.org/posts/EpdXLNXyL4EYLFwF8/an-increasingly-manipulative-newsfeed\">reference for evaluation</a> whether something is biased? The objective should be to make the best decisions with available information at any given time while supporting innovation and keeping 'openminded.' This 'bias' assessment should be conducted to identify harmful actions that individuals deem positive due to their biases and inform overall prioritization decisionmaking rather than seeking to change one's perspectives on causes they prefer. This can contribute to systemic change and optimal specialization development by individuals. This is a better way to approach biases.</p><p>The section on cooperation discourages collaboration because it understands cooperation as asserting one\u2019s perspectives where these are not welcome rather than advancing ventures. The part also states: \u201cinsofar as MCE is uncooperative, I think a large number of other EA interventions, including AIA, are similarly uncooperative.\u201d These author\u2019s assumptions, if not critically examined against evidence, can discourage persons who could be seeking encouragement to cooperate with others in this article from doing so, because one may wish to avoid sharing perspectives where they are not welcome.</p><p>An invitation for critical discussion can include an argument for the writing\u2019s relevance to the development of answers to open-ended questions. But I can agree with your point that this can be superfluous, so would add (added) <i>prima facie</i> and edited the conclusion.</p>", "parentCommentId": "WWgo6or9ebvRo9m4q", "user": {"username": "brb243"}}]