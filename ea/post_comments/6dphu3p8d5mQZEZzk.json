[{"_id": "gekxzRwyqEP7cuRGY", "postedAt": "2023-06-04T12:11:04.641Z", "postId": "6dphu3p8d5mQZEZzk", "htmlBody": "<p>Nitpick: It's fairly unlikely that GPT-4 is 1tn params; this size doesn't seem compute-optimal. I grant you the Semafor assertion is some evidence, but I'm putting more weight <a href=\"https://twitter.com/stemcaleese/status/1645496684850581504\">on compute arithmetic</a>.</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "dcaoEFGtMCiNSkvNk", "postedAt": "2023-06-04T17:51:40.045Z", "postId": "6dphu3p8d5mQZEZzk", "htmlBody": "<blockquote><p>One of the major limitations of using existing LLMs is their unreliability. No important processes can currently be trusted to LLMs, because we have very little understanding of how they work, limited knowledge of the limits of their capabilities, and a poor understanding of how and when they fail.</p></blockquote><p>I don't disagree with this, but I think it's very likely to stop being true in practice as the tech is commercialized. It won't be perfect, but the current generation of tweaks already pushes it into the range of at least 3-4 9s of reliability for non-adversarial settings, which seems like it will be enough for many applications, and for better work on how to make it even more reliable. More than that, business applications, or a lack of success thereof, will show whether or not this is true in the coming year, well before we hit GPT-5+.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Davidmanheim"}}, {"_id": "MQKmbmwsvyGecxBaN", "postedAt": "2023-06-05T20:57:30.501Z", "postId": "6dphu3p8d5mQZEZzk", "htmlBody": "<blockquote><p>For these reasons I do not believe the EA movement should focus too much or too exclusively on LLMs or similar models as candidates for an AGI precursor, or put too much of a focus on short time horizons. We should pursue a diverse range of strategies for mitigating AI risk, and devote significant resources towards longer time horizons.</p></blockquote><ol><li>Do you think that most strategies that are potentially useful given short timelines remain so as timelines lengthen? (i.e. where the effectiveness of the strategy is timeline-independent)</li><li>Which assumption carries the largest penalty if incorrect? (anticipating and planning for shorter timelines and being wrong vs. anticipating and planning for longer timelines and being wrong)</li></ol>", "parentCommentId": null, "user": {"username": "blueberry"}}]