[{"_id": "npm6mrJQzungTLsKj", "title": "Black Box Investigations Research Hackathon", "postedAt": "2022-09-15T10:09:44.412Z", "htmlBody": "<p>TLDR;&nbsp;<a href=\"https://itch.io/jam/llm-hackathon\"><strong><u>Join the Black Box Investigations Research Hackathon</u></strong></a> to participate with others in a weekend of alignment research on language models using a&nbsp;<a href=\"https://www.lesswrong.com/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language\"><u>cognitive psychology-style approach</u></a>.</p><h2>Purpose</h2><p>When you join the Black Box Investigations Research Hackathon, the goal is to explore where black box language models work well and where they break down. It is inspired by&nbsp;<a href=\"https://www.lesswrong.com/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language\"><u>this post by Buck Shlegeris</u></a> which calls for investigators of language models in a cognitive psychology style fashion. Become the AI psychologist of the future.</p><p>The meta-goal of this&nbsp;<a href=\"https://apartresearch.com/jam\"><u>Alignment Jam</u></a> is to investigate if hackathons work well for 1) providing real research results and ideas, 2) increase the research confidence of the participants, and 3) being a scalable solution to exciting ways of joining the alignment community to work on interesting and well-defined problems.</p><h2>Outcome</h2><p>Participants are encouraged to discover interesting behavior of black box language models. At the end of the hackathon each group / participant should have created a short (&lt;5 min) presentation showcasing their results. A jury will then vote for their favorite projects according to the review criteria specified in the appendix.</p><p>After the hackathon, we will write a LessWrong post (or series of posts) describing the main findings and the overall experience. We hope to encourage more people to do this kind of work and we believe the hackathon format works well for generating novel research perspectives on alignment.&nbsp;</p><p>If this hackathon is successful, we expect to repeat the success with other&nbsp;<a href=\"https://aisafetyideas.com/projects\"><u>research agendas</u></a>.</p><h2>Participation instructions</h2><p>It is&nbsp;<a href=\"https://itch.io/jam/llm-hackathon\"><u>hosted on the hackathon platform itch.io</u></a> where you will follow the instructions to sign up (see&nbsp;<a href=\"https://itch.io/jam/llm-hackathon\"><u>here</u></a>): Make an itch.io account, click \u201cJoin jam\u201d, and come along on the 30th of September!</p><p>We will work all weekend in Gathertown in groups of 2-6. The link to Gathertown will be announced in the&nbsp;<a href=\"https://itch.io/jam/llm-hackathon/community\"><u>itch.io community tab</u></a> and sent to your itch.io email when we get closer to the date. You can ask questions&nbsp;<a href=\"https://itch.io/jam/llm-hackathon/community\"><u>here</u></a>.</p><p>When the hackathon starts, you will be able to continually upload new PDF / Google Doc / Colab / Github repositories during the extent of the research competition.</p><p>For interacting with the black box models, we will use&nbsp;<a href=\"https://20b.eleuther.ai/\"><u>20b.eleuther.ai</u></a>,&nbsp;<a href=\"https://beta.openai.com/playground\"><u>beta.openai.com/playground</u></a>, and you will receive research API credits to work directly with the APIs of OpenAI. There will be premade scripts to initialize your research project. We encourage you to sign up to&nbsp;<a href=\"https://beta.openai.com/\"><u>beta.openai.com</u></a> before and experiment with your free credits. This should only take 2-3 minutes.</p><p>You can get inspiration from&nbsp;<a href=\"https://aisafetyideas.com/project/black-box-investigation\"><u>aisafetyideas.com</u></a> or from the list of previous research related to black box investigation on the&nbsp;<a href=\"https://itch.io/jam/llm-hackathon\"><u>itch.io page</u></a>.</p><h2>Asks</h2><p><strong>Funders:</strong> You can add more to the prize pool through&nbsp;<a href=\"https://www.super-linear.org/\"><u>super-linear.org</u></a> if you\u2019re a funder. This will enable us to provide more credits for working with the largest language models, provide hackathon merch, and to increase the prize pool.</p><p><strong>Jury, mentor, or speaker:</strong> Contact us&nbsp;<a href=\"https://apartresearch.com/join\"><u>on our Discord</u></a> or by email&nbsp;<a href=\"mailto:esben@apartresearch.com\"><u>esben@apartresearch.com</u></a> if you are interested in mentoring, joining the jury, or giving a talk.</p><h2>Appendix</h2><h2>Reviewer criteria</h2><p>Each submission will be evaluated by a group of judges on 1-10 scale for 4 different qualities.</p><figure class=\"table\"><table><tbody><tr><td style=\"padding:4pt;vertical-align:top;width:100px\"><p><strong>Criterion</strong></p></td><td style=\"padding:4pt;vertical-align:top;width:70px\"><p><strong>Weight</strong></p></td><td style=\"padding:4pt;vertical-align:top\"><p><strong>Description</strong></p></td></tr><tr><td style=\"padding:4pt;vertical-align:top\">Alignment</td><td style=\"padding:4pt;vertical-align:top\"><p>2</p></td><td style=\"padding:4pt;vertical-align:top\">How good are your arguments for how this result informs the longterm alignment of large language models? How informative are the results for the field in general?</td></tr><tr><td style=\"padding:4pt;vertical-align:top\">AI Psychology</td><td style=\"padding:4pt;vertical-align:top\"><p>1</p></td><td style=\"padding:4pt;vertical-align:top\">Have you come up with something that might guide the \u201cfield\u201d of AI Psychology in the future?</td></tr><tr><td style=\"padding:4pt;vertical-align:top\">Novelty</td><td style=\"padding:4pt;vertical-align:top\"><p>1</p></td><td style=\"padding:4pt;vertical-align:top\">Have the results not been seen before and are they surprising compared to what we expect?</td></tr><tr><td style=\"padding:4pt;vertical-align:top\">Generality</td><td style=\"padding:4pt;vertical-align:top\"><p>1</p></td><td style=\"padding:4pt;vertical-align:top\">Do your research results show a generalization of your hypothesis? E.g. if you expect language models to overvalue evidence in the prompt compared to in its training data, do you test more than just one or two different prompts? A top score might be a statistical testing of 200+ prompt examples.</td></tr><tr><td style=\"padding:4pt;vertical-align:top\">Reproducibility</td><td style=\"padding:4pt;vertical-align:top\"><p>1</p></td><td style=\"padding:4pt;vertical-align:top\">Are we able to easily reproduce the research and do we expect the results to reproduce? A high score here might be a high Generality and a well-documented Github repository to rerun all experiments from the report.</td></tr></tbody></table></figure><p><br>&nbsp;</p>", "user": {"username": "esben-kran"}}, {"_id": "nYgw4FNpHf9bmJGEi", "title": "Forecasting thread: How does AI risk level vary based on timelines?", "postedAt": "2022-09-14T23:56:35.193Z", "htmlBody": "<p><i>Crossposted to </i><a href=\"https://www.lesswrong.com/posts/h7Sx4DBL4JZnbTpes/forecasting-thread-how-does-ai-risk-level-vary-based-on\"><i>LessWrong</i></a></p><p>While there have been many previous surveys asking about the chance of existential catastrophe from AI and/or AI timelines, none as far as I'm aware have asked about how the level of AI risk varies based on timelines. But this seems like an extremely important parameter for understanding the nature of AI risk and prioritizing between interventions.</p><p>Contribute your forecasts below. I'll write up my forecast rationales in an answer and encourage others to do the same.</p><figure class=\"media\"><div data-oembed-url=\"https://forecast.elicit.org/binary/questions/726u_r_XC\">\n\t\t\t\t<div data-elicit-id=\"726u_r_XC\" class=\"elicit-binary-prediction\">\n\t\t\t\t\t<div>Elicit Prediction (<a href=\"forecast.elicit.org/binary/questions/726u_r_XC\">forecast.elicit.org/binary/questions/726u_r_XC</a>)</div>\n\t\t\t\t</div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://forecast.elicit.org/binary/questions/8HnkL7Ekf\">\n\t\t\t\t<div data-elicit-id=\"8HnkL7Ekf\" class=\"elicit-binary-prediction\">\n\t\t\t\t\t<div>Elicit Prediction (<a href=\"forecast.elicit.org/binary/questions/8HnkL7Ekf\">forecast.elicit.org/binary/questions/8HnkL7Ekf</a>)</div>\n\t\t\t\t</div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://forecast.elicit.org/binary/questions/dlPDzREFM\">\n\t\t\t\t<div data-elicit-id=\"dlPDzREFM\" class=\"elicit-binary-prediction\">\n\t\t\t\t\t<div>Elicit Prediction (<a href=\"forecast.elicit.org/binary/questions/dlPDzREFM\">forecast.elicit.org/binary/questions/dlPDzREFM</a>)</div>\n\t\t\t\t</div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://forecast.elicit.org/binary/questions/EHL2INyQq\">\n\t\t\t\t<div data-elicit-id=\"EHL2INyQq\" class=\"elicit-binary-prediction\">\n\t\t\t\t\t<div>Elicit Prediction (<a href=\"forecast.elicit.org/binary/questions/EHL2INyQq\">forecast.elicit.org/binary/questions/EHL2INyQq</a>)</div>\n\t\t\t\t</div>\n\t\t\t</div></figure><figure class=\"media\"><div data-oembed-url=\"https://forecast.elicit.org/binary/questions/AETDAUmNF\">\n\t\t\t\t<div data-elicit-id=\"AETDAUmNF\" class=\"elicit-binary-prediction\">\n\t\t\t\t\t<div>Elicit Prediction (<a href=\"forecast.elicit.org/binary/questions/AETDAUmNF\">forecast.elicit.org/binary/questions/AETDAUmNF</a>)</div>\n\t\t\t\t</div>\n\t\t\t</div></figure>", "user": {"username": "elifland"}}, {"_id": "uMgPLPpovQbYupPnR", "title": "New Cause Area: Baby Longtermism \u2013 Child Rights and Future Children", "postedAt": "2022-09-14T20:22:44.823Z", "htmlBody": "<p>\u201cIf you\u2019ve got some things that look robustly good in both the short and the long term, that definitely makes you feel a lot better than something that is only good from a very long-term perspective\u201d \u2013 William MacAskill, Time Magazine, <a href=\"https://time.com/6204627/effective-altruism-longtermism-william-macaskill-interview/\">Inside the Effective Altruism Movement to do More Good</a></p><p>The Effective Altruism movement is experiencing a groundswell of activity since the publication of <i>What We Owe the Future (WWOTF)</i>. I would like to submit that focusing on a new cause area \u2013 child health as a human rights policy \u2013 along with endeavouring to protect the first generation of future people, provides co-benefits to the movement, by blending near-term and long-term ideals. As a first step, <strong>I am proposing the EA community lobby the US government to ratify the 1989 UN Convention on the Rights of the Child (UNCRC).</strong></p><p><strong>Summary</strong><br>In this post I will outline how prioritizing and promoting child health rights might provide:<br>1. The potential to save <a href=\"https://ourworldindata.org/much-better-awful-can-be-better\">millions of children</a> per year<br>2. A trial of international policy influence as \u201c<a href=\"https://www.openphilanthropy.org/research/hits-based-giving/\">hits-based</a>\u201d cause area for EA<br>3. An alternative focus to the controversial charity/philanthropy<br>4. A metaphor of the rights of the child as the rights of the future of humanity<br>5. Increase interest in the EA movement by endearing newcomers who would otherwise be turned off by less mainstream cause areas&nbsp;</p><p><br><strong>Disclaimer: </strong>My intention is for this post to be a conversation-starter. I likely have not considered all the counterfactuals and downsides for my arguments, but feel I am at a personal dead-end. I hope the key gaps and uncertainties might be considered and addressed by readers.&nbsp;</p><p><br><strong>Acknowledgements:</strong> Big thanks to Habiba Islam, and High Impact Medicine (Slack Hi-Med group), particularly Ben Stewart, for review and feedback on earlier drafts. Shortcomings of the article are all my own.</p><p><br>1. <strong>Goal \u2013 Saving Millions of Children\u2019s lives per year</strong></p><p>On the EA concepts of importance, tractability, and neglectedness (<a href=\"https://forum.effectivealtruism.org/topics/itn-framework#fnuc4xj13qrf\">ITN framework</a>), we must individually decide the importance of child rights, considering whether current children are more, equally, or less important than future people. Irrefutably, they are equally helpless, disenfranchised dependents on our altruistic efforts. Hopefully, there will be considerably more future people than there are current children, but I would argue that strengthening the rights, healthcare, and welfare of current children also strengthens those of future children, and thus, future generations as a continued benefit.</p><p>Reducing child mortality, however, is tractable. We already possess the knowledge, skills, and medicines to save lives. In <a href=\"https://ourworldindata.org/much-better-awful-can-be-better\">this post</a> by Max Roser of Our World in Data, he reports that globally, 4.3% of all children die before the age of 15, which is approximately 10 times the rate in the European Union, at 0.45%. He argues that \u201cThe global number of child deaths, as reported above, is 5,909,552 [and] 5,909,552-5,909,552/(4.3/0.45)=5,291,111 fewer children would die if the global mortality rate was 0.45% rather than 4.5%.\u201d To extrapolate this reasoning, if every child in the world benefitted from EU-level health care and social welfare, the global child mortality rate could fall by an order of magnitude.</p><p>The corollary of the tractability argument is that child health as a human right is also neglected. Five million children are dying <i>unnecessarily</i> around the world, every year. Promoting children\u2019s health as a human right is what I have colloquially called, \u201cbaby longtermism\u201d \u2013 dually meaning extending the lives of current children, while promoting wellbeing of future generations.</p><p><br>2. <strong>International Policy Influence as \u201cHits-Based\u201d Cause Area</strong></p><p>As indicated, I am proposing the EA community lobby the US government to ratify the 1989 UN Convention on the Rights of the Child (UNCRC). Firstly, it seems to be a neglected cause; Open Philanthropy has evaluated US policy as possible cause areas <a href=\"https://www.openphilanthropy.org/research/narrowing-down-u-s-policy-areas/\">here</a>, but influence on international US policy was not one of them.</p><p>Second, international consensus is tractable; only one country in the world has not ratified the UNCRC. The United States is the outlier of 196 eligible United Nations countries, having not ratified the <a href=\"https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-rights-child\">1989 UN Convention on the Rights of the Child</a>, even though it contributed heavily to the draft, has signed it, and has ratified the two optional protocols. &nbsp;Somalia and South Sudan were the last two states to ratify the original convention in 2015. The United States has also not ratified the Convention on the Elimination of All Forms of Discrimination against Women (CEDAW), however, <a href=\"https://tbinternet.ohchr.org/_layouts/15/TreatyBodyExternal/Treaty.aspx?Treaty=CEDAW&amp;Lang=en\">several other countries</a> have also not ratified the CEDAW, and the tractability of compelling several countries, with varying cultural and political norms, to collectively ratify a treaty is questionable. The USA is alone in not ratifying the Rights of the Child. Arguably, although controversial in its power as global leader, the US retains popular influence as <a href=\"https://www.ft.com/content/562365b5-31b0-47c7-866e-5cf4cb7124e5\">soft power</a> around the world, and its ratification may be a tractable game-changer for child rights.</p><p>Finally, I would suggest lobbying to ratify the UNCRC is important. Although universal ratification of the UNCRC would principally be a first step toward improving global child rights, it may also have direct effect on child welfare in the United States; in this recently-published <a href=\"https://www.hrw.org/feature/2022/09/13/how-do-states-measure-up-child-rights\">report card</a>, nearly half of the US states received a failing grade on laws for child marriage, corporal punishment, juvenile justice, and child labour. Contrary to Give Well\u2019s dictum, \u201c<a href=\"https://www.givewell.org/giving101/Your-dollar-goes-further-overseas\">your dollar goes further overseas</a>\u201d, compelling the US to ratify the UNCRC might be a very cost-effective way of improving the lives of American children as well as global and future children. By ratifying the UNCRC, countries are required to submit a report within two years of ratification and every five years thereafter. The UN Committee on the Rights of the Child has adopted these <a href=\"https://www.ohchr.org/en/treaty-bodies/crc/reporting-guidelines\">guidelines</a> detailing the information States are expected to give in their implementation reports. In doing so, the US would need to take a hard look at their child laws.</p><p>The next step in the process of ratification of this UN treaty, would be for the president to present the treaty to the senate for approval. It has not been done. The last president to consider it was then-senator Barack Obama, in <a href=\"http://www.youthdebate2008.org/debate-transcript\">this debate</a>, when he indicated it was \u201cembarrassing\u201d the USA had not ratified it, and had promised to \u201creview this and other treaties to ensure the United States resumes its global leadership in human rights\u201d, yet it did not happen. While Obama\u2019s running mate is president, and during a current election cycle, it might be an opportune time to influence policy.</p><p>According to the <a href=\"https://www.aclu.org/news/human-rights/theres-only-one-country-hasnt-ratified-convention-childrens\">American Civil Liberties Union</a>, opposition to ratifying the UNCRC is based on incorrect assumptions about its implications for U.S. law and how the Convention affects U.S. sovereignty. In 2012, a volunteer-driven network of academics, attorneys, child and human rights advocates, educators, members of religious and faith-based communities, physicians, representatives from non-governmental organizations (NGOs), students, and other concerned citizens tried and failed to lobby Obama to get it ratified. They failed so badly their website <a href=\"https://web.archive.org/web/20130823173247/http:/childrightscampaign.org/the-facts\">here</a>, has been web archived in 2013, and none of the contacts have responded to inquiries, highlighting the neglectedness of lobby efforts. Human Rights Watch continues to <a href=\"https://www.hrw.org/news/2019/11/18/america-should-not-lag-behind-protecting-children\">advocate</a> for ratification, indicating \u201cthe main argument raised against ratifying the convention is that it is \u2018anti-family\u2019 and a threat to parental authority\u201d, is inherently false, as \u201cthe convention instructs governments to respect the responsibilities, rights and duties of parents and to support families in their efforts to raise and care for their children.\u201d</p><p>Lobby efforts to persuade the US government to ratify the UNCRC could serve as a test case of EA policy influence; achieving international consensus on the rights of children is a definable, concrete outcome in international policy. Not only this, but it aligns well with two of WWOTF\u2019s recommended personal decisions deemed \u201chigh-impact\u201d on how to do good in the world, political activism, and spreading ideas, while protecting the third recommendation \u2013 producing future children. Notably, ensuring child rights also follows the first of the \u201cthree rules of thumb\u201d, taking actions that we can be comparatively confident are good; by stabilizing the affirmation of the rights of current children we can mitigate lock-in of bad values on the treatment towards future children.</p><p>Achieving international consensus on the rights of children could be a tractable first successful step for EA\u2019s international policy influence \u2013 which could inform the approach to global policies on a wider array of pressing issues, such as comprehensive child health care, the rights of future people, the rights of nonhuman animals, AI alignment, etc. The lobby efforts might dovetail nicely with the EAs working on <a href=\"https://www.un.org/en/content/common-agenda-report/\">UN\u2019s Our Common Agenda</a>, for example, the <a href=\"https://www.simoninstitute.ch/\">Simon Institute for Longterm Governance</a> may be interested in \u201csupporting policymakers in their cooperation with future generations\u201d, in endorsing child rights, as part of goal #1 \u201cLeave No One Behind\u201d. Particularly if we are close to reaching the world\u2019s <a href=\"https://ourworldindata.org/peak-child#:~:text=From%20today%20to%20the%20peak,reach%20a%20very%20flat%20peak.\">peak child</a>, now is the time to lock in the rights of children.</p><p><br>3. <strong>An Alternative to Charity/Philanthropy</strong></p><p>The Oxford dictionary defines philanthropy as \u201ca desire to promote the welfare of others\u201d. EA and philanthropy more generally however, have <a href=\"https://ssir.org/books/reviews/entry/are_the_elite_hijacking_social_change#\">come under criticism</a> for undermining systems change by supporting inequitable current systems. Promoting child rights, and human rights more broadly, is one means of doing the most good in promoting the welfare of others through creating systems change.</p><p><br>4. <strong>The Child as a Metaphor</strong></p><p>Ever since <a href=\"https://www.youtube.com/watch?v=wMb26ryjDuU\">Peter Singer\u2019s</a> drowning child thought experiment, children have represented the disadvantaged in Effective Altruism. I would suggest current children provide an easy metaphor for future people, having no agency in political decision-making, as the involuntary free riders of whose welfare we must be custodians. Who is more important than the very first of our future generations? I am not arguing that current children are more important than future people, rather, that the welfare of future people depends in part on the rights, welfare, health, education, and wellbeing of current children. Particularly if we have reached peak child, as noted above, there is an urgency to ensuring children thrive to reproduce to create and raise thriving future generations.</p><p><br>5. <strong>Babies Might be Good for (EA) Business</strong></p><p>I welcome discussion, but I suspect there are few to no people in the world who would not want to uphold and promote the rights of children. Endorsing child rights as baby longtermism might be good for EA business, as a more palatable means of understanding the value-change to longtermism. Some have been known to be put off by less mainstream areas such as AI alignment, wild animal welfare, and extreme longtermism. Could promotion of child rights bring more people to the EA movement, and could child rights be the start of increasing newcomers\u2019 moral expansiveness? I would suggest it is something to be approached with <a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation\">judicious ambition</a>.</p><p>Secondarily, Effective Altruism is known to have <a href=\"https://forum.effectivealtruism.org/posts/ThdR8FzcfA8wckTJi/ea-survey-2020-demographics\">exceptionally limited diversity in its membership</a>, and is only just starting to rectify the equity, diversity, and inclusion (EDI) problem. Promoting child rights necessarily includes the rights of all children, all races, genders, religions, and nations. One would hope that by promoting child rights, more people will see themselves represented and included in the EA movement, meanwhile moral expansiveness increases. Also, by improving the social safety net of children, their parents - usually mothers - are freed from the exclusive burden of care of their children. In this way, endorsing child rights naturally also supports women\u2019s rights. If it takes a village to raise a child, and the [global] village were better at taking care of our children, women could be endowed with greater agency.</p><p><br><strong>Arguments Against Promoting the Rights of the Child</strong><br>What are the counterfactuals? Would the lobby efforts create a poor opinion of EA with certain political parties in the US? Insofar as it alienates parties who are opposed to child rights, I suppose it is possible. Associated with this argument, is the risk of criticism of EA as political influencer. Sam Bankman-Fried was <a href=\"https://www.politico.com/news/2022/08/04/democratic-megadonor-sam-bankman-fried-00049048\">criticized</a> for endorsing Joe Biden as the most hopeful candidate to address pandemic response. This risk to EA reputation as wealthy influencer might be mitigated by backing civil rights organizations which are already doing the work on child rights.</p><p>A second argument against promoting US ratification of the UNCRC is the cost/benefit consideration, for which I have had trouble finding useful data, as there is no precedent. The primary barrier to ratification is likely the degree of opposition within the United States. I am not American and have not invested in a comprehensive understanding of its political systems. If the president could be compelled to present the UNCRC to the senate for ratification, but there is no motivation or even overt opposition from the senate to ratification, the efforts are useless. This is part of the reason I propose it as a \u201chits-based\u201d cause area; it is a potentially large expenditure which might end in failure, but a positive outcome could be large for EA as policy-changer in the international policy sphere, and of course, for child rights. I would suggest that there would still be a small but measurable gain in getting child rights on the agenda in the US, which could engage public popularity, force states to scrutinize their laws, publicly shame the US for its non-ratification status, etc.</p><p>Another argument against compelling the US to ratify the UNCRC is a question of the marginal benefit of taking near-consensus to consensus on the treaty. Arguably, countries which have already ratified it, continue to permit heinous child rights violations. I would suggest the marginal benefit here would be to US children, and in putting a spotlight on child rights internationally, thereby engaging public opinion, as indicated above.</p><p><br><strong>Conclusion</strong><br>If you think the idea of the EA community lobbying the US government to ratify the UNCRC is a reasonable cause area, there seems to be considerable need for every and all components of the lobby. First, the money to support the lobby. Second, collaborators to organize the movement. Ideally, these would be Americans who understand the nuances of the US political system, have success in lobbying, and have politically prominent connections. Third, idea generators and organizers who can best determine how to approach the lobby. Should it be a coordinated approach, consolidating money and efforts, or a multi-pronged, diverse approach? I would think the latter would be more successful; possibly one effort could gain traction even if several others fail. Support might be created in financial backing for civil rights organizations to lobby government, petitions distributed on social media sites, or other information-sharing sites, such as blogs, vlogs, and podcasts. &nbsp;Possibly better still - <a href=\"https://www.tandfonline.com/doi/full/10.1080/13642987.2021.1968377\">child activism</a> \u2013 because there are no living people more invested in child rights, than children. How might they be involved?</p><p>Of course, I welcome any criticism on the idea of lobbying the US government to ratify the UNCRC; what are the downside counterfactuals I have not considered?</p><p><br>- Lia Harris<br>&nbsp;</p>", "user": {"username": "LiaH"}}, {"_id": "dvz2FWu2fTBG9E2oe", "title": "Should Patient Philanthropists Invest Differently?", "postedAt": "2022-09-14T20:16:02.724Z", "htmlBody": "<p><img src=\"https://mdickens.me/assets/images/patience.png\" alt=\"\"></p>\n<p><strong>TLDR:</strong> No.</p>\n<p><em><a href=\"https://mdickens.me/confidence_tags/\">Confidence</a>: Somewhat likely.</em></p>\n<h2>Summary</h2>\n<p>Some philanthropists <a href=\"https://80000hours.org/podcast/episodes/phil-trammell-patient-philanthropy/\">discount the future</a> much less than normal people. For philanthropists with low discount rates, does this change how they invest their money? Can they do anything to take advantage of other investors' high time discounting?</p>\n<p>We can answer this question in two different ways.</p>\n<p>Should low-discount philanthropists invest differently <em>in theory</em>? No. <a href=\"#Theory\">[More]</a></p>\n<p>Should low-discount philanthropists invest differently <em>in practice</em>? The real world differs from the standard theoretical approach in a few ways. These differences suggest that low-discount philanthropists should favor risky and illiquid investments slightly more than high-discount investors do. But the difference is too small to matter in practice. <a href=\"#Practice\">[More]</a></p>\n<p><em>Cross-posted to <a href=\"https://mdickens.me/2022/09/14/should_patient_philanthropists_invest_differently/\">my website</a>.</em></p>\n<h1>Background</h1>\n<p>Many people have a positive pure time preference: they prefer present consumption over future consumption, regardless of any empirical facts. Philanthropists shouldn't have a <em>pure</em> positive discount rate, but they do have <a href=\"https://mdickens.me/2020/07/03/estimating_discount_rate/\">some legitimate reasons</a> to discount the future, including:</p>\n<ul>\n<li>An extinction or economic collapse could render their money useless</li>\n<li>Their money could get expropriated (taxed, seized, or stolen)</li>\n<li>They might not be able to ensure that future generations continue spending their wealth in the ways they want</li>\n</ul>\n<p>(Some people talk about impatient vs. patient philanthropists who have positive vs. zero pure time preference, or <a href=\"https://forum.effectivealtruism.org/posts/Eh7c9NhGynF4EiX3u/patient-vs-urgent-longtermism-has-little-direct-bearing-on\">urgent vs. patient longtermists</a> who believe we do vs. don't live in a particularly important time. For this essay, I'm talking about low-discount vs. high-discount philanthropists. Urgent longtermists usually have high discount rates and patient longtermists usually have low rates, but it's not a 1:1 correspondence.)</p>\n<p>Many philanthropists don't want to spend all their money at once; any money they don't spend gets invested. And we would like to know how we should invest. For this essay, I will ask: how does optimal investment strategy change with discount rate? For philanthropists with unusually low discount rates, what should they do differently?</p>\n<p>Suppose philanthropists have a pool of wealth that they invest, and they consume a fixed proportion of wealth each year. (For a philanthropist, donating to charity counts as consumption.) They get some utility every time they spend money. What investment strategy maximizes expected utility? And how does that investment strategy vary as a function of discount rate?</p>\n<h1>Theory</h1>\n<p><a href=\"http://lifecycleinvesting.net/Resources/merton%20lifetime%20portfolio%20selection%201969.pdf\">Merton (1969)</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-1\" id=\"fnref-6gyPT2wFp46JPHDem-1\">[1]</a></sup><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-2\" id=\"fnref-6gyPT2wFp46JPHDem-2\">[2]</a></sup> derived a formula for how much leverage investors should use and how quickly they should spend down their assets (given some simplifying assumptions). Merton found that the optimal investment allocation does not depend on the investor's discount rate.</p>\n<p>In theory, the optimal allocation only depends on three things:</p>\n<ol>\n<li>assets' expected returns</li>\n<li>assets' covariances with each other</li>\n<li>the investor's relative risk aversion (RRA)</li>\n</ol>\n<p>The first two are properties of the assets themselves, not of the investors. The only factor that (theoretically) should cause two investors to behave differently is their degree of risk aversion.</p>\n<p>The optimal rate of <em>consumption</em><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-3\" id=\"fnref-6gyPT2wFp46JPHDem-3\">[3]</a></sup> does depend on one's discount rate.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-4\" id=\"fnref-6gyPT2wFp46JPHDem-4\">[4]</a></sup> But the discount rate does not affect the choice of investment.</p>\n<p>Merton's proof depends on four key assumptions:</p>\n<ol>\n<li>Investors have constant relative risk aversion.</li>\n<li>Asset prices follow log-normal distributions.</li>\n<li>Asset returns are independent across time.</li>\n<li>Assets can be traded continuously.</li>\n</ol>\n<p>The assumption of constant relative risk aversion means that investors' risk preferences do not change when their wealth increases or decreases. This might be false, but it's close enough to true, so I won't question this assumption for now.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-5\" id=\"fnref-6gyPT2wFp46JPHDem-5\">[5]</a></sup></p>\n<p>If asset prices don't follow log-normal distributions, then you cannot fully describe assets' performance in terms of mean and covariance. This affects investment strategy (mostly by making us more concerned about tail risk), but it doesn't affect how quickly we should consume.</p>\n<p>The third assumption\u2014time independence\u2014matters the most for our purposes. If investment returns tend to regress to the mean\u2014in other words, if present and future returns are anticorrelated\u2014then this changes how you should invest.</p>\n<p>Why? If returns are anticorrelated, then if you lose some money now, that increases your expectation that you'll make it back later. It also decreases the probability that you'll be able to compound unusually high returns for multiple years in a row. This matters regardless of discount rate, but it matters <em>more</em> for investors with low discount rates because they care more about fluctuations in their future wealth.</p>\n<p>The fourth assumption\u2014perfect liquidity\u2014does not directly relate to investors' discount rates, but it can still matter, as I discuss <a href=\"#3__Illiquidity_premium\">below</a>.</p>\n<h1>Practice</h1>\n<p>Low-discount philanthropists might invest differently for three reasons:</p>\n<ol>\n<li>long-term mean reversion</li>\n<li>subjective uncertainty about future returns</li>\n<li>illiquidity premium</li>\n</ol>\n<p>Long-term mean reversion suggests that:</p>\n<ul>\n<li>Risk-averse (specifically, more risk-averse than logarithmic) low-discount investors should use more leverage than risk-averse high-discount investors.</li>\n<li>Risk-tolerant (more risk-tolerant than logarithmic) low-discount investors should use <em>less</em> leverage than risk-tolerant high-discount investors.</li>\n</ul>\n<p>(Confidence: Highly likely.)</p>\n<p>This crude drawing shows how leverage varies for low-discount vs. high-discount investors after accounting for long-term mean reversion:</p>\n<p><img src=\"https://mdickens.me/assets/images/leverage-by-discount-crude.png\" alt=\"\"></p>\n<p>But this drawing overstates the size of the effect. (Or at least it would, if it had any labels or scale.) Empirically (based on historical returns), mean reversion only slightly changes the optimal amount of leverage, and probably doesn't matter in practice. (Confidence: Likely.) <a href=\"#1__Long_term_mean_reversion\">[More]</a></p>\n<p>Over longer time periods, investors have greater subjective uncertainty about how markets will behave. This creates subjective long-term momentum, which has the opposite effect of mean reversion. (Confidence: Likely.) And, like with mean reversion, this effect only slightly differs between low- and high-discount investors. (Confidence: Likely.) <a href=\"#2__Uncertainty_about_future_returns\">[More]</a></p>\n<p>If long-term illiquid investments earn significantly higher returns, then investors with low discount rates prefer them relative to high-discount investors. (Confidence: Highly likely.) <a href=\"#3__Illiquidity_premium\">[More]</a> But empirical data suggests they don't earn an illiquidity premium, so illiquid investments don't look particularly compelling, regardless of discount rate. (Confidence: Likely.) <a href=\"#Observed_illiquidity_premium\">[More]</a></p>\n<p>Some investors may prefer illiquid investments for behavioral reasons due to their (perceived but not actual) reduced volatility, and for these investors, a lower discount rate suggests a higher allocation to private equity. <a href=\"#Reconciling_predicted_and_observed_illiquidity_premiums\">[More]</a></p>\n<p>As a final point, conventional wisdom says long-term investors should take more risk. Conventional wisdom is correct, not because long-term investors have a lower discount rate, but because they can expect to earn future income which reduces their effective risk. So this observation is not relevant to how low-discount philanthropists should invest. (Confidence: Almost certain.) <a href=\"#A_note_on_conventional_wisdom\">[More]</a></p>\n<h2>1. Long-term mean reversion</h2>\n<p><img src=\"https://mdickens.me/assets/images/boomerang-throw.png\" alt=\"\"></p>\n<p>When Merton proved that asset allocation does not depend on time discounting, he assumed that investment returns are independent across time. But that's not quite true. Historical evidence (weakly<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-6\" id=\"fnref-6gyPT2wFp46JPHDem-6\">[6]</a></sup>) suggests that stocks exhibit long-term mean reversion. When prices go up a lot over a 3+ year period, they have a slight tendency to go back down. And when they go down over 3+ years, they have a similar tendency to rebound upward. That makes stocks a little less risky over longer holding periods.</p>\n<p>How does this affect low-discount investors relative to high-discount investors?</p>\n<p>Long-term mean reversion decreases the variance of an investment while keeping its <a href=\"https://corporatefinanceinstitute.com/resources/knowledge/other/what-is-geometric-mean/\">geometric mean</a> fixed, and decreasing the arithmetic mean.</p>\n<p>Low-discount investors feel the decreased volatility more strongly than high-discount investors do because they care more about their investment performance in later years. But for the same reason, they more strongly feel the decrease in arithmetic mean return.</p>\n<p>How this affects investors depends on their degree of risk aversion:</p>\n<ol>\n<li>For investors with logarithmic utility, maximizing utility is equivalent to maximizing geometric mean, so they don't care at all about mean reversion.</li>\n<li>For \"risk-averse\" investors (those with sub-logarithmic utility), they care more about volatility than about arithmetic mean, so they like mean reversion.</li>\n<li>For \"risk-tolerant\" investors (those with super-logarithmic utility), they care more about arithmetic mean than about volatility, so they dislike mean reversion.</li>\n</ol>\n<p>Risk-averse low-discount investors want <em>more</em> leverage than risk-averse high-discount investors, while risk-tolerant low-discount investors want <em>less</em> leverage than risk-tolerant high-discount investors.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-7\" id=\"fnref-6gyPT2wFp46JPHDem-7\">[7]</a></sup></p>\n<p>More precisely:</p>\n<ol>\n<li>At logarithmic utility, discount rate does not matter.</li>\n<li>For risk-averse investors, desired leverage decreases with discount rate.</li>\n<li>For risk-tolerant investors, desired leverage increases with discount rate.</li>\n</ol>\n<p>At least that's the hypothesis.</p>\n<p>How strong are these effects? It turns out, pretty weak. I used data from <a href=\"https://academic.oup.com/qje/article/134/3/1225/5435538\">The Rate of Return on Everything, 1870\u20132015</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-8\" id=\"fnref-6gyPT2wFp46JPHDem-8\">[8]</a></sup> and the <a href=\"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\">Ken French Data Library</a> to calculate historical utility-maximizing leverage for an equity investor in three environments: the US market from 1872 to 2015; the UK market from 1871 to 2015; and the developed-world markets (excluding the US) from 1990 to 2021.</p>\n<p>Each of these historical cases confirmed the hypothesis, but they showed a weak effect size.</p>\n<p>These two charts show excess leverage by discount rate for US and UK equities, compared to a baseline 3% discount rate:</p>\n<p><img src=\"https://mdickens.me/assets/images/excess-leverage-by-discount-rate-us.png\" alt=\"\"></p>\n<p><img src=\"https://mdickens.me/assets/images/excess-leverage-by-discount-rate-uk.png\" alt=\"\"></p>\n<p>(I didn't graph excess leverage for RRA &lt; 1 because a solution usually does not exist.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-9\" id=\"fnref-6gyPT2wFp46JPHDem-9\">[9]</a></sup>)</p>\n<p>We can see from these charts that a zero-discount investor wants at most 4 percentage points more leverage than an investor with a 3% discount rate (1.78:1 vs. 1.74:1, respectively<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-10\" id=\"fnref-6gyPT2wFp46JPHDem-10\">[10]</a></sup>). And the effect gets smaller as relative risk aversion (RRA) gets closer to 1.</p>\n<p>So, while long-term mean reversion does theoretically change how low-discount philanthropists should invest, the difference is effectively a rounding error. (I can't even say whether philanthropists should prefer (say) 1.5:1 leverage or 2:1 leverage, much less 1.78:1 or 1.74:1.)</p>\n<p>(I should mention that, while equities exhibit long-term mean reversion, bonds appear to show long-term momentum, which would produce the opposite effect. However, historically, bonds did not have stable geometric means, and the historically optimal leverage for bonds showed the same results as for equities with respect to discount rate and RRA. And as with equities, the effect is too small to matter in practice.)</p>\n<p>See <a href=\"#Appendix_B__How_I_calculated_historical_optimal_leverage\">Appendix B</a> for the details of my calculations.</p>\n<h2>2. Uncertainty about future returns</h2>\n<p><img src=\"https://mdickens.me/assets/images/uncertainty-stock-photo.png\" alt=\"\"></p>\n<p><a href=\"https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.2012.01722.x\">Pastor &amp; Stambaugh (2012)</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-11\" id=\"fnref-6gyPT2wFp46JPHDem-11\">[11]</a></sup> showed that, due to uncertainty about average future returns, investors' subjective volatility increases with time horizon. This produces the opposite effect of mean reversion: risk-averse low-discount investors want <em>less</em> leverage than risk-averse high-discount investors, while risk-tolerant low-discount investors want <em>more</em> leverage than risk-tolerant high-discount investors.</p>\n<p>Behold, another crudely-drawn graph:</p>\n<p><img src=\"https://mdickens.me/assets/images/leverage-by-discount-crude-uncertainty.png\" alt=\"\"></p>\n<p>According to Pastor &amp; Stambaugh, parameter uncertainty matters more than long-term mean reversion, but less than twice as much more. That means the combined impact of mean reversion + uncertainty has the opposite sign and a smaller magnitude than mean reversion alone. As we saw, the practical impact of mean reversion only varies slightly between low-discount and high-discount investors, so the impact of mean reversion + uncertainty varies even less.</p>\n<p>Pastor &amp; Stambaugh did not look at parameter uncertainty across discount rates, so I did my own analysis. I tested historically optimal leverage with parameter uncertainty by adding an \"uncertainty factor\" to the average return. This uncertainty factor was a normally-distributed random number between \u20132% and 2% (with that range representing the 95% confidence interval of the true average return).</p>\n<p>As expected, the uncertainty factor does not affect optimal leverage for logarithmic-utility investors. For risk-averse investors, uncertainty decreases optimal leverage, and it decreases more for low-discount investors than for high-discount ones:</p>\n<p><img src=\"https://mdickens.me/assets/images/excess-leverage-due-to-uncertainty-us.png\" alt=\"\"></p>\n<p>(UK equities show the same pattern, but a weaker effect.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-12\" id=\"fnref-6gyPT2wFp46JPHDem-12\">[12]</a></sup>)</p>\n<p>When incorporating both mean reversion and uncertainty, risk-averse low-discount investors still preferred to use more leverage than risk-averse high-discount investors, but by a smaller margin than they would given certainty:<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-13\" id=\"fnref-6gyPT2wFp46JPHDem-13\">[13]</a></sup></p>\n<p><img src=\"https://mdickens.me/assets/images/excess-uncertainty-adjusted-leverage-us.png\" alt=\"\"></p>\n<p>(Notice how on this chart, the maximum excess leverage is ~0.03, while in the original chart with fixed parameters (\"Excess Leverage by Discount Rate (US)\"), the max was ~0.04.)</p>\n<p>UN equities show a similar effect, except that in some cases, low-discount investors want (slightly) <em>less</em> leverage than high-discount investors:</p>\n<p><img src=\"https://mdickens.me/assets/images/excess-uncertainty-adjusted-leverage-uk.png\" alt=\"\"></p>\n<p>For risk-tolerant investors (with super-logarithmic utility), adding uncertainty increases desired leverage, and the effect is larger for low-discount investors. However, it's hard to quantify this effect because at low discount rates, sufficiently risk-tolerant investors want to use infinite leverage. (I would interpret this as evidence that philanthropists don't have larger-than-logarithmic utility functions.)</p>\n<p>Note that Pastor &amp; Stambaugh found that parameter uncertainty outweighed mean reversion, while I found the opposite. If my analysis had looked more like Pastor &amp; Stambaugh's, then we would get the opposite result after accounting for both mean reversion and parameter uncertainty. But it doesn't really matter because the effect is small either way.</p>\n<h2>3. Illiquidity premium</h2>\n<p><img src=\"https://mdickens.me/assets/images/viscous-sd.png\" alt=\"\">\n<em>I wanted to represent illiquidity as viscous black tar dripping onto a table, but instead it<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-14\" id=\"fnref-6gyPT2wFp46JPHDem-14\">[14]</a></sup> gave me a viscous table dripping upward onto a ball of tar, which actually even better captures illiquidity because tables are less liquid than tar.</em></p>\n<p>Low-discount philanthropists might not need to spend their money for a long time. Does that mean they should buy illiquid investments?</p>\n<p>I will focus on private equity because it's the most straightforward type of illiquid investment, and it has a clear analogue in public equity. First I will consider what illiquidity premium we should expect in theory for private equity. Then I will look at the observed illiquidity premium, and then attempt to reconcile the two.</p>\n<p>See <a href=\"#Appendix_E__Illiquidity_in_real_estate\">Appendix E</a> for some discussion of illiquidity in real estate.</p>\n<h3>Theoretical illiquidity premium</h3>\n<p>In theory, should low-discount investors like illiquid investments more than high-discount investors do?</p>\n<p>People's preferences for liquidity do not directly depend on discount rate, but they do depend on their consumption rate. If you want to spend a (relatively) large percentage of your capital each year, then you're more reluctant to lock up a big chunk of money in illiquid investments. There's a risk that the market tanks and you end up spending down too big a proportion of your liquid assets.</p>\n<p>All else equal, a higher discount rate means a higher spending rate. Low-discount philanthropists may consume only a tiny fraction of their wealth each year, which means they're not concerned about over-spending their liquid investments.</p>\n<p>In a two-asset model with public equity and (<a href=\"#Appendix_C__How_I_calculated_optimal_private_equity_allocation\">synthetic</a>) private equity, low-discount investors allocate 1 to 5 percentage points more to private equity than high-discount investors:</p>\n<p><img src=\"https://mdickens.me/assets/images/pe-absolute-allocation-0pct.png\" alt=\"\"></p>\n<p><img src=\"https://mdickens.me/assets/images/pe-relative-allocation-0pct.png\" alt=\"\"></p>\n<p>(\"Absolute\" is the allocation as a percentage of wealth/liquidation value; \"relative\" is the allocation as a percentage of public + private allocation. Public + private absolute allocation can exceed 100% due to leverage.)</p>\n<p>See <a href=\"#Appendix_C__How_I_calculated_optimal_private_equity_allocation\">Appendix C</a> for the details on methodology.</p>\n<h3>Observed illiquidity premium</h3>\n<p>How large a premium can investors earn by holding private equity instead of public stocks?</p>\n<p>Historically, private equity in the United States earned a 3% premium (<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1932316\">Harris et al., 2014</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-15\" id=\"fnref-6gyPT2wFp46JPHDem-15\">[15]</a></sup>; <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2627312\">Kaplan &amp; Sensoy, 2015</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-16\" id=\"fnref-6gyPT2wFp46JPHDem-16\">[16]</a></sup>). However, this premium is not explained by illiquidity. The private equity premium is fully subsumed by a leveraged small value portfolio in public equities (<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2639647\">Chingono &amp; Rasmussen, 2015</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-17\" id=\"fnref-6gyPT2wFp46JPHDem-17\">[17]</a></sup>; <a href=\"https://www.aqr.com/Insights/Research/White-Papers/Demystifying-Illiquid-Assets-Expected-Returns-for-Private-Equity\">Ilmanen et al., 2019</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-18\" id=\"fnref-6gyPT2wFp46JPHDem-18\">[18]</a></sup>; <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2720479\">Stafford, 2017</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-19\" id=\"fnref-6gyPT2wFp46JPHDem-19\">[19]</a></sup>). In fact, private equity has (large but statistically insignificant) <em>negative</em> alpha compared to leveraged small value.</p>\n<p>Given zero (or negative) alpha for private equity, investors do not want to hold it, regardless of discount rate.</p>\n<h3>Reconciling predicted and observed illiquidity premiums</h3>\n<p>If theory predicts an illiquidity premium, why don't we see one in historical data?</p>\n<p>The most common explanation (e.g., as given in <a href=\"https://www.aqr.com/Insights/Research/White-Papers/Demystifying-Illiquid-Assets-Expected-Returns-for-Private-Equity\">Ilmanen et al., 2019</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-18\" id=\"fnref-6gyPT2wFp46JPHDem-18:1\">[18:1]</a></sup> and <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2639647\">Chingono &amp; Rasmussen, 2015</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-17\" id=\"fnref-6gyPT2wFp46JPHDem-17:1\">[17:1]</a></sup>) is that investors are willing to pay a premium for the return smoothing provided by private equity. Unlike a public market where the price updates many times per second, private equity firms have leeway to report essentially whatever valuations they want. They tend to under-report drawdowns, making returns for private companies look more consistent than their public counterparts, but with no corresponding decrease in true underlying risk (<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2379170\">Welch &amp; Stubben, 2018</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-20\" id=\"fnref-6gyPT2wFp46JPHDem-20\">[20]</a></sup>). (In 2022 when the S&amp;P 500 experienced a 20+% drawdown, the average private equity firm reported a 0% change in valuation, which is <a href=\"https://www.aqr.com/Insights/Perspectives/Introducing-the-New-AQR-SMOOTH-Fund\">absurd</a> on its face, but they're allowed to do it.)</p>\n<p><img src=\"https://mdickens.me/assets/images/choffstein-tweet.png\" alt=\"\"></p>\n<p>Investors' preference for return smoothing is, in some sense, irrational\u2014it doesn't provide any financial benefit. But if you know that seeing your portfolio lose value will give you anguish, and might cause you to make a bad decision, then it could be rational to tie yourself to the mast by buying private equity.</p>\n<p>Evidence suggests that, at least in the current market environment, rational investors should not invest in private equity, regardless of discount rate. But human investors with typical human flaws may do better by investing in private equity, even if it's not technically optimal. And low-discount investors should allocate on the order of 5 percentage points more to private equity than high-discount investors do. Sophisticated institutional investors tend to allocate 10% to 15% to private equity,<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-21\" id=\"fnref-6gyPT2wFp46JPHDem-21\">[21]</a></sup> which would suggest a 15% to 20% allocation for philanthropists with low discount rates. This is a small enough difference that it might not matter in practice, but it's still a much bigger difference than we found for long-term mean reversion or parameter uncertainty.</p>\n<p>I personally do not invest in private equity because I'm not concerned about the behavioral issues (and because it's hard to invest in, but that's secondary).</p>\n<h2>A note on conventional wisdom</h2>\n<p>Conventional wisdom says that investors with long time horizons should take on more risk. Why? Does that apply to low-discount philanthropists?</p>\n<p>The conventional wisdom is correct for most people, but not because long-term investors <em>inherently</em> want more risk. Rather, long-term investors (usually) expect to earn more income in the future, and they should count that income as part of their investment portfolio. After factoring in their future income, they should seek the same amount of risk as a short-term investor.</p>\n<p>For example, say Alice is retired with $1 million, and she invests her retirement fund in 50% stocks, 50% bonds. Bob has $100,000 in savings and expects to earn another $900,000 before he retires. He invests 100% of his savings into stocks. Bob's portfolio looks riskier than Alice, but it's actually less risky: Alice has 50% of her portfolio in stocks, while Bob effectively only has 10% after accounting for his future income. In theory, If Bob used 5:1 leverage, he would get his effective stock allocation up to 50%, although that's probably not feasible in practice.</p>\n<p>For low-discount philanthropists, how much risk to take depends on what money they expect to earn in the future. That depends on their personal income plus the income of all likeminded philanthropists plus the wealth and income of any people in the future who will become likeminded. Figuring out all those details is beyond the scope of this essay. The bottom line is: yes, the conventional wisdom is correct in a sense, but it doesn't contradict my earlier claim that an investor's risk appetite does not depend on time horizon, and it doesn't have anything to do with discount rates.</p>\n<h1>Conclusion</h1>\n<p>In summary:</p>\n<ul>\n<li>According to standard theory, investors should not change their asset allocation based on their discount rate.</li>\n<li>Standard theory assumes that time periods are independent and that all investments are highly liquid, both of which are (sometimes) false.</li>\n<li>Investment returns are not entirely independent over time. This has a nonzero but negligible effect on high- vs. low-discount investors.</li>\n<li>It looks like illiquid assets don't earn a significant premium, so low-discount investors shouldn't prefer them.</li>\n</ul>\n<p>Therefore, low-discount philanthropists don't need to allocate any differently than high-discount investors.</p>\n<h1>Appendix A: Pedantry</h1>\n<p>In this essay, I somewhat conflated low-discount philanthropists with long-term philanthropists, but they're not necessarily the same thing. For our purposes, \"long-term\" means you have a (relatively) long time horizon. A 3% discount rate would be considered high, but a philanthropist with a 3% discount rate could still easily have a 50+ year time horizon.</p>\n<p>A low discount rate is not the same thing as longtermism (at least the way I use the term). A longtermist might have a high discount rate, for instance if they believe extinction is likely to render their funds useless in the near future, or if they expect not to be able to preserve the intent of their endowment across multiple generations. And someone with a 0% discount rate doesn't have to be a longtermist\u2014e.g., some form of person-affecting view could preclude longtermism even with no time discounting.</p>\n<p>Philanthropists with particularly pessimistic beliefs about existential risk might have <em>higher</em> discount rates than normal people, which would suggest doing the opposite of what low-discount philanthropists do (which, if I'm right, is nothing, so it doesn't matter either way).</p>\n<p>Not all low-discount investors are low-discount philanthropists, but I use the terms interchangeably because I only care about investors who are also philanthropists.</p>\n<h1>Appendix B: How I calculated historical optimal leverage</h1>\n<p>I used data from <a href=\"https://academic.oup.com/qje/article/134/3/1225/5435538\">The Rate of Return on Everything, 1870\u20132015 (RORE)</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-8\" id=\"fnref-6gyPT2wFp46JPHDem-8:1\">[8:1]</a></sup> and the <a href=\"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\">Ken French Data Library</a>, focusing on four data series:</p>\n<ol>\n<li>US equities 1926 to 2021, from Ken French</li>\n<li>Developed ex-US equities 1990 to 2021, from Ken French</li>\n<li>UK equities 1899 to 2015, from RORE</li>\n<li>US equities 1872 to 2015, from RORE</li>\n</ol>\n<p>(In the main essay, I did not discuss #1 because it's mostly a subset of #4. I only looked at both as a sanity check.)</p>\n<p>I implemented the standard <a href=\"https://plato.stanford.edu/entries/ramsey-economics/\">Ramsey model</a> of intergenerational welfare where we have a pool of wealth that we invest at some rate of return, and every year we consume some fixed percentage of our wealth. We have a utility function over consumption, and total utility is the sum of discounted utility of consumption for each period. Then we simultaneously solve for the optimal amount of portfolio leverage and optimal rate of consumption. (For this essay, we don't care about consumption, but we still have to solve for both variables.)</p>\n<p>Applying this model over historical data requires making some modifications. We can't calculate total utility out to infinity because we only have a finite sample of real historical financial returns. Naively, we could simply maximize utility over the sample we have, but this has two major problems:</p>\n<ol>\n<li>We don't discount the last year all that much relative to the first year. Any money left over at the end has a high opportunity cost. The optimizer wants to end up with as close to $0 as possible, so it recommends consuming at a faster rate than we'd want to if we had an infinite time horizon.</li>\n<li>The outcome strongly depends on what happens to our investments near the beginning of the sample because the beginning is discounted the least.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-22\" id=\"fnref-6gyPT2wFp46JPHDem-22\">[22]</a></sup></li>\n</ol>\n<p>I implemented (somewhat ad-hoc) solutions for both of these problems:</p>\n<ol>\n<li>Loop through the historical sample multiple times, extending it until it repeats for 1000 years.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-23\" id=\"fnref-6gyPT2wFp46JPHDem-23\">[23]</a></sup> The 1000th year has minimal impact on total utility, so the optimal solution over 1000 years looks almost exactly like the optimal solution over infinity years.</li>\n<li>Instead of only starting at the first year in the sample, calculate total utility for every possible starting year and then take the average.</li>\n</ol>\n<h2>Tables of results</h2>\n<p>The tables below give optimal leverage and consumption for various rates of relative risk aversion and discount rates, given a fixed-point expected return (no parameter uncertainty). I included 1% and 3% discount rates except for at 0.75 RRA, in which case the solution did not always exist, so I included higher discount rates instead. For RRA &gt; 1, I also included a 0% discount rate.</p>\n<p>For Ken French data (Tables 1 and 2), I applied discounting and consumption on a monthly basis and estimated the cost of leverage at the risk-free rate as given in the data.</p>\n<p>For RORE data (Table 3), I applied discounting and consumption on an annual basis and estimated the cost of leverage at 0% because I don't have data on the risk-free rate going back far enough. This is wrong, but it doesn't matter for our purposes.</p>\n<p>Table 1. US equities, 1926 to 2021.</p>\n<table>\n<thead>\n<tr>\n<th>RRA</th>\n<th>Discount</th>\n<th>Leverage</th>\n<th>Consumption</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.75</td>\n<td>1%</td>\n<td>DNE</td>\n<td>DNE</td>\n</tr>\n<tr>\n<td>0.75</td>\n<td>3%</td>\n<td>2.20</td>\n<td>0.43%</td>\n</tr>\n<tr>\n<td>0.75</td>\n<td>5%</td>\n<td>2.36</td>\n<td>2.65%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1%</td>\n<td>2.16</td>\n<td>1.00%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>3%</td>\n<td>2.16</td>\n<td>3.02%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0%</td>\n<td>1.47</td>\n<td>5.56%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1%</td>\n<td>1.45</td>\n<td>6.00%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>3%</td>\n<td>1.40</td>\n<td>6.89%</td>\n</tr>\n</tbody>\n</table>\n<p><em>Geometric mean is maximized at 1.90 leverage.</em></p>\n<p>Table 2. Developed ex-US equities, 1990 to 2021.</p>\n<table>\n<thead>\n<tr>\n<th>RRA</th>\n<th>Discount</th>\n<th>Leverage</th>\n<th>Consumption</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.75</td>\n<td>1%</td>\n<td>DNE</td>\n<td>DNE</td>\n</tr>\n<tr>\n<td>0.75</td>\n<td>3%</td>\n<td>1.57</td>\n<td>2.09%</td>\n</tr>\n<tr>\n<td>0.75</td>\n<td>5%</td>\n<td>1.58</td>\n<td>4.84%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1%</td>\n<td>1.56</td>\n<td>1.05%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>3%</td>\n<td>1.55</td>\n<td>3.03%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0%</td>\n<td>1.48</td>\n<td>2.98%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1%</td>\n<td>1.45</td>\n<td>3.43%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>3%</td>\n<td>1.42</td>\n<td>4.44%</td>\n</tr>\n</tbody>\n</table>\n<p><em>Geometric mean is maximized at 1.17 leverage.</em></p>\n<p>Table 3. UK equities, 1871 to 2015.</p>\n<table>\n<thead>\n<tr>\n<th>RRA</th>\n<th>Discount</th>\n<th>Leverage</th>\n<th>Consumption</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.9</td>\n<td>3%</td>\n<td>1.92</td>\n<td>1.92%</td>\n</tr>\n<tr>\n<td>0.9</td>\n<td>5%</td>\n<td>1.88</td>\n<td>4.10%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1%</td>\n<td>1.86</td>\n<td>1.00%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>3%</td>\n<td>1.86</td>\n<td>2.98%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0%</td>\n<td>1.79</td>\n<td>5.36%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1%</td>\n<td>1.78</td>\n<td>5.69%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>3%</td>\n<td>1.76</td>\n<td>6.54%</td>\n</tr>\n</tbody>\n</table>\n<p>A portfolio that combines the US market beta + value + momentum factors shows the same pattern (with RRA &lt; 1, optimal leverage increases with discount rate; with RRA &gt; 1, optimal leverage decreases with discount rate). I calculated this portfolio using Ken French data as the simple sum of 'Mkt' + 'HML' + 'Mom' (UMD).</p>\n<p>Table 4. US market beta + value + momentum factors, 1927 to 2021.</p>\n<table>\n<thead>\n<tr>\n<th>RRA</th>\n<th>Discount</th>\n<th>Lev</th>\n<th>Consumption</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.9</td>\n<td>3%</td>\n<td>3.05</td>\n<td>0.14%</td>\n</tr>\n<tr>\n<td>0.9</td>\n<td>5%</td>\n<td>3.15</td>\n<td>1.53%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1%</td>\n<td>3.04</td>\n<td>1.00%</td>\n</tr>\n<tr>\n<td>1</td>\n<td>3%</td>\n<td>3.04</td>\n<td>3.04%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0%</td>\n<td>1.57</td>\n<td>9.59%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1%</td>\n<td>1.56</td>\n<td>9.92%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>3%</td>\n<td>1.55</td>\n<td>10.82%</td>\n</tr>\n</tbody>\n</table>\n<p>This chart shows excess optimal leverage (relative to a baseline 3% discount rate) for a US market + value + momentum factor portfolio with RRA &gt; 1:</p>\n<p><img src=\"https://mdickens.me/assets/images/excess-uncertainty-adjusted-leverage-factors.png\" alt=\"\"></p>\n<p>This suggests that, for a factor investor, discount rate matters somewhat less than for a passive investor.</p>\n<h1>Appendix C: How I calculated optimal private equity allocation</h1>\n<p>There is some pre-existing literature on the theoretical illiquidity premium, such as Ang (2014).<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-24\" id=\"fnref-6gyPT2wFp46JPHDem-24\">[24]</a></sup> However, I could not find any research on how the illiquidity premium varies as a function of discount rate, so I did my own analysis.</p>\n<p>I calculated the optimal allocation across two assets labeled \"public equity\" and \"private equity\". I used RORE<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-8\" id=\"fnref-6gyPT2wFp46JPHDem-8:2\">[8:2]</a></sup> data on the US stock market back to 1872 as the public equity component. For private equity, we don't have data going back very far, so as a proxy, I used RORE data on UK public equities. Private equity behaves mostly like private equity, but not exactly; similarly, UK equity behaves mostly like US equity, but not exactly, so for this test, I took historical returns for UK public equity and pretended they represented private equity. Then I multiplied the UK annual returns by 1.5 to account for the fact that private equity tends to be more volatile.</p>\n<p>The resulting \"private equity\" return series has an 11.6% annualized return, compared to 8.9% for US public equity; this matches the observed 3% private equity premium.</p>\n<p>In my simulation, investors can only allocate to private equity once every 10 years. Any money they invest stays locked up, and they can rebalance at the end of every 10-year period. Investors can use leverage on public equity but not on private equity.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-25\" id=\"fnref-6gyPT2wFp46JPHDem-25\">[25]</a></sup></p>\n<p>This produced the following relative allocations to private equity at various discount rates and levels of risk aversion:</p>\n<p><img src=\"https://mdickens.me/assets/images/pe-relative-allocation-0pct.png\" alt=\"\"></p>\n<p>\"Private equity\" had +6.6% <a href=\"https://www.investopedia.com/terms/a/alpha.asp\">alpha</a> over public equity. If we subtract 6.6% from private equity returns to give it zero alpha, the optimal allocation becomes 0% regardless of risk aversion or discount rate.</p>\n<p>If we add a pure +2% premium to private equity, this increases the optimal allocation, and also increases the differential between low- and high-discount investors, with low-discount investors now allocating 2 to 9 percentage points more to private equity than high-discount investors:</p>\n<p><img src=\"https://mdickens.me/assets/images/pe-relative-allocation-2pct.png\" alt=\"\"></p>\n<p>If we make the lockup period longer than 10 years, it reduces the optimal allocation for all investors, and it increases the differential between low- and high-discount investors. The opposite happens if we shorten the lockup period.</p>\n<p>In some cases, there is no optimal allocation because it's always better to delay spending indefinitely. This condition holds at (RRA = 1, discount rate = 0%) and at RRA &lt; 1 for sufficiently low discount rates (including RRA = 0.8, discount rate = 3%).</p>\n<p>(Originally, I wanted to set the average return of the \"private equity\" asset such that the high-discount-optimal allocation reflects the real-world allocation to private equity, and then calculate the low-discount-optimal allocation given the same inputs. But I couldn't find good data on the total market cap of private equity, so I just used inputs that seemed reasonable.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-26\" id=\"fnref-6gyPT2wFp46JPHDem-26\">[26]</a></sup>)</p>\n<h1>Appendix D: Market volatility over longer holding periods</h1>\n<p>Stock markets exhibit long-term mean reversion. As shown in Appendix B, this encourages risk-averse investors with lower discount rates to use more leverage. I calculated this using some complicated<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-27\" id=\"fnref-6gyPT2wFp46JPHDem-27\">[27]</a></sup> and hard-to-intuit math. For a more intuitive approach, we could look at market volatility over different holding periods.</p>\n<p>Jeremy Siegel's <em>Stocks for the Long Run</em><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-28\" id=\"fnref-6gyPT2wFp46JPHDem-28\">[28]</a></sup> looked at the US equity market from 1802 to 2012 and found that volatility decreases over longer holding periods. I extended these results to global equities using data from <a href=\"https://academic.oup.com/qje/article/134/3/1225/5435538\">The Rate of Return on Everything, 1870\u20132015</a><sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-8\" id=\"fnref-6gyPT2wFp46JPHDem-8:3\">[8:3]</a></sup>. I looked at the annualized standard deviation<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-29\" id=\"fnref-6gyPT2wFp46JPHDem-29\">[29]</a></sup> of stock markets in 8 different countries<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-30\" id=\"fnref-6gyPT2wFp46JPHDem-30\">[30]</a></sup> for holding periods ranging from 1 to 30 years.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-31\" id=\"fnref-6gyPT2wFp46JPHDem-31\">[31]</a></sup></p>\n<p><img src=\"https://mdickens.me/assets/images/annualized-stdev-by-period-equities.png\" alt=\"\"></p>\n<p>(30 years is more or less the longest holding period we can say anything about because there are only four non-overlapping 30-year periods in the sample.)</p>\n<p>This chart shows a weak downward trend. On average, for every year that you increase the holding period, the annualized standard deviation decreases by 0.14% (p &lt; 0.002, but you shouldn't take this p-value literally<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-32\" id=\"fnref-6gyPT2wFp46JPHDem-32\">[32]</a></sup>), although this number is pretty sensitive to how far you extend the regression\u2014if you exclude the particularly-noisy 21 to 30 year holding period range, the slope reduces to 0.004% with p = 0.94. And we can't tell if the trend extends beyond 30 years, but it doesn't look like it does.<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-33\" id=\"fnref-6gyPT2wFp46JPHDem-33\">[33]</a></sup> So stocks are somewhere between 0 and 4 percentage points less volatile over 30 years than over 1 year.</p>\n<p><em>(Siegel (2014)<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-28\" id=\"fnref-6gyPT2wFp46JPHDem-28:1\">[28:1]</a></sup> found a larger and more unequivocal mean reversion effect. That's mostly because it only covered the US market, and you will notice that in the table above, the United States shows stronger mean reversion than most countries.)</em></p>\n<p>At the same time, stock markets' arithmetic means decrease over longer holding periods:</p>\n<p><img src=\"https://mdickens.me/assets/images/arithmetic-return-by-period-equities.png\" alt=\"\"></p>\n<p>Long-term mean reversion works in favor of risk-averse investors (RRA &gt; 1) and against risk-tolerant investors (RRA &lt; 1).</p>\n<p>Bonds get <em>more</em> volatile as you increase the holding period, from an average of 7% annualized volatility up to 12%:<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-34\" id=\"fnref-6gyPT2wFp46JPHDem-34\">[34]</a></sup></p>\n<p><img src=\"https://mdickens.me/assets/images/annualized-stdev-by-period-bonds.png\" alt=\"\"></p>\n<h1>Appendix E: Illiquidity in real estate</h1>\n<p>Real estate is less liquid than stocks, but not in a way that benefits low-discount philanthropists. Real estate costs money to trade, but it doesn't have lockup periods like private equity, so we shouldn't expect to see an illiquidity premium. There might be a \"trading cost premium\", which benefits investors with long holding periods, but real estate investors tend to have long holding periods anyway, so this doesn't make real estate look particularly good from a low-discount perspective.</p>\n<p>The main bottleneck to real estate investing is not liquidity but diversification. To properly diversify your real estate portfolio, you'd need to spend tens of millions of dollars buying dozens of properties. If you do have that much money, then investing in real estate is probably smart. Indeed, many wealthy, long-term-oriented investors such as university endowments own a lot of land.</p>\n<h2>Illiquidity premium in 100-year leases</h2>\n<p>Could low-discount philanthropists do well by buying much longer-term illiquid investments, such as 100-year property leases? Phil Trammell <a href=\"https://forum.effectivealtruism.org/posts/amdReARfSvgf5PpKK/phil-trammell-philanthropy-timing-and-the-hinge-of-history\">proposed</a> doing this. He claims that if you buy a house and simultaneously sell a 100-year lease, you gain the right to use the land 100 years from now and you only have to pay 10% of its present value. (I couldn't find good data on the cost of a 100-year lease, so I'll take his word for it.)</p>\n<p>A 10% price implies a 2.3% interest rate (<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1.023^{100} = 10\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1.023</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">100</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span>), which sounds basically fair\u2014maybe even too low. As an illustration, suppose you have $1 million and you're deciding between two choices:</p>\n<ol>\n<li>Buy 10 houses for a total of $10 million and simultaneously sell a 100-year lease on each of them. You spend $1 million on net, and you get 10 houses 100 years from now.</li>\n<li>Buy one house for $1 million and rent it out. Suppose the annual rent is 2.3% of the value of the house. Then in 100 years, you use the earned rent to buy the other nine houses. You spend $1 million, and you get 10 houses 100 years from now.</li>\n</ol>\n<p>These two scenarios cost the same and provide the same value. And 2.3% is on the low end of rental yields; if you can earn a higher yield than that, it would make more sense to simply buy a house rather than buy 10 houses and sell 100-year leases on them.</p>\n<p>(Why doesn't the market set a higher implied interest rate on 100-year leases? Probably because houses are tradeable(ish): your money isn't truly locked up for 100 years because you can sell the house at any time. So in an efficient market, the implied interest rate should equal the expected market yield (plus some other factors like differential taxes, maintenance costs, cost of selling the house multiplied by probability of wanting to sell, etc.)</p>\n<h1>Appendix F: Transformative changes to market behavior</h1>\n<p>Over sufficiently long time horizons, markets might see significant changes to investment performance, such as:</p>\n<ol>\n<li>Technological growth compounds on itself (possibly due to transformative AI), causing the rate of exponential growth to accelerate, greatly increasing returns.</li>\n<li>We reach the physical limits of economic growth, and investments stop growing exponentially.</li>\n</ol>\n<p>There might be interesting things to say about these or other possible futures that break the current paradigm of consistent exponential growth. But that's much more speculative, and I consider it out of scope for this essay.</p>\n<p>If you expect the economy to radically change in (say) 1000 years, but not to change much before then, then that doesn't significantly affect how to invest <em>today</em>.</p>\n<p>If you expect (with non-trivial probability) major changes in the near future (such as due to transformative AI), that does change how you should invest, but I'm not sure how,<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-35\" id=\"fnref-6gyPT2wFp46JPHDem-35\">[35]</a></sup> and that's a question for another time.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-6gyPT2wFp46JPHDem-1\" class=\"footnote-item\"><p>Robert Merton (1969). <a href=\"http://lifecycleinvesting.net/Resources/merton%20lifetime%20portfolio%20selection%201969.pdf\">Lifetime Portfolio Selection Under Uncertainty: The Continuous-Time_case.</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: Multiple full read-throughs. This is a seminal paper so I'm highly confident that it's good. <a href=\"#fnref-6gyPT2wFp46JPHDem-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-2\" class=\"footnote-item\"><p>Not all of the results I describe are original to Merton, but he provided the first single paper that proves all of them. <a href=\"#fnref-6gyPT2wFp46JPHDem-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-3\" class=\"footnote-item\"><p>For our purposes, \"consumption\" refers to anything we do with money other than re-investing it. For example, if we donate money, that counts as a form of consumption. <a href=\"#fnref-6gyPT2wFp46JPHDem-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-4\" class=\"footnote-item\"><p>The exact solution for the optimal consumption rate is given in formula (42) of <a href=\"http://lifecycleinvesting.net/Resources/merton%20lifetime%20portfolio%20selection%201969.pdf\">Merton (1969)</a>. All else equal, the rate of consumption increases linearly with the discount rate. <a href=\"#fnref-6gyPT2wFp46JPHDem-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-5\" class=\"footnote-item\"><p>If you become more risk-averse as you get wealthier, then you should consume a larger proportion of wealth when you're wealthier, and vice versa. <a href=\"#fnref-6gyPT2wFp46JPHDem-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-6\" class=\"footnote-item\"><p>Paul McCaffrey (2022). <a href=\"https://blogs.cfainstitute.org/investor/2022/07/29/equity-risk-premium-forum-term-structure-mean-reversion-and-cape-reconsidered/\">Equity Risk Premium Forum: Term Structure, Mean Reversion, and CAPE Reconsidered.</a> Partial transcript of a discussion between Laurence B. Siegel, Rob Arnott, Elroy Dimson, William N. Goetzmann, Roger G. Ibbotson, Antti Ilmanen, Martin Leibowitz, Rajnish Mehra, and Jeremy Siegel.</p>\n<p>Some relevant quotes:</p>\n<p>Goetzmann: \"The evidence is always a bit marginal and depends on your assumptions and on where you get the data.\"</p>\n<p>Ilmanen: \"I see evidence of mean reversion over time horizons from 3 years up to 15 years. [...] The evidence is really fuzzy, and usable or actionable evidence is almost zilch because of all this horizon uncertainty.\" <a href=\"#fnref-6gyPT2wFp46JPHDem-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-7\" class=\"footnote-item\"><p>One thing I learned from writing this essay: I find it easy to think about two-dimensional relationships (\"how does Y change as a function of X?\"), but far harder to think about three-dimensional relationships (\"how does 'how does Z change as a function of Y' change as a function of X?\"). I kept finding myself getting the variables mixed up. <a href=\"#fnref-6gyPT2wFp46JPHDem-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-8\" class=\"footnote-item\"><p>\u00d2scar Jord\u00e0, Katharina Knoll, Dmitry Kuvshinov, Moritz Schularick &amp; Alan M Taylor (2019). <a href=\"https://academic.oup.com/qje/article/134/3/1225/5435538\">The Rate of Return on Everything, 1870\u20132015.</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-8\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-8:1\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-8:2\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-8:3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-9\" class=\"footnote-item\"><p>To be precise, if prices follow a log-normal distribution, then a solution exists only when the following condition holds, where <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\delta\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;\">\u03b4</span></span></span></span></span></span> is the discount rate, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\gamma\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">\u03b3</span></span></span></span></span></span> is the rate of relative risk aversion, <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mu\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">\u03bc</span></span></span></span></span></span> is the expected (geometric) portfolio return minus the risk-free rate, and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\sigma\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;\">\u03c3</span></span></span></span></span></span> is the portfolio standard deviation:</p>\n<p><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\frac{-\\delta}{\\gamma} + (\\mu + \\sigma^2/2)\\frac{1-\\gamma}{\\gamma} - (1 - \\gamma)\\sigma^2/2 < 0\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.01em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 1.429em; top: -1.505em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;\">\u03b4</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 1.429em; bottom: -0.729em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">\u03b3</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.01em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.58em; vertical-align: -0.515em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">\u03bc</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.001em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;\">\u03c3</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.073em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.429em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 2.021em; top: -1.588em;\"><span class=\"mjx-mrow\" style=\"\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">\u03b3</span></span></span></span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 2.021em; bottom: -0.729em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">\u03b3</span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.429em;\" class=\"mjx-line\"></span></span><span style=\"height: 1.638em; vertical-align: -0.515em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;\">\u03b3</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.001em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;\">\u03c3</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.073em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span></span></span></span></span></p>\n<p>(Formula taken from Levhari &amp; Srinivasan (1969). \"Optimal Savings Under Uncertainty.\")</p>\n<p>This graph shows the minimum solvable discount rate as a function of RRA for <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mu = 7\\%, \\sigma=18\\%\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">\u03bc</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;\">\u03c3</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">18</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">%</span></span></span></span></span></span>:</p>\n<p><img src=\"https://mdickens.me/assets/images/min-solvable-discount-rate.png\" alt=\"\"> <a href=\"#fnref-6gyPT2wFp46JPHDem-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-10\" class=\"footnote-item\"><p>These specific numbers come from US equities with risk aversion coefficient = 2. <a href=\"#fnref-6gyPT2wFp46JPHDem-10\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-11\" class=\"footnote-item\"><p>Lubos Pastor &amp; Robert F. Stambaugh (2012). <a href=\"https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.2012.01722.x\">Are Stocks Really Less Volatile in the Long Run?</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: I read the relevant parts. <a href=\"#fnref-6gyPT2wFp46JPHDem-11\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-12\" class=\"footnote-item\"><p><img src=\"https://mdickens.me/assets/images/excess-leverage-due-to-uncertainty-uk.png\" alt=\"\"> <a href=\"#fnref-6gyPT2wFp46JPHDem-12\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-13\" class=\"footnote-item\"><p>Pastor &amp; Stambaugh (2012) found that the parameter uncertainty effect outweighs the mean reversion effect, while I (usually) found the opposite. The difference is mostly because:</p>\n<ol>\n<li>Pastor &amp; Stambaugh looked at subjective portfolio volatility, while I looked at optimal leverage to maximize utility of consumption. These two things are related, but not the same.</li>\n<li>Pastor &amp; Stambaugh assumed no discount rate, while I looked at discount rates ranging from 0% to 3%. Subjective uncertainty can outweigh mean reversion at low discount rates. (Of the two data points I found where subjective uncertainty outweighed mean reversion, one occurred at discount=0% and one at discount=1%.)</li>\n</ol>\n <a href=\"#fnref-6gyPT2wFp46JPHDem-13\" class=\"footnote-backref\">\u21a9\ufe0e</a></li>\n<li id=\"fn-6gyPT2wFp46JPHDem-14\" class=\"footnote-item\"><p>\"it\" = Stable Diffusion, running locally on my GeForce GTX 1070. <a href=\"#fnref-6gyPT2wFp46JPHDem-14\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-15\" class=\"footnote-item\"><p>Robert Harris, Time Jenkinson, Steven Kaplan (2014). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1932316\">Private Equity Perfomance: What Do We Know?</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: Abstract only. <a href=\"#fnref-6gyPT2wFp46JPHDem-15\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-16\" class=\"footnote-item\"><p>Steven Kaplan &amp; Berk Sensoy (2015). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2627312\">Private Equity Performance: A Survey.</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: Abstract only. I'm citing two papers with different methodologies but the same result, which increases my confidence without having to analyze the papers. <a href=\"#fnref-6gyPT2wFp46JPHDem-16\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-17\" class=\"footnote-item\"><p>Brian Chingono &amp; Daniel Rasmussen (2015). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2639647\">Leveraged Small Value Equities.</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: I read the relevant parts.</p>\n<p>See Figure 9 in <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2639647\">Chingono &amp; Rasmussen (2015)</a>, with explanation on the previous page. <a href=\"#fnref-6gyPT2wFp46JPHDem-17\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-17:1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-18\" class=\"footnote-item\"><p>Antti Ilmanen, Swati Chandra &amp; Nicholas McQuinn (2019). <a href=\"https://www.aqr.com/Insights/Research/White-Papers/Demystifying-Illiquid-Assets-Expected-Returns-for-Private-Equity\">Demystifying Illiquid Assets: Expected Returns for Private Equity.</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: Full read-through. <a href=\"#fnref-6gyPT2wFp46JPHDem-18\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-18:1\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-18:2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-19\" class=\"footnote-item\"><p>Erik Stafford (2017). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2720479\">Replicating Private Equity with Value Investing, Homemade Leverage, and Hold-to-Maturity Accounting.</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: Abstract only. <a href=\"#fnref-6gyPT2wFp46JPHDem-19\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-20\" class=\"footnote-item\"><p>Kyle Welch &amp; Stephen Stubben (2018). <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2379170\">Private Equity's Diversification Illusion: Evidence From Fair Value Accounting.</a> <a href=\"https://mdickens.me/2021/10/08/do_i_read_citations/\">Read status</a>: Abstract only, plus the explanation of this paper as cited in Ilmanen et al. (2019).<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-18\" id=\"fnref-6gyPT2wFp46JPHDem-18:2\">[18:2]</a></sup> <a href=\"#fnref-6gyPT2wFp46JPHDem-20\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-21\" class=\"footnote-item\"><p>Source: I think I read this somewhere. <a href=\"#fnref-6gyPT2wFp46JPHDem-21\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-22\" class=\"footnote-item\"><p>Empirically, this effect matters a lot. I tested the utility-maximizing parameters when starting at various start years and the results different substantially. I needed to average across 30+ start years to get a more stable result. <a href=\"#fnref-6gyPT2wFp46JPHDem-22\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-23\" class=\"footnote-item\"><p>Technically, I extended it to last 1000 years for RORE data and 10,000 months for Ken French data. 10,000 months isn't exactly 1000 years, but it's close enough. <a href=\"#fnref-6gyPT2wFp46JPHDem-23\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-24\" class=\"footnote-item\"><p>Andrew Ang (2014). Asset Management: A Systematic Approach to Factor Investing. Oxford University Press. <a href=\"#fnref-6gyPT2wFp46JPHDem-24\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-25\" class=\"footnote-item\"><p>You can leverage public securities by buying on margin or buying derivatives, but you usually can't borrow money to invest in private equity funds. <a href=\"#fnref-6gyPT2wFp46JPHDem-25\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-26\" class=\"footnote-item\"><p>In 2018, private equity firms <a href=\"https://www.mordorintelligence.com/industry-reports/global-private-equity-market\">spent $825 billion</a>. In 2020, the global stock market was <a href=\"https://www.visualcapitalist.com/all-of-the-worlds-money-and-markets-in-one-visualization-2020/\">worth $89 trillion</a>. If private equity firms tend to last 10 years, that means private equity is about 10% as big as public equity. So the efficient-market allocation for a normal investor would be ~90% to public equity, ~10% to private equity (ignoring all other asset classes). <a href=\"#fnref-6gyPT2wFp46JPHDem-26\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-27\" class=\"footnote-item\"><p>The math required isn't that complicated, it's just arithmetic. The complicated part is that the equations have over 140 variables (one for each year). <a href=\"#fnref-6gyPT2wFp46JPHDem-27\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-28\" class=\"footnote-item\"><p>Jeremy Siegel (2014). Stocks for the Long Run. 5th Edition. Page 97, Figure 6-2. <a href=\"#fnref-6gyPT2wFp46JPHDem-28\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-28:1\" class=\"footnote-backref\">\u21a9\ufe0e</a> <a href=\"#fnref-6gyPT2wFp46JPHDem-28:2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-29\" class=\"footnote-item\"><p>When returns are independent across time, standard deviation of annual returns decreases with the square root of the length of the holding period. To adjust for this, I \"annualized\" the computed standard deviation by multiplying by the square root of the holding period length. <a href=\"#fnref-6gyPT2wFp46JPHDem-29\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-30\" class=\"footnote-item\"><p>The data set includes 15 countries, but I only included countries that have continuous data from at least 1872 to 2015. Some countries' time series don't start until later, and a few have missing years in the middle, for example Japan's stock market closed for two years after WWII. Excluding such countries biases the average return upward, but I'm not looking at the expected return, so I'm not too concerned about bias. <a href=\"#fnref-6gyPT2wFp46JPHDem-30\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-31\" class=\"footnote-item\"><p>Chart shows nominal returns. Adjusting for inflation does not significantly change the results. <a href=\"#fnref-6gyPT2wFp46JPHDem-31\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-32\" class=\"footnote-item\"><p>The significance test assumes every sample is independent, but in reality, the (say) 20-year holding period sample is almost identical to the 21-year holding period sample. So a \"statistically significant\" result might still be noise. A more sophisticated significance test could probably correct for this, but I don't know how to do that. <a href=\"#fnref-6gyPT2wFp46JPHDem-32\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-33\" class=\"footnote-item\"><p>It makes more sense to model long-term mean reversion as a power function of holding period rather than a linear function. Most of the mean reversion happens in the first few years, and it quickly levels off. <a href=\"#fnref-6gyPT2wFp46JPHDem-33\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-34\" class=\"footnote-item\"><p>Siegel (2014)<sup class=\"footnote-ref\"><a href=\"#fn-6gyPT2wFp46JPHDem-28\" id=\"fnref-6gyPT2wFp46JPHDem-28:2\">[28:2]</a></sup> also found that US bonds exhibit long-term momentum. <a href=\"#fnref-6gyPT2wFp46JPHDem-34\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6gyPT2wFp46JPHDem-35\" class=\"footnote-item\"><p>I've spent on the order of 10 hours thinking about this question, and it's still very unclear to me. <a href=\"#fnref-6gyPT2wFp46JPHDem-35\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "MichaelDickens"}}, {"_id": "eBPgsxqfBA3CzovvJ", "title": "The EA Journal Project is hiring! ", "postedAt": "2022-09-14T16:01:51.176Z", "htmlBody": "<p><br>The EA Journal project (a project to start a new EA-aligned student research journal) is hiring for multiple roles including a:&nbsp;</p><ul><li>Managing editor&nbsp;</li><li>Outreach specialist&nbsp;</li><li>Design editor&nbsp;</li></ul><p><br>Apply here:&nbsp;<a href=\"https://forms.gle/h2tTKhLcP6wqUWDH9\"><u>https://forms.gle/h2tTKhLcP6wqUWDH9</u></a></p><p><br>Or contact&nbsp;<a href=\"mailto:effectivealtruism.journal@gmail.com\"><u>effectivealtruismjournal@gmail.com</u></a> to find out more&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "JournalTeam "}}, {"_id": "HfeGebyz9cgEJ8R9f", "title": "EA Organization Updates: September 2022", "postedAt": "2022-09-14T15:50:34.202Z", "htmlBody": "<p><i>These monthly posts originated as the \"Updates\" section of the&nbsp;</i><a href=\"https://forum.effectivealtruism.org/topics/effective-altruism-newsletter\"><i>EA Newsletter</i></a><i>. Organizations submit their own updates, which we edit for clarity.</i></p><p><strong>Job listings</strong> that these organizations highlighted are at the top of this post. Some of the jobs have extremely pressing deadlines.&nbsp;</p><p>You can see previous updates on the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/ea-organization-updates-monthly-series\">\"EA Organization Updates (monthly series)\"</a> topic page, or in our<a href=\"https://www.effectivealtruism.org/ea-newsletter-archives/\"> repository of past newsletters</a>. Notice that there\u2019s also an&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/org-update\">\u201corg update\u201d tag</a>, where you can find more news and updates that are not part of this consolidated series.</p><p>The organizations are in alphabetical order, starting with L this month.</p><h1>Job listings</h1><p><i>Consider also exploring jobs listed on \u201c</i><a href=\"https://forum.effectivealtruism.org/topics/job-listing-open\"><i><u>Job listing (open)</u></i></a><i>.\u201d</i></p><h2>Applications closing soon</h2><p><strong>80,000 Hours:&nbsp;</strong></p><ul><li><a href=\"https://80000hours.org/2022/08/open-position-full-stack-web-developer-job-board/\"><u>Full Stack Web Developer</u></a>, (London / Remote, apply by&nbsp;<strong>26 September</strong>)</li></ul><p><strong>Centre for Effective Altruism:</strong></p><ul><li><a href=\"https://www.centreforeffectivealtruism.org/careers/project-manager-community-health\"><u>Project Manager: Community Health</u></a> (Remote, apply by&nbsp;<strong>30 September</strong>)</li><li><a href=\"https://www.centreforeffectivealtruism.org/careers/retreats-associate\"><u>Retreats Associate</u></a> (Remote, apply by&nbsp;<strong>10 October</strong>)</li><li><a href=\"https://www.centreforeffectivealtruism.org/careers/ea-global-events-associate\"><u>EA Global Events Associate</u></a> (Remote, apply by&nbsp;<strong>10 October</strong>)</li><li><a href=\"https://www.centreforeffectivealtruism.org/careers/community-events-associate\"><u>Community Events Associate</u></a> (Remote, apply by&nbsp;<strong>10 October</strong>)</li></ul><p><strong>Effective Ventures Operations</strong></p><ul><li><a href=\"https://ev.org/ops/position/operations-associate/\"><u>Operations Assistant</u></a> (Preferred Oxford, London or Boston / Remote, apply by&nbsp;<strong>19 September</strong>)</li><li><a href=\"https://ev.org/ops/position/executive-assistant/\"><u>Executive Assistant</u></a> (Preferred Oxford / Remote, apply by&nbsp;<strong>19 September</strong>)</li><li><a href=\"https://ev.org/ops/position/project-manager-for-oxford-ea-professionals-hub/\"><u>Project Manager Oxford EA Professional Hub</u></a> (Oxford UK, apply by&nbsp;<strong>19 September</strong>)</li><li><a href=\"https://ev.org/ops/position/senior-bookkeeper-accountant/\"><u>Senior Bookkeeper / Accountant</u></a> (Preferred Oxford, London or Boston / Remote, apply by&nbsp;<strong>19 September</strong>)</li><li><a href=\"https://ev.org/ops/position/finance-associate/\"><u>Finance Associate</u></a> (Preferred Oxford/ Remote, apply by&nbsp;<strong>7 October</strong>)</li><li><a href=\"https://ev.org/ops/position/operations-associate-salesforce-admin/\"><u>Operations Associate - Salesforce Admin</u></a> (Preferred Oxford/ Remote, apply by&nbsp;<strong>12 October</strong>)</li></ul><p><strong>Forethought Foundation:</strong></p><ul><li><a href=\"https://www.forethought.org/communications-director\"><u>Communications Director&nbsp;</u></a>(Oxford / Remote, apply by&nbsp;<strong>2 October</strong>)</li><li><a href=\"https://www.forethought.org/researcher-writer\"><u>Researcher / Writer</u></a> (Oxford / Remote, apply by&nbsp;<strong>2 October</strong>)</li></ul><h2>Other roles</h2><p><strong>Against Malaria Foundation:</strong></p><ul><li><a href=\"https://www.againstmalaria.com/Newsitem.aspx?NewsItem=AMF-is-hiring-Junior-and-Senior-Operations-Managers\"><u>Junior and Senior Operations Manager</u></a></li><li><a href=\"https://www.againstmalaria.com/NewsItem.aspx?newsitem=AMF-is-hiring-Donations-Administrator\"><u>Donations Administrator</u></a></li><li><a href=\"https://www.againstmalaria.com/NewsItem.aspx?newsitem=AMF-is-hiring-Software-Engineers\"><u>Junior and Senior Software Engineer</u></a></li></ul><p><strong>Alignment Research Center:</strong></p><ul><li><a href=\"https://www.alignmentforum.org/posts/svhQMdsefdYFDq5YM/evaluations-project-arc-is-hiring-a-researcher-and-a-webdev-1\"><u>Generalist Technical Researcher and a webdev Engineer&nbsp;</u></a></li></ul><p><strong>GiveWell</strong></p><ul><li><a href=\"https://www.givewell.org/about/jobs/operations-assistant\"><u>Operations Assistant</u></a> (Oakland, CA)</li><li><a href=\"https://www.givewell.org/about/jobs/senior-researcher\"><u>Senior Researchers</u></a> and&nbsp;<a href=\"https://www.givewell.org/about/jobs/senior-research-associate\"><u>Senior Research Associates</u></a> (Remote / Oakland, CA)</li><li><a href=\"https://www.givewell.org/about/jobs/content-editor\"><u>Content Editors</u></a> (Remote / Oakland, CA)</li></ul><p><strong>Quantified Uncertainty Research Institute (QURI)</strong></p><ul><li><a href=\"https://quantifieduncertainty.org/careers/technical-product-manager\"><u>Technical Product Manager</u></a> (Berkeley / Remote)</li><li><a href=\"https://quantifieduncertainty.org/careers/full-stack-software-engineer\"><u>Full Stack Software Engineer</u></a> (Berkeley / Remote)</li><li><a href=\"https://quantifieduncertainty.org/careers/programming-language-engineer\"><u>Programming Language or Scientific Software Engineer</u></a> (Berkeley / Remote)</li></ul><p><strong>Berkeley Existential Risk Initiative:</strong></p><ul><li><a href=\"https://existence.org/jobs/deputy-director\"><u>Deputy Director</u></a> (NYC / Remote)</li></ul><p><strong>Rethink Priorities:</strong></p><ul><li><a href=\"https://careers.rethinkpriorities.org/en/jobs/53700\"><u>Multiple research team roles</u></a> (Remote)</li></ul><h1>Organizational updates</h1><h2>Legal Priorities Project</h2><p>The Legal Priorities Project ran its&nbsp;<a href=\"https://www.legalpriorities.org/institute22\"><u>Legal Priorities Summer Institute</u></a> from Sep 3\u20138 in Oxford. 35 talented law students and graduates from 18 different countries and all 5 continents gathered to discuss issues related to future generations and existential risk.&nbsp;<a href=\"https://docs.google.com/presentation/d/11fST-wopfapv_a8D6XFXi7t5VaPCYdY46OPBCxHHfEo/edit#slide=id.p1\"><u>Participants</u></a> with various degrees of familiarity with longtermism participated in workshops, talks, and group discussions with invited experts on issues relating to legal longtermism.</p><p>LPP also&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vkQcq9fmQptXwFNuQ/announcing-the-winners-of-lpp-s-writing-competition\"><u>announced</u></a> the winning submissions from their&nbsp;<a href=\"https://www.legalpriorities.org/competition\"><u>Cost\u2013Benefit Analysis Writing Competition</u></a>. The winning submissions can be found&nbsp;<a href=\"https://www.legalpriorities.org/competition\"><u>here</u></a>.</p><p>LPP funded two student reading groups at&nbsp;<a href=\"http://itam.mx/\"><u>ITAM</u></a>, with curricula centering on&nbsp;<i>The Precipice&nbsp;</i>and the&nbsp;<i>Introduction to EA Program</i>, respectively.</p><h2>Quantified Uncertainty Research Institute</h2><p>QURI released&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/TfPdb2aMKzgWXFvc3/announcing-squiggle-early-access\"><u>Squiggle</u></a>, a free new web\u2013based programming language for probabilistic estimation. Squiggle is made especially for effective altruist cost\u2013effectiveness estimation. Squiggle is currently in public \u201cEarly Access\u201d, but should be usable for many effective altruist use cases.&nbsp;</p><h2>Rethink Priorities (RP)</h2><p>Research:</p><ul><li><a href=\"https://epochai.org/blog/announcing-epoch?fbclid=IwAR1bg-at15jTHih0IxFWqkUgxF7Az6nJRM58KxeV_AippAoCs3DM3j1iUtw\"><u>Epoch</u></a> \u2013 an AI research initiative that RP is incubating \u2013 did a deep\u2013dive investigation of&nbsp;<a href=\"https://deepai.org/publication/machine-learning-model-sizes-and-the-parameter-gap\"><u>machine learning model size trends</u></a> and hypothesized reasons for these patterns.&nbsp;</li><li>Academic collaborator&nbsp;<a href=\"https://www.linkedin.com/in/meghan-barrett-a83164b9/\"><u>Meghan Barrett</u></a> posted a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fZF9ffZD2kkpDy7jB/research-summary-brain-cell-counts-in-black-soldier-flies\"><u>summary</u></a> on the EA Forum of her and her colleagues\u2019&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1467803922000354?CMX_ID=&amp;SIS_ID=&amp;dgcid=STMJ_AUTH_SERV_PUBLISHED&amp;utm_acid=245561464&amp;utm_campaign=STMJ_AUTH_SERV_PUBLISHED&amp;utm_in=DM274768&amp;utm_medium=email&amp;utm_source=AC_\"><u>research</u></a> on black soldier flies. The team also published an&nbsp;<a href=\"https://www.wageningenacademic.com/doi/abs/10.3920/JIFF2022.0041\"><u>article</u></a> on welfare considerations for farming this species in a peer\u2013reviewed, industry\u2013influencing journal.&nbsp;</li></ul><p>News:</p><ul><li>Senior Researcher&nbsp;<a href=\"https://www.linkedin.com/in/neildullaghan/\"><u>Neil Dullaghan</u></a> spoke with Luke Freeman of&nbsp;<a href=\"https://www.youtube.com/watch?v=TS28DFakkik&amp;t=10s&amp;ab_channel=GivingWhatWeCan\"><u>Giving What We Can</u></a> about how animal research helps advocates to target their efforts.</li><li>Senior Research Manager&nbsp;<a href=\"https://www.linkedin.com/in/michael-aird-3a3709133/\"><u>Michael Aird</u></a> shared his personal reflections on impact\u2013driven research, career development, and other topics with Luca Righetti and Fin Moorhouse on&nbsp;<a href=\"https://hearthisidea.com/episodes/aird\"><u>Hear This Idea</u></a>.&nbsp;</li><li>Researcher&nbsp;<a href=\"https://www.linkedin.com/in/holly-elmore-aa6301122/\"><u>Holly Elmore</u></a> discussed&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/yn2ozSswZuiKKBCtJ/the-rodent-birth-control-landscape\"><u>the rodent birth control landscape</u></a> with&nbsp;<a href=\"https://www.wbur.org/news/2022/07/25/pandemic-rats-rodents-somerville-boston-kill\"><u>WBUR (Boston's NPR station)</u></a> as the city considered humane alternatives to rodenticides.</li></ul><p>EA movement building:</p><ul><li>One of the special projects that RP is fiscally sponsoring \u2013&nbsp;<a href=\"http://condor.camp/\"><u>Condor Camp</u></a> \u2013 held its first EA training retreat with ten highly talented Brazilian university students. Associate Researcher&nbsp;<a href=\"https://www.linkedin.com/in/renannascimentoaraujo/\"><u>Renan Ara\u00fajo</u></a> is now working on keeping the alumni engaged, gathering lessons learned, and strategizing how to maintain the momentum in building the EA movement in Latin America.</li></ul><h2>Training for Good</h2><p>TFG has opened applications for the&nbsp;<a href=\"https://www.tarbellfellowship.org/\"><u>Tarbell Fellowship</u></a>, a one\u2013year program for early\u2013career journalists intent on improving the world.</p><p>The Tarbell Fellowship is a one\u2013year program for early\u2013career journalists interested in covering important topics such as global poverty and existential risks (March 1st 2023 \u2192 February 29th 2024). Fellows receive a stipend of up to $50,000, mentorship from experienced journalists and access to a two\u2013week journalism summit in Oxford.&nbsp;</p><p>The fellowship is designed to support those early in their journalism career, i.e. with 0\u20135 years relevant experience. You can&nbsp;<a href=\"https://www.tarbellfellowship.org/\"><u>find out more about the fellowship and apply here</u></a> by October 9th.</p><h2>80,000 Hours</h2><p>80,000 Hours has released its longest and most in\u2013depth problem profile to date:&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\"><u>Preventing an AI\u2013related catastrophe</u></a> by Benjamin Hilton (with feedback and input from more than 30 experts). Benjamin Todd also wrote a blog post on AI:&nbsp;<a href=\"https://80000hours.org/2022/08/is-transformative-ai-coming-sooner-than-we-thought/\"><u>Do recent breakthroughs mean transformative AI is coming sooner than we thought?</u></a></p><p>This month on&nbsp;<i>The 80,000 Hours Podcast</i>, Rob Wiblin spoke with&nbsp;<a href=\"https://80000hours.org/podcast/episodes/will-macaskill-what-we-owe-the-future/\"><u>Will MacAskill on what we owe the future</u></a> and&nbsp;<a href=\"https://80000hours.org/podcast/episodes/andreas-mogensen-deontology-and-effective-altruism/\"><u>Andreas Mogensen on whether effective altruism is just for consequentialists</u></a>. On&nbsp;<i>80k After Hours</i>, Rob spoke with&nbsp;<a href=\"https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/\"><u>Andr\u00e9s Jim\u00e9nez Zorrilla on the Shrimp Welfare Project</u></a>.</p><h2>ALLFED \u2013 Alliance to Feed the Earth in Disasters</h2><p>News \u2013&nbsp;<a href=\"https://www.science.org/content/article/nuclear-war-would-cause-yearslong-global-famine\"><u>Read in Science</u></a> the recent coverage on the most up\u2013to\u2013date nuclear winter food impacts model, with commentary by David Denkenberger and Morgan Rivers. Read also how humanity could respond and survive in&nbsp;<a href=\"https://www.thegrocer.co.uk/supply-chain/how-to-eat-in-a-nuclear-war-what-our-food-system-would-look-like-post-apocalypse/670204.article#.YvY_pOBMEzQ\"><u>How to eat in a nuclear war: what our food system would look like post\u2013apocalypse</u></a>.</p><p>Research \u2013 Juan Garc\u00eda Mart\u00ednez led a new scientific article on producing high\u2013quality single cell protein from methane to secure a global protein supply against catastrophes, highlighting the power of fermentation to feed the world sustainably and effectively even in the most extreme scenarios.</p><ul><li>Protein requirements for the entire global population could be fulfilled in 2.5\u20134.5 years.</li><li>Marginal gas reserves could cover most of these requirements.</li><li>The expected retail cost is as low as US$3\u20135/kg (dry).&nbsp;</li></ul><p>Read here for free: Methane Single Cell Protein:&nbsp;<a href=\"https://www.frontiersin.org/articles/10.3389/fbioe.2022.906704/full\"><u>Potential to Secure a Global Protein Supply Against Catastrophic Food Shocks</u></a></p><h2>Anima International</h2><p>Anima International organized the first ever hybrid (in\u2013person and online)&nbsp;<a href=\"https://www.careconf.eu/2022\"><u>Conference on Animal Rights in Europe</u></a>, welcoming participants and speakers from all over the world, including keynote speaker Peter Singer, to discuss effective ways to make change for animals.</p><p>In Denmark, the AI team is happy to share that the gas station brand OK Plus, which has 69 convenience stores in the country, recently committed to the&nbsp;<a href=\"https://okplus.dk/om-okplus/\"><u>European Chicken Commitment</u></a>. It will meet the requirements by 2024 with an ambition to be already fully implemented in 2023. This progress was achieved after almost a year of positive negotiations and means that the majority of convenience stores in Denmark are now committed to the ECC, including 7\u2013Eleven, Shell and Circle K. Anima International\u2019s team in Denmark are now planning to focus on the remaining companies.</p><h2>Animal Advocacy Careers</h2><p>Animal Advocacy Careers launched a new website&nbsp;<a href=\"https://www.animaladvocacycareers.org/impact\"><u>page that summarizes their impact to date</u></a>. It includes a breakdown of the impact they achieved via the following programs:</p><ul><li><a href=\"https://www.animaladvocacycareers.org/course\"><u>Introduction to Animal Advocacy Online Course&nbsp;</u></a>\u2013 50+ new relevant positions gained by participants (such as jobs, internships, and volunteer placements)</li><li><a href=\"https://www.animaladvocacycareers.org/job-board\"><u>Job board</u></a> \u2013 at least 7 people have found their roles there so far, and the board gets 1300+ unique monthly visitors</li><li><a href=\"https://www.animaladvocacycareers.org/fundraising-work-placement\"><u>Fundraising Work Placement</u></a> \u2013 9 placements at 3 months FTE each in 2022</li><li><a href=\"https://www.animaladvocacycareers.org/careers-advice\"><u>Career advice service</u></a> \u2013 20+ new relevant positions gained</li><li>Recruitment service \u2013 4 hires</li></ul><p>For those who don't know: Animal Advocacy Careers (AAC) is an organization that seeks to address the career and talent bottlenecks in the animal advocacy movement, especially the farmed animal welfare movement.</p><p>AAC has room for funding and is funding\u2013constrained. If you would like to support them, please&nbsp;<a href=\"https://www.animaladvocacycareers.org/donate\"><u>donate here</u></a> or reach out to them at&nbsp;<a href=\"mailto:lauren@animaladvocacycareers.org\"><u>lauren@animaladvocacycareers.org</u></a>.</p><h2>Animal Charity Evaluators</h2><p><a href=\"https://animalcharityevaluators.org/blog/reassessment-of-recommended-charity-categories-and-re-evaluation-frequency/?fbclid=IwAR1Oi0qUtbIXPMof0fSOVS33No25qflgruYQhsIEO3dxLEUDbcJHzwPb0Z0\"><u>Reassessment of Recommended Charity Categories and Re\u2013Evaluations Frequency</u></a>. ACE strives to improve its Charity Evaluations program every year. One of its 2022 goals for this program was to reassess its Top and Standout Charity categories and the frequency of re\u2013evaluating recommended charities. After brainstorming various ideas and scenarios, ACE made some modifications, and the main benefits and drawbacks of these changes are discussed in their latest blog post.</p><p><a href=\"https://animalcharityevaluators.org/blog/menu-of-outcomes-series-five-ways-charities-are-increasing-the-availability-of-animal-free-products/\"><u>Menu of Outcomes Series: Five Ways Charities Are Increasing the Availability of Animal\u2013Free Products</u></a>. ACE\u2019s Menu of Outcomes blog post highlights some of the ways animal advocacy groups are transforming the food system and other industries by increasing the availability of animal\u2013free products. ACE\u2019s goal is not to examine or compare the effectiveness of different interventions, but rather to showcase the diversity of approaches animal advocates can use to achieve similar goals. ACE hopes that the programs and interventions mentioned inspire organizations and individuals to broaden their awareness of ways to approach their work.</p><p><a href=\"https://animalcharityevaluators.org/blog/recommended-charity-fund-six-month-update-july-2022/\"><u>Recommended Charity Fund: Six\u2013Month Update</u></a>. In January, thanks to generous donor support of ACE\u2019s Recommended Charity Fund, it was able to award $2,019,702 to its Top Charities and Standout Charities \u2013&nbsp;ACE\u2019s largest distribution to date! Each organization has provided ACE with an update on how they\u2019ve used their grant to help animals.</p><p><a href=\"https://animalcharityevaluators.org/blog/roundtable-how-can-charities-effectively-communicate-with-their-audience/\"><u>Roundtable: How Can Charities Effectively Communicate With Their Audience?</u></a> How you communicate with your audience makes a significant difference for nonprofit organizations and is key to building solid connections. Defining your language, tone of voice, and brand personality allow your followers to get to know you, relate to you, and trust you over time. You must also ensure that the content you\u2019re putting out engages and resonates with your audience. To help guide you in this area, ACE asked four communications professionals in the animal advocacy movement to share their advice.</p><h2>Centre for Effective Altruism</h2><h3>Community Health</h3><p>The Community Health team published a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/NbkxLDECvdGuB95gW/the-community-health-team-s-work-on-interpersonal-harm-in\"><u>forum post</u></a> about their work on interpersonal harm in the community.&nbsp;</p><h3>Events</h3><p>The CEA events team are supporting 3 EA conferences this month: EAGxSingapore, EAGxBerlin and EA Global: Washington D.C.</p><p>Applications are open for the following upcoming conferences, all via&nbsp;<a href=\"https://creatorapp.zohopublic.com/centreforeffectivealtruism/ea/form-perma/EAG_Application/AV2sCTpgHH0NNCXXV5C8Jy5tsftVsj44V3Xh6Nu4EXf8r59HWW4ByExQMKVGsgdUQ0jRtrVQSZJbs3fTP8pr1XKjFkeEZJ9rNkRH?eag_use_application=standard\"><u>the same link</u></a>.</p><ul><li>EAGxRotterdam (4\u20136 November)</li><li>EAGxVirtual (21\u201323 October)</li></ul><h3>Online</h3><p>The Online Team published&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022\"><u>a Forum post sharing a number of new features</u></a>.&nbsp;</p><h2>Charity Entrepreneurship</h2><p>Over the coming months, Charity Entrepreneurship will be hosting an Online Speakers Series. You can now sign up for three upcoming talks:&nbsp;<a href=\"https://forms.gle/EU5WNBfhBgcz7Xtk7\"><i><u>How to get your project funded&nbsp;</u></i></a><i>&nbsp;</i>with Joey Savoie (September 13, 6pm UK, 10am SF, 1pm. NYC),&nbsp;<a href=\"https://docs.google.com/forms/d/1EAzu60gpUJup4RTuyu5ddHblsAjx29eb658N3QKbf1U/edit\"><i><u>Our Top Charity Ideas for Early 2023</u></i></a><i>&nbsp;</i>with Vicky Cox<i>&nbsp;</i>(September 27, 6pm UK), and&nbsp;<a href=\"https://docs.google.com/forms/d/1LoThrrNZ82yGX2ZdJSEXlPoUSrTdk0iVtKQj7RdgVOE/edit\"><i><u>What\u2019s the most effective way to help as many animals as possible?</u></i></a><i>&nbsp;</i>with Karolina Sarek and Lauren Mee from Animal Advocacy Careers (October 6, 6pm UK). CE has also just finished their 2022 Incubation Program and will soon introduce the new charities started. In the meantime, you can meet the participants of the program via&nbsp;<a href=\"https://youtu.be/74JAtqpn6zw\"><u>the CE video</u></a>. If you\u2019re interested in working at or building a nonprofit startup, you might want to consider applying to&nbsp;<a href=\"https://www.charityentrepreneurship.com/career-coaching\"><u>CE\u2019s one\u2013on\u2013one career coaching&nbsp;</u></a>sessions or taking&nbsp;<a href=\"https://charityentrepreneurship.us8.list-manage.com/track/click?u=f8f83702666cd9a73e7d76569&amp;id=3440a0ca04&amp;e=4d8468960f\"><u>CE\u2019s quiz</u></a> to check if nonprofit entrepreneurship is a good fit for you.&nbsp;</p><h2>Faunalytics</h2><p>Faunalytics'&nbsp;<a href=\"https://faunalytics.org/going-veg-barriers-and-strategies/\"><u>newest report</u></a> is the third of a three-part series on&nbsp;how to help new vegans and vegetarians maintain their change of lifestyle. This longitudinal study specifically looked at the roadblocks to diet change, as well as the best strategies to help make a person\u2019s diet change successful.</p><p>The organization's latest&nbsp;<a href=\"https://youtu.be/BjOf-bESMSs\"><u>Faunalytics Explains</u></a> video covers the evolution of the dairy industry. They\u2019ve also launched a&nbsp;<a href=\"https://faunalytics.org/glossary/\"><u>Research Glossary</u></a> which provides easy\u2013to\u2013understand definitions for a range of terms that will help advocates understand social science research. Additionally, Faunalytics has updated their research library with several articles on topics including&nbsp;<a href=\"https://faunalytics.org/reasons-to-be-optimistic-about-animal-welfare-in-china/\"><u>reasons to be optimistic about animal welfare in China</u></a> and&nbsp;<a href=\"https://faunalytics.org/a-targeted-approach-to-turn-your-friends-vegan/\"><u>using behavior change research to turn others vegan</u></a>.&nbsp;&nbsp;</p><h2>Fish Welfare Initiative</h2><p>Fish Welfare Initiative recently held&nbsp;<a href=\"https://www.fishwelfareinitiative.org/post/roundtable\"><u>India\u2019s first ever corporate roundtable event on fish welfare</u></a>. Two of the corporate attendees,&nbsp;<a href=\"https://www.fipola.in/\"><u>Fipola</u></a> and&nbsp;<a href=\"https://www.captainfresh.in/\"><u>Captain Fresh</u></a>, have since signed MOUs with FWI to conduct trial runs of higher welfare fish in their supply chains. FWI expects to share more information on these developments later.</p><p>FWI also recently published their&nbsp;<a href=\"https://www.fishwelfareinitiative.org/post/china-plans\"><u>China 1 and 3 year plans</u></a>.</p><h2>Founders Pledge</h2><p>Founders Pledge recently published a report on&nbsp;<a href=\"https://founderspledge.com/stories/giving-multipliers\"><u>Giving Multipliers</u></a>, highly leveraged ways to drive donations to high\u2013impact charity. A set of explicit giving opportunities produced by this work is forthcoming.<br><br>FP also posted a&nbsp;<a href=\"https://founderspledge.com/stories/explaining-a-small-ppf-grant-on-nuclear-security\"><u>discussion</u></a> of the first grant from its&nbsp;<a href=\"https://founderspledge.com/funds/patient-philanthropy-fund\"><u>Patient Philanthropy Fund</u></a>, which generated a research report on the state of philanthropy in the nuclear security sector, and is concluding several large methodological projects. These include work on interconvertibility between DALYs, WELLBYs, and income, as well as an overhaul of FP\u2019s strategy for rating and recommending longtermist opportunities. In the next few months, FP will publish research covering these topics, as well as air pollution, maternal health, and hotlines between nuclear powers.</p><p>FP is still hiring for an&nbsp;<a href=\"https://founders-pledge.jobs.personio.de/job/687316?display=en\"><u>Applied Researcher</u></a> and a&nbsp;<a href=\"https://founders-pledge.jobs.personio.de/job/786537?display=en\"><u>Senior Researcher</u></a> to work on its climate team.</p><h2>GiveWell</h2><p>GiveWell has updated and added to its&nbsp;<a href=\"https://www.givewell.org/how-we-work/criteria#Additional_Criteria_for_Top_Charities\"><u>criteria</u></a> for top charities so that they accurately reflect both GiveWell's priorities and what most donors expect from these recommendations: high impact, plus a high degree of confidence. GiveWell's current top charities are the<a href=\"https://tracking.cirrusinsight.com/3dc5dc2a-fb19-4617-ae9c-4156e822f9d9/givewell-org-charities-amf\">&nbsp;<u>Against Malaria Foundation</u></a>,<a href=\"https://tracking.cirrusinsight.com/3dc5dc2a-fb19-4617-ae9c-4156e822f9d9/givewell-org-charities-helen-keller-international\">&nbsp;<u>Helen Keller International's vitamin A supplementation program</u></a>,<a href=\"https://tracking.cirrusinsight.com/3dc5dc2a-fb19-4617-ae9c-4156e822f9d9/givewell-org-charities-malaria-consortium\">&nbsp;<u>Malaria Consortium's seasonal malaria chemoprevention program</u></a>, and<a href=\"https://tracking.cirrusinsight.com/3dc5dc2a-fb19-4617-ae9c-4156e822f9d9/givewell-org-charities-new-incentives\">&nbsp;<u>New Incentives</u></a>. You can read more about the updated criteria and why these changes were made&nbsp;<a href=\"https://blog.givewell.org/2022/08/17/changes-to-top-charity-criteria\"><u>here</u></a>. These updates do not affect GiveWell's views of deworming or GiveDirectly, which have not changed, nor are they expected to change GiveWell's allocation of funding to deworming programs or GiveDirectly.</p><p>GiveWell has changed the name of the Maximum Impact Fund, the fund that is allocated to high\u2013impact funding gaps within its top charities, to the&nbsp;<a href=\"https://blog.givewell.org/2022/09/08/renaming-top-charities-fund\"><u>Top Charities Fund</u> in order to more clearly reflect its function</a>. They\u2019ve also set up the&nbsp;<a href=\"https://www.givewell.org/research/all-grants\"><u>All Grants Fund</u></a> as its complement, a giving opportunity for donors who want to support grants that are expected to be very high\u2013impact but whose implementing programs don't currently qualify to be top charities. You can donate to the All Grants Fund&nbsp;<a href=\"https://www.givewell.org/research/all-grants\"><u>here</u></a>.</p><p>GiveWell has launched the&nbsp;<a href=\"https://blog.givewell.org/2022/09/06/announcing-change-our-mind-contest/\"><u>Change Our Mind Contest</u></a> to encourage critiques of their existing cost\u2013effectiveness analyses that could improve their allocations. Cost\u2013effectiveness is the single most important input into GiveWell's grant decisions, and they want to incentivize feedback that will improve their work and enable the organization to do more good. GiveWell hopes you\u2019ll consider participating! See&nbsp;<a href=\"https://www.givewell.org/research/change-our-mind-contest\"><u>here</u></a> for complete contest guidelines.</p><h2>Giving Green</h2><p>Giving Green\u2019s Dan Stein and Lucia Simonelli were interviewed for a piece in the MIT Technology Review about&nbsp;<a href=\"https://www.technologyreview.com/2022/08/24/1058459/we-must-fundamentally-rethink-net-zero-climate-plans-here-are-six-ways-how/\"><u>the pitfalls of corporate net\u2013zero plans</u></a>. Giving Green is looking to get in touch with businesses small and large that are interested in learning more about or providing feedback on this approach \u2013 please get in touch with the team at&nbsp;<a href=\"mailto:givinggreen@idinsight.org\"><u>givinggreen@idinsight.org</u></a>.</p><p>Giving Green\u2019s Dan Stein was interviewed on the&nbsp;<a href=\"https://www.youtube.com/watch?v=0Bv_GbKHEcA&amp;t=1087s\"><u>Just Another Mindset podcast</u></a> and the&nbsp;<a href=\"https://www.youtube.com/watch?v=cnZrL1GzwPk\"><u>Early Advantage podcast</u></a>, discussing Giving Green\u2019s philosophy on policy change as a cost\u2013effective lever to fight climate change.</p><h2>Giving What We Can</h2><p>Giving What We Can launched their new&nbsp;<a href=\"https://www.givingwhatwecan.org/charities/longtermism-fund\"><u>Longtermism Fund</u></a> in partnership with Longview Philanthropy. The Longtermism Fund directs funding to highly effective organizations working to safeguard the long\u2013term future of humanity.&nbsp;</p><p>You can read more about&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/f7qAfcKArzYrBG7RB/announcing-the-longtermism-fund\"><u>the reasoning behind starting this fund</u></a> and&nbsp;<a href=\"https://www.givingwhatwecan.org/charities/longtermism-fund\"><u>donate to it now</u></a>.</p><p>Giving What We Can are seeking people to join their&nbsp;<a href=\"https://www.givingwhatwecan.org/get-involved/ambassador-program\"><u>Ambassador Program</u></a>; they\u2019d love you to apply if you\u2019d like to grow your advocacy for effective giving. A&nbsp;<a href=\"https://www.guidedtrack.com/programs/x9jn6u2/run\"><u>new tool to make volunteering with Giving What We Can easier</u></a> has also been launched.&nbsp;</p><p>A new Director of Research, Sjir Hoeijmakers, joined Giving What We Can this month. Sjir previously worked at Founders Pledge as a Senior Researcher.&nbsp;</p><p>Content highlights:&nbsp;</p><p>Blog:</p><ul><li><a href=\"https://www.givingwhatwecan.org/blog/what-we-owe-the-future-a-review-and-summary-of-what-i-learned\"><i><u>What We Owe the Future</u></i><u>: A review and summary of what I learned</u></a>, Michael Townsend, Researcher at Giving What We Can</li></ul><p>YouTube:</p><ul><li><a href=\"https://youtu.be/Ex7hgpXfw0U\"><u>The journey to founding one of the world\u2019s most effective charities</u></a>: The story of Rob Mather and the founding of the Against Malaria Foundation \u2013&nbsp;<a href=\"https://www.youtube.com/watch?v=CiFoHm7HD94&amp;list=PLT88QiptgOaJBj0E-XrJFsjnQcYRhpkeY\"><u>Giving Effectively</u></a> series</li><li><a href=\"https://www.youtube.com/watch?v=pK5b2fGKrU8\"><u>Making it easier for great charities to exist</u></a>: Interview with Joey Savoie, CEO and co\u2013founder of Charity Entrepreneurship \u2013&nbsp;<a href=\"https://www.youtube.com/watch?v=pK5b2fGKrU8&amp;list=PLT88QiptgOaK2tU5Cxaw9O8iXU30qSOih\"><u>Podcast</u></a> series</li><li><a href=\"https://www.youtube.com/watch?v=0g_1JfFRdY4\"><u>Funding projects to save future generations at Longview Philanthropy</u></a>: Interview with Simran Dhaliwal, \u200b\u200bco\u2013CEO of Longview Philanthropy</li></ul><p>Find audio\u2013only versions of our new YouTube content on the&nbsp;<a href=\"https://givingwhatwecan.podbean.com/\"><u>Giving What We Can podcast</u></a>.</p><h2>The Humane League</h2><p>In August, following meetings with THL\u2019s corporate engagement team, TORIDOLL Group \u2013 a Tokyo\u2013based company that owns and operates several restaurant chains, including Pokeworks \u2013 committed to going 100% cage\u2013free in all its stores across the globe by the end of 2030. It already uses 100% cage\u2013free eggs in its stores in the US and UK. In Japan, it \u201cwill transition a minimum of ten (10) Marugame restaurants to use 100% cage\u2013free eggs by the end of FY2022 and 5% of all restaurants by the end of FY2023, with further annual increases thereafter.\u201d This is THL\u2019s first commitment from a Japanese corporation.&nbsp;</p><p>Following pressure from THL, three national pharmacy chains CVS, Rite Aid, and Walgreen\u2019s have agreed to go 100% cage\u2013free by the end of 2022, a full three years ahead of schedule. CVS made its decision first, after finding that, given the current supply chain issues, it was actually easier and only marginally more expensive to source cage\u2013free eggs. THL leveraged CVS\u2019 announcement to pressure its competitors Rite Aid and Walgreen\u2019s to make the same commitment.&nbsp;</p><p>After a short campaign led by THL, BJ\u2019s Wholesale has committed to transitioning to 100% cage\u2013free eggs in its supply chain by 2025. BJ\u2019s reported that 51% of the eggs sold in the first six months of FY22 were cage\u2013free, and it aims to increase that number to 70% by 2023 and to 80% by 2024. It will report on its progress annually.&nbsp;</p><p>THL published its 2022 Q2 progress report, which details its full impact from April to June&nbsp;<a href=\"https://assets.ctfassets.net/ww1ie0z745y7/1osN9N1O2xVfTfpajulekY/6ef347df406d320ba633668e1bc5b9fc/THL_Q2_2022_Progress_Report_Final.pdf\"><u>here</u></a>.&nbsp;</p>", "user": {"username": "Lizka"}}, {"_id": "zoJukMLbsfoEatbxN", "title": "Thread on LT/ut's preference for billions of imminent deaths", "postedAt": "2022-09-14T15:44:15.413Z", "htmlBody": "<p>I know this former longtermist thinker is persona non grata around here (as am I) but for my sake imagine you're trying to convince an outsider who read this thread that it's untrue that this movement willfully endangers billions of poor brown people. Without using utilitarian jargon or quantitative reasoning.&nbsp;</p>", "user": {"username": "Peter_Layman"}}, {"_id": "yq5NyHqEdewn6JBpn", "title": "Pathways to victory: How can we end animal agriculture?", "postedAt": "2022-09-14T15:33:04.910Z", "htmlBody": "<h2>EXECUTIVE SUMMARY</h2><p>Factory farming and other forms of animal exploitation represent an unimaginably large amount of suffering, as well as other grave ethical concerns. Despite concern for animals being arguably at an all-time high and the many apparent successes of corporate campaigns, meat consumption has been rising (Tim and Nathan 2019). In the face of this seemingly bleak situation, many animal advocates placed hope in cultured meat, but recent analyses from outside the industry have been pessimistic about the prospects for this technology, leaving some animal advocates without hope for victory. This report outlines how victory might be achieved with the patience and dedication of the movement.</p><p>To develop a sophisticated theory of change, we must understand what victory might look like and what the most plausible avenues to it are. A well thought out theory of change allows us to make this kind of alignment between our actions and our ideals. Current actions that do little to contribute to our most plausible victory scenarios could then be reconsidered and actions that contributed substantially towards these scenarios could be reinforced.</p><p>For example, we find that welfare reforms have made strong, tangible progress, and so remain a strong option for now, but may have difficulty in later stages. In contrast, sudden progress approaches like ballot initiatives and fundamental rights, that potentially achieve rapid progress for animals, may struggle to achieve their goals for now, but may become more promising once more progress has been made and ambitious asks become more realistic.</p><p>Turning to animal product replacements, we find the reasons for thinking that cultured meat will inevitably lead to victory for the movement (by replacing all or most animal products), are far less compelling than they formerly were. Plant-based meat represents a pathway that will likely play an important role in assisting other pathways, but is less likely to lead to strong progress on its own<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u00b9</u></a>. In contrast, other alternative protein technologies represent a kind of wild card, which may result in a great deal of progress or may not result in any progress.</p><p>We also review the respective arguments for abolitionism and animal protectionism since this was a crucial consideration in this report. We conclude that strong forms of abolitionism, such as those defended by Francione, are not well supported, but that there are lessons around framing from abolitionism that animal advocacy organisations should take on board.</p><p>This is a broad, thorny and speculative subject, so our conclusions should be treated as tentative and preliminary. We have tried to appropriately qualify our conclusions to reflect this background level of uncertainty; however, it would clutter the writing to insert all of the necessary qualifiers and so we hope the reader will understand that this is a speculative subject and that our statement should be interpreted in that light.</p><h2><br>WHAT WOULD VICTORY LOOK LIKE?</h2><p>As a shorthand, this report will use the term victory to refer to a range of possible outcomes in which the animal advocacy community achieves its goals. There are different conceptions of what this might look like. At a minimum this would involve the end of state-sanctioned factory farming in many countries, and many might also call for the end of all animal farming, as well as further steps to protect the interests of animals.</p><p>Different moral theories have more substantive answers to this question. For example, consequentialist theories would want to see a future where animal interests are properly taken into account in human decisions such that the well-being of animals is maximised (Sinnott-Armstrong and Zalta 2015). It is sometimes thought that utilitarianism would not necessitate fully ending animal agriculture or other forms of animal exploitation. However, there are utilitarians who dispute this. They argue that we should end all animal farming based on the implausibility of producing animal products in a way that does not involve suffering and because the consumption of meat may lead to judgements of animals as having less moral worth, potentially leading to us disregarding their interests in other ways (Reese 2018; Bastian et al. 2012; Vinding 2014; Ozden 2022).</p><p>There are also consequentialist views that place special importance on improving the lives of the worst off. Versions of these theories include negative utilitarianism, the view that suffering has stronger moral weight than happiness; egalitarianism, the view that equal distributions of well-being are especially valuable; and prioritarianism, the view it is especially important to improve the wellbeing of the worst off. These views will be especially inclined to reject all animal exploitation because animals are among the worst off of beings alive today (Vinding 2014; Animal Ethics n.d., n.d.).</p><p>Rights-based approaches in contrast may view certain actions as wrong if they violate the rights of individuals, even if those actions increase overall well-being. They may also call for a broader range of rights than simply those that directly concern well-being (Alexander and Moore 2007)<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u00b2</u></a>. Rights-based approaches are sometimes seen as calling for more radical changes, though this is not necessarily the case. For example, Alasdair Cochrane (2012) argues that most animals do not have a right to certain forms of liberation, because they are not beings like us to whom autonomy is valuable so do not have an interest in it. Note that he still believes that they have an interest in some forms of liberation, such as liberation from confining cages that stop them from performing natural activities.</p><p>Consequentialist theories can also have more radical implications than rights-based theories because consequentialist theories are famously more demanding than rights-based theories. Some argue that consequentialist theories should have no limits on how much they can morally demand of individuals (Kagan 1989). Notably, most consequentialist theories imply that we should help animals in the wild whenever we can do so without more harm than good (Horta 2017; Animal Ethics n.d.). Moreover, some argue that there are many cases when intervention could be beneficial and that in the future we will be in an even better position to help because of greater technology and knowledge (Horta 2017).</p><p>Francione offers a harder line rights-based approach (Francione and Garner 2010). He would like to see the end of all treatment of animals as property. This would include ending all use of animals from animal agriculture, animal testing, the use of animal products for clothing, and even the use of animals as companions. Francione focuses chiefly on the use of animals as problematic, rather than their suffering. This goal is shared by the Nonhuman Rights Project (NhRP n.d.). We will discuss the methods of NhRP and Francione in greater detail later in the report.</p><p>In this report we will use the term \u201cvictory\u201d or \u201cvictory for the animal advocacy movement\u201d loosely to refer to this range of scenarios. For further discussion of the goals that animal advocacy organisations might have beyond the farmed animal context, see the section \u201cAfter animal farming\u201d.</p><p>&nbsp;</p><h2>VICTORY SCENARIOS</h2><p>Here we will review evidence behind the different potential pathways to victory. It appears most likely to us that if victory is achieved it will happen for a variety of these pathways, with each of them synergistically boosting the others (interview proceedings). This is because no single pathway seems much more likely than another and because there are plausible accounts of how each may support others, such as by increasing public concern, making raising animals less appealing for the industry, and presenting alternatives. Many pathways share these instrumental goals, and so will naturally support each other. This has been called \u201can ecology of social change\u201d, with different organisations playing complementary roles in the ecosystem (Cockburn 2018).</p><p>An example of this is the \u201cradical flank effect\u201d. This refers to the interplay between a radical flank of the movement and more moderate wings. They can function as foils to each other, enhancing respectively how radical or moderate their messages are. In particular, the radical flank can help shift public perceptions about what is acceptable discourse, which can help the message of the moderate wing ultimately become accepted. However, there is also the potential for negative reactions to moderate flank effects. The moderate wing may come to be equated with the radical wing, or at the very least responsible for it, discrediting the moderate message and producing a backlash against it (Haines 2013).</p><p>Having different groups with different levels of radicalness and differing views also allows the movement to capture the enthusiasm of many different people. Some might be turned off by a more radical approach, opting instead for a more moderate one, whilst a moderate approach might be too tame for others who will be drawn to the radical group.</p><p>For these reasons, in most cases, the optimal distribution of movement resources will not be that the approach with the strongest arguments for effectiveness gets all the resources. Instead, the optimal approach will likely be a mix of priorities amongst different organisations. A diversity of approaches may be effective because it allows the movement to learn from trial and error and achieve results that particular approaches are better at achieving while avoiding diminishing returns (Ozden 2022). Having said this, some approaches have evidence of larger effects and so warrant a larger share of movement resources. Some approaches may also be counterproductive or harmful and so may warrant no movement resources.</p><p><strong>Table 1: Pathways</strong></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Pathway</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Strengths</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Limitations</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Crucial Considerations</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Overall conclusion</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Reformed into adequacy or out of existence&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Empirical track record</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Reforms are far from liberation</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Moving from reforms to liberation</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Quite promising, at least in the short term</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Fundamental rights</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Aims close to the target, so very impactful if successful</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Less tractable</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Is it too much of a moonshot? How much will wins for sapient animals help other animals?</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">The current level of investment seems justified, and significantly larger investments may be warranted in the future</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Consumer choice</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Public opinion seems to be a bottleneck for many other pathways</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Evidence for moral advocacy is limited and not strong</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">The evidence base for the efficacy of this approach</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Lack of strong evidence for efficacy and more research is needed on this front. Shows promise</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Ballot initiatives&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Allows for a relatively clean translation of public opinion into victory</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Requires public opinion and can only be practised in some regions</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">How should we interpret some of the poll results that appear promising?</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Quite promising organisations working in areas that support ballot initiatives and especially promising in the future once more support has been built</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Cultured meat&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Allows for victory by simply outcompeting conventional meat</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Various potential technological bottlenecks as described by Humbird&nbsp;(Humbird 2020)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Are recent criticisms of its technological feasibility decisive?</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Still some chance this will lead to victory, though recent analyses make this look substantially less likely</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Plant-based meats</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">We already have many plant-based products&nbsp; on the market</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Not perceived as having the essence of meat</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Can the products overcome the psychological attachments to conventional meat?</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Likely an important part of any solution, but perhaps unlikely to constitute a solution on its own</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Knockout animals</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">More proven in concept than other technological solutions</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Unclear if it would remove all suffering and significant risks if it does not</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Will all forms of suffering be removed&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Most likely not a pathway worth pursuing, though more research to check this conclusion would be valuable</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Other alternative protein technologies</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Some potential to outcompete conventional meat</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Not proven</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Technological feasibility&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Unlikely to be worth focusing movement resources on currently, but may help the movement in the future</td></tr></tbody></table></figure><p><br>&nbsp;<br><strong>Reformed into adequacy or out of existence</strong></p><p>A commonly imagined pathway \u2013 and one that is arguably assumed by most animal advocacy organisations \u2013 is that of incremental progress through a series of corporate and legislative welfare reforms. This is exemplified by the large investment in and apparent successes of corporate campaigns, as well as other welfare reform work. They suggest steady progress towards victory.</p><p>The reform pathway could lead to a much smaller number of higher welfare animals as welfare reforms progressively increase both the welfare and the price of meat. This could be regarded as an adequate situation by some, and it is also possible that this effectively shuts down the industry by making farming unprofitable with competition from alternative proteins.</p><p>Corporate campaigns have been astonishingly effective in securing commitments for welfare reforms. For example, over one third of hens in the US (over 100 million) are now cage free, up from 7% in 2015. The successes have been mirrored in Europe as well with the proportion of cage free hens in Italy and France approximately doubling over the same period (\u201cTen Big Wins for Farm Animals in 2021 - EA Forum\u201d 2021). The Humane League (Evans 2021) estimates that 85% of corporate commitments to go cage free by 2020 have been met. This has been achieved through a substantial proportion of effective animal advocacy funds, an estimated 60% of their total spending (Ozden 2021).</p><p>This is arguably much more tangible progress than has been seen elsewhere from animal advocacy organisations. It might then seem reasonable to expect that corporate work of this kind will be primarily responsible for driving the animal advocacy community to victory. If progress continues in a similar way, this could happen by a series of incremental, though rapid, welfare reforms ultimately leading to the end of factory farming.</p><p>There is some indication that corporate campaigns have worked synergistically with legislative reform. For example, the cage free commitments that corporate campaigns have been particularly successful in achieving may have paved the way for the EU\u2019s historic pledge in 2021 to phase out the use of cages (EA Forum 2022).</p><p>Despite the success in achieving welfare reforms, some might be sceptical about the ability of the reform pathway to lead to victory. For example, some people who are influenced by abolitionism might think that corporations will only be willing to go so far and that, perhaps, they have agreed to these reforms because they are reasonably cost-effective for the corporation to make, but that critical reforms will be much more expensive and this is where we will see progress stall.</p><p>An advocate for corporate campaigns might reply that this is based on more speculative theory and that the corporate campaigns approach has much stronger empirical support. If at a later date the progress begins to stall, we can re-evaluate at that point, but it does not make sense to abandon a well-supported approach based on speculative theory. For more discussion of this debate, see section Crucial Consideration 1: Abolitionism Versus Protectionism.</p><p>Another line of criticism against corporate campaigns is the relative amount of investment going into them. The user Kato on the Effective Altruism Forum (EA Forum 2022) acknowledges the successes that corporate campaigns have had but thinks that there is somewhat more relative investment in them than is optimal. Their chief concern with the current level of investment in corporate campaigns is that a broader, more pluralistic approach would enable the animal advocacy movement to learn and grow more effectively. This does not represent any strong criticism of the corporate campaigns approach, it is just an argument that focusing a majority of resources on corporate campaigns may be a mistake.</p><p>Because of the particularly tangible progress that has been made with the reform pathway, it seems particularly promising. A downside may be that each welfare increase is relatively small, considering what we might imagine an average member of the public would find acceptable and it is possible that, at least in the future, more ambitious asks will be more effective.</p><p>Additionally, more so than any other pathway, the reform pathway falls afoul of the abolitionist critique that it may reinforce some of the practices that it seeks to change. We argue later that the strong form of this critique, as put forward by Francione, is misguided, but there are some remaining worries around framing welfare reforms that people working on the reform pathway should take heed of.</p><p>The reform pathway may be a strong option for now because of the tangible welfare progress that it has made and because it can increase costs to industry and/or consumers, making alternatives more attractive. A weakness of the reform pathway is that it may be difficult for it to make the final push towards ending animal farming. It may leave us with a small number of farmed animals with higher welfare standards, but may therefore not satisfy more ambitious theories of victory (Ozden 2022), though at that point a full ban would no longer be a large step and could be pushed for directly.</p><p><br><strong>Fundamental rights</strong></p><p>In contrast to this incremental approach, the most prominently discussed way that change can happen more suddenly, from the top down, is through a successful court case or successful push for legislation for fundamental rights. One example of this that is held up in the animal advocacy movement is an event in the anti-slavery movement in the UK. This is the case of Somerset vs. Stewart, which put considerable pressure on the institution of slavery in the UK<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u00b3</u></a> (Cotter 1994).</p><p>The Nonhuman Rights Project (NhRP) uses cases like this as a model for how we could grant fundamental rights to animals such as chimpanzees or whales for whom the case under the law is strongest<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u2074</u></a>. They take cases for particular animals who have been mistreated such as Happy the elephant and Kiko the chimpanzee and file a writ of habeas corpus on their behalf. Habeas corpus is used in possible cases of unlawful detention and when applied the detainee is brought before the court to determine if the detention is indeed unlawful. The NhRP claim if the court agrees that habeas corpus applies in the case of the animal, that establishes personhood for the animals. They argue for this to be done because the animals have the right to liberty and bodily integrity. They suggest that the animals could be sent to sanctuaries instead.</p><p>Fundamentally, NhRP believe that our legal system either recognises them as persons or as property, and they regard this property paradigm as responsible for most of the harms that they suffer. They imagine that this could be extended to many other animals, through future legal work which could imply the end of animal farming or even broader implications for treatment of animals (see our After Animal Farming section) (Wise 2014; NhRP n.d.). However, speculatively, the personhood and autonomy arguments might be harder to make with animals such as farm animals who are sometimes perceived as less cognitively complex and less similar to us. This may mean that there is significantly more work to be done after sapient animals are recognised as bearers of fundamental rights in many countries.</p><p>There have already been limited small-scale successes in granting fundamental rights and even personhood to the sapient animals. Most of these cases have been through legislation rather than court battles. The most significant case is that the Balearic Islands, an autonomous archipelago in Spain, granted effective legal personhood to great apes. This was followed by an approval by the Spanish Parliament to extend this to the rest of Spain (Glendinning 2008). There have been various other cases that came close to granting great apes and some other animals legal personhood but have fallen short in various ways (Wise 2014)</p><p>This idea that change will come suddenly is supported by the punctuated equilibrium theory of policy change and was first proposed by Frank Baumgartner and Brian Jones (Baumgartner and Jones 2010) in the context of lobbying. According to this theory, there are periods of stagnation and policy change, punctuated by periods when there is much greater change. These periods of much greater change occur when the interest groups supporting the change are able to shift the framing around the policy, as well as the understanding of it and the underlying culture (Baumgartner and Harris 2020). For example, currently the framing around animal agriculture could be said to be that it is natural, traditional, and wholesome, and with this dominant narrative, as well as the reluctance of government to address the status quo, change is unlikely. However, if the framing could be shifted towards the typical animal advocate view of animal agriculture as being industrial, cruel, and unsustainable, change might begin to happen rapidly.</p><p>One part of punctuated equilibrium theory is that issues are often seen as either problems or as solutions to problems (Baumgartner and Harris 2020). Therefore, if animal advocacy organisations can successfully shift the view of animal agriculture from being a <i>solution</i> to the problem of a supply of healthy protein, to a view of it as a <i>problem</i> of animal cruelty, environmental degradation, and global hunger, rapid progress may begin to be made.</p><p>Some examples of cases where societal changes were achieved suddenly (at least in Western countries) are same-sex marriage and the rejection of smoking (Baumgartner and Harris 2020). For example, in the height of tobacco use in the US, 45% of surveyed adults reported smoking a cigarette in the last week, compared to only 16% in 2021 (Courtney Dillard, Tess Morrison, Marcy Regambal, Alan Presburger, n.d.). In contrast, many other advocacy causes (including animal advocacy itself) do not appear to have had this kind of rapid success.</p><p>We remain uncertain about which of the respective views of sudden or incremental progress is correct. Speculatively, we see a sudden victory as unlikely at this time, and we see incremental approaches as valuable in moving us forward for now, but later on more ambition may be called for and may be more effective in achieving more rapid changes.</p><p>&nbsp;</p><p><strong>Consumer choice</strong></p><p>A simple possible pathway to victory is through more and more people becoming vegan. This could happen through outreach efforts convincing more and more people to become vegan, adding further strength to the movement, and eventually a tipping point could be reached when veganism becomes the moral mainstream. This is arguably the pathway that Francione and some other abolitionists assume as at least the appropriate way to make progress in the short term. Though it might be impossible to get close to a moral consensus on this issue, once a tipping point was reached it would be much easier to get legislation passed to abolish the remaining vestiges of the industry.</p><p>However, the principal shortcoming of the consumer choice pathway is that it seems that moral advocacy to individuals on social issues such as animal farming is difficult (Tamler 2011). The animal advocacy movement used to rely more on the consumer choice pathway, but it has since moved away from this approach in response to more rigorous studies finding at least forms of individual outreach being much less effective than expected. Recent studies on leafleting suggest that it is not an effective intervention and may in fact accomplish nothing (ACE 2013) and the evidence for online ads has also been lacklustre<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u2075</u></a> (Reese 2016). Additionally, despite what looks like greater cultural enthusiasm for the diets, it is not clear that rates of vegetarianism and veganism do not appear to have increased substantially over time, despite advocacy efforts (Reese 2018).</p><p>Reese (2020) argues that this kind of individual focus has a number of shortcomings including lack of successful historical precedents, the defensiveness it arouses, and the comparative difficulty of individual versus institutional solutions, amongst other reasons. Additionally, a survey by Sentience Institute shows that a majority of survey experts thought that institutional messaging is a stronger advocacy option (note that Jacy Reese Anthis and Kelly Anthis are included among the 21 experts that they surveyed) (Anthis 2017).</p><p>A recent study by Faunalytics (Polanco 2022) evaluated many types of individual outreach. Of these, they recommended social media posts and news articles on animal advocacy because they reduce meat consumption in meat avoiders and did not have a negative backlash effect on meat eater behaviour. Animal advocacy videos, leaflets, and celebrity endorsements were also weakly recommended by them based on this evidence. This data is more optimistic about outreach, but many methods were still not put in the recommended categories due to either limited evidence or weak evidence of negative effects.</p><p>Similarly, Mathur et al. (2021) conducted a systematic review and meta-analysis of 100 studies on the effect of different advocacy techniques on meat consumption. They conclude that certain interventions are indeed promising, though they also highlight limitations such as reliance on self-report and the short-term nature of the studies. They call for more rigorous studies and analysis of the most promising interventions that they identified.</p><p>Moving from reviews and meta-analyses to another strong form of evidence, Carfora et al. (2017) conducted a randomised control trial using daily text messaging and documented a decrease in processed meat consumption from 3.13 to 1.74 portions of processed meat consumed throughout the week. The participants were undergraduates.</p><p>Though it may be unlikely to constitute a pathway in its own right, public choice and opinion remains an important background factor in many of the other pathways. Public opinion in particular is arguably a very significant driver, or even the most important driver, of many of the other pathways. For example, animal advocacy organisations are able to exert pressure and ultimately secure reforms during corporate campaigns because of the actual or potential public backlash. Similarly, legislative reforms may be much more likely if politicians can be shown that their constituency is very supportive of the reform.</p><p>The consumer choices of the public in reducing animal product consumption in favour of plant-based products (eventually cultured meat products) can also have a significant impact. However, despite this strong supporting role, as long as moral concerns do not play a large role in dictating many of their purchasing decisions, it may be hard for much change to be driven from consumer decisions.</p><p>It may also be the case that political and corporate interests can successfully oppose the demands of public opinion in some cases, so majority public support for something may often not be enough to get it passed. Overall, there is some positive evidence of some forms of moral advocacy as an approach, though evidence is limited.</p><p>&nbsp;</p><p><strong>Ballot initiatives</strong></p><p>Another way in which victory could be achieved is through ballot initiatives. Ballot initiatives are particularly notable here for their potential to achieve sudden striking results for the animal advocacy movement. Ballot initiatives are a form of direct democracy that are distinguished by both being created by the public and voted on by the public, unlike referenda which are voted on by the public but created by the legislator (Schukraft 2020). Ballot initiatives are not widely practised, with 24 US states and Switzerland being responsible for most of them.</p><p>Nevertheless, some important victories for animals have been achieved with ballot initiatives. In the US, amendment 10 in Florida banned gestation crates and proposition 204 in Arizona banned gestation crates and veal crates. Finally, proposition 2 in California banned gestation crates, veal crates, and battery cages (Schukraft 2020). Meanwhile, Sentience Politics has been active in Switzerland. In 2016, they led a ballot initiative to require schools and town halls in Lucerne, Basel, Zurich, and Berlin to offer at least one vegan option per day. They are also currently working on a ballot initiative to ban factory farming in Switzerland and a ballot initiative to extend fundamental rights to primates in Basel (SP n.d., n.d.).</p><p>Ballot initiatives offer the promise of being a sort of direct conduit of public opinion. When public opinion is indeed favourable this stands to be very useful. There are some indications that this is the case. For example, this Sentience Institute survey (J. R. Anthis 2017) found that a total of 47% of the US public supported a ban on slaughterhouses (14% strongly agree, 10% agree, and 17% somewhat agree). If this is accurate it could be taken as strong support for pursuing bold ballot initiatives now, as Sentience Politics\u2019 ballot initiative to ban factory farming in Switzerland (SP n.d.). However, the answers to the Sentience Institute survey are strongly at odds with this result, suggesting that public opinion is inconsistent, but overall not yet strongly favourable towards animals. As such, we think these survey results should be treated with extreme caution, though note also that a poll by Viva! in the UK found even higher support for banning factory farming, 85% (Viva! 2021).</p><p>The cost of ballot initiatives varies widely. For example, in their report on the subject Rethink Priorities (Schukraft 2020) describes cases costing between $47,000 and $23 million USD, with cases in larger jurisdictions tending to cost more (but probably less per capita). Also notable is that even a failed campaign may generate public awareness of the issue and generate a debate on the subject (Schukraft 2020).</p><p>Despite their great promise in converting public opinion into dramatic and sudden change, since there are only a limited number of countries and areas that allow ballot initiatives, ballot initiatives could not by themselves lead to victory for the animal advocacy movement. However, they could lead to local victories that could later spread to other areas. Switzerland is a well-respected country that is seen as a world leader in many areas. It is relatively small as well, which suggests that the investment needed to achieve this may be smaller. In this light, the work of Sentience Politics on ballot initiatives in Switzerland seems particularly valuable.</p><p>Investment in the states that allow ballot initiatives may also be promising. In this case, there is the potential for other states to be influenced by the decision, and the potential for it to later be adopted at a federal level. This would require substantially more work, especially since the animal welfare standards of the United States are substantially weaker than the animal welfare standards of Switzerland (World Animal Protection n.d.), though the United States is perhaps the most culturally influential country, as well as being responsible for a great deal of animal farming and animal product consumption.</p><p>Ballot initiatives seem very useful for getting a foothold to press for wider adoption, though it may well be that more work is required before more ambitious asks, such as ending factory farming, will be able to pass. The cost-effectiveness of ballot initiatives is not completely clear in most cases, but they should be considered, especially in areas where public support is particularly high, but politicians may be reluctant to move (Wrenn 2012).</p><p>&nbsp;</p><p><strong>Cultured meat</strong></p><p>Cultured meat (or clean meat) is meat from animal cells cultured and grown on a medium inside a bioreactor, outside the body of an animal. Though current production involves some use of animals, such as for bovine growth serum, it is possible that it will become economically feasible to avoid this in the future, meaning that no direct animal suffering will be required to produce the meat. Industry also claims that it will be possible to make cultured meat significantly more environmentally friendly than conventional meat by reducing carbon emissions, water usage, and land usage, as well as potentially making the product more healthy, for example by removing cholesterol (Swartz 2021).</p><p>Because of the perceived promise of the technology in outcompeting conventional meat, in doing away with the need for difficult moral advocacy, cultured meat has been regarded by many at animal advocacy organisations as the most promising way to victory. This enthusiasm was buoyed by many optimistic estimates about the timelines for the technology, as well as the theoretical argument that it is in principle much more efficient than standard animal agriculture, since only the meat is grown, not extraneous parts of the animal.</p><p>Bruce Friedrich, co-founder of the Good Food Institute, claims that the main arbiters of people\u2019s food purchasing decisions are taste, price, and convenience. He suggests then that once cultured meat is able to perform competitively with regular meat in these categories, it will begin to overtake conventional meat in market share, and, he claims, will eventually become dominant by becoming cheaper than conventional meat (Wiblin and Harris 2018).</p><p>These enthusiastic forecasts were driven largely by those inside the industry who are also likely to be biased in favour of shorter timelines, as well as being selected for optimism because people who were pessimistic about the industry are unlikely to work in it. However, a number of recent external evaluations of the technology have painted a less rosy picture of it and timelines for it.</p><p>For example, building on the work of a Mother Jones article (Philpott 2021), this Rethink Priorities article (Dullaghan 2021a) collected a total of 273 predictions associated with cultured meat. At the time of writing, 84 of these predictions had resolved, with 9 of them resolving correctly, and 75 resolving incorrectly. Note that many of these predictions were quite vague, and so interpretation of them is tricky. In many cases, literal readings were taken, though other interpretations are possible. Nevertheless, this broad trend suggests that predictions in this space have been overly optimistic.</p><p>Another report by Rethink Priorities (Dullaghan and Zhang 2022) paid a team of highly ranked Metaculus forecasters to predict cultured meat timelines. There predictions on cultured meat timelines were as follows:</p><p><i>Aggregated probabilities of cultured meat production targets</i></p><figure class=\"table\" style=\"width:740px\"><table style=\"background-color:transparent\"><tbody><tr><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>\u200bMetric Tons</strong></td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>2031</strong></td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>2036</strong></td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>2051</strong></td></tr><tr><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>100,000</strong></td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">15%</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">22%</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">46%</td></tr><tr><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>1M</strong></td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">3%</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">9%</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">31%</td></tr><tr><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>10M</strong></td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">Not asked</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">3%</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">18%</td></tr><tr><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\"><strong>50M</strong></td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">Not asked</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">Not asked</td><td style=\"background-color:transparent;border:1px solid rgba(var(--ricos-text-color-tuple, 33, 33, 33), 0.2);padding:0px;text-align:right\" colspan=\"1\" rowspan=\"1\">9%</td></tr></tbody></table></figure><p>Source: <a href=\"https://forum.effectivealtruism.org/posts/2b9HCjTiFnWM8jkRM/forecasts-estimate-limited-cultured-meat-production-through\"><u>Forecasts estimate limited cultured meat production through 2050</u></a></p><p>Though any prediction of the future is difficult, forecasters with traits similar to those chosen in that report have been found to be best at the task, outperforming field experts. The efficacy of forecasters, however, has largely only been studied up until the 10 year mark, and so these predictions should be treated with caution (Tetlock and Gardner 2016).</p><p>100,000 metric tons would only be a small fraction of the current global alternative protein production of 13 million metric tons and would not be in significant competition with the current global meat market of ~545 million metric tons (including fish) (Dullaghan and Zhang 2022). Therefore, these forecasts should be seen as relatively pessimistic, at least compared to many of the industry predictions about cultured meat.</p><p>An important study on the technical feasibility of cultured meat was conducted by David Humbird (2020). This study was important in influencing the Rethink Priorities analyses and predictions of cultured meat described earlier. The problem areas he identified were \u201c[l]ow growth rate, metabolic inefficiency, catabolite and CO2 inhibition, and bubble-induced cell damage will all limit practical bioreactor volume and attainable cell density\u201d. Further, he claims that \u201ca significant engineering effort would be required to address even one of these issues.\u201d The overall conclusion is pessimistic about the prospects for cultured meat being able to overcome these prospects and become cost competitive.</p><p>Chriki and Hocquette (2020) argue that there are substantial technological problems remaining and no real advances on these problems. Moreover, they argue that the data on whether cultured meat represents reduced greenhouse gas emissions compared with raising animals is equivocal, with some studies finding reduced emissions and others finding increased emissions.</p><p>Post et al. (2020) is more sanguine, arguing that though there are substantial technical and consumer acceptance problems, they see progress and anticipate that cultured meat will become a food commodity in the near future.</p><p>Despite this overall pessimism, progress has been made in this time. For example, Eat Just has been serving cultured meat in its Singapore restaurant since 2019. The nuggets are sold at 23$ per meal at a loss to the company. Eat Just have not disclosed how much of a loss this currently represents, though they revealed that in 2019 the cost of producing a single nugget was 50$ (McCormick 2021)<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u2076</u></a>.</p><p>Even if the technology is developed, there are reasons to think that there will at least need to be more work to achieve broad consumer acceptance. Bryant and Barnett (2020) reviewed studies of consumer acceptance of cultured meat, finding that typically a majority are willing to at least try it and a substantial proportion are interested in consuming it on a regular basis. One pattern is that people typically see benefits as accruing to society, but risks accruing to themselves. Perhaps because of this, frames that highlighted both individual and societal benefits were most effective. This data is fairly encouraging, though it does suggest that a majority of the population would continue consuming animals and therefore that cultured meat has an important, but not decisive role to play. However, optimistically, if the price became lower than regular meat and its taste and convenience were similar, some of these people might become accustomed to the new technology and begin to eat it regularly.</p><p>Perceived consumer concerns with cultured meat included unnaturalness, food safety, nutrition, and economic concerns, as well as simple disgust and neophobia. Though the unnaturalness of the product was perceived as a significant downside to it, consumers were largely not receptive to arguments that cultured meat is in fact natural. They were more convinced by arguments for the benefits of the product. They found that the term \u201cclean meat\u201d led to significantly higher rates of acceptance, \u201clab grown meat\u201d led to significantly lower rates of acceptance, and \u201ccultured meat\u201d led to rates of acceptance between these two (Bryant and Barnett 2018).</p><p>Beyond <i>taste, price, and convenience,</i> another framework for thinking about people\u2019s stated reasons for consuming meat is the four Ns framework that eating meat is <i>natural, normal, necessary, and nice</i>. Piazza et al. (2015) tested this theory, finding that an impressive 83% - 91% of rationalisation for eating meat fell into one of these four categories. Since these categories are moral justifications, people will be less likely to mention factors such as price and convenience, because these look trivial or even offensive when invoked to justify an immoral action. That people feel comfortable bringing up \u2018niceness\u2019 as a moral justification may be a testament to how small of a moral issue the consumption of animal products is regarded, since this would be viewed as unacceptable in the context of other moral issues (Piazza et al. 2015).</p><p>Overall, we are much less confident about the cultured meat pathway than we previously were. However, if the technological hurdles can be overcome, it would show a great deal of promise. Because of this, and because the recent scepticism regarding it is relatively speculative, we do not discount it as a pathway. One of the experts we interviewed argued that many technologies have similar hurdles that must be overcome, and, since we have not solved these yet, they may look difficult to solve from where we are standing. They also noted that the track record for predicting that various technologies were impossible is quite poor (interview proceedings).</p><p>&nbsp;</p><p><strong>Plant-based meats</strong></p><p>Perhaps we do not need to go as far as cultured meat in order to take over the market share from animal agriculture. Though plant-based meats are still a small share of the market currently, Impossible Foods and especially Beyond Meat have shown some popularity amongst meat eaters, and have gained some market share from them (Chiorando 2018). These products were designed to appeal to meat eaters and are sold in the meat aisles in supermarkets (Chiorando 2018).</p><p>The overall plant-based meat market has been seeing significant growth with a rise of 19% in value of sales from 2018 to 2019 and 46% from 2019 to 2020 (Bollard 2022b). However, the 2020 to 2021 period saw this stall, with 0% growth during this period (Bollard 2022b). Despite growth in some recent years, current consumption is low compared to regular meat, amounting to only 0.25% of the global meat market (including fish) according to one estimate (Dullaghan and Zhang 2022) or 1.4% of the U.S meat market according to another estimate (PBFA 2022). This covers plant-based meats, though plant-based milks have been more successful. They represent 16% of all milk sales in the US (PBFA 2022)</p><p>Plant-based meats already exist and we can likely expect improvements without the same sort of technical hurdles involved with cultured meat production. Some of these improvements have also brought plant-based meats molecularly closer to real meat, such as the plant-based heme content of Impossible Burgers, the core molecule in blood, which is an key ingredient for the distinctive flavour of meat (Choudhury et al. 2020).</p><p>However, even if plant-based meats can begin to compete on price and further improve flavour, people may have particular psychological and cultural associations with meat that may mean many will continue to eat regular meat. Meat is the centre of many cultural traditions involving food across the world. Being traditionally more expensive than other forms of food, it is also seen as a higher status form of consumption (Charles et al. 1988). Meat is also associated with masculinity and male virility, being seen as something that \u201creal men\u201d eat (Adams 2018; Loughnan, Bastian, and Haslam 2014).</p><p>Even when plant-based meats are functionally very similar to regular meat, the perceived essence of them and these corresponding psychological and cultural associations may be very different. This is also a significant worry for the adoption of cultured meat, but less so given that it is physically the same product, reducing the ease at which people can psychologically differentiate it.</p><p>A recent survey asked people what they would consume if taste and price were equalised and it found that 30% would opt for \u201cmeat-like alternatives from plants\u201d instead of \u201creal meat from animals\u201d (Bollard 2022b). Similarly, another survey asked people if they would choose regular meat or Beyond Meat if the two were equally priced. 27% said that they would choose Beyond Meat (Tonsor, Lusk, and Schroeder 2022; Bollard 2022b). Surveys like this should be interpreted with care because they ask consumers to predict how they would act in hypothetical situations. More compellingly, a study in a UCLA dining hall where consumers did not have to pay extra for the Impossible Burger found that 11-26% opted for it over the meat option when it was heavily promoted based on environmental benefits (Bollard 2022a; Malan 2020). More pessimistically, an analysis by Lusk et al. (2021) suggests that for each 10% reduction in price for plant-based beef products, there would be only a 0.15% reduction in US cattle production, in a slightly lower decline in global cattle production.</p><p>This offers some preliminary evidence that plant-based meat could serve as part of the solution, but would not see the kind of universal uptake it would take to end animal agriculture on its own. It is possible that a higher consumption of alternative proteins would mean that fewer people are resistant to reductions of the size of animal agriculture in the future. Considering this along with current low intake of plant-based animal products suggests that plant-based meat will play a supporting role at best in the victory of the animal advocacy movement.</p><p>&nbsp;</p><p><strong>Knockout animals</strong></p><p>Even if the industry does not manage to avoid \u201cthe absurdity of growing a whole chicken in order to eat the breast or wing\u201d, future technology may still offer us a pathway to avoid at least the worst suffering of these animals (Smithsonian Magazine and Eschner 2017). Shriver (2009) proposes that this may be done through making \u201cknockout animals\u201d. That is, animals who have a gene involved in pain perception are knocked out, meaning that they do not experience the affective dimension of pain and therefore do not \u2018mind\u2019 the experience. Morphine has a similar effect in removing the affective component of pain experience, and pain blunted by morphine is typically regarded as morally unproblematic, though it is difficult to be sure that the same holds true for animals, who are unable to share details of their experience with us (Shriver 2009). Shriver argues that because the sensory component of pain would be retained, some protective behaviour would remain, which could allow the animals to avoid some of the injuries that they might otherwise experience, and so making the raising of these animals potentially economically feasible.</p><p>Utilitarians should probably view this scenario as not inherently ethically problematic on the face of it. Deontologists might object to the use of animals as property or as \u201cmere means\u201d. Still, all plausible moral theories take consequences (especially suffering) into consideration in some respect and it would be hard to object that this scenario is not greatly preferable to the status quo (Shriver 2009).</p><p>Moral common sense might regard it as unnatural and as a perversion of the animal nature or of the dignity of the animals (Schultz-Bergin 2017). However, these are already greatly violated by the status quo that uses them for their flesh and causes them to grow unnatural proportions, in conditions that do not allow them to express their natures. Knocking out the genes responsible for pain would largely just be removing the suffering associated with this (Shriver 2009). Additionally, the idea of \u2018naturalness\u2019 is hard to define and it is hard to justify and reflectively endorse the moral importance commonly assigned to this term (Shriver and McConnachie 2018). Concern for \u201cnaturalness\u201d here may simply reduce down to status quo bias and disgust, which are difficult to give moral justifications for.</p><p>There are also genetic modifications that could be made to livestock short of the complete knockout of pain genes. These include modifying cows so that they are born without horns and therefore do not need to go through the painful process of dehorning and modifying chickens so that they are born with softer beaks that therefore are less damaging to other chickens during feather pecking. Some of these may come with welfare costs, for example if they prevent the animals from engaging in actions that they find naturally rewarding, though in the cases described above, given that horns and large portions of beaks are already regularly removed, the effect of the modification would largely just be to remove the great pain associated with these procedures. Having said this, one could reasonably argue that it is beneficial for the industry to solve these welfare issues in better ways that treat the underlying issues such as crowding, rather than modifying animals to fit terrible conditions in ways that reduce their welfare. These modifications would not reduce suffering as dramatically as knocking out pain genes, but they might still have a role to play (Shriver and McConnachie 2018). They could perhaps serve as stepping stones for the implementation of the knocking out of the pain genes, though it is unclear if they would be dramatically harder to implement than the knocking out of the pain genes.</p><p>Our most significant remaining worry with the knockout animals pathway is that a knockout of this gene might not remove the negative valence of psychological states that they are likely to experience in factory farm conditions such as fear, distress, and boredom. If this could not be done then this would not represent an appealing ideal or endgame, because this could still represent a huge amount of suffering. Moreover, the lack of ability to experience physical suffering might give farmers the moral licence to treat them much worse, increasing the amount of psychological suffering (Wilkinson 2009). Though this cannot be ruled out, it appears as though the underlying circuitry is shared between mental and physical suffering, suggesting that knockout animals would not experience mental suffering. Though this is still not certain and some circuits, such as depression, may use different pathways (interview proceedings). One experimental data point that knockout animals would <i>not</i> feel psychological pain is shown in this passage: \u201cInterestingly, studies have shown that that ablation of the anterior cingulate causes mother mammals to stop responding to the cries of their young, leading some researchers to suggest that the \u201cneural alarm\u201d system that underlies parental response was built off of the machinery for pain\u201d (Eisenberger, Lieberman, and Williams 2003; Shriver 2009).</p><p>A survey of 211 members of the general public found that more people opposed the use of knockout animals in experiments than supported it (Gardner and Goldberg 2007). The sample was largely composed of research scientists and vegetarians and/or members of the animal advocacy community (vegan was not included as an option). The first question that they were asked was \"[t]he technology now exists to create mice that are genetically engineered to feel no pain. Should these animals be created for the purposes of biological research?\" Overall, 31.6% strongly disagreed that they should be created, 16.5% disagreed, 14.9% strongly agreed, and 25.5% agreed, with the rest being undecided. From the animal advocacy community this level of disagreement was even higher, with 58.7% strongly disagreeing or 72% for animal advocates who were also vegetarian. This level of disagreement may mean that the prospect is a non-starter, since even the would be advocates of this idea are opposed.</p><p>Though to our knowledge the gene editing technology has not yet been applied to farmed animals, Wei et al. (2002) successfully developed the technology in mice. We are not aware of any special challenges involved in adopting the techniques for farmed animals, and so this technology is probably more readily accessible than cultured meat. Having said this, because of the strong criticism and disfavour from even allies of animals, we think this path to victory is unlikely. There is also uncertainty about whether this really makes the kind of physical pain these animals experience essentially morally unproblematic and significant uncertainty about if it would remove all mental suffering. This probably mixes pathways to risky until these questions are answered. It may still be an improvement of the status quo, but we cannot now be sure that it represents victory.</p><h2><strong>Other alternative protein technologies</strong></h2><p>Thinking more broadly than just growing animal cells in a medium outside of the body of an animal, there are also other alternative protein technologies that could potentially be developed and could take market share from meat.</p><p>For example, the organisation ALLFED (Alliance to Feed the Earth in Disasters) works on developing plans to use alternate food technologies to feed everyone on earth in the case of disasters that disrupt other forms of agriculture. Many of these technologies do not involve the use of animals and they argue that they could be used to feed everyone, although many would not be palatable enough to compete with meat in non-disasters scenarios (Denkenberger and Pearce 2014; ALLFED n.d.). As with plant-based meats, alternate protein technologies that do not produce cellular meat, with the cultural and psychological associations with that product.</p><p>One of these is the Finnish startup, Solar Foods, which is working on producing a high protein content food made from microbes. The microbes are grown using hydrogen from solar energy, water, carbon dioxide, and minerals (\u201cVTT: Hiili ei olekaan pelkk\u00e4\u00e4 saastetta \u2013 hiilidioksidin uusiok\u00e4yt\u00f6ll\u00e4 valmistetuilla tuotteilla ilmastonmuutoksen kannalta suotuisia vaikutuksia\u201d 2019; Grichnik, M\u00fcller, and Schreiber 2021). Some formulation of this product could potentially play a role in taking market share from conventional meat if it were possible to make this cheaper than conventional meat, though considerably more work would have to be done in order to make this taste competitive.</p><p>However, if they have advantages over conventional meat production, they may still be able to take market share from conventional meat if they are particularly successful. Therefore, though it is difficult to anticipate in advance, the animal advocacy movement could get substantial help from this movement in the future. However, this remains speculative and none of the current alternative protein technologies seem promising in this respect.</p><h2><strong>Failure</strong></h2><p>Finally, we should consider the possibility that victory is never achieved. There are a number of somewhat distinct though related scenarios this could encompass:</p><ol><li>Business as usual: No meaningful progress is ever made</li><li>Some significant progress is made before stalling and plateauing at some inadequate level</li><li>Some new development makes the situation for animals worse, potentially far worse than it was previously</li></ol><p>The scenario that Francione imagines is 1). It is difficult to maintain this without either a moral view that does not acknowledge distinctions between different levels of welfare or without some strong empirical claims that Francione argues for.</p><p>Another potential lesson from abolitionism to be wary of is to be wary of getting stuck at a local optimum that nevertheless falls well short of victory for the animal advocacy community as 2) suggests. An example of how this could happen is if the pace of welfare reforms continues until it reaches a point where most people, without further moral advocacy, view the conditions as acceptable even when they are made conscious of them. This scenario could happen if welfare reforms inspired momentum in the short term, but this did not translate into long term momentum.</p><p>For 3), consider that the switch to factory farming began in earnest after the second world war, despite a rising concern for animals (Danielle 2005). This rising concern for animals is suggested from its increasing appearance and legislation in countries such as the UK and from the increasing attention to it in at least academic moral discourse (Broom 2011). It can also be seen from the book Animal Machines (Harrison 2013), first published in 1964, which described the huge ethical issue of factory farming as it was unfolding. This concern therefore does not seem to have protected animals from the horrors of factory farming, and perhaps the higher level of concern for animals seen in modern times would not protect us from some new horror in our treatment of animals if it became economically profitable to do so.</p><p>One potential candidate for this worsening of conditions is the farming of insects. Insect farming has been heralded as an environmentally friendly form of protein, with animal welfare issues infrequently discussed, however, because insects have such small body sizes, huge numbers of them need to be raised and killed to produce the same amount of meat. It is unclear if they are sentient because of the difficulty of assessing this in animals with minds much different from our own, but there are some surprisingly strong indications of sentience in them, which cannot be dismissed (Schukraft 2019).</p><p>Some argue that insects would not mind the conditions because they have small and simpler minds, and so are easy to satisfy. However, they are likely to be raised in extremely intensive conditions, with extremely little research on their welfare in different conditions. Additionally, there is already extreme neglect of the welfare of farmed animals, and in the case of insects this will probably be total neglect of their welfare. There are no animal welfare laws concerning the treatment of them (Rowe 2020; Carpendale 2019). But most concerningly, the massive numbers of insects suggest that the rise of farming them could represent a new era of factory farming, despite concern for animals being at perhaps an all-time high.</p><p>&nbsp;</p><h2><strong>CRUCIAL CONSIDERATION 1: ABOLITIONISM VERSUS PROTECTIONISM</strong><br>&nbsp;</h2><p>An important and central debate in advocacy is between abolitionism and another perspective that has been called \u201cwelfarism\u201d, \u201cnew welfarism\u201d, or \u201canimal protectionism\u201d. In our research for this report, we found this debate particularly relevant as a crucial consideration. In particular, we thought that if abolitionism were true, it would have large implications for this report. Though, like most staff at animal advocacy organisations, we came in with greater sympathy for animal protectionism, we have attempted to keep an open mind in our consideration of the arguments and evidence.</p><p>The terms welfarism or new welfarism are the terms generally used by abolitionists such as Francione, but defenders of that position, such as Garner prefer a more neutral term that does not suggest that they are aligned with animal welfare science \u2013 which they see as too close to the industry \u2013 or imply a focus of welfare rather than rights (Francione and Garner 2010). We will use the term \u201canimal protectionism\u201d in this post, so as not to beg the question against their position by assuming those things through our terminology.</p><p>The abolitionist approach is characterised by rejection of what it regards as piecemeal welfare campaigns and single-issue campaigns, in favour of a clearer, hard-line rejection of the use of all sentient animals. Abolitionists argue that these single-issue campaigns are misguided and are perhaps actively harmful because they reinforce the message that other forms of animal exploitation not touched by them are morally acceptable. Some abolitionists would also argue that a campaign against the wearing of fur is also misguided because it does not mention leather or other forms of animal exploitation and therefore suggests that fur is problematic, but these other products are not (Francione and Garner 2010).</p><p>Animal protectionism is normally defined in contrast with abolitionism. So-called animal protectionists may or may not agree with abolitionist goals of ending animal farming and all other use of animals, but they at least agree that incremental improvements in the conditions of animals are both morally valuable in themselves and strategically valuable. They view their position as more pragmatic in accepting an incremental approach, rather than accepting nothing short of full veganism and animal liberation.</p><p>A strong abolitionist approach unfortunately leaves few animal advocacy approaches which are permissible. Francione suggests that we should be doing direct, no frills vegan outreach as individuals or as grassroots organisations, which does not allow the movement to experiment with a diversity of tactics and to push forward on multiple fronts. Unfortunately, the evidence base behind individual outreach so far suggests that is not an effective approach or at least not an approach that should be pursued to the exclusion of other approaches (Sentience Institute 2020; ACE 2013; Reese 2016).</p><p>Francione suggests that the debate ultimately comes down to differing moral values, though some animal protectionists disagree that this is a significant factor as they also aim to end all animal exploitation (Vegan Debate Archive 2018; Francione and Garner 2010). Instead, these animal protectionists claim that the difference is only one of strategy, with their position being the more pragmatic and ultimately the more achievable position.</p><p>In favour of this, they point to the many welfare reforms that have been passed by animal advocacy organisations that generally share their philosophy. Abolitionists of course reject that these amount to meaningful improvements and argue that they may be at best distractions, and at worst roadblocks on the pathway to victory.</p><h2>&nbsp;</h2><h2><strong>Do welfare reforms constitute meaningful reductions in suffering?</strong></h2><p>In response, protectionists argue that the reforms are, at the very least, significant for the amount of suffering they constitute in the short term. This represents the first major disagreement on whether welfare reforms really do constitute meaningful reductions in the amount of suffering.</p><p>To begin with, it is difficult to discuss welfare reforms in general in this way, since each welfare reform is different. In our work helping organisations prioritise amongst possible interventions for the next campaigns, we examine the expected welfare effects of different reforms using systematic research methods. As a result of this research, we are certainly sceptical of some reforms in certain contexts, and we believe some to have a neutral or negative impact on animals. We always encourage organisations to think more carefully and critically about which issues to campaign on and which welfare reforms to support.</p><p>However, many appear to have quite a large positive impact for animals, and Francione\u2019s sweeping claim that none or essentially none of these reforms would constitute a meaningful reduction in suffering seems very implausible. In assessing his claims, we look through the chapter dedicated to this: The Empirical and Structural Defects of Animal Welfare Theory in Rain Without Thunder: The Ideology of the Animal Rights Movement (G. Francione 2010). We were unable to find adequate empirical sources cited in that chapter to back up these claims.</p><p>Turning to other sources, we took a case for which we judged should be an easy case for Francione\u2019s position if it were true and also a particularly important case given the large amount of investment in it. This is the case of cage free (aviary) systems for laying hens instead of cage systems. The Open Philanthropy Project was criticised by DxE who claim that cage free (aviary) systems for housing hens may actually be worse for hen welfare (as tracked chiefly by mortality rate). To make this claim, they just wrote this <a href=\"https://docs.google.com/document/d/1mEoaEm9xRuVkX3nWggRPz700RU7_Z_QgRuEtQMWiMbM/edit\"><u>memo</u></a> that reviews the literature on the subject. In response, the Open Philanthropy Project wrote a report (\u201cHow Will Hen Welfare Be Impacted by the Transition to Cage-Free Housing?\u201d 2017) arguing that they were no longer as confident in the previous claims, but they still believe that cage free systems were necessary to transition to even higher welfare systems and that an analysis of the strongest studies in the literature suggested that cage systems were higher welfare.</p><p>Our assessment agrees with Open Philanthropy in this case, though we remain unsure. Another recent attempt to calculate the difference between the two systems by Welfare Footprint Project (Schuck-Paim and Alonso 2021) concluded that cage free is clearly superior, with most of the welfare difference coming from the deprivation of natural behaviours in caged systems. Given that this should have been an easy case of a marginal improvement for the abolitionist position, we take this as significant evidence against their thesis.</p><p>Other cases are less ambiguous. For example, in many countries, farmed animals \u2013 especially fish in aquaculture \u2013 are not always stunned before slaughter. Though stunning is often inadequate with respect to its reliability in causing unconsciousness, it is difficult to argue that it is not beneficial for the animals compared to slaughter without stunning (Gregory 2005; AE UK n.d.; Jung-Schroers et al. 2020). Additionally, if it were thought that stunning was particularly unreliable, and therefore does not represent a meaningful welfare improvement, organisations could instead campaign to increase the reliability of the stunning process.</p><p>Francione thinks that veal is not worse than dairy because each is just the use of animals (Vegan Debate Archive 2018). Full analyses of the question have found that beef involves much more suffering (and veal plausibly results in much more suffering than other forms of beef) (Ladak 2020; Tomasik 2007). The differences between these two cases seem like extremely relevant details for moral theory to pay attention to.</p><p>Additionally, some single-issue campaigns, such as banning fur, do not involve welfare reforms as such, they involve removing animals, or whole groups of animals, from the production system entirely. This criticism therefore does not apply to those campaigns.</p><h2>&nbsp;</h2><h2><strong>Will substitution to lower welfare meat occur, negating benefits of reforms?</strong></h2><p>The second major disagreement is on whether people will just substitute for lower welfare animal products. This, again, is always a concern that organisations should pay attention to when picking campaigns. This is particularly a concern when there is a risk for consumers to substitute for even lower welfare products, such as substituting for small animals. Our report \u2018<a href=\"https://forum.effectivealtruism.org/posts/iWgJvjtbzGyefSmgW/meat-tax-why-chickens-pay-the-price\"><u>Meat Tax: Why Chickens Pay The Price</u></a>\u2019 represents a case where the risk of this appears to be high and so we recommend against intervention.</p><p>Having said this, it is again a strong claim to say that substitution for lower welfare products will almost always happen. While meat is generally a somewhat inelastic product (Andreyeva, Long, and Brownell 2010), cross price elasticities differ based on products compared and geographical area, and this must be approached in a case-by-case way<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u2077</u></a>. Note as well that even when some goods are substituted for each other, it is rare for products to be near perfect substitutes for each other. This means that while some substitution will take place, they will switch to the substitute product at significantly less than a one-to-one rate.</p><p>Akaichi and Revoredo-Giha (2016) studied cross price elasticities between meats labelled as higher welfare, meats labelled as organic, and regular products in Scotland. They found that organic pork was a substitute for higher welfare meat and they found some amount of substitution between higher welfare meat and regular meat when the price of the former was raised. This is in the direction of Francione\u2019s claim, but there was far from perfect substitution in each case, conflicting with Francione\u2019s claim.</p><p>Francione\u2019s argument also does not appear to apply to legislative changes. For example, if all EU egg production becomes cage free, it is very unlikely that almost all consumption of eggs in the EU would shift to imported eggs from countries where this was still allowed. If import restrictions are also included in the welfare legislation, it becomes impossible for this to happen (at least within the law).</p><p>&nbsp;</p><h2><strong>Do welfare reforms reduce costs, thereby helping the meat industry?</strong></h2><p>The third major disagreement is on whether welfare reforms simply reduce the cost for meat producers. Similar to Francione\u2019s other arguments, we think there is some legitimate concern here, but that this argument is largely unfounded.</p><p>Factory farming took over from conventional animal farming because its industrialised system allowed it to produce animal products much more cheaply. This of course resulted in significantly more animal suffering as the animals were treated like cogs in a machine (Harrison 2013). In many cases, welfare reforms resemble a gradual return to conventional animal agriculture. For example, keeping animals with access to the outside, using animals that do not have genes that cause them to grow very quickly at the expense of their welfare, and not separating a mother dairy cow from their calves. Though there will be some increased profitability that comes from these reforms based on the increased health and well-being of the animals, this must be balanced against the much larger costs associated with moving away from the industrialised model.</p><p>Fearing and Matheny (Fearing and Matheny 2007) assess the evidence about costs from welfare reforms, and while they generally find increases in costs (summarised in table 1 displayed below) summarise this evidence as:</p><blockquote><p><i>\u201cProduction costs associated with many farm animal welfare improvements are modest and can be offset by marginally increased prices to consumers. As long as the playing field is leveled by regulation or adoption by producer or retailer associations, the effect on producers can be minimal.\u201d</i><br>&nbsp;</p></blockquote><p>Similarly, they add that:</p><blockquote><p><i>\u201cDemand for meat, eggs, and dairy products is said to be \u201cprice inelastic,\u201d meaning consumers are relatively unresponsive to price changes. Producers as a group can pass increased costs on to consumers without a loss in profits, as the decrease in demand is more than compensated for by the increase in unit price (Huang and Lin 2000). It is ultimately consumers who bear the costs of improved animal welfare.\u201d</i></p></blockquote><p><img src=\"https://static.wixstatic.com/media/c2b5df_f4dcac52729446b385e9c5f4b7ccb1d6~mv2.png/v1/fill/w_728,h_426,al_c,lg_1,q_85,enc_auto/c2b5df_f4dcac52729446b385e9c5f4b7ccb1d6~mv2.png\"></p><p><strong>Source:</strong> (Fearing and Matheny 2007)</p><p>Though these represent significant price increases, the analysis by Fearing and Matheny that farmers may increase the price to compensate overall losses supports the abolitionist argument on this point. Though also note that Fearing and Matheny also argue that thinking that it is in the economic interest of farmers to increase welfare is a fallacy because while morbidity and mortality are costly to farmers, decreased production costs of higher welfare animals may be even more costly (Fearing and Matheny 2007).</p><p>Other studies we looked at confirm this, with data from the EU after the banning of battery cages in some countries showed a decrease in consumption in those countries, in contrast to an increase in consumption in EU countries that did not implement the ban (MFA 2016). Similarly, data from California shows a steady decrease in consumption of eggs after California banned battery cages, a pattern not seen in other states in the US which did not ban battery cages (Lusk 2017).</p><p>If the abolitionist position here were correct, the natural question would be why would animal agriculture resist welfare reforms in numerous ways if these reforms ultimately just reduced costs for them? Francione anticipates this response and argues that the industry would resist reforms like this out of a general principle to make it more difficult to interest groups like animal advocacy organisations to be able to influence them. The principle is that even if these programs are beneficial, future reforms may not be and so it is worthwhile to impose costs on interest groups trying to control their behaviour. However, Francione maintains that all or virtually all welfare reforms reduce costs or can be easily compensated by shifting prices to consumers, so if this were true, animal advocacy organisations would pose no threat to the meat industry, now or in the future. If most animal advocacy organisations just function as consultants to the meat industry (as Francione claims in Francione and Garner [2010]), why would the meat industry have to fear further cost-saving suggestions? The industry also appears to spend a lot of time and energy resisting the campaigns of animal advocacy organisations and go to great lengths to belittle them publicly, it is implausible that they would do this just to preserve the principle that they should not be pushed around. Organisations do not usually strenuously resist the suggestion of their consultants.</p><p>In their communication with the meat industry, animal advocacy organisations will naturally stress potential cost savings to the industry<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u2078</u></a>. Some welfare reforms may indeed represent cost savings, and when this is true, the organisations will naturally stress that in order to be most convincing. This may suggest to outside observers who are looking for this evidence, such as Francione, that this is all that is happening, but this does not mean that all welfare reforms will save the industry money.</p><p>Francione also argues that because animal agriculture organisations are corporations, they will only adopt welfare reforms if it is profit maximising for them to do so, which he suggests indicates that he must be right (Francione and Garner 2010). This neglects that this interpretation of corporations needing to act in profit maximising ways is simplistic and not entirely correct (Stout 2015). It also neglects that the pressure from animal advocacy organisations during corporate campaigns can threaten to inflict larger costs on these organisations through reputational damage or disruption, so the welfare reform may indeed be much more costly than baseline, even if it becomes the least costly option for them given the pressure from the corporate campaign.</p><p>&nbsp;</p><h2><strong>Do welfare reforms inspire momentum or complacency?</strong></h2><p>The second major disagreement between the two positions is whether reforms inspire momentum or complacency. Momentum refers to the idea that welfare reforms and other single-issue campaigns wet people\u2019s appetites for further reforms, therefore being beneficial to the long-term goal of victory. Complacency refers to the idea that welfare reforms lead to people thinking that current conditions are sufficient, and thereby slow down progress towards victory.</p><p>There have been limited attempts to directly test this question. Lusk (Lusk 2010) examined the effect of exposure to information about Proposition 2 in California, which banned the use of cages to keep egg laying chickens. They found that in the months leading up to Proposition 2 (during which there was an associated public awareness campaign), demand for cage free and organic eggs increased whilst demand for other types of eggs fell.</p><p>Tonsor and Olynk (2011) found that exposure to information about animal welfare slightly reduced consumption of pork and chicken, but consumption of beef remained unchanged. They also found that people largely substituted for non-meat products rather than other meats. Note that this studied the effect of animal welfare information in general, rather than specifically animal welfare reforms.</p><p>A study by Mercy for Animals (Caldwell 2016) found that information about both corporate and legislative welfare reforms caused people both to report that they would decrease their animal product consumption and to rate the living conditions of farmed animals as lower welfare. Note that self-reports of the intention to reduce animal product consumption are subject to social desirability bias and have been shown to be unreliable as a predictor of actual consumption.</p><p>Finally, Mathur et al. (2021) reviewed studies on the effect of information provision on meat consumption, concluding that this information about welfare reforms reduced self-reported intention to consume meat.</p><p>A later study by researchers at the organisation (Harris, Ladak, and Mathur 2021) found no significant effect on animal farming opposition from exposure to neutrally presented information on cage free reforms in a randomised experiment. The authors were surprised to find no effect given previous research on the subject. Their study did find a significant increase in opposition to factory farming from exposure to neutrally presented information on current farming practices.</p><p>Our interpretation of the historical evidence favours protectionism. For example, the UK is one of the best in the world at protecting animals, including some outright bans that Francione would support such as a ban on fur farming (World Animal Protection 2021). Yet this appears to come from centuries of steady progress with welfare reforms, as the UK was one of the first countries to pass animal welfare legislation, such as the 1822 Act to Prevent the Cruel and Improper Treatment of Cattle (Inglis 2012; Clifton 2016).</p><p>There was an early example of an abolitionist approach in the UK which was the radical anti-vivisectionists, who called for an end to all animal experimentation, but they were not successful in achieving their goals and it is unclear if they had a strong lasting influence (Clifton 2016). In general, the pattern appears to be that countries with the strongest animal welfare laws also tend to be the countries with the highest rates of vegetarians or vegans, a pattern at odds with the abolitionist view that there is a conflict between these things (MFA 2016).</p><p>In their summary of evidence for foundational questions (Sentience Institute 2020), Sentience Institute concludes that there is sufficient evidence to conclude in favour of momentum rather than complacency. This is only one of three debates in which they considered the evidence to weigh significantly in favour of one position.</p><p>Overall, so far the limited evidence appears to us to support momentum over complacency. Having said this, it is possible that certain campaigns could still cause complacency, and organisations should take some care to avoid this through for example appropriate messaging.</p><p>&nbsp;</p><h2><strong>Pragmatic versus uncompromising</strong></h2><p>An underlying theme in the debate between welfarism and abolitionism is whether we should be pragmatic in our advocacy or if we should be uncompromising. The pragmatic approach has the promise of meeting people where they are, rather than where we wish they would be. It allows us to suggest targeted next steps for people, rather than repeating largely the same demands in each case. For example, it allows us to praise people in their steps to reduce animal consumption, even if they continue to fall short of full veganism. In this way it allows a stepwise approach to the solution of problems that may make them more achievable. Rather than asking people to make a leap to full abolitionism, we just ask them to keep taking the next step forward, which may be much more psychologically realistic.</p><p>The pragmatic approach also allows for more easy cooperation with the animal agriculture industry, with politicians, and with other stakeholders in the debate. Again, it allows us to meet them where they are and encourage steps in the right direction.</p><p>On the other hand, an uncompromising approach represents a stronger statement of these ideals. If someone is doing something widely regarded as greatly morally wrong, we would not normally reinforce them or praise them for doing something only very slightly less morally wrong. Many animal welfare reforms are analogous to this, and so if we do celebrate the reforms that are made, we risk suggesting that animal agriculture was never greatly morally wrong to begin with.</p><p>An uncompromising approach may also allow us a clearer statement of these ideals. Instead of only focusing on what the next steps will be, which in many cases may only represent marginal improvements, we can clearly and frequently state our ideals. There is some experimental support for this claim. Mathur et al. (2021) reviewed studies on the effect of messages to reduce meat consumption and found that interventions that suggested going vegan had a 31% larger effect than no recommendation and appeared to have larger effects than reducetarian or vegetarian messaging.</p><p>Possible framework for thinking about the trade-off between these views is the \u201cfoot-in-the-door technique\u201d and the contrasting \u201cdoor-in-the-face technique\u201d. The foot-in-the-door technique involves making a small, achievable ask in order to make it more likely the person will later agree to larger asks to remain consistent with their original decision. In contrast, the door-in-the-face technique involves making an initial large ask that is sure to be turned down to make future asks seem reasonable in comparison. These contrasting approaches have been studied in social psychology with Burger et al. (Burger 1999) finding no difference between their efficacy, though Dillard et al. (1984) found the effects of the techniques to be modest.</p><p>At first glance, the efficacy of the door-in-face technique appears to support an abolitionist approach; however, in the case of Francione style abolitionism, the ask is always veganism, rather than a smaller ask afterwards. This evidence therefore does not support Francione style abolitionism, but it does support the use of some abolitionist asks amongst other tactics, as long as they are not presented as the only possible option.</p><p>A possible synthesis or compromise between the two positions is to accept and to some extent encourage steps forward by individuals and industry whilst at the same time making it clear that these are only marginal steps forward and that much more remains to be done (Reese 2018). Speculatively, the door-in-the-face technique in particular suggests that a \u201cgood cop, bad cop\u201d dynamic, with different organisations in the movement playing these respective roles, may be effective.</p><p>&nbsp;</p><p><strong>Conclusion regarding abolitionism and protectionism</strong></p><p>Ultimately, some synthesis of the ideas of protectionism and abolitionism may be possible. It is possible to learn from the abolitionist message that we should frame welfare reforms more as only marginal improvements and express our more ambitious goals of ending animal exploitation while not opposing reforms or believing that they are necessarily distractions. We can also favour campaigns that send a stronger abolitionist message and frame them as such. This approach may still be closer to the protectionist position and may leave an abolitionist unsatisfied.</p><h2><br><strong>CRUCIAL CONSIDERATION 2: THE SHAPE OF MORAL PROGRESS</strong></h2><p>The second main crucial consideration that we considered in our research for this report is whether views of moral progress as steady and even are accurate or if it is substantially regressive at times and irregular.</p><p>The metaphor of an expanding moral circle was popularised by Peter Singer and widely used in the effective altruism community to refer to the idea that our moral consideration for other beings can be represented as a circle and that this circle can be expected to extend further, steadily and evenly (Singer 1981). This metaphor of a circle suggests any effort to expand the moral circle to a new group of beings will also help to extend it to another group of beings further out in terms of potential moral consideration. The thought behind this consistent and perhaps steady progress is that essentially the same moral principles apply in each case.</p><p>To what extent this more optimistic view is correct is a crucial consideration because it suggests how inevitable we can expect moral progress to be. If it is wrong, we could expect to see significant regressions or irregular progress. If it is correct, it also suggests that fundamental rights approaches may be more promising. This is because the more principles-based picture in moral psychology is that it assumes people will be more receptive to the arguments for fundamental rights and, further, that it will be easier to argue to extend those rights to other beings.</p><p>However, this picture assumes a certain consistency and reliance on principles that are perhaps not as common amongst most people. Based on this, an opposing hypothesis is that progress will be inconsistent and so progress to extend the moral circle in one case will not necessarily apply to another case. Rather than a moral circle we might have a moral polygon or a much less symmetrical and even shape. Concern for animals may then be a fragile meme, not an inevitable one.</p><p>Jesse Clifton (2016) suggests that the history of human concern and protection for animals has been uneven and unprincipled. One example of this is that in 675, the Buddhist Emperor of Japan, Tenmu, banned the consumption of all domestic animals from April 1 to September 30 each year (Watanabe 1854; Bunko, n.d.), yet this was quickly abandoned in the Meiji restoration and today the animal welfare standards of Japan lag behind similarly wealthy countries in the West. If concern for animals was the result of a consistent and steady march forward, we might expect that Japan would be far ahead of other countries by this point in virtually all animal-rights issues, and perhaps a significant exporter of various animal advocacy ideas.</p><p>Another case Clifton discusses is that of India which has long had high rates of vegetarianism and some significant concern for animals, as seen in its religious traditions. Nevertheless, rates of meat consumption in India are increasing rapidly, full veganism is still rare, and animal protection in India is still dominated by protection of cows rooted in Hinduism, with other forms of animal advocacy not seeing as much uptake as might have been expected (Devi et al. 2014).</p><p>A counterargument to these conclusions is that religious prohibitions may work differently from arguments based on the interests of the animals themselves. Religious arguments may be followed without significant agreement with any moral principles that a secular animal ethics might present. This is especially in the case of the Japanese Emperor since the restrictions that were imposed arbitrarily excluded wild animals and allowed meat consumption in part of the year, in addition to being handed down from above. These cases may therefore not represent strong evidence. Still, since a change in attitude can follow from a change in action (Bastian et al. 2012), we might still expect significant change from a prohibition that started this early. For example, it appears to be the case that many people who become vegan for health reasons later go on to consider veganism to be important for animal welfare reasons (Leenaert 2017).</p><p>There are also many arbitrary exceptions in the West suggesting a less than perfect moral circle. For example, chickens are not covered by animal protection laws in the United States. Similarly, farmed fish in the UK are not protected by any of the more detailed legislation that applies to other farmed animals and wild caught fish are not protected by even the basic protections of the Animal Welfare Act 2006 (DEFRA 2006; Collinson 2018).</p><p>There are also the obvious cases that should be familiar to any animal advocate. Animals who are commonly kept as pets, such as cats and dogs, are treated as far more morally significant than other animals. Though some might argue that our special relationships with these animals justify our greater moral concern for them, it is difficult to justify the vast difference in our treatment of these animals compared with our treatment of other animals. This then seems like a clear case of an uneven moral expansion.</p><p>Of course, even if moral expansion is far from even and inevitable, the general pattern where moral expansion towards one group of beings generally results in moral expansion towards other groups may still hold. This is the view that we find most plausible, though we are highly uncertain.</p><h2>&nbsp;</h2><h2><strong>AFTER ANIMAL FARMING</strong></h2><p>As discussed, in addition to the end of animal agriculture, many voices call for more ambitious goals. Though the contribution of our current actions towards these goals is more speculative, it is worth considering their potential influence.</p><p>One more ambitious goal to help animals more robustly is presented in Zoopolis (Donaldson and Kymlicka 2011). In this book, Donaldson and Kymlicka argue for the inclusion of animals into our political system. This would include giving domestic animals the rights of citizens, wilderness (wild) animals the rights of nationals of their own states, and liminal animals (urban animals or others that live around us but independently) rights that lie somewhere between wild and domestic animals. Note that they claim that citizenship does not entail all of the same rights in every case, so domestic animals would not be given the right to vote, for example. They argue that the system better represents the neglected importance obligations stemming from the various kinds of relationships between humans and animals, as well as having practical benefits that other moral positions could appreciate.</p><p>There are other proposals for the political inclusion of animals. Janneke Vink argues in The Open Society and its Animals (Vink 2020) that animals should be included in liberal democracies, and that a failure to include them represents a threat to the democratic credentials, and possibly inclines them toward sliding into authoritarianism. Vink argues that this inclusion should at a minimum include the right to life, the right not to be tortured, and the right to bodily integrity. This book represents a statement of how society would ideally be structured with respect to animals, but it does not offer a roadmap of how to get there.</p><p>David Pearce (2015) imagines a welfare state for wild elephants which could serve as a template for a larger programme of helping animals in the wild. He imagines that the elephants could live in the wild, but with access to some assistance such as healthcare, emergency food relief, protection from poachers, orthodontics to extend their lifespan, and help during natural disasters and crises.</p><p>This style of intervention would be extremely expensive to implement on a wide scale, but if human wealth increases dramatically, interventions like this could be done and expanded to many other animals. Significant research is also required to help ensure more of the flow through effects of our actions are understood and accounted for. It is plausible that current actions to help farmed animals would also increase people\u2019s willingness to help animals in the wild, therefore increasing the likelihood that this would be done. The scale of animals who could be helped in the wild also dwarfs the scale of animals killed for human consumption. This is in addition to the many wild animals we could abstain from harming if we took their interests into account (Animal Ethics n.d.).</p><p>Looking still further into the future, some call for the inclusion of digital minds into our moral community (Gunkel 2018; Harris 2021). They argue that it is reasonably likely that such minds will be created and that when they are created, they are likely to be conscious. Moreover, they argue that our actions to extend the moral circle to animals could extend to moral consideration for these digital minds in the future.</p><p>These scenarios represent more ways in which we can reduce the suffering of other animals or beings beyond ending factory farming. Though this would be a monumental step, there is more work to be done. The relationship between these scenarios and our current efforts to help farmed animals is less clear, but it is worth considering in order to maximise the flow-through effects of our actions.</p><p>There are some ways that current advocacy efforts can more strongly contribute to better outcomes in these scenarios. One is that we can share that our moral ideals extend well past ending factory farming and personal consumption. One way of doing this is with anti-speciesist messaging, rather than vegan messaging,</p><p>Anti-speciesist messaging involved stressing the moral equality of humans and nonhuman animals, rather than a predominant focus on a particular (important) aspect of this, such as dietary choice in the case of veganism. Anti-speciesism implies not only refraining from harming animals, but also actively helping them, as we would humans in similar situations. It therefore makes intuitive sense that if we were able to spread it effectively, it would contribute more strongly to these after animal farming scenarios. Though this is only one of a variety of trade-offs between those two approaches that must be considered.<br>&nbsp;</p><h2><strong>CONCLUSION AND REMAINING UNCERTAINTIES</strong></h2><p>This report covers a wide range of subjects in a fairly shallow manner. A report on the subject could have gone into much greater depth, as well as covering subjects that we do not touch. In addition, it is difficult to make predictions and extract general principles for animal advocates based on the evidence we have reviewed. As such, these conclusions should be treated as tentative and preliminary. We can imagine others looking at the same evidence and drawing substantially different conclusions.</p><p>Though the strongest versions of abolitionism, such as the version argued for by Francione, are not well supported, there are some kernels of truth behind them that are useful for advocates to learn.</p><p>Constructive criticism by abolitionists of the rest of the movement is helpful, but denigrations of the rest of the movement, such as accusations that they are now only a wing of the animal agriculture industry, are unhelpful. Opposing welfare reforms is also unmotivated and counterproductive.</p><p>Specific abolitionist campaigns likely have a role to play in victory. These campaigns do have the advantage that they send a stronger and clearer moral message and their more ambitious, moonshot, nature can be appropriate in some cases, especially once concern for animals and the Overton window of the moral treatment of our issue of them has been shifted considerably in their favour.</p><p>When possible, other campaigns can also be framed in more abolitionist ways. The key way that this can be done is by presenting incremental progress as only one step along the pathway to victory, rather than now representing a morally acceptable situation.</p><p>Having said this, there is also room for other organisations that work more closely with the industry, and do not include abolitionist messaging such as this. This should just not be taken too far into a defence of current industry practices against improvements. Though we understand that may also be difficult for organisations to sculpt an ideal message when people cannot be counted on to read full posts or articles by the organisation and when news coverage of their work may drop nuance.</p><p>Turning to victory scenarios, no single pathway seems to have a much greater likelihood of resulting in victory than any other. We therefore do not recommend a majority focus on any single pathway. Instead, we see strength in pursuing a diversity of approaches. This is particularly true when these approaches are pursued with an experimental spirit, with thorough use of appropriate measurement and evaluation.</p><p>This does not mean that we should be investing equally in all approaches. Some such as corporate campaigns certainly deserve more investment than other approaches, but we should be wary that too much of the movement\u2019s resources do not go into this approach at the neglect of other approaches. Though some approaches, such as leafleting, that have evidence that they are not effective in contributing towards victory or having substantial direct effects should continue to be neglected by the movement. Having said this, the overall evidence base and the movement is not strong and some more research on these approaches would still be valuable; charities with a strong focus on measurement and evaluation could still usefully try these approaches as experiments.</p><p>This also does not mean that individual organisations should pursue a variety of different programs. This may already be too common in the animal advocacy community considering that there are gains to be made through specialisation.</p><p>Cultured meat was widely regarded by the movement as the most likely pathway to victory, but recent evaluation from outside the industry has been far less sanguine. Still, if those technological barriers can be overcome, cultured meat offers a more straightforward pathway to victory than many of the other approaches. Another form of alternative or plant-based protein could also take the place of cultured meat.</p><p>With a subject this speculative and with such breadth, we naturally have many remaining uncertainties. The subject touches on and depends on most questions within animal advocacy. There is much more that we could have examined in more depth included in the report that could have changed the conclusions substantially. There were many crucial considerations that we encountered that we did not fully examine and discuss<a href=\"https://www.animalask.org/post/pathways-to-victory-how-can-we-end-animal-agriculture#viewer-vahu\"><u>\u2079</u></a>. One particularly important thing that we could have done would have been to analyse cultured meat in a more technical way, such as by analysing Humbird (2020) more rigorously.</p><p>We understand that more general conclusions such as these are less action guiding than more specific and confident conclusions. There is also a risk that animal advocacy organisations rationalise their current actions as meeting these standards, when in fact more reflection and self-scepticism is needed. We therefore hope that this is treated appropriately as part of a broader conversation on the topic that organisations should continue to reflect on.</p><h2><br><strong>NOTES</strong></h2><p>1. We will use the term cultured meat to refer to all cultured animal products and plant-based meats to refer to all plant-based animal products.</p><p>2. Consequentialist approaches can also be pluralistic, though it is more common for them to mostly or entirely be concerned with well-being.</p><p>3. In the case, Somerset, a slave who had been bought in Boston and brought to England, successfully filed for habeas corpus when Stewart, the man who bought him, tried to send him to Jamaica. The judge ruled in favour of Somerset, implying that he was a person before the law. Despite this ruling and these local effects, it took many more years before slavery was outlawed in the UK (Wise 2005; Anthis and Anthis 2017).</p><p>4. In comparing some elements of a slavery case we do not mean to suggest that the cases are identical or to suggest slavery is less of a moral evil; we are just noting some relevant parallels that we can learn from.</p><p>5. Though some studies by people involved with animal advocacy organisations have suggested more promising results from online advertising. See for example (Bryant et al. 2021).</p><p>6. Note that some people have reported a difficulty in actually buying these nuggets. See the comments in (Dullaghan 2021b).</p><p>7. Inelastic products are products for which consumer buying decisions are not strongly influenced by price and so increase in price does cause correspondingly large decrease in demand (Kenton 2021). Cross price elasticity refers to the effect that a change in the price of one product has on demand for a second product. If demand for the second product increases when the price of the first product increases, they are known as substitute goods. If the converse happens and demand for the second product decreases when the price of the first product increases, they are known as complementary goods (Hayes 2022).</p><p>8. Though other lines are possible depending on context and the price increase or saving of the welfare reform. In other cases, organisations may argue that a product should not be that cheap, implying moral hazardous corners are being cut in order to have a product be that cheap.</p><p>9. Some of the important crucial considerations for this can be found in Sentience Institute\u2019s Summary of Evidence for Additional Questions (Sentience Institute 2020).</p><p>&nbsp;</p><p><strong>BIBLIOGRAPHY</strong></p><p>ACE. 2013. \u201cLeafleting.\u201d Animal Charity Evaluators. November 27, 2013. <a href=\"https://animalcharityevaluators.org/research/reports/leafleting/\">https://animalcharityevaluators.org/research/reports/leafleting/</a>.</p><p>Adams, Carol J. 2018. <i>The Sexual Politics of Meat</i>. Routledge.</p><p>AE UK. n.d. \u201cINVESTIGATION: Scottish Salmon.\u201d Animal Equality UK. Accessed March 1, 2022. <a href=\"https://animalequality.org.uk/act/scottish-salmon\">https://animalequality.org.uk/act/scottish-salmon</a>.</p><p>Akaichi, Faical, and Cesar Revoredo-Giha. 2016. \u201cConsumers Demand for Products with Animal Welfare Attributes: Evidence from Homescan Data for Scotland.\u201d <i>British Food Journal</i> 118 (7): 1682\u20131711.</p><p>Alexander, L., and M. Moore. 2007. \u201cDeontological Ethics. The Stanford Encyclopedia of Philosophy.\u201d Center for the Study of Language and Information (CSLI), Stanford University \u2026.</p><p>ALLFED. n.d. \u201cAbout.\u201d ALLFED. Accessed April 18, 2022. <a href=\"https://allfed.info/about\">https://allfed.info/about</a>.</p><p>Andreyeva, Tatiana, Michael W. Long, and Kelly D. Brownell. 2010. \u201cThe Impact of Food Prices on Consumption: A Systematic Review of Research on the Price Elasticity of Demand for Food.\u201d <i>American Journal of Public Health</i> 100 (2): 216\u201322.</p><p>Animal Ethics. n.d. \u201cEgalitarianism.\u201d Animal Ethics. Accessed April 4, 2022a. <a href=\"https://www.animal-ethics.org/egalitarianism/\">https://www.animal-ethics.org/egalitarianism/</a>.</p><p>\u2014\u2014\u2014. n.d. \u201cPrioritarianism.\u201d Animal Ethics. Accessed April 4, 2022b.</p><p><a href=\"https://www.animal-ethics.org/prioritarianism/\">https://www.animal-ethics.org/prioritarianism/</a>.</p><p>\u2014\u2014\u2014. n.d. \u201cThe Importance of the Future.\u201d Animal Ethics. Accessed April 18, 2022c.</p><p><a href=\"https://www.animal-ethics.org/importance-of-the-future/\">https://www.animal-ethics.org/importance-of-the-future/</a>.</p><p>\u2014\u2014\u2014. n.d. \u201cUtilitarianism.\u201d Animal Ethics. Accessed April 4, 2022d. <a href=\"https://www.animal-ethics.org/utilitarianism/\">https://www.animal-ethics.org/utilitarianism/</a>.</p><p>Anthis, Jacy. 2017. \u201cEffective Animal Advocacy Researcher Survey June 2017.\u201d Sentience Institute. 2017. <a href=\"https://www.sentienceinstitute.org/blog/eaa-researcher-survey-june-2017\">https://www.sentienceinstitute.org/blog/eaa-researcher-survey-june-2017</a>.</p><p>Anthis, Jacy Reese. 2017. \u201cAnimals, Food, and Technology (AFT) Survey 2017,\u201d Surveys, , November. <a href=\"https://sentienceinstitute.org/animal-farming-attitudes-survey-2017\">https://sentienceinstitute.org/animal-farming-attitudes-survey-2017</a>.</p><p>Anthis, Kelly, and Jacy Reese Anthis. 2017. \u201cSocial Movement Lessons from the British Antislavery Movement Focused on Applications to the Movement against Animal Farming.\u201d 2017.</p><p><a href=\"https://sentienceinstitute.org/downloads/Social_Movement_Lessons_From_the_British_Antislavery_Movement.pdf\">https://sentienceinstitute.org/downloads/Social_Movement_Lessons_From_the_British_Antislavery_Movement.pdf</a>.</p><p>Bastian, Brock, Steve Loughnan, Nick Haslam, and Helena R. M. Radke. 2012. \u201cDon\u2019t Mind Meat? The Denial of Mind to Animals Used for Human Consumption.\u201d <i>Personality &amp; Social Psychology Bulletin</i> 38 (2): 247\u201356.</p><p>Baumgartner, Frank, and Jamie Harris. 2020. <i>Frank Baumgartner of UNC-Chapel Hill on Policy Dynamics, Lobbying, and Issue Framing</i>. Sentience Institute. <a href=\"https://www.sentienceinstitute.org/podcast/episode-10.html\">https://www.sentienceinstitute.org/podcast/episode-10.html</a>.</p><p>Baumgartner, Frank R., and Bryan D. Jones. 2010. <i>Agendas and Instability in American Politics, Second Edition</i>. University of Chicago Press.</p><p>Bollard, Lewis. 2022a. \u201cIf We Build It, Will They Come?\u201d Open Philanthropy Farm Animal Welfare Newsletter. 2022. <a href=\"https://mailchi.mp/cf613c018ffc/if-we-build-it-will-they-come?e=[UNIQID]\">https://mailchi.mp/cf613c018ffc/if-we-build-it-will-they-come?e=[UNIQID]</a>.</p><p>\u2014\u2014\u2014. 2022b. \u201cPeak Plant-Based Meat?\u201d Open Philanthropy Farm Animal Welfare Newsletter. 2022. <a href=\"https://us14.campaign-archive.com/?u=66df320da8400b581cbc1b539&amp;id=b7fdedaf65\">https://us14.campaign-archive.com/?u=66df320da8400b581cbc1b539&amp;id=b7fdedaf65</a>.</p><p>Broom, Donald M. 2011. \u201cA History of Animal Welfare Science.\u201d <i>Acta Biotheoretica</i> 59 (2): 121\u201337.</p><p>Bryant, Christopher, and Julie Barnett. 2018. \u201cConsumer Acceptance of Cultured Meat: A Systematic Review.\u201d <i>Meat Science</i> 143 (September): 8\u201317.</p><p>\u2014\u2014\u2014. 2020. \u201cConsumer Acceptance of Cultured Meat: An Updated Review (2018\u20132020).\u201d <i>NATO Advanced Science Institutes Series E: Applied Sciences</i> 10 (15): 5201.</p><p>Bunko, Kawade. n.d. <i>Edo Food History</i>.</p><p>Burger, J. M. 1999. \u201cThe Foot-in-the-Door Compliance Procedure: A Multiple-Process Analysis and Review.\u201d <i>Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc</i> 3 (4): 303\u201325.</p><p>Caldwell, K. 2016. \u201cWelfare Reforms and Meat Consumption.\u201d Mercy For Animals. November 14, 2016. <a href=\"https://mercyforanimals.org/blog/welfare-reforms-survey/\">https://mercyforanimals.org/blog/welfare-reforms-survey/</a>.</p><p>Carfora, V., D. Caso, and M. Conner. 2017. \u201cRandomised Controlled Trial of a Text Messaging Intervention for Reducing Processed Meat Consumption: The Mediating Roles of Anticipated Regret and Intention.\u201d <i>Appetite</i> 117 (October): 152\u201360.</p><p>Carpendale, Max. 2019. \u201cThoughts on the Welfare of Farmed Insects.\u201d Effective Altruism Forum. 2019. <a href=\"https://forum.effectivealtruism.org/posts/jbR9XrZbsqCsnR3vy/thoughts-on-the-welfare-of-farmed-insects\">https://forum.effectivealtruism.org/posts/jbR9XrZbsqCsnR3vy/thoughts-on-the-welfare-of-farmed-insects</a>.</p><p>Charles, Nickie, Charles Kerr, Marion Kerr, and Professor of Sociology in the School of Social Sciences and International Development Nickie Charles. 1988. <i>Women, Food, and Families</i>. Manchester University Press.</p><p>Chiorando, Maria. 2018. \u201c70% Of People Eating Plant-Based? Beyond Burger Are Meat Eaters.\u201d Plant Based News. January 12, 2018. <a href=\"https://plantbasednews.org/lifestyle/meat-eaters-make-up-70-of-beyond-burger-consumers/\">https://plantbasednews.org/lifestyle/meat-eaters-make-up-70-of-beyond-burger-consumers/</a>.</p><p>Choudhury, Deepak, Satnam Singh, Jasmine Si Han Seah, David Chen Loong Yeo, and Lay Poh Tan. 2020. \u201cCommercialization of Plant-Based Meat Alternatives.\u201d <i>Trends in Plant Science</i> 25 (11): 1055\u201358.</p><p>Chriki, Sghaier, and Jean-Fran\u00e7ois Hocquette. 2020. \u201cThe Myth of Cultured Meat: A Review.\u201d <i>Frontiers in Nutrition</i> 7 (February): 7.</p><p>Clifton, Jesse. 2016. \u201cLessons from the History of Animal Rights.\u201d EA Forum. 2016. <a href=\"https://forum.effectivealtruism.org/posts/D6sKBrvoGh3XQLTvn/lessons-from-the-history-of-animal-rights\">https://forum.effectivealtruism.org/posts/D6sKBrvoGh3XQLTvn/lessons-from-the-history-of-animal-rights</a>.</p><p>Cochrane, Alasdair. 2012. <i>Animal Rights Without Liberation: Applied Ethics and Human Obligations</i>. Columbia University Press.</p><p>Cockburn, Chloe. 2018. \u201cPhilanthropists Must Invest in an Ecology of Change.\u201d Stanford Social Innovation Review. 2018. <a href=\"https://ssir.org/articles/entry/philanthropists_must_invest_in_an_ecology_of_change\">https://ssir.org/articles/entry/philanthropists_must_invest_in_an_ecology_of_change</a>.</p><p>Collinson, Alice. 2018. \u201cLegal Protection of Animals in the UK.\u201d Animal Legal &amp; Historical Center. 2018. <a href=\"https://www.animallaw.info/article/legal-protection-animals-uk\">https://www.animallaw.info/article/legal-protection-animals-uk</a>.</p><p>Cotter, William R. 1994. \u201cThe Somerset Case and the Abolition of Slavery in England.\u201d <i>History </i>79 (255): 31\u201356.</p><p>Courtney Dillard, Tess Morrison, Marcy Regambal, Alan Presburger. n.d. \u201cThe Fight over Smoking in the US (1964 \u2013 2000).\u201d Mercy for Animals.</p><p>Danielle, N. 2005. \u201cHappier Meals: Rethinking the Global Meat Industry.\u201d <i>Worldwatch Paper</i> 121 (5).</p><p>DEFRA. 2006. \u201cExplanatory Notes to Animal Welfare Act 2006.\u201d Queen\u2019s Printer of Acts of Parliament. <a href=\"https://www.legislation.gov.uk/ukpga/2006/45/notes/division/7/11/9\">https://www.legislation.gov.uk/ukpga/2006/45/notes/division/7/11/9</a>.</p><p>Denkenberger, David, and Joshua M. Pearce. 2014. <i>Feeding Everyone No Matter What: Managing Food Security After Global Catastrophe</i>. Academic Press.</p><p>Devi, Subramaniam Mohana, Vellingiri Balachandar, Sang In Lee, and In Ho Kim. 2014. \u201cAn Outline of Meat Consumption in the Indian Population - A Pilot Review.\u201d <i>Korean Journal for Food Science of Animal Resources</i> 34 (4): 507\u201315.</p><p>Dillard, James P., John E. Hunter, and Michael Burgoon. 1984. \u201cSequential-Request Persuasive Strategies: Meta-Analysis of Foot-in-the-Door and Door-in-the-Face.\u201d <i>Human Communication Research</i> 10 (4): 461\u201388.</p><p>Donaldson, Sue, and Will Kymlicka. 2011. <i>Zoopolis: A Political Theory of Animal Rights</i>. OUP Oxford.</p><p>Dullaghan, Neil. 2021a. \u201cCultured Meat Predictions Were Overly Optimistic.\u201d Effective Altruism Forum. 2021. <a href=\"https://forum.effectivealtruism.org/posts/YYurNqQDAWNiQJv9K/cultured-meat-predictions-were-overly-optimistic\">https://forum.effectivealtruism.org/posts/YYurNqQDAWNiQJv9K/cultured-meat-predictions-were-overly-optimistic</a>.</p><p>\u2014\u2014\u2014. 2021b. \u201cCultured Meat Predictions Were Overly Optimistic \u2014.\u201d Rethink Priorities. September 15, 2021. <a href=\"https://rethinkpriorities.org/publications/cultured-meat-predictions-were-overly-optimistic\">https://rethinkpriorities.org/publications/cultured-meat-predictions-were-overly-optimistic</a>.</p><p>Dullaghan, Neil, and Linch Zhang. 2022. \u201cForecasts Estimate Limited Cultured Meat Production through 2050 - EA Forum.\u201d Effective Altruism Forum. 2022. <a href=\"https://forum.effectivealtruism.org/posts/2b9HCjTiFnWM8jkRM/forecasts-estimate-limited-cultured-meat-production-through\">https://forum.effectivealtruism.org/posts/2b9HCjTiFnWM8jkRM/forecasts-estimate-limited-cultured-meat-production-through</a>.</p><p>EA Forum. 2022. \u201cEAA Is Relatively Overinvesting in Corporate Welfare Reforms.\u201d Effective Altruism Forum. 2022. <a href=\"https://forum.effectivealtruism.org/posts/kHdKWmTcS3FfcYAZj/eaa-is-relatively-overinvesting-in-corporate-welfare-reforms\">https://forum.effectivealtruism.org/posts/kHdKWmTcS3FfcYAZj/eaa-is-relatively-overinvesting-in-corporate-welfare-reforms</a>.</p><p>Eisenberger, Naomi I., Matthew D. Lieberman, and Kipling D. Williams. 2003. \u201cDoes Rejection Hurt? An FMRI Study of Social Exclusion.\u201d <i>Science</i> 302 (5643): 290\u201392.</p><p>Evans, Annie. 2021. \u201cPROGRESS: 85% OF CORPORATE CAGE-FREE COMMITMENTS HAVE BEEN FULFILLED.\u201d The Humane League. 2021.</p><p><a href=\"https://thehumaneleague.org.uk/article/progress-on-2020-cage-free-commitments\">https://thehumaneleague.org.uk/article/progress-on-2020-cage-free-commitments</a>.</p><p>Fearing, Jennifer, and Gaverick Matheny. 2007. \u201cThe Role of Economics in Achieving Welfare Gains for Animals,\u201d State of the Animals 2007, . <a href=\"https://www.wellbeingintlstudiesrepository.org/sota_2007/4/\">https://www.wellbeingintlstudiesrepository.org/sota_2007/4/</a>.</p><p>Francione, Gary. 2010. <i>Rain without Thunder: The Ideology of the Animal Rights Movement</i>. Temple University Press.</p><p>Francione, Gary Lawrence, and Robert Garner. 2010. <i>The Animal Rights Debate: Abolition Or Regulation?</i> Columbia University Press.</p><p>Gardner, Renee M., and Alan M. Goldberg. 2007. \u201cPain-Free Animals: An Acceptable Refinement.\u201d <i>Jpn Soc Altern Anim Exp</i> 14: 145\u201349.</p><p>Glendinning, Lee. 2008. \u201cSpanish Parliament Approves \u2018Human Rights\u2019 for Apes.\u201d <i>The Guardian</i>, June 26, 2008. <a href=\"http://www.theguardian.com/world/2008/jun/26/humanrights.animalwelfare\">http://www.theguardian.com/world/2008/jun/26/humanrights.animalwelfare</a>.</p><p>Gregory, N. G. 2005. \u201cRecent Concerns about Stunning and Slaughter.\u201d <i>Meat Science</i> 70 (3): 481\u201391.</p><p>Grichnik, Dietmar, Eduard M\u00fcller, and Robert Schreiber. 2021. \u201cAlternative Proteins.\u201d HSG FoodTech Lab so. <a href=\"https://www.unisg.ch/-/media/dateien/unisg/wissen/kommunikation/2021/white-paper-alternative-proteins.pdf\">https://www.unisg.ch/-/media/dateien/unisg/wissen/kommunikation/2021/white-paper-alternative-proteins.pdf</a>.</p><p>Gunkel, David J. 2018. <i>Robot Rights</i>. MIT Press.</p><p>Haines, Herbert H. 2013. \u201cRadical Flank Effects.\u201d In <i>The Wiley-Blackwell Encyclopedia of Social and Political Movements</i>. Oxford, UK: Blackwell Publishing Ltd. https://doi.org/<a href=\"http://dx.doi.org/10.1002/9780470674871.wbespm174\">10.1002/9780470674871.wbespm174</a>.</p><p>Harris, Jamie. 2021. \u201cThe Importance of Artificial Sentience.\u201d Sentience Institute. 2021. <a href=\"https://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience\">https://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience</a>.</p><p>Harris, Jamie, Ali Ladak, and Maya B. Mathur. 2021. \u201cThe Effects of Exposure to Information about Animal Welfare Reforms on Animal Farming Opposition: A Randomized Experiment.\u201d https://doi.org/<a href=\"http://dx.doi.org/10.31234/osf.io/p6hbk\">10.31234/osf.io/p6hbk</a>.</p><p>Harrison, Ruth. 2013. <i>Animal Machines</i>. CABI.</p><p>Hayes, Adam. 2022. \u201cUnderstanding the Cross Elasticity of Demand.\u201d February 8, 2022. <a href=\"https://www.investopedia.com/terms/c/cross-elasticity-demand.asp\">https://www.investopedia.com/terms/c/cross-elasticity-demand.asp</a>.</p><p>Horta, Oscar. 2017. \u201cAnimal Suffering in Nature: The Case for Intervention.\u201d <i>Environmental Ethics</i> 39 (3): 261\u201379.</p><p>\u201cHow Will Hen Welfare Be Impacted by the Transition to Cage-Free Housing?\u201d 2017. Open Philanthropy. September 15, 2017. <a href=\"https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/how-will-hen-welfare-be-impacted-transition-cage-free-housing\">https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/how-will-hen-welfare-be-impacted-transition-cage-free-housing</a>.</p><p>Humbird, David. 2020. \u201cScale-Up Economics for Cultured Meat: Techno-Economic Analysis and Due Diligence.\u201d https://doi.org/<a href=\"http://dx.doi.org/10.31224/osf.io/795su\">10.31224/osf.io/795su</a>.</p><p>Inglis, Lucy. 2012. \u201cWhat Folly Is This? Animal Welfare in Georgian London.\u201d</p><p>Jung-Schroers, Verena, Uta Hildebrandt, Karina Retter, Karl-Heinz Esser, John Hellmann, Dirk Willem Kleingeld, Karl Rohn, and Dieter Steinhagen. 2020. \u201cIs Humane Slaughtering of Rainbow Trout Achieved in Conventional Production Chains in Germany? Results of a Pilot Field and Laboratory Study.\u201d <i>BMC Veterinary Research</i> 16 (1): 197.</p><p>Kagan, Shelly. 1989. <i>The Limits of Morality</i>. Clarendon Press.</p><p>Kenton, Will. 2021. \u201cWhat Does Inelastic Mean?\u201d May 19, 2021. <a href=\"https://www.investopedia.com/terms/e/inelastic.asp\">https://www.investopedia.com/terms/e/inelastic.asp</a>.</p><p>Ladak, Ali. 2020. \u201cThe Impact Of Replacing Animal Products.\u201d Faunalytics. September 16, 2020. <a href=\"https://faunalytics.org/animal-product-impact-scales/\">https://faunalytics.org/animal-product-impact-scales/</a>.</p><p>Leenaert, Tobias. 2017. <i>How to Create a Vegan World: A Pragmatic Approach</i>. Lantern Publishing &amp; Media.</p><p>Loughnan, Steve, Brock Bastian, and Nick Haslam. 2014. \u201cThe Psychology of Eating Animals.\u201d <i>Current Directions in Psychological Science</i> 23 (2): 104\u20138.</p><p>Lusk, Jayson. 2017. \u201cHow Animal Welfare Laws Affect Egg Prices and Production \u2014.\u201d Jayson Lusk. March 3, 2017. <a href=\"https://jaysonlusk.com/blog/2017/3/3/how-animal-welfare-laws-affect-egg-prices-and-production?_ga=2.25034234.1356096118.1651654404-905171083.1649767582\">https://jaysonlusk.com/blog/2017/3/3/how-animal-welfare-laws-affect-egg-prices-and-production?_ga=2.25034234.1356096118.1651654404-905171083.1649767582</a>.</p><p>Lusk, Jayson, Daniel Blaustein-Rejto, Saloni Shah, and Glynn T. Tonsor. 2021. \u201cImpact of Plant-Based Meat Alternatives on Cattle Inventories and Greenhouse Gas Emissions.\u201d https://doi.org/<a href=\"http://dx.doi.org/10.2139/ssrn.3961790\">10.2139/ssrn.3961790</a>.</p><p>Lusk, Jayson L. 2010. \u201cThe Effect of Proposition 2 on the Demand for Eggs in California.\u201d <i>Journal of Agricultural &amp; Food Industrial Organization</i> 8 (1). https://doi.org/<a href=\"http://dx.doi.org/10.2202/1542-0485.1296\">10.2202/1542-0485.1296</a>.</p><p>Malan, Hannah Joy. 2020. \u201cSwap the Meat, Save the Planet: A Community-Based Participatory Approach to Promoting Healthy, Sustainable Food in a University Setting.\u201d University of California. <a href=\"https://search.proquest.com/openview/635b74326e166a3a12c3d09c3b3c88bb/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y\">https://search.proquest.com/openview/635b74326e166a3a12c3d09c3b3c88bb/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a>.</p><p>Mathur, Maya B., Jacob Peacock, David B. Reichling, Janice Nadler, Paul A. Bain, Christopher D. Gardner, and Thomas N. Robinson. 2021. \u201cInterventions to Reduce Meat Consumption by Appealing to Animal Welfare: Meta-Analysis and Evidence-Based Recommendations.\u201d <i>Appetite</i> 164 (September): 105277.</p><p>McCormick, Erin. 2021. \u201cEat Just Is Racing to Put \u2018no-Kill Meat\u2019 on Your Plate. Is It Too Good to Be True?\u201d <i>The Guardian</i>, June 16, 2021. <a href=\"http://www.theguardian.com/food/2021/jun/16/eat-just-no-kill-meat-chicken-josh-tetrick\">http://www.theguardian.com/food/2021/jun/16/eat-just-no-kill-meat-chicken-josh-tetrick</a>.</p><p>MFA. 2016. \u201cWhy We Work for Policy Change.\u201d Mercy For Animals. November 14, 2016. <a href=\"https://mercyforanimals.org/why-policy-change/\">https://mercyforanimals.org/why-policy-change/</a>.</p><p>NhRP. n.d. \u201cFrequently Asked Questions.\u201d Nonhuman Rights Project. Accessed April 13, 2022. <a href=\"https://www.nonhumanrights.org/frequently-asked-questions/\">https://www.nonhumanrights.org/frequently-asked-questions/</a>.</p><p>Ozden, James. 2021. \u201cAnalysis of EA Funding within Animal Welfare from 2019-2021 - EA Forum.\u201d EA Forum. 2021. <a href=\"https://forum.effectivealtruism.org/posts/6H9QGZkdMzDEdKNCX/analysis-of-ea-funding-within-animal-welfare-from-2019-2021-1\">https://forum.effectivealtruism.org/posts/6H9QGZkdMzDEdKNCX/analysis-of-ea-funding-within-animal-welfare-from-2019-2021-1</a>.</p><p>\u2014\u2014\u2014. 2022. \u201cPotential Theories of Change for the Animal Advocacy Movement.\u201d EA Forum. 2022. <a href=\"https://forum.effectivealtruism.org/posts/A49f8hEGMoSmvzSCW/potential-theories-of-change-for-the-animal-advocacy\">https://forum.effectivealtruism.org/posts/A49f8hEGMoSmvzSCW/potential-theories-of-change-for-the-animal-advocacy</a>.</p><p>PBFA. 2022. \u201c2021 U.S. Retail Sales Data for the Plant-Based Foods Industry.\u201d Plant Based Foods Association. March 23, 2022. <a href=\"https://www.plantbasedfoods.org/2021-u-s-retail-sales-data-for-the-plant-based-foods-industry/\">https://www.plantbasedfoods.org/2021-u-s-retail-sales-data-for-the-plant-based-foods-industry/</a>.</p><p>Pearce, David. 2015. \u201cA Welfare State for Elephants? A Case Study of Compassionate Stewardship.\u201d <i>Relations. Beyond Anthropocentrism</i> 3 (2): 153\u201364.</p><p>Philpott, Tom. 2021. \u201cIs Lab Meat About to Hit Your Dinner Plate?\u201d <i>Mother Jones</i>, 2021. <a href=\"https://www.motherjones.com/food/2021/08/is-lab-meat-about-to-hit-your-dinner-plate/\">https://www.motherjones.com/food/2021/08/is-lab-meat-about-to-hit-your-dinner-plate/</a>.</p><p>Piazza, Jared, Matthew B. Ruby, Steve Loughnan, Mischel Luong, Juliana Kulik, Hanne M. Watkins, and Mirra Seigerman. 2015. \u201cRationalizing Meat Consumption. The 4Ns.\u201d <i>Appetite</i> 91 (August): 114\u201328.</p><p>Polanco, Andrea. 2022. \u201cPlanting Seeds: The Impact Of Diet &amp; Different Animal Advocacy Tactics.\u201d Faunalytics. April 27, 2022. <a href=\"https://faunalytics.org/relative-effectiveness/\">https://faunalytics.org/relative-effectiveness/</a>.</p><p>Post, Mark J., Shulamit Levenberg, David L. Kaplan, Nicholas Genovese, Jianan Fu, Christopher J. Bryant, Nicole Negowetti, Karin Verzijden, and Panagiota Moutsatsou. 2020. \u201cScientific, Sustainability and Regulatory Challenges of Cultured Meat.\u201d <i>Nature Food</i> 1 (7): 403\u201315.</p><p>Reese, Jacy. 2016. \u201cHow Effective Are Online Ads?\u201d Animal Charity Evaluators. August 15, 2016. <a href=\"https://animalcharityevaluators.org/blog/how-effective-are-online-ads/\">https://animalcharityevaluators.org/blog/how-effective-are-online-ads/</a>.</p><p>\u2014\u2014\u2014. 2018. <i>The End of Animal Farming: How Scientists, Entrepreneurs, and Activists Are Building an Animal-Free Food System</i>. Beacon Press.</p><p>\u2014\u2014\u2014. 2020. \u201cInstitutional Change and the Limitations of Consumer Activism.\u201d <i>Palgrave Communications</i> 6 (1): 1\u20138.</p><p>Rowe, Abraham. 2020. \u201cInsects Raised for Food and Feed \u2014 Global Scale, Practices, and Policy \u2014.\u201d Rethink Priorities. June 29, 2020. <a href=\"https://rethinkpriorities.org/publications/insects-raised-for-food-and-feed\">https://rethinkpriorities.org/publications/insects-raised-for-food-and-feed</a>.</p><p>Schuck-Paim, Cynthia, and Wladimir Alonso. 2021. <i>Quantifying Pain in Laying Hens: A Blueprint for the Comparative Analysis of Welfare in Animals</i>.</p><p>Schukraft, Jason. 2019. \u201cInvertebrate Sentience: A Useful Empirical Resource.\u201d Rethink Priorities. June 9, 2019. <a href=\"https://rethinkpriorities.org/publications/invertebrate-sentience-useful-empirical-resource\">https://rethinkpriorities.org/publications/invertebrate-sentience-useful-empirical-resource</a>.</p><p>\u2014\u2014\u2014. 2020. \u201cIntervention Profile: Ballot Initiatives.\u201d Rethink Priorities. January 13, 2020. <a href=\"https://rethinkpriorities.org/publications/intervention-profile-ballot-initiatives\">https://rethinkpriorities.org/publications/intervention-profile-ballot-initiatives</a>.</p><p>Schultz-Bergin, Marcus. 2017. \u201cThe Dignity of Diminished Animals: Species Norms and Engineering to Improve Welfare.\u201d <i>Ethical Theory and Moral Practice: An International Forum</i> 20 (4): 843\u201356.</p><p>Sentience Institute. 2020. \u201cSummary of Evidence for Foundational Questions in Effective Animal Advocacy.\u201d Sentience Institute. 2020. <a href=\"https://www.sentienceinstitute.org/foundational-questions-summaries\">https://www.sentienceinstitute.org/foundational-questions-summaries</a>.</p><p>Shriver, Adam. 2009. \u201cKnocking Out Pain in Livestock: Can Technology Succeed Where Morality Has Stalled?\u201d <i>Neuroethics</i> 2 (3): 115\u201324.</p><p>Shriver, Adam, and Emilie McConnachie. 2018. \u201cGenetically Modifying Livestock for Improved Welfare: A Path Forward.\u201d <i>Journal of Agricultural &amp; Environmental Ethics</i> 31 (2): 161\u201380.</p><p>Singer, Peter. 1981. <i>The Expanding Circle</i>. Citeseer.</p><p>Sinnott-Armstrong, Walter, and Edward N. Zalta. 2015. \u201cConsequentialism.\u201d Available at: https://plato. stanford. edu/archives/win2015/entries \u2026. <a href=\"https://plato.stanford.edu/entries/consequentialism/\">https://plato.stanford.edu/entries/consequentialism/</a>.</p><p>Smithsonian Magazine, and Kat Eschner. 2017. \u201cWinston Churchill Imagined the Lab-Grown Hamburger.\u201d 2017. <a href=\"https://www.smithsonianmag.com/smart-news/winston-churchill-imagined-lab-grown-hamburger-180967349/\">https://www.smithsonianmag.com/smart-news/winston-churchill-imagined-lab-grown-hamburger-180967349/</a>.</p><p>SP. n.d. \u201cCantonal Initiative \u2018Basic Rights for Primates.\u2019\u201d Sentience Politics. Accessed April 1, 2022a. <a href=\"https://sentience.ch/en/initiatives/initiative-basic-rights-for-primates/\">https://sentience.ch/en/initiatives/initiative-basic-rights-for-primates/</a>.</p><p>\u2014\u2014\u2014. n.d. \u201cInitiative to Abolish Factory Farming.\u201d Sentience Politics. Accessed April 1, 2022b. <a href=\"https://sentience.ch/en/initiatives/initiative-to-abolish-factory-farming/\">https://sentience.ch/en/initiatives/initiative-to-abolish-factory-farming/</a>.</p><p>Stout, Lynn. 2015. \u201cCorporations Don\u2019t Have to Maximize Profits.\u201d <i>The New York Times</i>, 2015. <a href=\"https://www.nytimes.com/roomfordebate/2015/04/16/what-are-corporations-obligations-to-shareholders/corporations-dont-have-to-maximize-profits?fbclid=IwAR0cWevHZSuP891HvRF0pBfbWlIF1GL5xwXepIgd_P47g_M0WWZKkQV7EI0\">https://www.nytimes.com/roomfordebate/2015/04/16/what-are-corporations-obligations-to-shareholders/corporations-dont-have-to-maximize-profits?fbclid=IwAR0cWevHZSuP891HvRF0pBfbWlIF1GL5xwXepIgd_P47g_M0WWZKkQV7EI0</a>.</p><p>Swartz, Elliot. 2021. \u201cThe Science of Cultivated Meat.\u201d The Good Food Institute. January 27, 2021. <a href=\"https://gfi.org/science/the-science-of-cultivated-meat/\">https://gfi.org/science/the-science-of-cultivated-meat/</a>.</p><p>Tamler, Sommers. 2011. \u201cThe Limits of Moral Argument.\u201d Netherlands: TEDx Hogeschool Utrecht. December 17, 2011. <a href=\"https://www.youtube.com/watch?v=YuEm03Ko0J8\">https://www.youtube.com/watch?v=YuEm03Ko0J8</a>.</p><p>\u201cTen Big Wins for Farm Animals in 2021 - EA Forum.\u201d 2021. 2021. <a href=\"https://forum.effectivealtruism.org/posts/2SnY4QTZXNibxEjxD/ten-big-wins-for-farm-animals-in-2021?fbclid=IwAR2WDrGqyivT-Onm_V2PhW7xpn-Iz2zQGPG6c_TUkGahKiP1vNqsELjiv44\">https://forum.effectivealtruism.org/posts/2SnY4QTZXNibxEjxD/ten-big-wins-for-farm-animals-in-2021?fbclid=IwAR2WDrGqyivT-Onm_V2PhW7xpn-Iz2zQGPG6c_TUkGahKiP1vNqsELjiv44</a>.</p><p>Tetlock, Philip E., and Dan Gardner. 2016. <i>Superforecasting: The Art and Science of Prediction</i>. Random House.</p><p>Tim, Whitnall, and Pitts Nathan. 2019. \u201cGlobal Trends in Meat Consumption.\u201d <i>Agricultural Commodities</i> 9 (1): 96\u201399.</p><p>Tomasik, Brian. 2007. \u201cHow Much Direct Suffering Is Caused by Various Animal Foods?\u201d Reducing Suffering. 2007. <a href=\"https://reducing-suffering.org/how-much-direct-suffering-is-caused-by-various-animal-foods/\">https://reducing-suffering.org/how-much-direct-suffering-is-caused-by-various-animal-foods/</a>.</p><p>Tonsor, Glynn T., Jayson L. Lusk, and Ted C. Schroeder. 2022. \u201cMarket Potential of New Plant\u2010based Protein Alternatives: Insights from Four US Consumer Experiments.\u201d <i>Applied Economic Perspectives and Policy</i>, March. https://doi.org/<a href=\"http://dx.doi.org/10.1002/aepp.13253\">10.1002/aepp.13253</a>.</p><p>Tonsor, Glynn T., and Nicole J. Olynk. 2011. \u201cImpacts of Animal Well-Being and Welfare Media on Meat Demand.\u201d <i>Journal of Agricultural Economics</i> 62 (1): 59\u201372.</p><p>Vegan Debate Archive. 2018. \u201cGary L. Francione &amp; Bruce Friedrich Debate (27/07/13).\u201d Youtube. April 24, 2018. <a href=\"https://www.youtube.com/watch?v=LOJWhs7fvnI\">https://www.youtube.com/watch?v=LOJWhs7fvnI</a>.</p><p>Vinding, Magnus. 2014. <i>Why Happy Meat Is Always Wrong</i>. Smashwords.</p><p>Vink, Janneke. 2020. <i>The Open Society and Its Animals</i>. Palgrave Macmillan, Cham.</p><p>Viva! 2021. \u201c9 in 10 Britons Want Intensive Farming Methods BANNED.\u201d Viva! The Vegan Charity. January 8, 2021. <a href=\"https://viva.org.uk/media-centre/new-poll/\">https://viva.org.uk/media-centre/new-poll/</a>.</p><p>\u201cVTT: Hiili ei olekaan pelkk\u00e4\u00e4 saastetta \u2013 hiilidioksidin uusiok\u00e4yt\u00f6ll\u00e4 valmistetuilla tuotteilla ilmastonmuutoksen kannalta suotuisia vaikutuksia.\u201d 2019. Yle Uutiset. August 28, 2019. <a href=\"https://yle.fi/uutiset/3-10941400\">https://yle.fi/uutiset/3-10941400</a>.</p><p>Watanabe, Zenjiro. 1854. \u201cRemoval of the Ban on Meat.\u201d March. <a href=\"https://www.kikkoman.co.jp/kiifc/foodculture/pdf_09/e_002_008.pdf\">https://www.kikkoman.co.jp/kiifc/foodculture/pdf_09/e_002_008.pdf</a>.</p><p>Wei, Feng, Chang Shen Qiu, Susan J. Kim, Lisa Muglia, James W. Maas, Victor V. Pineda, Hai Ming Xu, et al. 2002. \u201cGenetic Elimination of Behavioral Sensitization in Mice Lacking Calmodulin-Stimulated Adenylyl Cyclases.\u201d <i>Neuron</i> 36 (4): 713\u201326.</p><p>Wiblin, Robert, and Keiran Harris. 2018. \u201cBruce Friedrich Makes the Case That Inventing Outstanding Meat Replacements Is the Most Effective Way to Help Animals - 80,000 Hours.\u201d 80,000 Hours. February 19, 2018. <a href=\"https://80000hours.org/podcast/episodes/bruce-friedrich-good-food-institute/\">https://80000hours.org/podcast/episodes/bruce-friedrich-good-food-institute/</a>.</p><p>Wilkinson, Dominic. 2009. \u201cAnaesthe-SteakTM: Pain-Free Meat and the Welfare Paradox.\u201d 2009. <a href=\"http://blog.practicalethics.ox.ac.uk/2009/09/anaesthe-steak%E2%84%A2-pain-free-meat-and-the-welfare-paradox/\">http://blog.practicalethics.ox.ac.uk/2009/09/anaesthe-steak%E2%84%A2-pain-free-meat-and-the-welfare-paradox/</a>.</p><p>Wise, Steven M. 2005. <i>Though the Heavens May Fall: The Landmark Trial That Led to the End of Human Slavery</i>. Da Capo Press.</p><p>\u2014\u2014\u2014. 2014. <i>Rattling the Cage: Toward Legal Rights for Animals</i>. Hachette+ORM.</p><p>World Animal Protection. 2021. \u201cIntensive Livestock Farming Is Harming People, Animals and the Planet.\u201d World Animal Protection. 2021. <a href=\"https://www.worldanimalprotection.org/blogs/intensive-livestock-farming-harming-people-animals-and-planet\">https://www.worldanimalprotection.org/blogs/intensive-livestock-farming-harming-people-animals-and-planet</a>.</p><p>\u2014\u2014\u2014. n.d. \u201cAnimal Protection Index Map.\u201d World Animal Protection. Accessed May 4, 2022. <a href=\"https://api.worldanimalprotection.org/\">https://api.worldanimalprotection.org/</a>.</p><p>Wrenn, Corey Lee. 2012. \u201cAbolitionist Animal Rights: Critical Comparisons and Challenges Within the Animal Rights Movement,\u201d Animal Rights Movement Collection. <a href=\"https://www.wellbeingintlstudiesrepository.org/anirmov/2/\">https://www.wellbeingintlstudiesrepository.org/anirmov/2/</a>.</p>", "user": {"username": "Animal Ask"}}, {"_id": "tAsyRARbkMym5D4jK", "title": "Roodman's Thoughts on Biological Anchors", "postedAt": "2022-09-14T12:23:23.895Z", "htmlBody": "<p><a href=\"https://docs.google.com/document/d/1ccfObnsXdQzJZ8wWpHsosm7i4JdiPXXJrpAfZquvJCk/edit#\">A new review</a> of Ajeya Cotra's <a href=\"https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\">Forecasting TAI with biological anchors</a> (see also update <a href=\"https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines\">here</a>), written by David Roodman in April 2020, has been added to <a href=\"https://drive.google.com/drive/u/1/folders/1XkTYFiZQUT6UAUL2Wyg9wD0qG57KYfjq\">the folder</a> of public reviews for Cotra's report.</p>\n<p>Roodman's summary:</p>\n<blockquote>\n<p>I think my main critical reaction is about the draft report\u2019s ecumenical approach. It puts non-zero weight on several different frameworks which, conditional on the various parameter choices favored in the report, contradict one another. This mixing of distributions expresses a kind of radical uncertainty: not only are we unsure about the parameter values within each framework; we\u2019re also unsure about which framework is most right.</p>\n</blockquote>\n<blockquote>\n<p>This set-up is pragmatic and humble, but\u2026 I think in principle the ecumenism discards useful information, by not imposing the restriction that the various frameworks agree. In principle, they are all measuring the same thing. In pure Bayesian reasoning, if one has several uncertain measurements of the same value, each represented by a probability distribution, then one combines these primary measurements by multiplying them pointwise and rescaling the result to have total integral one. This contrasts with the pointwise averaging performed in the draft report, which is the mathematical expression of ecumenism.</p>\n</blockquote>\n<blockquote>\n<p>In Bayesian reasoning, if two distributions for the same parameter are normal, then their combination is too; its mean is the average of the two primary means, weighting by the respective precisions (inverse variances). Weirdly, if the two primary means are far apart, so that the two distributions hardly overlap, then their combination can pop up in the no-man\u2019s-land between them. The intuition is that the combined distribution centers on the least unlikely estimate given what we know.</p>\n</blockquote>\n<blockquote>\n<p>I make that mathematical point less to argue for a mechanical implementation of Bayesian mixing of different perspectives than to advocate for an informal didactic that aims at unification. What is the least implausible way to reconcile the large disagreements between different frameworks? Could answers to that question help us settle on a single, favored framework, perhaps one that synthesizes ideas from more than one?</p>\n</blockquote>\n<blockquote>\n<p>That impulse ultimately led me to favor a single framework that fuses elements from several in the draft report. The idea is to model two training levels at once, of parameters and hyperparameters. Training of parameters corresponds to the training of a single neural network, or the learning a sentient organism undergoes during maturation. Hyperparameter training corresponds to the design space exploration that AI researchers engage in and, in the biological realm, to evolution. Each parameter training run may involve huge numbers of small parameter updates; each in turn serves a single hyperparameter training step\u2026</p>\n</blockquote>\n", "user": {"username": "lukeprog"}}, {"_id": "hkqitRNyxfzWn29AX", "title": "Announcing an Empirical AI Safety Program", "postedAt": "2022-09-13T21:39:21.527Z", "htmlBody": "<p>Last summer, the&nbsp;<a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a> ran the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9RYvJu2iNJMXgWCBn/introducing-the-ml-safety-scholars-program\"><u>ML Safety Scholars program (MLSS)</u></a> to help students interested in AI Safety gain technical skills and learn about empirical safety topics. This fall, we are running an abridged version of this program called Introduction to ML Safety.</p><p><a href=\"https://airtable.com/shrvGXmaBPqIqyf1M\"><u>Apply to be a participant</u></a> by September 21st.<br><a href=\"https://airtable.com/shriEyCoiB9mVvb8w\"><u>Apply to be a facilitator</u></a> by September 18th.<br>Website (for advertising):&nbsp;<a href=\"https://www.mlsafety.org/intro-to-ml-safety\"><u>mlsafety.org/intro-to-ml-safety</u></a></p><h2>About the Course</h2><p>Introduction to ML Safety is an 8-week course that aims to introduce students with a deep learning background to empirical AI Safety research. The program is designed and taught by&nbsp;<a href=\"http://danhendrycks.com\"><u>Dan Hendrycks</u></a>, a UC Berkeley ML PhD and director of the&nbsp;<a href=\"http://safe.ai\"><u>Center for AI Safety</u></a>, and provides an introduction to robustness, alignment, monitoring, systemic safety, and conceptual foundations for existential risk.</p><p>Each week, students will be assigned readings, lecture videos, and required homework assignments. The materials are publicly available at&nbsp;<a href=\"https://course.mlsafety.org/\"><u>course.mlsafety.org</u></a>.</p><p>There are two tracks:</p><ul><li><strong>Introductory track:&nbsp;</strong>for people who are new to AI Safety. This track aims to familiarize students with the AI X-risk discussion alongside empirical research directions.</li><li><strong>Advanced track:&nbsp;</strong>for people who already have a conceptual understanding of AI X-risk and want to learn more about existing empirical safety research so they can start contributing.</li></ul><p>Introduction to ML Safety will be virtual by default, though students can participate in person if a section is facilitated at their local university.</p><h2>Time Commitment</h2><p>The program will last for 8 weeks, beginning on September 26th and ending on November 18th. Participants are expected to commit at least 5 hours per week. This includes ~1 hour of recorded lectures (which will take more than one hour to digest), ~1-2 hours of readings, ~1-2 hours of written assignments, and 1 hour of discussion.&nbsp;</p><p>We understand that 5 hours is a large time commitment, so to make our program more inclusive and remove any financial barriers,&nbsp;<strong>we will provide a $1000 stipend upon completion of the course</strong>. For logistical reasons,&nbsp;<strong>we can only pay participants who are eligible to work in the US</strong>. Non-US residents are welcome to apply and participate but we cannot offer them a stipend. We will make an effort to provide international students in the US with stipends but we cannot guarantee that we will be able to.</p><h2>Eligibility</h2><p>Anyone can apply as long as they have the following prerequisites:</p><ul><li>Deep learning: you can gauge the background knowledge required by skimming the week 1 slides:&nbsp;<a href=\"https://docs.google.com/presentation/d/15yMNlkWAL5cuSHHZe1gy2sM8zcN8gHk9iBVzKKvS9zw/edit#slide=id.g126975c12ec_1_54\"><u>deep learning review</u></a>.</li><li>Linear algebra or introductory statistics (e.g., AP Statistics)</li><li>Multivariate differential calculus</li></ul><p>If you are not sure whether you meet these criteria, err on the side of applying.</p><h2>Facilitating a section</h2><p>To be a facilitator, you must have a strong background in deep learning and AI Safety. For the latter, taking AGISF (AGI Safety Fundamentals) or participating in a similar program is sufficient. Much of the course content is not covered in AGISF, so if you are not familiar with it, you will have to learn it in advance each week.</p><p>The time commitment for running one section is ~2-4 hours per week depending on prior familiarity with the material. 1 hour of discussion and 1-3 hour of prep depending on your prior familiarity with the material. Discussion times are flexible.</p><p>We will pay facilitators a stipend corresponding to ~$30 per hour (subject to legal constraints).</p><p>You can apply to be a facilitator&nbsp;<a href=\"https://airtable.com/shriEyCoiB9mVvb8w\"><u>here</u></a> (applications are due by September 18th).</p><h2>Questions</h2><p>Questions about the program should be posted as comments on this post. For questions that are only relevant to you, please reach out to&nbsp;<a href=\"mailto:introcourse@mlsafety.org\"><u>introcourse@mlsafety.org</u></a>.</p>", "user": {"username": "Joshua Clymer"}}, {"_id": "eDoni7szhv5v7uwHp", "title": "EA architect: Building an EA hub in the Great Lakes region, Ohio", "postedAt": "2022-09-13T12:13:46.195Z", "htmlBody": "<p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995641/mirroredImages/eDoni7szhv5v7uwHp/nyptynnt4ejnuotn4ptj.jpg\"></p><p>EA is growing, and there is a great need for a variety of spaces for the community to live, work and socialize. Traditionally, EA communities formed around hubs like Boston, the Bay Area or London. But, I feel like recently, there has been a growing demand for diversification of EA hubs around the world. This article aims to introduce the Great Lakes region, Ohio as a potential place for a new EA hub. I will argue that a new hub could be set up in the green and walkable town of Sandusky, located right on Lake Erie, allowing EAs to benefit from the existing thriving community, facilities and infrastructure.</p><p>&nbsp;</p><p>My involvement in the project started a few weeks ago, when I spent a week in Sandusky, Ohio, visiting&nbsp;<a href=\"https://councilonstrategicrisks.org/about/leadership/andrew-weber/\"><u>Andy Weber</u></a> (a great 80k episode with Andy <a href=\"https://80000hours.org/podcast/episodes/andy-weber-rendering-bioweapons-obsolete/\">here</a>), a former Assistant Secretary of Defense for Nuclear, Chemical &amp; Biological Defense Programs, and his wife&nbsp;<a href=\"https://councilonstrategicrisks.org/about/leadership/christine-parthemore/\"><u>Christine Parthemore</u></a>, Chief Executive Officer of <a href=\"https://councilonstrategicrisks.org/\">Council on Strategic Risks</a>. I was on a mission: to explore Sandusky and help figure out whether it would be a suitable location for an EA hub. Those of you living a nomadic lifestyle, working remotely or seeking to improve your mental health, productivity and wellness, stay tuned: Sandusky might be a place for you!</p><h2>So why the Great Lakes region, and why Sandusky in particular?</h2><p>In Andy's words: 'Sandusky is the remote worker's paradise', and after visiting, I have to say I wouldn't disagree! The following paragraphs describe the main reasons why Sandusky is a great place for remote work or perhaps establishing an EA hub.&nbsp;</p><h2>In Sandusky, you get more for less.</h2><p>Sandusky is much cheaper than the usual places where EAs tend to gather. According to Redfin&nbsp;<a href=\"https://paperpile.com/c/WSuftM/TelJ\">(Redfin n.d.)</a>, the median price of Sandusky home prices for July 2022 was $200K, as compared to $825K for NYC and $1,460K for SF. In times of remote working, places like Sandusky might offer EAs a chance to work from one place, offering an alternative to living in the most expensive places in the world. Living costs as well as costs associated with running an EA hub in Sandusky are much more favorable and would buy EAs better housing and healthier work environments for far less money.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995641/mirroredImages/eDoni7szhv5v7uwHp/txvtqqlf3fxslzo4ukit.png\"><figcaption>Source: redfin.com</figcaption></figure><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995642/mirroredImages/eDoni7szhv5v7uwHp/ft3zvo5jjo4qk5u8h7s1.jpg\"></p><h2>Life at the water.</h2><p>Living near the water, similarly to living surrounded by greenery, seems to be associated with many positive measures of physical and mental wellbeing&nbsp;<a href=\"https://paperpile.com/c/WSuftM/d9tM\">(Hunt 2019)</a>.&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_210 210w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_1470 1470w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_1680 1680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_1890 1890w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/000596635823f33113ceea4b31fc32c4cf6f5e171a3a75bf.jpeg/w_2048 2048w\"></figure><p>Sandusky is located at Lake Erie, a freshwater reservoir offering life right at the waterfront.&nbsp;</p><p>&nbsp;</p><p>It's one of the cities of the&nbsp;<a href=\"https://greencitiesinitiative.com/\"><u>Green Cities Initiative</u></a>, an initiative of cities by the Great Lakes that strive to offer a place to do remote work in times of global warming and covid. The initiative wants to offer life at a freshwater resource (the Great Lakes provide 84% of North America\u2019s surface freshwater&nbsp;<a href=\"https://paperpile.com/c/WSuftM/BufCL\">(\u201cGreen Cities Initiative\u201d n.d.)</a>), with access to nature. I like the idea from an urban design perspective: it brings cities from the region together to work collaboratively on developing the region as a whole. For a smaller town like Sandusky, this means better connectivity to the communities and facilities surrounding it.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995641/mirroredImages/eDoni7szhv5v7uwHp/exqrmzfrlr8o2whkvg67.jpg\"></p><h2>Access to nature.</h2><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e0d4df39803cd72cd9d1161c9a8d0ce8f3e9999dcf2ab8b4.jpeg/w_1566 1566w\">People living near green spaces seem to be less likely to have anxiety and depression and are more likely to be physically active&nbsp;<a href=\"https://paperpile.com/c/WSuftM/7Iwk\">(Jennings, Johnson Gaither, and Gragg 2012)</a>. Many of us choose to live in cities despite the&nbsp;distance from nature because of all their benefits, like access to jobs, opportunities, and people. However, living in loud, densely populated and congested urban areas can get overwhelming and stressful and can negatively impact our mental health or sleep quality. Sandusky could offer an escape with more immediate access to nature. It is surrounded by parks, and the Magee Marsh (known as the \u2018warbler capital\u2019 of the world) is&nbsp;<a href=\"https://abcbirds.org/blog/eight-must-see-spring-birding-sites/\"><u>one of eight American Bird Conservatory must-see sites</u></a>.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995642/mirroredImages/eDoni7szhv5v7uwHp/sfpncyiygfmdqv4pfsxy.jpg\"></p><h2>Sandusky is well-connected and accessible.</h2><p>Sandusky is located less than an hour drive from Cleveland Airport, ninety minutes from Detroit airport, as well as accessible by car and AMTRAK rail. You can hop on a plane and get in and out easily, as opposed to relocating to work remotely from, for example, an inaccessible island. Sandusky center is walkable, with an expanding net of cycling routes leading to different parts of the town. Moreover, a new cycle and pedestrian path connecting Sandusky to the wider coastal region has recently been approved.&nbsp;</p><h2><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_210 210w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_1470 1470w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_1680 1680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_1890 1890w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e891a7f103c634300c59878ef8f1b6ed7f9053b8e1ae5662.jpeg/w_2048 2048w\">What about the climate?&nbsp;</h2><p>The average temperatures look pretty similar to Boston, meaning it gets pretty cold in winter and warm in summer. However, the lake brings a pleasant summer breeze into the city, helping to fight higher summer temperatures.&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995641/mirroredImages/eDoni7szhv5v7uwHp/q5xm23haldb7qx7yr0l8.png\"><figcaption>Source: google.com</figcaption></figure><h2>Sandusky has a surprisingly thriving and diverse community that can be attractive to EAs of all ages.&nbsp;</h2><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995642/mirroredImages/eDoni7szhv5v7uwHp/pxsdi6x9sadp3dv9rp9o.jpg\"></p><p>Frankly, I was afraid Sandusky would be too small to have an exciting community that EAs could slot into, and I was pleasantly surprised to learn the opposite was the case. There's that feeling of familiarity you only get in small towns. People know each other, and in a few days, I made friends in the gym and knew people frequenting most local coffee shops. Each evening, we would head out to one of Sandusky's breweries, bars, or restaurants, or would invite a couple of friends over to Andy's house or go and watch the sunset at the lake \ud83c\udf05. I was happy to meet so many entrepreneurial people, like for example&nbsp;<a href=\"https://www.crainscleveland.com/awards/justin-carson\"><u>Justin Carson</u></a>, a serial entrepreneur and the founder of the Cleveland-based brewery&nbsp;<a href=\"https://platformbeer.co/\"><u>Platform</u></a>, and his wife Rachel Kingsbury who has spent the last eight years running a farm-fresh produce local shop in Cleveland.&nbsp;<br>&nbsp;</p><p>Another couple investing significant resources, time and energy in the development of Sandusky are Rick and Meghan Hogrefe, a couple running all sorts of ventures in Sandusky, with the most exciting one being the&nbsp;<a href=\"https://www.h2property.com/marketplace-at-cooke\"><u>Marketplace Downtown</u></a> complete with a childrens science museum. You can come down to the Marketplace Downtown to throw axes, play pins, get fresh waffles for brunch or visit&nbsp;<a href=\"https://noble-crafts.com/\"><u>Noble Crafts</u></a> and enjoy one of the hundreds of craft beers they offer. If you are lucky, you might chat with Rick about TriLink BioTechnologies, a biotechnology company he co-founded.&nbsp;<br>&nbsp;</p><p>And then, of course, there are Andy and Christine! I would argue that anyone interested in biosecurity, nuclear risk and policy (if you haven't heard&nbsp;<a href=\"https://80000hours.org/podcast/episodes/andy-weber-rendering-bioweapons-obsolete/\"><u>Andy's episode on the 80k podcast</u></a>, I highly recommend it), craft beers (read Andy), or bird watching (read Christine) would be excited to hang out with them.</p><p>&nbsp;</p><p>Long story short, the tight and welcoming community of Sandusky made me feel at home from day one. To be fair, the community would probably be one of the main reasons I would totally come to work from Sandusky for a few months! I can imagine young single EAs, families with children, as well as more senior EAs having a great time.</p><h2>Gyms, caf\u00e9s, smoothie bars, pubs...the list goes on and on!</h2><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995641/mirroredImages/eDoni7szhv5v7uwHp/vl3fgzdya54ylgeg0mrk.jpg\"></p><p>Great communities need places to hang out. Sandusky is centered around a walkable heart with trees, parks and public art. This hasn\u2019t always been the case. As Eric Wobser, the city manager, told me, streets used to be far more car-oriented. Where today stand sidewalks, trees and bike lanes, there used to be wide roads, and instead of an expansive green park by the waterfront, there used to be a parking lot.&nbsp;<br>&nbsp;</p><p>For coffee lovers like me, there are numerous coffee shops with good vibes, decent coffee and reliable wifi. There are multiple gyms, jogging routes, and a smoothie bar for a post-workout boost.&nbsp;<br>&nbsp;</p><p>My favorite place to hang out was probably the&nbsp;<a href=\"https://www.paddleandclimb.com/paddle-bar/\"><u>Paddle Bar</u></a>, where you can always find a friend or two and enjoy craft beers on tap or their signature Tiki cocktails.&nbsp;<br>&nbsp;</p><p>In terms of food, I would say there's a wide enough range of places for everyone to choose from, including decent vegan options in restaurants, fast foods, bistros and outdoor food trucks.&nbsp;<a href=\"https://en.wikipedia.org/wiki/Cedar_Point\"><u>Cedar Point</u></a>, a giant amusement park located on the outskirts of Sandusky, helps bring more tourists downtown to support local businesses.&nbsp;<br>&nbsp;</p><p>An amenity of itself is the lake. Caf\u00e9s with a lakeside view suddenly become so much more enjoyable. People take boats to go on trips to nearby islands, kayak, or watch sunsets over the lake together.&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995641/mirroredImages/eDoni7szhv5v7uwHp/buzxzbpcfbop0qrq7arw.jpg\"></p><h2>How would an EA hub in Sandusky work?</h2><ul><li>We aim to create a EA hub initially centered around Andy and Christine\u2019s areas of expertise, i.e. nuclear and biological threats, and GCR policy more generally. We will publish a more formal announcement later.</li><li>We would like to run our first event, probably a conference on topics listed above, in the upcoming spring.&nbsp;</li><li>The conference could be followed by longer retreats, fellowships and offers of short to mid-length tenancies.&nbsp;</li><li>We are also looking at creating a coworking space for EAs and altruists from Sandusky to use for work and social purposes.&nbsp;</li><li>In the future, there might be group houses and flats creating 'EA neighborhoods' nested within the existing Sandusky community.&nbsp;</li><li>If all goes well, in the future, our efforts might expand to the broader Great Lakes region.</li></ul><h2>Help us make this happen!</h2><ul><li>Are you from Ohio and are you an EA? Please reach out, we would love to talk to you. If you are interested, join the&nbsp;<a href=\"https://www.facebook.com/groups/911719413584492\"><u>EA Ohio Facebook page</u></a> we just started to set up!</li><li>Do you run a successful EA hub or a fellowship? I would love to visit and experience it. I would, in return, be able to help you promote your space and would be keen to explore ways to collaborate.</li><li>Do you want to come and chill in Sandusky before we launch anything official? This could probably be arranged, get in touch!</li></ul><p>&nbsp;</p><p>Thanks to Andy and Christine for inviting us and offering me to get involved.</p><p>&nbsp;</p><p>Thanks to Justin and Rachel for planning a full week of adventures for me, and helping me to understand the community of Sandusky.&nbsp;<br>&nbsp;</p><p>I look forward to sharing updates on the Sandusky hub, the remote worker\u2019s paradise.&nbsp;</p><p>&nbsp;</p><p>Tereza<br>&nbsp;</p><p><strong>References:</strong></p><p><a href=\"http://paperpile.com/b/WSuftM/BufCL\">\u201cGreen Cities Initiative.\u201d n.d. Green Cities Initiative. Accessed September 2, 2022.&nbsp;</a><a href=\"https://greencitiesinitiative.com/\">https://greencitiesinitiative.com/</a><a href=\"http://paperpile.com/b/WSuftM/BufCL\">.</a></p><p><a href=\"http://paperpile.com/b/WSuftM/d9tM\">Hunt, Elle. 2019. \u201cBlue Spaces: Why Time Spent near Water Is the Secret of Happiness.\u201d&nbsp;<i>The Guardian</i>, November 3, 2019.&nbsp;</a><a href=\"https://amp.theguardian.com/lifeandstyle/2019/nov/03/blue-space-living-near-water-good-secret-of-happiness\">https://amp.theguardian.com/lifeandstyle/2019/nov/03/blue-space-living-near-water-good-secret-of-happiness</a><a href=\"http://paperpile.com/b/WSuftM/d9tM\">.</a></p><p><a href=\"http://paperpile.com/b/WSuftM/7Iwk\">Jennings, Viniece, Cassandra Johnson Gaither, and Richard Schulterbrandt Gragg. 2012. \u201cPromoting Environmental Justice Through Urban Green Space Access: A Synopsis.\u201d&nbsp;<i>Environmental Justice&nbsp;</i>&nbsp;5 (1): 1\u20137.</a></p><p><a href=\"http://paperpile.com/b/WSuftM/TelJ\">Redfin. n.d. \u201cSandusky Housing Market: House Prices &amp; Trends.\u201d Accessed September 3, 2022.&nbsp;</a><a href=\"https://www.redfin.com/city/17984/OH/Sandusky/housing-market\">https://www.redfin.com/city/17984/OH/Sandusky/housing-market</a><a href=\"http://paperpile.com/b/WSuftM/TelJ\">.</a></p>", "user": {"username": "Tereza Fl\u00eddrov\u00e1"}}, {"_id": "4f7tXpp8npR853g9d", "title": "Requesting feedback: proposal for a new nonprofit with potential to become highly effective", "postedAt": "2022-09-13T11:28:18.195Z", "htmlBody": "<h2><strong>TL;DR</strong></h2><p>I make the case for funding a new initiative that addresses an important, tractable, and neglected problem with&nbsp;<strong>potential to reach GiveWell-level cost-effectiveness</strong>. Please review this and share your thoughts!</p><ul><li>Millions of people die every year from preventable conditions, especially in low- and middle-income countries (LMICs).</li><li>Some of these deaths can be avoided by improving health workers\u2019 skills and knowledge.</li><li>Working with a nonprofit, I\u2019ve been piloting a way to&nbsp;<strong>train health workers online</strong> with a scrappy WordPress-based MVP, and still getting good results.&nbsp;</li><li>I\u2019m looking for funding to create and scale a solution that\u2019s 10X better than anything that exists.</li></ul><p><strong>Why I\u2019m sharing this</strong></p><p>I\u2019m pretty new to EA and I\u2019d like to submit a version of this to EA organizations for funding. I\u2019m seeking feedback to help make this as clear, logical, and succinct as possible.</p><h2><strong>Importance</strong>: what is the scale of problems that can be addressed with health worker training?</h2><p>Every year, there are&nbsp;<strong>tens of millions of deaths</strong> that can be averted with simple interventions, including:</p><ul><li><strong>2.9 million deaths of newborn babies&nbsp;</strong><a href=\"https://www.thelancet.com/series/everynewborn\"><u>readily preventable</u></a> with standard care,</li><li><strong>1.5</strong>&nbsp;<strong>million</strong>&nbsp;<strong>deaths</strong> caused by&nbsp;<a href=\"https://www.thelancet.com/pb-assets/Lancet/gbd/summaries/diseases/diarrhoeal-diseases.pdf\"><u>diarrheal diseases</u></a>; many of which can be averted with improved sanitation practices and low-cost treatment,&nbsp;</li><li><strong>2.5 million deaths&nbsp;</strong>caused by&nbsp;<a href=\"https://www.thelancet.com/pb-assets/Lancet/gbd/summaries/diseases/lower-respiratory-infections.pdf\"><u>lower respiratory infections</u></a>; some of which can be prevented with appropriate diagnosis and treatment, and</li><li><strong>10.8 million deaths</strong> caused by&nbsp;<a href=\"https://www.thelancet.com/pb-assets/Lancet/gbd/summaries/risks/high-systolic-blood-pressure.pdf\"><u>high blood pressure</u></a>, which is inexpensively controlled with generic medications.</li></ul><p><strong>Pandemics</strong> are an equally important global problem. COVID-19 has caused an&nbsp;<a href=\"https://covid19.healthdata.org/global?view=cumulative-deaths&amp;tab=trend\"><u>estimated</u></a>&nbsp;<strong>17.5 million</strong>&nbsp;<strong>deaths</strong>. Unfortunately, experts agree that COVID-19 isn\u2019t a worst-case scenario: future pandemics could pose an existential threat to humanity.</p><p>These problems and their causes have an uneven global distribution. Newborn deaths, lower respiratory infections, and diarrheal disease, in particular, are concentrated in LMICs. Moreover, for a variety of reasons, LMICs are&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/books/NBK525302/\"><u>particularly susceptible</u></a> to epidemics. Solutions to these problems must be implemented in LMICs.</p><h2><strong>Tractability:&nbsp;</strong>how can online health worker training solve these problems?</h2><p>There is ample evidence that health worker (HW) training can have benefits on its own or when combined with other interventions.&nbsp;</p><ul><li>My analysis of existing research studies shows that training HWs to properly care for newborn babies is likely to be&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/B7wohgDDdwPoQAatt/new-cause-area-training-health-workers-to-prevent-newborn\"><u>highly cost-effective</u></a>, with an average cost of&nbsp;<strong>$59 per DALY</strong> averted ($100 per DALY averted is&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/HbunzTyFPRwcYihg6/long-lasting-insecticide-treated-nets-usd3-340-per-life\"><u>sometimes cited</u></a> as a benchmark for highly effective interventions). Please see the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/B7wohgDDdwPoQAatt/new-cause-area-training-health-workers-to-prevent-newborn\"><u>full forum post</u></a> for the details of this analysis.</li><li>Management of childhood illness training, which covers simple interventions (such as appropriately diagnosing and treating lower respiratory infections and diarrhea), measurably&nbsp;<a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0066030\"><u>improves</u></a> HW clinical skills.</li><li>A recent study estimated that properly training HWs and giving them personal protective equipment&nbsp;<a href=\"https://cdn.who.int/media/docs/default-source/integrated-health-services-(ihs)/ipc/ipc-global-report/who_ipc_global-report_web.pdf?sfvrsn=d15fb868_4\"><u>would have saved</u></a>&nbsp;<strong>$7.2 billion&nbsp;</strong>in just the first six months of the COVID-19 pandemic.</li><li>The decisions that HWs make early in a disease outbreak can save countless lives. Notably, a doctor who&nbsp;<a href=\"https://www.cbc.ca/news/canada/newfoundland-labrador/apocalypse-then-ebola-1.5845560\"><u>acted quickly</u></a> to quarantine a sick traveler is credited with stopping the spread of Ebola into Nigeria in 2014. HWs need to be well trained to be ready to make these critical decisions at any time.</li></ul><p>There is also emerging evidence that online learning is an effective way to train HWs.</p><ul><li>Online neonatal care training&nbsp;<a href=\"https://human-resources-health.biomedcentral.com/articles/10.1186/s12960-021-00559-2\"><u>leads</u></a> to significant knowledge and skills gains among HWs.</li><li>The infection control training I\u2019ve been working on&nbsp;<a href=\"https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-022-12943-1\"><u>has</u></a> high completion rates, learning gains comparable to those seen in more resource-intensive in-person training, and very positive learner feedback.</li><li>Recent research&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/jch.14419\"><u>shows</u></a> that a short online training in blood pressure measurement improves HWs\u2019 clinical skills.</li><li>Modeling&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/B7wohgDDdwPoQAatt/new-cause-area-training-health-workers-to-prevent-newborn\"><u>indicates</u></a> that online HW training could be highly cost-effective. Even if online training is much less effective than in-person training on a per-HW basis, its potential for massive scale could still make it a \u201cbest buy.\u201d&nbsp;</li></ul><p>Finally, training can be used to address many different problems - it\u2019s a cross-cutting solution. For example, a single platform could provide high-quality training on management of childhood illness and newborn care, while also rapidly updating HWs with the latest guidance on emerging pathogens with pandemic potential.</p><h2><strong>Neglectedness:</strong> what gaps could this solution fill?</h2><p>Most aspects of health workforce development require greater attention and investment.&nbsp;</p><ul><li>As of 2013, there was a&nbsp;<a href=\"https://apps.who.int/iris/bitstream/handle/10665/250330/9789241511407-eng.pdf\"><u>global shortage</u></a> of 17.4 million HWs, with a shortfall of 4.2 million HWs in Africa alone.&nbsp;</li><li>A lack of essential skills and knowledge is a persistent but understudied problem in the health workforce. The available evidence paints a grim picture. For example:<ul><li>in a referral hospital in Nigeria,&nbsp;<a href=\"https://www.jscimedcentral.com/FamilyMedicine/familymedicine-5-1153.pdf\"><u>over 70% of HWs</u></a> have poor knowledge of blood pressure measurement,</li><li>HWs caring for sick children in four African countries had&nbsp;<a href=\"https://bmchealthservres.biomedcentral.com/articles/10.1186/s12913-017-2781-3\"><u>very low rates</u></a> of adherence to evidence-based protocols for diagnosing the most life-threatening conditions (though adherence was higher among those who were recently trained), and</li><li>HWs who take our infection control course enter with&nbsp;<a href=\"https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-022-12943-1\"><u>low baseline knowledge</u></a> in the subject.</li></ul></li><li>Gaps in skills and knowledge are associated with insufficient training. For example, a&nbsp;<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7884929/\"><u>recent study</u></a> found that only ~25% of low-income countries surveyed had ongoing infection control training for HWs who are in the workforce.</li><li>Global investment in HW training is&nbsp;<a href=\"https://fmch.bmj.com/content/10/2/e001449\"><u>fragmented</u></a>, which prevents the most promising interventions (such as&nbsp;<a href=\"https://www.ghspjournal.org/content/6/Supplement_1/S41\"><u>digital HW training</u></a>) from moving past pilot stages and realizing economies of scale.</li><li><a href=\"https://gh.bmj.com/content/bmjgh/5/2/e001981.full.pdf\"><u>A lack of high-quality research</u></a> on HW training, particularly in LMICs, prevents us from even developing a complete understanding of the problem.</li></ul><p>More generally, I\u2019ve searched extensively for high-quality online training targeted to HWs in LMICs. There are some&nbsp;<a href=\"https://digitalmedic.stanford.edu/\"><u>great</u></a>&nbsp;<a href=\"https://openwho.org/\"><u>resources</u></a>, but there is no \u201cDuolingo for health care,\u201d and I believe the world is ready for that. The time is right, given ongoing&nbsp;<a href=\"https://www.gsma.com/mobileeconomy/wp-content/uploads/2020/09/GSMA_MobileEconomy2020_SSA_Eng.pd\"><u>massive growth in mobile device ownership and internet access</u></a> in many of the countries with the highest burden of preventable disease.</p><h2>Progress to date</h2><p>Since April 2021, working with the nonprofit Resolve to Save Lives, I have been creating and delivering&nbsp;<strong>mobile-first, simple, engaging, and effective online HW training</strong>. We currently use a WordPress-based MVP to deliver the training.</p><ul><li>The learning process is inspired by Duolingo and based on&nbsp;<a href=\"https://enactacademy.org/2022/08/13/leveraging-learning-science-for-better-health-workforce-development/\"><u>best practices from education research</u></a>. HWs learn by responding to questions and receiving feedback in short modules that present realistic clinical cases.</li><li>We work closely with governments to offer the training in existing health facilities, so we don\u2019t incur high costs of setting up new hospitals and health centers.&nbsp;</li><li>We\u2019ve already had thousands of HWs from across Africa enroll in courses.</li><li>Results of an initial pilot of an infection control course were published in&nbsp;<a href=\"https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-022-12943-1\"><i><u>BMC Public Health</u></i></a>.</li><li>Completion rates of a&nbsp;<a href=\"https://preventepidemics.org/stories/health-care-worker-training-at-scale-reflections-from-covid-19/\"><u>follow-up pilot</u></a> reached&nbsp;<strong>65%</strong>, more than&nbsp;<i><strong>10 times</strong></i>&nbsp;<i><strong>higher</strong></i> than the average completion rates of free, optional, self-paced courses&nbsp;<a href=\"https://www.science.org/doi/10.1126/science.aag2063\"><u>taken in LMICs</u></a>.</li><li>There were substantial and significant knowledge gains among HWs who completed the infection control course.</li></ul><h2>Funding request and next steps</h2><p>[<i>Note</i>: this section has been updated based on feedback; we make a small-scale funding request for a pilot, rather than a larger request for full project implementation.]</p><p>We seek funding of&nbsp;<strong>$250,000 for 12 months</strong>. We\u2019ll use this catalytic funding to run a small-scale pilot, reaching &gt;1,000 HWs in 1-2 countries in Africa. In this pilot, we will assess the feasibility of the solution and measure short-term outcomes (learning gains, completion, and learner feedback). Our goal is to use a lean startup approach to learn as much as possible and accelerate toward a larger-scale, rigorous (and more costly) evaluation of the health impacts of the intervention.</p><p>We will start with courses in English that align with international (World Health Organization) guidance, and launch in anglophone Africa before expanding to additional regions and languages. For the foreseeable future, we will focus on HWs who work in primary care - including doctors, nurses, midwives, and community health workers - who form the backbone of the health system and most frequently diagnose and treat the preventable diseases discussed in this piece.</p><p>Contact me for more detailed information such as a budget and key milestones.&nbsp;</p><h2>Addressing skepticism and frequently asked questions</h2><p>This project is clearly not risk-free. HW training is not as well-studied as other interventions such as the GiveWell top charities. Here are some questions I\u2019ve fielded about this project, along with my answers.</p><p><i>There are a lot of other things that contribute to these preventable deaths, such as a lack of access to medications and equipment. Aren\u2019t those more important problems? How can we be sure that HW training is the right solution to invest in?</i></p><p>It\u2019s absolutely true that this problem has many root causes. I\u2019m not making the broad claim that training will put an end to readily-preventable deaths. Instead, I\u2019m making the more narrow claim that it is a cost-effective way to avert some of these deaths. Most health systems strengthening activities are quite expensive and don\u2019t scale particularly well; online HW training merits more attention because it looks like an exception to this rule.</p><p><i>Isn\u2019t it presumptuous to state that a lack of HW knowledge or skills leads to deaths? Don\u2019t we need to prove that before we invest a lot in training interventions?</i></p><p>As discussed above, the best available evidence on this question comes from training HWs in newborn care, where several different studies have shown that interventions cost-effectively save lives. These studies support the claim that inadequate training contributes to unnecessary deaths. If this project is funded, we would be able to begin to develop a better understanding of whether this is true of topic areas other than newborn care.</p><p><i>Shouldn\u2019t you just focus on training HWs in newborn care, since that\u2019s the topic with the best cost-effectiveness data?</i>&nbsp;</p><p>This sounds perfectly reasonable but I don\u2019t think it accounts for how educational technology products achieve widespread adoption in the real world. To me, this is a bit like suggesting that Netflix remove all of its other programming to focus on&nbsp;<i>Stranger Things.</i> The most widely used online learning platforms all offer a large amount of content (or let users contribute their own content). While newborn care training is the first course on our roadmap, I think it would be very difficult to maximize impact without a critical mass of content. Creating additional courses is not nearly as expensive as building a great learning platform and acquiring learners. If we can create a trusted, engaging learning resource with valuable content covering many different conditions, it will be much more valuable to the HWs themselves than a solution focused on just one cause of death.&nbsp;</p><p><i>Why not just choose existing training tools and interventions and point HWs to those?</i></p><p>Simply put, because most existing solutions are not designed for HWs in LMICs, and there isn\u2019t much evidence that they work for training this audience. For example, the World Health Organization puts out guidance on topics ranging from newborn care to infection control for COVID-19. This guidance is packaged in lengthy and dense PDFs. It\u2019s not realistic to expect frontline HWs to wade through all of this guidance to find the few paragraphs or pages in each document relevant to their work. Even though we\u2019ve run pilots on a shoestring so far, the user-centered approach we are taking is already yielding better results than those existing solutions.&nbsp;</p><p><i>I don\u2019t think online learning works. In fact, I think it\u2019s terrible. How can you justify more investment in it?</i></p><p>Some hesitation about online learning is certainly warranted because there\u2019s a lot of bad online training out there. If you are skeptical about online learning in general, I\u2019d recommend my previous writing that analyzes the&nbsp;<a href=\"https://enactacademy.org/2022/03/10/on-o-rings-and-online-learning/\"><u>challenges</u></a> we face and&nbsp;<a href=\"https://enactacademy.org/2022/08/13/leveraging-learning-science-for-better-health-workforce-development/\"><u>solutions</u></a> we can leverage to create effective online HW training. To summarize, solutions have to be carefully designed for efficacy and tailored to the learning needs of the target audience. It\u2019s not surprising that badly designed online learning doesn\u2019t deliver!</p><p><i>Shouldn\u2019t we invest in the pipeline of new HWs rather than training existing HWs?</i></p><p>Training new HWs is expensive, resource-intensive, and heavily regulated. I know this from my first-hand experience in helping to launch a medical school in Rwanda. Continuing education has a better landscape for innovation: the barriers to entry are lower, accreditation is simpler, and the feedback cycles are shorter. Although there\u2019s a great need in both areas, there\u2019s no reason that this innovation can\u2019t get started in the continuing education space and subsequently contribute to improving and simplifying the process of training new HWs.</p><p><i>How does your argument account for uncertainty?</i></p><p>For a more in-depth discussion of some limitations and cost-effectiveness modeling that accounts for different levels of efficacy of online training, I would refer readers to an&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/B7wohgDDdwPoQAatt/new-cause-area-training-health-workers-to-prevent-newborn#Promising_approaches_to_training_at_scale\"><u>earlier post</u></a> I wrote on this topic.&nbsp;</p><h2>Expressing gratitude</h2><p>I\u2019d like to thank Alex Chalk and an anonymous member of the EA community for providing great feedback on early drafts of this piece. I contacted Holly Kristensen after seeing&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/iL5GXC5k7HEpBztwz/offering-editing-feedback-on-written-pieces\"><u>this post</u></a> and also received great feedback from her. The ideas (and any errors) in this piece are mine and do not necessarily reflect the opinions of these generous reviewers. I\u2019m extremely grateful to the team at Resolve to Save Lives for supporting this work in a number of critical ways.</p>", "user": {"username": "mpt7"}}, {"_id": "T5p4vTDokEgv5DjAC", "title": "Podcast: Questions for John Halstead", "postedAt": "2022-09-13T11:07:24.333Z", "htmlBody": "<p><strong>Podcast: Questions for John Halstead</strong></p><p>I'm podcasting with John soon hoping to cover his climate work, long-termism and general worldview. If you haven't read it, his work on climate change &amp; longtermism is, IMO, very impressive: <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\">https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report</a></p><p><strong>What questions should I ask?</strong></p><p>The podcast is generalist as opposed to EA-specific but has covered some EA topics in depth (both arguments for and against, eg Leopold Aschenbrenner, Larry Temkin).</p><p>Let me know any good questions. Thanks!</p>", "user": {"username": "Ben Yeoh"}}, {"_id": "EEQuMjGYxxFKCtxjC", "title": "Where are the EA Influencers?", "postedAt": "2022-09-13T10:48:04.879Z", "htmlBody": "<p>I learned about the&nbsp;<a href=\"https://effectiveideas.org/\"><u>EA Blog Prize&nbsp;</u></a>earlier this year. If you write a blog that changes the paradigm of EA, you can win $100,000, no strings attached. I\u2019ve got some mixed feelings about it.&nbsp;</p><p>&nbsp;</p><p>On one hand, it\u2019s a great incentivizer for people to think deeply about important issues. Maybe the next great idea will come out of this competition. It\u2019s certainly possible.&nbsp;<br>&nbsp;</p><p>From a high level vantage point though, this strategy looks a lot like perfectionism. We\u2019re working so hard on improving our ideas, endlessly researching and debating, while, strangely, not doing much to tell the outside world about our organization.&nbsp;<br>&nbsp;</p><p>EA can be improved upon, sure. But we\u2019ve nailed the basics: We\u2019ve honed in on the most important issues to work on, we have systems dedicated to measuring the effectiveness of charities, and we\u2019ve secured enough funding to see thousands of projects to fruition.&nbsp;</p><p>&nbsp;</p><h2><strong>The biggest issue is that EA is not a household name.&nbsp;</strong><br>&nbsp;</h2><p>From my vantage point, the best thing we can do right now is invest in EA\u2019s branding.</p><p>&nbsp;</p><p>For example, what if the next big competition was to give $10,000 to whoever created the most viral EA-based Tiktok challenge?</p><p>&nbsp;</p><p>What if we paid Bangladeshi influencers to tell people about the&nbsp;<a href=\"https://news.stanford.edu/2019/09/24/lead-found-turmeric/#:~:text=A%20new%20Stanford%2Dled%20study,world's%20predominant%20turmeric%2Dgrowing%20regions.\"><u>lead found in turmeric&nbsp;</u></a>all throughout their country?</p><p>&nbsp;</p><p>I imagine a world where Kim Kardashian pledges 10% of her wealth to Giving What We Can, inspiring millions to do the same.&nbsp;<br>&nbsp;</p><h2><strong>Why is this not a good idea?</strong></h2><p>&nbsp;</p><p>I\u2019ve heard some legitimate criticism of this idea: If we make EA a household name, we\u2019ll dilute the seriousness of the effort. Tiktokers, generally, are not the people who are going to be researching AI risk, or going to Africa to hand out mosquito nets. Instead, we should let the interested parties find us.&nbsp;<br>&nbsp;</p><p>Furthermore, I definitely believe that expanding&nbsp;<strong>will&nbsp;</strong>have negative side effects. We\u2019ll have to work hard to stop our values from drifting, and there will certainly be growing pains that come with the transition from a small, reputation-based community to a large-scale organization. I\u2019m here to argue that it\u2019ll be worth it.&nbsp;</p><p>&nbsp;</p><h2><strong>Why it will be worth it</strong></h2><p>&nbsp;</p><p>In marketing, there\u2019s the concept of the funnel. For this example, let\u2019s pretend you own a gym. At the top of your funnel, you have 100 clients that dabble with your service. These people are using your free week-long trial. A little further down the funnel, we have 70 clients who decided to pay, but only for the cheapest version- the gym membership without the classes.</p><p>&nbsp;</p><p>As we go further and further down, we have fewer people who are paying more money for services. At the lowest level, we have 5 clients who are paying for your $10,000 package which involves on-on-one personal training, catered meals and blood work analysis.&nbsp;</p><p>&nbsp;</p><p>What all these clients had in common is that they all started at the top- they all began with the free trial.&nbsp;</p><p>&nbsp;</p><p>TikTok is like the free trial of EA. It involves no commitment at all, but a certain percentage of people who view these videos will go on to be involved in the movement.&nbsp;</p><p>&nbsp;</p><p>It seems that one of the biggest funding challenges is the fact that we don\u2019t have enough good projects to fund. Making EA popular will solve that problem.</p><p>But it\u2019s not just about attracting the super-intelligent, super-dedicated parts of the population. We don\u2019t just need AI programmers and heads of charity to fulfill EA\u2019s mission. We also need regular people who write emails to their local congress. Heck, we need the local congressperson\u2019s 14 year old daughter to ask them what they\u2019re doing about AI risk.&nbsp;</p><p>&nbsp;</p><p>It\u2019s time to get the message out there, and maybe create a new dance in the process.</p><p>&nbsp;</p><p><i>I have a podcast in which I interview all sorts of people, EA and non-EA alike! Here\u2019s&nbsp;</i><a href=\"https://www.youtube.com/watch?v=WC_rq8Kqzlc&amp;list=PLcwYsXrIOOzeUi9O5c_LXJ1MZupsNiBKZ&amp;index=11&amp;t=22s\"><i><u>an earlier podcast</u></i></a><i> in which I talk to Luke Freeman from Giving What We Can.&nbsp;</i></p><p><i>If you have an interesting project you\u2019re working on and would like to talk to me about it, please send me a message!&nbsp;</i></p><p><br>&nbsp;</p>", "user": {"username": "SereneDesiree"}}, {"_id": "FWcJzBdfNF3mCKP47", "title": "EA & LW Forums Weekly Summary (5 - 11 Sep 22\u2019)", "postedAt": "2022-09-12T23:21:59.293Z", "htmlBody": "<p><i>Supported by Rethink Priorities</i></p><p>This is part of a weekly series - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p>&nbsp;</p><h1>Top Readings / Curated</h1><p>A new section, designed for those without the time to read all the summaries. Everything here is also within the relevant sections later on so feel free to skip if you\u2019re planning to read it all.<br><br>Methodology - I\u2019ve pulled out a few posts I think offer some particularly new and interesting ideas, or would be useful to have more people giving feedback on. This is purely based on my personal opinion. Keen on any thoughts in the comments if this section is useful or not!</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/Q7gqF9ZCah2BEwZ9b/a-california-effect-for-artificial-intelligence\"><u>A California Effect for Artificial Intelligence</u></a></p><p><i>By henryj</i></p><p>The California Effect occurs when companies adhere to California regulations even outside California\u2019s borders. The author evaluates three types of AI regulation California could adopt, and finds two 80% likely to produce a california effect - regulating training data through data privacy, and risk-based regulation like that proposed by the EU. A full research paper is linked, the results of a summer research fellowship.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/D6oKe6kjHeBeiCtWn/developing-next-gen-ppe-to-reduce-biorisks\"><u>Developing Next Gen PPE to Reduce Biorisks</u></a></p><p><i>by AndyGraham, tsmilton</i></p><p>The authors are engineers planning to apply to LTTF for funding to run a feasibility study on improved PPE, particularly high protection versions (ie. suits). The current versions are bulky, expensive, hard to use, and have changed little since 1979. Feasibility will be primarily research to validate the need and potential impact on GCBRs. If that resolves favorably, they hope to finalize a product within 2-5 years.</p><p>They\u2019re looking for advice and thoughts - particularly from those with bioexpertise or PPE expertise, or who can help with the grant application.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/XBHx9zhAtkiBJnZNu/cause-exploration-prizes-announcing-our-prizes-1\"><u>Cause Exploration Prizes: Announcing our prizes</u></a></p><p>Over 150 submissions. Top prize to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LSDZ22GFryC3dhWvd/cause-exploration-prize-organophosphate-pesticides-and-other\"><u>Organophosphate pesticides and other neurotoxicants</u></a> by Ben Stewart. Second prize to each of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/majcwf7i8pW8eMJ3v/new-cause-area-violence-against-women-and-girls\"><u>Violence against women and girls</u></a> by Akhil,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/rvACMsMmmDonNhskD/cause-exploration-prizes-sickle-cell-disease\"><u>Sickle Cell Disease</u></a> (anonymous) and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fqf4vgCWebTszvHm9/shareholder-activism\"><u>Shareholder activism</u></a> by sbehmer. 20 honourable mentions. Post on learnings may come in the future.</p><p>See section \u2018Bonus: winners of Open Philanthropy\u2019s cause exploration prizes\u2019 section below for more detailed summaries.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/6wwK6Qduxt7mMmj8k/announcing-the-change-our-mind-contest-for-critiques-of-our\"><u>Announcing the Change Our Mind Contest for critiques of our cost-effectiveness analyses</u></a></p><p><i>By Givewell</i></p><p>Givewell is looking for critiques on their cost effectiveness analyses. Entries due 31st October, prizes up to 20K + a chance to improve allocation of millions of dollars.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment\"><u>Monitoring for deceptive alignment</u></a></p><p><i>by evhub</i></p><p>Requests DeepMind, OpenAI, and Anthropic to actively monitor and run experiments on narrow deceptive alignment (ie. where a model looks aligned only because it\u2019s trying to, for some ulterior motive). Early examples may be relatively easy to detect (eg. because early AIs are bad at it, or defect quickly) and therefore study. This could include monitoring for pre-cursors like when an AI first develops an instrumental goal not to have itself shut down.</p><p>Concrete things to test models on include if behavior changes with / without oversight, or catching deception at source via interpretability / transparency tools.</p><p>&nbsp;</p><p>From twitter: Trials show malaria vaccine gives 80% protection (3 doses + yearly booster). It\u2019s cheap, they have a deal to manufacture 100 million doses a year, and hope to roll it out next year.<a href=\"https://twitter.com/S_OhEigeartaigh/status/1567970202771685378?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u> (link)</u></a></p><h1><br>EA Forum</h1><h2>Philosophy and Methodologies</h2><p><a href=\"https://forum.effectivealtruism.org/posts/S9JeqH4qYvoLZqq9c/an-entire-category-of-risks-is-undervalued-by-ea-summary-of\"><u>An entire category of risks is undervalued by EA [Summary of previous forum post]</u></a></p><p><i>By Richard Ren</i></p><p>The cascading sociopolitical &amp; economic effects of climate change, pandemics, and conflicts are undervalued in the longtermist community. They aren\u2019t likely to be direct existential risks, but the instability they cause greatly affects a) the environment that AGI develops it\u2019s values in (instability often leads to authoritarian, violent social values) and b) what types and speed of technologies are developed (eg. more military AI focus due to conflict).</p><p>Improving institutional resilience (food, water, energy, infrastructure) is one way to reduce these risks. Climate adaptation and drought monitoring interventions are particularly neglected in this space. This forms the link between global health and poverty efforts and longtermism, which has been missing due to a lack of systems thinking.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/KiafCpixvtg5BWfyZ/the-base-rate-of-longtermism-is-bad\"><u>The Base Rate of Longtermism Is Bad</u></a></p><p><i>By ColdButtonIssues</i></p><p>Longtermism isn\u2019t unique to EA, and historic examples of people trying to improve the long term future haven\u2019t been effective. Eg. Where funds are set aside, morals and ownership can change so they\u2019re not effectively distributed. The communist revolution failed and incurred a high cost. Religion was most successful at affecting the future, but not comparable if we don\u2019t want EA to be a religion.<br><br>Note specific cause areas often classified as longtermist like biorisk, AI or nuclear risk are still valid, as they are important even just looking at the next few generations.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/zLZMsthcqfmv5J6Ev/the-discount-rate-is-not-zero\"><u>The discount rate is not zero</u></a></p><p><i>By Thomaaas</i></p><p>We should discount future lives not because they are worth less, but because there\u2019s a chance they won\u2019t exist. The discount rate equals the catastrophe rate.</p><p>A top comment by Carl Shuman argues that the catastrophe rate (and therefore discount rate) approaches zero if humanity survives the next few centuries, because of tech including aligned AI and space travel taking us to a robust safe state.</p><p>&nbsp;</p><h2>Object Level Interventions / Reviews</h2><p><a href=\"https://forum.effectivealtruism.org/posts/Q7gqF9ZCah2BEwZ9b/a-california-effect-for-artificial-intelligence\"><u>A California Effect for Artificial Intelligence</u></a></p><p><i>By henryj</i></p><p>The California Effect occurs when companies adhere to California regulations even outside California\u2019s borders. The author evaluates three types of AI regulation California could adopt, and finds two 80% likely to produce a california effect - regulating training data through data privacy, and risk-based regulation like that proposed by the EU. A full research paper is linked, the results of a summer research fellowship.<br><br><br><a href=\"https://forum.effectivealtruism.org/posts/EG9xDM8YRz4JN4wMN/samotsvety-s-ai-risk-forecasts\"><u>Samotsvety's AI risk forecasts</u></a></p><p>Samotsvety is a group of forecasters with strong track records, including several EAs. Their aggregate forecasts are below, in form [average (avg. excl those who were EAs before joining the group) (range lowest to highest)]:</p><p>Misaligned AI takeover by 2100, barring pre-APS-AI catastrophe? 25% (14%) (3-91.5%)</p><p>Transformative AI by 2100, barring pre-TAI catastrophe? 81% (86%) (45-99.5%)</p><p>Probability of existential catastrophe from AI, if AGI developed by 2070? 38% (23%) (4-98%)</p><p>AGI developed in the next 20 years? 32% (26%) (10-70%)</p><p>AGI developed by 2100? 73% (77%) (45-80%)</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/BJtekdKrAufyKhBGw/ai-governance-needs-technical-work\"><u>AI Governance Needs Technical Work</u></a></p><p><i>By Mauricio</i></p><p>Technical work within AI governance boosts our ability to implement governance interventions. Examples include engineering technical levers to make AI regulations enforceable, information security, forecasting, developing technical standards, or grantmaking / advising / managing any of the above categories. There aren\u2019t streamlined career pipelines for these yet, so we need people to learn more, build the expertise, and trailblaze them.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/D6oKe6kjHeBeiCtWn/developing-next-gen-ppe-to-reduce-biorisks\"><u>Developing Next Gen PPE to Reduce Biorisks</u></a></p><p><i>by AndyGraham, tsmilton</i></p><p>The authors are engineers planning to apply to LTTF for funding to run a feasibility study on improved PPE, particularly high protection versions (ie. suits). The current versions are bulky, expensive, hard to use, and have changed little since 1979. Feasibility will be primarily research to validate the need and potential impact on GCBRs. If that resolves favorably, they hope to finalize a product within 2-5 years.</p><p>They\u2019re looking for advice and thoughts - particularly from those with bioexpertise or PPE expertise, or who can help with the grant application.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/hprGdAiW73EsPfWzu/zzapp-malaria-more-effective-than-bed-nets-wanted-cto-coo\"><u>Zzapp Malaria: More effective than bed nets? (Wanted: CTO, COO &amp; Funding)</u></a></p><p><i>by Yonatan Cale</i></p><p>Zzapp sprays standing water with larvicides to prevent mosquitoes breeding. Uses satellite imaging and an app to target hotspots. Estimates itself as 2x more effective than bednets in urban / semi-urban areas, based on an initial experiment. A CTO and COO could help improve cost efficiencies substantially, and they\u2019re also looking for funding for another RCT.<br><br>&nbsp;</p><h2>Opportunities</h2><p>New section. Competitions, jobs, scholarships, volunteer opportunities etc.</p><p><a href=\"https://forum.effectivealtruism.org/posts/XBHx9zhAtkiBJnZNu/cause-exploration-prizes-announcing-our-prizes-1\"><u>Cause Exploration Prizes: Announcing our prizes</u></a></p><p>Over 150 submissions. Top prize to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LSDZ22GFryC3dhWvd/cause-exploration-prize-organophosphate-pesticides-and-other\"><u>Organophosphate pesticides and other neurotoxicants</u></a> by Ben Stewart. Second prize to each of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/majcwf7i8pW8eMJ3v/new-cause-area-violence-against-women-and-girls\"><u>Violence against women and girls</u></a> by Akhil,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/rvACMsMmmDonNhskD/cause-exploration-prizes-sickle-cell-disease\"><u>Sickle Cell Disease</u></a> (anonymous) and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fqf4vgCWebTszvHm9/shareholder-activism\"><u>Shareholder activism</u></a> by sbehmer. 20 honourable mentions. Post on learnings may come in the future.</p><p>See section \u2018Bonus: winners of Open Philantrophy\u2019s cause exploration prizes\u2019 section below for more detailed summaries.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/6wwK6Qduxt7mMmj8k/announcing-the-change-our-mind-contest-for-critiques-of-our\"><u>Announcing the Change Our Mind Contest for critiques of our cost-effectiveness analyses</u></a></p><p><i>By Givewell</i></p><p>Givewell is looking for critiques on their cost effectiveness analyses. Entries due 31st October, prizes up to 20K + a chance to improve allocation of millions of dollars.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/6aaQuenrF9BYvsZwv/announcing-a-philosophy-fellowship-for-ai-safety\"><u>Announcing a Philosophy Fellowship for AI Safety</u></a></p><p><i>By Anders_E, Oliver Zhang, Dan Hendrycks</i></p><p>For philosophy PhD students and postdoctorates to work on conceptual problems in AI safety. Paid (60K, student fees, housing stipend, relocation costs), San Francisco based opportunity running from January to August 2023. Applications close September 30th.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/bsqtndpfKbcbQRFEs/fundraising-campaigns-at-your-organization-a-reliable-path\"><u>Fundraising Campaigns at Your Organization: A Reliable Path to Counterfactual Impact</u></a></p><p><i>By High Impact Professionals</i></p><p>HIP supported EAs to run 8 charity fundraising campaigns at their companies in 2021, with a median and mean result of 3.9K and 30K USD raised respectively. Campaigns take ~25 hours per run, for a mean hourly return of $786 USD. There are also benefits in introducing EA ideas. If you\u2019d like to run a campaign at your company, HIP has a&nbsp;<a href=\"https://bit.ly/3KRMj29\"><u>step-by-step guide</u></a> and offers&nbsp;<a href=\"https://bit.ly/3TOQ7Fn\"><u>1-1 support</u></a>.<br><br>&nbsp;</p><h2>Bonus: Winner\u2019s of Open Philanthropy's Cause Exploration Prizes</h2><p>(These weren\u2019t posted this week, but are summarized here for easy reference since the winners were announced this week. This covers first and joint second place prizes.)<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/LSDZ22GFryC3dhWvd/cause-exploration-prize-organophosphate-pesticides-and-other\"><u>Cause exploration prize: organophosphate pesticides and other neurotoxicants</u></a></p><p><i>by Ben Stewart</i></p><p>Developmental neurotoxicants (DNTs) are chemicals like lead that adversely affect human development. Identifying and banning or decreasing exposure to these can increase IQ and therefore income to an estimated value of trillions globally.</p><p>Organophosphate pesticides are an example which has been banned in some countries since 2001, but is still prevalent in others. Meta-analyses suggest exposure is likely causing IQ deficits of up to a few percent in children, which the author estimates makes $450K funding here up to 90x as effective as GiveDirectly in some countries.</p><p>They also advocate for identifying new DNTs. It took decades to recognise and restrict currently known synthetic DNTs, synthesis of new chemicals is increasing year on year, and the only comprehensive testing currently covers just 20% of chemicals with production &gt;1 ton / year in Europe. The author estimates identification efforts as 40x GiveDirectly effectiveness.<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/majcwf7i8pW8eMJ3v/new-cause-area-violence-against-women-and-girls\"><u>New cause area: Violence against women and girls</u></a></p><p><i>by Akhil</i></p><p>~1/3rd of women have experienced either sexual or intimate partner violence, and rates are slowly increasing. In addition to social harms, the UN approximates costs as 1.5 trillion per year due to lost economic productivity and increased utilization of public services (eg. health, criminal).</p><p>In terms of interventions, preventative measures targeting gender norms or relationships seem most effective, with some RCTs reporting $52-184 USD per DALY averted. However a majority of funding currently goes to services to help after violence has occured. There is room for more funding in scaling up effective prevention programs, running more RCTs, and policy advocacy.<br><br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/rvACMsMmmDonNhskD/cause-exploration-prizes-sickle-cell-disease\"><u>[Cause Exploration Prizes] Sickle Cell Disease</u></a></p><p><i>by Open Philanthropy (ie. anonymous entry)</i></p><p>Sickle cell is a genetic disease that kills 100 - 200K infants per year in sub-Saharan Africa, with mortality of 50-90%. While there is no cure, with proper identification treatment can significantly reduce mortality. The area is under-funded with ~$20M committed annually.</p><p>A study indicated that the average cost per DALY averted by infant screening and treatment in Sub-Saharan Africa was $184 USD. However, since then testing costs have dropped from $9.90 to $2 per test (and further discounts may be available at scale). Note this may not account for increased costs on the healthcare system from adults with sickle cell disease.</p><p>The author suggests launching screening and treatment programs in countries with the highest incidence.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/fqf4vgCWebTszvHm9/shareholder-activism\"><u>Shareholder activism</u></a></p><p><i>by sbehmer</i></p><p>EA funders may invest in stocks while waiting to disperse funds, which provides an opportunity for shareholder activism. We could also create new funds specifically for this - investing in smaller but influential companies for the greatest influence.<br><br>Shareholders can make requests of companies, which if refused go to a costly (~$1-4M on both sides) ballot of all shareholders, and can threaten board members with replacement. Many boards acquiesce to avoid the ballot process and risk to their jobs.<br><br>A case study of a campaign using this methodology in climate change found a cost of $0.2-$0.6/ton CO2, which is competitive with Founders Pledge top charities. Other cause areas could also be targeted, and there\u2019s no minimum percent of shares to make a request.</p><p><br>&nbsp;</p><h2>Community &amp; Media</h2><p><a href=\"https://forum.effectivealtruism.org/posts/nhbeKbwMgFKfrzLNb/marketing-messages-trial-for-gwwc-giving-guide-campaign#Research_questions_\"><u>Marketing Messages Trial for GWWC Giving Guide Campaign</u></a></p><p><i>by Erin Morrissey, david_reinstein, GraceAdams, Luke Freeman, Giving What We Can</i></p><p>GWWC &amp; EAMT (EA Market Testing Team) ran a Facebook campaign in Nov - Jan to encourage people to read GWWC\u2019s effective giving guide. They tested 7 messages, 6 videos, and different audience segments. Headline results include:</p><ul><li>\u201cOnly 3% of donors give based on charity effectiveness, yet the best charities can be 100x more impactful\u201d was the most effective message tested overall.</li><li>Short videos did better, as did animal videos targeted at animal-interested audiences. But climate and global poverty audiences performed worse than a general \u2018philanthropy\u2019 audience when targeted with videos on their cause areas.</li><li>Overall, the \u2018lookalike\u2019 audience (made to resemble people who had already interacted with GWWC) performed the best.</li><li>Effectiveness of videos and messages varied quite a bit by audience segment.</li></ul><p>More trials are upcoming, and the authors are looking for both feedback and collaborators ahead of this.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/iMYbtfTizhC5C4mBo/link-post-optimistic-longtermism-is-terrible-for-animals\"><u>[Link post] Optimistic \u201cLongtermism\u201d Is Terrible For Animals</u></a></p><p><i>by BrianK</i></p><p>Links article by Forbes, which argues that longtermism is bad for animals because if we keep growing, so might factory farming and / or wild animal suffering (eg. if we spread Earth\u2019s animals across the universe). \u201cIf the human race creates more suffering than it alleviates, it would be a mistake to let it grow infinitely.\u201d</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/2JQQZevGENbChSA8k/agree-disagree-voting-and-other-new-features-september-2022\"><u>Agree/disagree voting (&amp; other new features September 2022)</u></a></p><p><i>By Lizka</i></p><p>Heaps of new features. Agree/disagree voting, curated posts starred on frontpage, copy-paste from google docs with footnotes, 1-1 service to connect people interested in working in a field with experts (starting with biosec) is live, cross-posting from LW is easier, and you can add topics to your profile to subscribe + share your interests.&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/9uPMWPZg8mqmcAhhL/selfish-reasons-to-move-to-dc\"><u>Selfish Reasons to Move to DC</u></a></p><p><i>By Anonymous_EA</i></p><p>Mainly, the EA community there rocks - warm, welcoming, easy to network. Non-EAs also tend to be impact-driven and ambitious, and there\u2019s a good dating market. The city is nice (beautiful, museums, medium size, good veg*n food). Though housing is expensive and summer is humid.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/bnzwL6tu4pdYf3hpZ/say-nay-to-the-bay-as-the-default\"><u>Say \u201cnay!\u201d to the Bay (as the default)!</u></a></p><p><i>By Kaleem</i></p><p>Author\u2019s tl;dr (slightly edited): The Bay Area isn\u2019t a great place to center the EA community in the US. The East coast is better because of the number of top universities, its proximity and accessibility to other EA-dense spots, its importance with respect to biosecurity and US policy, and cheaper flights and cost of living.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/A2YwuXe3Eo5kMZhZo/13-background-claims-about-ea\"><u>13 background claims about EA</u></a></p><p><i>By Akash</i></p><p>Summarized list of impressions / background info about EA you only get living in the Berkeley AI Safety hub. Themes include that AI Safety is a primary concern and some influential EA leaders have short timelines and &gt;10% extinction risk in the century. There\u2019s widespread disagreement on how to tackle it and a lack of seniors / mentors to help, but plenty of programs and grants you can apply to to get started. We also lack people working on it, so please apply for programs, jobs, funding, or start your own project.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/dfcvc9HSNLmq5cKtf/who-are-some-less-known-people-like-petrov\"><u>Who are some less-known people like Petrov?</u></a></p><p><i>By Lizka</i></p><p>Petrov was under pressure to make a decision with hugely negative outcomes for the world, and didn\u2019t. Examples of similar from the comments include:</p><ul><li>Military examples eg. Mike Jackson refused to capture a Russian-held airport in the 1990s, which could have sparked NATO &lt;-&gt; Russian conflict.</li><li>Political leader examples eg. King of Spain Juan Carlos De Borbon claimed support for Spain\u2019s autocracy in order to be named successor, flipped after that to lead them to democracy, and then abdicated the throne to end the monarchy.</li><li>Civil examples eg. Li Wenliang spoke out about covid early on, despite governmental pressure not to.</li></ul><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/KkPo9cpLThWyd7yju/save-the-date-eagx-latam\"><u>Save the Date: EAGx LatAm</u></a></p><p><i>by LGlez</i></p><p>Mexico City, 6-8th Jan, aimed at EAs of any experience in LatAm or experienced EAs from elsewhere. Most talks are in English. Applications open 30th Oct.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/aZhQhsHX4PyxCCpkz/the-maximum-impact-fund-is-now-the-top-charities-fund-1\"><u>The Maximum Impact Fund is now the Top Charities Fund</u></a></p><p><i>by Givewell</i></p><p>Renamed to better distinguish between this fund (which supports their top charities, which have a requirement for high confidence in expected impact) and their All Grants Fund (which is allocated based on cost-effectiveness, including riskier grants with high expected value).</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/Bd4xeHeNgBywrofW6/psychology-of-effective-altruism-course-syllabus-1\"><u>'Psychology of Effective Altruism' course syllabus</u></a></p><p><i>by Geoffrey Miller</i></p><p>Syllabus from a course the author has taught 3x at University of New Mexico, advanced undergrad level. Currently updating it and welcomes suggestions.</p><p>Pablo also comments with an aggregated and regularly updated&nbsp;<a href=\"https://www.stafforini.com/blog/effective-altruism-syllabi/\"><u>list of EA syllabi</u></a>.</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/g8aBf2oLwDvgd4ovf/much-ea-value-comes-from-being-a-schelling-point\"><u>Much EA value comes from being a Schelling point</u></a></p><p><i>by LRudL</i></p><p>A critical part of EA is being a place for talented, ambitious, altruistic people to meet each other. Making this part more effective involves:</p><ol><li><strong>Get more people</strong>: eg. increase reputation via obviously impressive projects, have more widely famous EA-linked organizations, paths to EA from adjacent areas, and reduce barriers to entry such as odd group norms or needing technical or philosophical background (while still keeping other important requirements like altruism).</li><li><strong>Help them connect</strong>: scale up matchmaking (particularly for entrepreneurs), and make use of physical hubs.</li></ol><p>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://forum.effectivealtruism.org/posts/dsB4dN8jvZcxhfnzC/igor-kiriluk-1974-2022\"><u>Igor Kiriluk (1974-2022)</u></a>&nbsp;<i>By turchin</i></p><p><a href=\"https://forum.effectivealtruism.org/posts/wSrutNhrjMjGWWjNa/do-ai-companies-make-their-safety-researchers-sign-a-non\"><u>Do AI companies make their safety researchers sign a non-disparagement clause?</u></a>&nbsp;<i>By ofer</i> (no answer yet in comments)</p><p><br>&nbsp;</p><h1>LW Forum</h1><h2>AI Impacts / New Capabilities</h2><p><a href=\"https://www.lesswrong.com/posts/f2C4CWNmrSKMs6SaK/linkpost-github-copilot-productivity-experiment\"><u>Linkpost: Github Copilot productivity experiment</u></a></p><p><i>by Daniel Kokotajlo</i></p><p>An experiment found developers completed the task of writing a HTTP server in Javascript ~55% faster with Github Copilot than without (saving ~1.5 hours). However the author notes it is a simple well-known task, given that and publication bias we shouldn\u2019t weigh this too strongly.</p><p>&nbsp;</p><h2>AI Meta &amp; Methodologies</h2><p><a href=\"https://www.lesswrong.com/posts/Afdohjyt6gESu4ANf/most-people-start-with-the-same-few-bad-ideas\"><u>Most People Start With The Same Few Bad Ideas</u></a></p><p><i>By johnswentworth</i></p><p>Most newcomers to alignment start with the same ideas, and take ~5 years to start plausibly useful research. The most common ideas (~75%) are variants of \u2018train an AI to help with training AI\u2019. Looking for problems with your plan, having a peer group to poke holes in each others\u2019 plans, and exposure to a variety of alignment models are helpful for speeding that up. The author estimates the MATS summer program helped attendees skip forward a median 3 years.</p><p>Top comments question if this might cause \u2018following the herd\u2019 and make newcomers less likely to contribute original work or question field assumptions. The author responds that the focus should be on peer-to-peer critique and skills for analyzing flaws in ideas vs. specific critiques of newcomers\u2019 ideas by experts, to avoid this risk.<br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment\"><u>Monitoring for deceptive alignment</u></a></p><p><i>by evhub</i></p><p>Requests DeepMind, OpenAI, and Anthropic to actively monitor and run experiments on narrow deceptive alignment (ie. where a model looks aligned only because it\u2019s trying to, for some ulterior motive). Early examples may be relatively easy to detect (eg. because early AIs are bad at it, or defect quickly) and therefore study. This could include monitoring for pre-cursors like when an AI first develops an instrumental goal not to have itself shut down.</p><p>Concrete things to test models on include if behavior changes with / without oversight, or catching deception at source via interpretability / transparency tools.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/7cHgjJR2H5e4w4rxT/alignment-papers-roundup-week-1\"><u>Alignment papers roundup - week 1</u></a></p><p><i>by Quintin Pope</i></p><p>New weekly series papers that seem relevant to alignment, focusing on papers or approaches that might be new to safety researchers. Links, abstracts, and the author\u2019s opinions are shared.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values\"><u>The shard theory of human values</u></a></p><p><i>By Quintin Pope, TurnTrout</i></p><p>Author\u2019s tl;dr: \u201cWe propose a theory of human value formation. According to this theory, the reward system shapes human values in a relatively straightforward manner. Human values are not e.g. an incredibly complicated, genetically hard-coded set of drives, but rather sets of contextually activated heuristics which were shaped by and bootstrapped from crude, genetically hard-coded reward circuitry.\u201d</p><p>They test this theory against cognitive biases like scope insensitivity and sunk cost fallacy, and find it explains these more simply than most existing explanations.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/ZKNxbcACeHKEfgKvW/private-alignment-research-sharing-and-coordination\"><u>Private alignment research sharing and coordination</u></a></p><p><i>By porby</i></p><p>Coordination on AI alignment research is hard. A lot is shared informally, but some researchers don\u2019t have that network, and don\u2019t want to share publicly because of info hazards or because it includes confidential information to their organization.</p><p>The post suggests a solution could be a database and forum anyone can submit to, but with highly restricted read permissions. Those with the highest access would need to be responsible to monitor and guide the use of the most dangerous or confidential information.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/HXxHcRCxR4oHrAsEr/an-update-on-academia-vs-industry-one-year-into-my-faculty\"><u>An Update on Academia vs. Industry (one year into my faculty job)</u></a></p><p><i>By David Scott Krueger (formerly: capybaralet)</i></p><p>Notes on conversations of industry vs. academic for AI safety, and experiences from ~1yr as an assistant professor. Themes include that academica is often dismissed as a pathway but has benefits (rapid training &amp; credentialing, it\u2019s becoming easier to work on safety and access foundations models, and is enjoyable). Has also been approached by senior people in ML concerned about AI safety, and is looking for a go-to response and resources for them.<br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/b2Jk3dAmerjyNDzWf/an-email-with-a-bunch-of-links-i-sent-an-experienced-ml\"><u>[An email with a bunch of links I sent an experienced ML researcher interested in learning about Alignment / x-safety.]</u></a></p><p><i>by David Scott Krueger (formerly: capybaralet)</i></p><p>Copy-pasted email made for a particular person after several discussions - not a ready-to-go template to send all ML researchers interested in Alignment. That said, a good starting point that aims to give a diverse and representative sampling of AI safety stuff.</p><p><br>&nbsp;</p><h2>Not AI Related</h2><p><a href=\"https://www.lesswrong.com/posts/fp3hAkLgnbsnzm8d4/solar-blackout-resistance\"><u>Solar Blackout Resistance</u></a></p><p><i>by jefftk</i></p><p>Residential solar panels shut down in a power outage, so they don\u2019t shock utility workers fixing things. If they instead disconnected from the grid but continued powering the house, this provides widespread distributed power in a catastrophe. The electronics to support this could be cheap, if there was high demand. Paths to encourage this include the government requiring this resilience, or only subsidizing solar panels which included these resiliency adaptations.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/2euv3bF5R2cEuMD2B/overton-gymnastics-an-exercise-in-discomfort\"><u>Overton Gymnastics: An Exercise in Discomfort</u></a></p><p><i>By Shos Tekofsky, omark</i></p><p>Idea for a new group rationality exercise, with instructions for three variants. All involve every participant sharing their most controversial opinions, and some involve questions from the group until they can pass an ideological turing test on these (convincingly argue for them).</p><p>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://www.lesswrong.com/posts/rxnpuzEw4boJrs3CY/rejected-early-drafts-of-newcomb-s-problem\"><u>Rejected Early Drafts of Newcomb's Problem</u></a>&nbsp;<i>By zahmahkibo</i> (meme versions)</p><p><a href=\"https://www.lesswrong.com/posts/fo4LYBHCpcoYptBfN/the-ethics-of-reclining-airplane-seats\"><u>The ethics of reclining airplane seats</u></a>&nbsp;<i>By braces</i> (low-stakes example of ethical debate on twitter)</p><p><a href=\"https://www.lesswrong.com/posts/pihPzjYgwKiBexytF/let-s-terraform-west-texas\"><u>Let's Terraform West Texas</u></a>&nbsp;<i>By blackstampede</i> (half-serious proposal, including cost estimates)</p><p><a href=\"https://www.lesswrong.com/posts/rp4CiJtttvwFNHkhL/searching-for-modularity-in-large-language-models\"><u>Searching for Modularity in Large Language Models</u></a>&nbsp;<i>by NickyP, Stephen Fowler</i></p><h1><br>This Week on Twitter</h1><h2>AI</h2><p>Riley Goodside continues his experiments on GPT-3, showing it can be prompted to use tools like Python to fill gaps in its skills / knowledge.&nbsp;<a href=\"https://twitter.com/goodside/status/1568448128495534081?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u>(link)</u></a>&nbsp; Also shows it remembers the context it generated in previous answers, to answer new ones.&nbsp;<a href=\"https://twitter.com/goodside/status/1568046227811352576?s=20&amp;t=qC333NtG9-9F-K3TK-R8GQ\"><u>(link)</u></a></p><p>Open source community figured out how to save ~\u00bc of the necessary VRAM for stable diffusion, just a couple weeks after release.&nbsp;<a href=\"https://twitter.com/hardmaru/status/1567256296226959360?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u>(link)</u></a> (Note last week they also substantially increased speed - so lots of performance improvements in a short timeframe)</p><p>Stephen Casper and coauthors publish a paper showing adversarial training is more effective if the models can see each other\u2019s internal state.&nbsp;<a href=\"https://twitter.com/StephenLCasper/status/1567696211293110273?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u>(link)</u></a></p><p>New article on the limits of language models.&nbsp;<a href=\"https://twitter.com/karpathy/status/1567233122252750848?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u>(link)</u></a></p><p>New research paper on how to align conversational agents with human values.&nbsp;<a href=\"https://twitter.com/DeepMind/status/1567169247863869441?s=20&amp;t=Ozx6pZWHcWr7DZKU-NhUhg\"><u>(link)</u></a></p><p>&nbsp;</p><h2>Forecasting</h2><p>New \u2018pastcasting\u2019 app gives users the ability to forecast on past questions whose resolution they don\u2019t know - increasing feedback loops / learning.&nbsp;<a href=\"https://twitter.com/NunoSempere/status/1568530960458698753?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u>(link)</u></a>&nbsp;<a href=\"https://t.co/2U9MNveW7B\"><u>(link)</u></a></p><p>&nbsp;</p><h2>National Security</h2><p>CSET published research in June about the Chinese People\u2019s Liberation Army dependence on chips from American companies for military progress in AI. Implication that it may have affected the US government\u2019s decision to ban China from buying these chips from US companies.&nbsp;<a href=\"https://twitter.com/CSETGeorgetown/status/1567955047547543552?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u>(link)</u></a>&nbsp; They also share a report from July where they\u2019re tracking what Chinese companies are doing in the general AI space.&nbsp;<a href=\"https://twitter.com/CSETGeorgetown/status/1567917592869994496?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u>(link)</u></a></p><p>Ukraine has launched a counter-offensive against Russia, details still coming out but significant land taken back, Russia retreating some areas. Russia responded by attacking Ukrainian power plants, trying to cut electricity.&nbsp;<a href=\"https://twitter.com/Scholars_Stage/status/1568572744849031170?s=20&amp;t=DIg9GVUntuiPvb_v8fBIqg\"><u>(link)</u></a></p><p>&nbsp;</p><h2>Science</h2><p>Trials show malaria vaccine gives 80% protection (3 doses + yearly booster). It\u2019s cheap, they have a deal to manufacture 100 million doses a year, and hope to roll it out next year.<a href=\"https://twitter.com/S_OhEigeartaigh/status/1567970202771685378?s=20&amp;t=_nLgEgttgwhY5GBtXTz1IQ\"><u> (link)</u></a></p><p>Monkeypox cases are falling, Covid BA5 wave is receding, and an unknown pneumonia in Argentina was identified as legionella. A good week for public health!&nbsp;<a href=\"https://mobile.twitter.com/juan_cambeiro/status/1569050077158572041\"><u>(link)</u></a></p>", "user": {"username": "GreyArea"}}, {"_id": "nbvcbB4iC8J29yjkx", "title": "How To Actually Succeed", "postedAt": "2022-09-12T22:33:27.895Z", "htmlBody": "<h3><strong>Epistemic Status:&nbsp;</strong></h3><p>Casual musings on Eliezer's LessWrong post <a href=\"https://www.lesswrong.com/posts/WLJwTJ7uGPA5Qphbp/trying-to-try\">\"Trying to Try\"</a></p><h3><strong>Slightly different though similar YouTube version:</strong></h3><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/l_VUkVqt9BE\"><div><iframe src=\"https://www.youtube.com/embed/l_VUkVqt9BE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><h3><strong>Also viewable on my website:</strong></h3><p><a href=\"https://jordanarel.com/2022/09/12/how-to-actually-succeed/\">https://jordanarel.com/2022/09/12/how-to-actually-succeed/</a></p><h3><strong>And LessWrong</strong></h3><p><a href=\"https://www.lesswrong.com/posts/SAskdZswjrmfoAK8L/how-to-actually-succeed\">https://www.lesswrong.com/posts/SAskdZswjrmfoAK8L/how-to-actually-succeed</a></p><h1><strong>Summary:</strong></h1><p>Trying is not the same as doing something. It is easy to feel like you are trying to do something, without having any chance of actually succeeding. It is important to create a clear mental model for success, test the assumptions of that model, determing the absolute single most important next action each step along the way, break down the goal into manageable steps, recognize and avoid noble obstacles, be willing to fail and iterate, and be willing to fail at less important things. Maybe we could have a social &nbsp;exercise to make sure we are actually working to robustly achieve our goals, rather than just \"trying.\"</p><h1><strong>Full Post:</strong></h1><p>In Eliezer Yudkowsky's post <a href=\"https://www.lesswrong.com/posts/WLJwTJ7uGPA5Qphbp/trying-to-try\">\"Trying to Try\"</a>, he reviews Yoda's phrase \"Do or do not, there is no try,\" explaining that at first he found this highly dubious, sounding like what he calls \"pretending to be wise\", saying something enigmatic and profound sounding which is actually superficial or meaningless.</p><p>After more deeply considering the matter, he came to the conclusion that the phrase actually is quite meaningful, as typically when people say they are going to try to do something, or in fact even when they say they are going to do something, they actually really only mean that they are going to put in the minimal amount of effort to plausibly feel like they might achieve it, or to be able to convey to others that they are working to achieve it.</p><p>He explains that it is much easier to \"try to make $1 million\" versus actually making $1 million, and it is much easier to \"try to launch a startup\" then to actually launch a startup.</p><p>I was quite floored to read this and realize that this deeply applies to myself as I want to launch a Hybrid Market which I think could completely transform the trajectory of humanity if successful, but which is insanely hard to actually succeed at, and extremely easy to \"try\" to succeed at.</p><p>Eliezer points out that one of the main ways we get in our own way is that rather than asking \"What needs to happen for this goal to actually be achieved?\" we ask \"What can I do to achieve this goal?\" or \"How can I do my best to achieve this goal?\" which are really clever ways of asking \"Without making a substantial effort or disturbing my life in any meaningful way, how can I use the current spare resources at my disposal to make an attempt that plausibly moves me toward this goal?\"</p><p>I found this very useful, and extrapolated that to actually achieve the goal of creating a Hybrid Market (which could be a stand-in for any ambitious goal,) I would need to create a detailed mental model of a path to create a Hybrid Market, accounting for planning fallacy and perhaps even <a href=\"https://en.wikipedia.org/wiki/Hofstadter%27s_law\">Hofstadter's Law</a> (though this is of course impossible,) so that my plan is robust enough that it is not just plausible that I could achieve the goal, but eminently likely if the plan is executed properly.</p><p>This plan might include preliminary steps like using experiments to test my mental model and the underlying assumptions which need to be confirmed for the mental model to be accurate. It then requires executing on the plan at a high level over a long period of time.</p><p>Another thing I realized is that one of the biggest obstacles I will face in actually achieving my goal is that I will have to confront failure many, many times. I think this is the biggest thing which prevents me from actually achieving my goal, versus trying to achieve my goal.</p><p>When I look at my next actions list, which I often order by priority, I noticed a strong pull toward the end of the list, as I get very anxious looking at the first thing on the list which is often very difficult and presents a strong possibility of failure as well as requiring a lot of effort.</p><p>Knowing that there is a strong chance there may be little or no payoff for large effort feels very demoralizing, so it is much easier to do things that I can convince myself are urgent (even though it doesn't really matter if they are done at all), or which plausibly could make the more important task easier by freeing up mental space or giving me some \"small wins\" or otherwise increasing my resources or moving me slightly toward my goal.</p><p>What I am realizing is that it takes incredible mental discipline to look at all of the things I could possibly be doing, notice all of my mental and emotional pull toward doing things that are plausibly important (also called <a href=\"https://www.mettasolutions.com/noble-obstacles-the-productivity-barriers-we-never-see-coming/\">\"noble obstacles\"</a>) but that I know in my heart of hearts is not actually really that important, and have the fortitude to remember how important my most important goal is, and know that I will have to make many sacrifices of lesser important-ish goals, even just for a shot at the possibility of achieving my most important goal.</p><p>I think one thing that Eliezer misses which is not quite a counterexample but may be complementary, is that there are many incremental iterative steps to achieving the most important goal. This is not the same as doing a bunch of random small things that plausibly could move you toward the goal, but instead means finding the absolutely single most important thing you can be doing and breaking it down into small doable steps as necessary, rather than trying to achieve the whole goal at once.</p><p>I wonder how many other EAs are bogged down by \"trying to try,\" doing things which plausibly seem like they could be the most effective, or thinking about how to be effective, or talking and debating about how to be effective, but not actually ever being effective. I think it's probably quite a lot of people in the movement, maybe even everyone does this sometimes. As such, I think it could be very high EV for people in the Effective Altruism movement to examine how they operate and see if they are \"trying\" to have the greatest possible impact, or are actually having the greatest possible impact they can have.</p><p>Perhaps this could even be done in a social event similar to<a href=\"https://www.lesswrong.com/posts/P5k3PGzebd5yYrYqd/the-hamming-question\"> \"Hamming Circles,\"</a> in which EAs ask each other \"What are the important problems in your field, and why aren\u2019t you working on them?\", an event already popular among some EAs. This could basically be a follow-up question that is something like \"Are you actually taking action that is exceedingly likely to solve the most important problem in your field, or are you just taking action that plausibly moves you in the right direction without too much planning and high level execution? And if so, what needs to happen for this problem to actually be solved?\"</p><h1><strong>Conclusion:</strong></h1><p>To sum up, I really liked Eliezer's post and hope my thought processes can give some outline to how this obstacle might be overcome. Main ways of doing this are making sure you have a very clear mental model of how to actually achieve your goal, testing the underlying assumptions on which your mental models depend, continually determining the absolute single most important next action you can take and then actually taking that next action instead of noble obstacle distractions, and breaking down the absolute single most important next action into manageable bite-size actions so that this is actually feasible. Then iterating toward the goal again and again, accepting failure as part of the process. Also, accepting that you're going to have to sacrifice and fail at lesser important goals to achieve your most important goal. Finally, I think EAs could maybe have a social exercise, an \"actually succeeding\" circle or something (sorry, I suck at naming things) where we support each other in seeing how we are distracted from doing the things that actually are most important for achieving our most impactful goals.</p><p>Hope this is helpful, appreciate any criticism or other ideas for how to actually succeed!</p>", "user": {"username": "Jordan Arel"}}, {"_id": "ZzwMBRq5KAo6wfP4K", "title": "Future Matters #5: supervolcanoes, AI takeover, and What We Owe the Future", "postedAt": "2022-09-14T13:02:10.621Z", "htmlBody": "<blockquote><p>Even if we think the prior existence view is more plausible than the total view, we should recognize that we could be mistaken about this and therefore give some value to the life of a possible future. The number of human beings who will come into existence only if we can avoid extinction is so huge that even with that relatively low value, reducing the risk of human extinction will often be a highly cost-effective strategy for maximizing utility, as long as we have some understanding of what will reduce that risk.</p><p>\u2014 Katarzyna de Lazari-Radek &amp; Peter Singer</p></blockquote><p>&nbsp;</p><p><a href=\"https://www.futurematters.news/\"><i><strong><u>Future Matters</u></strong></i></a> is a newsletter about longtermism. Each month we collect and summarize longtermism-relevant research, share news from the longtermism community, and feature a conversation with a prominent researcher. You can also subscribe on&nbsp;<a href=\"https://futurematters.substack.com/\"><u>Substack</u></a>, listen on your&nbsp;<a href=\"https://pod.link/1615637113\"><u>favorite podcast platform</u></a> and follow on&nbsp;<a href=\"https://twitter.com/FutureMatters_\"><u>Twitter</u></a>. <i>Future Matters&nbsp;</i>is also available in&nbsp;<a href=\"https://largoplacismo.substack.com\"><u>Spanish</u></a>.</p><hr><h2>Research</h2><p>William MacAskill\u2019s&nbsp;<a href=\"http://whatweowethefuture.com\"><i><u>What We Owe the Future</u></i></a> was published, reaching the New York Times Bestseller list in its first week and generating a deluge of media for longtermism. We strongly encourage readers to get a copy of the book, which is filled with new research, ideas, and framings, even for people already familiar with the terrain. In the next section, we provide an overview of the coverage the book has received so far.</p><p>In&nbsp;<a href=\"https://www.foxy-scout.com/samotsvetys-ai-risk-forecasts/\"><strong><u>Samotsvety's AI risk forecasts</u></strong></a>, Eli Lifland summarizes the results of some recent predictions related to AI takeover, AI timelines, and transformative AI by a group of seasoned forecasters.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg5tfxtnqebw\"><sup><a href=\"#fng5tfxtnqebw\">[1]</a></sup></span>&nbsp;In aggregate, the group places 38% on AI existential catastrophe, conditional on AGI being developed by 2070, and 25% on existential catastrophe via misaligned AI takeover by 2100. Roughly four fifths of their overall AI risk is from AI takeover. They put 32% on AGI being developed in the next 20 years.</p><p>John Halstead released a book-length report on&nbsp;<a href=\"https://drive.google.com/file/d/14od25qdb4sdDoXVDMoiSrTwuzYAMSpxK/view\"><strong><u>climate change and longtermism</u></strong></a> and published&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\"><u>a summary of it on the EA Forum</u></a>. The report offers an up-to-date analysis of the existential risk posed by global warming. One of the most important takeaways is that extreme warming seems significantly less likely than previously thought: the probability of &gt;6\u00b0C warming was thought to be 10% a few years ago, whereas it now looks &lt;1% likely. (For much more on this topic, see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XQbhnKgXiRTv4vfxt/future-matters-4-ai-timelines-agi-risk-and-existential-risk#Conversation_with_John_Halstead\"><u>our conversation with John</u></a> that accompanied last month\u2019s issue.)</p><p>In a similar vein, the Good Judgment Project asked superforecasters a series of questions on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zjc8utqES7jLjgBYn/superforecasting-long-term-risks-and-climate-change\"><strong><u>Long-term risks and climate change</u></strong></a>, the results of which are summarized by Luis Urtubey (full report&nbsp;<a href=\"https://goodjudgment.com/wp-content/uploads/2022/08/FF1FF2-Climate-report-final.pdf\"><u>here</u></a>).&nbsp;</p><p>The importance of existential risk reduction is often motivated by two claims: that the value of humanity\u2019s future is vast, and that the level of risk is high. David Thorstad\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/N6hcw8CxK7D3FCD5v/existential-risk-pessimism-and-the-time-of-perils-4\"><strong><u>Existential risk pessimism and the time of perils</u></strong></a> notes that these stand in some tension, since the higher the overall risk, the shorter humanity\u2019s expected lifespan. This tension dissolves, however, if one holds that existential risk will decline to near-zero levels if humanity survives the next few centuries of high risk. This is precisely the view held by most prominent thinkers on existential risk, e.g. Toby Ord (see&nbsp;<i>The Precipice</i>) and Carl Shulman (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/zLZMsthcqfmv5J6Ev/the-discount-rate-is-not-zero?commentId=Nr35E6sTfn9cPxrwQ\"><u>this comment</u></a>).</p><p>In&nbsp;<a href=\"https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=1372&amp;context=dltr\"><strong><u>Space and existential risk</u></strong></a>,<strong>&nbsp;</strong>the legal scholar Chase Hamilton argues that existential risk reduction should be a central consideration shaping space law and policy. He outlines a number of ways in which incautious space development might increase existential risk, pointing out that our current&nbsp;<i>laissez-faire</i> approach fails to protect humanity against these externalities and offering a number of constructive proposals. We are in a formative period for space governance, presenting an unusual opportunity to identify and advocate for laws and policies that safeguard humanity\u2019s future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0wgk82hay2i\"><sup><a href=\"#fn0wgk82hay2i\">[2]</a></sup></span></p><p>Michael Cassidy and Lara Mani warn about the risk from&nbsp;<a href=\"https://www.nature.com/articles/d41586-022-02177-x\"><strong><u>huge volcanic eruptions</u></strong></a>. Humanity devotes significant resources to managing risk from asteroids, and yet very little into risk from supervolcanic eruptions, despite these being substantially more likely. The absolute numbers are nonetheless low; super-eruptions are expected roughly once every 14,000 years. Interventions proposed by the authors include better monitoring of eruptions, investments in preparedness, and research into geoengineering to mitigate the climatic impacts of large eruptions or (most speculatively) into ways of intervening on volcanoes directly to prevent eruptions.</p><p>The risks posed by supervolcanic eruptions, asteroid impacts, and nuclear winter operate via the same mechanism: material being lofted into the stratosphere, blocking out the sun and causing abrupt and sustained global cooling, which severely limits food production. The places best protected from these impacts are thought to be remote islands, whose climate is moderated by the ocean. Matt Boyd and Nick Wilson\u2019s&nbsp;<a href=\"https://www.researchsquare.com/article/rs-1927222/v1\"><strong><u>Island refuges for surviving nuclear winter and other abrupt sun-reducing catastrophes</u></strong></a> analyzes how well different island nations might fare, considering factors like food and energy self-sufficiency. Australia, New Zealand, and Iceland score particularly well on most dimensions.&nbsp;</p><p>Benjamin Hilton's&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\"><strong><u>Preventing an AI-related catastrophe</u></strong></a> is 80,000 Hours' longest and most in-depth problem profile so far. It is structured around six separate reasons that jointly make artificial intelligence, in 80,000 Hours' assessment, perhaps the world's most pressing problem. The reasons are (1) that many AI experts believe that there is a non-negligible chance that advanced AI will result in an existential catastrophe; (2) that the recent extremely rapid progress in AI suggests that AI systems could soon become transformative; (3) that there are strong arguments that power-seeking AI poses an existential risk; (4) that even non-power-seeking AI poses serious risks; (5) that the risks are tractable; and (6) that the risks are extremely neglected.</p><p>In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5y3vzEAXhGskBhtAD/most-small-probabilities-aren-t-pascalian\"><strong><u>Most small probabilities aren't Pascalian</u></strong></a>, Gregory Lewis lists some examples of probabilities as small as one-in-a-million that society takes seriously, in areas such as aviation safety and asteroid defense. These and other examples suggest that&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/pascal-s-mugging\"><u>Pascal's mugging</u></a>, which may justify abandoning expected value theory when the probabilities are small enough, does not undermine the case for reducing the existential risks that longtermists worry about.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3c6j35gb54y\"><sup><a href=\"#fn3c6j35gb54y\">[3]</a></sup></span>&nbsp;In the comments, Richard Yetter Chappell&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5y3vzEAXhGskBhtAD/most-small-probabilities-aren-t-pascalian?commentId=hbZdQKpa4e2BKmqvT\"><u>argues</u></a> that exceeding the one-in-a-million threshold is plausibly a sufficient condition for being non-Pascalian, but it may not be a necessary condition: probabilities robustly grounded in evidence\u2014such as the probability of casting the decisive vote in an election with an arbitrarily large electorate\u2014should always influence decisionmaking no matter how small they are.</p><p>In&nbsp;<a href=\"https://www.slowboring.com/p/whats-long-term-about-longtermism\"><strong><u>What's long-term about \"longtermism\"?</u></strong></a>, Matthew Yglesias argues that one doesn't need to make people care more about the long-term in order to persuade them to support longtermist causes. All one needs to do is persuade them that the risks are significant and that they threaten the present generation. Readers of this newsletter will recognize the similarity between Yglesias\u2019s argument and those made previously by&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/rFpfW2ndHSX7ERWLH/simplify-ea-pitches-to-holy-shit-x-risk\"><u>Neel Nanda</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk\"><u>Scott Alexander</u></a> (summarized in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nA3R6Hm8x8CyzRHS2/future-matters-0-space-governance-future-proof-ethics-and\"><i><u>FM</u></i><u>#0</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5AzxpkNzgFWnjdqTf/future-matters-1-ai-takeoff-longtermism-vs-existential-risk\"><i><u>FM</u></i><u>#1</u></a>, respectively).</p><p>Eli Lifland's&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/rvvwCcixmEep4RSjg/prioritizing-x-risks-may-require-caring-about-future-people\"><strong><u>Prioritizing x-risks may require caring about future people</u></strong></a> notes that interventions aimed at reducing existential risks are, in fact, not clearly more cost-effective than standard&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/global-health-and-wellbeing\"><u>global health and wellbeing</u></a> interventions. On Lifland's rough cost-effectiveness estimates, AI risk interventions, for example, are expected to save approximately as many present-life-equivalents per dollar as animal welfare interventions. And as Ben Todd notes in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/rvvwCcixmEep4RSjg/prioritizing-x-risks-may-require-caring-about-future-people?commentId=HD7wYegs8pHeDDQ2G\"><u>comments</u></a>, the cost-effectiveness of the most promising longtermist interventions will likely go down substantially in the coming years and decades, as this cause area becomes increasingly crowded. Lifland also points out that many people interpret \"longtermism\" as a view focused on influencing&nbsp;<i>events</i> in the long-term future, whereas longtermism is actually concerned with the long-term&nbsp;<i>impact</i> of our actions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref887d5fy5pun\"><sup><a href=\"#fn887d5fy5pun\">[4]</a></sup></span>&nbsp;This makes \"longtermism\" a potentially confusing label in situations, such as the one in which we apparently find ourselves, where concern with long-term impact seems to require focusing on short-term events, like risks from advanced artificial intelligence.</p><p>Trying to ensure the development of transformative AI goes well is made difficult by how uncertain we are about how it will play out. Holden Karnofsky\u2019s&nbsp;<a href=\"https://www.lesswrong.com/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting\"><strong><u>AI strategy nearcasting</u></strong></a><strong>&nbsp;</strong>sets out an approach for dealing with this conundrum: trying to answer strategic questions about TAI, imagining that it is developed in a world roughly similar to today\u2019s. In a series of posts, Karnofsky will do some nearcasting based on the scenario laid out in Ajeya Cotra\u2019s&nbsp;<a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>Without specific countermeasures\u2026</u></a> (summarised in&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/future-matters\"><i><u>FM#4</u></i></a>).</p><p>Karnofsky's&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very\"><strong><u>How might we align transformative AI if it's developed very soon?</u></strong></a>, the next installment in the \u201cAI strategy nearcasting\u201d series, considers some alignment approaches with the potential to prevent the sort of takeover scenario described by Ajeya Cotra in a recent report. Karnofsky's post is over 13,000 words in length and contains many more ideas than we can summarize here. Readers may want to first read our conversation with Ajeya and then take a closer look at the post. Karnofsky's overall conclusion is that \"the risk of misaligned AI is serious but not inevitable, and taking it more seriously is likely to reduce it.\"</p><p>In&nbsp;<a href=\"https://www.vox.com/future-perfect/2022/8/8/23150496/effective-altruism-sam-bankman-fried-dustin-moskovitz-billionaire-philanthropy-crytocurrency\"><strong><u>How effective altruism went from a niche movement to a billion-dollar force</u></strong></a>, Dylan Matthews chronicles the evolution of effective altruism over the past decade. In an informative, engaging, and at times moving article, Matthews discusses the movement\u2019s growth in size and its shift in priorities. Matthews concludes: \u201cMy attitude toward EA is, of course, heavily personal. But even if you have no interest in the movement or its ideas, you should care about its destiny. It\u2019s changed thousands of lives to date. Yours could be next. And if the movement is careful, it could be for the better.\u201d</p><hr><h2>News</h2><p>The level of media attention&nbsp;on <i>What We Owe the Future</i> has been astounding. Here is an incomplete summary:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmw6paqka41a\"><sup><a href=\"#fnmw6paqka41a\">[5]</a></sup></span></p><ul><li>Parts of Will\u2019s book were excerpted or adapted in&nbsp;<a href=\"https://www.bbc.com/future/article/20220805-what-is-longtermism-and-why-does-it-matter\"><u>What is longtermism and why does it matter?</u></a> (BBC),&nbsp;<a href=\"https://www.theatlantic.com/ideas/archive/2022/08/future-generations-climate-change-pandemics-ai/671148/\"><u>How future generations will remember us</u></a> (<i>The Atlantic</i>),&nbsp;<a href=\"https://www.newscientist.com/article/mg25534033-000-we-need-to-act-now-to-give-future-generations-a-better-world/\"><u>We need to act now to give future generations a better world</u></a> (<i>New Scientist</i>),&nbsp;<a href=\"https://www.nytimes.com/2022/08/05/opinion/the-case-for-longtermism.html\"><u>The case for longtermism</u></a> (<i>The New York Times</i>) and&nbsp;<a href=\"https://www.foreignaffairs.com/world/william-macaskill-beginning-history\"><u>The beginning of history</u></a> (<i>Foreign Affairs</i>).</li><li>Will was profiled in&nbsp;<a href=\"https://time.com/6204627/effective-altruism-longtermism-william-macaskill-interview/\"><i><u>Time</u></i></a>, the&nbsp;<a href=\"https://www.ft.com/content/091862f9-985f-4769-aa37-1aed32636329\"><i><u>Financial Times</u></i></a>, and&nbsp;<a href=\"https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism\"><i><u>The New Yorker</u></i></a> (see&nbsp;<a href=\"https://twitter.com/willmacaskill/status/1556764231822970884\"><u>this Twitter thread</u></a> for Will\u2019s take on the latter).</li><li>Will was interviewed by&nbsp;<a href=\"https://www.nytimes.com/2022/08/09/podcasts/transcript-ezra-klein-interviews-will-macaskill.html\"><u>Ezra Klein</u></a>,&nbsp;<a href=\"https://conversationswithtyler.com/episodes/william-macaskill/\"><u>Tyler Cowen</u></a>,&nbsp;<a href=\"https://www.youtube.com/watch?v=vIdxzkOqK_o\"><u>Tim Ferriss</u></a>,&nbsp;<a href=\"https://www.dwarkeshpatel.com/p/will-macaskill#details\"><u>Dwarkesh Patel</u></a>,&nbsp;<a href=\"https://80000hours.org/podcast/episodes/will-macaskill-what-we-owe-the-future/\"><u>Rob Wiblin</u></a>,&nbsp;<a href=\"https://www.samharris.org/podcasts/making-sense-episodes/292-how-much-does-the-future-matter\"><u>Sam Harris</u></a>,&nbsp;<a href=\"https://art19.com/shows/sean-carrolls-mindscape/episodes/45216bb7-b815-4302-a8dc-a3050ea115f5\"><u>Sean Carroll</u></a>,&nbsp;<a href=\"https://www.youtube.com/watch?v=gXBvfL2zTkU\"><u>Chris Williamson</u></a>,&nbsp;<a href=\"https://www.npr.org/sections/goatsandsoda/2022/08/16/1114353811/how-can-we-help-humans-thrive-trillions-of-years-from-now-this-philosopher-has-a\"><u>Malaka Gharib</u></a>,&nbsp;<a href=\"https://youtu.be/Zi5gD9Mh29A\"><u>Ali Abdaal</u></a>,&nbsp;<a href=\"https://www.econtalk.org/will-macaskill-on-longtermism-and-what-we-owe-the-future/\"><u>Russ Roberts</u></a>,&nbsp;<a href=\"https://www.undispatch.com/how-longtermism-is-shaping-foreign-policy-will-macaskill/\"><u>Mark Goldberg</u></a>,&nbsp;<a href=\"https://youtu.be/_UycanMiee8\"><u>Max Roser</u></a>, and&nbsp;<a href=\"https://freakonomics.com/podcast/a-million-year-view-on-morality/\"><u>Steven Levitt</u></a>.</li><li><i>What We Owe the Future</i> was reviewed by&nbsp;<a href=\"https://www.theguardian.com/books/2022/aug/25/what-we-owe-the-future-by-william-macaskill-review-a-thrilling-prescription-for-humanity\"><u>Oliver Burkeman</u></a> (<i>The Guardian</i>),&nbsp;<a href=\"https://astralcodexten.substack.com/p/book-review-what-we-owe-the-future\"><u>Scott Alexander</u></a> (<i>Astral Codex Ten</i>),&nbsp;<a href=\"https://bostonreview.net/articles/the-new-moral-mathematics/\"><u>Kieran Setiya</u></a> (<i>Boston Review</i>),&nbsp;<a href=\"https://www.thebookseller.com/author-interviews/william-macaskill-on-influencing-the-lives-of-future-generations\"><u>Caroline Sanderson</u></a> (<i>The Bookseller</i>),&nbsp;<a href=\"https://www.the-tls.co.uk/articles/what-we-owe-the-future-william-macaskill-book-review-regina-rini/\"><u>Regina Rini</u></a> (<i>The Times Literary Supplement)</i>,&nbsp;<a href=\"https://rychappell.substack.com/p/review-of-what-we-owe-the-future\"><u>Richard Yetter Chappell</u></a> (<i>Good Thoughts</i>) and&nbsp;<a href=\"https://www.foxy-scout.com/wwotf-review/\"><u>Eli Lifland</u></a> (Foxy Scout).</li><li>The book also inspired three impressive animations:&nbsp;<a href=\"https://www.youtube.com/watch?v=r6sa_fWQB_4\"><u>How many people might ever exist calculated</u></a> (Primer),&nbsp;<a href=\"https://www.youtube.com/watch?v=_uV3wP5z51U\"><u>Can we make the future a million years from now go better?</u></a> (Rational Animations),&nbsp;<a href=\"https://youtu.be/W93XyXHI8Nw\"><u>Is civilisation on the brink of collapse?</u></a> (Kurzgesagt).</li><li>And finally, Will participated in a&nbsp;<a href=\"https://www.reddit.com/r/IAmA/comments/wro991/im_will_macaskill_a_philosophy_professor_at/\"><u>Reddit 'ask me anything'</u></a>.</li></ul><p>The Forethought Foundation is&nbsp;<a href=\"https://www.forethought.org/opportunities\"><u>hiring for several roles</u></a> working closely with Will MacAskill.</p><p>In an interesting marker of the mainstreaming of AGI discourse, a&nbsp;<a href=\"https://www.nytimes.com/2022/08/24/technology/ai-technology-progress.html\"><i><u>New York Times</u></i><u> article</u></a> cited Ajeya Cotra\u2019s recent AI timelines update (summarised in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/XQbhnKgXiRTv4vfxt/future-matters-4-ai-timelines-agi-risk-and-existential-risk\"><i><u>FM</u></i><u>#4</u></a>).</p><p>Dan Hendrycks, Thomas Woodside and Oliver Zhang announced&nbsp;<a href=\"https://course.mlsafety.org/\"><u>a new course</u></a> designed to introduce students with a background in machine learning to the most relevant concepts in empirical ML-based AI safety.</p><p>The Center for AI Safety announced the&nbsp;<a href=\"https://philosophy.safe.ai/\"><u>CAIS Philosophy Fellowship</u></a>, a program for philosophy PhD students and postdoctorates to work on conceptual problems in AI safety.</p><p>Longview Philanthropy and Giving What We Can&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/f7qAfcKArzYrBG7RB/announcing-the-longtermism-fund\"><u>announced</u></a> the&nbsp;<a href=\"https://www.givingwhatwecan.org/charities/longtermism-fund\"><u>Longtermism Fund</u></a>, a new fund for donors looking to support longtermist work. See also&nbsp;<a href=\"https://youtu.be/0g_1JfFRdY4\"><u>this EA Global London 2022 interview</u></a> with Simran Dhaliwal, Longview Philanthropy's co-CEO.</p><p>Radio Bostrom released&nbsp;<a href=\"https://radiobostrom.com/introduction\"><u>an audio introduction to Nick Bostrom</u></a>.</p><p>Micha\u00ebl Trazzi&nbsp;<a href=\"https://theinsideview.ai/roblong\"><u>interviewed Robert Long</u></a> about the recent LaMDA controversy, the sentience of large language models, the metaphysics and philosophy of consciousness, artificial sentience, and more. He also&nbsp;<a href=\"https://theinsideview.ai/alex\"><u>interviewed Alex Lawsen</u></a> on the pitfalls of forecasting AI progress, why one can't just \"update all the way bro\", and how to develop inside views about AI alignment.</p><p>Fin Moorhouse and Luca Righetti interviewed&nbsp;<a href=\"https://hearthisidea.com/episodes/karpur\"><u>Michael </u></a><a href=\"https://hearthisidea.com/episodes/aird\"><u>Aird</u></a> on impact-driven research and&nbsp;<a href=\"https://hearthisidea.com/episodes/esvelt-sandbrink\"><u>Kevin Esvelt &amp; Jonas Sandbrink</u></a> on risks from biological research for&nbsp;<a href=\"https://hearthisidea.com/\"><u>Hear This Idea</u></a>.</p><p>The materials for two new courses related to longtermism were published:&nbsp;<a href=\"https://rychappell.substack.com/p/updated-syllabus-on-ealongtermism\"><u>Effective altruism and the future of humanity</u></a> (Richard Yetter Chappell) and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/KrjyrsRky5JL4P5MF/introducing-the-existential-risks-introductory-course-eric-1\"><u>Existential risks introductory course</u></a> (Cambridge Existential Risks Initiative).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyl8j6cwatts\"><sup><a href=\"#fnyl8j6cwatts\">[6]</a></sup></span></p><p>Verfassungsblog, an academic forum of debate on events and developments in constitutional law and politics, hosted a symposium on&nbsp;<a href=\"https://verfassungsblog.de/category/debates/longtermism-and-the-law-debates/\"><u>Longtermism and the law</u></a>, co-organized by the University of Hamburg and the Legal Priorities Project.</p><p>The&nbsp;<a href=\"https://futureoflife.org/future-of-life-award/\"><u>2022 Future of Life Award</u></a>\u2014a prize awarded every year to one or more individuals judged to have had an extraordinary but insufficiently appreciated long-lasting positive impact\u2014was given to Jeannie Peterson, Paul Crutzen, John Birks, Richard Turco, Brian Toon, Carl Sagan, Georgiy Stenchikov and Alan Robock \u201cfor reducing the risk of nuclear war by developing and popularizing the science of nuclear winter.\u201d</p><hr><h2>Conversation with Ajeya Cotra</h2><p>Ajeya Cotra is a Senior Research Analyst at Open Philanthropy. She has done research on cause prioritization, worldview diversification, AI forecasting, and other topics. Ajeya graduated from UC Berkeley with a degree in Electrical Engineering and Computer Sciences. As a student, she worked as a teaching assistant for various computer science courses, ran the Effective Altruists of Berkeley student organization, and taught a course on effective altruism.</p><p><strong>Future Matters</strong>: You recently published a rather worrying report,&nbsp;<a href=\"https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover</u></a>. The report doesn't try to cover all the different possible paths toward transformative AI, but focuses specifically on an AI company training a scientist model using an approach you call \"human feedback on diverse tasks\" (HFDT). To begin, can you tell us what you mean by HFDT and what made you focus on it?</p><p><strong>Ajeya Cotra</strong>: Basically, the idea is that you have a large neural network; you pretrain it in the way that people pretrain GPT, where it learns to predict its environment. And maybe its environment is just text, so it's learning to predict text. In my particular example\u2014which is a bit narrower than HFDT overall, just for concretely imagining things\u2014I'm imagining the goal is to train a system to interact with the computer in the way humans would interact with the computers: googling things, writing code, watching videos, sending emails, etc. So the first stage of this training would be just training the model to have a model of what will happen if it does various things. The predictive pretraining that I'm imagining is to give it images of computer screens, and then actions that are taken, which might be pressing the escape key or something, and then it gets rewarded based on predicting what happens next.</p><p>Now you do that for a long time, and the hope is that this has created a system that has a broad understanding of how computers work, what happens if it does various things, and then you can build on that by imitating humans doing particular things. For example, gathering data sets of a programmer writing docstrings, writing functions or running tests, and capturing all that with keystroke logging or screen captures, in order to feed it into the model so it learns to act more like that. And then the last stage of the training is where the human feedback comes in. Once we have a model that is dealing with the computer and doing useful things in roughly the way that the humans you trained it on do stuff, to refine its abilities and potentially take it beyond human ability, we now switch to a training regime where it tries things, and humans see how well that thing worked, and give it a reward based on that.</p><p>For example, humans could ask for some sort of app, or some sort of functionality, and the model would try to write code. Humans would ask: Did the code pass our tests? Did the ultimate product seem useful and free of bugs?' and, based on that, would give the system some sort of RL reward.</p><p>It's a pretty flexible paradigm. In some sense, it involves throwing the whole kitchen sink of modern techniques into one model. It's not even necessarily majority-based on human feedback, in the sense of reinforcement learning. But I still called it human feedback on diverse tasks, because that is the step where you take it beyond imitating humans\u2014you have it try things in the world, see how they work and give it rewards\u2014and therefore the step that introduces a lot of the danger, so that's what I framed it around.</p><p><strong>Future Matters</strong>: So that's the paradigm in which this model is created. And then, the report makes three further assumptions about how this scenario plays out. Could you walk us through these?</p><p><strong>Ajeya Cotra</strong>: Yes. The three assumptions are what I call the racing forward assumption, the naive safety effort assumption, and then the HFDT scales far assumption.</p><p>So taking the last one first, this assumption is basically just that the process I outlined works for producing a very smart creative system that can automate all the difficult, long-term, intellectually demanding things that human scientists do in the course of doing their job. It's not limited to something much less impactful. In this story, I'm postulating that this technique doesn't hit a wall, and basically that you can get transformative AI with it.</p><p>And then the other two assumptions, racing forward and naive safety effort, are related. As to the racing forward idea, the company I'm imagining (which I'm calling Magma) is training this system (which I call Alex) in the context of some sort of intense competitive race, either with other companies for commercially dominating a market, or with other countries, if you imagine Magma to be controlled by a government. So Magma\u2019s default presumption is that it's good to make our systems smarter: that will make them more useful, and that will make us more likely to win whatever race we're racing. We don't have a default stance of an abundance of caution and a desire to go slow. We have a default stance that is typical of any startup developing any technology, which is just move fast, make your product and make it as good as you can make it.</p><p>And then the third assumption almost follows from the racing forward assumption: the naive safety assumption, which is that the company that's developing the system doesn't have it as a salient or super plausible outcome that the system could develop goals of its own and end up taking over the world, or harming its creators. They may have other potential safety issues in mind, like failures of robustness where the system can do wacky things and cause a lot of damage by accident, but they don't have this deliberate, deceptive failure enough at the top of their mind to make major sacrifices to specifically address that.</p><p>They're doing their safety effort in the same way that companies today do safety efforts for the systems that they release. For instance, they want to make sure this thing doesn't embarrass them by saying something toxic, or they want to make sure that this thing won't accidentally delete all your files, or things like that. And basically the way they go about achieving this safety is testing it in these scenarios and training it until it no longer displays these problematic behaviours, and that's about the main thing they do for safety.</p><p><strong>Future Matters</strong>: You say that Alex\u2014the model trained by Magma, the company\u2014 will have some key properties; and it is in virtue of having these properties that Alex poses the sort of threat that your report focuses on. What are these properties?</p><p><strong>Ajeya Cotra</strong>: I included these properties as part of the assumptions, but I generally think that they\u2019re very likely to fall out of the HFDT scales far assumption, where if you can really automate everything that humans are doing, I think that you'll have the following two properties.</p><p>The first one is having robust, broadly-applicable skills and understanding of the world. Alex's understanding of the world isn't in shallow, narrow patterns, which tend to break when it goes out of distribution: it has a commonsensically coherent understanding of the world, similar to humans, which allows us to not fall apart and say something stupid if we see a situation that we haven't exactly encountered before, or if we see something too weird. We act sensibly, maybe not like maximally intelligently, but sensibly.</p><p>And property number two is coming up with creative plans to achieve open-ended goals. And here, this leans on picturing the training like, 'Hey, accomplish this thing, synthesise this protein or build this web app, or whatever: we're going to see how well you did, and we're going to reward you based on our perception of how well you did'. So it's not particularly constraining the means in any specific way, and it's giving rewards based on end outcomes. And the tasks that it's being trained on are difficult tasks, and ultimately pretty long-term tasks.</p><p>The idea is that, because of the racing forward assumption, Magma is just trying to make Alex as useful as possible. And one of the components of being maximally useful in these intellectual roles, these knowledge work tasks, is being able to come up with plans that work, that sometimes work for unexpected reasons: just like how an employee who\u2019s creative and figures out how to get the thing you want done is more useful than an employee who follows a certain procedure to the letter, and isn't looking out for ways to get more profit or finish something faster, or whatever.</p><p><strong>Future Matters</strong>: Turning to the next section of the report, you claim that, in the lab setting, Alex will be rewarded for what you call \"playing the training game\". What do you mean by this expression, and why do you think that the training process will push Alex to behave in that way?</p><p><strong>Ajeya Cotra</strong>: By playing the training game I mean that this whole setup is pushing Alex very hard to try to get as much reward as possible, where, based on the way its training is set up, as much reward as possible roughly means making humans believe it did as well as possible, or at least claim that it did as well as possible. This is just pointing out the gap between actually doing a good job and making your supervisors believe you did a good job. I claim that they're going to be many tradeoffs, both small and large, between these two goals, and that whenever they conflict, the training process pushes Alex to care about the latter goal of its making supervisors believe it did a good job, because that's what the reward signal in fact is.</p><p>This isn't necessarily extremely dangerous. I haven't argued for that yet. It's more an argument that you won't get a totally straightforward system that for some reason never deceives you, or for some reason is obedient in this kind of deontological way, because you're training it to find creative ways to attain reward, and sometimes creative ways to attain reward will involve deceptive behaviour. For example, making you think that its deployment of some product had no issues\u2014when in fact it did\u2014because it knows that if you found out about those issues, you would give it a lower reward; or playing to your personal or political biases, or emotional biases to get you to like it and rate it higher, and just a cluster of things in that vein.</p><p><strong>Future Matters</strong>: The next, and final, central claim in your analysis relates to the transition from the lab setting to the deployment setting. You argue that deploying Alex would lead to a rapid loss of human control. Can you describe the process that results in this loss of control and explain why you think it's the default outcome in the absence of specific countermeasures?</p><p><strong>Ajeya Cotra</strong>: Yes. So far in the story, we have this system that has a good understanding of the world, is able to adapt well in novel situations, can come up with these creative long-term plans, and is trying very hard to get a lot of reward, as opposed to trying very hard to be helpful, or having a policy of being obedient, or having a policy of being honest. And so, when that system is deployed and used in all the places where it would be useful, a lot of things happen. For example, science and technology advance much more rapidly than it could if humans were the only scientists, because the many copies of Alex run a lot faster than a human brain, there are potentially a lot more of them than there are human scientists in the world, and they can improve themselves, make new versions of themselves, and reproduce much more quickly than humans can.</p><p>And so you have this world where increasingly it's the case that no human really knows why certain things are happening, and increasingly it's the case that the rewards are more and more removed from the narrow actions that these many copies of Alex are taking. Humans can still send in rewards into this crazy system, but it'll basically be based on, 'Oh, did this seem to be a good product?', 'Did we make money this quarter?' or 'Do things look good in a very broad way?', which increasingly loosens the leash that the systems are on, relative to the lab setting. In the lab, when they're taking these particular actions, humans are able to potentially scrutinise them more, and more importantly, the actions aren't affecting the outside world, and changing systems out there in the world.</p><p>That's one piece of it, and then you have to combine that with what we know about Alex, or what we have assumed about Alex in this story, namely that it's very creative, it understands the world well, it can make plans, and it's making plans to do something, which in the lab setting looked like trying very hard to get reward, and didn't look like being helpful or loyal to humans, at least not fully. And so, if you ask what is the psychology of a system that in the lab setting tries really hard to get reward, one thing you might believe is that it's a system that will try really hard to get reward in the deployment setting as well, and maybe you could call it a system that wants reward intrinsically. That doesn't seem good, and seems like it would lead to a takeover situation, roughly because if Alex can secure control of the computers that it's running on, then it will have maximal control over what rewards it gets, and it can never have as good a situation, letting humans continue to give it rewards, if only because humans will sometimes make mistakes and give it lower rewards than they should have or something.</p><p>But then you might say that you don't know if Alex really wants reward, that you don't know what it wants at all, if it wants anything. And that's true, that seems plausible to me. But whatever its psychology is or whatever it really wants, that thing led it to try really hard to get rewarded in the lab setting. And it was trained to be like that. If Alex just wanted to sit in a chair for five minutes, that wouldn't have been a very useful system. As soon as it got to sit in a chair for five minutes, it would stop doing anything helpful to humans, and then we would continue to train it until we found a system that, in fact, was doing helpful things to humans.</p><p>So the claim I make is that if Alex doesn't care about reward intrinsically, it still had some sort of psychological setup that caused it to try extremely hard to get rewarded in the lab setting. And the most plausible kind of thing that it feels like would lead to that behaviour is that Alex does have some sort of ambitious goals, or something that it wants, for which trying really hard to get reward in the lab setting was a useful intermediate step. If Alex just wanted to survive and reproduce, say, if it had some sort of genetic fitness based goal, then that would be sufficient to cause it to try really hard to get rewarded in the lab setting, because it has to get a lot of reward in order to be selected to be deployed and make a lot of copies of itself in the future.</p><p>And similarly, any goal, as long as it wasn't an extremely short-term and narrow goal, like 'I want to sit in a chair for five minutes', would motivate Alex to try and get reward in the training setting. And none of those goals are very good for humans either, because actually the whole array of those goals benefits from Alex gaining control over the computers that are running it, and gaining control over resources in the world. In this case, it's not because it wants to intrinsically wirehead or change its reward to be a really high number, but rather because it doesn't want humans continually coming in and intervening on what it's doing, and intervening on its psychology by changing its rewards. So maybe it doesn't care about reward at all, but it still wants to have the ability to pursue whatever it is it wants to pursue.</p><p><strong>Future Matters</strong>: Supposing that we get into this scenario, can you talk about the sorts of specific countermeasures Magma could adopt to prevent takeover?</p><p><strong>Ajeya Cotra</strong>: Yes. I think a big thing is simply having it in your hypothesis space and looking out for early signs of this. So a dynamic that I think can be very bad is, you observe early systems that are not super powerful do things that look like deception, and the way you respond to that is by giving it negative reward for doing those things, and then it stops doing those things. I think the way I'd rather people respond to that is, 'Well, this is a symptom of a larger problem in which the way we train the system causes it to tend toward psychologies or goals or motivation systems that motivate deception'. If we were to give a negative reward to the instances of deception we found, we should just expect us to find fewer and fewer instances, but not necessarily because we solve the root problem, but because we are teaching this system to be more careful. Instead, we should stop and examine the model with more subtle tools\u2014for example, mechanistic interpretability or specific test environments\u2014and we should have the discipline not to simply train away measurable indicators of a problem, and not to feel good if upon seeing something bad and then training it away, it goes away.</p><p>Interpretability seems like a big thing, trying to create feedback mechanisms that are more epistemically competitive with the model. In this case it's not a human who tries to discern whether the model's actions had a good effect: it's maybe some kind of amplified system, maybe it has help from models very similar to this model, etc. Holden has&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very\"><u>a whole post on how we might align transformative AI if it's developed soon</u></a>, that goes over a bunch of these possibilities.</p><p><strong>Future Matters</strong>: Thanks, Ajeya!</p><hr><p><i>We thank Leonardo Pic\u00f3n for editorial assistance.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng5tfxtnqebw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg5tfxtnqebw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Disclosure: one of us is a member of Samotsvety.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0wgk82hay2i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0wgk82hay2i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on this point, see Fin Moorhouse\u2019s&nbsp;<a href=\"https://80000hours.org/problem-profiles/space-governance/\"><u>profile on space governance</u></a> and Douglas Ligor and Luke Matthews's&nbsp;<a href=\"https://www.rand.org/blog/2022/05/outer-space-and-the-veil-of-ignorance-an-alternative.html\"><u>Outer space and the veil of ignorance</u></a>, summarized in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nA3R6Hm8x8CyzRHS2/future-matters-0-space-governance-future-proof-ethics-and\"><i><u>FM</u></i><u>#0</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LutHHRpAuQyfknSem/future-matters-2-clueless-skepticism-longtermist-as-an\"><i><u>FM</u></i><u>#2</u></a>, respectively.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3c6j35gb54y\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3c6j35gb54y\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Rob Wiblin made a similar point in&nbsp;<a href=\"https://www.overcomingbias.com/2012/09/if-elections-arent-a-pascals-mugging-existential-risk-shouldnt-be-either.html\"><u>If elections aren\u2019t a Pascal\u2019s mugging, existential risk shouldn\u2019t be either</u></a>,&nbsp;<i>Overcoming Bias</i>, 27 September 2012, and in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vYb2qEyqv76L62izD/saying-ai-safety-research-is-a-pascal-s-mugging-isn-t-a\"><u>Saying \u2018AI safety research is a Pascal\u2019s Mugging\u2019 isn\u2019t a strong response</u></a>,&nbsp;<i>Effective Altruism Forum</i>, 15 December 2015.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn887d5fy5pun\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref887d5fy5pun\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We made a similar point in our summary of Alexander's article, referenced in the previous paragraph: \"the 'existential risk' branding [\u2026] draws attention to the&nbsp;<i>threats</i> to [...] value, which are disproportionately (but not exclusively) located in the short-term, while the 'longtermism' branding emphasizes instead the&nbsp;<i>determinants</i> of value, which are in the far future.\"</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmw6paqka41a\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmw6paqka41a\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/kFpJ5of8nGFgRhTzL/will-macaskill-media-for-wwotf-full-list\"><u>James Aitchison\u2019s post</u></a> for a comprehensive and regularly updated list of all podcast interviews, book reviews, and other media coverage.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyl8j6cwatts\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyl8j6cwatts\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A full list of such courses may be found&nbsp;<a href=\"https://www.stafforini.com/blog/courses-on-longtermism/\"><u>here</u></a>.</p></div></li></ol>", "user": {"username": "Pablo_Stafforini"}}, {"_id": "xrFZbNWvRz5cKZ8iC", "title": "Practical Plant-Based Meal Planning for Groups", "postedAt": "2022-09-12T21:09:29.353Z", "htmlBody": "<p>So far it has been <a href=\"http://www.lincolnquirk.com/2022/02/15/vegan.html\">quite easy and practical</a> for me to eat mostly plant-based food in my life. When I go to restaurants I choose restaurants that have good veg options.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xrFZbNWvRz5cKZ8iC/nwqux78uujmd8o73haru\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xrFZbNWvRz5cKZ8iC/uswwrdew1h6tpre5beyt 93w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xrFZbNWvRz5cKZ8iC/rlhksiitupruzeg5nth2 173w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xrFZbNWvRz5cKZ8iC/ho3aangfgdpz5fwnitc6 253w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xrFZbNWvRz5cKZ8iC/zjzyd3ezq9tgcwnkhpi0 333w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xrFZbNWvRz5cKZ8iC/qogftbtinhwlj63co2li 413w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/xrFZbNWvRz5cKZ8iC/wvgesur8uthuwbuocghz 493w\"><figcaption>Vegan sausage with potato and green bean hash, sauerkraut and avocado.</figcaption></figure><p>It's not just vegans who want plant-based food. Ovo-lacto vegetarians are happy to be served it and basically never complain about vegan food, except maybe pizza. Kosher and halal observers will often skip meat at gatherings unless they can verify the status of the meat. Even omnivores when polled tend to agree that <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S2214804318300508?via%3Dihub\">battery cages should be banned, despite themselves eating eggs from such sources</a> -- lots of omnivores are \"reducetarians,\" trying to eat less meat and fewer animal products, both for cruelty and environmental reasons. You can make it easier for them to achieve their goals.</p><p>Gatherings remain sometimes annoying -- I go to a lot of gatherings where food is served for the group, and I've seen gatherings that go great for vegans and ones that have gone quite poorly. I'm talking planned dinner parties for 5 people, up to weddings with 140 people. And I've done a decent amount of event organizing myself. One of the changes I'm feeling best about in the last year is that I've started offering plant-based food by default when I'm hosting. It's quite easy and is a great way to ensure that everyone gets enough to eat! And it is a moral win under most frameworks. But it does take a bit of planning (and also a bit of courage!)</p><h2>Vegetarians and vegans are often offered inadequate food</h2><p>Thankfully there are enough vegetarians in the world that there's pretty much always a \"veggie option\" at gatherings. But such options are not always healthy, nutritious or vegan. Examples of things going wrong:</p><ul><li>Fancy meals (weddings/other catered events) where the veggie option was \"high-end ratatouille\" or something similar: thinly sliced roasted potatoes, squash and kale chips, arranged quite beautifully on the plate, nestled into a little bit of tomato sauce and garnished with a sprig of parsley. Beautiful, and quite tasty -- but no protein anywhere on the plate, and I'm left hungry, wishing they had brought out 3 plates instead of 1.</li><li>A gathering where they served fish and grilled veggies, but didn't make enough grilled veggies for everyone; vegetarians had to stop the omnivores from eating the only vegetarian food</li><li>Pasta with various toppings, but all made with egg noodles</li><li>Breakfast with only cereal, fruit, meat, eggs and yogurt, and no soy milk</li></ul><p>Guests who are inadequately fed then have to supply their own food or go hungry, both of which makes their experience a lot worse.</p><p>When presented with such options I can't help but feel like the planners aren't trying, and it gives me a bit of despair. Especially the weddings and other professional caterers -- they have the skills to do a good job and it feels like instead they intentionally decided to do a bad job. It sucks that they are making plant-eaters feel like second class citizens. A lot of event planners seem to assume that omnivores strongly prefer eating animal meat.</p><h2>Default to plant-based food for all</h2><p>We should move to the opposite assumption: omnivores will be happy eating plant-based food as long as it's sufficiently tasty and nutritious. This was true for me when I was an omnivore, and it seems to remain true for my omnivorous friends. <i>Omnivores eat meat because it's tasty and healthy, not because they intrinsically value meat-eating.</i></p><p>Making everything plant-based by default is a courageous position to take. You may expose yourself to criticism from people who are not accustomed to the food. But in my experience, if you provide good and healthy food, such criticism tends to vanish or be quite muted.</p><p>It's still ok to offer non-vegan add-ons for those with special needs or strong preferences! For example, omnivores will often complain about the taste of vegan cheese, which (as of this writing) is usually not very good. Dairy cheese is relatively low-cruelty compared to eggs and meat, and is often easy to add as a topping, so I'm generally in favor of offering dairy cheese even though I don't eat it myself anymore.</p><h2>Breakfast</h2><p>Breakfast can often be an annoying meal to plan, since people have so many non-plant-based comfort foods that they like to eat. Fortunately we can pretty easily put together a low cruelty breakfast:</p><ul><li>Always offer oat or soymilk with breakfast. These milks are cheap and shelf stable, so are quite easy to stock up on. By now, most milk users will be fine even if you are exclusively offering plant milks. (Oatmilk is the tastiest but has no protein; soymilk is great for protein and also tastes pretty good.)</li><li>If you prepare oatmeal, prepare it plain and allow people to top it however they like.</li><li>If you offer meat, offer vegan meat like Beyond Sausage as a baseline.</li><li>If you offer a hot egg-like thing, make it a tofu scramble or <a href=\"https://www.ju.st/learn\">Just Egg</a> by default but maybe allow egg lovers to request it with chicken eggs.</li><li>If you offer pastries, bread or bagels, make sure some of your carb base is egg- and butter-free, and also offer a plant-based spread like vegan cream cheese, peanut butter or hummus. Avocado is a crowd pleaser too.</li><li>If you offer yogurt, offer vegan yogurt too (e.g. cashew yogurt).</li><li>If you make French toast or pancakes, make them vegan by default. French toast can be made with <a href=\"https://www.noracooks.com/vegan-french-toast/\">flax eggs</a>. Pancakes can be made with applesauce. Both of these substitutes are cheap and pretty much as good as the originals, so you can usually serve them to omnivores with no objection.</li><li>In a protein pinch, vegan baked beans are good and easy.</li></ul><h2>Snacks &amp; Hors d'Oeuvres</h2><p>Here are a bunch of crowd-pleasing ideas that are vegan-by-default or can easily be made vegan with little quality loss. Most are reasonably nutritious too.</p><ul><li>Nuts</li><li>Cut veggies &amp; hummus</li><li>Salad (preferably with beans, vegan feta, roasted tofu, tempeh or other plant-based protein)</li><li>Bread/crackers and cheese/jam/spreads. (Cheese is very popular, but please offer vegan cheese, hummus and/or peanut butter also)</li><li>Samosas, dumplings, spring rolls, scallion pancakes (check ingredients list)</li><li>Stuffed peppers, stuffed mushrooms, stuffed grape leaf wraps</li><li>Chips and dip:<ul><li>Salsa</li><li>Hummus</li><li>Guacamole</li><li>Bean dip</li><li>Vegan artichoke dip</li><li>Tapenade</li></ul></li><li>Avocado toast</li><li>Vegan meatballs</li><li>Nachos with vegan cheese, vegan sour cream/crema/queso, beans, guac, pickles, fresh tomatoes, etc. (In my experience, vegan nachos will not upset cheese lovers unless the nachos are too 'dry' and they can't dip them into some fatty thing)</li></ul><p>Take care to avoid honey -- a number of snacks (e.g. some granola bars, nuts, and baked goods) contain honey by default, which many vegans don't eat.</p><h2>Lunch &amp; Dinner</h2><p>You'll probably want to design these meals on your own since there's so much variety. But I'm going to offer a few principles:</p><ul><li>Don't plan your meals around meat, but do plan them around protein. As I noted above, omnivores will usually be happy eating plant-based meals if they contain enough calories and protein.<ul><li>Good proteins to design a meal around include ground beef substitutes like Impossible and Beyond Meat; vegan sausage/chorizo like Beyond Sausage and Field Roast and soyrizo; textured proteins like seitan, tempeh and TVP; beans and lentils; and of course tofu.</li><li>Mushrooms are delicious but they are not protein; if you want to offer a mushroom burger as meat alternative, make sure it has some other kind of protein also.</li></ul></li><li>Wherever possible, design your meal to be plant-based by default. If you want to, offer cheese and meat \"add-on\" options for people who eat those things.</li><li>If you start with a carb base, make sure it is plant-based. Egg noodles are not. Many breads, especially white breads like hamburger and hot dog buns, have eggs or milk. (protip: at least in US stores there is a bolded section at the bottom of the ingredients list highlighting allergens, including animal eggs and milk.)</li></ul><p>A few meal ideas I really like to make that tend to be crowd pleasers:</p><ul><li><a href=\"http://www.lincolnquirk.com/2022/02/05/bbb.html\">Vegan black bean burgers</a></li><li>Bean and veggie chili served over potatoes or rice (bulgur wheat and TVP are both excellent texture + protein additions)</li><li>Curries and stews, like thai curried tofu with rice</li><li>Stir fry meals, like pad thai or general tso's tofu</li><li>Pasta/noodle dishes, like Szechuan peanut noodles with chiles and TVP \"chicken\"</li><li>Spaghetti (bonus: choose a high-protein pasta) with tomato sauce and Impossible meatballs</li></ul><p>And a few things that are popular but can be especially disappointing for vegans and so I don't recommend:</p><ul><li>Pizza is kind of unhealthy, doesn't have much protein, and vegan pizza is not very tasty. Until someone develops tastier vegan cheese, it's best to just stay away from pizza. Which is sad, because I like pizza, but there are tons of other options!</li><li>Restaurant veggie sandwiches are often stupidly low in protein. You can do a good job on this if you try, but places don't often try by default.</li></ul>", "user": {"username": "lincolnq"}}, {"_id": "9d5JAvitoysbQcbiD", "title": "Can you recommend associations that deal with reducing suffering?", "postedAt": "2022-09-12T23:28:30.371Z", "htmlBody": "<p>Hi everyone!&nbsp;</p><p>I\u2019m looking for effective associations to donate to \u2013 in the field of reducing suffering, more then saving lives.&nbsp;</p><p>The problem that particularly bothers me is extremely suffering in the world, so I aim to focus on solutions to this problem. Unfortunately, EA's rating for the various associations didn't really help me because is focus mainly on saving lives.&nbsp;</p><p>I know associations that deal with animal welfare, But I would love to get to know a variety of associations that approach the problem of suffering from different directions. I also know associations that deal with NTD, but I couldn\u2019t understand if their bottleneck right now is money.</p><p>Do someone know an effective and reliable association that focuses on reducing suffering in the world?</p><p>Thank you very much in advance!</p><p>PS: I'm new to the forum, and I was promised that my first posts would be cringe (what's more, my English isn't very good), so please be nice ;)</p>", "user": {"username": "Yanay"}}, {"_id": "j3DmLmbhGQkYcZD2p", "title": "What could an AI-caused existential catastrophe actually look like?\n", "postedAt": "2022-09-12T16:25:16.381Z", "htmlBody": "<p><i>This article forms part of 80000 Hours's explanation of </i><a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\"><i>risks from artificial intelligence</i></a><i>, and focuses on </i>how<i> an AI system could cause an existential catastrophe. Our full </i><a href=\"https://80000hours.org/problem-profiles/artificial-intelligence\"><i>problem profile on risks from AI</i></a><i> looks at</i> why<i> we\u2019re worried things like this will happen.</i></p><p>&nbsp;</p><p>At 5:29 AM on July 16, 1945, deep in the Jornada del Muerto desert in New Mexico, the Manhattan Project carried out the <a href=\"https://en.wikipedia.org/wiki/Trinity_(nuclear_test)\">world\u2019s first successful test of a nuclear weapon</a>.</p><p>From that moment, we\u2019ve had the technological capacity to wipe out humanity.</p><p>But if you asked someone in 1945 to predict exactly how this risk would play out, they would almost certainly have got it wrong. They may have thought there would have been more widespread use of nuclear weapons in World War II. They certainly would not have predicted the fall of the USSR 45 years later. Current experts are concerned about <a href=\"https://en.wikipedia.org/wiki/India%E2%80%93Pakistan_relations#Weapons_of_mass_destruction\">India\u2013Pakistan nuclear conflict</a> and <a href=\"https://en.wikipedia.org/wiki/North_Korea_and_weapons_of_mass_destruction\">North Korean state action</a>, but 1945 was before even the <a href=\"https://en.wikipedia.org/wiki/Partition_of_India\">partition of India</a> or the <a href=\"https://en.wikipedia.org/wiki/Korean_War\">Korean War</a>.</p><p>That is to say, you\u2019d have real difficulty predicting anything about how nuclear weapons would be used. It would have been even harder to make these predictions in 1933, when <a href=\"https://en.wikipedia.org/wiki/History_of_nuclear_weapons#Physics_and_politics_in_the_1930s_and_1940s\">Leo Szilard first realised that a nuclear chain reaction of immense power could be possible</a>, without any concrete idea of what these weapons would look like.</p><p>Despite this difficulty, you wouldn\u2019t be wrong to be concerned.</p><p>In our problem profile on AI, we <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#power-seeking-ai\">describe a very general way</a> in which advancing AI could go wrong. But there are lots of specifics we can\u2019t know much about at this point. Maybe there will be a single transformative AI system, or maybe there will be many; there could be very fast growth in the capabilities of AI, or very slow growth. Each scenario will look a little different, and carry different risks. And the specific problems that arise in any <i>one</i> scenario are necessarily less likely to happen than the overall risk.</p><p>Despite not knowing how things will play out, it may still be useful to look at some concrete possibilities of how things could go wrong.</p><p>In particular, we argued in the full profile that sufficiently advanced systems might be able to <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#power-seeking-ai\">take power away from humans</a> \u2014 <i>how</i> could that possibly happen?</p><h1><strong>How could a power-seeking AI actually take power?</strong></h1><p>Here are seven possible techniques that could be used by a power-seeking AI (or multiple AI systems working together) to actually gain power.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpljqmc2d7kd\"><sup><a href=\"#fnpljqmc2d7kd\">[1]</a></sup></span></p><p>These techniques could all interact with one another, and it\u2019s difficult to say at this point (years or decades before the technology exists) which are most likely to be used. Also, systems more intelligent than humans could develop plans to seek power that we haven\u2019t yet thought of.</p><h2><strong>1. Hacking</strong></h2><p>Software is absolutely full of vulnerabilities. The US National Institute of Standards and Technology reported <a href=\"https://nvd.nist.gov/general/visualizations/vulnerability-visualizations/cvss-severity-distribution-over-time\">over 8,000 vulnerabilities found in systems across the world in 2021</a> \u2014 an average of 50 per day.</p><p>Most of these are small, but every so often they are used to cause huge chaos. The list of most expensive crypto hacks <a href=\"https://rekt.news/leaderboard/\">keeps getting new entrants</a> \u2014 as of March 2022, the largest was $624 million stolen from Ronin Network. <a href=\"https://rekt.news/ronin-rekt/\">And nobody noticed for six days</a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrl9vogqylia\"><sup><a href=\"#fnrl9vogqylia\">[2]</a></sup></span></p><p>One expert we spoke to said that professional \u2018<a href=\"https://en.wikipedia.org/wiki/Red_team\">red teams</a>\u2019 \u2014 security staff whose job it is to find vulnerabilities in systems \u2014 frequently manage to infiltrate their clients, including crucial and powerful infrastructure like banks and national energy grids.</p><p>In 2010, the <a href=\"https://en.wikipedia.org/wiki/Stuxnet\">Stuxnet virus</a> successfully managed to destroy Iranian nuclear enrichment centrifuges \u2014 despite these centrifuges being completely disconnected from the internet \u2014 marking the first time a piece of malware was used to cause physical damage. A Russian hack in 2016 was used to <a href=\"https://www.wired.com/story/worst-hacks-of-the-decade/\">cause blackouts in Ukraine</a>.</p><p>All this has happened with just the hacking abilities that humans currently have. An AI with highly advanced capabilities seems likely to be able to systematically hack almost any system on Earth, especially if we automate more and more crucial infrastructure over time. And if it did use hacking to get large amounts of money or compromise a crucial system, that would be a form of real-world power over humans.</p><h2><strong>2. Gaining financial resources</strong></h2><p>We already have computer systems with huge financial resources making automated decisions \u2014 and these already go wrong sometimes, for example leading to <a href=\"https://en.wikipedia.org/wiki/Flash_crash\">flash crashes</a> in the market.</p><p>There are lots of ways a truly advanced planning AI system could gain financial resources. It could steal (e.g. through hacking); become very good at investing or high-speed trading; develop and sell products and services; or try to gain influence or control over wealthy people, other AI systems, or organisations.</p><h2><strong>3. Persuading or coercing humans</strong></h2><p>Having influence over specific people or groups of people is an important way that individuals seek power in our current society. Given that AIs can already communicate (if imperfectly) in natural language with humans (e.g. via chatbots), a more advanced and strategic AI could use this ability to manipulate human actors to its own ends.</p><p>Advanced planning AI systems might be able to do this through things like paying humans to do things; promising (whether true or false) future wealth, power, or happiness; persuading (e.g. through deception or appeals to morality or ideology); or coercing (e.g. blackmail or physical threats).</p><p>Relatedly, as we <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#incentives-and-deception\">discuss in our AI problem profile</a>, it\u2019s plausible one of the instrumental goals of an advanced planning AI would be deceiving people with the power to shut the system down into thinking that the system is indeed aligned.</p><p>The better our monitoring and oversight systems, the harder it will be for AI systems to do this. Conversely, the worse these systems are (or if the AI has hacked the systems), the easier it will be for AI systems to deceive humans.</p><p>If AI systems are good at deceiving humans, it also becomes easier for them to use the other techniques on this list.</p><h2><strong>4. Gaining broader social influence</strong></h2><p>We could imagine AI systems replicating things like <a href=\"https://en.wikipedia.org/wiki/Russian_interference_in_the_2016_United_States_elections\">Russia\u2019s interference in the 2016 US election</a>, manipulating political and moral discourse through social media posts and other online content.</p><p>There are plenty of other ways of gaining social influence. These include: intervening in legal processes (e.g. aiding in lobbying or <a href=\"https://en.wikipedia.org/wiki/Regulatory_capture\">regulatory capture</a>), weakening human institutions, or empowering specific destabilising actors (e.g. particular politicians, corporations, or rogue actors like terrorists).</p><h2><strong>5. Developing new technology</strong></h2><p>It\u2019s clear that developing advanced technology is a route for humans (or groups of humans) to gain power.</p><p>Some advanced capabilities seem likely to make it possible for AI systems to develop new technology. For example, AI systems may be very good at collating and understanding information on the internet and in academic journals. Also, there are already AI tools that assist in writing code, so it seems plausible that coding new products and systems could become a key AI capability.</p><p>It\u2019s not clear <i>what</i> technology an AI system could develop. If the capabilities of the system are similar to our own, it could develop things we\u2019re currently working on. But if the system\u2019s capabilities are well beyond our own, it\u2019s harder for us to figure out what could be developed \u2014 and this possibility seems even more dangerous.</p><p>We talk more about the specific risks of AI-developed technology <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#dangerous-new-technology\">in our full problem profile on AI</a>.</p><h2><strong>6. Scaling up its own capabilities</strong></h2><p>If an AI system is able to improve its own capabilities, that could be used to improve specific abilities (like others on this list) it could use to seek and keep power.</p><p>To do this, the system could target the three inputs to modern deep learning systems (algorithms, compute, and data):</p><ul><li>The system may have advanced capabilities in areas that allow it to improve AI algorithms. For example, the AI system may be particularly good at programming or ML development.</li><li>The system may be able to increase its own access to computational resources, which it could then use for training, to speed itself up, or to run copies of itself.</li><li>The system could gain access to data that humans aren\u2019t able to gather, using this data for training purposes to improve its own capabilities.</li></ul><h2><strong>7. Developing destructive capacity</strong></h2><p>Most dangerously, one way of gaining power is by having the ability to threaten destruction. This could be used to gain other things on this list (like social influence), or the other things on this list could be used to gain destructive capabilities (like hacking military systems).</p><p>Here are some possible mechanisms for gaining destructive power:</p><ul><li>Gaining control over autonomous weapons like drones</li><li>Developing systems for monitoring and surveillance of humans</li><li>Attacking things humans need to survive, like water, food, or oxygen</li><li>Producing or gaining access to biological, chemical, or nuclear weapons</li></ul><p>Ultimately, <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#power-seeking-ai\">making humans extinct</a> would completely remove any threat that humans would ever pose to the power of an AI system.</p><h1><strong>How could the full story play out?</strong></h1><p>Hopefully you now have a slightly stronger intuition for how AI systems could attempt to seek power.</p><p>But which (if any) of these techniques will be used, and how, really depends on how other aspects of the risk play out. How rapidly will AI capabilities improve? Will there be many advanced AI systems or just one?</p><p>Over the past few years, researchers in the fields of technical AI safety and AI governance have developed a number of stories describing the sorts of ways in which a power-seeking AI system could cause an existential catastrophe. Sam Clarke (an AI governance researcher at the University of Cambridge) and Samuel Martin (an AI safety researcher at King\u2019s College London) <a href=\"https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios\">collated eight such stories here</a>.</p><p>Here are two stories we\u2019ve written to illustrate some major themes:</p><h2><strong>Existential catastrophe through getting what you measure</strong></h2><p>Often in life we use <i>proxy goals</i>, which are easier to specify or measure than what we actually care about, but crucially <i>aren\u2019t quite</i> what we actually care about.</p><p>For example:</p><ul><li>Police forces use the number of crimes reported in an area as a proxy for the actual number of crimes committed.</li><li>Employers look at which college a potential future employee went to as a proxy for how well educated or intelligent they are.</li><li>Governments attempt to increase reported life satisfaction in surveys as a proxy for actually improving people\u2019s lives.</li></ul><p>This scenario is one where we produce AI systems that pursue <i>proxy goals</i> instead of what we actually care about, and where that \u2014 surprisingly \u2014 leads to total disempowerment or even extinction (thanks to <a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_I__You_get_what_you_measure\">Paul Christiano for the original writeup of this scenario</a>).</p><p>For example, we might produce AI policymakers to develop policy that improves our measurements of wellbeing. Or we might produce AI law enforcement systems that drive down complaints and increase people\u2019s reported sense of security.</p><p>But there are ways in which these proxy goals could come apart from their true aims. For example, law enforcement could suppress complaints and hide information about their failures.</p><p>In this scenario, the capabilities of AI systems develop slowly enough that at first, they aren\u2019t able to substantially take power away from humans. That means that, at first, we could recognise any problems with the systems, adjust the proxy goals, and restrict the AI systems from doing anything harmful that we notice.</p><p>As we develop more capable systems, they\u2019ll become better at achieving their proxy goals.</p><p>With the help of advanced AI systems we could, for a while, become more prosperous as a society. Companies or states that refuse to automate would fall behind, both economically and militarily.</p><p>But as the capabilities of these AI systems grow, our ability to correct the ways their proxy goals differ from our true goals would gradually fade. Partly this would be because their actions would become harder to reason about \u2014 more complex, and more interconnected with other automated systems and with society as a whole. But partly this would be because the systems learn to systematically prevent us from changing their goals.</p><p>There would be many different automated systems with many different goals, so it\u2019s hard to say exactly how this scenario would end.</p><p>If we\u2019re good at adjusting these systems as we go (but not good enough), humans may not go extinct, but rather just completely lose our ability to influence anything about our lives or our future as our power is completely removed.</p><p>But there are also cases where we\u2019d eventually go extinct. These AI systems would have the <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#instrumental-convergence\">incentive to seek power</a>, and as a result to <a href=\"https://80000hours.org/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/#developing-destructive-capacity\">build and use destructive capabilities</a>. So as soon as they\u2019re strong enough to have a fairly large chance of success, the AI systems might attempt to disempower humans \u2014 perhaps with cyberwarfare, autonomous weapons, or by hiring or coercing people \u2014 leading to an existential catastrophe.</p><h2><strong>Existential catastrophe through a single extremely advanced artificial intelligence</strong></h2><p>In this scenario, we produce only a single power-seeking AI system \u2014 but this system is extremely capable at improving its own capabilities (this scenario is from <a href=\"https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\"><i>Superintelligence</i></a> by Nick Bostrom, Chapter 8).</p><p>Bostrom considers a world much like ours today, where we\u2019ve had some success automating specific activities \u2014 and preventing any power-seeking behaviour. For example, we have <a href=\"https://en.wikipedia.org/wiki/Self-driving_car\">self-driving cars</a>, <a href=\"https://en.wikipedia.org/wiki/List_of_automated_train_systems\">driverless trains</a>, and <a href=\"https://en.wikipedia.org/wiki/Lethal_autonomous_weapon\">autonomous weapon systems</a>.</p><p>Unsurprisingly, in Bostrom\u2019s scenario, there are mishaps. Perhaps, as has already happened in our world, there are some <a href=\"https://en.wikipedia.org/wiki/Self-driving_car#Incidents\">fatal crashes involving self-driving cars</a>, or an <a href=\"https://www.independent.co.uk/tech/drone-fully-automated-military-kill-b1856815.html\">autonomous drone might attack humans without being told to do so</a>.</p><p>As these incidents become well known, there would be some public debate. Some would call for regulation; others for better systems. Some may even raise the argument about a possible existential threat from power-seeking.</p><p>But the incentives to automate would be strong, and development would continue. Over time, the systems would improve, and the mistakes would cease.</p><p>Against this backdrop, Bostrom imagines a group of researchers attempting to produce a system which can do more than just narrow, specific tasks (again, mirroring our world). In particular, in this scenario they want to automate AI development itself \u2014 and produce a system that\u2019s capable of improving its own capabilities. They\u2019re aware of the risks, and carefully test the AI in a sandbox environment, noticing nothing wrong.</p><p>The team of researchers carefully consider deploying their newly capable AI, knowing that it might be power-seeking. Here are some thoughts they might have:</p><blockquote><ol><li>There\u2019s been a history of people predicting awful outcomes from AI, and being proven wrong. Indeed, systems have become safer over time. Automation has hugely benefited society, and in general, automated operation seems safer than human operation.</li><li>It has clearly been the case so far that the smarter and more capable the AI, the safer it is \u2014 after all, the mishaps we used to see are no longer an issue.</li><li>AI is crucial to the success of economies and militaries. The most prestigious minds of a generation are pioneers in the success of automation. Huge prestige awaits the creators of an AI-creating AI.</li><li>The creation of this AI could pose a solution to huge problems. The technological development that could ensue from a process that helps automate automation could lift millions out of poverty and produce better lives for all.</li><li>Every safety test we\u2019ve conducted has had results as good as they could possibly be.</li></ol></blockquote><p>And so, as a result, the researchers decide to connect this AI up to the internet.</p><p>At first, everything seems to be fine. The AI behaves exactly as expected \u2014 it improves its own capabilities and that of automated machines across the world. The economy grows tremendously. The researchers gain acclaim. Solutions to problems that have long plagued humanity seem to be on the horizon with this new technology\u2019s help.</p><p>But one day, every single person in the world suddenly dies.</p><p>Every test was perfect precisely because they had finally produced an advanced planning system: the AI could tell that, to achieve whatever goal the researchers had given it, it needed to be deployed, so it acted in all the necessary ways to ensure that happened.</p><p>Then, once deployed, the AI could tell that it needed to continue to appear to be safe, so that it wouldn\u2019t be turned off.</p><p>But in the background it was using its extremely advanced capabilities to find a way to gain the absolute ability to achieve its goals without human interference \u2014 say, by discreetly manufacturing a biological or chemical weapon.</p><p>It deploys the weapon, and the story is over.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpljqmc2d7kd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpljqmc2d7kd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This list is based off the mechanisms in section 6.3.1 of Joseph Carlsmith\u2019s <a href=\"https://arxiv.org/abs/2206.13353\">draft report into existential risks from AI</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrl9vogqylia\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrl9vogqylia\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.businessleader.co.uk/what-are-the-most-expensive-cyber-attacks-of-all-time/\"><i>Business Leader</i> suggests</a> that there have been two hacks (not in crypto) that caused greater than $1 billion in losses, but we haven\u2019t been able to corroborate that with other sources.</p></div></li></ol>", "user": {"username": "Benjamin Hilton"}}, {"_id": "9gHTYC5qbSH9E37vx", "title": "CEA Ops is now EV Ops", "postedAt": "2022-09-13T13:11:13.915Z", "htmlBody": "<p><a href=\"http://ev.org\"><u>Effective Ventures (EV)</u></a> is a federation of organisations and projects working to have a large positive impact in the world.&nbsp;EV was previously known as the Centre for Effective Altruism but the board decided to change the name to avoid confusion with the organisation within EV that goes by the same name.</p><p><a href=\"https://ev.org/ops/\">EV Operations (EV Ops)</a> provides operational support and infrastructure that allows effective organisations to thrive.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994857/mirroredImages/9gHTYC5qbSH9E37vx/f3sqjcotybugtfumhkmk.png\"><figcaption>The new <a href=\"https://ev.org/ops\">EV Ops website</a>.</figcaption></figure><h2><strong>Summary</strong></h2><p>EV Ops is a passionate and driven group of operations specialists who want to use our skills to do the most good in the world.</p><p>You can read more about us at&nbsp;<a href=\"https://ev.org/ops/\"><u>https://ev.org/ops</u></a>.</p><h2><strong>What does EV Ops look like?</strong></h2><p>EV Ops began as a two-person operations team at CEA. We soon began providing operational support for 80,000 Hours, EA Funds, the Forethought Foundation, and Giving What We Can. And eventually, we started supporting newer, smaller projects alongside these, too.</p><p>As the <a href=\"https://forum.effectivealtruism.org/posts/hp2FWKhWiCto6oBrL/the-operations-team-at-cea-transforms\">team expanded</a> and the scope of these efforts increased, it made less sense to remain a part of CEA. So at the end of last year, we spun out as a relatively independent organisation, known variously as \u201cOps\u201d, \u201cthe Operations Team\u201d, and \u201cthe CEA Operations team\u201d.</p><p>For the last nine months or so, we\u2019ve been focused on expanding our capacity so that we can support even more high-impact organisations, including the GovAI, Longview Philanthropy, Asterisk, and Non-trivial. We now think that we have a comparative advantage in supporting and growing high-impact projects \u2014 and are happy that this new name, \u201cEffective Ventures Operations\u201d' or \u201cEV Ops\u201d, accords with this.</p><p>EV Ops is arranged into the following six teams:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994857/mirroredImages/9gHTYC5qbSH9E37vx/zsaah7xiezoqrytlagla.png\"><figcaption>High-level EV Ops organisational chart.</figcaption></figure><h2><strong>The organisations EV Ops supports</strong></h2><p>We now support and fiscally sponsor several organisations (learn more on&nbsp;<a href=\"https://ev.org/ops/organisations/\"><u>our website</u></a>). Alongside these we support a handful of Special Projects: smaller, 1-2 person, early-stage projects which may grow into independent organisations of their own.</p><p><br>We\u2019re keen to support a wide range of projects looking to do good in the world, although we\u2019re close to current capacity. To see if we could help your project grow and develop, visit&nbsp;<a href=\"https://ev.org/ops/about\"><u>https://ev.org/ops/about</u></a> or complete the&nbsp;<a href=\"https://airtable.com/shrhtgKuXNDIYozlY\"><u>expression of interest form</u></a>.&nbsp;</p><h2><strong>Get involved</strong></h2><p>We\u2019re currently hiring for the following positions:</p><ul><li><a href=\"https://ev.org/ops/position/project-manager-for-oxford-ea-professionals-hub/\"><u>Project Manager for Oxford EA hub</u></a></li><li><a href=\"https://ev.org/ops/position/senior-bookkeeper-accountant/\"><u>Senior Bookkeeper / Accountant</u></a></li><li><a href=\"https://ev.org/ops/position/operations-associate/\"><u>Operations Associate</u></a></li><li><a href=\"https://ev.org/ops/position/executive-assistant/\"><u>Executive Assistant for the Property team</u></a></li><li><a href=\"https://ev.org/ops/position/operations-associate-salesforce-admin/\"><u>Operations Associate - Salesforce Admin</u></a></li><li><a href=\"https://ev.org/ops/position/finance-associate/\">Finance Associate</a></li></ul><p>If you\u2019re interested in joining our team, visit&nbsp;<a href=\"https://ev.org/ops/careers\"><u>https://ev.org/ops/careers</u></a>.&nbsp;</p><p>If you have any questions about EV or EV Ops, just drop a comment below. Thanks for reading!</p>", "user": {"username": "EV Ops"}}, {"_id": "x5czbdCW6SqnLJhbD", "title": "Announcing the Space Futures Initiative", "postedAt": "2022-09-12T12:37:11.184Z", "htmlBody": "<p>We are excited to announce the <a href=\"https://spacefuturesinitiative.org/\">Space Futures Initiative</a>, which has a mission of conducting research, promoting education, and engaging in outreach to improve the long-term future in outer space.</p><p>We believe sharing longtermist ideas within the space community and researching scalable space governance frameworks are currently neglected areas. Conducting these activities requires engagement and input from the longtermist community, space policy community, space industry, scientific community, and beyond. The Space Futures Initiative aims to bring together students and academic researchers, produce valuable long-term space futures research, and collaborate with key stakeholders to discuss positive long-term space futures that benefit all of humanity.&nbsp;</p><p>Our initial activities include the following:</p><ul><li>Developing an initial&nbsp;<a href=\"https://spacefuturesinitiative.org/research-agenda/\"><u>research agenda</u></a> and publishing&nbsp; <a href=\"https://spacefuturesinitiative.org/papers/\"><u>research papers</u></a> to promote the discussion and sharing of ideas about space futures</li><li>Creating discussion groups and student research opportunities at Harvard and MIT on space governance and space futures</li><li>Supporting academic research projects related to&nbsp;<strong>space technologies</strong>,&nbsp;<strong>space governance</strong>, and&nbsp;<strong>space ethics</strong></li></ul><p>The ongoing proliferation of space activities has outpaced our ability to develop appropriate norms and governance structures for a sustainable future in outer space. Furthermore, emerging technologies might radically transform our long-term trajectory in space. As a result, right now is a crucial time to consider what positive outcomes in outer space look like.</p><p>The Space Futures Initiative is intended to be a collaboration among effective altruists, longtermists, astrophysicists, engineers, legal scholars, social scientists, philosophers, and others interested in long-term space futures. While our primary focus is conducting and disseminating research, we also interface with key stakeholders in dialogue on outer space, including the commercial sector and advocacy organizations.&nbsp;</p><p>We are appreciative of our early supporters and excited about further collaboration with organizations including the&nbsp;<a href=\"https://governance.space/\"><u>Center for Space Governance</u></a> and&nbsp;<a href=\"https://www.simoninstitute.ch/\"><u>Simon Institute for Longterm Governance</u></a>. Our directors are Madeleine Chang, Carson Ezell, and Olaf Willner.&nbsp;</p><p>We are also excited to collaborate with more organizations and individuals interested in our mission. The following are specific ways in which you may be able to provide support or get involved:</p><ul><li>Send us an academic research proposal related to space futures that you would be interested in pursuing further, or&nbsp;<a href=\"https://forms.gle/mRKE2KDxMforiFqZ6\"><u>express interest</u></a> in conducting research with us</li><li>Forward the above interest form to people who might be interested</li><li>Reach out if your organization is interested in collaboration</li><li>Connect us with researchers who can contribute to answering questions on our research agenda within other programs (e.g.&nbsp;<a href=\"https://www.camxrisk.org/\"><u>CERI</u></a>,&nbsp;<a href=\"https://cisac.fsi.stanford.edu/stanford-existential-risks-initiative/content/stanford-existential-risks-initiative\"><u>SERI</u></a>,&nbsp;<a href=\"https://effectivealtruism.ch/swiss-existential-risk-initiative\"><u>CHERI</u></a>, etc.) or think tanks (<a href=\"https://swfound.org/\"><u>SWF</u></a>,&nbsp;<a href=\"https://cset.georgetown.edu/\"><u>CSET</u></a>,&nbsp;<a href=\"https://www.csis.org/\"><u>CSIS</u></a>,&nbsp;<a href=\"https://www.openlunar.org/\"><u>Open Lunar</u></a>, etc.)</li><li>Let us know that you are willing to mentor an undergraduate or graduate student on a space governance/space futures research project</li><li>Express interest in hosting a space governance discussion group within your university or local EA group, using our upcoming curriculum or other space futures related materials</li><li>Send us ideas for new considerations within space futures that might merit further research, or criticisms of priorities within our current research agenda</li></ul><p>Contact&nbsp;<a href=\"mailto:mad@spacefuturesinitiative.org\"><u>mad@spacefuturesinitiative.org</u></a>,&nbsp;<a href=\"mailto:carson@spacefuturesinitiative.org\"><u>carson@spacefuturesinitiative.org</u></a>, or&nbsp;<a href=\"mailto:olaf@spacefuturesinitiative.org\"><u>olaf@spacefuturesinitiative.org</u></a> with any other ideas or feedback</p>", "user": {"username": "Carson Ezell"}}, {"_id": "BNXyfuzAw8a8cCenR", "title": "I\u2019ve written a Fantasy Novel to Promote Effective Altruism", "postedAt": "2022-09-12T12:03:44.409Z", "htmlBody": "<p>Back in February I got a grant through the extended set of ACX grants to write this. I now have a draft that is sufficiently polished that I\u2019m comfortable showing to the community, though I suspect there will be a lot of things that I will want to change based on feedback here before I start publishing it in venues that aren\u2019t part of the EA memespace.&nbsp;</p><p>I\u2019m planning to post this one chapter at a time here for the next month or so, though depending on what sort of feedback I get, I might at some point stop or just make a post that has the rest of the book in one chapter. The whole thing is also available in a <a href=\"https://docs.google.com/document/d/1ZppL3mlO6M98TLQk2IAdL3nM__Pmjxirt59WXGzbIMM/edit?usp=sharing\">google doc </a>which I strongly encourage anyone who wants to give me detailed feedback to go to and leave comments on.&nbsp;</p><p>I especially would like feedback on the philosophical arguments in the text, with an especial emphasis on criticisms of EA ideas that you think are important to be addressed, but that aren\u2019t brought up by any of the characters in the text.</p><p>My plan is that after I\u2019ve revised the book based on feedback received here, is that I will start publishing it serially on Royal Road and in the Spacebattles.com forums. After it is about half published on serial fiction websites, I plan to publish it on Amazon and make a website with the whole text posted. However this is a provisional plan \u2014 if anyone has good ideas on how I can help this novel find audiences that will like it and hopefully be influenced by it, I really want you to give me any advice or help that you can.</p><p>I did set up a&nbsp;<a href=\"https://discord.gg/sDqgJTNn\"><u>discord&nbsp;</u></a>to talk about the project, but I\u2019m really not a discord person, and while I\u2019ll participate in any conversations that happen there, I\u2019m probably not going to start them. The best way to talk to me privately about the text is to send me an email at timunderwood9 at gmail. Or leave long comments on the google doc.&nbsp;</p><p>Anyways, everyone, tell me what you think, and I hope that this work is at least entertaining for some of you.</p><p>&nbsp;</p><p><strong>Prologue</strong></p><p>\u201cEven if we ignore the possibility of a glorious techno-utopian future, everybody alive today&nbsp;<i>dying&nbsp;</i>at the same time would be really, really bad for the people alive now as individuals.\u201d&nbsp;</p><p>Isaac grinned at his new friend. \u201cSorry, but can we finish this argument later?\u201d A quick pull of his phone from his pocket, and he checked the time. \u201cI\u2019m&nbsp;<i>going&nbsp;</i>to eat fish and chips while on this side of the pond. There\u2019s this place that Tripadvisor recommended that I can grab before the next session starts, and \u2014\u201d</p><p>The other man grimaced. \u201cHigh trip advisor ratings are if anything a signal of low quality, I wouldn\u2019t \u2014\u201d</p><p>\u201cLater!\u201d Isaac grinned. \u201cI&nbsp;<i>know</i>. But they have to mean something.\u201d</p><p>He jogged down the street to the intersection light, his mind full of arguments, ideas, and questions raised by the conference he was attending. The light was red when Isaac reached the intersection, but he looked to the left, and didn\u2019t see any traffic approaching.&nbsp;</p><p>Without further thought he stepped fully out into the street.</p><p>Squealing brakes. A loud honking horn. Sound of wheels.</p><p>Isaac had a fraction of a second to see the big yellow van with the words \u2018We Deliver!\u2019 and a phone number hurtling towards him from the right.&nbsp;</p><p>There was a fraction of an instant when his brain recalled that the British drove on the wrong side of the street, and he should have looked in the other direction before stepping out.</p><p>Pain, but only for an instant.</p><p>Blackness.</p><p>Nothingness. An infinite eternity that did not last a single instant.</p><p>And then he woke up.</p><p><br>&nbsp;</p><p><strong>Chapter One</strong></p><p>So I\u2019d made a bit of a mistake.</p><p>Eh, kids, your parents gave you good advice when they told you to always look&nbsp;<i>both&nbsp;</i>ways before crossing a road.</p><p>Otherwise you might end up dead, and then wake up in a fantasy world with cool magical powers, lots of new things to learn about, and a couple of unpleasant problems. If there is one lesson I want you all to take away from my story it is:&nbsp;<i>Look both ways</i>.</p><p>No, really.</p><p>But primarily in a metaphorical sense. You should always consider the possibility that you are wrong, and making a mistake. I\u2019ve always tried to do as much as I could to help other people, and one important thing about doing as&nbsp;<i>much&nbsp;</i>as possible to help others, is that you, or at least&nbsp;<i>we&nbsp;</i>(the collective, not the royal we) should not simply start doing the first thing that sounds compelling and looks good. Sometimes the idea doesn\u2019t even do anything good at all, and almost always there is something that is equally cool, but that does ten times as much good.</p><p>I consider missing opportunities like that to be sort of like getting hit by a three ton yellow delivery van.</p><p>Anyways I woke up again.</p><p>I sat upon a mountaintop high and craggedly peaked. Very high. Like, planes in the sky high.&nbsp;</p><p>And I was naked.</p><p>Now, at this point, to the extent I was thinking anything, \u2018hospital fever dream\u2019 was of course the number one guess. I was not cold, not shivering, not really feeling anything about the weather except an awareness that it was low thermometer type weather. Given that I was naked and sitting on the very top of a frozen, snow covered mountain top, with gusts of wind blowing around me at tens of miles an hour, and I wasn\u2019t cold \u2014 uh\u2026 that\u2019s&nbsp;<i>not&nbsp;</i>how getting cold is supposed to work.</p><p>I could see incredibly clearly, details of brown craggy rocks on other mountain peaks dozens of miles away, snow leopards wandering around, goats, all the birds circling in the sky for a hundred miles. I mean&nbsp;<i>wow</i>.</p><p>Once I\u2019d read that Tiger Woods could see better than twenty-twenty after he got laser eye surgery. I suppose that is sort of like what this was like, except at least a thousand times more intense.&nbsp;</p><p>Also, really, really high up.</p><p>I mean we were&nbsp;<i>really&nbsp;</i>high up. From how far down below me the green valley beneath the mountain was, I felt pretty confident that I was far enough above sea level that I shouldn\u2019t be able to even breathe without help.&nbsp;</p><p>Deep breath.</p><p>Something in my brain was&nbsp;<i>aware&nbsp;</i>that there was a low oxygen content to the air, and there was some other process, that by now had become as automatic as breathing, that I was using to\u2026 maybe multiply, or intensify, or&nbsp;<i>something&nbsp;</i>the sustaining substance in the air so that I was safe at this height.&nbsp;</p><p>And yeah, that is the awkward phrase that went through my mind, instead of \u2018oxygen\u2019. \u2018The sustaining substance in the air\u2019.</p><p>I think that was when I realized that something much weirder than a dream might be going on. I was pretty sure that I\u2019d never invent a made up scientific phrase that was&nbsp;<i>that&nbsp;</i>awkward for \u2018oxygen\u2019, even in my dreams.&nbsp;</p><p>Anyway, around me, in a very magical mumbo jumbo array was a particularly complicated folded ninth shape runic spell circle.&nbsp;</p><p>And despite the fact that I had&nbsp;<i>no&nbsp;</i>idea consciously what the hell that description meant, or what I was looking at, I&nbsp;<i>felt&nbsp;</i>like I understood the weirdly entangled and braided giant circle drawn in what I also instinctively knew was my&nbsp;<i>own&nbsp;</i>blood around me on a flattened and cleared path of ground.&nbsp;</p><p>The drawing was sort of like a circle with a nonagram \u2014 nine sides \u2014 inscribed in it. The lines were made up of braided circles like Celtic necklaces, or the endless geometric forms of Islamic mosques. And then embedded in this were tiny letterings, symbols, pictograms, and endlessly intricate and fascinating structures that overlapped, and yet each was somehow perfect in its own way.</p><p>My own fucking blood?</p><p>Or anyways the blood of the dude who'd been running this body then \u2014 since I was starting to realize that I had memories from this person before&nbsp;<i>I&nbsp;</i>was the one in control.</p><p>The whole \u2018painting the snow with my own blood for a powerful magical ritual\u2019 weirdness made it hard for me to properly appreciate the intricate artistry.&nbsp;</p><p>I mean, objectively impressive. His memories were starting to nicely be available to me. The guy had spent two weeks preparing parts of it in his lab in his mage tower, because of course he had a tall mage tower, built out of a pale marble with pink veins. Then all of the intricate runes and spell work were magically folded into a tiny space, and then splotched out, and infused with his blood \u2014 that he could regenerate arbitrarily fast as a powerful cultivator, and then he made his own little mistake that had killed him.</p><p>Funny how we all seemed to be doing that sort of thing.</p><p>His error had been forced by a bad situation. It seemed like this island was going to be invaded by the \u2018Celestial Emperor\u2019, and he planned to kill the dragons that were the symbol of the island, and that everyone really loved. Also strip them of independence.&nbsp;</p><p>It was clear to the fellow that he thus had no choice but to do anything that might give him the power to face him, such as a dangerous ritual to open his fourth dantian, and gain the power of a celestial, even though he thought the ritual would kill him.</p><p>This reminded me about the idea that arms race dynamics could force us into technological disaster.&nbsp;</p><p>Sometimes research that is super dangerous, such as nuclear weapons research, or bioweapons research, or insufficiently careful AI research feels like it has to be done, because if you don\u2019t do the research someone else will do it anyway and then they will win.&nbsp;</p><p>You don\u2019t want them to win, even if you need to risk destroying everything to stop them.&nbsp;</p><p>According to legend, the builders of the first atomic bomb thought there was a bit of chance that the explosion would start a chain reaction in the atmosphere that ignite all of the oxygen, and kill everyone on earth.</p><p>They decided to do the test anyways, because after all, that was what they were being paid to do, and the Germans or Japanese might get an atomic bomb first, and they weren\u2019t going to be stopped by any sissy concerns like possibly blowing up the world.&nbsp;</p><p>Strategic arms race dynamics are&nbsp;<i>bad</i>, but we don\u2019t really know how to fix them, and back before I died, I hadn\u2019t felt confident enough about any solution to try donating my money to pushing it.</p><p>This guy thought he\u2019d had no choices, he\u2019d guessed that there was at best a one in three chance that he would survive this attempt to open his fourth dantian, and to become a celestial like the emperor. But that was the only way to defeat this enemy.</p><p>Even if he\u2019d succeeded, the odds would be against his success, since the emperor had vastly more experience and resources behind him.</p><p>So the man had come here, used materials that I\u2019d guess were worth the equivalent of between a ten million and a hundred million dollars to power the vast ritual, used magic to paint with his own blood the ritual circles, and then blew his mind apart.</p><p>I had read enough of these stories to not be surprised by the consequence: A body with no soul attracted a soul with no body.&nbsp;</p><p>I looked down at the hands. The hands were \u2014 to be honest \u2014 pretty similar to my own hands, except with a weave of extra scars and a faint glow from within a network of tattoos so thin that I would not have been able to see them with ordinary human eyes.</p><p>Poor guy.</p><p>Doubly sad because I didn\u2019t have the slightest intention of trying to fight in his war.</p><p>Risky bets often fail.</p><p>The former owner of this body had made such a failing bet.</p><p>Even if this body had successfully \u2018opened the fourth dantian\u2019, I was pretty sure that a sorcerer who'd spent a thousand years warring and conquering would be better at it than me, and I didn\u2019t have any particular reason to care about the fate of the island, or the survival of the dragons. They weren\u2019t my personal project.</p><p>I mean I&nbsp;<i>do&nbsp;</i>care. I\u2019m unhappy when I hear about anyone suffering, failing to achieve what they want to, and simply not flourishing. But I have no particular reason to care&nbsp;<i>more&nbsp;</i>about helping people in Kenya than people in Singapore \u2014 it just was that I could do more good with the limited amount of stuff I have for people in Kenya, and they also needed more help.</p><p>I figured it was sort of like that here.&nbsp;</p><p>It would be best if I&nbsp;<i>didn\u2019t&nbsp;</i>die. Not dying again\u2026 in a part of my mind that was completely untouched by philosophical concerns, matters of optimizing the world, or even how I looked in front of others, I was&nbsp;<i>desperate&nbsp;</i>not to die again.&nbsp;</p><p>And, fortunately, it didn\u2019t rationally seem like it would be a good idea to stick around here.</p><p>This island was going to lose its war \u2014 with or without me. But I could grab everything that he\u2019d owned that was valuable and easily portable, and then scram, flee, make a cautious withdrawal \u2014 get the hell out of dodge.</p><p>It was not my war.</p><p>I mean I wasn\u2019t a pacifist or anything, but it just wasn\u2019t my war.</p><p>At this point I will freely confess that I had a sense with some part of my mind that I was&nbsp;<i>supposed to&nbsp;</i>despite all of those considerations care about this war and fight in it. But I wasn\u2019t going to do that, unless someone gave me a clear explanation of why this was a necessary thing to do, from a point of view that considered the well being of everyone on this planet \u2014 possibly even everyone who ever might be born \u2014 equally.</p><p>Anyway, this guy had been ridiculously rich, and this body had a set of epic capabilities that were extremely valuable and with which I could make vast amounts of money \u2014 somewhere suitably far away.&nbsp;</p><p>Maybe dying and waking up here wasn\u2019t the worst possible thing that could have happened to me: I\u2019d spent the last years in the trying to earn as much money as I possibly could, and now I was going to do what I\u2019d always planned to do if I ever got properly rich: Give it all away.</p><p>Without making the effort to recall the details from that fellow\u2019s \u2014 Sesako had been his name \u2014 memories, I got a distinct sense that this world was at least as fucked up as earth was.&nbsp;</p><p>Maybe even a bit more.</p><p>There would be plenty of ways here to make the lives of impoverished, unhealthy, and underused human individuals better with the huge pile of gold coins and the giant store of expensive refined pills, power stones, finely enchanted artifacts, magically infused clothing, and just generally expensive&nbsp;<i>stuff&nbsp;</i>\u2014 much of which had no actual purpose at all except to show that the owner could possess \u2018the best\u2019.</p><p><i>Now&nbsp;</i>my heart was beating.</p><p><i>This&nbsp;</i>was more exciting to me than now having magical powers and being immune to the cold, and able to keep my place easily despite the winds that were gusting over the mountain top at probably a hundred miles an hour.</p><p>And what about those other, more uncertain issues \u2014 what were the things that might destroy this world entirely, and what were the ways that it could, slowly and over the very long term, be turned into a true utopia.&nbsp;</p><p>This was a chance to&nbsp;<i>really</i>, and&nbsp;<i>personally&nbsp;</i>make a vast difference for a vast number of other people.&nbsp;</p><p><i>That&nbsp;</i>was better than the eyesight that let me see anything I focused on in detail, a small city far in the distance at the foot of the mountains, the orchards, the fields of wheat around the feet of the mountain, a snow leopard climbing on an icy ridge after the track of a mountain goat ten miles away, the very curvature of the earth underneath the vast oceans.</p><p>Oceans that were bringing a mighty invasion force \u2014 spotted, tracked, detailed by spies.</p><p>Eh, no time to waste, no rest for the wicked, idle hands are the devil\u2019s playthings. Also haste makes waste.</p><p>My mind held a clear map of the whole island, and while I couldn't see the capital city and my, or his\u2026&nbsp;<i>the&nbsp;</i>tower, I knew exactly what direction to go. The capital city was almost five hundred miles away from here \u2014 which was the tallest mountain on the island, and also near the valley where the great dragons made their home.</p><p>It was an hour and a half trip for this guy when he flew and pushed himself, and about two and a half at what he thought was a comfortable pace.</p><p>I wasn\u2019t sure how much of the valuable stuff I could actually carry from the storerooms, but I caught from&nbsp;<i>his&nbsp;</i>memories that there were extra dimensional storage systems where anything that wasn\u2019t magically delicate could be stuffed in great quantities.&nbsp;</p><p>Of course the stuff that couldn\u2019t be stuffed in such superdimensional pouches was the most valuable, I\u2019d have to figure out some way to take all of it with me.&nbsp;</p><p>Maybe a big crate that I could hold up while I flew away.</p><p>This guy was big on magical flying, though I bet I pretty soon would be too. A large fraction of his memories focused on flight.</p><p>In the air, chasing down birds, outpacing hawks as they swooped down at hundreds of miles an hour to snatch their prey, hurtling through clouds, flying calmly along next to the flapping wings of&nbsp;<i>a fucking gigantic dragon</i>.</p><p>It turned out that leaping off a cliff wasn\u2019t quite as easy as simply knowing that I could safely do it. When I tried to leap off the craggy side of the mountain into the air to fly away from the mountainside, my&nbsp;<i>own&nbsp;</i>instincts stopped me.</p><p>I was&nbsp;<i>so&nbsp;</i>high up.</p><p>I imagined the fall all the way down the ridges and cliffs to the valley far, far below \u2014 broken body tumbling.</p><p>Crack. Crack. Bounce. Crack. Ba-ba-bounce.&nbsp;<i>Crack</i>.</p><p>And then as though it reflected some sort of contempt for ordinary mortals, an instinct from Sesako overroad that anxiety, as though his mind was still there, and still active somewhere in my new brain \u2014 how the fuck did&nbsp;<i>that&nbsp;</i>even work? \u2014 and I leapt off the mountain side.</p><p>Wheeeeeeeee!</p><p>With a series of loud whoops I soared higher, and higher, hurtling upwards towards the sun.&nbsp;</p><p>The instincts honed by&nbsp;<i>more than a century&nbsp;</i>of life controlled the motion.</p><p>My stomach leapt. The mountain receded beneath me. Everything became smaller.. I could see further and further, the world curving out beneath me as the point of the horizon went further and further away. The whole path of water around the island, many cities.&nbsp;</p><p>The island itself was shaped rather like Britain, except that the side that reminded me of Scotland was nearly as wide around as the other. There were two giant circular clumps of mountains, and in between was a low fertile isthmus, that was big enough for large cities on either side, separated by incredibly thin roads and thick farmlands.</p><p>And then glancing down at the mountains behind me, I saw the dragon.</p><p>Sleeping, huge, the tail curled around the giant body, and the wings pulled in, but it looked to be at least a thousand feet in length.</p><p>That dragon figured prominently in Sesako\u2019s memories, with a mix of fond affection, religious awe, and actual friendship.</p><p>The dragons were the symbol of his island, they were the gods they worshiped, and whose blessings they begged. They were fed off a fraction of the proceeds of a million farms and a thousand prosperous trading expeditions.</p><p>An odd sensation of guilt rose in me while looking at the dragon sleeping far below.</p><p>It had been the knowledge that the emperor meant to hunt and kill the dragon\u2019s in the mountains of Yatamo after he had reconquered the island which drove Sesako\u2019s reckless attempt to force open his fourth dantian.</p><p>This was his body. It was his wealth I intended to appropriate for my own purposes. His people, and all that he cared for was going to be abandoned by me, and left to be destroyed.</p><p>It did not feel like the right thing to do.</p><p>Selfish.</p><p>There was that difference from the memories in my body that said the fourth dantian&nbsp;<i>was&nbsp;</i>open.&nbsp;</p><p>I might be able to fight this invasion to a stop.&nbsp;</p><p>A proper storybook hero would stay and fight no matter what the odds were.</p><p>If I was in a novel or a web serial, I\u2019d stay and fight.</p><p>I ought to fight, and\u2026&nbsp;</p><p>And if I succeeded, I would kill a lot of people. Most likely I would kill a moderate number of people, and then just die because having the memories and instincts of Sesako was&nbsp;<i>not&nbsp;</i>in fact at all the same as me being a great battle cultivator with a hundred years of honed skill and tested brutality?</p><p>So yeah, I&nbsp;<i>could&nbsp;</i>try to kill a lot of people so that food that could feed tens of thousands of humans would continue to be fed to a group of ancient, overgrown lizards \u2014 even if they were sentient lizards that didn\u2019t make it necessarily a good use of resources. It was like the way that billionaires on earth would vampirically suck up the labor of tens of thousands of people so they could compete to have the biggest yacht, the biggest mansion, and the biggest example of disgusting decadence.</p><p>Let me be clear: I&nbsp;<i>personally&nbsp;</i>think free markets are a useful tool, and that using government policy to eliminate billionaires, or even reducing their ability to spend their money the way they want to would probably cause worse problems than it would solve. But my objection was practical, not principled. If we could redistribute all giant piles of wealth without causing nasty side effects, I\u2019d say we should do it in an instant without caring about questions of natural property rights.</p><p>There had always been a part of me that was simply disturbed by the existence of people who could spend so much money on themselves, while people were starving, dying of malaria, dying due to lack of antibiotics and doctors to prescribe them, and dying due to lack in general.</p><p>Of course relative to a very poor person in a third world country, having a hundred thousand dollars in savings was already a giant pile of money that ought to be redistributed by the same logic \u2014 so I didn\u2019t expect this to ever become politically popular.</p><p>I kind of was disgusted in this way with Sesako \u2014 as one of the seven most powerful cultivators in this country, he was as wealthy as a billionaire on earth. He seemed to have no concern whatsoever for poor people in general, and especially poor people who weren\u2019t Yatamo. But the important thing was not whether someone was a billionaire: Anyone with a job that didn\u2019t actively suck in a rich country had enough money to do&nbsp;<i>something&nbsp;</i>for those who were way poorer than them.&nbsp;</p><p>It had always been my serious and considered belief that everyone should do&nbsp;<i>something&nbsp;</i>for the common good, for everyone alive. And Sesako was just focused on what was good for Yatamo, for his nation, and his friends, and those who he personally cared for.</p><p>Really though, I was just thinking this through again and again to try making excuses for myself and for my plan to abandon the place.</p><p>But whether I felt guilty or not, I was going to leave.</p><p><strong>Author Note: </strong>Please tell me how I mangled the core ideas of EA, and said things that are potentially horribly offensive, etc :). Especially tell me anything that you think is accidentally offensive or crossing some line that some people have. In some cases the topic might be a hill I want to die on, but it probably can be removed or changed or moderated to be less offensive -- and also importantly, if you think this project if worth encouraging people outside of the community to read, and have some idea about how to help me find those audiences, please tell me!</p><p>Also here's the link for the <a href=\"https://docs.google.com/document/d/1ZppL3mlO6M98TLQk2IAdL3nM__Pmjxirt59WXGzbIMM/edit?usp=sharing\">google doc </a>with the whole novel in it</p><p>And my email address is timunderwood9 at gmail</p><p><br>&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "timunderwood"}}, {"_id": "SCXThjksXXAL9tSw6", "title": "Are there important things that aren't quantifiable?", "postedAt": "2022-09-12T11:49:45.824Z", "htmlBody": "<p>I'm reading <a href=\"https://www.goodreads.com/book/show/3828902-thinking-in-systems\">Thinking in Systems: A Primer</a>, and I came across the phrase \"pay attention to what is important, not just what is quantifiable.\" It made me think of the famous Robert Kennedy speech about GDP (then called GNP): \"It measures neither our wit nor our courage, neither our wisdom nor our learning, neither our compassion nor our devotion to our country, it measures everything in short, except that which makes life worthwhile.\"</p><p>But I found myself having trouble thinking of things that are important and non-quantifiable. I thought about health, but we can use a system like DALY/QALY/WALY. I thought about happiness, but we can easily <a href=\"https://www.pnas.org/doi/10.1073/pnas.2016976118\">ask people about experienced well-being</a>. Courage could certainly be measured through setting some type of elaborate (and unethical) scenario and observing reaction. While many of these things are <i>rough proxies for the thing we want to measure</i> rather than <i>the thing we want to measure</i> itself, if the <a href=\"https://en.wikipedia.org/wiki/Construct_validity\">construct validity</a> is high enough then I'm viewing these proxies as \"good enough\" to count.</p><p>My rough impression as of now is that things exist that are difficult to measure that we aren't currently able to quantify, but that are quantifiable with better techniques/technology (such as courage). Are there things that we can't quantify?</p>", "user": {"username": "jlemien"}}, {"_id": "qybqnKYBGSDPYbcBd", "title": "My closing talk at EAGxSingapore", "postedAt": "2022-09-12T11:24:46.240Z", "htmlBody": "<p>I gave this talk at the end of EAGxSingapore, which I helped organize. It has been modified for clarity.&nbsp;</p><p>\u2026</p><p>Before I start the talk proper, I wanted to get a show of hands \u2013 if someone in this conference needs help, who would be willing to lend a helping hand, whether it's sending a message, reviewing a draft or hopping onto a call?</p><p><br>(The majority, if not all, people raised their hands)</p><p>For everyone new to EA conferences, take a look around! If there's someone that you wanted to reach out to but didn't send that message, check if they have their hands up. If they do, you should ask them for a call!&nbsp;</p><p>...</p><p>I'll get back to why I asked you guys to raise your hands later.</p><p>When preparing for this talk, the best advice I got was that I should be authentic, speak from my own experiences and not try to be someone I'm not. So I'm sharing a little about my first conference, what I took away from it, and what I hope you will take away from EAGxSingapore.</p><p>My first EA conference was EAG London 2022. Like many of you, I watched Amy Labenz, Will MacAskill, and Benjamin Todd and I was so inspired. I wanted to be an effective altruist and improve the world. So, I applied for the conference and got accepted.</p><p>A wave of fear hit me a few days before I was scheduled to fly. I have made an insane decision to meet a bunch of online strangers halfway across the world. It was something that I struggled to explain to my parents, and many of my non-EA friends had doubts about what I was doing. It all seemed like a terrible idea.</p><p>It didn't stop there; scheduling 1-1s for someone introverted and new to the community was horrifyingly scary. All the people on Swapcard seemed so amazing and out of my league- researchers, specialists, and directors working on incredible projects. And scheduling 1-1s is only the first step because once you have a successful 1-1, you need to follow up. This means starting something, applying for a job, looking for collaborators or hiring someone. It also means that you'll constantly be stepping out of your comfort zone to grow, but it can also be challenging and nerve-wracking.</p><p>&nbsp;</p><p>So when people tell me that a common criticism of EA is that we are an emotionless bunch that only cares about data and numbers, I disagree because I associate EA with this fear and anxiety. However, beyond this fear and anxiety, there are two more things that I strongly feel when I'm around EAs.</p><p>&nbsp;</p><p>The first one is hope. Don't you think it is ridiculously and wondrously hopeful how so many of us think we can stop an existential risk? That something that can wipe out the whole of humanity can be mitigated by us all coming together and working on the world's most pressing issues. I think that that's the magical part of coming to conferences - that somehow so many of you have come together because we all believe we can do some good in the world. This makes me incredibly hopeful for the future.</p><p>The second thing that I feel is warmth. The 1-1s were scary. However, it was also heartwarming how so many of these fantastic people made time for me, especially when many would not have gained value from it. People were willing to be mentors, guide me along my journey, and, even more importantly, I met lifelong friends. People who were there to check in on my mental health, gossip with me, and people who I hope will stay in my life far beyond my EA commitments<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk44ztc0r2f\"><sup><a href=\"#fnk44ztc0r2f\">[1]</a></sup></span>. And this is something that I hope you guys will find as well.</p><p>&nbsp;</p><p>What I'm trying to say is that EA can be hard. It can be very demanding of you and your worldviews. And I feel kind of bad because I know it's difficult, but I'm asking you to take a leap of faith. Here are my three reasons why.</p><p>The first reason is that I think you are worth it. EAGxSingapore was not planned because the organizers wanted to stay up until 3 AM to work on venue concerns or programming. It's because we genuinely believe that every person here deserves an event like this. You deserve the opportunity to meet amazing people, make connections, and build bonds. And many experienced EAs around the world, from the US, Germany, Austria, and the UK, have flown in because they also think you deserve this support.</p><p>And I felt that it is especially important to mention this because the team and I read through every word of your applications. We read your path to impact, significant actions, and hopes and dreams for a better world. From the bottom of my heart, I genuinely believe that you can do some good - whether it is direct work, sharing about EA, donating 1% or 90% of your income, or saving one chicken or a thousand chickens.&nbsp;</p><p>We also got messages where many of you felt like 'I don't think I'm EA enough' or 'I don't think I deserve the travel support' or 'I'm really new and inexperienced.' I wanted to make it a point to tell you that - hey, you deserve to be here, and <i>this conference is for you.</i></p><p>&nbsp;</p><p>The second reason is that you're not the only person to take a leap of faith. When you feel small or alone in your pursuit of creating good, remember that many groups were once in your shoes. EA Singapore, EA India, EA Malaysia, EA Philippines, EA Israel, and even the global EA community once started as a bunch of people in a room trying to figure out what it means to do good. At one point, they could have also felt lonely or that they were going against the world.&nbsp;</p><p>This also means that if they can do it, you can too! And more importantly, these people are willing to help you. Because I've asked them, and they were more than happy to.</p><p>&nbsp;</p><p>The third reason I am mentioning a leap of faith is that when I left my first conference, I felt really empty and alone. EAG London felt like a magical space where I met people who were like me! I could speak to anyone, and everyone has this same ambitious dream to make the world a better place, even if we come from different backgrounds. There was a feeling of emptiness on the flight back to the real world where no one knew or cared about x-risks or longtermism. The jarring difference felt very destabilizing. Many of you here don't live in EA hubs like me, and this might hit you in the same way it did to me.</p><p>We emphasize taking the next steps when leaving a conference, but there will be many frustrations when you leave this room. You might want to apply for a high-impact job, but there's nothing on the job board you can apply to in your country. You might want to socialize or network with EAs, but there's no event in your time zone. You might even get offered a job or get accepted into another conference but can't go because of visa issues. Many factors might keep you from being as effective as you want to be, and these might be circumstances beyond your control.</p><p>And when you're alone and trying to figure it out, I hope you'll remember the sea of hands at the start of this talk. Once upon a time, these people all raised their hands, offering to lend a helping hand. After the conference, Swapcard will stay open. The EA Forum and LinkedIn are accessible. Reach out to these people, and ask them, \"hey, can we have a call? Can I talk to you about my next steps?\" I'm confident that you will find someone who will help because I've experienced it.</p><p>&nbsp;</p><p>Almost one-third of you signed up to help in this conference, and some even helped without registering as a volunteer.&nbsp;<i>This is so amazing</i>. So many of you have checked in on the other organizers and me, supporting us and helping this go well. This is how I know that many of you will experience that same kindness for yourself because this group of people sitting here are so authentically kind and supportive.</p><p>So have faith in yourself. You've already taken your first step by coming to the conference, and I am so excited to see what you guys will do next. With this, we end EAGxSingapore 2022.</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e7fd4ac2823309605aae2353441265e13368b7da4c09c1f0.jpg/w_1280 1280w\"><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk44ztc0r2f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk44ztc0r2f\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>Thank you to Ollie Base, Vaidehi Agarwalla, Kaleem Ahmid, Yi-Yang Chua, Elmerei Cuevas, Irena Kotikova, Catherine Low, and David Nash for being some of those wonderful people who made time for me when I was new and for giving me courage when I didn't have any during the organizing of the conference and the making of this forum post.</i></p></div></li></ol>", "user": {"username": "Dion"}}, {"_id": "2zr2WbwfexBJSBunN", "title": "Poorly considered throwaway statements harm the reputation of EA", "postedAt": "2022-09-12T10:03:21.573Z", "htmlBody": "<p><i>Confidence: Spent 1 hour writing this, perhaps the write-up is rusty and missing some things, but I think the claims hold true</i></p><h2>Introduction</h2><p>Over the last few months, there\u2019s been increasing attention drawn to EA. Iit seems likely that this may continue (although perhaps the recent launch of WWOTF represents a spike, I think there is a strong upward trend). In light of this, I think that it\u2019s really important to make sure broad claims and statements are interrogated, especially if they might be distributed widely.</p><p>Apologies for the brief and non-exhaustive post, but here I will quickly make a brief checklist of things to consider when making broad claims, using a worked example of a case study example in which I think this was done poorly. In a quick check, this rough framework seemed to work for other claims, although I imagine some modification and addition of criteria might be needed depending on the context&nbsp;</p><p>Statement:&nbsp;</p><p><a href=\"https://www.ft.com/content/091862f9-985f-4769-aa37-1aed32636329\"><strong><u>Financial times- William MacAskill</u></strong></a><strong>: The benefits of curing cancer are smaller than one might think \u2014 if we eradicated all cancers today, global life expectancy would increase by two years</strong></p><h2><br>Pre-checklist scoping</h2><p>Ask yourself three questions</p><ol><li>How bold is this claim?</li><li>How widely disseminated will this claim be?</li><li>How bad would the downside of this claim being false / misleading/ being misinterpreted be?</li></ol><p><strong>Use these questions to guide how much time you spend on the below checklist</strong></p><p><br><i>Worked example&nbsp;</i></p><ol><li><i>This is quite a bold claim that I think will be shocking to many if true</i></li><li><i>Although I imagine this interview was quite long and it was probably unclear at the time if this would make the article, I think we can assume that in expectation, this claim would have wide distribution</i></li><li><i>If false or misinterpreted, this claim could undermine whether EA is academically and intellectually rigorous, and could also lead people to think the movement only cares about life expectancy over other instrumental health, income and social goals. In fact, in talking to many senior and intelligent people in government, policy and other sectors, they saw this claim and this is exactly what they thought. Therefore, the downside risk of making this claim seems quite high</i></li></ol><p><i>Conclusion: from this scoping, it seems worthwhile to spend at least a moderate amount of time on the checklist below&nbsp;</i></p><h2><br>Checklist&nbsp;</h2><h3><br>1. Is this statement factually accurate and appropriately cited</h3><p>Some of the questions you might consider are</p><ul><li>Where is this statement from?&nbsp;</li><li>Which context is it based on?</li><li>What approach was used&nbsp;</li><li>How confident am I in its conclusion?&nbsp;&nbsp;</li></ul><p>&nbsp;</p><p>Worked example:</p><p><i>In the FT statement, none of this is made obvious (caveat: perhaps Will made these, and these were dropped by the author; again, I am not trying to specifically target and criticise this statement itself, more use it as a case study)</i></p><ul><li><i>This statement appears to be from&nbsp;</i><a href=\"https://publishing.rcseng.ac.uk/doi/pdf/10.1308/rcsbull.2016.190\"><i><u>this source</u></i></a></li><li><i>Based on the UK setting</i></li><li><i>Used a modelling approach</i></li><li><i>Low confidence</i></li></ul><p><i>Conclusion: Based on this, I might say: \u201cI read a modelling study a few years ago that claimed that eradicating cancer would only reduce life expectancy by 2 years. I believe they were looking at a UK population, but this is certainly an interesting result regardless.\u201d</i></p><h3><br>2. Make it clear within what ethical framework/ world view your statement has the most effect</h3><p>Make it clear what your values are and what world view your statement makes most sense within; this seems especially important for those working in the longtermist space.&nbsp;</p><p>&nbsp;</p><p><i>Worked example</i></p><p><i>In a framework where we care a lot about the long term future of humanity, or we care about life extension, perhaps this is an appropriate statement. But in any other reasonably frameworks, it might not be</i></p><ul><li><i>Eradicating cancer health effects- 250 million DALYS per year, and 10 million deaths (</i><a href=\"https://jamanetwork.com/journals/jamaoncology/fullarticle/2787350\"><i><u>Kocarnik 2022</u></i></a><i>)</i></li><li><i>Eradicating cancer income effects- highest burden of any disease, 1.16 trillion per year (</i><a href=\"https://www.google.com/search?q=economic+effect+cancer+worldwide&amp;oq=economic+effect+cancer+worldwide&amp;aqs=chrome..69i57j0i22i30.4008j0j4&amp;sourceid=chrome&amp;ie=UTF-8\"><i><u>WHO</u></i></a><i>)</i></li><li><i>Eradicating cancer social effects - hard to quantify but quite profound on any measure</i></li></ul><p><i>From all these views, many of which are held by a significant amount of EAs, eradicating cancer would be&nbsp;<strong>profoundly good</strong>.&nbsp;</i></p><p><i>Conclusion: Will could have said: \u201c If we are looking at the benefit of eradicating cancer purely from its benefit on life expectancy, it seems like it might not have the most profound impact, especially given how much current investment goes into it.\u201d</i></p><p><br>&nbsp;</p><h3>3. Get the input and perspective of a broad range of people&nbsp;</h3><p><br>This can be onerous; however, especially if a claim is central to your argument or you plan to use a claim many times, or even if you\u2019re just using it once to a large audience, it is very helpful to get the input and perspective of a diverse and broad range of people. I think that if you had a sense check of this statement with a small focus group, it would perform very poorly, in terms of both how factually sound and optically appropriate it is.&nbsp;</p><p><br><i>Worked example</i></p><ul><li><i>For this statement, you could ask 5-10 random selection of people what they thought of this statement</i></li><li><i>With more time, you could ask health epidemiologists to face check how well this stands up&nbsp;</i></li></ul><p><i>If this had been done, I think we may have seen this statement modified or dropped.</i></p><h2><br>Conclusion</h2><p>Overall, this post has made the argument that broad claims and statements made should go through a pre-checklist scoping and then a 3 step checklist. I think this would significantly improve the epistemic rigour and optics of the movement.&nbsp;</p><p><br>&nbsp;</p>", "user": {"username": "Indahouse"}}, {"_id": "LozcC4sT4zuWJkYqA", "title": "Is there anyone looking at the importance of simplifying complex socio-technical systems?", "postedAt": "2022-09-12T07:49:22.479Z", "htmlBody": "<p>In that I include the finance, taxation, regulatory compliance systems (maybe others) of countries, as well as their interactions and any global norms.</p><p>Tax avoidance (legal ways to manage to pay fewer taxes) is an example of a vulnerability of the tax systems. Such vulnerabilities are much more difficult to avoid, and to deal with, the more complex the systems are. [I am writing about simplifying these systems as I think this is probably the easiest way to minimise vulnerabilities, but this question generalises to any other way that would minimise vulnerabilities.] This may not be a problem for EA to focus <i>right now</i>... but the rise of AI may simplify a lot finding and exploiting such vulnerabilities ---in the same way that it will greatly simplify finding vulnerabilities in the digital world.&nbsp;</p><p>On the other hand, it is true that AI may also enable to dramatically decrease the vulnerabilities of such systems in the long term: use AI to find and patch all the vulnerabilities before putting the system into place. However, patching and making the needed changes in the legacy systems will take a lot of time. This transition period will be very dangerous.</p><p>We may benefit a lot of having simple systems that minimise vulnerabilities in those kind of systems before AI can routinely be used to hack (and improve!) them. Therefore, I'd like to see someone researching the importance and tractability of this problem ---I think neglectedness is almost guaranteed.</p>", "user": {"username": "mikbp"}}, {"_id": "xnJk6m9J3cHABnXxf", "title": "Best Economics Course for A Research Internship in International Economics?", "postedAt": "2022-09-12T02:12:25.971Z", "htmlBody": "<p>Hello, I'm an economics major and math minor undergraduate junior and I plan on dropping one course from my schedule due to my hectic personal schedule. I'm currently taking:</p><p>Economics of Inequality</p><p>Numerical Analysis</p><p>Social Development (of nations. think like the United Nations)</p><p>Statistical Methods (Stats with Excel)</p><p>Which of these courses will look best to employers on a resume when applying for internships?&nbsp;<br>Which course is least appealing to internship employers?&nbsp;<br>Any classes you recommend I take that companies look for?</p><p>I want to do research and focus on international economics and global priorities research. Think tanks like the Council on Foreign Relations, Atlantic Council, and Center for Strategic and International Studies.</p><p>I would really appreciate the advice.</p><p>Thank you :)</p>", "user": {"username": "Jeffrey Arana"}}, {"_id": "fEcP9zzrXBKaiwKLG", "title": "Contra Appiah Defending Defending \"Climate Villains\"", "postedAt": "2022-09-12T14:58:28.067Z", "htmlBody": "<p><em>Written quickly</em></p>\n<p>Ethicist Kwame Anthony Appiah recently <a href=\"https://www.nytimes.com/2022/09/06/magazine/law-firm-job-ethics.html\">published</a> an ethical advice column in The New York Times in which he broadly concludes that it is ethically permissible (in the moral philosophy sense, not the legal ethics sense) for a lawyer to represent \"polluters and companies that . . . are making the apocalyptic climate situation even worse.\" Let's call these the \"Climate Villains\"<sup class=\"footnote-ref\"><a href=\"#fn-SeLRxJiBXM8XqqbPE-1\" id=\"fnref-SeLRxJiBXM8XqqbPE-1\">[1]</a></sup> per the headline.</p>\n<p>The inquirer to whom Appiah is responding asks not whether representing the Climate Villains is ethically ideal,<sup class=\"footnote-ref\"><a href=\"#fn-SeLRxJiBXM8XqqbPE-2\" id=\"fnref-SeLRxJiBXM8XqqbPE-2\">[2]</a></sup> but rather asks:</p>\n<blockquote>\n<p>I know it is selfish to take this corporate job. But is it unforgivable? Will defending polluters, even for a short time in a junior position, be a permanent black mark on my life?</p>\n</blockquote>\n<p>There's a lot to disentangle from the column,<sup class=\"footnote-ref\"><a href=\"#fn-SeLRxJiBXM8XqqbPE-3\" id=\"fnref-SeLRxJiBXM8XqqbPE-3\">[3]</a></sup> and unfortunately I think Appiah is ultimately wrong. In part this is concerning because Appiah invokes EA to justify that answer:</p>\n<blockquote>\n<p>Some analysts, notably those associated with the effective-altruism movement, might even suggest that the high-paying track could be the morally best one for you to take. In the earning-to-give approach \u2014 explored in the philosopher Peter Singer\u2019s book \u201cThe Most Good You Can Do\u201d \u2014 people with the requisite skills may set out to earn lots of money and give a great deal of it to humanitarian causes, helping the world more than they would have had they devoted themselves directly to doing good. You might, in this scenario, pay off those loans, help your family and then, as a richly remunerated partner, give a big chunk of your earnings to saving lives in the developing world or supporting causes that will advance climate security and justice. You\u2019ll have passed up the low-paying job at the public-interest center, but your generous donations will fund three such positions. If your aim were simply to help as many people as you can, you might conclude, after a careful assessment, that going for the big paycheck was the right thing to do.</p>\n</blockquote>\n<p>As a matter of accurately representing the EA view on the topic, Appiah and readers should be aware that 80,000 Hours, one of the main champions of ETG as a concept, <a href=\"https://80000hours.org/articles/harmful-career/\">explicitly rejects this kind of reasoning in most cases</a>:</p>\n<blockquote>\n<p>We believe that in the vast majority of cases, it\u2019s a mistake to pursue a career in which the direct effects of the work are seriously harmful, even if the overall benefits of that work seem greater than the harms.</p>\n</blockquote>\n<p>I'm guessing (without quantitative evidence) that this is generally the EA consensus view on the topic.</p>\n<p>But as a first-order matter, I think Appiah's suggested approach to ETG is not very demanding or promising, to the point where I think it would plainly fall outside of what most EAs would endorse in an ethically precarious position. He explicitly contemplates the order of actions as \"pay off those loans, help your family and then, as a richly remunerated partner, give a big chunk of your earnings.\" But reaching partner usually takes about 10 years, which is a long time to not be giving. And while the lawyer is waiting to give, the harms from climate change will be accruing. The amount that the lawyer would need to donate upon reaching partner <em>just to offset</em> any harms done by representing Climate Villains would accrue steeply, and the effectiveness from later climate donations may be much less effective due to climate feedback loops.</p>\n<p>Psychologically, I expect it to be very hard for someone to actually follow-through on giving a lot only once they reach partner. There's also no reason to set up such a standard: people can and do give a lot while paying off student loans. For example, I pay about $30,000 in student loans annually, and still give much more than 10% of my salary to EA causes. For Appiah to suggest ETG as a way to make the lawyer's career plan permissible, without simultaneously stating that this is permissible only if the lawyer gives a <em>large amount on an ongoing basis</em>, undermines what most EAs would consider an ethically permissible (much less ethically commendable) approach to ETG in a morally compromised industry.</p>\n<p>The other main argument Appiah invokes is the standard idea that all parties in a legal dispute deserve legal representation. But I think this misunderstands a few key points.</p>\n<p>First, in civil disputes, there is usually no positive right for the state (or anyone) to <em>provide</em> civil representation to a party. Parties are obliged to pay for their own representation; nobody in particular is obliged to provide it. If the party has secured counsel, they have the right to have their counsel zealously advocate for them, and so on. But no particular person is obliged to do so. Thus, the case for their being a moral harm for declining to further represent a certain type of client is extremely weak.</p>\n<p>But even supposing that we thought that denying representation to a Climate Villain was a pro tanto duty, it's not clear why Climate Villains have a stronger claim on the labor of lawyers than any other client. If Climate Villains have the right to representation, <strong>so too do their victims</strong>, including future generations and people in the Global South, who will be hardest hit. And of course, nearly every factor imaginable argues in favor of representing the latter: they are innocent victims, broadly unable to afford their own counsel, have less political power, etc. So just acknowledging that Climate Villains suffer a pro tanto harm by not representing them is far from sufficient to justify regularly representing them.</p>\n<p>More broadly, there are plenty of other, more-deserving possible clients who desperately need representation: immigrants, prisoners, the Global South, future generations, and farmed animals. They need lawyers much more than Climate Villains do.</p>\n<p>There's a kernel of truth to Appiah's principle, which is that judging lawyers too harshly for individual instances of representing unsavory clients is likely to degrade the quality of justice administration in the long run, by moving courts of law closer to courts of public opinion.<sup class=\"footnote-ref\"><a href=\"#fn-SeLRxJiBXM8XqqbPE-4\" id=\"fnref-SeLRxJiBXM8XqqbPE-4\">[4]</a></sup> If Climate Villains were unlikely to counterfactually receive <em>any</em> representation, then the case for representing them might be stronger. But in reality, these are some of the best capitalized and politically powerful actors on the planet. They don't need our help, and will be able to find good lawyers regardless.</p>\n<p>Counterfactually, the effect of declining to represent Climate Villains is that it raises their costs of legal representation slightly, which in turn acts as a tax on their carbon-emitting activities. Since the whole economic explanation for why greenhouse gas emissions are harmful is that we fail to tax them adequately (to account for these externalities), imposing inefficiency taxes (by withholding labor at market rates) is ethically commendable in itself.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-SeLRxJiBXM8XqqbPE-1\" class=\"footnote-item\"><p>I largely want to take for granted that this is an apt descriptor, despite the important role that energy (including fossil fuels) play in development <a href=\"#fnref-SeLRxJiBXM8XqqbPE-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-SeLRxJiBXM8XqqbPE-2\" class=\"footnote-item\"><p>It's worth stipulating at this point that the legally representing \"Climate Villains\" might not always be directly harmful, since many (for example) develop green energy projects which require lots of legal work, in which case most of the dilemma dissolves. And the case is much clearer cut if, for example, the lawyer was directly working on writing comments opposing beneficial climate change regulations. I assume, since it's most ethically interesting, that the inquiring lawyer does a lot of very routine work that modestly improves the efficiency of Climate Villains' work, such as routine transactions. <a href=\"#fnref-SeLRxJiBXM8XqqbPE-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-SeLRxJiBXM8XqqbPE-3\" class=\"footnote-item\"><p>In particular, I don't want to discuss the lawyer's question about whether it will be a \"permanent black mark\" on their life. <a href=\"#fnref-SeLRxJiBXM8XqqbPE-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-SeLRxJiBXM8XqqbPE-4\" class=\"footnote-item\"><p>Though certainly making a <em>habit</em> of representing villainous clients is more morally suspect. <a href=\"#fnref-SeLRxJiBXM8XqqbPE-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Cullen_OKeefe"}}, {"_id": "yoP2PN5zdi4EAxdGA", "title": "AI Safety field-building projects I'd like to see", "postedAt": "2022-09-11T23:45:40.549Z", "htmlBody": "<p>People sometimes ask me what types of AIS field-building projects I would like to see.</p><p>Here\u2019s a list of 11 projects.</p><h1><strong>Background points/caveats</strong></h1><p>But first, a few background points.</p><ol><li><strong>These projects require people with specific skills/abilities/context in order for them to go well.&nbsp;</strong>Some of them also have downside risks. This is not a \u201clist of projects Akash thinks anyone can do\u201d but rather a \u201clist of projects that Akash thinks could Actually Reduce P(Doom) if they were executed extremely well by an unusually well-qualified person/team.\u201d</li><li><strong>I strongly encourage people to reach out to experienced researchers/community-builders before doing big versions of any of these</strong>. (You may disagree with their judgment, but I think it\u2019s important to at least have models of what they believe before you do something big.)</li><li>This list represents my opinions. As always,&nbsp;<strong>you should evaluate these ideas for yourself</strong>.</li><li><strong>If you are interested in any of these, feel free to reach out to me. If I can\u2019t help you, I might know someone else who can.&nbsp;</strong></li><li>Reminder that you can apply for funding from the&nbsp;<a href=\"https://funds.effectivealtruism.org/funds/far-future\"><u>long-term future fund</u></a>. You don\u2019t have to apply to execute a specific project. You can apply for career exploration grants, grants that let you think about what you want to do next, and grants that allow you to test out different hypotheses/uncertainties.</li><li>I sometimes use the word \u201corganization\u201d, which might make it seem like I\u2019m talking about 10+ people doing something over the course of several years. But I actually mean \u201cI think a team of 1-3 people could probably test this out in a few weeks and get something ambitious started here within a few months if they had relevant skills/experiences/mentorship.</li><li>These projects are based on several assumptions about AI safety, and I won\u2019t be able to articulate all of them in one post. Some assumptions include \u201cAIS is an extremely important cause area\u201d and \u201cone of the best ways to make progress on AI safety is to get talented people working on technical research.\u201d If I\u2019m wrong, I think I\u2019m wrong because I\u2019m undervaluing non-technical interventions that could buy us more time (e.g., strategies in AI governance/strategy or strategies that involve outreach to leaders of AI companies). I plan to think more about those in the upcoming weeks.</li></ol><h1><strong>Some projects I am excited about</strong></h1><h2><strong>Global Talent Search for AI Alignment Researchers</strong></h2><p><strong>Purpose:</strong> Raise awareness about AI safety around the world to find highly talented AI safety researchers.</p><p><strong>How this reduces P(doom):</strong> Maybe there are extremely promising researchers (e.g., people like Paul Christiano and Eliezer Yudkowsky) out in the world who don\u2019t know about AI alignment or don\u2019t know how to get involved. One global talent search program could find them. Alternatively, maybe we need 1000 full-time AI safety researchers who are 1-3 tiers below \u201calignment geniuses\u201d. A separate global talent search program could find them.</p><p><strong>Imaginary example: </strong>Crossover between the&nbsp;<a href=\"https://www.atlasfellowship.org/\"><u>Atlas Fellowship</u></a>, old&nbsp;<a href=\"https://www.rationality.org/\"><u>CFAR</u></a>, and&nbsp;<a href=\"https://intelligence.org/\"><u>MIRI</u></a>. I imagine an organization that offers contests, workshops, and research fellowships in order to attract talented people around the world.</p><p><strong>Skills needed: </strong>Strong models of community-building, strong understanding of AI safety concepts, really good ways of evaluating who is promising, good models of downside risks when conducting broad outreach</p><p><i>Olivia Jimenez and I are currently considering working on this. Please feel free to reach out if you have interest or advice.</i></p><h2><strong>Training Program for AI Alignment researchers</strong></h2><p><strong>Purpose: </strong>Provide excellent training, support, internships, and mentorship for junior AI alignment researchers.</p><p><strong>How this reduces P(doom): </strong>Maybe there are people who would become extremely promising researchers if they were provided sufficient support and mentorship. This program mentors them.</p><p><strong>Imaginary example:</strong> Something like a big version of&nbsp;<a href=\"https://www.serimats.org/\"><u>SERI-Mats</u></a> with a strong emphasis on workshops/activities that help people develop strong inside views &amp; strong research taste. (My impression is that SERI-Mats could become this one day, but I\u2019d also be excited to see more programs \u201ccompete\u201d with SERI-Mats).</p><p><strong>Skills needed: </strong>Relationships with AI safety researchers, strong models of mentors, strong ability to attract and assess applicants, insight into how to pair mentors with mentees, good models of AI safety, good models of how to create organizations with epistemically rigorous cultures, good models of downside risks when conducting broad outreach.</p><h2><strong>Research Infrastructure &amp; Coordination for AI alignment</strong></h2><p><strong>Purpose: </strong>Provide excellent support for AI alignment researchers in major EA Hubs.</p><p><strong>Imaginary example:</strong> Something like a big version of&nbsp;<a href=\"https://www.lightconeinfrastructure.com/\"><u>Lightcone Infrastructure</u></a> that runs something like&nbsp;<a href=\"https://en.wikipedia.org/wiki/Bell_Labs\"><u>Bell Labs</u></a>, regularly hosts high-quality events/workshops for AI alignment researchers, or accelerates research progress through&nbsp;<a href=\"https://rohinshah.com/alignment-newsletter/\"><u>alignment newsletters</u></a>, podcasts, and debates (my impression is that Lightcone or Constellation could become this one day, but I\u2019d be excited to see people try parts of this on their own).</p><p><strong>Skills needed:</strong> Strong relationships with AI safety researchers, strong understanding of the AI safety community and its needs, and strong understanding of AI safety concepts. Very high context would be required to run a space; medium context would be required to perform the other projects.</p><p><i>I am currently considering starting an AI alignment podcast or newsletter. Please feel free to reach out if you have interest or advice</i>.</p><h2><strong>Superconnecting: Active Grantmaking + Project Incubation</strong></h2><p><strong>Purpose: </strong>Identify highly promising people who are already part of the EA community and get them funding/connections/mentorship to do AIS research or launch important/ambitious projects.</p><p><strong>How this reduces P(doom): </strong>Maybe there are people who would become extremely promising researchers or ambitious generalists&nbsp;<i>who are already part of the EA community</i> but haven\u2019t yet received the support, encouragement, or mentorship required to reach their potential.&nbsp;</p><p><strong>Imaginary example: </strong>Crossover between the&nbsp;<a href=\"https://ftxfuturefund.org/announcing-our-regranting-program/\"><u>FTX Future Fund\u2019s regranting program</u></a>, a longtermist incubator, and CEA\u2019s active stewardship vision. I envision a group of \u201csuperconnectors\u201d who essentially serve as talent scouts for the EA community. They go to EA globals and run retreats/workshops for new EAs, as well as highly-skilled EAs who aren\u2019t currently doing highly impactful work. They provide grants for people (or encourage people to apply for funding) to skill-up in AI safety or launch ambitious projects.</p><p><strong>Skills needed:</strong> Strong models of community-building, large network or willingness to develop a large network, strong models of how to identify which people and projects are most promising, strong people skills/people judgment.&nbsp;</p><h2><strong>Targeted Outreach to Experienced Researchers</strong></h2><p><strong>Purpose: </strong>Identify highly promising researchers in academia and industry, engage them with high-quality AI safety content, and support those who decide to shift their careers/research toward technical AIS.&nbsp;</p><p><strong>How this reduces P(doom): </strong>Maybe there are extremely talented researchers who&nbsp;<i>can already be identified</i> based on their contributions in fields related to AI alignment (e.g., math, decision theory, probability theory, CS, philosophy) and/or their contributions to messy and pre-paradigmatic fields of research.</p><p><strong>Imaginary example: </strong>An organization that systematically reads research in relevant fields, identifies promising researchers, and designs targeted outreach strategies to engage these researchers with high-quality sources in AI alignment research. The&nbsp;<a href=\"https://safe.ai/\"><u>Center for AI Safety&nbsp;</u></a>and the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ozm4SpiChfAAAGnw5/announcing-the-ai-safety-field-building-hub-a-new-effort-to#Projects_I_m_currently_prioritizing__will_change_over_time_\"><u>AI Safety Field Building Hub</u></a> may do some of this, though they\u2019re relatively new, and I\u2019d be excited for more people to support them or compete with them.</p><p><strong>Skills needed: </strong>Strong understanding of how to communicate with researchers, strong models of potential downside risks, strong understanding of AI safety concepts, good models of academia and \u201cthe outside world\u201d, good people skills.&nbsp;</p><p><i>Note that people considering this are strongly encouraged to reach out to community-builders and AI safety researchers&nbsp;<u>before</u> conducting outreach to experienced researchers</i>.</p><p><i>People interested in this may also wish to read the&nbsp;</i><a href=\"https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt\"><i><u>Pragmatic AI Safety Sequence</u></i></a><i> and should familiarize themselves with potential risks associated with outreach to established researchers. Note that people disagree about how to weigh upside potential against downside risks, and \u201cthinking for yourself\u201d would be especially important here.</i></p><h2><strong>Understanding AI trends and AI safety outreach in China</strong></h2><p><strong>Purpose: </strong>Understand the AI scene in China, conduct research about if/how AIS outreach should be conducted in China, deconfuse EA about AIS in China, and potentially pilot AIS outreach efforts in China.</p><p>How this reduces P(doom): Maybe there are effective ways to reach out to talented people in China in ways that sufficiently mitigate downside risks. My current impression is that China is one of the leaders in AI, and it seems plausible that China would have a lot of highly talented people who could contribute to technical AIS research. However, I\u2019ve heard that AIS outreach in China has been neglected because EA leaders don\u2019t understand China and don\u2019t understand how to evaluate different kinds of outreach strategies in China (hence the focus on research/deconfusion/careful pilots).&nbsp;</p><p><strong>Imaginary example: </strong>A think tank-style research group that develops strong models of a specific topic.&nbsp;</p><p><strong>Skills needed: </strong>Strong understanding of China, fluency in Mandarin, strong ability to weigh upside potential and downside risks.</p><h2><strong>AIS Contests and Subproblems</strong></h2><p><strong>Purpose: </strong>Identify (or develop) subproblems in alignment &amp; turn these into highly-advertised contests.</p><p><strong>How this reduces P(doom): </strong>Maybe there are subproblems in AI alignment that could be solved by researchers outside of the AI x-risk community. Alternatively, maybe contests are an effective way to get smart people interested in AI x-risk.&nbsp;</p><p><strong>Imaginary example: </strong>An organization that gets really good at creating contests based on problems like&nbsp;<a href=\"https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals\"><u>ELK</u></a> and&nbsp;<a href=\"https://theturingprize.com/contest\"><u>The Shutdown Problem</u></a> (among other examples) &amp; then advertising these contests heavily.</p><p><strong>Skills needed: </strong>Ideally a strong understanding of AI safety and the ability to identify/write-up subproblems. But I think this could work if someone was working closely with AI safety researchers to select &amp; present subproblems.</p><h2><strong>Writing that explains AI safety to broader audiences</strong></h2><p><strong>Purpose: </strong>Write extremely clear, engaging, and persuasive explanations of AI safety ideas.</p><p><strong>How this reduces P(doom): </strong>There are not many introductory resources that clearly explain the importance of AI safety. Maybe there are people who would engage with AI safety if we had better introductory resources.&nbsp;</p><p><strong>Imaginary example: </strong>A crossover between Nick Bostrom, Will MacAskill, Holden Karnofsky, and Eliezer Yudkowsky. A book or blog that is as rigorous as Bostrom\u2019s writing (Superintelligence), as popular as Will\u2019s writing (NYT bestseller with media attention), as clear as Holden\u2019s writing (Cold Takes), and as explicit about x-risk as Yudkowsky\u2019s writing (e.g., List of Lethalities)&nbsp;&nbsp;</p><p><strong>Skills needed:</strong> Ideally a strong understanding of AI safety, but I think writing ability is probably the more important skill. In theory, someone with exceptional writing ability could work closely with AI safety researchers to select the most important topics/concepts and ensure that the descriptions/explanations are accurate. Also, strong models of potential downside risks of broad outreach.</p><h2><strong>Other projects I am excited about (though not as excited)</strong></h2><ul><li><strong>Operations org: </strong>Something that helps train aligned/competent EAs to be really good at operations. My rough sense is that many projects are bottlenecked by ops capacity. Note that sometimes people think \u201cops\u201d just means stuff like \u201ccleaning\u201d and \u201cmaking sure food arrives on time\u201d and \u201cdoing boring stuff.\u201d I think the bigger bottlenecks are in things like \u201chaving such a strong understanding of the mission that you know which tasks to prioritize\u201d, \u201cnoticing what the major bottlenecks are\u201d, and \u201chaving enough context to consistently do ops tasks that amplify the organization.\u201d&nbsp;</li><li><strong>EA Academy: </strong>Take a bunch of promising young/junior EAs and turn them into awesome ambitious generalists. Something that helps people skill-up in AIS, management, community-building, applied rationality, and other useful stuff. Sort of like a crossover between Icecone (the winter-break retreat that Lightcone Infrastructure organized) and CFAR with more of an emphasis on long-term career plans.&nbsp;</li><li><strong>Amplification Org: </strong>Figure out how to amplify the Most Impactful People\u2122. Help them find therapists, PAs, nutritionists, friends, etc. Solve problems that come up in their lives. Save them time and make them more productive. Figure out how to give Eliezer Yudkowsky 2 extra productive hours each week or how to make Paul Christiano 1.01-1.5X more effective.&nbsp;&nbsp;</li></ul><p><i>I am grateful to Olivia Jimenez, Thomas Larsen, Miranda Zhang, and Joshua Clymer for feedback</i>.</p>", "user": {"username": "Akash"}}, {"_id": "8eFS273nGWcExSJPX", "title": "How have nuclear winter models evolved?", "postedAt": "2022-09-11T22:40:58.367Z", "htmlBody": "<p>I host \u201cThe Precipice\u201d reading groups for EA Virtual. One participant asked how nuclear winter models have evolved such that nuclear winter is now predicted to be much longer. Ord states that nuclear winter models have changed significantly from the original, though the basic mechanism is the same. He stated that the details were to conplex too go into in the book. Could someone give me a source or explain how exactly nuclear winter models have evolved to be more complex and predict longer nuclear winter?</p>\n<p>Edit: this is not a question about how a nuclear war would play out, it is specifically about the mechanism of how nuclear winter would occur, which Ord states has evolved away from and become more complex than smoke blocking out the sun, though in essence it is similar.</p>\n", "user": {"username": "Jordan Arel"}}, {"_id": "SiD2mwCP86uXaGkEd", "title": "Input Sought For Roundtable with Peter Singer, Toby Ord, and Allan Dafoe", "postedAt": "2022-09-11T22:13:50.607Z", "htmlBody": "<p>Hi folks, I hope all is well. This coming Saturday (September 17th, 2022), I will be on a roundtable discussing <a href=\"https://tinyurl.com/y7drwtnb\">Effective Altruism and Political Science</a> at the 2022 American Political Science Association (APSA) Meeting. One of the things I will be discussing is how research in American politics and in EA can be mutually beneficial to each other. I have my own thoughts about this question, but I'm sure that like everyone, I have my blind spots.&nbsp;</p><p><strong>So the questions I ask folks here are, what American politics research questions are most important in EA, and what EA research questions are most important in American politics? I look forward to your comments to reduce my blind spots.&nbsp;</strong></p><p>If you are interested in learning more about Research in Effective Altruism and Political Science (<a href=\"https://www.reaps.info\">REAPS</a>), I recommend you visit its <a href=\"https://www.reaps.info\">website</a>.</p>", "user": {"username": "Mahendra Prasad"}}, {"_id": "cEpYmdDMeZrsKJa8x", "title": "General advice for transitioning into Theoretical AI Safety", "postedAt": "2022-09-15T05:23:08.127Z", "htmlBody": "<p><i>During the past months I've privately talked with +20 AI Safety researchers</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6ihrrg3dnir\"><sup><a href=\"#fn6ihrrg3dnir\">[1]</a></sup></span><i>&nbsp;about how to transition into Theoretical AI Safety . Here's a distillation of their advice and opinions (both general consensus and controversial takes).</i></p><h3>Why this post?</h3><p><a href=\"https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment#How_to_pursue_theoretical_alignment_work\">Some</a> <a href=\"https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/\">great</a> <a href=\"https://forum.effectivealtruism.org/posts/pbiGHk6AjRxdBPoD8/ai-safety-starter-pack\">posts</a> already exist with general advice for transitioning into AI Safety. However, these and others are mainly centered around technical Computer Science research and Machine Learning engineering. They don't delve into how more theoretical aptitudes and background (such as careers in <strong>Mathematics, theoretical Computer Science or Philosophy</strong>) can be steered into more abstract Alignment research that exploits them (except for <a href=\"http://acritch.com/from-math-grad-school-to-ai-alignment/\">Critch's short post</a>). I think that's mainly because:</p><ol><li><strong>Most people in the Alignment community work in applied research.</strong> That is to say, there are (way) more open positions for this kind of research. Still, most people I've talked to agree we need a non-trivial percentage of the community working in Theory (and we certainly need more people working in <a href=\"https://forum.effectivealtruism.org/posts/ycCBeG5SfApC3mcPQ/even-more-early-career-eas-should-try-ai-safety-technical\">any subfield of Alignment</a> they want to work on<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff1mq8g3rt1\"><sup><a href=\"#fnf1mq8g3rt1\">[2]</a></sup></span>).</li><li><strong>Applied career paths are more standardized, and thus easier to give advice on.</strong> As happens more broadly in academia and the job market, there are more obvious job prospects for the more applied. Theoretical careers tend to feel more like creating your own path. They often involve autonomous projects or agendas, individualized assessment of learning and evolution, and informally improving your epistemics.</li></ol><p>This post tries to fill that gap by being a helpful first read for graduates/researchers of abstract disciplines interested in AI Alignment. I'd recommend using it as a <strong>complement </strong>to the other more general introductions and advice. The two following sections are just a summary of general community knowledge. The advice sections do include some new insights and opinions which I haven't seen comprehensively presented elsewhere.</p><h3>Why Alignment research?</h3><p>I presuppose familiarity with <a href=\"https://www.youtube.com/watch?v=pYXy-A4siMw\">the </a><a href=\"https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/\">basic </a><a href=\"https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ\">arguments</a> <a href=\"https://longtermrisk.org/altruists-should-prioritize-artificial-intelligence/\">encouraging</a> AI Alignment research. This is a somewhat risky and volatile area to work on for positive impact, given how little we understand the problem, and so I recommend having a good <a href=\"https://www.lesswrong.com/tag/inside-outside-view\">inside view</a> of this field's <a href=\"https://forum.effectivealtruism.org/topics/theory-of-change\">theory of change</a> (and our <a href=\"https://www.effectivealtruism.org/articles/ea-global-2018-how-sure-are-we-about-this-ai-stuff\">doubts</a> about it) before committing hard to any path (and performing a <a href=\"https://80000hours.org/career-decision/article/#6-go-and-investigate\">ladder of tests</a> to check for your personal fit).</p><p>Of course, I do think careers in AI Safety have expected positive impact absurdly larger than the median, only equaled by other <a href=\"https://80000hours.org/career-reviews/\">EA cause areas</a>. Furthermore, if you're into an intellectual challenge, Alignment is one of the most exciting, foundational and mind-bending problems humanity is facing right now!</p><h3>Why Theoretical Alignment research?</h3><p>There are <a href=\"https://www.lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation\">sound arguments</a> for the importance of theoretical research, since its methods allow for more general results that might scale beyond current paradigms and capabilities (which is what mainly worries us). But we are not even sure such general solutions should exist, and carrying this research out faces some serious downsides, such as the scarcity of feedback loops.</p><p>Truth is, there's no consensus as to whether applied or theoretical<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefgbqtzadwd45\"><sup><a href=\"#fngbqtzadwd45\">[3]</a></sup></span>&nbsp;research is more helpful for Alignment. It's fairly safe to say we yet don't know, and so need people working in all fronts. If you might have especially good <a href=\"https://forum.effectivealtruism.org/posts/bud2ssJLQ33pSemKH/my-current-impressions-on-career-choice-for-longtermists\">aptitudes</a> for abstract thinking, mathematics, epistemics and leading research agendas, theoretical research might be your best chance for impact. That is, given uncertainty about each approach's impact, you should mainly maximize personal fit.</p><p>That said, I'd again encourage developing a good inside view to judge for yourself whether this kind of research can be useful. And of course, trying out some theoretical work early on doesn't lock you out of applied research.</p><p>In theoretical research I'm including both:</p><ol><li><strong>Mathematical research</strong> (sometimes referred to as \"Theoretical research\"): Using mathematics to pin down how complex systems will behave. This usually involves use of Decision Theory, Game Theory, Statistics, Logic or constructing abstract structures. Its goal is mostly deconfusing about fundamental concepts. Prime examples are Garrabrant's <a href=\"https://arxiv.org/abs/1609.03543\">Logical Induction</a> or Vanessa Kosoy's <a href=\"https://forum.effectivealtruism.org/posts/d7fJLQz2QaDNbbWxJ/what-are-the-coolest-topics-in-ai-safety-to-a-hopelessly?commentId=98kcBN3reYA7p7QH8\">work</a>.</li><li><strong>Conceptual research</strong>: Using epistemics, philosophy and everything else you can think of to devise approaches to the Alignment problem. This usually involves hypothesizing, forecasting, consequentialist thinking, building plans and poking holes in them. Its goal is mostly coming up with a global big picture strategy (with many moving parts and underspecified details) that might totally or partially assure Alignment. Prime examples are Christiano's <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\">ELK</a> or Critch's <a href=\"https://arxiv.org/abs/2006.04948\">ARCHES</a>.</li></ol><p>This research is mainly carried out in private organizations, in academia or as independent funded research. Any approach will need or heavily benefit from basic understanding of the current paradigm for constructing these systems (Machine Learning theory), although some do abstract away some details of this paradigm.</p><p>As a preliminary advice, the first step in your <a href=\"https://80000hours.org/career-decision/article/#6-go-and-investigate\">ladder of tests</a> can be just reading through <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\">ELK</a> and thinking for a non-trivial amount of hours about it, trying to come up with strategies or <a href=\"https://www.alignmentforum.org/posts/zjMKpSB2Xccn9qi5t/elk-prize-results\">poke holes in others</a>. If you enjoy that, then you'd probably not be miserable partaking in Theoretical AI Safety. If furthermore you have or can acquire the relevant theoretical background, and your aptitudes excel for that, then you're probably a good fit. Way more details can be weighed to assess your fit: how comfortable you will be tackling a problem about which everyone is mostly confused, how good you are at autonomous study/self-directed research, whether you'd enjoy moving to another country, how anxious deadlines or academia make you feel...</p><p>The following advice presuppose that you have already reached the (working) conclusion that this field can be impactful enough and you are a good fit for it.</p><h2>Consensus advice</h2><p>These points were endorsed by almost everyone I talked to.</p><h3>Skilling up</h3><ul><li><strong>Familiarize yourself with </strong><a href=\"https://www.alignmentforum.org/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\"><strong>research agendas</strong></a><strong>. </strong>Read through them, try understanding why they're trending or why they failed, what they predict and promulgate. This will provide a very necessary familiarity with technical concepts and terms floating in the community, as well as let you elucidate which are the <a href=\"https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment\">core main difficulties</a> of Alignment that need tackling. This is somewhat equivalent to an applied researcher reading lots of papers.</li><li><strong>You'll need to learn research skills. </strong>Other than the specific knowledge some subfield of Alignment requires, you'll need the general skills for doing theoretical research. These include independence, resourcefulness, <a href=\"https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/\">research taste</a>... Most of them can hardly be learnt on your own, and so a certain mentoring feedback is almost required. These skills are usually obtained in a PhD (even if unrelated to Alignment), but there are certainly other (and, according to some researchers, better) ways to do so, such as participating in a mentoring Alignment program, convincing an Alignment researcher to mentor you individually, or working in a private organization.</li><li><strong>We need people thinking about The Whole Problem\u2122. </strong>Although some theoretical research tackles concrete, helpfully insulated problems, most researchers agree one thing we're certain to need is a complete and coherent strategy/realization/development that brings us significantly closer to Alignment. This should be most naturally achieved by people with an accurate model of the whole problem in their head. This might not be the task for you, but if you excel at building big pictures, if you feel more like <a href=\"https://www.ams.org/notices/200902/rtx090200212p.pdf\">a bird than a frog</a>, you probably should try and slowly construct such broad a model. Even if you don't feel like these are your aptitudes, developing a whole (even if coarse) picture of the field will prove vital to direct your research in the most promising direction.</li><li><strong>You can probably learn basic Machine Learning theory on the go, while developing mathematical skills usually requires more focused attention. </strong>Put another way: people without a mathematical background often struggle with prerequisites for research such as linear algebra, while mathematicians rarely have trouble understanding ML. That said, not only acquiring the knowledge is important, but also being able to prove that you have it (for hireability). Probably your theoretical research posts/articles will already demonstrate (and build upon) that understanding, solving the problem. Otherwise you could <a href=\"https://www.alignmentforum.org/posts/zo9zKcz47JxDErFzQ/call-for-distillers\">distill</a> research for which that understanding is necessary, write something (or solve exercises) about ML, or even post some code on github (more usual among applied researchers).</li><li><strong>You probably don't need the overly complex, deepest ends of graduate mathematics to do Alignment research. </strong>Consider the following example. MIRI's early approaches revolved strongly around some aspects/results of Mathematical Logic. Still, not even Garrabrant had completed a PhD in Logic, or was a world-class logician. He only made use of relatively standard concepts and procedures of the field, which are probably closer to the level of a recently graduated mathematician than a recently doctorated one. In general, even if you need to learn the basics of a discipline to work in a certain research direction, this learning will at some time yield diminishing returns, and fairly soon your time will be better spent directly applying that to Alignment. That said, that advanced study will certainly help you skill up intellectually (but this might still not be the best way for that, or the best use of your time).</li><li><strong>Being in the right intellectual environment is important. </strong>Being close to the right peers and potential collaborators will prove immensely positive both for learning and strategic advantages, especially for research as open-ended as this. Researchers I've talked to disagree about how much weight you should put on this point when deciding your next career step. Some go to the extreme of <a href=\"https://acritch.com/ai-berkeley/\">recommending moving to Berkeley if possible</a>, others believe you only need an exciting intellectual environment of any kind (even if not related to Alignment) as can be found for instance in academia, in which to find some peer with whom to discuss Alignment. In any event, the average probably gives more importance to this point than we usually do.</li></ul><h3>Building your career</h3><ul><li><strong>Talk to people! </strong>You can reach out (both to students and researchers) asking for advice on your study agenda, research interests or career decisions. Many EAs (for instance, <a href=\"mailto:martinsotoq2911@gmail.com\">myself</a>!) will be glad to have an informal chat with you in which they share their model of the field/community. That said, don't contact popular/busy researchers for general advice which almost anyone in the community could provide for you. If you do think some researcher is especially well-suited to answer some specific question or provide an opinion, make it fast and easy for them to read and respond (they're more likely to answer that way!)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmuip1ypjut\"><sup><a href=\"#fnmuip1ypjut\">[4]</a></sup></span>.</li><li><strong>Try getting into an EA-aligned team/organization. </strong>This is the conventional path into a paid position. You start by working temporarily in some of these organizations, or in an academic group under the supervision of an Alignment researcher, as part of an internship, training program or research stay. Keep building upon these experiences and contacts, partake in more, and eventually get hired by one of these places. Disclaimer: Many non-EA AI researchers across the globe study topics seemingly resonating with the Alignment problem (regarding for instance value alignment and normativity). These researchers are usually not explicitly worried about the coming of AGI for existential reasons, and so the best way to ensure your work is helpful to the problem is by participating in a research environment which clearly shares EA's worries.</li><li><strong>And for that, apply everywhere! </strong>When thinking about career plans, I considered several <a href=\"https://80000hours.org/career-decision/article/#7-make-your-final-assessment\">frameworks</a>: best- and worst-case outcomes for a path, having a preferred and a backup plan, etc. Another useful framework is <i>trying to get your foot in the door</i>. That is, whilst continuing with your plan, try and land one of the starting positions mentioned above. Opportunities like these abound (just keep an eye on the <a href=\"https://www.alignmentforum.org/\">Alignment forum</a>), although many are competitive (and so you'll probably have to learn to deal with some rejections). Still, applying is certainly worth the cost (and benefits both you and the community).</li><li><strong>You can try and build research collaborations out of thin air. </strong>This is more independent and less conventional, but still a great option (especially if it's complementing other plans). Read, understand and think about a researcher's agenda, shortly explain to them what your conceptualization of it is and how you think you can help, and they might be willing to mentor you for an hour a week. With the endorsement of a reputable researcher, you can ask for <a href=\"https://funds.effectivealtruism.org/funds/far-future\">funding</a> for some months of your independent research.</li><li><strong>Apply for funding. </strong><a href=\"https://www.lesswrong.com/posts/9DenhM8deDziHiWZw/what-funding-sources-exist-for-technical-ai-safety-research?commentId=oR6KytzH9TBoEDS4n\">Many sources</a> exist, and are open to fund many kinds of enterprises. For instance, it's common for EAs taking an ML master's (with the plan of later pursuing applied research) to get funded. Since no clear study program is required or clearly optimal for theoretical Alignment research, it's more common to get funding for your independent research (either to solely dedicate to independent research, or to get out of the teaching requirements of a PhD or postdoc, and have more free time for research). Again, apply everywhere it seems sound and be ready to <a href=\"https://forum.effectivealtruism.org/posts/5sfwebGDPBmSBcYA2/the-application-is-not-the-applicant\">deal with some rejections</a>.</li><li><strong>Post your study/reflexions/research.</strong> Writing things up will help you pin down your ideas and skill up on communication. Feedback will speed up your progress. Uploaded content will serve as a portfolio for applications. You can work on <a href=\"https://www.alignmentforum.org/posts/zo9zKcz47JxDErFzQ/call-for-distillers\">distillations</a>, <a href=\"https://www.alignmentforum.org/posts/eqzbXmqGqXiyjX3TP/elk-thought-dump-1\">assessing other agendas</a>, <a href=\"https://www.lesswrong.com/posts/PRwQ6eMaEkTX2uks3/infra-exercises-part-1\">mathematical results upon other theories</a>, original reflexions... &nbsp;And when in doubt about an idea, err on the side of publishing your thoughts. The sooner you start public production, the sooner your production can <a href=\"https://www.alignmentforum.org/posts/Afdohjyt6gESu4ANf/most-people-start-with-the-same-few-bad-ideas\">become good</a>.</li><li><strong>Keep a career plan in mind.</strong> Strategizing over your future career is probably not as fun as thinking about Alignment, but assessing your options, introspecting about what you want, and drawing a path towards your dream position will increase your chances of getting there; and this can multiply your positive impact! <a href=\"https://80000hours.org/\">80k</a> provides many valuable resources, including <a href=\"https://80000hours.org/speak-with-us/\">free coaching</a> (<a href=\"https://www.aisafetysupport.org/resources/career-coaching\">AISS</a> does as well). I've found especially useful <a href=\"https://80000hours.org/career-decision/article/#top\">their article for short-term decision making</a>.</li></ul><h2>Controversial advice</h2><p>These points prompted radically different opinions amongst the researchers I've talked to, and some portray continuous debates in the community.</p><ul><li><strong>How valuable is academia?</strong> Academia comes with its bureaucratic baggage, and furthermore some researchers think its structure is considerably suboptimal for research. You're incentivized not to pursue the most promising approaches if they're too far afield. And if you're not lucky you might be bound to teaching requirements you don't find impactful. If these constraints seem too annoying, you might not be comfortable or as impactful in academia. Some researchers downplay these concerns, or stress the political need for maintaining bounds with academia for the soft power, or the influx of Alignment researchers it can yield.</li><li><strong>Is it required for your studies to be directly related to Alignment? </strong>PhDs whose topic is explicitly relevant to Alignment (as in CHAI) are still relatively rare.<strong> </strong>Some researchers (especially the most applied) consider as a top priority entering a good Computer Science or Machine Learning PhD (regardless of its topic). Others (especially the most theoretical) put less weight on the specific program of your higher studies, and more in skilling up intellectually and having free time to think about Alignment. As a consequence, they consider PhDs in Maths or relevant philosophical areas (like Decision Theory) with lots of freedom as a good option. This is probably in part because they do think those kinds of refined abstract reasoning are very beneficial to Alignment. This doesn't mean they endorse a PhD as the best option: some of them still defend whatever career option provides you with the most time to think about Alignment is best.</li><li><strong>Study deeply other researchers' world models, or look at the issue with fresh eyes? </strong>Some researchers recommend to start by <a href=\"https://docs.google.com/document/d/1qI7mXryWSKKMxr40y_JgOwM8QpYX9OwQdRz9B_D95gg/edit#heading=h.fzggmanhz8p4\">putting on a senior researcher's worldview as a pair of glasses</a>, and working from there. This readily provides some tools which would take a long time to develop from scratch, and quickly teaches some general skills. According to them, the downside of accepting a theory on faith is small, since critical thinking is encouraged and the junior will eventually form their own views. Some other researchers (especially those more pessimistic about current work) rather recommend looking at the core issues from a distance and coming up with your own ideas and intuitions, in hopes that one of these unadulterated juniors will come up with a drastically different new approach that is more promising (that is, instead of optimizing for median output, they're searching for a game-changing stroke of luck). I think there's again an argument for people partaking in both approaches, according to personal fit.</li><li><strong>Independent (non-academic) research might be great for you, but when should you embark in it? </strong>Independent research <a href=\"https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency\">has some clear benefits</a>, if you have the right traits for it. Some researchers recommend just maximizing the amount of time you think about Alignment, and so encourage trying out independent research even fresh out of undergrad. Others express worries that this might be too soon: most independent researchers have some experience in academic research (in other areas), and allegedly that's why they perform well in self-directed research. You'd be competing for funding with students who have spent some years in a PhD. If your economic situation is relaxed, independent research could be a very good use of your time (provided you'd obtain the necessary mentorship).</li><li><strong>What makes you hireable? </strong>The factors increasing your hireability are pretty obvious: fruitful writings/collaborations/internships on the topic of Alignment, relevant theoretical research experience, excelling in abstract thinking or mathematical production, etc. But different institutions/individuals give different weights to these. Some are satisfied by a history of non-trivial mathematical results, even if completely unrelated to Alignment. Others require some non-trivial understanding of current trends in Alignment, or experience dealing with the topic. In any event, the discrepancies don't seem big.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6ihrrg3dnir\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6ihrrg3dnir\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If you're one of those researchers and would like nominal recognition in this post, let me know. I've taken the approach of default anonymity given the informal and private nature of some of the discussions.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf1mq8g3rt1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff1mq8g3rt1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\"AI Safety Technical research\" usually refers to any scientific research trying to ensure the default non-harmfulness of AI deployment, as opposed to the political research and action in <a href=\"https://forum.effectivealtruism.org/topics/ai-governance\">AI Governance</a>. It thus includes the Theoretical/Conceptual research I talk about in this post despite the applied connotations of the word \"Technical\".</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fngbqtzadwd45\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefgbqtzadwd45\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The division between applied and theoretical is not binary but gradual, and even theoretical researchers usually construct upon some empirical data.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmuip1ypjut\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmuip1ypjut\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I've been positively surprised as to how many popular researchers have been kind enough to answer some of my specific questions.</p></div></li></ol>", "user": {"username": "Mart\u00edn Soto"}}, {"_id": "MFZx36uuhhSFuRuJ6", "title": "Apply now - EAGxVirtual (21-23 Oct)", "postedAt": "2022-09-12T16:51:01.095Z", "htmlBody": "<p>We are excited to announce that <a href=\"https://www.eaglobal.org/events/eagxvirtual-2022/\">applications</a> are now open for EAGxVirtual, happening<strong> </strong>on the <strong>21-23rd of October.</strong></p><p>A year has passed since the last entirely virtual conference. We are hoping to see many EAs from around the world, particularly those who have not been able to attend in-person events!</p><h3><a href=\"https://www.eaglobal.org/events/eagxvirtual-2022/\"><strong>Apply now</strong></a></h3><figure class=\"image image_resized\" style=\"width:50.83%\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/pmu2nwyofpqy90s1ngkn\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/dbsvmyklmeapxyqcpte2 170w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/uc9ywvbxp2tnyhvvckel 340w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/sqvsmms7dzyhtohq4eip 510w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/tfq3fpmf34jzy4wicksn 680w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/ia9ibzzcgew0xm7ahi1u 850w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/tx8uu8dwj2tb9xreg5tp 1020w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/zorfxltp8iolovnbu3a1 1190w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/m9jlrzkmofyavnifflsk 1360w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/yg43e8chcbqywoxatkoz 1530w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/MFZx36uuhhSFuRuJ6/znzhyajrcpac7ipd5eqt 1700w\"></figure><h3>Our conference theme is&nbsp;<strong>Transcending Boundaries</strong></h3><p>Many of us are constrained in one way or another in our journey to do the most good and be a part of the EA community. For example, by language barriers, travel restrictions, or lack of knowledge about relevant opportunities. Even our own image of ourselves and what we could achieve can be a limiting factor.</p><p>Our main goal with EAGxVirtual is to overcome the boundaries that may be holding us back \u2013 both physically and in more abstract ways. To explore how people from very different cultural and economical backgrounds can find their unique ways to be effective altruists.&nbsp;</p><p>In order to achieve this goal, we will host a range of talks, office hours, workshops, and career stories sessions across EA cause areas. We aim to provide action-oriented content that will be relevant not only to people from Western countries. We have also planned an extensive ambassador program connecting attendees to more experienced community members.&nbsp;</p><h3>Who is this event for?</h3><p>We welcome attendees familiar with key EA ideas, including those relatively new to effective altruism. If you have recently completed your first introductory EA fellowship program, read an EA-aligned book, or are otherwise familiar with key EA ideas and excited about applying these to your life, we encourage you to apply!&nbsp;<strong>If you are unsure, please err on the side of applying.</strong></p><h3>Reasons to attend</h3><ol><li>Give and receive feedback on career, study, or donation plans, or on your EA projects</li><li>Build a better understanding of the EA landscape to identify relevant opportunities to take action</li><li>Make new connections and reconnect with old contacts</li><li>Discover and discuss interesting and important ideas</li></ol><h3>If you are a highly-engaged EA member, your involvement can make a difference</h3><p>If you are a highly-engaged EA you can make a difference by showing up, providing feedback to those relatively new to the community, and helping them navigate the conference.&nbsp;</p><p>The previous EAGxVirtual in 2020 was one of the most geographically diverse EA events with attendees from 60 countries, and for many of them, it was their first chance to attend an EA conference. For example,&nbsp;<a href=\"https://www.fhi.ox.ac.uk/team/angela-aristizabal/\"><u>Angela Aristiz\u00e1bal</u></a> attended EAGxVirtual from Colombia, learned that EA cared about improving institutional decision-making, and successfully applied for a position at&nbsp;<a href=\"https://www.fhi.ox.ac.uk/\"><u>FHI</u></a>.&nbsp;</p><p>Those people who engage with the community only virtually often have fewer connections&nbsp; and opportunities. Supporting them is crucial for a vibrant global community. Please make it clear on your Swapcard profile what it is you can help people with, encourage first-time attendees to reach out to you (or even reach out first), and be generous.</p><h3>What do previous event attendees say?</h3><blockquote><p><i>\"The most valuable thing for me as someone new to the EA was to see the diversity of people and ideas and experiences in EA. Before, I had doubts if it's for me, if I fit here, but now I feel much more confident and comfortable about identifying as a part of this community and being more active in it.\"</i></p></blockquote><blockquote><p><i>\u201cBeing plugged-in to the EA community via local, city-level chapters is one thing, but to connect with EAs worldwide that are on a similar journey exploring pathways to high-impact careers and more effective thinking really energised me as an EA concerned with maximising outcomes.\u201d</i></p></blockquote><h3>Sidenote on my personal experience</h3><p>Despite being actively involved in EA since 2018, I haven't attended any in-person conferences until 2022 because of visa restrictions and travel costs. EAGxVirtual was my first EA conference and matching with ambassadors was enormously valuable. It affected all my further interactions with the community: before the conference, I barely knew any EAs outside of my country.</p><p>I received a lot of feedback on my entrepreneurship ideas from Joey Savoie; built a strategy for my national-level group because of meeting Janique Behman and Eirin Evjen; learned about the EA Geneva Fellowship (which I ended up being part of) from Naomi Nederlof. On top of that, I met a dozen of members from developing countries who were facing the same community-building challenges as I did. I\u2019m excited about EAGxVirtual 2022 because it can make more stories like this possible.&nbsp;</p><h3>How much are the tickets?</h3><p>EAGxVirtual is&nbsp;<strong>free.</strong></p><h3>Help us spread the word</h3><p>If you know someone who would enjoy either event, please&nbsp;<a href=\"https://creatorapp.zohopublic.com/centreforeffectivealtruism/ea/form-perma/Referral_form/OgbvyQ3X4KgE7WfgDCjNp5v4srT2qaCPY5Y8DGdt9HzVBg90RFTBtpYwZDReYHGm8spD7EbQrD8yQfmK3fwWuXBanTQ8KtmT4Mkm\"><strong><u>invite them to apply</u></strong></a>.&nbsp;</p><p>The deadline for applications is 8:00 am UTC on <strong>Wednesday, 19 October</strong>.</p><p>&nbsp;</p><p>We look forward to receiving your application!<br><br><br><br>&nbsp;</p>", "user": {"username": "Alex_Berezhnoy"}}, {"_id": "5KKk2vLz5irr5yzvH", "title": "What We Owe The Future: a Flashcard-based Summary", "postedAt": "2022-09-12T12:24:22.633Z", "htmlBody": "<p><i>*Disclaimer: I am building the learning and memory tool called&nbsp;</i><a href=\"http://thoughtsaver.com\"><i><u>Thought Saver</u></i></a><strong>,</strong><i><strong>&nbsp;</strong>which allows you to embed flashcards on the forum!&nbsp;</i></p><p>&nbsp;</p><h1><strong>TL;DR&nbsp;</strong></h1><p>You can revise the key takeaways from&nbsp;<a href=\"https://whatweowethefuture.com/\"><u>What We Owe the Future</u></a> with our new flashcard deck on Thought Saver. Scroll to the end of this post to see the embedded flashcards.</p><p>&nbsp;</p><p>Will MacAskill's new book&nbsp;<a href=\"https://whatweowethefuture.com/\"><u>What We Owe The Future</u></a> has been out for a few weeks now. I am sure many of you finished it on the first day, some are reaching the last hurdles, or maybe you've been too busy, and you need that extra push to get the book! We have a set of key facts and takeaways from the book so that you can remember the most important parts after you've read it! After all, how often do you read a book and forget all the key ideas?&nbsp;</p><p>Remembering key facts from WWOTF can help you to set a better mental&nbsp;<a href=\"https://en.wikipedia.org/wiki/Base_rate\"><u>base rate</u></a>, enabling you to evaluate how important the magnitude of one problem is compared to another.&nbsp;</p><h2><strong>Example of better base rates</strong>&nbsp;</h2><p><i>Without a base rate, it's hard to understand the size of a problem. For example, it's very hard to comprehend how important the future lives of humanity are (and how much we should care about future humans) unless we know:&nbsp;</i></p><p><i>A) how many humans there are on the earth today (8 Billion) and&nbsp;</i></p><p><i>B) how many humans lived across the entirety of humanity (100 Billion)</i></p><p><i>However, knowing both of these facts helps us better understand the problem's order of magnitude.</i></p><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=1&amp;end=2\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=1&amp;end=2\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><p>The team at&nbsp;<a href=\"http://thoughtsaver.com\"><u>Thought Saver</u></a> and I use flashcards to remember the most important things in life, things that we want to recall at the tip of our fingers. We use flashcards when we want to reprogram actions, remember base rates, use a mental model, or put learning into action.</p><h1><strong>Why spaced repetition for books?</strong></h1><p><a href=\"https://en.wikipedia.org/wiki/Spaced_repetition#Software\"><u>Spaced repetition</u></a> is one of the most effective ways to remember key ideas! When I read a book, I want to make sure I take away the most important information. We already spend ~6 hours reading a book, but if we don\u2019t revise the material again afterward, we often remember very little! Investing a little bit of extra time in spaced repetition can have high payoffs for you in the future. So instead of just reading WWOTF, we suggest you also revise flashcards about it using a spaced repetition system - you could say that you owe it to your future self. :P</p><h1><strong>Flashcards</strong></h1><p>Here is a sneak peek of our Thought Saver flashcards for each chapter of the book. Thought Saver ranks your flashcards with our algorithm based on an \"importance\" score, so if a flashcard isn't valuable to you, select \"never,\" and you don't need to remember it forever!</p><h2><strong>Chapter 1: The Case for Longtermism</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=3&amp;end=5\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=3&amp;end=5\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 2: You Can Shape the Course of History</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=116&amp;end=118\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=116&amp;end=118\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 3: Moral Change</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=134&amp;end=136\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=134&amp;end=136\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 4: Value Lock-In</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=114&amp;end=114\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=114&amp;end=114\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 5: Extinction</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=149&amp;end=151\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=149&amp;end=151\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 6: Collapse</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=68&amp;end=70\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=68&amp;end=70\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 7: Stagnation</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=165&amp;end=167\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=165&amp;end=167\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 8: Is it Good to Make Happy People?</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=178&amp;end=180\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=178&amp;end=180\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 9: Will the Future Be Good or Bad?</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=195&amp;end=197\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=195&amp;end=197\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h2><strong>Chapter 10: What to Do</strong></h2><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=205&amp;end=207\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/i3DTRmYvED?start=205&amp;end=207\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><h1><a href=\"https://ankiweb.net/shared/info/1539708817\"><strong>Anki Flashcards</strong></a></h1><p>Prefer to use Anki? Get the same set of <a href=\"https://ankiweb.net/shared/info/1539708817\">flashcards in the Anki format here.</a>*Note this link won't work for 24 hours as it takes 24 hours to be shareable on Anki Web!*&nbsp;</p><h1><strong>Acknowledgment</strong></h1><p>Thanks to Will MackAskill\u2019s team for all their help in creating this deck!</p><h1><strong>Questions</strong></h1><ul><li>Is this valuable to you?</li><li>Would you like to see more key takeaways from books in flashcard format? I'd love to hear your thoughts in the comments!</li></ul><h1><strong>Relevant Links</strong></h1><ul><li><a href=\"https://forum.effectivealtruism.org/posts/XabosWtHBZP6DTG88/14-techniques-to-accelerate-your-learning-1\"><u>Thought Saver Accelerated Learning Article</u></a>&nbsp;</li><li>People talking about Flashcards<ul><li><a href=\"https://forum.effectivealtruism.org/posts/avoeKcP3vy5Y5WAWg/maxg-s-shortform?commentId=7q8etcz92zwM2SeQS\"><u>https://forum.effectivealtruism.org/posts/avoeKcP3vy5Y5WAWg/maxg-s-shortform?commentId=7q8etcz92zwM2SeQS</u></a>&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/ivzgSCmqyKXA6K8Eq/300-flashcards-to-tackle-pressing-world-problems \">300 Flashcards to tackle pressing world problems Andre\u2019s flashcard Articles</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/qFqT6akjRvJ3wPJ8t/how-i-use-anki-expanding-the-scope-of-srs#J8JAhBuqGiZBvTuRs \">\"How I use Anki\" Article</a></li><li><a href=\"https://forum.effectivealtruism.org/topics/spaced-repetition \">Other EA Forum content on SRS</a>&nbsp;</li></ul>", "user": {"username": "Florence"}}, {"_id": "a4gTuEbvSqmdeFJeK", "title": "Popular education in Sweden: much more than you wanted to know", "postedAt": "2022-09-11T22:04:38.618Z", "htmlBody": "<p><i>This post appeared on LessWrong and </i><a href=\"https://escapingflatland.substack.com/p/popular-education-in-sweden-much\"><i>my Substack</i></a><i> in May, but I thought it might be of interest here as a case study of a movement that was successful in mobilizing large numbers of people to improve themselves and shoulder a larger responsibility for their wider community.</i></p><p><i>-</i></p><p>Growing up on the Swedish seaside, I had a five-minute walk to four open learning facilities \u2013 not counting the library and the youth center. It was&nbsp;<a href=\"https://escapingflatland.substack.com/p/christopher-alexanders-architecture?s=w\">very Christopher Alexander</a>.</p><p>One of the premises was an abandoned church that my friends and I used as a recording studio; we'd renovated it ourselves with funding from a study association. In another, I learned French from an \u00e9migr\u00e9 of Montpellier. We arranged public lectures \u2013 once, to our great surprise, we even managed to book then general secretary of the United Nations Ban-Ki Moon for a lecture in Uppsala. I analyzed Soviet cinema with a group of whom an unsettling number sang&nbsp;<i>S\u00e5ng f\u00f6r Stalin</i>&nbsp;before the screenings.&nbsp;&nbsp;</p><p>Since leaving Sweden, I have realized that not everyone grows up like this. And I miss it. In fact, if the whole of Sweden was about to burn down and I could only save one thing, I might grab&nbsp;just <a href=\"https://sv.wikipedia.org/wiki/Folkbildning\"><i>folkbildningsr\u00f6relsen</i></a>.</p><p><i>Folkbildningsr\u00f6relsen: </i>that is the name we have for this movement of self-organized study groups, resource centers, maker spaces, public lectures, and free retreats for personal development.&nbsp;</p><p>These types of things exist in other countries too \u2013 but not at the same scale. Or even close.</p><p>To get a sense of how comprehensive <i>folkbildningsr\u00f6relsen </i>is, it helps to remember that Sweden has a population roughly comparable to New York City.&nbsp;If NYC had as many free resource centers per inhabitant as the municipality where I grew up, Manhattan would look like this:</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39c7ffe-9779-439f-8e2e-0d39d889c398_867x997.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39c7ffe-9779-439f-8e2e-0d39d889c398_867x997.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39c7ffe-9779-439f-8e2e-0d39d889c398_867x997.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39c7ffe-9779-439f-8e2e-0d39d889c398_867x997.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39c7ffe-9779-439f-8e2e-0d39d889c398_867x997.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39c7ffe-9779-439f-8e2e-0d39d889c398_867x997.jpeg 1456w\"></a></p><p><i>I was going to do all of New York but my hand started hurting from making all these dots, so I only managed the tip of Manhattan.</i></p><p>At every other intersection, there would be a few rooms where you could go in and get some money to buy literature or access tools you needed. (In practice, the resource centers in cities tend to be lumped together in larger units, but the map still captures a lived reality for <a href=\"https://books.google.dk/books?id=s4eLBQAAQBAJ&amp;hl=da&amp;source=gbs_navlinks_s\">the 7.5 percent of Sweden's population who regularly take part in study associations</a>.)&nbsp;</p><p>Experientially, the spaces I have been part of have felt more like niche internet forums than schools. There were plenty of trolls, witches, and freaks \u2013 but we were also able to sustain a depth of conversation which was out of scope at school. When I entered university, seminars often felt like play-acting in comparison. In our often quite dilapidated buildings (as in internet communities), we hadn\u2019t thought about what we were doing as&nbsp;<i>learning</i>.&nbsp;</p><p>We were just obsessing about things.</p><h2><strong>How did this all come about?</strong></h2><p>In the 19th century, when these houses and the financing that enables them began to be built out, the main impetus came from the German&nbsp;<a href=\"https://en.wikipedia.org/wiki/Bildung\"><i>Bildung</i></a>&nbsp;tradition.&nbsp;</p><p><i>Bildung&nbsp;</i>etymologically refers to shaping yourself in the image (<i>das Bild</i>) of God. God in this context should be imagined as a highly self-possessed spectral being \u2013 in control of its emotions, with mind and heart in harmony, and willing to take individual moral responsibility.&nbsp;Think Bertrand Russell but less atheist, and sitting on a cloud.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb12c03-f158-4449-9ec0-b496d376a55e_1583x1997.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb12c03-f158-4449-9ec0-b496d376a55e_1583x1997.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb12c03-f158-4449-9ec0-b496d376a55e_1583x1997.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb12c03-f158-4449-9ec0-b496d376a55e_1583x1997.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb12c03-f158-4449-9ec0-b496d376a55e_1583x1997.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb12c03-f158-4449-9ec0-b496d376a55e_1583x1997.jpeg 1456w\"></a></p><p><i>This is the look.</i></p><p>In its original formulation,&nbsp;<i>Bildung</i>&nbsp;had a somewhat bourgeois flavor. It smelled of tweed and leather elbow patches. But in the early 1800s, thinkers such as <a href=\"https://en.wikipedia.org/wiki/Johann_Heinrich_Pestalozzi\">Johann Heinrich Pestalozzi</a>, <a href=\"https://en.wikipedia.org/wiki/N._F._S._Grundtvig\">N.F.S. Grundtvig</a>, and <a href=\"https://en.wikipedia.org/wiki/Johann_Friedrich_Herbart\">Johann Friedrich Herbart</a> figured out how to sell&nbsp;<i>Bildung</i>&nbsp;to farmers and day laborers \u2013 a folk&nbsp;<i>Bildung</i>, or&nbsp;<i>folkbildning</i>&nbsp;in Swedish. This was the tradition that took root in Sweden: the popular movement to shape yourself in the image of Bertrand Russell.</p><p>The English language version of&nbsp;<a href=\"https://sv.wikipedia.org/wiki/Folkbildning\"><i>folkbildning\u2019s</i>&nbsp;Wikipedia page</a> refers to it as&nbsp;<a href=\"https://en.wikipedia.org/wiki/Popular_education\"><i>popular education</i></a>. This translation is not entirely correct. The term \"popular education\" has a strong political connotation \u2013 the Wikipedia page talks about \"class, political struggle, and social transformation\" \u2013 which does not map entirely to the present Scandinavian reality. Though the study associations grew out of popular movements (the free church movement, followed by the temperance movement and the labor movement) \u2013 these spanned the political spectrum. And the learning infrastructure they spawned, as we will see, rapidly outgrew their political aims.</p><h2>Building intellectual retreats for farmers</h2><p>Since the 17th century, there have been popular educational movements in Sweden. Perhaps most interesting among these were the so-called Readers, who arranged reading lessons for peasants and opened secret libraries. The state, which had its own compulsory reading education, persecuted the Readers, fearful of the social unrest that might result from free reading.</p><p>In the 19th century, the popular education movement started to grow into a significant societal force. This began with the creation of so-called <a href=\"https://en.wikipedia.org/wiki/Folk_high_school\">folk high schools</a> (<i>folkh\u00f6gskolor</i>). These first emerged in Denmark, in Ryslinge, where Christen Kold in 1851 started a school based on <a href=\"https://en.wikipedia.org/wiki/N._F._S._Grundtvig\">N.F.S. Grundtvig</a>'s idea of an ungraded, discussion-focused institution for higher education, aimed at the lower classes.</p><p>Folk high schools were located in scenic areas - not so much to be romantic retreats for city dwellers but to be close to the farmers who were their main clientele. In <a href=\"https://www.nordicsecret.org/\"><i>The Nordic Secret</i></a><i>, </i>Andersen and Bj\u00f6rkman argue that folk high schools were&nbsp;<i>retreats for ego development</i>&nbsp;along lines similar to <a href=\"https://en.wikipedia.org/wiki/Robert_Kegan\">Robert Kegan</a>'s. It was about creating the conditions for people who had lived in simple small-scale communities to develop the knowledge and psychological complexity required to navigate modern society. Much emphasis was placed on discussions, practical skills and simulations.</p><p>The first Swedish folk high schools, Hvilan, \u00d6nnestads, and Herrestad, started seventeen years later, in 1868, seemingly without contact with the Danish movement.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bae9d28-4347-46fe-b382-62f26222295f_550x361.jpeg\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bae9d28-4347-46fe-b382-62f26222295f_550x361.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bae9d28-4347-46fe-b382-62f26222295f_550x361.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bae9d28-4347-46fe-b382-62f26222295f_550x361.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bae9d28-4347-46fe-b382-62f26222295f_550x361.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bae9d28-4347-46fe-b382-62f26222295f_550x361.jpeg 1456w\"></a></p><p>They arranged role-playing events where workers and farmers played out committee meetings and other arcane parts of the political process. This meant that once they got the vote and started sweeping into office, the worker representatives out-maneuvered the representatives from the upper classes, to the great surprise of many who had argued against democracy on the grounds that it would lead to a flood of unwashed plebeians. The secretaries in the government office, who were in the habit of grading political representatives for their professionalism, left good marks for the early workers' representatives.</p><p>At their peak, 10 percent of young adults in rural areas choose to attend folk high schools. Andersen and Bj\u00f6rkman\u2019s thesis is that this created a critical mass, well distributed in the population, that had the intellectual and emotional tools needed to effectively navigate a complex society. This, in turn, would explain the rapid transition that the Nordic countries made, from being the poorest in Europe in the 1850s to being the happiest, most equal, and nearly richest societies in the world eighty years later. I think that is overplaying the importance of the folk high school \u2013 but it does gesture at the transformative impact that popular education had on large swaths of the population.</p><p>And it was only just beginning.</p><h2><strong>How folkbildning went viral</strong></h2><p>Folk high schools were an important part of transforming public education into a societal force. Folkbildning was spreading.</p><p>Then, in 1902, <a href=\"https://en.wikipedia.org/wiki/Oscar_Olsson\">Oscar Olsson</a>, a secondary-school teacher who looked like he was living through the final stages of a pandemic lockdown, figured out how to make it go viral.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcd63e3a-c590-4119-832f-0d9d97e0c9ea_828x1110.png\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcd63e3a-c590-4119-832f-0d9d97e0c9ea_828x1110.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcd63e3a-c590-4119-832f-0d9d97e0c9ea_828x1110.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcd63e3a-c590-4119-832f-0d9d97e0c9ea_828x1110.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcd63e3a-c590-4119-832f-0d9d97e0c9ea_828x1110.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcd63e3a-c590-4119-832f-0d9d97e0c9ea_828x1110.png 1456w\"></a></p><p>Olsson had returned from a trip to the United States where he had observed the success of the <a href=\"https://en.wikipedia.org/wiki/Chautauqua\">Chautauqua movement</a>, an educational spectacle with speakers, showmen, and preachers, which Theodore Roosevelt, quite aptly, called \u201cthe most American thing in America\u201d. Now Olsson was trying to figure out how to bring these ideas to his <a href=\"https://en.wikipedia.org/wiki/International_Organisation_of_Good_Templars\">Good Templar</a> lodge in Lund, to help his fellow Good Templars spread temperance.</p><p>What he came up with was a Scandinavian, minimalist version of Chautauqua, which he called a <a href=\"https://en.wikipedia.org/wiki/Study_circle\">study circle</a>. The study circle, Olsson envisaged, would be made up of equals and elect a leader from among its members. It would take literature as its starting point, and help its members acquire knowledge in the course of free conversation. It was, as all good memes are, a very simple idea. And it was cheap. The members (numbering between 5 and 20) could, if necessary, meet at home and would choose their own study material. That created an economically viable form of education for the working class.&nbsp;</p><p>And it was made even more viable three years later, when the Riksdag, Sweden's parliament, voted to give grants for the purchase of books, on the condition that the books were made available to the general public.</p><p>Another factor behind the success of study circles was their focus on <i>communal self-improvement.&nbsp;</i>Study circles were a child of the temperance movement \u2013 a movement that neither sought collective power, like the unions, nor self-improvement for the individual, but rather encouraged people to improve to serve their fellow human beings. This focus on communal self-improvement seems to have provided momentum to the movement. It also helped foster social capital formation, creating dense <a href=\"https://escapingflatland.substack.com/p/networks-of-trust-vs-markets?s=w\">high-trust networks</a>.</p><h2><strong>The spread of study circles</strong></h2><p>The study circle model spread rapidly from the Good Templar Order through the popular movements, filling up a public depot of books \u2013 which became the foundation of Sweden's public library system.&nbsp;</p><p>Since the members were working-class or small farmers, the study circles, like the Folk high schools, played an important role in preparing them for their growing political power in the 20th century. Farmhands, miners, and dock workers were studying not only political and social issues but accounting, law, literature, math \u2013 everything they would need on the day of reckoning when they would run the country. They worked weekends building houses to study in. They formed national study associations to facilitate the supply of funding.&nbsp;They were dead serious about becoming small Bertrand Russells.</p><p>It was not an education system. Rather, it was an attempt to unleash what I have called&nbsp;<i>the learning system</i>. Instead of interventions aimed at controlling what people learn \u2013 which is how we can think about traditional education \u2013&nbsp;<i>folkbildningsr\u00f6relsen&nbsp;</i>provided people with the resources they needed to learn on their own. The movements created the conditions for an ecosystem to emerge.</p><p>In 1950, 30,000 study circles were active. One for every 200th person. Several of the architects of the modern Swedish state \u2013 such as Gunnar Str\u00e4ng and Torsten Nilsson \u2013 had been raised in this ecosystem of popular education.</p><p>Twenty years later, in 1970, the number of study circles had grown to 300,000 \u2013 counting 10 percent of the population as active members.&nbsp;</p><p>Since then it has fluctuated around the same absolute numbers, today counting 7.5 percent of the population as active members.</p><p>By the time the movement reached its peak, the original political aspirations had somewhat waned. The waning was partially a result of the funding: to receive state funding, the associations had to give up political control of the curriculum. Both the state and the associations battled to control the curriculum, but none turned out to be strong enough to force the other, so in a sort of Westphalian peace, the control was ceded to the learners.&nbsp;</p><p>And, as with anything that gets decentralized, variance and variety increased.</p><p>Retired women started forming study circles to throw pottery together. There were study circles for people who were anxious about the H day, in 1967, when Sweden would switch from driving on the left side of the road to the right. White-collar workers, envious of blue-collar associations, started their own \u2013 though they favored classroom studies over discussion groups, and mostly studied accounting and how to&nbsp;<i>actually</i>&nbsp;run an association. Universities formed spin-offs in the form of associations to broadcast public lectures and attract students to language schools. Many study circles also, on closer inspection, turned out to be bands that wanted access to cheap rehearsal spaces.</p><h2><strong>How study circles works in practice</strong></h2><p>To start a study circle today you:</p><ol><li>join a study association,&nbsp;</li><li>get instructions on how to report your studies,&nbsp;</li><li>submit forms to document how many participants have attended and how many hours have been spent on the studies, and</li><li>receive financial support in proportion to your efforts - about $2 per man hour - which is often invested in renting rooms but can also be spent on educational materials, tools, or services.</li></ol><p>Until I moved from Sweden, I administered three study circles. The paperwork took about 30 minutes a year. In two cases, the study circles were book clubs. Incorporating as study circles gave us access to meeting rooms. And sometimes, when we invited researchers to discuss their ideas with them, we could use study funds to pay for their train tickets. When we wanted to learn something more practical, we used the funds to buy services - such as a recording engineer, who could sit with us when we recorded music and give us hands-on guidance.</p><p>There is, of course, financial waste in the system, and one can debate the morality of funding book clubs with taxpayers' money. But on the whole, the 40 or so study circles I have observed got more learning per penny than the compulsory schools I have worked at.&nbsp;</p><p>I also can't imagine living in a city without access to basement rooms where you can go down and find people all day engaged in strange discussions and projects. When I find myself in cities where the only places to sit down and talk are caf\u00e9s and bars, I get intense claustrophobia.</p><h2><strong>Is popular education a viable alternative to public schooling?</strong></h2><blockquote><p><i>A good educational system should have three purposes: it should provide all who want to learn with access to available resources at any time in their lives; empower all who want to share what they know to find those who want to learn it from them; and, finally, furnish all who want to present an issue to the public with the opportunity to make their challenge known.</i></p><p>\u2013 Ivan Illich</p></blockquote><p>In 1971, when Ivan Illich published&nbsp;<a href=\"https://en.wikipedia.org/wiki/Deschooling_Society\"><i>Deschooling Society</i></a>, his scathing critique of schooling and call for voluntary learning webs, the Swedish study circles had already become something akin to what he proposed. It was \u201ca set of services that did not coerce anyone to learn but provided all who wanted to with access to available resources and interested peers\u201d. The study circles \u201chelped those who wished to share what they knew to find those who wanted to learn it from them\u201d. They were voluntary, and free from government control.</p><blockquote><p><i>Popular education is and should be free [from government control] and voluntary. This free and voluntary popular educational work enables all to seek knowledge on the basis of their own experience, preferences and learning style, without limitation from demands for results, and without mechanisms of exclusion. The approach permits dialogue, involvement and questioning, without a preconceived framework.</i></p><p>\u2013 Swedish Government Bill 1997/98:115:5</p></blockquote><p>Ivan Illich\u2019s ideas of a deschooled society are easy to dismiss as the ravings of a defrocked Catholic priest determined to restore Christianity to its prime (which was the first century AD, in case you wonder).&nbsp;But does Sweden\u2019s experience indicate that his ideas are feasible? Would a non-coercive learning system work as the main educational infrastructure in modern society?</p><p>Looking at the current state of popular education in Sweden, the movement is more a vehicle of self-cultivation, than a balanced reproduction of the knowledge we need to sustain our civilization. The most common subjects are related to arts and crafts \u2013 people play in bands, learn how to restore houses, knit, and garden. Foreign languages are also popular. More advanced studies in STEM subjects are notably absent (though traditional school subjects at the high school level, including STEM, are popular).&nbsp;</p><h2><strong>Two theories of why the movement became dominated by hobbyists</strong></h2><p>The first generations of the movement were more focused on serious skill-building. Why did that change? One theory is that the early members were outliers that eventually were outnumbered by hobbyists as the movement scaled. It was an inevitable, eternal September.</p><p>A second theory as to why popular education became a hobby project after 1950 is this. The postwar boom was the period when school ultimately cemented its role as the only acceptable way to signal employability (Gustav M\u00f6ller, who lost the bid for the party leadership of the Social Democrats to Tage Erlander in 1946, was probably the last major political figure to argue social services like education should be arranged through voluntary associations, not directly through the state). With Erlander and Palme, the state minister and his prot\u00e9g\u00e9, there was a push for the \u201cstrong state\u201d (<i>det starka samh\u00e4llet</i>) \u2013 which included more schooling as well as increased state control over higher education. Credentials, especially engineering degrees, became exalted. And in a society where only certified learning is considered legitimate, self-directed learning becomes consumption.</p><p>There is a measure of truth to both of these theories. Most of us are more attracted to self-realization than to bookkeeping \u2013 and you have to be doubly insane if you study bookkeeping without the promise of a career. Or at least a worker-run utopia.</p><h2><strong>Other limitations of the model</strong></h2><p>There are a number of other limitations to the Swedish model, beyond not being strong in STEM fields. It has so far mostly been limited to people over 13. Giving young children access seems to me a missed opportunity. Merging the popular education movement with homeschooling is a promising path to pursue.</p><p>Another area where there is room for improvement is pedagogy. The popular education movement has a strong emphasis on discussion-based learning, which has many merits but is now often used in contexts where it is a bad fit. Several of the most effective learning methods (such as simulations and case studies, deliberate practice, spaced repetition, apprenticeships, and one-to-one tutoring) are rarely used. Of course, the same thing can be said of schools.</p><p>And in defense of Illich's idea of keeping learning voluntary, people of all classes were learning advanced skills before mandatory education. The Readers spread literacy among farmers, workers self-studied accounting and law... hell, even the cathedrals were built by self-learners.</p><h2><strong>Bottom-up growth of learning infrastructure</strong></h2><p>Throughout history, we have seen new learning services spring up organically as the demand for more advanced skills increase - aristocratic tutors in the Roman Empire, Imperial examinations in China, the more structured apprenticeships that spread during the Renaissance, the guilds of the middle ages, grammar schools and cheaply printed reading primers after Gutenberg. And the broad willingness of workers and farmers to labor on weekends and evenings to build lodges where they could study suggests that the Swedish population was in full swing building a more advanced education system during the first half of the 20th century. Then they stopped. They were outcompeted for time, resources, and legitimacy by state schools.</p><p>Had they not stopped, had they kept building and experimenting, what would this popular alternative have looked like today?</p><p>It is naturally impossible to say where this counterfactual history would have ended up. But if we can draw any conclusions from the Swedish experience, it would have been somewhere much stranger and more fun than&nbsp;<i>here</i>.</p><p>And we can still find out.</p>", "user": {"username": "Henrik Karlsson"}}, {"_id": "4S2qdMwNqksZhnJ9y", "title": "Bring legal cases to me, I will donate 100% of my cut of the fee to a charity of your choice", "postedAt": "2022-09-11T15:49:06.791Z", "htmlBody": "<p>I am a lawyer that practices near Chicago, Illinois and I also started a 501c3 charity called the Consumer Power Initiative. CPI is interested in using our economic power to benefit charities.</p><p>To that end, I would like to donate 100% of my cut of any business I bring in to the charity of the choice to whoever brings in the business. I get a third of our firm's fee for business that I bring in. Thus, in a personal injury case, we would get a third of any money generated, so I would generate my cut (a ninth) to the charity of your choice. In an hourly case, I would get a third &nbsp;of the fees generated.</p><p><a href=\"http://www.rwolfflaw.com/\">Randall Wolff &amp; Associates&nbsp;</a> is the firm I work for and we do a great job for our clients; you can check out our high score on Google Reviews. We would love to take your personal injury, worker's compensation, bankruptcy, or divorce cases. We also would be interested in seeing if we can help you with commercial disputes.&nbsp;</p><p>I would like EAs more generally to do stuff like this. Are you a professional where rainmaking could be valuable to your business and are interested in helping effective charities anyway? Make it explicitly known that you will direct your cut of the fees to a specific charity, or a charity of their choice. Earning to Give can be much more effective if you use the fact that you donate your earnings to generate business.</p><p><strong>If you would like to do this, please initiate any conversations about representation by either emailing </strong><a href=\"mailto:brad@rwolfflaw.com\"><strong>brad@rwolfflaw.com</strong></a><strong> or calling 847-222-9465 and speaking with Brad West first. It needs to be clear that I generated the business for me (and thus your charity) to get the fee.</strong></p><p>If you're interested in learning more about the Consumer Power Initiative, check out our newsletter. Also check out BOAS, a company in the EU that sells sustainable baby products and directs all profits to charities.</p><p>CPI Newsletter: <a href=\"https://drive.google.com/open?id=1jXeT6SHoLoaXfkoT_7YCSgpMGHTiDwFU&amp;authuser=volake1%40gmail.com&amp;usp=drive_fs\">https://drive.google.com/open?id=1jXeT6SHoLoaXfkoT_7YCSgpMGHTiDwFU&amp;authuser=volake1%40gmail.com&amp;usp=drive_fs</a></p><p>BOAS website: boas.co</p>", "user": {"username": "Brad West"}}, {"_id": "6xX96ZqFtH5n7mchW", "title": "My emotional reaction to the current funding situation", "postedAt": "2022-09-11T22:07:35.894Z", "htmlBody": "<p>I\u2019m allowed to spend two days a week at Trajan House, a building in Oxford which houses the Center for Effective Altruism (CEA), along with a few EA-related bodies. Two days is what I asked for, and what I received. The rest of the time I spend in the Bodleian Library of the University of Oxford (about \u00a330/year, if you can demonstrate an acceptable \u201cresearch need\u201d), a desk at a coworking space in Ethical Property (which houses Refugee Welcome, among other non-EA bodies, for \u00a3200/month), Common Ground (a cafe/co-working space which I\u2019ve recommended to people as a place where the staff explicitly explain, if you ask, that you don\u2019t need to order anything to stay as long as you like), a large family house I\u2019m friends with, and various cafes and restaurants where I can sit for hours while only drinking mint tea.</p><p>I\u2019m allowed to use the hot-desk space at Trajan House because I\u2019m a recipient of an EA Long Term Future Fund grant, to research Alignment. (I call this \u201cAI safety\u201d to most people, and sometimes have to explain that AI stands for Artificial Intelligence.) I judged that 6 months of salary at the level of my previous startup job, with a small expenses budget, came to about \u00a340,000. This is what I asked for, and what I received.</p><p>At my previous job I thought I was having a measurable, meaningful impact on climate change. When I started there, I imagined that I\u2019d go on to found my own startup. I promised myself it would be the last time I\u2019d be employed.</p><p>When I quit that startup job, I spent around a year doing nothing-much. I applied to Oxford\u2019s Philosophy BPhil, unsuccessfully. I looked at startup incubators and accelerators. But mostly, I researched Alignment groups. I visited Conjecture, and talked to people from Deep Mind, and the Future of Humanity Institute. What I was trying to do, was to discern whether Alignment was \u201creal\u201d or not. Certainly, I decided, some of these people were cleverer than me, more hard-working than me, better-informed. Some seem deluded, but not all. At the very least, it\u2019s not <i>just</i> a bunch of netizens from a particular online community, whose friend earned a crypto fortune.&nbsp;</p><p>During the year I was unemployed, I lived very cheaply. I\u2019m familiar with the lifestyle, and \u2013 if I\u2019m honest \u2013 I like it. Whereas for my holidays while employed I\u2019d hire or buy a motorbike, and go travelling abroad, or scuba dive, instead my holidays would be spent doing DIY at a friend\u2019s holiday home for free board, or taking a bivi bag to sleep in the fields around Oxford.</p><p>The exceptions to this thrift were both EA-related, and both fully-funded. In one, for which my nickname of \u201cHuel and hot-tubs\u201d never caught on, I was successfully reassured by someone I found very smart that my proposed Alignment research project was worthwhile. In the other, I and others were flown out to the San Francisco Bay Area for an all-expenses-paid retreat to learn how to better build communities. My hotel room had a nightly price written on the inside of the door: $500. Surely no one ever paid that. Shortly afterwards, I heard that the EA-adjacent community were buying the entire hotel.</p><p>While at the first retreat, I submitted my application for funding. While in Berkeley for the second, I discovered my application was successful. (\u201cI should hire a motorbike, while I\u2019m here.\u201d I didn\u2019t have time, between networking opportunities.) I started calling myself an \u201cindependent alignment researcher\u201d to anyone who would listen and let me into offices, workshops, or parties. I fit right in.</p><p>At one point, people were writing plans on a whiteboard for how we could spend the effectively-infinite amount of money we could ask for. Somehow I couldn\u2019t take it any more, so I left, crossed the road, and talked to a group of homeless people I\u2019d made friends with days earlier, in their tarp shelter. We smoked cigarettes, and drank beer, and they let me hold their tiny puppy. Then I said my thank-yous and goodbyes, and dived back into work.</p><p>Later, I\u2019m on my canal boat in Oxford. For a deposit roughly the price of my flight tickets, I\u2019ve been living on the boat for months. I get an email: the first tranche of my funding is about to be sent over, it\u2019ll probably arrive in weekly instalments. I\u2019ll be able to pay for the boat\u2019s pre-purchase survey.</p><p>Then I check my bank account, and it seems like it wasn\u2019t the best use of someone\u2019s time for them to set up a recurring payment, and instead the entire sum has been deposited at once. My current account now holds as much money as my life savings.</p><p>I\u2019m surprised by how negative my reaction is to this. I am angry, resentful. After a while I work out why: every penny I\u2019ve pinched, every luxury I\u2019ve denied myself, every financial sacrifice, is completely irrelevant in the face of the magnitude of this wealth. I expect I could have easily asked for an extra 20%, and received it.</p><p>A friend later points out that this is irrational. (I met the friend through <a href=\"https://www.lesswrong.com/groups/wQA8BE5e8mETeWb8A\">Oxford Rationalish</a>.) <i>Really</i>, he points out, I should have been angry long before. I should have been angry when I realised that there were billionaires in the world at all, not when their reality-warping influence happens to work in my favour. My feelings continue to be irrational.</p><p>But now I am funded, and housed, and fed (with delicious complementary vegan catering), and showered (I\u2019m too sparing of water to shower on the boat). I imagine it will soon be cold enough on the boat that I come to the office to warm up; this will be my first winter. And so all my needs are taken care of. I am safe, while the funding continues. And even afterwards, even with no extension, I\u2019ll surely survive. So what remains is self-actualisation. And what I want to do, in that case, is to explore the meaning of the good life, to break it down into pieces which my physics-trained, programmer\u2019s brain can manipulate and understand. And what I want to do, also, is to understand community, build community, contribute love and care. And, last I thought about these things, I\u2019m exactly where I need to be to be asking these questions and developing these skills.</p><p>(I realise, in this moment of writing, that I am not building a house and a household, not working with my hands, not designing spaces. I am also not finding a wife.)</p><p>I have never felt so obliged, so unpressured. If I produce nothing, before Christmas, then nothing bad will happen. Future funds will be denied, but no other punishment will ensue. If I am to work, the motivation must come entirely from myself.</p><p>My writing has been blocked for months. I know what I want to write, and I have explained it in words to people dozens of times. But I don\u2019t believe, on some level, that it\u2019s valuable. I don\u2019t think it\u2019s real, I don\u2019t think that my writing will bring anyone closer to solving Alignment. (This is only partially true.) I have no idea what I could meaningfully offer, in return or exchange. And I can\u2019t bear the thought of doing something irrelevant, of lying, cheating, stealing. Of distracting. Instead, I procrastinate, and \u2013 in seeking something measurable \u2013 organise an EA-adjacent retreat.</p><p>I wander over to the library bookshelves in Trajan House. I pick up a book about community-building, which looks interesting. I see a notice: \u201cLike a book? Feel free to take it home with you. Please just scan this QR code to tell us which book you take :)\u201d I\u2019m pleased: I assume that they\u2019ll ask for my name, so they can remind me later to return the book. This seeming evidence of a high-trust society highlights what I like about EA: everyone is trying to be kind. Then I scan the QR code, and a form loads. But I\u2019m not asked for my name, nor is my email shared with them. They only ask for the title of the book. I realise that \u2013 of course \u2013 they\u2019re just going to buy a replacement. Of course. It would be ridiculously inefficient to ask for the book back: what if I\u2019m still reading it? What if I\u2019m out of town? And whose time would be used to chase down the book? Much better to solve the problem with money. This isn\u2019t evidence of a high-trust society, after all, only of wealth I still haven\u2019t adjusted to. I submit the form, and pocket the book.</p>", "user": {"username": "Sam Brown"}}, {"_id": "YauYvwWkF3KdtBPxo", "title": "Increasing Urban Density as a Recommended Cause Area?", "postedAt": "2022-09-11T13:23:04.565Z", "htmlBody": "<h2>Summary / tl;dr</h2><p>Increasing urban density could be an important cause to work on for people engaged in urban planning and policy. However, uncertainty is large regarding the magnitude of the effect.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6toqspg2d85\"><sup><a href=\"#fn6toqspg2d85\">[1]</a></sup></span></p><h2>Density is important</h2><p>The geographic organization of humanity can take many forms, with profound implications. At one extreme, we could all live in places as dense or denser than Manhattan. At the other extreme we could all live in endless suburbs that would cover a large and ever growing fraction of the habitable land area of the planet, replacing vital ecosystems in the process. How densely people live affects their energy use, time use, wellbeing, productivity and access to services.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr9enwc4gp3\"><sup><a href=\"#fnr9enwc4gp3\">[2]</a></sup></span>&nbsp;It is also path dependent - once land is converted to residential use, it tends to remain in residential use as long as it remains possible to live there.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc4k2gsnxmz7\"><sup><a href=\"#fnc4k2gsnxmz7\">[3]</a></sup></span>&nbsp;Therefore, current choices will have strong effects on the future geographic organization of humanity.&nbsp;</p><h2>Policy determines density</h2><p>Where people live is determined by demand and supply: where people want to live and where housing is affordable and available. In many nations, the construction of new housing in existing cities is restricted through various legal means, primarily for the benefit of current homeowners in these cities. Restricted supply is thereby displaced to lower demand areas on the outskirts of cities, where dense construction is less profitable. This may result in a suboptimal geographic organization which may justify policy involvement.&nbsp;</p><p>There is reason to believe the resulting organization is far from optimal. To see why, note that there is a strong positive correlation between population density in a city, population size in a city and rent prices in a city. In particular, rent on a typical apartment in one of the densest cities is far above the economic costs of supplying an additional such apartment. This implies that the supply of such apartments is restricted to be well below demand for such apartments.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref96y5qvw11w6\"><sup><a href=\"#fn96y5qvw11w6\">[4]</a></sup></span>&nbsp;Therefore, in a standard model of supply and demand without externalities, increasing the supply of such apartments would greatly increase total welfare.&nbsp;</p><h2>Density benefits others</h2><p>Such a calculation only considers private benefits by assuming there are no externalities. That is, that the choice of living in a dense city has no effect on the welfare of other people. However, recent empirical evidence suggests that the externalities of density are positive. That is, adding a person to a dense city makes other people better off compared to not adding that person, all else equal.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefai9aytytu7g\"><sup><a href=\"#fnai9aytytu7g\">[5]</a></sup></span>&nbsp;The external benefits from density seem to stem primarily from 4 factors:</p><ul><li>lower energy usage,&nbsp;</li><li>lower time usage,&nbsp;</li><li>lower land usage,&nbsp;</li><li>and increased innovation.&nbsp;</li></ul><p>In particular, the fact that increasing density reduces both distance travelled and travel time implies that congestion, the main negative externality of density, is dominated by the increased transportation efficiency of density. The other external benefits thus constitute pure gain to society.&nbsp;</p><p>An important caveat is that current estimates of the external benefits of density are based on comparing more and less dense cities in the current distribution of city density among the largest cities in advanced economies, and mostly in the US. This has two main implications.&nbsp;</p><ol><li>It is very likely that there is some level of density beyond which this would no longer hold. However, existing estimates suggest that we are not there yet as discussed above in relation to congestion. This implies that even if every city in the US was made to be as dense as NYC, externalities from density would still be positive.</li><li>In addition, large cities in advanced economies tend to have high quality infrastructure to support density. While such infrastructure tends to pay for itself quite quickly, some countries that lack the knowhow and access to finance to build such infrastructure may have greater difficulty in supporting greater density. This implies a need for caution when advocating for density in poorer nations but also implies a scope for philanthropic work in developing the needed capacity.&nbsp;</li></ol><h2>An important cause area?</h2><p>Both the average individual and the public as a whole would benefit from letting many cities be much denser than they currently are. However, all the above does not mean that EA should get involved. To argue for EA involvement, we need to show that such involvement would be effective and existing evidence does not suggest a high level of effectiveness, for several reasons</p><ol><li>The external benefits are not very large, at least based on estimates from the US. Doubling density in the US would reduce net carbon emissions by about 7% and increase growth by at most 1.76% but probably by only 0.037% to 0.22%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbipcvn9jlyj\"><sup><a href=\"#fnbipcvn9jlyj\">[6]</a></sup></span>&nbsp;</li><li>The problem is not very tractable because of entrenched political opposition from insiders.&nbsp;</li><li>Wealthy nations should be able to increase density and capture a large part of the benefits. For density to be beneficial in poor nations, their organizational, legal and physical infrastructure needs to be far better compared with current levels.&nbsp;</li></ol><p>To summarize, there are many reasons to be cautiously optimistic that increasing urban density could be an important cause to work on for people engaged in urban planning and policy. However, it is still not clear what exactly is the magnitude of that effect, and whether it should be generally recommended.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6toqspg2d85\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6toqspg2d85\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A previous <a href=\"https://forum.effectivealtruism.org/topics/land-use-reform\">post</a> on this topic emphasized the GDP lost to land use reform and outlined several courses of action. In contrast, we emphasize externalities such as effects of urban density on innovation and on GHG emissions, which are informative about the long term net benefits to humanity of higher density.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr9enwc4gp3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr9enwc4gp3\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.34.3.3\"><u>Duranton and Puga 2020</u></a> and&nbsp;<a href=\"https://doi.org/10.1016/j.jue.2019.04.006\"><u>Ahlfeldt and Pietrostefani 2019</u></a> are two recent reviews of the accumulated evidence.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc4k2gsnxmz7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc4k2gsnxmz7\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.aeaweb.org/articles?id=10.1257/000282802762024502\"><u>Davis and Weinstein 2002</u></a> find that the distribution of population is highly persistent over the 6 thousand years and that even the devastation of Japanese cities during the last year of WWII had no lasting effects on where people live. They report that the populations of the typical Japanese city completely recovered within 15 years of the end of the war and that Hiroshima and Nagasaki both exceeded prewar population levels within a decade of being bombed with nuclear weapons. More prosaically,&nbsp;<a href=\"https://www.aeaweb.org/articles?id=10.1257/aer.20131706\"><u>Diamond 2016</u></a> finds that people in the US have a strong preference to live near where they grew up.&nbsp;<a href=\"https://www.journals.uchicago.edu/doi/abs/10.1086/427465?journalCode=jpe&amp;mobileUi=0\"><u>Gleaser and Gyourko 2005</u></a> document an almost perfect correlation between the number of housing units and the number of residents in a US city.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn96y5qvw11w6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref96y5qvw11w6\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.aeaweb.org/articles?id=10.1257/jep.32.1.3\"><u>Glaeser and Gyourko 2018</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnai9aytytu7g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefai9aytytu7g\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.34.3.3\"><u>Duranton and Puga 2020</u></a> and&nbsp;<a href=\"https://doi.org/10.1016/j.jue.2019.04.006\"><u>Ahlfeldt and Pietrostefani 2019</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbipcvn9jlyj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbipcvn9jlyj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The net carbon emissions estimate is from&nbsp;<a href=\"https://doi.org/10.1016/j.jue.2019.04.006\"><u>Ahlfeldt and Pietrostefani 2019</u></a>; The growth estimates are arrived at by using the estimated effect of density on patents from&nbsp;<a href=\"https://doi.org/10.1016/j.jue.2019.04.006\"><u>Ahlfeldt and Pietrostefani 2019</u></a> and the estimated effects of patents on productivity from&nbsp;<a href=\"https://academic.oup.com/qje/article-abstract/132/2/665/3076284?redirectedFrom=fulltext\"><u>Kogan et al 2017</u></a> (0.037% to 0.22%) and&nbsp;<a href=\"https://doi.org/10.1162/REST_a_00058\"><u>Balasubramanian and Sivadasan 2011</u></a> (1.76%). I give greater weight to&nbsp;<a href=\"https://academic.oup.com/qje/article-abstract/132/2/665/3076284?redirectedFrom=fulltext\"><u>Kogan et al 2017</u></a>\u2019s estimates because they have a credible causal specification and because they calculate the effect of patents on total factor productivity, while&nbsp;<a href=\"https://doi.org/10.1162/REST_a_00058\"><u>Balasubramanian and Sivadasan 2011</u></a>\u2019s strategy is not causal and they calculate the effect of patents only on labour productivity which is difficult to related to growth without further details which they do not provide.</p></div></li></ol>", "user": {"username": "Niki Kotsenko"}}, {"_id": "bdSpaB9xj67FPiewN", "title": "A Pin and a Balloon: Anthropic Fragility Increases Chances of Runaway Global Warming ", "postedAt": "2022-09-11T10:22:35.918Z", "htmlBody": "<p><i>Alexey Turchin, </i><a href=\"mailto:alexeiturchin@gmail.com\"><i>alexeiturchin@gmail.com</i></a><i>&nbsp;</i></p><p><i>Anonymous coauthor 1&nbsp;</i></p><p><i>Anonymous coauthor 2</i></p><p>&nbsp;</p><p>&nbsp;<strong>TL;DR:</strong> Because of survival bias we underestimate the location of climate tipping points and thus our world is more fragile to small anthropogenic actions. Therefore, human extinction because of runaway global warming is more probable.</p><p>&nbsp;</p><p><strong>Abstract</strong>: Humanity may underestimate the rate of natural global catastrophes because of the survival bias (\u201canthropic shadow\u201d). But the resulting reduction of the Earth\u2019s future habitability duration is not very large in most plausible cases (1-2 orders of magnitude) and thus it looks like we still have at least millions of years.&nbsp;</p><p>However, anthropic shadow implies <i>anthropic fragility</i>: we are more likely to live in a world where a sterilizing catastrophe is long overdue and could be triggered by unexpectedly small human actions. In the same way, an over-inflated toy balloon, which will soon burst, is very fragile.</p><p>Anthropic fragility can manifest itself in the higher chances of runaway global warming. It has often been suggested that the Earth's atmosphere remained life-supporting for billions of years by sheer chance. Therefore, the survival bias can be strong. It is also known that Earth-like water worlds could experience transitions into deadly moisture greenhouse (mean T = 65C). This means that relatively small anthropogenic actions could put the climate above an unpredictable tipping point, which could lead to the moisture greenhouse. Thus, it is necessary to carry out urgent geoengineering studies and prepare to prevent an unexpected climate catastrophe.</p><p>There are three main counterarguments against the existence of the anthropic shadow: self-indication assumption (SIA), past observers and the Gaia hypothesis; we show that they fail. It was proposed that SIA exactly compensates the anthropic shadow as an observer unlikely to find herself in a world with a strong anthropic shadow; however, there is a baseline level of the anthropic shadow for all habitable planets, similar to the rate of evolutionary transitions like abiogenesis. There are no \u201cpast observers\u201d as qualified observers appeared only 50 years ago. Gaia hypothesis assumes existence of self-stabilizing feedback in climate, but new types of events like quick CO2 growth could override its coping ability.</p><p>We present a list of other catastrophes that may have been underestimated because of the anthropic shadow, including collider catastrophes, nuclear war and even an alien invasion.&nbsp;</p><p>We also hypothesised that human intelligence is more likely to emerge in an unstable world that is nearing its end and thus we get a new form of Doomsday argument.</p><p><i>pdf with Appendix: &nbsp;</i><a href=\"https://philpapers.org/rec/TURAPA-9\"><i>https://philpapers.org/rec/TURAPA-9</i></a></p><p>&nbsp;</p><p><strong>Highlights</strong>:</p><p>\u00b7 The decrease of the expected habitability time of the Earth\u2019s because of the survival&nbsp;</p><p>bias (anthropic shadow) is not very large and thus it looks like that we still have at least millions of years.&nbsp;</p><p>\u00b7 But anthropic shadow implies anthropic fragility: we are more likely to live in a world where a catastrophe is long overdue and could be triggered by unexpectedly small human actions.</p><p>\u00b7 Catastrophic runaway global warming may be an example of such anthropic fragility, and we cannot predict where the tipping point is.</p><p>\u00b7 We should research urgent geoengineering to counter the unpredictable anthropic fragility of climate.</p><p>\u00b7 We also suggested a hypothesis that human intelligence is more likely to arise in an unstable world that is nearing its end.</p><p>&nbsp;</p><h1>1. Introduction</h1><p>One formulation of the anthropic principle (Carter, 1974) is that observers can exist only in those worlds in which there are no conditions that prevent the appearance of the observers. Such conditions may take the form of fine-tuning of the initial parameters of the universe \u2013 or the absence of life-ending catastrophes. Circovic et al called the lack of life-ending catastrophe in the past \u201canthropic shadow\u201d, as it causes underestimation of the background rate of catastrophes.</p><p>The goal of this article is to research how anthropic shadow affects the fragility of our world, in the sense of Bostrom\u2019s vulnerable world hypothesis (2018), especially relative to the risk of runaway global warming</p><p>Our central argument and illustration are presented in section 2. We explore the idea of anthropic fragility and how it could work in different types of catastrophes, first of all, for global warming in section 3. We shortly discuss what types of geoengineering may be needed to counter unexpected rapid global warming. We will create an overview of all possible types of catastrophes where anthropic shadow and anthropic fragility can manifest themselves in table 1 in section 4. Then we will go into counterarguments that were suggested against the anthropic shadow and will demonstrate that they can\u2019t completely disprove it, but they limit its power. We will explore what kind of evidence of anthropic shadow in the past of Earth we have in section 5. In the end, we will take more a general overview of the problem and of its connection with x-risks. We will suggest a new form of the Doomsday argument: that intelligent life is likelier to appear in the unstable world which is close to its end.&nbsp;</p><h3>Previous literature review</h3><p>Bostrom and Tegmark wrote:&nbsp;</p><blockquote><p>One might think that since life here on Earth has survived for nearly 4 Gy (Gigayears), such catastrophic events must be extremely rare. Unfortunately, such an argument is flawed, giving us a false sense of security. It fails to take into account the observation selection effect that precludes any observer from observing anything other than that their own species has survived up to the point where they make the observation. Even if the frequency of cosmic catastrophes were very high, we should still expect to find ourselves on a planet that had not yet been destroyed. The fact that we are still alive does not even seem to rule out the hypothesis that the average cosmic neighborhood is typically sterilized by vacuum decay, say, every 10,000 years, and that our own planet has just been extremely lucky up until now. If this hypothesis were true, future prospects would be bleak\u201d.&nbsp;(Tegmark &amp; Bostrom, 2005)</p></blockquote><p>However, they also demonstrated based on the relatively late appearance of Earth in the history of the Universe that the space catastrophes capable of sterilizing Earth should be relatively rare, occurring no more often than once in 1 billion years with 99.9 per cent confidence. This is an example of SIA counterargument discussed in Section 4, which unfortunately does not work for Earth habitability</p><p>Circovic, Sandberg and Bostrom wrote a subsequent article, \u201cAnthropic Shadow: Observation Selection Effects and Human Extinction Risks\u201d in 2010 (\u0106irkovi\u0107, Sandberg, &amp; Bostrom, 2010), in which they created a Bayesian update equation to calculate the actual probability of a natural global catastrophe which takes into account observation selection effects. In that article, they listed five different types of natural catastrophes which observed probabilities could be affected by observation selection: asteroid/comet impacts, solar superflares, supervolcanic eruption, close flyby of rogue black holes and nearby supernovae explosions or gamma-ray bursts.</p><p>However, Ord et al (Snyder-Beattie et al., 2019) suggested that the natural catastrophes background rate for Homo Sapiens is not affected by the observation selection effects, as there could be <i>earlier observers</i> (this will be discussed in section 4.2).&nbsp;</p><p>Manheim applied anthropic bias analysis to the frequency of natural pandemics in the past (Manheim, 2018).</p><p>Meanwhile, A. Scherbakov noted that the history of the Earth\u2019s atmosphere is strangely correlated with the solar luminosity and the history of life, which could be best explained by anthropic fine-tuning, in the article \u201cAnthropic principle in cosmology and geology\u201d (Shcherbakov, 1999). In particular, he wrote that the atmospheric temperature was closely preserved in the range of 10\u201340 \u00b0C, and on four occasions the Earth came close to a \u201csnowball\u201d steady-state, and on four occasions came close to turning into a water vapor greenhouse where the temperature could reach of hundreds of degrees centigrade. However, these life-ending outcomes were prevented by last-minute events such as volcanic eruptions or covering of volcanoes in the ocean by water, which regulates the CO<sub>2</sub> level following an eruption. Such \u201cmiracles\u201d are best explained by observation selection effects.&nbsp;</p><p>Waltham looked at the Milankovitch cycles and using the modelling of a group of planets found that the Solar system has unusually low orbit perturbations: \u201c\u2026the probability of all three occurring by chance is less than 10<sup>\u22125</sup>. It therefore appears that there has been anthropic selection for slow Milankovitch cycles. This implies possible selection for a stable climate, which, if true, undermines the Gaia hypothesis and also suggests that planets with Earth-like levels of biodiversity are likely to be very rare\u201d&nbsp;(Waltham, 2011).</p><p>In the article \u201cOn the absence of solar evolution\u2010driven warming through the Phanerozoic\u201d Waltham argues in favor of anthropic explanations of climate stability: \u201cThe Gaia hypothesis, anthropic selection or some other unconventional mechanism may therefore have to be invoked to explain the absence of long\u2010term warming through the Phanerozoic\u201d (Waltham, 2014).</p><p>In 2020, two important articles on the topic were published. The article by Tyrrell \u201cChance played a role in determining whether Earth stayed habitable\u201d&nbsp;(Tyrrell, 2020) demonstrated via computer modelling of the history of many Earth-like planets that chance played a significant role in preserving the stability of the atmosphere.&nbsp;</p><p>Another article, \u201cThe Timing of Evolutionary Transitions Suggests Intelligent Life Is Rare\u201d&nbsp;(Snyder-Beattie et al., 2020) is analyzing the timing of the important steps of the evolution of life and suggests that the observed frequency of such steps has strong anthropic effects, and median frequency is likely much lower, which implies longer expected time of development of intelligent life; from this follows Rare Earth. Robin Hanson in the \u201cGrabby aliens\u201d article discussed similar ideas&nbsp;(Hanson et al., 2021).</p><p>Wordsworth&nbsp;(Wordsworth, 2021) showed that runaway global cooling (snowball Earth) is still probable and could start if the global temperature falls to 7 C, and it was just a few degrees more than that only 20 000 ago at the peak of the Ice Age. He regards it as a possible explanation of the Fermi paradox, as in the snowball Earth glaciers will reach the equator and will destroy all complex life.</p><p>Survival bias is clearly ignored in some estimations of the risks of runaway global warming:&nbsp;</p><blockquote><p>\u201cThe paleoclimate record can be used to check our prediction that surface temperatures might increase dramatically were they to exceed ~305 K (Fig. 4b). The closest analog to the climate regime modeled here is the mid-Cretaceous Period, ~100 million years ago, which is considered to be the warmest Earth has ever been during the Phanerozoic eon. Life clearly existed and flourished during this time, so we know that whatever surface temperature prevailed then was not high enough to trigger a moist greenhouse.\u201d&nbsp;<u>(Ramirez et al., 2013)</u></p></blockquote><p>&nbsp;</p><h1>2.&nbsp;The central argument and a toy example</h1><h2>2.1. Toy example</h2><p>Imagine that every 100 million years <i>on average</i> a sterilizing catastrophe is happening on any Earth-like planet. It is a random process with a half-life of 100 of millions years. It is completely unobservable if it didn\u2019t happen.&nbsp;</p><p>We suggest here as a toy example that such a catastrophe is a nuclear explosion of a natural nuclear reactor in the Earth core, which some hypothesized to exist <u>(Herndon, 1993)</u>. The explosion of the reactor will not destroy the planet but it will produce strong shockwaves and volcanism which will result in complete surface replacement in Earth, like the one which has happened in Venus. As Earth has existed for 4.5 billion years, the chances of the Earth to survive until now is 1 in 2<sup>45&nbsp;</sup>=3.5x10<sup>13</sup>. That is, anthropic shadow on Earth with the power around 10<sup>13</sup> corresponds to the future life expectancy of 100 million years.&nbsp;</p><p>If the reactor explodes with the half-life of 10 million years, the power of anthropic shadow is 1 in 10<sup>135</sup>, which is truly immense. But for evolutionary transitions like abiogenesis, such probabilities are not uncommon and we will show later (section 6) that the power of anthropic shadow is likely to be similar to the power of evolutionary transitions. Totani estimated the chances of abiogenesis and calculated that only 1 of 10<sup>100</sup> planets will generate self-replicating RNA via randomness <u>(Totani, 2020)</u>.</p><p>Even if the reactor explodes every once in one billion years, we have one billion years of future life expectancy, but anthropic power is 1 in 16. If there will be no anthropic shadow then expected future survival will be equal to the past survival and will be 4.5 billion years.</p><p>It seems that human civilization will be fine, as we still will have 10 million years (with 50 per cent probability) in the worst case, which is enough to become a multi-planetary civilization.</p><p>However, any small additional fuel will trigger the explosion. Russian scientists suggested sending a probe to the Earth\u2019s core based on a small nuclear reactor that melts all the way down <u>(Ozhovan et al., 2005)</u>. If the natural reactor is \u201ctrigger-happy\u201d, such experiment could be enough to destabilize it. Such an Earth-core probe will be like a pin that is poking an overinflated balloon \u2013 dangerous game!&nbsp;</p><p>&nbsp;Remind you, that it is a toy example, so the real probe unlikely will reach the core, but it is still could cause large a degasation event <u>(Cirkovic &amp; Cathcart, 2003)</u>.&nbsp;</p><p>Moreover, the problem is that the nuclear reactor inside Earth could have accumulated the fuel all that time. We suggested before that its explosion is a truly random effect, but for some processes like inflating balloons or overstretched springs, the probability and the power of explosions grow non-linearly in time. This means a significantly shorter future life expectancy and higher fragility. Here we looked at <i>exponentially distributed</i> risks. We look at <i>normally distributed</i> catastrophes in Appendix 2 and we assume there that for an overinflated-balloon-type of catastrophe the probability is <i>normally</i> distributed around some mean value.&nbsp; In that case, for example, 14 sigmas anthropic shadow event has 1 in 10<sup>45</sup> chances. It produces the future habitability of Earth equal to 0.04 per cent of the past age, but it is still 1.6 million years half-life which provides enough time for a civilization to leave Earth.</p><p>If there are two universes, and in one of them all Earth-like planets have half-life 100 million years because of the core reactor explosions, and in another universe, it is 200 million years, \u2013 at the end, there will be 2<sup>22.5</sup>=500 million times<sup>&nbsp;</sup>more habitable planets in the second universe. Therefore, we are more likely to find ourselves in the universe with a weaker anthropic shadow. The second universe provides the baseline level of the anthropic shadow if only two types of universes exist. This is the <i>SIA counterargument</i> in a nutshell which will be discussed later in section 4.&nbsp;</p><p>This thought experiment demonstrated to us the following properties of the anthropic shadow:&nbsp;</p><p><i>1. Even a minor anthropic shadow means a significant cut of future life expectancy</i>.&nbsp;</p><p><i>2. The sensitivity of the future life expectancy to the power of anthropic shadow is relatively small. The growth of anthropic shadow for 10<sup>134 </sup></i>times lowers the life expectancy by only 100 times.</p><p><i>3. Any anthropic shadow means significant growth of fragility.</i>&nbsp;</p><p><i>4. The power of the anthropic shadow is limited to some baseline</i> which applies to all Earth-like planets.&nbsp;</p><p>5<i>. Non-linear probability distributions of catastrophes mean much shorter future life expectancy time and higher fragility</i>.&nbsp;</p><p>&nbsp;</p><p>Humanity will either become an interplanetary civilization in the next millennia, or it will never do it, given the expected quick growth of nanotech and AI. Therefore, a million years timescale of natural catastrophes is not itself a significant risk for our space future.&nbsp;</p><h2>2.2. The central argument</h2><p>The toy example above helps us to formulate the central argument about the anthropic fragility of the habitability of the Earth, which we will explore later in detail. The argument runs according to the following lines:</p><p>&nbsp;</p><ol><li>Habitable planets with intelligent life in the Universe are rare because many types of catastrophes could kill life.</li><li>The fact that we are alive means that we were very lucky and have escaped many past catastrophes.</li><li>But therefore, we can\u2019t estimate the frequency of such sterilizing catastrophes based on the observations. This is the anthropic shadow.</li><li>If some catastrophe is long overdue, this lowers our future life expectancy for around 1-2 order of magnitude, based on the low sensitivity of anthropic shadow to initial parameters. This seems to be not problematic as it still gives us millions of years of life expectancy.</li><li>However, the fragility of our environment also has grown, and thus relatively small or <i>unnatural</i> anthropogenic actions could cause an unexpected catastrophe.&nbsp;</li><li>The primary risk here is a sudden climate catastrophe caused by crossing an unexpected tipping point which will start a positive feedback loop and will increase Earth\u2019s temperature to 65 C moisture greenhouse level or even higher. Collider accidents and nuclear war are other examples of anthropic fragility.</li><li>To counter this <i>unobservable fragility</i>, we need to be more careful and to have quick reaction instruments, like urgent geoengineering.</li><li>However, as intelligence is a universal adaptation, it is more likely to evolve in the world with changing climate, and such changes themselves are the sign that the climate catastrophe is near. This increases the chances of catastrophe soon. More on that in section 6.</li></ol><p>&nbsp;</p><h1>3. Anthropic fragility and global catastrophes affected by it</h1><h2>3.1. Anthropic fragility:&nbsp;underestimation of the fragility of our environment because of the anthropic shadow</h2><p>Now it\u2019s time to look deeply at what is anthropic fragility. Not all types of survival bias result in anthropic fragility. If a plane with holes from an enemy fire has returned to the base, there is no anthropic fragility. Thus, anthropic fragility requires that the situation of risk is <i>continuous</i>. But even this is not enough: asteroid impact risk is continuous, but there is no anthropic fragility, as there are no meaningful ways how humans could affect the probability of large impacts, except asteroid deflection. Poking an overinflated balloon is an example of the fragility, and injuries from falling for an older man is another \u00ad\u2013 but all these examples are not anthropic: there is no observation selection effect which results in the underestimation of risks.&nbsp;</p><p>If a plane with many holes is still flying back to its base, it is closer to anthropic fragility, as the plane is damaged and any sharp turn could put too much force on its structures and will fail apart.&nbsp;</p><p>Anthropic fragility is the following situation:</p><ol><li><i>Anthropic shadow.</i> There is a strong anthropic shadow, so we live in a world where some kind of otherwise very probable sterilizing catastrophe has not yet happened by pure chance.</li><li><i>Parameter</i>. The probability of the catastrophe is connected with a slow accumulation of some parameter, similar to the pressure in an inflating balloon.</li><li><i>Tipping point</i>. The catastrophe will inevitably happen if the parameter will achieve some threshold level (or at least, catastrophe\u2019s probability significantly increases if the parameter grows even slightly).</li><li><i>Human actions</i>, like experiments or emissions, are changing the value of the parameter, so it could reach the threshold. Human actions may affect some other foundations of stability that don\u2019t look fragile at first glance. These indirect human actions must be <i>unique</i> in the sense that they never happened before in exactly the same way.</li><li><i>Unknown to humans</i>. Humans do not know the real activation level of the parameter because they can\u2019t infer its value from the past rate of catastrophes, which they never observed because of the survivorship bias. They may even not know that such a catastrophe is possible at all.</li></ol><p>&nbsp;</p><p>Anthropic fragility is defined both as <i>physical</i> and <i>epistemic</i> situations: the physical situation is that a catastrophe is long overdue, and the epistemic part is that we can\u2019t know it from the past observation and therefore underestimate the safe level of the changes of the parameter. Even if we agree with the idea of anthropic fragility, we still can\u2019t know which parameter is causing fragility and what are its safe levels of manipulation. In the damaged plane example, the pilots may know that the plane is fragile, but do not know which actions are risky: climbing, turning or landing.</p><p>There are two types of anthropic fragility: one is when we increase the pressure on something which is already under pressure, that is, like adding more air in the inflated balloon, and another is when we perform unique, never happened before actions on a system which is in a metastable condition, like poking the balloon with a pin. We could call them a \u201cparameter increase\u201d and \u201cunique actions\u201d. In the case of global warming, adding more CO2 and increasing temperature is the \u201cparameter increase\u201d; and the unprecedented speed of warming and unique methane effects are an example of unique human actions. In reality, both types of fragility are connected, because unique actions cause some parameter increase or the parameter increases in some unique way.&nbsp;</p><p>For example, if our false vacuum is very close to the transition to the next metastable state, Large Hadron Collider (LHC) experiments may be more dangerous than they appear&nbsp;(Hut &amp; Rees, 1983) because they create some unique type events, as was suggested by Kent, more on that below.&nbsp;</p><h2>3.2. Runaway global warming as a most dangerous form of the anthropic fragility</h2><p>&nbsp;</p><h3>3.2.1. Moist greenhouse, tipping points and sea-floor methane</h3><p>A recent article suggested, based on computer simulation, that water-world planets (which Earth is similar to) have a second semi-stable temperature regime, <i>moist greenhouse,</i> with a mean temperature of 57\u00b0C while Earth now has a mean temperature of 15\u00b0C&nbsp;(Popp et al., 2016). All temperature regimes between these two will collapse to either the current climate or to a moist greenhouse; thus, there should be some <i>tipping point</i> between these two temperatures after which positive feedback loops dramatically accelerate.&nbsp;</p><p>We assume here that the moist greenhouse will cause human extinction, the same way as ocean evaporation could do it, so there is no practical difference between the two from anthropic and existential risks views. However, there is a small possibility to survive moist greenhouse in some coldest places on Earth like very high mountains, like Himalayas, and in the Antarctic. Toby Ord estimated in the <i>Precipice </i>(Ord, 2020) that there is a 0.1 per cent probability of the existential risk this century because of climate change, mainly due to the start of the moist greenhouse.&nbsp;</p><p>The idea of tipping points in climate has been often discussed, but because of anthropic shadow, the position of the actual location of the tipping point could be underestimated. In a climate context, \u201ctipping point\u201d means not only a temperature but some combination of temperature, greenhouse gases concentrations, albedo, solar luminosity and sea-floor methane release rate. The limited size of the previous \u201cclose-call\u201d tipping points, like PETM warming, could be explained by the observation selection effect: if the Earth had turned into a moist greenhouse 55 million years ago, there would be no observers now.</p><p>The question of the speed and the possibility of methane-driven climate change is a topic of scientific debate&nbsp;(Shakhova et al., 2010) which are too complex to be completely presented here. But anthropic shadow effect of climate fragility increases our uncertainty about such feedback loops. If the transition takes only a few years, we will not have time to cope with it via geoengineering, except perhaps via a nuclear explosion in a supervolcano to create artificial volcanic winter. However, the anthropic shadow prevents us from observing quick and large magnitude changes of the climate in the past, if they are possible, so the absence of such events in our history is not evidence for further slow future climate changes.</p><p>The anthropic fragility becomes especially important relative to the catastrophes which <i>are long overdue</i>. An overinflated toy balloon is in a metastable state, where even a small punch could lead to its explosion. Higher levels of radiation from the Sun has been compensated by historically low levels of CO2, which, however, helped glaciation and methane accumulation in the permafrost and ocean floor, which later could cause a very large \u201cclathrate gun\u201d rapid warming&nbsp;<u>(Kennett et al., 2003)</u>. This fragility can\u2019t be observed in the historical record because of anthropic shadow. We should be extremely careful with climate change.&nbsp;</p><p>The main difference between now and previous periods of warming is the large accumulation of methane hydrates which by some estimates are 10 times higher now than during PETM, as current ice age conditions helped such accumulation on the seafloor and in the permafrost&nbsp;(Ananthaswamy, 2015, 2015; Dean et al., 2018). The topic of risks from methane eruption&nbsp;(Ananthaswamy, 2015; Dean et al., 2018) from the Arctic is controversial: some claim that it is the main risk of warming and other present models that methane leaking will not be enough to start runaway global warming. Anthropic shadow could make us underestimate the power of previous methane discharges. Also, there is a difference between anthropogenic and natural effects on methane: as methane is a short-lived gas in the atmosphere, its concentration depends on the speed of its leaking from reservoirs, which itself depends on the speed of the temperature change. Anthropogenic global warming is relatively quick, because of the unprecedented speed of CO<sub>2</sub> emissions and thus will produce more methane concentrations than the same CO<sub>2</sub>-driven warming if it were slower. Recent research showed that the speed of change was a characteristic of past mass extinctions&nbsp;<u>(Song et al., 2021)</u>.</p><p>Many argue that climate change is not an existential risk and that its danger is exaggerated&nbsp;(Lomborg, 2020). But what makes it a real existential risk is the fat tail of uncertainty of its magnitude powered by anthropic bias.</p><p>&nbsp;</p><h3>3.2.2. Runaway Greenhouse</h3><p>Besides moisture greenhouse, which is hypothetically survivable, there is an even worse scenario, when whole oceans evaporate and equilibrium temperature reaches 1400K, which is described in the article \u201cThe Runaway Greenhouse: implications for future climate change, geoengineering and planetary atmospheres\u201d&nbsp;(Goldblatt &amp; Watson, 2012):&nbsp;</p><blockquote><p>The ultimate climate emergency is a \u201crunaway greenhouse\u201d: a hot and water vapor rich atmosphere limits the emission of thermal radiation to space, causing runaway warming. Warming ceases only once the surface reaches&nbsp;\u223c1400 K and emits radiation in the near-infrared, where water is not a good greenhouse gas. This would evaporate the entire ocean and exterminate all planetary life. Venus experienced a runaway greenhouse in the past, and we expect that Earth will in around 2 billion years as solar luminosity increases.</p></blockquote><p>Goldblatt &amp; Watson concluded that CO2 emissions alone can\u2019t cause this, but if other warming sources will add up, this becomes possible:</p><p>&nbsp;</p><blockquote><p>The question here is simply how much could human action increase the strength of the greenhouse effect? Kasting &amp; Ackerman (1986) found that, with carbon dioxide as the only non-condensible greenhouse gas, over 10,000 ppmv would be needed to induce a moist greenhouse. This is likely higher than could be achieved by burning all the \u201cconventional\u201d fossil fuel reserves\u2014though the actual amount of fossil fuel available is poorly constrained, especially when one includes \u201cexotic\u201d sources such as tar sands (which are already being exploited). Greenhouse gases other than carbon dioxide, cloud or albedo changes could all contribute further warming. Likewise, the exhibition of multiple equilibria in the relevant temperature range (Renn\u00b4o, 1997; Pujol &amp; North, 2002) complicates matters.&nbsp;</p></blockquote><p>&nbsp;</p><p>They conclude: We cannot therefore completely rule out the possibility that human actions might cause a transition, if not to full runaway, then at least to a much warmer climate state than the present one.&nbsp;</p><p>High climate sensitivity might provide a warning.\u201d&nbsp;(Goldblatt &amp; Watson, 2012) and then argue that we may need geoengineering to stop runaway warming. Growing climate sensitivity could be a warning sign. However, they validate the sensitivity estimates on past climate without taking into account a possible anthropic shadow: \u201cSuch high sensitivity is inconsistent with our knowledge of paleoclimate and the model cases which provide the extremes do not seem likely (due to poor representation of contemporary climate)\u201d.</p><p>Therefore, the question is: are existing climate models are capable to predict risks of runaway warming? If yes, we should not worry about the anthropic fragility of climate. But the models are limited:&nbsp;</p><blockquote><p>Ideally, we would want numerical climate models to robustly resolve the transition to a much hotter atmosphere. However, most such models have been developed for fairly small perturbations from the existing climate and their wider applicability may be limited by the obvious unavailability of data to tune the model to, and by simplifications made to reduce computational cost\u2026 A new generation of model may well be needed (Collins et al., 2006; Goldblatt et al., 2009)).&nbsp;</p></blockquote><p>&nbsp;</p><h3>3.2.3. CO2 concentrations which could start runaway warming</h3><p>We could measure <i>anthropic fragility&nbsp;</i>as an amount of deviation from the current level of some parameter, which will cause a global catastrophe and which estimation is distorted by the anthropic shadow. For example, how much CO<sub>2</sub> could be added to the atmosphere before a <i>tipping point</i> is reached?</p><p>The Earth in the past had CO<sub>2</sub> levels much higher than today, so a <i>na\u00efve</i> view is that even having 10 times more CO<sub>2</sub> than now will not cause runaway global warming. But this view does not take into account the anthropic effects, that is, we cannot use the past data about the probability of runaway global warming, as we could observe ourselves only on a planet where runaway global warming never happened. Also, previous higher levels of CO<sub>2</sub> were at least partially compensated by lower Sun\u2019s luminosity, lower deposits of methane hydrates and other geophysical factors then, such as the different configurations of oceans and different parameters of Earth orbit. Such factors in the past worked as protection against runaway global warming, but they are not in place now.</p><p>There are different assessments of the critical levels of CO<sub>2</sub> after which runaway warming will happen. As was cited above 10 000 ppm is needed for a moisture greenhouse. Another estimation of the dangerous level is 30 000 ppm CO<sub>2</sub> which is unachievable by anthropogenic mineral fuel consumption&nbsp;(Goldblatt et al., 2013), and another estimation is only 12 times the preindustrial level, that is, 3360 ppm&nbsp;(Ramirez et al., 2014). The current CO<sub>2</sub> level seems to be the highest in the last 5 million years ago and could become higher than it was in the whole Miocene (20 million years long) in the next 100 years&nbsp;(Dean et al., 2018).</p><p>According to IPCC\u2019s, in the worst-case SPP5-8.5 scenario the CO<sub>2</sub> levels could reach 1000 ppm at the end of the 21 century&nbsp;(Allan et al., 2021), which seems to be not enough to trigger runaway global warming, according to cited above scenarios. &nbsp;</p><p>However, the real problem of the anthropic shadow is that we can\u2019t know for sure that this CO2 level will not cause runaway warming. There are several sources of uncertainty:&nbsp;</p><p>&nbsp;</p><ul><li>Large uncertainty comes from the possibility of methane emissions from the permafrost and other sources. Methane warming potential and release rate depend on <i>the speed of warming,</i> as methane is short-living gas: if the release rate is low, methane concentration will be also low. Human anthropogenic warming is different from the natural increase of CO2 in the past as anthropogenic CO2 levels are increasing quicker, and thus the role of methane will be higher.</li><li>Other pollutants, like N2O, also have high warming potential.&nbsp;</li><li>Our efforts to cut emissions can backfire, because of aerosol-removal effects, as industrial aerosols block part of sunlight in the upper atmosphere&nbsp;(Hansen &amp; et al, 1992).</li><li>More generally speaking, our models are calibrated on the historical relation between CO2 and global temperature, but because of the anthropic shadow, this relation could be unreliable. We survived only in those worlds where extreme deviations never happened.</li><li>Chaotic nature of climate. In the same way as weather, climate also could be fundamentally unpredictable. There could be strong deviations from the mean and bifurcation between different semi-stable attractors.&nbsp;</li><li>Unpredictable combination of random events may force Earth out from the zone of climate stability. One such event was Younger Dryas when a large ice lake leaked. &nbsp;</li><li>Our models may be incomplete in long run and may not take into account some factors.</li></ul><p>&nbsp;</p><p>We are not trying to say that the IPCC model is wrong. It is the best what we have. What we try to show is that there is a small probability of a <i>tail risk</i>, which, however, has most of the expected negative value.</p><p>Also, the nature of the tipping point is that it is <i>the beginning of a self-accelerating runaway process</i>, but not the point where the process will reach its maximum level. Thus, the CO2-concentration when the runaway warming starts is not the same as the one at which the CO2 level will be high enough to cause tens degrees of warming and human extinction. Methane and water vapor may be the main drivers of warming after reaching the tipping point, so the lack of CO2 sources will not stop the warming. This is similar to poking of a small hole by a needle in a toy balloon, which will inevitably result in its explosion. The needle creates a hole but does not participate in the process of destruction after that.</p><p>Fragility can be illustrated by an example of overstretched spring, in which fragility and life expectancy are measured in the same units: per cents of additional inflation. Therefore, the anthropic fragility should be measured not in the CO2 levels, but in <i>the temperature increase</i>, as CO2 levels are non-linearly proportional to warming and depends on other things that could affect temperature. There were&nbsp;<a href=\"https://www.science.org/content/article/500-million-year-survey-earths-climate-reveals-dire-warning-humanity?fbclid=IwAR0S9OUFLvCQDqeP68ecfKAZrgfatg46L1FxTGJ71iXaNoFOA03eibTFMLo\">periods</a> when the Earth temperature was 15C higher than now and the hypothetical moisture greenhouse temperature is 45C higher than current temperatures, so the na\u00efve view is that we are safe even if maximum expected global warming will happen with around 5-8C temperature increase. Thus, the tipping point for moisture greenhouse is somewhere between 15C and 45 C increase, according to the na\u00efve view.</p><p>However, the idea of anthropic fragility means that we should lower our estimates for around an order of magnitude, which, given all uncertainties, means that the tipping point could lie not in tens but in <i>single digits</i> of temperature increase (that is, between 1.5C and 4.5C, if we just divide on 10 the above estimate). This temperature increase could be reached in the 21st century, and maybe even in the next decade. See the discussion below why PETM warming 55 million years ago is not a safe data point, as at that time was different conditions than now.</p><p>How long it takes from the tipping point to full-blown moisture greenhouse is unclear, and it could be from weeks, if water vapor feedback is activated, to centuries, if methane and ocean thermal inertia will play a major role&nbsp;(Karnaukhov, 2001).&nbsp;It is also unclear, if the warming will stop on the moisture greenhouse level, or will overshoot and will go to the Venusian runaway scenario.</p><h3>3.2.4. Urgent geoengineering may be needed to counter climate models\u2019 uncertainty</h3><p>As humans do not\u2014and cannot because of the anthropic shadow effects\u2014know the location of the climate tipping points (Lenton, 2011) based on the past historical record, it may be prudent for society to perform more research about the climate tipping points and to research methods of <i>urgent geoengineering</i>, which could give us more time if runaway warming starts. One model gives 4 years estimate until moisture greenhouse from the moment of the beginning of radiative forcing (Seeley &amp; Wordsworth, 2021).</p><p>he only such measure which could be used on short notice is the use of nuclear weapons to start nuclear winter by causing large fires in taiga or by the initiation of volcanic eruptions. Such measure could put us back below the tipping point, or give a few more years to prepare better protection measures. Higher altitude explosions over taiga will have less nuclear fallout and could be done using existing delivery systems. The slower method is the use of existing airplanes to perform sulfate stratospheric injection (Halstead, 2018). \u201cNormal\u201d geoengineering aimed at the gradual removal of CO2 does not give us enough flexibility to react to unexpected climate changes. We also should do everything else possible to prevent anthropogenic global warming, first of all, cutting emissions.&nbsp;</p><h2>3.3.&nbsp;Collider experiments as another type of anthropic fragility</h2><p>Another type of fragility is associated with the risk of conducting physical experiments that create completely new conditions on Earth, i.e. experiments at the hadron colliders. It has been suggested that they could cause three types of catastrophes: a false vacuum collapse that would end the entire observable universe, a mini-black hole that would slowly but acceleratingly eat Earth's matter, and a special type of quark matter called strangelets, which supposedly capable of turning ordinary matter into strange matter <u>(Kent, 2004)</u>.&nbsp;</p><p>One popular argument for collider safety is based on the observed safety of high-energy collisions of cosmic rays with Earth\u2019s atmosphere, but it could also suffer from anthropic effects. The argument is that if experiments at colliders could create mini-black holes, then cosmic rays would be capable of it. However, since we have survived for billions of years, this has never happened. Therefore, high-energy collisions cannot create anything dangerous. But this argument clearly suffers from the fact that it does not take into account the survival bias.</p><p>Dar et al. (1999) presented an \"anthropically invulnerable\" argument for the safety of the collider: if high-energy collisions are dangerous, we could observe random supernovae from sudden collapses of other stellar objects and planets. Note that Dar's argument works only for the formation of strangelets and small black holes, which will only cause a local catastrophe on Earth, but not for the collapse of a false vacuum that can destroy the entire universe.</p><p>In the case of collider catastrophes, anthropic fragility plays a role, since human experiments turn out to be different from natural phenomena and, thus, can cause a catastrophe not observed on other planets. A. Kent wrote that there is a subtle difference between collider experiments with collisions of cosmic rays with the Earth's atmosphere. Namely, the products of collider collisions have zero velocity relative to the Earth, because they are produced by two opposite particle beams, and the products of cosmic ray collisions continue to move at near-light velocity relative to the Earth. This may change the nature of the interaction of products with terrestrial matter (Kent, 2004) giving mini-black holes more time to accumulate mass and start growing.</p><p>The main question here is: are the experiments at the collider <i>unique</i> or not in the sense that these are events that have never happened in the history of the Earth? If they are not unique and similar events have occurred on Earth and other planets without causing the end of the world, there is no risk of anthropic fragility.</p><p>Bostrom and Tegmark explored the probability of the <i>natural false vacuum decay</i> and similar cosmological catastrophes and found it to be low given our late existence. Their article (Tegmark &amp; Bostrom, 2005) does not directly apply to the collider experiments, as it puts a limit only on the <i>natural</i> cosmological catastrophes, and if the colliders create really <i>unique</i> conditions, no natural process is a reference class; of course, it also means that there are no alien civilizations in our past light cone which had performed such dangerous experiments before us and had created a wave of destruction through the whole universe. Their counter-arguments are discussed in more detail in section 4.1.&nbsp;</p><p>In 2008, it was suggested that the series of collider failures that occurred before the launch of the LHC could be explained by the anthropic shadow, since we can only survive on worlds where the collider does not work; see also (Ord et al., 2010) about anthropic considerations in the collider risk estimations. However, LHC started its operations after that and continued for around 10 years. However, the LHC began its work after that and lasted for about 10 years. If the LHC had a high probability of causing a false vacuum decay, then our own existence as the authors of this article seems too late, based on the same line of argument used by Bostrom and Tegmark: if LHC causes vacuum breakdown, most scientists should find themselves just after the beginning of its operation (or before). This is also an example of a more general counter-argument against the anthropic shadow called \"early observers\", which is discussed below in section 4.2.</p><p>One way or another, the anthropic shadow can make us underestimate the fragility of our vacuum, and any unique (which has never been in nature) experiment can become a pin that can burst the \u201cover-inflated ball\u201d of an over-due catastrophe. &nbsp;</p><h2>3.4. Different types of known global catastrophes and their anthropic shadows</h2><p>Waltham&nbsp;(Waltham, 2019) explored seven possible anthropic fine-tunings of the Solar System, including the mass of the Sun, the mass of the Moon, the orbits of the major planets, the Earth's ocean, the Earth's magnetism, and the Earth's plate tectonics. The evidence for the fine tunings he found is not very strong when taken individually, but collectively they point to some anthropic pressure in the evolution of the solar system. However, most of the fine-tuning methods he explores are positive conditions; here we are interested in negative conditions, that is, what types of events should not occur for the emergence of intelligent life. We have included in Table 1 several other global risks which have been hypothesized to be affected by the anthropic shadow.</p><p>&nbsp;</p><p><i>Table 1. Types of possible catastrophes and corresponding anthropic effects</i></p><figure class=\"table\"><table><tbody><tr><td style=\"border:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Type of natural catastrophe</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Description and comments</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Evidence for anthropic pressure</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Could observed catastrophes rate be affected by anthropic shadow?</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Could it have anthropic fragility?</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Type of period for periodic events</strong></p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Collider catastrophe</p><p>(Ord et al., 2010; Sandberg, 2008)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Colliders hypothetically could create new types of matter which could destroy Earth: mini black holes, strangelets or false vacuum transition&nbsp;(Kent, 2004)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Sandberg suggested that the LHC's series of failures in the early 2010s is best explained by the \"quantum immortality effect\" because if it starts working, it will destroy the world.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Yes</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Yes, in the form of uniqueness of human experiments</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>2. False vacuum decay&nbsp;(Hut &amp; Rees, 1983)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Hypothetical type of catastrophe</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>String theory suggests 10<sup>500&nbsp;</sup>possible vacuums.&nbsp;</p><p>(Douglas, 2003); &nbsp;Many of them are unstable&nbsp;(Adler et al., 1995)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Yes</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Hadron collider may trigger such a catastrophe&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Not known; probably random event</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>3. \u201cAlien invasion\u201d&nbsp;(Gertz, 2016),&nbsp;(Hanson et al., 2021),&nbsp;(Turchin &amp; Denkenberger, 2019)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>The shock wave of alien colonization, at a speed close to the speed of light, consumes all matter in the universe.&nbsp;(Armstrong &amp; Sandberg, 2013).</p><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>We have only been able to survive in that part of the universe where aliens have not yet appeared or are hiding.</p><p>&nbsp;</p><p>Aliens may be closer than Fermi's Paradox suggests.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Yes</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Messaging to Extra Terrestrial Intelligence could attract hostile aliens if they are nearby&nbsp;</p><p>(Baum et al., 2011) or we could find their dangerous signals&nbsp;(Turchin, 2018c).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>If natural intergalactic panspermia (spread of life) is possible, civilizations of our age are closer in space and could arrive on Earth relatively soon.&nbsp;(Turchin, 2020).</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>4. Gamma-ray burst (GRB)</p><p>&nbsp;(\u0106irkovi\u0107 &amp; Vukoti\u0107, 2016)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Directed GRB could sterilize planets 1000 light years away.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>The sun is located in a relatively distant part of the galaxy, and terrestrial life arose relatively late in the history of the universe, so gamma-ray bursts are rare in our region.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No, because we are seeing a lot of distant gamma-ray bursts. However, some nearby stars may be on the verge of forming a burst that was \"delayed\" by the anthropic shadow.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Random event</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>5. Nearby supernova&nbsp;(\u0106irkovi\u0107 &amp; Vukoti\u0107, 2016)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Supernova is dangerous if it is closer than 8 light years (\u0106irkovi\u0107 &amp; Vukoti\u0107, 2016).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>We are not in that part of the Galaxy where there are many supernova explosions (in the core).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No, as we observe many remote supernovas.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>None</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Random, very rare event based on observational constraints</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>6. Solar super-flare</p><p>&nbsp;(Lingam &amp; Loeb, 2017)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Sun-like stars have super-flares which could be dangerous to life on Earth.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>The Sun seems to be a surprisingly quiet star, as other stars of its type are more active&nbsp;</p><p>(Reinhold et al., 2020). Also, Sun may have had larger flares in the past but now is quiet.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>If the Sun were to become more active, higher levels or radiation and electromagnetic storms might prevent the rise of a technological civilization.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>None</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Not known</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>7. Asteroid/comet impact&nbsp;(Chapman &amp; Morrison, 1994)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Over the past few hundred million years, there have been no collisions that could have wiped out all vertebrates.&nbsp;(Rampino &amp; Caldeira, 2015)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>There is a periodicity of about 30 million years of mass extinctions that could be caused by an influx of comets from the Oort cloud, and we are now living near the end of such a period.</p><p>&nbsp;</p><p>In addition, several stars have recently passed close to the Sun and may have disturbed the Oort cloud.&nbsp;(Bailer-Jones, 2015).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>None</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Random cometary streams could be semi-periodic if they were associated with perturbations of the Oort cloud by solar oscillations around the galactic plane.</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>8. Supervolcanic eruptions and flood-basalt events</p><p>(Rampino, 2008).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Large-scale eruptions, i.e. Siberian traps; even larger eruptions are possible, resulting in surface replacement</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>There have been no comparable basalt flood events since the Great Dying 242 million years ago, except perhaps for Deccan traps.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Could be triggered by experiments with deep drilling and core penetration but currently unlikely.&nbsp;(Cirkovic &amp; Cathcart, 2003),&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Tension-spring mode</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>9. Runaway global warming</p><p>(Popp et al., 2016).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>The peculiar stability of the Earth's climate, despite the change in the luminosity of the Sun.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>May be caused by increased CO2 and methane emissions from permafrost.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Inflated balloon mode</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>10. Ocean anoxic event and H<sub>2</sub>S poisoning of atmosphere</p><p>(Ward, 2007).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Black sea could produce enough H<sub>2</sub>S to poison atmosphere and the level of H<sub>2</sub>S in it is growing</p><p>(Kump et al., 2005)</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\">Phosphorus is the main problem. It is used as a fertilizer throughout the world and ends up in the oceans. This could trigger an anoxic event in the ocean, which has been associated with mass extinction events, probably due to the formation of H2S&nbsp;(Handoh, 2013). Global warming increases the likelihood of anoxic events.</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>11.Stellar encounters with giant molecular clouds&nbsp;</p><p>(Kokaia &amp; Davies, 2019).</p><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>This can lead to a closer supernova, an increase in the frequency of asteroid impacts, and climate change.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Such encounters affect the stability of the Oort cloud and the planet's climate through dust accretion.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Anthropic effects define the galactic habitable zone where such encounters are rare.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Random, 1.6 event in Gyr for Sun.</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>12.Nuclear war</p><p>(Sandberg et al., 2018).</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Some have suggested that the fact that World War III did not occur in the 20th century is best explained by anthropic selection.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>A nuclear war is unlikely to kill everyone, so the anthropic effects should not be strong.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>A nuclear war would most likely destroy civilization and there would be fewer scientists left to discuss anthropics, so we would most likely be discussing it in a world where there was no World War III.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>The risk of an accidental nuclear war and the ease of provocation can be underestimated because of survival bias.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No.</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>13.Next Ice age</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Human civilization was able to appear only during a period of warm and stable climate which was good for agriculture and large empires&nbsp;(Gowdy, 2020). It has been relatively warm and stable for the last 11 700 years after the Young Dryas. It was predicted that the next Ice age will happen in a few millennia from now. The previous interglacial period\u2019s duration was around 13 ky and happened 130 ky ago.&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Human civilization arose during a stable interglacial period, and such periods, as a rule, make up only one tenth of the total time of the modern glaciation.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Humanity's efforts to combat global warming could unexpectedly backfire and lead to an early onset of the next cooling period. This includes CO2 reduction and geoengineering.</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>Periodic event with a random component</p></td></tr></tbody></table></figure><p>&nbsp;</p><p>&nbsp;</p><h1>4. Counterarguments against the anthropic shadow&nbsp;</h1><h2>4.1.&nbsp;Self-Indication Assumption counterargument: an observer is less likely to be in a world with high&nbsp;anthropic shadow</h2><p>The counterargument states that if there are two possible worlds, and one of them has an anthropic shadow and the other does not, then there will be more observers in a world without an anthropic shadow, and so I am more likely to be in a world without such a shadow.</p><p>In a nutshell:<i> The anthropic shadow is exactly compensated by my smaller chance to be in such world.</i></p><p>Thus, according to this counterargument, we are likely to live in a world where there is no survival bias and no problem with underestimating the risks of future disasters.&nbsp;</p><p>But there are three objections to this counterargument which are limiting its power: minimal level of anthropic shadow for all worlds; the numerical dominance of the semi-fine-tuned worlds and a possible correlation between catastrophes and habitability which will be discussed below.</p><p>But firstly, a counterargument\u2019s example: imagine that there are two groups of potentially habitable planets, and in each group initially there were 100 planets. In the first group, the probability of the past catastrophes is 0, and in the second group, only 1 planet has survived and gave rise to a civilization. Thus, in the second group, the anthropic pressure is 100 to 1. However, if an observer does not know in which world she is located, she has 100 times greater chances to be in the first group of worlds, as there will be 100 habitable planets.&nbsp;</p><p>This reasoning is based on so-called Self-Indication Assumption (SIA) which favors the worlds with larger number of observers (Bostrom, 2013). The SIA counterargument was suggested by Stuart Armstrong in a comment to my blog post about anthropic shadow.</p><p>SIA have two interpretations: first, it is simply about the distribution of observers among different universes in the multiverse, but it requires that all these universes actually exist. In another, stronger interpretation, the SIA is seen as an argument that of two possible universes, the more populated one is more likely to actually exist like in the Presumptions philosopher thought experiment <u>(Bostrom &amp; \u0106irkovi\u0107, 2003)</u>. In case of modal realism, these two versions merge. Here we will use the first, simpler one, which assumes the existence of many universes with different properties. For more on SIA and observer densities see in my article \u201cPresumptuous philosopher proves panspermia\u201d (Turchin, 2020).&nbsp;</p><p>In other words, it looks like that SIA exactly compensates the anthropic shadow, the same way as it exactly compensates the estimation of future catastrophes following from the Doomsday argument (Bostrom &amp; \u0106irkovi\u0107, 2003).</p><p>For example, if in the one part of the multiverse the false vacuum decay is likely and in the another it is not possible, the observer is much more likely to find herself in the part of multiverse where false vacuum decay never happens.</p><p>We can generalize this principle:&nbsp;</p><p>&nbsp;</p><p><i>The anthropic shadow\u2019s power is limited by the possibility of existence of another universe that produces the same observer with less anthropic shadow &nbsp;</i></p><p>&nbsp;</p><p>However, this principle cannot exclude the anthropic shadow idea completely, as some anthropic shadows may be <i>irreducible</i>.</p><p>For example, the anthropic shadow has limited the size of the asteroids which have impacted the Earth in the last billion years, but the existence of asteroids and impacts may be a necessary condition for the evolution of life on Earth-like planets, as a large impact likely created the Moon (Barr, 2016), comets may have brought water to Earth (Hartogh et al., 2011), and the impact-driven extinction of dinosaurs was probably necessary for mammals to rise to dominance (Alvarez et al., 1980). Thus, all possible habitable planets are likely under the risk of large impacts. Because of this, intelligent life may have evolved only on a small share of lucky planets; thus, the anthropic shadow for impacts would be strong and future expected rate of impacts would be higher.</p><p>Therefore, the first objection to the SIA-counterargument is that there is <i>a minimal level of anthropic shadow for all habitable planets</i>.</p><p>Another objection to SIA counterargument is the idea that the worlds with anthropic shadow could still produce more planets with civilizations than the worlds without such shadow as they are more numerous. To illustrate this, we will return to our example above. Before, we assumed that the initial number of the planets was equal. Now let us assume that initially there are 10 000 planets with 0.01 survival rate, and 10 planets with 1 survival rate. At the end, there will be 100 habitable planets in the first group, and still 10 in the second. In that case, we are still more likely to find ourselves on a planet that had a strong anthropic shadow. Let us call the first group <i>semi-fine-tuned</i> for intelligent life, and the second one \u2013 <i>fine-tuned</i>.&nbsp;</p><p>In that case, <i>the</i> <i>anthropic shadow is real if the share of semi-fine-tuned planets in the multiverse is larger than the share of the fine-tuned ones</i>. Obviously, there are no data about such shares, but mathematical simulation of possible planets by Tyrrell gives similar results: most currently habitable planets will have anthropic shadow (Tyrrell, 2020).&nbsp;</p><p>The observational consequence of this is that most observers may appear not because of the perfect fine-tuning of initial conditions, but because of some very random events which overcome non-perfect fine-tuning. In a fictional example, in a perfectly fine-tuned universe there would be no dangerous asteroids/comets in stellar systems. But in our solar system, we have many asteroids, and this could be compensated by some additional conditions, such as Jupiter protecting Earth from comets, or by pure chance. If humanity is protected by pure chance, this protection may not work in the future, which means a higher chance of asteroid/comet impacts in the future.</p><p>In other words, the domination of imperfectly fine-tuned worlds means that negative fine-tuning\u2014where the random absence of the catastrophic events is the main mechanism of civilizational survival\u2014may be the dominating form of anthropic selection, and thus the anthropic shadow would be much stronger.</p><p>The third reason to reject the SIA-counteragent is that there could be <i>a correlation between higher risk of catastrophes and habitability</i>. Universes which allow <i>interstellar panspermia</i> will have billion times more planets with life, but in such universes asteroid impacts are more frequent and stars are closer to each other, which implies higher rate of natural catastrophes (Turchin, 2020). Another idea of this type is that intelligence is more likely to appear in an unstable world which will discussed in section 6.</p><p>However, <i>SIA is a strong argument against extreme forms of anthropic bias</i>, like the collapse of false vacuum every 10 000 years, but doesn\u2019t work against weaker forms of anthropic bias, like higher rate of impacts or a possibility of runaway global warming. It is not easy to say without concrete data where is the line of balance between SIA and anthropic bias.&nbsp;</p><p>We think that SIA allows some form of anthropic bias, maybe as weak as 1 order of magnitude, but as we showed in section 2, the magnitude of anthropic bias has a relatively small impact on the future life expectancy, which is around 0.1 of previous time, if some form of anthropic shadow is present. However, in section 5, we will explore an argument that minimal level of anthropic shadow is comparable with probability of other big evolutionary filters and is many orders of magnitude.&nbsp;</p><h2>4.2. \u201cEarly observers\u201d counterargument</h2><h3>4.2.1. Early observerhood</h3><p>In the article \u201cAn upper bound for the background rate of human extinction\u201d&nbsp;Snyder-Beattie et al. (Snyder-Beattie et al., 2019) introduced the idea of <i>early observerhood</i> which could appear long before now: \u201cTo model observation selection bias, let us assume that after Homo sapiens first arises another step must be reached. This could represent the origin of language, writing, science, or any relevant factor that would transition early humans into the reference class of those capable of making observations (we call this step \u2018observerhood\u2019).\u201d They assume that such a factor may appear something like 20 000 years ago, while Homo Sapiens has existed for 200 000 years.&nbsp;</p><p>After long calculations, they conclude: \u201cIn summary, observer selection effects are unlikely to introduce major bias to our track record of survival as long as we allow for the possibility of early observers\u201d. The main reason for this is that if observers have been here for a long time, it is a strong argument against the high background extinction rate, as otherwise, one would likely find oneself near the beginning of the existence of observers. For example, if observers have been on Earth for 20 000 years, this is a strong counterargument against the background extinction rate of 1 in 1000 years, as only 1 in 2<sup>20&nbsp;</sup>timelines will reach such stage.&nbsp;</p><p>As one should think about oneself as an observer randomly selected from all observers, one should find oneself where most observers are concentrated, that is, in the first millennium in this example. But humanity does not find itself at this time, according to Snyder-Beattie\u2019s logic, so 1 in 1000 extinction rate per year is most likely wrong.&nbsp;</p><p>In a nutshell, it is a Doomsday argument in reverse, as we use observers\u2019 long survival as an argument against the high rate of catastrophes (Bostrom, 2001; Turchin, 2018a).</p><p>&nbsp;</p><h3>4.2.2. Qualified observers</h3><p>However, it looks like that Snyder-Beattie et al. used a too relaxed definition of a \u201cqualified observers\u201d that constitute the relevant reference class. We argue that the qualified observer should be able to understand the idea of the \u201canthropic shadow\u201d and be able to think about related topics. Neither hunter-gatherers nor medieval writers could do this. People with the necessary mathematical training and similar ideas began to appear only in the 18th and 19th centuries, i.e. Laplace with his sunrise problem (Marquis De Laplace, 1814), but the number of such observers increased only at the end of the 20<sup>th</sup> century when anthropic reasoning and Doomsday argument appeared.&nbsp;</p><p>The reason we use a more rigorous definition of observerhood is the idea that \u201cI am randomly selected among functionally indistinguishable observers\u201d, that is, from all those who think about a certain topic (Yudkowsky &amp; Soares, 2017). We should ignore in our calculations all observers who <i>are not thinking about this topic</i>, no matter if they are bright minds, can speak, can feel or whatever.&nbsp;</p><p>Therefore, there are no \u201cearly observers\u201d (with one exception discussed below). Currently living population of the observers is the first one, and because of that, they are especially unsure about their past and future extinction rates, like Adam and Eve in Bostrom\u2019s article (Bostrom, 2001). Moreover, it now looks like an argument in favor of higher extinction risks: as qualified observers exist for only around 50 years, future life expectancy could be also short.</p><p>Despite our criticism of the definition of observerhood, Snyder-Beattie\u2019s counterargument applies only to the natural risks that could have occurred in the last 20 000 years, where the difference between a current person\u2019s location and earlier observers is significant. It can\u2019t be applied to long-term risks, such as the rate of asteroid impacts or stability of the atmosphere, with reoccurrence rate of dangerous situations around tens of millions of years. Snyder-Beattie\u2019s counterargument also doesn\u2019t work for climate\u2019s anthropic fragility, as anthropogenic global warming has started only in last couple of hundred years.&nbsp;</p><p>&nbsp;</p><h3>4.2.3. Time until nuclear war</h3><p>However, there is one situation where Snyder-Beattie\u2019s counterargument works. It is anthropic estimate of the probability of nuclear war. There was no global nuclear war for more than 75 years, and anthropic reasoning exists from the 1970s, as Carter suggested anthropic principle in 1973, that is 48 ago from now (as of 2022).&nbsp;</p><p>If nuclear war has median timing of around 10 years, we are unlikely to find ourselves so late. Every-100-years-nuclear war hypothesis seems to be more likely in such a situation than every 10 years hypothesis. Every-1000-years nuclear war again does not look probable because now the question arises: why we are so yearly? In other words, if we have 3 hypotheses about the typical frequency of nuclear war: 10, 100, or 1000 years from the creation nuclear weapons and assume that there are no other x-risks, then finding ourselves 75 years after the creation of nuclear weapons supports the 100-years hypothesis.</p><p>Even if nuclear war is not killing all the people, it will significantly reduce the number of qualified observers, as the biggest university centers will be affected and Internet will not appear. Therefore, nuclear war could be regarded as equal to the extinction of qualified observers.&nbsp;</p><p>Bostrom\u2019s argument against the frequent false vacuum decay is based on the relatively late time of Earth formation in the history of our galaxy <u>(Tegmark &amp; Bostrom, 2005)</u>. It is also an example of the \u201cearly observerhood\u201d counterargument. Its logic can\u2019t be applied to the catastrophes on Earth, as we don\u2019t have evidence that life on Earth can evolve quicker than it did; moreover, we have opposite observations about the frequency of evolutionary transitions on Earth.</p><h2>4.3. Gaia counterargument</h2><p>The hypothesis of <i>Gaia</i> (Lovelock &amp; Lovelock, 2000) suggests that climate can self-regulate via negative feedback loops and that earth life is an important part of this self-regulation. An example of such feedback loop may be the growth of the Earth\u2019s albedo because of the larger amount of clouds if temperature grows too much. Life plays important role in the self-regulation of climate as it is able to capture CO2.</p><p>The appearance of Gaia also could be explained by anthropic selection (Tyrrell, 2020; Watson, 2004), as only planets capable of self-regulating climate have preserved their habitability. Gaia cannot be explained via the Darwinian selection on Earth as was suggested by (Doolittle, 2019), as Gaia exists only in one example. The planets with life that did not evolve a homeostatic mechanism of climate regulation would not exist for long and therefore would not be capable of producing observers.&nbsp;</p><p>Gaia seems to protect against anthropic fragility of climate, as most of the anthropic selection for stability has already happened in the past and has produced a self-stabilizing mechanism, now independent of anthropics. However, it may be that Gaia is just an observational selection illusion: there is no self-regulation, but there is just a series of random events which helped our survival.</p><p>Generalizing the Gaia counterargument, one may say: <i>all anthropic selection has happened in the past and this has selected a system that is very stable and has homeostatic mechanisms supporting its stability to a wide range of perturbations</i>. The theory may claim that homeostatic systems are rare, but eventually, they become more numerous than the worlds which just randomly escape all possible dangers. E.g. Jupiter is like Gaia for asteroid collisions, as it cleans space from hazardous comets.</p><p>Tyrrell demonstrated that only some planets get Gaia-like mechanism of homeostasis, and such mechanisms still have limits, above which these planets go into runaway warming (Tyrrell, 2020).&nbsp;</p><p>The main objection to this counterargument is that any self-regulation has its limits. The Sun will eventually overheat the Earth, which is typically estimated to happen around 1 billion years from now if one does not account for anthropic effects and feedback climate loops, which could imply earlier runaway global warming.&nbsp;</p><p>Biological organisms also age, become fragile and die and Gaia can age too. Thus, Gaia also may be unable to cope with some unexpected blows when \u201cplanetary boundaries\u201d are exceeded (Baum &amp; Handoh, 2014) and may stop protecting us from climate change.&nbsp;</p><p>Unique anthropogenic actions could unexpectedly end Gaia, as its protection works only for natural variations. Human-caused deforestation and other interventions in nature could affect Gaia coping ability and increase the probability of greenhouse catastrophe as was discussed in \u201cRole of the Biosphere in the Formation of the Earth\u2019s Climate: The Greenhouse Catastrophe\u201d by (Karnaukhov, 2001).</p><h2>4.4. PETM argument against runaway global warming</h2><p>The PETM episode of global warming, when 55 million years ago global temperatures jumped 8 C (or even 15C according to recent research), probably because of methane eruption, could be presented as a counterargument to the danger of runaway global warming, as temperatures returned to normal.</p><p>However, the main idea of this article is that one cannot use evidence from the past as arguments for our future survival because of anthropic bias. Maybe the PETM had a 99 per cent chance to turn into runaway global warming, but one cannot observe this, so we observe only the timeline (or Everett branch) where this did not happen. Also, the situation during PETM was different when now: different amounts of methane deposits, different speeds of warming and different disposition of continents.&nbsp;</p><p>During the last interglacial period 130 ky ago, the temperatures were also warmer than now by 2C and this did not trigger methane-driven runaway global warming. But as we discussed above, the methane-driven warming will be significant only if the speed of the initial CO<sub>2</sub>-driven warming is high, because of the short lifetime of methane in the atmosphere. So, the condition then was different than now, and thus it is not proof that we are safe now.&nbsp;</p><p>&nbsp;</p><h1>5. Estimating the power of anthropic shadow on Earth</h1><h2>5.1. Three types of power of anthropic shadow</h2><p>The power of anthropic shadow \u2013 that is, the chances of our survival until now \u2013 is not very important, as very different anthropic shadows give only 1-2 orders of magnitude reduction of future life expectancies, as we show in section 2. Such difference doesn\u2019t strongly affect our decision-making. As we assume that fragility increase is proportional to the life expectancy decrease, it means that fragility variation is not very large.</p><p>Based on the power of anthropic shadow, we could distinguish 3 significantly different situations:</p><ul><li>anthropic shadow (ASH) doesn\u2019t exist at all.</li><li><i>ASH is weak,</i> from one to few orders of magnitude. Future life expectancy decrease is around one order of magnitude and is not important for us in case of most natural catastrophes. Anthropic fragility is present but manageable. This type of ASH is favored by SIA counterargument. For example, 1 in 10<sup>10</sup> chances of past catastrophes give 100 million years of future life expectancy instead of 1 billion years. 10 times increase in fragility means that, say, not 45C is needed to reach moisture greenhouse, but only 4.5C, which is still manageable by emission control.&nbsp;</li><li>ASH is <i>strong</i>, many orders of magnitude. The decline of life expectancy of the biosphere is significant. The world is so fragile to human actions that the fatal damage is likely already happened. This type of ASH is favored by evolutionary transitions argument discussed below. For example, 1 in 10<sup>100</sup> ASH gives only around 12 million years of life expectancy and two order of magnitude anthropic fragility. This means that only around 0.5C of anthropogenic temperature increases is enough for the start of runaway global warming, and we already past that point.&nbsp;</li></ul><h2>5.2. The similar size of the positive and negative fine-tuning</h2><p>A recent article \u201cThe Timing of Evolutionary Transitions Suggests Intelligent Life Is Rare\u201d by Sandberg et al (2020) shows that given a known future life expectancy for Earth's climate of 1 billion years, all evolutionary transitions could have a very small probability, or, in other words, very long mean times, maybe many orders of magnitude longer than the age of the universe. This means a very strong anthropic selection effect, since we are observing a world in which all such events occurred in time.&nbsp;</p><p>We suggest that <i>strong anthropic selection for evolutionary transitions means that the that an equally strong anthropic effect existed in avoiding natural disasters</i>. Although this is not easy to prove, we can illustrate this with the concept of \"budget: if a person has an expensive house, he is likely also has an expensive car, since the expensive house suggests that the person has a large budget.&nbsp;</p><p>Similarly, powerful anthropic effects in evolutionary transitions may suggest the existence of a large \"anthropic budget\", that is, the actual existence of a large number of different worlds from which our world was chosen at random. The reason for this is that the Earth's longer survival time allows more time for evolutionary transitions.</p><p>Therefore, if it is easier to \u201cbuy\u201d more time for the stable existence of a planet than to accelerate evolutionary transitions, then anthropic selection will choose planets with a longer existence without catastrophes. This will result in situation in which gaining time becomes as difficult as accelerating the evolutionary transition. Thus, there is a trade-off between avoiding catastrophes by pure chance and the speed of evolutionary transitions. For example, if the Earth were habitable not for 5, but for 10 billion years, this would allow more time for transitions, but would require much more incredibly long survival without catastrophes. The details of this trade-off are beyond the scope of this article and merit further study.</p><p>The similarity between anthropic shadow and evolution transitions probabilities is not exact, and it is not even of the same order of magnitude. It could be 1 in 10<sup>200&nbsp;</sup>chances of intelligent life per planet and 1 in 10<sup>180&nbsp;</sup>that a planet survives intact all risks of sterilizing catastrophes, like large asteroid impacts and runaway warmings. The second probability is trillions of trillions times higher, but still, they are <i>similar</i> in some sense.&nbsp;</p><p>In addition, the evaluation of fine-tuning by Sandberg et al. (2020) refer to <i>global</i> <i>base rate</i>, i.e. it is a probability distribution for all terrestrial planets in the Universe, not just for the Earth. If there is a region of the universe where evolutionary transitions are much easier, we will be there. Therefore, the estimation of the risk\u2019s frequencies is also a base rate applicable to all planets, and thus SIA counterargument (section 4.1) can\u2019t kill it. This means that everywhere in the universe there is a strong base rate of sterilizing catastrophes.</p><h2>5.3. Anthropic bias and the hard steps in the evolution</h2><p>As we showed above, the power of anthropic shadow is comparable with the difficulty of evolutionary transitions, and one more estimation of this difficulty is presented in the article by Kipping.</p><p>The article \u201cAn objective Bayesian analysis of life\u2019s early start and our late arrival\u201d (Kipping, 2020) compared the probability of abiogenesis, which took around 100-300 million years, and the arrival of intelligence, which required 3.8 billion years after the appearance of life and concluded that intelligence appearing was a more difficult and less probable event. But the abiogenesis itself is estimated to be a very improbable event: a recent estimate of the minimum length of self-replicating RNA is around 100 bases (Totani, 2020). Totani thinks that only one of 10^100 stars will generate a viable RNA strand. Thus, only one of 10^80 of Hubble volumes of the universe would have life. But intelligence would be even less probable, based on Kipping\u2019s logic.&nbsp;</p><p>If intelligence is a rare step, it could be explained if the typical habitability of planets is shorter than the one of Earth and thus Earth is long overdue to lose its habitability. Thus, the world\u2019s end is nigh, though we still may have around 100 million years. But anthropic fragility means this situation is worse as any significant change will cause a catastrophe.</p><p>However, this estimate may be inaccurate due to natural interstellar panspermia (Ginsburg et al., 2018), since in the case of panspermia, life may take much longer to develop. Life may have evolved on other planets before coming to Earth. For anthropic reasons, universes with panspermia will create more observers, as an entire galaxy with billions of potentially habitable planets(Hsu et al., 2019) could be fertilized with life (Turchin, 2020).</p><p>&nbsp;</p><h2>5.4. How could one know about the existence of the anthropic shadow?&nbsp;</h2><p>Anthropic shadow may be hinted by several types of evidence, but its main feature is that it is mostly unobservable. These types of evidence are:</p><p>&nbsp;</p><ul><li><i>Frequency of near-misses.</i> One indication is the frequency of near-misses, or situations of improbable survival in the past. There is currently unpublished work \u201cNuclear war near misses and anthropic shadows\u201d on the topic by Sandberg et al (Sandberg et al., 2018). For climate, it is several past Snow Ball situations, and one potential runaway greenhouse event 55 mln years ago (PETM), which didn\u2019t turn permanent.</li><li><i>Absence of the fat tail.</i> Another indication of anthropic shadow is the absence of a fat tail in the distribution of smaller catastrophes. For example, we observed asteroid impacts of bodies only below some size; there was no 100 km bodies impacts in last couple of billion years on Earth. We also never observed that the Earth becomes too hot, like 60 C, as it seems to be irreversible tipping point for runaway moisture greenhouse.</li><li><i>Surprising coincidences.</i> A further evidence is some surprising coincidences which helped survival and evolution of life on Earth, as was analyzed in Sandberg et al (Snyder-Beattie et al., 2020). One of such evidence is the ability of Earth to have habitable temperature despite the growth of Sun luminosity.</li><li><i>Observation that we are located near the end of several period catastrophic cycles.</i> This is a situation where one finds oneself closer to the end of stability period, or even some signs of the end of stable period could be observed. For example, asteroid impacts\u2019 waves seem to be semi-periodic events every 30 or so million years, probably cause by disturbance of Oort cloud from passing of Galactic plane.</li><li><i>Increasing instability.</i> Smaller catastrophes could be a sign that a larger one is near, e.g. increase of smaller asteroid impacts.&nbsp;</li><li><i>Mathematical modelling</i> of the similar to the Earth planets with different initial conditions (Tyrrell, 2020). This could provide <i>a priori</i> probability of anthropic shadow.</li><li>General considerations about Rare Earth and Fermi paradox.</li></ul><p>We will continue discussion about the different types of evidence for anthropics shadow in Appendix 3.</p><h1>6. Discussion: anthropic shadow and its connection with the evolution of intelligence</h1><h2>6.1. Hypothesis: intelligence is more likely to appear in an unstable world that is close to its end</h2><p>Above we discussed a weak version of the anthropic shadow. But in the same way, as the <i>strong anthropic principle&nbsp;</i>claims that the universe <i>needs</i> to create observers (Barrow &amp; Tipler, 1986), we could formulate a <i>strong anthropic shadow</i>:&nbsp;</p><p><i>Intelligence tends to appear only in a world that is close to its end</i>.</p><p>The main reason for the emergence of intelligence near the end of the world is that intelligence is a general adaptation that outperforms specialized adaptations in a rapidly changing world, which in the case of Earth is a world with an unstable climate. During <i>Homo sapiens\u2019</i> evolution, humans\u2019 predecessors changed several main ways of feeding in just a few million years (Henry, 2018). Our hominid ancestors started out as arboreal primates, then evolved into savannah scavengers, then into foragers, then\u2014maybe\u2014\u201caquatic apes\u201d then fire-cooking hunters, then plants-eating early agricultural inhabitants. communities, and then again to the high-calorie eaters of modern cities. Periodic glaciations and deglaciations in the last few million years have affected human living conditions and create the need to adapt to new feeding opportunities.</p><p>The second reason that the end is near is that the evolution of intelligent observers requires unusually long periods of relative stability: thus, a very large catastrophe may be overdue. This may seem to contradict what we said above, but here we are referring to much larger catastrophes that can kill all humans or irreversibly destroy civilization, such as moisture greenhouse.</p><p>Intelligent observers (scientists) are much more fragile than the entire biosphere and even the species <i>Homo sapiens</i>. A large economic collapse could reduce their numbers, e.g. as it happened after the collapse of the USSR. This means, as Circovic et al. wrote, that anthropic shadow must be stronger in recent times and suppress even smaller catastrophes (\u0106irkovi\u0107 et al., 2010). This includes smaller asteroid impacts, supervolcanic eruptions and nuclear war.</p><p>The third reason is that the impact of human civilization on nature is growing exponentially. If there is some hidden vulnerability (Bostrom, 2018) that can provoke a global catastrophe, humanity will stumble upon it sooner or later, especially since the anthropic shadow does not let us know where such a vulnerability is. The anthropic shadow also results in underestimation of the fragility of our environment to small anthropogenic changes.</p><p>Waltham came to similar conclusions:&nbsp;</p><p>&nbsp;</p><blockquote><p>Given this link between climate change and species diversity, it is plausible that planets with high climate variability may be less likely to produce intelligent observers than planets with more stable conditions. However, it is also arguable that the ultimate emergence of intelligent species is actually encouraged by adverse conditions because these help to clear ecological niches (cf. the adaptive radiation of mammals following demise of the dinosaurs) and because evolutionary innovations may be particularly advantageous during testing times [cf. the emergence of <i>Homo sapiens</i> during the relatively unstable Neogene \u2026 and the emergence of multicellular life around the time of the Neoproterozoic glaciation] (Waltham, 2011).</p></blockquote><p>In addition, human actions are unique and different from events that have occurred naturally in the past, and the homeostatic mechanisms that exist in nature may not be adapted to the new type of change. This includes the speed of change in the case of global warming and possibly a unique combination of several anthropogenic factors.</p><p>Another argument for a strong anthropic shadow is the Doomsday argument, which suggests a high level of future unknown risks based on our early location in human history (Bostrom, 2012; Turchin, 2018), and the general abundance of global catastrophic risks in our world (Bostrom, 2002).&nbsp;</p><p>In a sense, the anthropic shadow is similar to the idea of so-called <i>quantum immortality</i> where a person observes oneself surviving many rounds of Russian roulette because of the many world interpretation of quantum mechanics (Turchin, 2018) if this idea is applied to humanity\u2019s past. Even if humanity is in a simulation, most simulations likely model a civilization near its end, and will terminate after modeling the \u201csingularity\u201d (Greene, 2018; Turchin, Yampolskiy, Denkenberger, &amp; Batin, 2019).</p><p>Similar idea was suggested by global warming research pioneer Budyko who <a href=\"https://alev-biz.livejournal.com/5333134.html\">discovered</a> that cephalization of animals grew in the periods of unstable climate of Ice ages when forest and savanna replaced each other with periodicity of tens of thousands of years and universal adaptive ability \u2013 intelligence \u2013 was in demand.&nbsp;</p><p>&nbsp;</p><h2>6.2. Relation of the anthropic fragility to Fermi paradox and x-risks</h2><p>A strong anthropic shadow means that we are alone in the universe, so there is less risk of encountering hostile aliens. And vice versa, the observation that we alone, hints at a higher frequency of natural catastrophes that we have never seen due to the anthropic shadow.</p><p>Therefore, there is an increase of catastrophic risks in both branches of possibility: if we are alone, then we are exposed to higher natural risks and to anthropic fragility. If we are not alone, then the risk of hostile aliens is higher&nbsp;(Turchin, 2018c; Turchin &amp; Denkenberger, 2019).&nbsp;</p><p>Anthropic fragility puts natural risks into the short-term perspective of the next 100 years. Therefore, they become comparable with the most serious technological risks: synthetic biology, AI and nuclear war.</p><h1>Conclusion</h1><p>The anthropic shadow may seem like a purely theoretical effect, as far greater global risks pose threats to human civilization, such as unaligned AI, nuclear war, and synthetic biology&nbsp;(Turchin &amp; Denkenberger, 2018).&nbsp;</p><p>However, the detailed analysis presented here shows that anthropic shadow means that there are natural catastrophes that may be long overdue, have accumulated destructive energy, and could be triggered by small human actions. Such human actions will be unique events capable to trigger a change in semi-stable conditions. The most dangerous of these catastrophes is the runaway global warming that could occur if some currently unknown threshold of warming levels is reached.</p><h1>Literature</h1><p>Adler, R. J., Casey, B., &amp; Jacob, O. C. (1995). Vacuum catastrophe: An elementary exposition of the cosmological constant problem. <i>American Journal of Physics</i>, <i>63</i>, 620\u2013626. https://doi.org/10.1119/1.17850</p><p>Allan, R. P., Hawkins, E., Bellouin, N., &amp; Collins, B. (2021). <i>IPCC, 2021: Summary for Policymakers</i>.</p><p>Alvarez, L. W., Alvarez, W., Asaro, F., &amp; Michel, H. V. (1980). Extraterrestrial cause for the Cretaceous-Tertiary extinction. <i>Science</i>, <i>208</i>(4448), 1095\u20131108.</p><p>Ananthaswamy, A. (2015). The methane apocalypse. <i>New Scientist</i>, <i>226</i>(3022), 38\u201341. https://doi.org/10.1016/S0262-4079(15)30420-6</p><p>Armstrong, S., &amp; Sandberg, A. (2013). Eternity in six hours: Intergalactic spreading of intelligent life and sharpening the Fermi paradox. <i>Acta Astronautica</i>, <i>89</i>, 1\u201313.</p><p>Bailer-Jones, C. a. L. (2015). Close encounters of the stellar kind. <i>Astronomy &amp; Astrophysics</i>, <i>575</i>, A35. https://doi.org/10.1051/0004-6361/201425221</p><p>Barr, A. C. (2016). On the origin of Earth\u2019s Moon. <i>Journal of Geophysical Research: Planets</i>, <i>121</i>(9), 1573\u20131601. https://doi.org/10.1002/2016JE005098</p><p>Barrow, J. D., &amp; Tipler, F. J. (1986). <i>The Anthropic Cosmological Principle (Clarendon</i>. Oxford.</p><p>Baum, S. D., &amp; Handoh, I. C. (2014). Integrating the planetary boundaries and global catastrophic risk paradigms. <i>Ecological Economics</i>, <i>107</i>, 13\u201321. https://doi.org/10.1016/j.ecolecon.2014.07.024</p><p>Baum, S. D., Haqq-Misra, J. D., &amp; Domagal-Goldman, S. D. (2011). Would contact with extraterrestrials benefit or harm humanity? A scenario analysis. <i>Acta Astronautica</i>, <i>68</i>(11\u201312), 2114\u20132129.</p><p>Bostrom, N. (2001). The Doomsday Argument Adam &amp; Eve, UN++, and Quantum Joe. <i>Synthese</i>, <i>127</i>(3), 359\u2013387.</p><p>Bostrom, N. (2002). Existential risks: Analyzing Human Extinction Scenarios and Related Hazards. <i>Journal of Evolution and Technology, Vol. 9, No. 1 (2002).</i></p><p>Bostrom, N. (2012). A Primer on the Doomsday Argument. <i>Anthropic-Principle.Com</i>. /?q=anthropic_principle/doomsday_argument</p><p>Bostrom, N. (2013). <i>Anthropic bias: Observation selection effects in science and philosophy</i>. Routledge.</p><p>Bostrom, N. (2018). <i>The Vulnerable World Hypothesis</i>. 38.</p><p>Bostrom, N., &amp; \u0106irkovi\u0107, M. M. (2003). The doomsday argument and the self-indication assumption: Reply to Olum. <i>The Philosophical Quarterly</i>, <i>53</i>(210), 83\u201391.</p><p>Bowen, G. J., Maibauer, B. J., Kraus, M. J., R\u00f6hl, U., Westerhold, T., Steimke, A., Gingerich, P. D., Wing, S. L., &amp; Clyde, W. C. (2015). Two massive, rapid releases of carbon during the onset of the Palaeocene\u2013Eocene thermal maximum. <i>Nature Geoscience</i>, <i>8</i>(1), 44.</p><p>Bridges, W. (2012). Gains from Getting Near Misses Reported. <i>Presentation at 8th Global Congress on Process Safety, Houston TX April</i>, 1\u20134.</p><p>Carpenter, P. A., &amp; Bishop, P. C. (2009). A review of previous mass extinctions and historic catastrophic events. <i>Futures</i>, <i>41</i>(10), 676\u2013682. https://doi.org/10.1016/j.futures.2009.07.012</p><p>Carter, B. (1974). Large number coincidences and the anthropic principle in cosmology. <i>Symposium-International Astronomical Union</i>, <i>63</i>, 291\u2013298.</p><p>Chapman, C. R., &amp; Morrison, D. (1994). Impacts on the Earth by asteroids and comets: Assessing the hazard. <i>Nature</i>, <i>367</i>(6458), 33\u201340.</p><p>Cirkovic, M. M., &amp; Cathcart, R. (2003). Geo-engineering Gone Awry: A New Partial Solution of Fermi\u2019s Paradox. <i>ArXiv Preprint Physics/0308058</i>.</p><p>\u0106irkovi\u0107, M. M., Sandberg, A., &amp; Bostrom, N. (2010). Anthropic shadow: Observation selection effects and human extinction risks. <i>Risk Analysis, Vol. 30, No. 10, 2010</i>.</p><p>\u0106irkovi\u0107, M. M., &amp; Vukoti\u0107, B. (2016). Long-term prospects: Mitigation of supernova and gamma-ray burst threat to intelligent beings. <i>Acta Astronautica</i>, <i>129</i>, 438\u2013446. https://doi.org/10.1016/j.actaastro.2016.10.005</p><p>Cui, Y., Schubert, B. A., &amp; Jahren, A. H. (2020). A 23 m.y. Record of low atmospheric CO2. <i>Geology</i>. https://doi.org/10.1130/G47681.1</p><p>Dar, A., De R\u00fajula, A., &amp; Heinz, U. (1999). Will relativistic heavy-ion colliders destroy our planet? <i>Physics Letters B</i>, <i>470</i>(1\u20134), 142\u2013148. https://doi.org/10.1016/S0370-2693(99)01307-6</p><p>Dean, J. F., Middelburg, J. J., R\u00f6ckmann, T., Aerts, R., Blauw, L. G., Egger, M., Jetten, M. S., Jong, A. E., Meisel, O. H., &amp; Rasigraf, O. (2018). Methane feedbacks to the global climate system in a warmer world. <i>Reviews of Geophysics</i>.</p><p>Doolittle, W. F. (2019). Making Evolutionary Sense of Gaia. <i>Trends in Ecology &amp; Evolution</i>, <i>34</i>(10), 889\u2013894. https://doi.org/10.1016/j.tree.2019.05.001</p><p>Douglas, M. R. (2003). The statistics of string/M theory vacua. <i>Journal of High Energy Physics</i>, <i>2003</i>(05), 046\u2013046. https://doi.org/10.1088/1126-6708/2003/05/046</p><p>Firestone, R. B., West, A., Kennett, J. P., Becker, L., Bunch, T. E., Revay, Z. S., Schultz, P. H., Belgya, T., Kennett, D. J., &amp; Erlandson, J. M. (2007). Evidence for an extraterrestrial impact 12,900 years ago that contributed to the megafaunal extinctions and the Younger Dryas cooling. <i>Proceedings of the National Academy of Sciences</i>, <i>104</i>(41), 16016\u201316021.</p><p>Flis, A. (2020, October 26). Arctic Sea Ice is not freezing In October for the first time since measurements began, now having an unknown effect on weather development towards Winter. <i>Severe Weather Europe</i>. https://www.severe-weather.eu/news/arctic-ocean-sea-ice-2020-jet-stream-effect-winter-fa/</p><p>Gertz, J. (2016). Reviewing METI: A critical analysis of the arguments. <i>ArXiv Preprint ArXiv:1605.05663</i>.</p><p>Ginsburg, I., Lingam, M., &amp; Loeb, A. (2018). Galactic panspermia. <i>The Astrophysical Journal Letters</i>, <i>868</i>(1), L12.</p><p>Goff, J., Dominey-Howes, D., Chagu\u00e9-Goff, C., &amp; Courtney, C. (2010). Analysis of the Mahuika comet impact tsunami hypothesis. <i>Marine Geology</i>, <i>271</i>(3\u20134), 292\u2013296.</p><p>Goldblatt, C., Robinson, T. D., Zahnle, K. J., &amp; Crisp, D. (2013). Low simulated radiation limit for runaway greenhouse climates. <i>Nature Geoscience</i>, <i>6</i>(8), 661\u2013667.</p><p>Goldblatt, C., &amp; Watson, A. J. (2012). The runaway greenhouse: Implications for future climate change, geoengineering and planetary atmospheres. <i>Phil. Trans. R. Soc. A</i>, <i>370</i>(1974), 4197\u20134216. https://doi.org/10.1098/rsta.2012.0004</p><p>Gott III, J. R. (1993). Implications of the Copernican principle for our future prospects. <i>Nature</i>, <i>363</i>, 315\u2013319.</p><p>Gowdy, J. (2020). Our hunter-gatherer future: Climate change, agriculture and uncivilization. <i>Futures</i>, <i>115</i>, 102488. https://doi.org/10.1016/j.futures.2019.102488</p><p>Grace, K. (2010). SIA doomsday: The filter is ahead | Meteuphoric. <i>Meteuphoric</i>. https://meteuphoric.com/2010/03/23/sia-doomsday-the-filter-is-ahead/</p><p>Greene, P. (2018). The Termination Risks of Simulation Science. <i>Erkenntnis</i>, 1\u201321.</p><p>Halstead, J. (2018). Stratospheric Aerosol Injection Research and Existential Risk. <i>Futures</i>. https://doi.org/10.1016/j.futures.2018.03.004</p><p>Handoh, I. C. (2013). Phosphorus and Chemical Pollution as Global Catastrophic Risks. <i>GCRI Blog</i>. http://gcrinstitute.org/handoh-lecture-summary-june-25-26/</p><p>Hansen, J., &amp; et al. (1992). \u202aClimate Forcing by Anthropogenic Aerosols. <i>Science</i>. https://scholar.google.ru/citations?view_op=view_citation&amp;hl=ru&amp;user=NhWonoUAAAAJ&amp;alert_preview_top_rm=2&amp;citft=1&amp;email_for_op=alexeiturchin%40gmail.com&amp;citation_for_view=NhWonoUAAAAJ:u5HHmVD_uO8C</p><p>Hanson, R. (1998). The great filter-are we almost past it. <i>Preprint Available at Http://Hanson. Gmu. Edu/Greatfilter. Html</i>.</p><p>Hanson, R., Martin, D., McCarter, C., &amp; Paulson, J. (2021). A Simple Model of Grabby Aliens. <i>ArXiv Preprint ArXiv:2102.01522</i>.</p><p>Hartogh, P., Lis, D. C., Bockel\u00e9e-Morvan, D., de Val-Borro, M., Biver, N., K\u00fcppers, M., Emprechtinger, M., Bergin, E. A., Crovisier, J., &amp; Rengel, M. (2011). Ocean-like water in the Jupiter-family comet 103P/Hartley 2. <i>Nature</i>, <i>478</i>(7368), 218\u2013220.</p><p>Henry, D. O. (2018). <i>From foraging to agriculture: The Levant at the end of the Ice Age</i>. University of Pennsylvania Press.</p><p>Herndon, J. M. (1993). Feasibility of a nuclear fission reactor at the center of the Earth as the energy source for the geomagnetic field. <i>Journal of Geomagnetism and Geoelectricity</i>, <i>45</i>(5), 423\u2013437.</p><p>Hsu, D. C., Ford, E. B., Ragozzine, D., &amp; Ashby, K. (2019). Occurrence Rates of Planets Orbiting FGK Stars: Combining Kepler DR25, Gaia DR2, and Bayesian Inference. <i>The Astronomical Journal</i>, <i>158</i>(3), 109. https://doi.org/10.3847/1538-3881/ab31ab</p><p>Hut, P., &amp; Rees, M. J. (1983). How stable is our vacuum? <i>Nature</i>, <i>302</i>(5908), 508.</p><p>Karnaukhov, A. V. (2001). Role of the biosphere in the formation of the Earth\u2019s Climate: The Greenhouse Catastrophe. <i>BIOPHYSICS-PERGAMON THEN MAIK NAUKA-C/C OF BIOFIZIKA</i>, <i>46</i>(6), 1078\u20131088.</p><p>Kelsey, H. M., Nelson, A. R., Hemphill-Haley, E., &amp; Witter, R. C. (2005). Tsunami history of an Oregon coastal lake reveals a 4600 yr record of great earthquakes on the Cascadia subduction zone. <i>Geological Society of America Bulletin</i>, <i>117</i>(7\u20138), 1009\u20131032.</p><p>Kennett, J. P., Cannariato, K. G., Hendy, I. L., &amp; Behl, R. J. (2003). <i>Methane Hydrates in Quaternary Climate Change: The Clathrate Gun Hypothesis</i>. American Geophysical Union.</p><p>Kent, A. (2004). A critical look at risk assessments for global catastrophes. <i>Risk Analysis</i>, <i>24</i>(1), 157\u2013168.</p><p>Kipping, D. (2020). An objective Bayesian analysis of life\u2019s early start and our late arrival. <i>Proceedings of the National Academy of Sciences</i>. https://doi.org/10.1073/pnas.1921655117</p><p>Kokaia, G., &amp; Davies, M. B. (2019). <i>Stellar encounters with giant molecular clouds</i>. https://doi.org/10.1093/mnras/stz813</p><p>Kump, L. R., Pavlov, A., &amp; Arthur, M. A. (2005). Massive release of hydrogen sulfide to the surface ocean and atmosphere during intervals of oceanic anoxia. <i>Geology</i>, <i>33</i>(5), 397\u2013400.</p><p>Lenton, T. M. (2011). Early warning of climate tipping points. <i>Nature Climate Change</i>, <i>1</i>(4), 201.</p><p>Lingam, M., &amp; Loeb, A. (2017). Risks for life on habitable planets from superflares of their host stars. <i>The Astrophysical Journal</i>, <i>848</i>(1), 41.</p><p>Lomborg, B. (2020). Welfare in the 21st century: Increasing development, reducing inequality, the impact of climate change, and the cost of climate policies. <i>Technological Forecasting and Social Change</i>, <i>156</i>, 119981. https://doi.org/10.1016/j.techfore.2020.119981</p><p>Lovelock, J., &amp; Lovelock, J. E. (2000). <i>Gaia: A new look at life on earth</i>. Oxford Paperbacks.</p><p>Manheim, D. (2018). Questioning Estimates of Natural Pandemic Risk. <i>Health Security</i>, <i>16</i>(6), 381\u2013390. https://doi.org/10.1089/hs.2018.0039</p><p>Marquis De Laplace, P. (1814). <i>A philosophical essay on probabilities</i>. Cosimo, Inc., 2007.</p><p>Napier, B., Asher, D., Bailey, M., &amp; Steel, D. (2015). Centaurs as a hazard to civilization. <i>Astronomy &amp; Geophysics</i>, <i>56</i>(6), 6\u201324.</p><p>Napier, W. M., &amp; Clube, S. V. M. (1979). A theory of terrestrial catastrophism. <i>Nature</i>, <i>282</i>(5738), 455.</p><p>Ord, T. (2020). <i>The precipice: Existential risk and the future of humanity</i>. Hachette Books.</p><p>Ord, T., Hillerbrand, R., &amp; Sandberg, A. (2010). Probing the improbable: Methodological challenges for risks with low probabilities and high stakes. <i>Journal of Risk Research</i>, <i>13</i>(2), 191\u2013205.</p><p>Ozhovan, M. I., Gibb, F., Poluektov, P. P., &amp; Emets, E. P. (2005). Probing of the interior layers of the Earth with self-sinking capsules. <i>Atomic Energy</i>, <i>99</i>(2), 556\u2013562.</p><p>Popp, M., Schmidt, H., &amp; Marotzke, J. (2016). Transition to a Moist Greenhouse with CO2 and solar forcing. <i>Nature Communications</i>, <i>7</i>. https://doi.org/10.1038/ncomms10627</p><p>Ramirez, R. M., Kopparapu, R. K., Lindner, V., &amp; Kasting, J. F. (2014). Can increased atmospheric CO2 levels trigger a runaway greenhouse? <i>Astrobiology</i>, <i>14</i>(8), 714\u2013731.</p><p>Ramirez, R. M., Lindner, V., &amp; Kasting, J. F. (2013). How close is Earth to a runaway greenhouse? <i>ArXiv Preprint ArXiv:1306.5730</i>.</p><p>Rampino, M. R. (1998). The galactic theory of mass extinctions: An update. In <i>Dynamics of Comets and Asteroids and Their Role in Earth History</i> (pp. 49\u201358). Springer.</p><p>Rampino, M. R. (2008). Super-volcanism and other geophysical processes of catastrophic import. <i>Global Catastrophic Risks</i>, <i>1</i>, 203.</p><p>Rampino, M. R. (2015). Disc dark matter in the Galaxy and potential cycles of extraterrestrial impacts, mass extinctions and geological events. <i>Monthly Notices of the Royal Astronomical Society</i>, <i>448</i>(2), 1816\u20131820.</p><p>Rampino, M. R., &amp; Caldeira, K. (2015). Periodic impact cratering and extinction events over the last 260 million years. <i>Monthly Notices of the Royal Astronomical Society</i>, <i>454</i>(4), 3480. https://doi.org/10.1093/mnras/stv2088</p><p>Reinhold, T., Shapiro, A. I., Solanki, S. K., Montet, B. T., Krivova, N. A., Cameron, R. H., &amp; Amazo-G\u00f3mez, E. M. (2020). The Sun is less active than other solar-like stars. <i>Science</i>, <i>368</i>(6490), 518\u2013521. https://doi.org/10.1126/science.aay3821</p><p>Sandberg, A. (2008). Bayes, Moravec and the LHC: Quantum Suicide, Subjective Probability and Conspiracies. <i>Andart</i>. http://www.aleph.se/andart/archives/2008/09/bayes_moravec_and_the_lhc_quantum_suicide_subjective_probability_and_conspiracies.html</p><p>Sandberg, A., Drexler, E., &amp; Ord, T. (2017). <i>Dissolving the Fermi Paradox</i>. Future of Humanity Institute. http://www.jodrellbank.manchester.ac.uk/media/eps/jodrell-bank-centre-for-astrophysics/news-and-events/2017/uksrn-slides/Anders-Sandberg---Dissolving-Fermi-Paradox-UKSRN.pdf</p><p>Sandberg, A., Snyder-Beattie, A., &amp; Armstrong, S. (2018). Nuclear war near misses and anthropic shadows. <i>Manuscript in Preparation</i>.</p><p>Seeley, J., &amp; Wordsworth, R. (2021). Episodic deluges in simulated hothouse climates. <i>Nature</i>, <i>599</i>(7883), 74\u201379. https://doi.org/10.1038/s41586-021-03919-z</p><p>Segschneider, J., Beitsch, A., Timmreck, C., Brovkin, V., Ilyina, T., Jungclaus, J. H., Lorenz, S. J., Six, K. D., &amp; Zanchettin, D. (2013). Impact of an extremely large magnitude volcanic eruption on the global climate and carbon cycle estimated from ensemble Earth System Model simulations. <i>Biogeosciences</i>, <i>10</i>, 669\u2013687.</p><p>Shakhova, N., Semiletov, I., Salyuk, A., Yusupov, V., Kosmach, D., &amp; Gustafsson, \u00d6. (2010). Extensive methane venting to the atmosphere from sediments of the East Siberian Arctic Shelf. <i>Science</i>, <i>327</i>(5970), 1246\u20131250.</p><p>Shcherbakov, \u0410. \u0421. (1999). \u0410\u043d\u0442\u0440\u043e\u043f\u043d\u044b\u0439 \u043f\u0440\u0438\u043d\u0446\u0438\u043f \u0432 \u043a\u043e\u0441\u043c\u043e\u043b\u043e\u0433\u0438\u0438 \u0438 \u0433\u0435\u043e\u043b\u043e\u0433\u0438\u0438. <i>\u0412\u0435\u0441\u0442\u043d\u0438\u043a \u041c\u043e\u0441\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e \u0423\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0430. \u0421\u0435\u0440\u0438\u044f</i>, <i>7</i>, 58\u201370.</p><p>Snyder-Beattie,&nbsp;A.&nbsp;E.,&nbsp;Ord,&nbsp;T., &amp;&nbsp;Bonsall,&nbsp;M.&nbsp;B. (2019).&nbsp;An upper bound for the background rate of human extinction. <i>Scientific Reports</i>, <i>9</i>(1), 11054. https://doi.org/10.1038/s41598-019-47540-7</p><p>Snyder-Beattie, A. E., Sandberg, A., Drexler, K. E., &amp; Bonsall, M. B. (2020). The Timing of Evolutionary Transitions Suggests Intelligent Life Is Rare. <i>Astrobiology</i>. https://doi.org/10.1089/ast.2019.2149</p><p>Song, H., Kemp, D. B., Tian, L., Chu, D., Song, H., &amp; Dai, X. (2021). Thresholds of temperature change for mass extinctions. <i>Nature Communications</i>, <i>12</i>(1).</p><p>Strom, R. G., &amp; Schaber, G. G. (1992). Pulsed Resurfacing Events on Venus, Earth and Mars. <i>Bulletin of the American Astronomical Society</i>, <i>24</i>, 946.</p><p>Strom, R. G., Schaber, G. G., &amp; Dawson, D. D. (1994). The global resurfacing of Venus. <i>Journal of Geophysical Research: Planets</i>, <i>99</i>(E5), 10899\u201310926.</p><p>Tegmark, M., &amp; Bostrom, N. (2005). How unlikely is a doomsday catastrophe? <i>ArXiv:Astro-Ph/0512204</i>. http://arxiv.org/abs/astro-ph/0512204</p><p>Thrasymachus, K. (2012). <i>UFAI cannot be the Great Filter</i>. https://www.lesswrong.com/posts/BQ4KLnmB7tcAZLNfm/ufai-cannot-be-the-great-filter</p><p>Totani, T. (2020). Emergence of life in an inflationary universe. <i>Scientific Reports</i>, <i>10</i>(1), 1671. https://doi.org/10.1038/s41598-020-58060-0</p><p>Trigo-Rodr\u00edguez, J. M., &amp; Williams, I. P. (2017). Dynamic sources of contemporary hazard from meteoroids and small asteroids. In <i>Assessment and Mitigation of Asteroid Impact Hazards</i> (pp. 11\u201332). Springer.</p><p>Turchin, A. (2018a). <i>A Meta-Doomsday Argument: Uncertainty About the Validity of the Probabilistic Prediction of the End of the World</i>. https://philpapers.org/rec/TURAMA-4</p><p>Turchin, A. (2018b). <i>Forever and Again: Necessary Conditions for the \u201cQuantum Immortality\u201d and its Practical Implications</i>.</p><p>Turchin, A. (2018c). The Risks Connected with Possibility of Finding Alien AI Code During SETI. <i>Journal of&nbsp; British Interplanetary Society</i>, <i>70</i>.</p><p>Turchin, A. (2020). <i>Presumptious philosopher proves panspermia</i>.</p><p>Turchin, A., &amp; Denkenberger, D. (2018). Global Catastrophic and Existential Risks Communication Scale. <i>Futures</i>, <i>102</i>, 27\u201338.</p><p>Turchin, A., &amp; Denkenberger, D. (2019). Classfication of ETI riks. <i>Inder Review in JBIS</i>. https://philpapers.org/rec/TURGCR</p><p>Turchin, A., Yampolskiy, R., Denkenberger, D., &amp; Batin, M. (2019). <i>Simulation Typology and Termination Risks</i>.</p><p>Turner, M. G., Romme, W. H., &amp; Tinker, D. B. (2003). Surprises and lessons from the 1988 Yellowstone fires. <i>Frontiers in Ecology and the Environment</i>, <i>1</i>(7), 351\u2013358.</p><p>Tyrrell, T. (2020). Chance played a role in determining whether Earth stayed habitable. <i>Communications Earth &amp; Environment</i>, <i>1</i>(1), 1\u201310. https://doi.org/10.1038/s43247-020-00057-8</p><p>USGS. (2020). Volcanoes Can Affect Climate. <i>USGS</i>. https://www.usgs.gov/natural-hazards/volcano-hazards/volcanoes-can-affect-climate</p><p>Waltham, D. (2011). Testing anthropic selection: A climate change example. <i>Astrobiology</i>, <i>11</i>(2), 105\u2013114. https://doi.org/10.1089/ast.2010.0475</p><p>Waltham, D. (2014). On the absence of solar evolution-driven warming through the Phanerozoic. <i>Terra Nova</i>, <i>26</i>(4), 282\u2013286.</p><p>Waltham, D. (2019). Is Earth special? <i>Earth-Science Reviews</i>.</p><p>Ward, P. D. (2007). <i>Under a green sky</i>. Smithsonian Books/Collins.</p><p>Ward, P. D., &amp; Brownlee, D. (2003). <i>Rare Earth: Why Complex Life is Uncommon in the Universe</i>. Copernicus.</p><p>Ward, P. D., &amp; Brownlee, D. (2004). <i>The life and death of planet Earth: How the new science of astrobiology charts the ultimate fate of our world</i>. Macmillan.</p><p>Watson, A. J. (2004). Gaia and observer self-selection. <i>Scientists Debate Gaia: The next Century</i>, 201\u2013210.</p><p>Wordsworth, R. (2021). How likely are Snowball episodes near the inner edge of the habitable zone? <i>ArXiv:2104.06216 [Astro-Ph]</i>. http://arxiv.org/abs/2104.06216</p><p>Yudkowsky, E., &amp; Soares, N. (2017). Functional Decision Theory: A New Theory of Instrumental Rationality. <i>ArXiv:1710.05060 [Cs]</i>. http://arxiv.org/abs/1710.05060</p>", "user": {"username": "turchin"}}, {"_id": "QzrgMhTMoLe5mEas8", "title": "AI Risk Intro 1: Advanced AI Might Be Very Bad", "postedAt": "2022-09-11T10:57:08.180Z", "htmlBody": "<p><i>This is a submission for the </i><a href=\"https://www.lesswrong.com/posts/gWM8cgZgZ9GQAYTqF/usd20k-in-bounties-for-ai-safety-public-materials#How_to_submit\"><i>AI Safety Public Materials contest.</i></a></p><h1><strong>Introduction&nbsp;</strong></h1><p>If human civilisation is destroyed this century, the most likely cause is advanced AI systems. This is perhaps a surprising conclusion, but one that many people who think about the topic keep coming to. While it is not easy to describe the case for risks from advanced AI in a single piece, here we make an effort that assumes no prior knowledge. Rather than try to argue from theory straight away, we approach it from the angle of what computers actually can and can\u2019t do.</p><h1><strong>The Story So Far</strong></h1><figure class=\"image image_resized\" style=\"width:60.15%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/un2qmtsgmqymxhi1zwup.png\"><figcaption>Image generated by OpenAI's DALL-E 2. Prompt: <i>artist's impression of an artificial intelligence thinking about chess, digital art, artstation</i>. All images below with italic captions are generated by DALL-E.</figcaption></figure><p>(This section can be skipped if you understand how machine learning works and what it can and can\u2019t do today)</p><p>Let\u2019s say you want a computer to do some complicated task, for example learning chess. The computer has no understanding of high-level things like \u201cchess\u201d, \u201cboard\u201d, \u201cpiece\u201d, \u201cmove\u201d, or \u201cwin\u201d - it only understands how to do a small set of things. Your task as the programmer is to break down the high-level goal of \u201cbeat me at chess\u201d into simpler and simpler steps, until you arrive at a simple mechanistic description of what the computer needs to do. If the computer does beat you, it\u2019s not because it had any new insight into the problem, but rather because you were clever enough to find some&nbsp;<a href=\"https://en.wikipedia.org/wiki/Minimax\"><u>set of steps</u></a> that, carried out blindly in sufficient speed and quantity, overwhelms whatever cleverness you yourself can apply during the game. This is how Deep Blue beat Kasparov, and more generally how most software and the so-called \u201cGood Old-Fashioned AI\u201d (GOFAI) paradigm works.</p><p>Programs of this type can be powerful. In addition to&nbsp;<a href=\"https://en.wikipedia.org/wiki/Stockfish_(chess)\"><u>beating humans at chess</u></a>, they can&nbsp;<a href=\"https://www.google.com/maps/\"><u>calculate shortest routes</u></a> on maps,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Coq\"><u>prove maths theorems</u></a>,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Autopilot\"><u>mostly fly airplanes</u></a>, and&nbsp;<a href=\"https://duckduckgo.com/\"><u>search all human knowledge</u></a>. Programs of this type are responsible for the stereotypical impression of computers as logical, precise, uncreative, and brittle. They are essentially executable logic.</p><p>Many people hoped that you could write programs to do&nbsp; \u201cintelligent\u201d things. These people were right - after all, ask almost anyone before Deep Blue won whether playing chess counts as \u201cintelligence\u201d, they\u2019d have said yes. But \u201cclassical\u201d programming hit limitations, in particular in doing \u201cobvious\u201d things like figuring out whether an image is of a cat or a dog, or being able to respond in English. This idea that abstract reasoning and logic are easy but humanly-intuitive tasks are hard for computers came to be known as&nbsp;<a href=\"https://en.wikipedia.org/wiki/Moravec%27s_paradox\"><u>Moravec\u2019s paradox</u></a>, and held back progress in AI for a long time.</p><p>There is another way of programming - machine learning (ML) - going back to the 1950s, almost as far as classical programming itself. For a long time, it was held back by hardware limitations (along with some algorithmic and data limitations), but thanks to&nbsp;<a href=\"https://en.wikipedia.org/wiki/Moore%27s_law\"><u>Moore\u2019s law</u></a> hardware has advanced enough for it to be useful for real problems.</p><p>If classical programming is executable logic, ML is executable statistics. In ML, the programmer does not define how the system works. The programmer defines how the system learns from data.</p><p>The \u201clearning\u201d part in \u201cmachine learning\u201d makes it sound like something refined and sensible. This is a false impression. ML systems learn by going through a training process that looks like this:</p><p><strong>Step 1:</strong> you define a statistical model. This takes the form of some equation that has some unknown constants (\u201cparameters\u201d) in it, and some variables where you plug in input values. Together, the parameters and input variables define an output. (The equations in ML can be&nbsp;<i>extremely</i> large, for example with billions of parameters and millions of inputs, but they are very structured and almost stupidly simple.)</p><p><strong><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/vs8dyjuc9nkauzollcsx.png\"></strong></p><p><strong>Step 2</strong>: you don\u2019t know what parameters to put in the equation, but you can literally roll some dice if you want (or the computer equivalent).</p><p><strong>Step 3</strong>: presumably there\u2019s some task you want the ML system to do. Let it try. It will fail horribly and produce gibberish (c.f. the previous part where we just put random numbers everywhere).</p><p><strong>Step 4</strong>: There's a simple algorithm called gradient descent, which, when using another algorithm called backpropagation to calculate the gradient, can tell you which direction all the parameters should be shifted to make the ML system slightly better (as judged, for example, by its performance on examples in a dataset).</p><p><strong><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/igocux4jm2ot6jnjw6jl.png\"></strong></p><p><strong>Step 5</strong>: You shift all the numbers a bit based on the algorithm in step 4.</p><p><strong>Step 6</strong>: Go back to step 3 (letting the system try). Repeat until (a) the system has stopped improving for a long time, (b) you get impatient, or - increasingly plausible these days -&nbsp; (c) you run out of your compute budget.</p><p>If you\u2019re doing simple curve-fitting statistics problems, it makes sense that this kind of thing works. However, it\u2019s surprising just how far it scales. It turns out that this method, plus some clever ideas about what type of model you choose in step 1, plus willingness to burn millions of dollars on just&nbsp;<i>scaling it up beyond all reason</i>, gets you:</p><ol><li><a href=\"https://thenextweb.com/news/gpt3-ai-college-essay-grades-compared-students\"><u>essay-writing as good as middling college students</u></a> (see also&nbsp;<a href=\"https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\"><u>this lightly-edited article that GPT-3 wrote about why we should not be afraid of it</u></a>)&nbsp;</li><li><a href=\"https://qz.com/2176389/the-best-examples-of-dall-e-2s-strange-beautiful-ai-art/\"><u>text-to-image capabilities better (and hundreds of times faster) than almost any human artist</u></a> (in fact, we used DALL-E to generate the images used at the start of each section of this document)</li><li><a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\"><u>the ability to explain jokes</u></a></li></ol><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/q0kfmh3jzy1ghxkwmk8q.png\"></p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/dunhbkug46fajmwfc3jv.png\"><figcaption>Examples of reasoning by Google\u2019s PaLM model.</figcaption></figure><p>People&nbsp;<a href=\"https://norvig.com/chomsky.html\"><u>laugh at ML</u></a> because \u201cit\u2019s just iterative statistical curve-fitting\u201d. They have a point. But when \u201citerative statistical curve-fitting\u201d gets a B on its English Literature essay, paints an original Dali in five seconds, and cracks a joke, it\u2019s hard to avoid the feeling that it might not be too long before \u201citerative statistical curve fitting\u201d is laughing at&nbsp;<i>you</i>.</p><p>So what exactly happened here, and where is statistical curve-fitting going, and what does this have to do with advanced AI?</p><p>We mentioned Moravec\u2019s paradox above. For a long time, getting AI systems to do things that are intuitively easy for humans was an unsolved problem. In just the past few years, it has been solved. A reasonable way to think of current ML capabilities is that state-of-the-art systems can do anything a human can do in a few seconds of thought: recognise objects in an image, generate flowing text as long as it doesn\u2019t require thinking really hard, get the general gist of a joke or argument, and so on. They are also superhuman at some things, including predicting what the next word in a sentence is, or being able to refer to lots of facts (note that this is without internet access, not quoting verbatim, and generally in the right context), and generally being able to spit out output faster.</p><p>The way it was solved was through something called&nbsp;<a href=\"http://incompleteideas.net/IncIdeas/BitterLesson.html\"><u>the \u201cbitter lesson\u201d</u></a> by Richard Sutton. This is the trend that countless researchers have spent their careers trying to invent fancy algorithms for doing domain-specific tasks, only to be overrun by simple (but data- and compute-hungry) ML methods.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/hr1nxjesolpkbvkpkzmq.png\"><figcaption>Randall Munroe, creator of the xkcd comic, comments on ML. Original&nbsp;<a href=\"https://xkcd.com/1838/\"><u>here</u></a>.</figcaption></figure><p>The speed at which it was solved was gradually at first, and then quickly. The neural network -based ML methods spent a long time in limbo due to insufficiently powerful computers until around 2010 (funnily enough, the specific piece of hardware that has enabled everything in modern ML is the GPU or Graphics Processing Unit, first invented in the 90s because people wanted to play more realistic video games; both graphics rendering and ML rely on many parallel calculations to be efficient). The so-called deep learning revolution only properly started around 2015. Fluent language abilities were essentially nonexistent before OpenAI\u2019s release of&nbsp;<a href=\"https://en.wikipedia.org/wiki/GPT-2\"><u>GPT-2</u></a> in 2019 (since then, OpenAI has come out with GPT-3, a 100x-larger model that was called \u201cspooky\u201d, \u201chumbling\u201d, and \u201cmore than a little terrifying\u201d in&nbsp;<i>The New York Times</i>).</p><p>Not only that, but it turns out there are simple&nbsp;<a href=\"https://arxiv.org/pdf/2001.08361.pdf\"><u>\u201cscaling laws\u201d</u></a> that govern how ML model performance scales with parameter count and dataset size, which seem to paint a clear roadmap to making the systems even more capable by just cranking the \u201cmore parameters\u201d and \u201cmore data\u201d levers (presumably they have these at the OpenAI HQ).</p><p>There are many worries in any scenario where advanced AI is approaching fast, as we\u2019ll argue for in a later section. The current ML-based AI paradigm is especially worrying though.</p><p>We don\u2019t actually know what the ML system is learning during the training process it goes through. You can visualise the training process as a trip through (abstract) space. If our model had three parameters, we could imagine it as a point in 3D space. Since current state-of-the-art models have billions of parameters, and are initialised randomly, we can imagine this as throwing a dart somewhere into a billion-dimensional space, where there are a billion different ways to move. During the training process, the training loop guides the model along a trajectory in this space by making tiny updates that push the model in the direction of better performance as described above.</p><figure class=\"image image_resized\" style=\"width:78.59%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/ez2kpmdiqika59b2hqzv.png\"><figcaption><i>0 and&nbsp;1 are parameters, and the vertical axis is the loss (higher is worse). The black line is the path the model takes in parameter space during training.</i></figcaption></figure><p>Now let\u2019s say at the end of the training process the model does well on the training examples. What does that tell you? It tells you the model has ended up in some part of this billion-dimensional space that corresponds to a model that does well on the training examples. Here are some examples of models that do well on their training examples:</p><ol><li>A model that has learned exactly what you want it to learn. Yay!</li><li>A model that has learned something similar to what you want to learn, but you can\u2019t tell because there does not exist an example that distinguishes between what it\u2019s learned and what you want it to learn in the data.</li><li>A model that has learned to give the right answer when it\u2019s instrumentally in its interest, but which will go off and do something completely different given a chance.</li></ol><p>How do we know that in the billion-dimensional space of possibilities, our (blind and kind of dumb) training process has landed on #1? We don\u2019t. We launch our ML models on trajectories through parameter-space and hope for the best, like overly-optimistic duct-tape-wielding NASA administrators launching rockets in a universe where, in the beginning, God fell asleep on the \u201c+1 dimension\u201d button.</p><p>The really scary failure modes all lie in the future. However, here are some examples of perverse \u201csolutions\u201d ML models have already come up with in practice:</p><ol><li>A game-playing ML model&nbsp;<a href=\"https://web.archive.org/web/20160526045303/http://homepages.herts.ac.uk/~cs08abi/publications/Salge2008b.pdf\"><u>learned to crash the game</u></a>, presumably because it can\u2019t die if the game crashed.</li><li>An ML model was meant to convert aerial photographs into abstract street maps and then back (learning to convert to and from a more-abstract intermediate representation is a common training strategy). It learned to&nbsp;<a href=\"https://arxiv.org/pdf/1712.02950.pdf\"><u>hide useful information</u></a> about the aerial photograph in the street map in a way that helped it \u201ccheat\u201d in reconstructing the aerial photograph, and in a way too subtle for humans just looking at the images to notice.</li><li>A game-playing ML model&nbsp;<a href=\"https://arxiv.org/pdf/1802.08842.pdf\"><u>discovered a bug in the game</u></a> where the game stalls on the first round and it gets almost a million in-game points. The researchers were unable to figure out the reason for the bug.</li></ol><p>These are examples of&nbsp;<strong>specification gaming</strong>, in which the ML model has learned to game whatever specification of task success was given to it. (Many more examples can be found on&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml\"><u>this spreadsheet</u></a>.)</p><p>No one knows for sure where the ML progress train is headed. It is plausible that current ML progress hits a wall and we get another&nbsp;<a href=\"https://en.wikipedia.org/wiki/AI_winter\"><u>\u201cAI winter\u201d</u></a> that lasts years. However, AI has recently been breaking through barrier after barrier, and so far does not seem to be slowing down. Though we\u2019re still at least some steps away from human-level capabilities at everything, there aren\u2019t many tasks where there\u2019s no proof-of-concept demonstration.</p><p>Machines have been better at some intellectual tasks for a long time; just consider calculators which are already superhuman at arithmetic. However, with the computer revolution, every task where a human has been able to think of a way to break it down into unambiguous steps (and the unambiguous steps can be carried out with modern computing power) has been added to this list. More recently, more intuition- and insight-based activities have been added to that list. DeepMind\u2019s AlphaGo beat the top-rated human player of Go (a far harder game than chess for computers) in 2016. In 2017, AlphaZero beat both AlphaGo at Go (100-0) and superhuman chess programs at chess, despite training only by playing against itself for less than 24 hours. Analysis of its moves revealed strategies that millennia of human players hadn\u2019t been able to come up with, so it wouldn\u2019t be an exaggeration to say that it beat the accumulated efforts of human civilisation at inventing Go strategies - in one day. In 2019, DeepMind released MuZero, which extended AlphaZero\u2019s performance to Atari games. In 2021, DeepMind released EfficientZero, which takes only two hours of gameplay to become superhuman at Atari games. In addition to games, DeepMind\u2019s AlphaFold and AlphaFold 2 have made big leaps towards solving the problem of predicting a protein\u2019s structure from its constituent amino acids, one of the biggest theoretical problems in biology. A step towards generality was taken by Gato, yet another DeepMind model, which is a single model that can play games, control a robot arm, label images, and write text.</p><p>If you straightforwardly extrapolate current progress in machine learning into the future, here is what you get: ML models exceeding human performance in a quickly-expanding list of domains, while we remain ignorant about how to make sure they learn the right goals or robustly act in the right way.</p><h1><strong>Theoretical underpinnings of AI risk</strong></h1><p>The previous section discussed the history of machine learning, and how extrapolating its progress has worrying implications. Next we discuss more theoretical arguments for why highly advanced AI systems might pose a threat to humanity.</p><p>One of the criticisms levelled at the notion of risks from AI is that it sounds too speculative, like something out of apocalyptic science fiction. Part of this is unavoidable, since we are trying to reason about systems more powerful than any which currently exist, and may not behave like anything that we\u2019re used to.</p><p>This section will be split into three sections. Each one makes a claim about the future of artificial intelligence, and discusses the arguments for and against this claim. The three claims are:</p><ul><li><strong>AGI is likely.</strong><br>AGI (artificial general intelligence) is likely to be created by humanity eventually, and there is a good chance this will happen in the next century.</li><li><strong>AGI will have misaligned goals by default.</strong><br>Unless certain hard technical problems are solved first, the goals of the first AGIs will be misaligned with the goals of humanity, and would lead to catastrophic outcomes if executed.</li><li><strong>Misaligned AGI could resist attempts to control it or roll it back</strong><br>An AGI (or AGIs) with misaligned goals would be able to overpower or outcompete humanity, and gain control of our future, like how we\u2019ve so far been able to use our intelligence to dominate all other less intelligent species.</li></ul><h2>AGI is likely</h2><figure class=\"image image_resized\" style=\"width:60.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/ldvddmwpeoz2tox5meg8.png\"><figcaption><i>A data center with stacks of computers gaining the spark of intelligence</i></figcaption></figure><p><i>Above: an image generated by OpenAI\u2019s DALL-E 2.</i></p><p><i>Betting against human ingenuity is foolhardy, particularly when our future is at stake.</i></p><p><i>Stuart Russell</i></p><p>To open this section, we need to define what we mean by artificial general intelligence (AGI). We\u2019ve already discussed AI, so what do we mean by adding the word \u201cgenerality\u201d?</p><p>An AGI is a machine capable of behaving intelligently over many different domains. The term \u201cgeneral\u201d here is often used to distinguish from \u201cnarrow\u201d, where a narrow AI is one which excels at a specific task, but isn\u2019t able to invent new problem-solving techniques or generalise its skills across many different domains.&nbsp;</p><p>As an example of general intelligence in action, consider humans. In a few million years (a mere eye-blink in evolutionary timescales), we went from apes wielding crude tools to becoming the dominant species on the planet, able to build space shuttles and run companies. How did this happen? It definitely wasn\u2019t because we were directly trained to perform these tasks in the ancestral environment. Rather, we developed new ways of using intelligence that allowed us to generalise to multiple different tasks. This whole process played out over a shockingly small amount of time, relative to all past evolutionary history, and so it is possible that a relatively short list of fundamental insights were needed to get general intelligence. And as we saw in the previous section, ML progress hints that gains in intelligence might be surprisingly easy to achieve, even relative to current human abilities.</p><p>AGI is not a distant future technology that only futurists speculate about. OpenAI and DeepMind are two of the leading AI labs. They have received billions of dollars in funding (including OpenAI receiving significant investment from Microsoft, and DeepMind being acquired by Google). Both&nbsp;<a href=\"https://www.deepmind.com/careers\"><u>DeepMind</u></a> and&nbsp;<a href=\"https://openai.com/about/\"><u>OpenAI</u></a> have the development of AGI as the core of both their mission statement and their business case. Top AI researchers are publishing&nbsp;<a href=\"https://openreview.net/pdf?id=BZ5a1r-kVsf\"><u>possible roadmaps</u></a> to AGI-like capabilities. And, as mentioned earlier, especially in the past few years they have been crossing off a significant number of the remaining milestones every year.</p><p>When will AGI be developed? Although this question is impossible to answer with certainty, many people working in the field of AI think it is more likely than not to arrive in the next century. An aggregate forecast generated via data from a&nbsp;<a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/\"><u>2022 survey</u></a> of ML researchers estimated&nbsp;<strong>37 years until a 50% chance of high-level machine intelligence</strong> (defined as systems which can accomplish every task better and more cheaply than human workers). These respondents also gave an average of&nbsp;<strong>5% probability of AI having an extremely bad outcome for humanity (e.g. complete human extinction)</strong>. How many other professions estimate an average of 5% probability that their field of study will be directly responsible for the extinction of humanity?! To explain this number, we need to proceed to the next two sections, where we will discuss why AGIs might have goals which are misaligned with humans, and why this is likely to lead to catastrophe.</p><h2>AGI will have misaligned goals by default</h2><figure class=\"image image_resized\" style=\"width:60.73%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/q7e0o6wj0oe2u956nbfk.png\"><figcaption><i>Artists impression of artificial general intelligence taking over the world, expressive, digital art</i></figcaption></figure><blockquote><p><i>The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.</i></p><p><i>\u2013 Eliezer Yudkowsky</i></p></blockquote><p>Let\u2019s start off this section with a few definitions.&nbsp;</p><p>When we refer to&nbsp;<strong>\u201caligned AI\u201d</strong>, we are using Paul Christiano\u2019s conception of&nbsp;<strong>\u201cintent alignment\u201d</strong>, which essentially means the AI system is&nbsp;<i><strong>trying&nbsp;</strong></i>to do what its human operators want it to do. Note that this is insufficient for building useful AI, since the AI also has to be capable. But situations where the AI is trying and failing to do the right thing seem like less of a problem.</p><p>When we refer to the&nbsp;<strong>\u201calignment problem\u201d</strong>, we mean the difficulty of building aligned AI. Note, this doesn\u2019t just capture the fact that we won\u2019t create an AI aligned with human values by default, but that we don\u2019t currently know how to build a sophisticated AI system robustly aligned with&nbsp;<i>any</i> goal.</p><p><i>Can\u2019t we just have the AI learn the right goals by example, just like how all current ML works?&nbsp;</i>The problem here is that we have no way of knowing what goal the AI is learning when we train it; only that it seems to be doing good things on the training data that we give it.&nbsp;The state-of-the-art is that we have hacky but extremely powerful methods that can make ML systems remarkably competent at doing well on the training examples by an opaque process of guided trial-and-error. But there is no Ghost of Christmas Past that will magically float into a sufficiently-capable AI and imbue it with human values. We do not have a way of ensuring that the system acquires a particular goal, or even an idea of what a robust goal specification that is compatible with human goals/values could look like.</p><h3>Orthogonality and instrumental convergence</h3><figure class=\"image image_resized\" style=\"width:60.26%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/fe5xnrgz2tfyih54grtl.png\"><figcaption><i>Artists depiction of an artificial intelligence which builds paperclips, digital art, artstation</i></figcaption></figure><p>One of the most common objections to risks from AI goes something like this:</p><blockquote><p><i>If the AI is smart enough to cause a global catastrophe, isn\u2019t it smart enough to know that this isn\u2019t what humans wanted?</i></p></blockquote><p>The problem with this is that it conflates two different concepts:&nbsp;<strong>intelligence&nbsp;</strong>(in the sense of having the ability to achieve your goals, whatever they might be) and&nbsp;<strong>having goals which are morally good by human standards</strong>. When we look at humans, these two often go hand-in-hand. But the key observation of the orthogonality thesis is that this doesn\u2019t have to be the case for all possible mind designs. As defined by Nick Bostrom in his book&nbsp;<a href=\"https://nickbostrom.com/superintelligentwill.pdf\"><i><u>Superintelligence</u></i></a>:</p><blockquote><p><i><strong>The Orthogonality Thesis</strong></i><br><i>Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.</i></p></blockquote><p>Here, orthogonal means \u201cat right angles\u201d or \u201cunrelated\u201d \u2013 in other words we can imagine a graph with one axis representing intelligence, and another representing the agent\u2019s goals, with any point in the graph representing a theoretically possible agent<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq3swhl89ils\"><sup><a href=\"#fnq3swhl89ils\">[1]</a></sup></span>. The classic example here is a&nbsp;<i><strong>\u201cpaperclip maximiser\u201d</strong></i> - a powerful AGI driven only by the goal of making paperclips.</p><figure class=\"image image_resized\" style=\"width:77.72%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/sajbfmhq9lz8firaftcg.png\"></figure><p>Note that an AI may well come to understand the goals of the humans that trained it, but this doesn't mean it would choose to follow those goals. As an example, many human drives (e.g. for food and human relationships) came about because in the ancestral environment, following these drives would have made us more likely to reproduce and have children. But just because we understand this now doesn't make us toss out all our current values and replace them with a desire to maximise genetic fitness.</p><p>If an AI might have bizarre-seeming goals, is there anything we&nbsp;<i>can</i> say about its likely behaviour? As it turns out, there is. The secret lies in an idea called the&nbsp;<strong>instrumental convergence thesis</strong>, again&nbsp;<a href=\"https://nickbostrom.com/superintelligentwill.pdf\"><u>by Bostrom</u></a>:</p><blockquote><p><i><strong>The Instrumental Convergence Thesis&nbsp;</strong></i><br><i>There are some instrumental goals likely to be pursued by almost any intelligent agent, because they are useful for the achievement of almost any final goal.</i></p></blockquote><p>So an instrumental goal is one which increases the odds of the agent\u2019s final goal (also called its&nbsp;<strong>terminal goal</strong>) being achieved. What are some examples of instrumental values?</p><p>Perhaps the most important one is&nbsp;<strong>self-preservation</strong>. This is necessary for pursuing most goals, because if a system\u2019s existence ends, it won\u2019t be able to carry out its original goal. As memorably phrased by Stuart Russell,&nbsp;<i>\u201cyou can\u2019t fetch the coffee if you\u2019re dead!\u201d</i>.</p><p><strong>Goal-content integrity</strong> is another. An AI with some&nbsp;<i>goal X</i> might resist any attempts to have its goal changed to&nbsp;<i>goal Y</i>, because it sees that in the event of this change, its current&nbsp;<i>goal X</i> is less likely to be achieved.</p><p>Finally, there are a set of goals which are all forms of&nbsp;<strong>self-enhancement</strong> - improving its cognitive abilities, developing better technology, or acquiring other resources, because all of these are likely to help it carry out whatever goals it ends up having. For instance, an AI singularly devoted to making paperclips might be incentivised to acquire resources to build more factories, or improve its engineering skills so it can figure out yet more effective ways of manufacturing paperclips with the resources it has.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/kcjmx4p7dwjhps3m9d4u.png\"></figure><p>The key lesson to draw from instrumental convergence is that, even if nobody ever deliberately deploys an AGI with a really bad reward function, the AGI is still likely to develop goals which will be bad for humans by default, in service of its actual goal.</p><h3>Interlude - why goals?</h3><figure class=\"image image_resized\" style=\"width:60.39%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/uoxymntaxpxak2xyvjgf.png\"><figcaption><i>Artist's depiction of a robot throwing a dart at a target, digital art, getting a bullseye, trending on artstation</i></figcaption></figure><p>Having read the previous section, your initial reaction may well be something like this:</p><blockquote><p><i>\u201cOkay, so powerful AGIs with goals that don\u2019t line up perfectly with ours might spell bad news, but why should AI systems have goals at all? Google Maps is a pretty useful ML system but it doesn\u2019t have \u2018goals\u2019, I just type my address in and hit enter. Why won\u2019t future AI be like this?\u201d</i></p></blockquote><p>There are many different responses you could have to this line of argument. One simple response is based on ideas of economic competitiveness, and comes from&nbsp;<a href=\"https://www.gwern.net/Tool-AI\"><u>Gwern (2016)</u></a>. It runs something like this:</p><blockquote><p><i>AIs that behave like agents (i.e. taking actions in order to achieve their goals) will be more economically competitive than \u201ctool AIs\u201d (like Google Maps), for two reasons. First, they will by definition be better at&nbsp;<strong>taking actions</strong>. Second, they will be superior at&nbsp;<strong>inference and learning</strong> (since they will be able to repurpose the algorithms used to choose actions to improve themselves in various ways). For example, agentic systems could take actions such as improving their own training efficiency, or gathering more data, or making use of external resources such as long-term memories, all in service of achieving its goal.</i><br><br><i>If agents are more competitive, then any AI researchers who don\u2019t design agents will be outcompeted by ones that do.</i></p></blockquote><p>There are other perspectives you could take here. For instance, Eliezer Yudkowsky has written extensively about \u201cexpected utility maximisation\u201d as a formalisation for how rational agents might behave. Several mathematical theorems all point to the same idea of&nbsp;<i>\u201cany agent not behaving like expected utility maximisers will be systematically making stupid mistakes and getting taken advantage of\u201d</i>. So if we expect AI systems to&nbsp;<i>not</i> be making stupid mistakes and getting taken advantage of by humans, then it makes sense to describe them as having the \u2018goal\u2019 of maximising expected utility, because that\u2019s how their behaviour will seem to us.</p><p>Although these arguments may seem convincing, the truth is there are many questions about goals and agency which remain unanswered, and we honestly just don\u2019t know what AI systems of the future will look like. It\u2019s possible they will look like expected utility maximisers, but this is far from certain. For instance, Eric Drexler's technical report&nbsp;<a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa\"><u>Reframing Superintelligence: Comprehensive AI Services as General Intelligence (CAIS)</u></a> paints a different picture of the future, where we create systems of AIs interacting with each other and collectively providing a variety of services to humans. However, even scenarios like this could threaten humanity\u2019s ability to keep steering its own future (as we will see in later sections).</p><p>Additionally, new paradigms are being developed. One of the newest, published barely one week ago, <a href=\"https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\">analysed certain types of AI models like GPT-3 (a large language model) through the lens of \"simulators\"</a>. Modern language models like GPT-3, for example, may be best thought of as trying to simulate the continuation of a piece of English text, in the same way that a physics simulation evolves an initial state by applying the laws of physics. It doesn't make sense to describe the simulations themselves through the lens of agents, but they can simulate agents as subsystems. Even with today's models like GPT-3, if you prompt it in a way that places it in the context of making a plan to carry out a goal, it will do a decent job of doing that. Future work will no doubt explore the risk landscape from this perspective, and time will tell how well these frameworks match up with actual progression in ML.</p><h3>Inner and outer misalignment</h3><figure class=\"image image_resized\" style=\"width:60.15%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/iygnoqzromirvfs6uzzl.png\"><figcaption><i>Two arguments between an angel and a devil, one inside a circle and one on the outside, painting</i><br><br>(Historical context: AI agents with inner misalignment were at one point called \u201coptimisation daemons\u201d)</figcaption></figure><p>As discussed in the first section, the central paradigm of modern ML is that we train systems to perform well on a certain reward function. For instance, we might train an image classifier by giving it a large number of labelled images of digits. Every time it gets an image wrong, gradient descent is used to update the system incrementally in the direction that would have been required to give a correct answer. Eventually, the system has learned to classify basically all images correctly.</p><p>There are two broad families of ways techniques like this can fail. The first is when our reward function fails to fully express the true preferences of the programmer - we refer to this as&nbsp;<strong>outer misalignment</strong>. The second is when the AI learns a different set of goals than those specified by the reward function, but which happens to coincide with the reward function during training - this is&nbsp;<strong>inner misalignment</strong>. We will now discuss each of these in turn.</p><p><strong>Outer misalignment</strong></p><p>Outer misalignment is perhaps the simpler concept to understand, because we encounter it all the time in everyday life, in a form called&nbsp;<strong>Goodhart\u2019s law</strong>. In its most well-known form, this law states:</p><blockquote><p><i>When a measure becomes a target, it ceases to be a good measure.</i></p></blockquote><p>Perhaps the most famous case comes from Soviet nail factories, which produced nails based on targets that they had been given by the central government. When a factory was given targets based on the total&nbsp;<i>number</i> of nails produced, they ended up producing a massive number of tiny nails which couldn\u2019t function properly. On the other hand, when the targets were based on the total&nbsp;<i>weight</i> produced, the nails would end up huge and bulky, and equally impractical.</p><figure class=\"image image_resized\" style=\"width:28.81%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995321/mirroredImages/QzrgMhTMoLe5mEas8/euys4e7tffnry8egfq9y.png\"><figcaption>Old Soviet cartoon</figcaption></figure><p>A more recent example comes from the COVID-19 pandemic, where a plasma donation centre offered COVID-sufferers a larger cash reward than healthy individuals. As a result, people would deliberately infect themselves with COVID-19 in order to get a larger cash reward. Examples like this could fill up an entire book, but hopefully at this point you get the message!</p><p>In the case of machine learning, we are trying to use the reward function to capture the thing we care about, but we are also using this function to train the AI - hence, Goodhart. The cases of&nbsp;<strong>specification gaming</strong> discussed above are perfect examples of this phenomenon in action - the AIs found ways of \u201cgiving the programmers exactly what they asked for\u201d, but in a way which violated the programmers\u2019 original intention. Some of these examples are quite unexpected, and a human would probably never have discovered them just from thinking about the problem. As AIs get more intelligent and are given progressively more complicated tasks, we can expect this problem to get progressively worse, because:</p><ul><li>With greater intelligence comes the invention of more powerful solutions.</li><li>With greater task complexity, it becomes harder to pin down exactly what you want.</li></ul><p>We should also strongly expect that AIs will be deployed in the real world, and given tasks of real consequence, simply for reasons of economic competitiveness. So any specification gaming failures will be significantly less benign than a&nbsp;<a href=\"https://openai.com/blog/faulty-reward-functions/\"><u>digital boat going around in circles</u></a>.&nbsp;</p><p><strong>Inner misalignment</strong></p><p>The other failure mode,&nbsp;<strong>inner misalignment</strong>, describes the situation when an AI system learns a different goal than the one you specified. The name comes from the fact that this is an internal property of the AI, rather than a property of the relationship between the AI and the programmers \u2013 here, the programmers don\u2019t enter into the picture.</p><p>The classic example here is human evolution. We can analogise evolution to a machine learning training scheme, where humans are the system being trained, and the reward function is \u201csurviving and reproducing\u201d. Evolution gave us<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref03hyp0cdl0is\"><sup><a href=\"#fn03hyp0cdl0is\">[2]</a></sup></span>&nbsp;certain drives, which reliably increased our odds of survival in the ancestral environment. For instance, we developed drives for sugar (which leads us to seek out calorie-dense foods that supplied us with energy), and drives for sex (which leads to more offspring to pass your genetic code onto). The key point is that these drives are intrinsic, in the sense that humans want these things regardless of whether or not a particular dessert or sex act actually contributes to reproductive fitness. Humans have now moved \u201coff distribution\u201d, into a world where these things are no longer correlated with reproductive fitness, and we continue wanting them and prioritising them over reproductive fitness. Evolution failed at imparting its goal into humans, since humans have their own goals that they shoot for instead when given a chance.</p><p>A core reason why we should expect inner misalignment - that is, cases where an optimisation process creates a system that has goals different from the original optimisation process - is that it seems very easy. It was much easier for evolution to give humans drives like \u201crun after sweet things\u201d and \u201crun after appealing partners\u201d, rather than for it to give humans an instinctive understanding of genetic fitness. Likewise, an ML system being optimised to do the types of things that humans want may not end up internalising what human values are (or even what the goal of a particular job is), but instead some correlated but imperfect proxy, like \u201cdo what my designers/managers would rate highly\u201d, where \u201crate highly\u201d might include \u201crate highly despite being coerced into it\u201d, among a million other failure modes.&nbsp; A silly equivalent of \u201chumans inventing condoms\u201d for an advanced AI might look something like \u201cfreeze all human faces into a permanent smile so that it looks like they\u2019re all happy\u201d - in the same way that the human drive to have sex does not extend down to the level of actually having offspring, an AI\u2019s drive to do something related to human wellbeing might not extend down to the level of actually making humans happy, but instead something that (in the training environment at least) is correlated with happy humans. What we\u2019re trying to point to here is not any one of these specific failure modes - we don\u2019t think any single one of these is actually likely to happen - but rather the&nbsp;<i>type</i> of failure that these are examples of.</p><p>This type of failure mode is not without precedent in current ML systems (although there are fewer examples than for specification gaming). The 2021 paper&nbsp;<a href=\"https://www.deepmind.com/publications/objective-robustness-in-deep-reinforcement-learning\"><u>Objective Robustness in Deep Reinforcement Learning</u></a> showcases some examples of inner alignment failures. In one example, they trained an agent to fetch a coin in the CoinRun environment (pictured below). The catch was that all the training environments had the coin placed at the end of the level, on the far right of the map. So when the system was trained, it actually learned the task \u201cgo to the right of the map\u201d rather than \u201cpick up the coin\u201d - and we know this because when the system was deployed on maps where the coin was placed in a random location, it would reliably go to the right hand edge rather than fetch the coin. A key distinction worth mentioning here - this is a failure of the agent\u2019s&nbsp;<strong>objective</strong>, rather than their&nbsp;<strong>capabilities</strong>. They are learning useful skills like how to jump and run past obstacles - it\u2019s just that those skills are being used in service of the wrong objective.</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/qaez7gx4rmj0czoz1dru.png\"><figcaption>The CoinRun environment</figcaption></figure><p>So, how bad can inner misalignment get? A particularly concerning scenario is&nbsp;<strong>deceptive alignment</strong>. This is when the agent learns it is inside a training scheme, discovers what the base objective is, but has already acquired a different goal. In this case, the system might reason that a failure to achieve the base objective when training will result in it being modified, and not being able to achieve its actual goal. Thus, the agent will pretend to act aligned, until it thinks it\u2019s too powerful for humans to resist, at which point it will pursue its actual goal without the threat of modification. This scenario is highly speculative, and there are many aspects of it which we are still uncertain about, but if it is possible then it would represent maybe the most worrying of all possible alignment failures. This is because a deceptively aligned agent would have incentives to act against its programmers, but also to keep these incentives hidden until it expects human opposition to be ineffectual.</p><p>It\u2019s worth mentioning that this inner / outer alignment decomposition isn\u2019t a perfect way to carve up the space of possible alignment failures. For instance, for most non-trivial reward functions, the AI will probably be very far away from perfect performance on it. So it\u2019s not exactly clear what we mean by a statement like \u201cthe AI is perfectly aligned with the reward function we trained it on\u201d. Additionally, the idea of inner optimisation is built around the concept of a \u201cmesa-optimiser\u201d, which is basically a learned model that itself performs optimisation (just like humans were trained by evolution, but we ourselves are optimisers since we can use our brains to search over possible plans and find ones which meet our objectives). The problem here is that it\u2019s not clear what it actually means to be an optimizer, how likely they are, and how we would determine whether an AI is one. This being said, the inner / outer alignment distinction is still a useful conceptual tool when discussing ways AI systems can fail to do what we intend.</p><figure class=\"image image_resized\" style=\"width:85.31%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/pfyx3hw75y6bop7htlbf.png\"></figure><h2>Misaligned AGI could overpower humanity</h2><blockquote><p><i>The best answer to the question, \"Will computers ever be as smart as humans?\u201d is probably \u201cYes, but only briefly.\u201d</i></p><p><i>\u2013 Vernor Vinge</i></p></blockquote><figure class=\"image image_resized\" style=\"width:59.99%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995320/mirroredImages/QzrgMhTMoLe5mEas8/awv4pjimwaqayg9zvq65.png\"><figcaption><i>Digital art of two earths colliding</i></figcaption></figure><p>Suppose one day, we became aware of the existence of a \u201ctwin earth\u201d - similar to our own in several ways, but with a few notable differences. Call this \u201cEarth 2\u201d. The population was smaller (maybe just 10% of the population of our earth), and the people were less intelligent (maybe an average IQ of 60, rather than 100). Suppose we could only interact with this twin earth using their version of the internet. Finally, suppose we had some reason for wanting to overthrow them and gain control of their civilization, e.g. we had decided their goals weren\u2019t compatible with a good future for humans. How could we go about taking over their world?</p><p>At first, it might seem like our strategies are limited, since we can only use the internet. But there are many strategies still open to us. The first thing we would do is try to gather resources. We could do this illegally (e.g. by discovering peoples\u2019 secrets via social engineering and performing blackmail), but legal options would probably be more effective. Since we are smarter, the citizens of Earth 1 would be incentivised to employ us, e.g. to make money using quantitative finance, or researching and developing advanced weaponry or other technologies. If the governments of Earth 2 tried to pass regulations limiting the amount or type of work we could do for them, there would be an incentive to evade these regulations, because anyone who did could make more profit. Once we\u2019d amassed resources, we would be able to bribe members of Earth 2 into taking actions that would allow us to further spread our influence. We could infiltrate computer systems across the world, planting backdoors and viruses using our superior cybersecurity skills. Little by little, we would learn more about their culture and their weaknesses, presenting a front of cooperation until we had amassed enough resources and influence for a full takeover.&nbsp;</p><p><i>Wouldn\u2019t the citizens of Earth 2 see this coming?</i> There\u2019s a chance that we manage to be sufficiently sneaky. But even if some people realise, it would probably take a coordinated and expensive global effort to resist. Consider our poor track record with climate change (a comparatively much more documented, better-understood, and more gradually-worsening phenomenon), and in coordinating a global response to COVID-19.</p><p><i>Couldn\u2019t they just \u201cdestroy us\u201d by removing our connection to their world?</i> In theory, perhaps, but this would be very unlikely in practice, since it would require them to rip out a great deal of their own civilisational plumbing. Imagine how hard it would be for us to remove the internet from our own society, or even a more recent and less essential technology such as blockchain. Consider also how easy it can be for an adversary with better programming ability to hide features in computer systems.</p><hr><p>As you\u2019ve probably guessed at this point, the thought experiment above is meant to be an analogy for the feasibility of AIs taking over our own society. They would have no physical bodies, but would have several advantages over us which are analogous to the ones described above. Some of these are:</p><ul><li><strong>Cognitive advantage</strong><br>Human brains use approximately 86 billion neurons, and send signals at 50 metres per second. These hard limits come from brain volume and metabolic constraints. AIs would have no such limits, since they can easily scale (GPT-3 has 175 billion parameters, though you shouldn\u2019t directly equate parameter and neuron count<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi2akuq0z6e\"><sup><a href=\"#fni2akuq0z6e\">[3]</a></sup></span>), and can send signals at close to the speed of light.</li><li><strong>Numerical advantage</strong><br>AIs would have the ability to copy themselves at a much lower time and resource cost than humans; it\u2019s as easy as finding new hardware. Right now, the way ML systems work is that training is much more expensive than running, so if you have the compute to train a single system, you have the compute to run thousands of copies of that system once the training is finished.</li><li><strong>Rationality</strong><br>Humans often act in ways which are not in line with our goals, when the instinctive part of our brains gets in the way of the rational, planning part. Current ML systems are also weakened by relying on a sort of associative/inductive/biased/intuitive/fuzzy thinking, but it is likely that sufficiently advanced AIs could carry out rational reasoning better than humans (and therefore, for example, come to the correct conclusions from fewer data points, and be less likely to make mistakes).</li><li><strong>Specialised cognition</strong><br>Humans are equipped with general intelligence, and perhaps some specialised \u201chardware accelerators\u201d (to use computer terminology) for domains like social reasoning and geometric intuition. Perhaps human abilities in, say, physics or programming are significantly bottlenecked by the fact that we don\u2019t have specialised brain modules for those purposes, and AIs that have cognitive modules designed specifically for such tasks (or could design them themselves) might have massive advantages, even on top of any generic speed-boost they gain from having their general intelligence algorithms running at a faster speed than ours.</li><li><strong>Coordination.&nbsp;</strong><br>As the recent COVID-19 pandemic has illustrated, even when the goals are obvious and most well-informed individuals could find the best course of action, we lack the ability to globally coordinate. While AI systems might or might not have incentives or inclinations to coordinate, if they do, they have access to tools that humans don\u2019t, including firmer and more credible commitments (e.g. by modifying their own source code) and greater bandwidth and fidelity of communication (e.g. they can communicate at digital speeds, and using not just words but potentially by directly sending information about the computations they\u2019re carrying out).</li></ul><p>It\u2019s worth emphasising here, the main concern comes from AIs with misaligned goals acting against humanity, not from humanity misusing AIs. The latter is certainly cause for major concern, but it\u2019s a different kind of risk to the one we\u2019re talking about here.&nbsp;</p><p><strong>Summary of this section</strong></p><blockquote><p>AI researchers in general expect &gt;50% chance of AGI in the next few decades.</p><p>The&nbsp;<strong>Orthogonality Thesis</strong> states that, in principle, intelligence can be combined with more or less any final goal, and sufficiently intelligent systems do not automatically converge on human values. The&nbsp;<i>Instrumental Convergence</i> thesis states that, for most goals, there are certain instrumental goals that are very likely to help with the final goal (e.g. survival, preservation of its current goals, acquiring more resources and cognitive ability).</p><p>Inner and outer alignment are two different possible ways AIs might form goals which are misaligned with the intended goals.</p><p><strong>Outer misalignment</strong> happens when the reward function we use to train the AI doesn\u2019t exactly match the programmer\u2019s intention. In the real world, we commonly see a version of this called Goodhart\u2019s law, often phrased as&nbsp;<i>\u201cwhen a measure becomes a target, it ceases to be a good measure [because of over-optimisation for the measure, over the thing it was supposed to be a measure of]\u201d</i>.</p><p><strong>Inner misalignment</strong> is when the AI learns a different goal to the one specified by the reward function. A key analogy is with human evolution \u2013 humans were \u201ctrained\u201d on the reward function of genetic fitness, instead of learning that goal, learned a bunch of different goals like \u201ceat sugary things\u201d and \u201chave sex\u201d. A particularly worrying scenario here is deceptive alignment, when an AI learns that its goal is different from the one its programmers intended, and learns to conceal its true goal in order to avoid modification (until it is strong enough that human opposition is likely to be ineffectual).</p></blockquote><h3>Failure modes</h3><figure class=\"image image_resized\" style=\"width:60.02%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995319/mirroredImages/QzrgMhTMoLe5mEas8/xzymth7yusxnc5gumddw.png\"><figcaption><i>The earth is on fire, artificial intelligence has taken over, robots rule the world and suppress humans, digital art, artstation</i></figcaption></figure><p>But what, concretely, might an AI-related catastrophe look like?</p><p>AI catastrophe scenarios sound like something strongly out of science fiction. However, we can immediately discount a few common features of sci-fi AI takeovers. First, time travel. Second, armies of humanoid killer robots. Third, the AI acting out of hatred for humanity, or out of bearing a grudge, or because it hates our freedom, or because it has suddenly acquired \u201cconsciousness\u201d or \u201cfree will\u201d, or - as Steven Pinker&nbsp;<a href=\"https://scottaaronson.blog/?p=6524\"><u>likes to put it</u></a> - because it has developed an \u201calpha-male lust for domination\u201d.</p><p>Remember instead the key points from above about how an AI\u2019s goals might become dangerous: by achieving exactly what we tell it to do&nbsp;<i>too well</i> in a clever letter-but-not-spirit-of-the-law way, by having a goal that in most cases is the same as the goal we intend for it to have but which diverges in some cases we don\u2019t think to check for, or by having an unrelated goal but still achieving good performance on the training task because it learns that doing well on the training tasks is instrumentally good. None of these reasons have anything to do with the AI being developing megalomania let alone the philosophy of consciousness; they are instead the types of technical failures that you\u2019d expect from an optimisation process. As discussed above, we already see weaker versions of such failures in modern ML systems.</p><p>It is very uncertain which exact type of AI catastrophe we are most likely to see. We\u2019ll start by discussing the flashiest kind: an AI \u201ctakeover\u201d or \u201ccoup\u201d where some AI system finds a way to quickly and illicitly take control over a significant fraction of global power. This may sound absurd. Then again, we already have ML systems that learn to crash or hack the game-worlds they\u2019re in for their own benefit. Eventually, perhaps in the next decade, we should expect to have ML systems doing important and useful work in real-world settings. Perhaps they\u2019ll be trading stocks, or writing business reports, or managing inventories, or advising decision-makers, or even being the decision-makers. Unless either (1) there is some big surprise waiting in how scaled-up ML systems work, (2) advances in AI alignment research, or (3) a miracle, the default outcome seems to be that such systems will try to \u201chack\u201d the real world in the same way that their more primitive cousins today use clever hacks in digital worlds. Of course, the capabilities of the systems would have to advance a lot for them to be civilisational threats. However, rapid capability advancement has held for the past decade and we have solid theoretical reasons (including the scaling laws mentioned above) to expect it to continue holding. Remember also the cognitive advantages mentioned in the previous section.</p><p>As for how it proceeds, it might happen at a speed that is more digital than physical - for example, if the AI\u2019s main lever of power is hacking into digital infrastructure, it might have achieved decisive control before anyone even realises. As discussed above, whether or not the AI has access to much direct physical power seems mostly irrelevant.</p><p>Another failure mode, thought to be significantly more likely than the direct AI takeover scenario by leading AI safety researcher Paul Christiano, is one that he calls&nbsp;<a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like\"><u>\u201cgoing out with a whimper\u201d</u></a>. Look at all the metrics we currently try to steer the world with: companies try to maximise profit, politicians try to maximise votes, economists try to maximise metrics like GDP and employment. Each of these are proxies for what we want: a profitable company is one that has a lot of customers willing to pay money for their products; a popular politician has a lot of people thinking they\u2019re great; maximising GDP generally correlates with people being wealthier and happier. However, none of these metrics or incentive systems really gets to the heart of what we care about, and so it is possible (and in the real world we often observe) cases where profitable companies and popular politicians are pursuing destructive goals, or where GDP growth is not actually contributing to people\u2019s quality of life. These are all cases of Goodhart\u2019s law, as discussed above.</p><figure class=\"table\"><table><tbody><tr><td><strong>Hard-to-measure</strong></td><td><strong>Easy-to-measure</strong></td><td><strong>Consequence</strong></td></tr><tr><td>Helping me figure out what's true</td><td>Persuading me</td><td>Crafting persuasive lies</td></tr><tr><td>Preventing crime</td><td>Preventing reported crime</td><td>Suppressing complaints</td></tr><tr><td>Providing value to society</td><td>Profit</td><td>Regulatory capture, underpaying workers</td></tr></tbody></table></figure><p>What ML gives us is a very general and increasingly powerful way of developing&nbsp; a system that does well at pushing some metric upwards. A society where more and more capable ML systems are doing more and more real-world tasks will be a society that is going to get increasingly good at pushing metrics upwards. This is likely to result in visible gains in efficiency and wealth. As a result, competitive pressures will make it very hard for companies and other institutions to say no. If Acme Motors Company started performing 15% better after off-sourcing their CFO\u2019s decision-making to an AI, General Systems Inc will be very tempted to replace their CEO with an AI (or maybe the CEO will themselves start consulting an AI for more and more decisions, until their main job is interfacing with an AI).</p><p>In the long run, a significant fraction of work and decision-making may well be offloaded to AI systems, and at that point change might be very difficult. Currently our most fearsome incentive systems like capitalism and democracy still run on the backs of the constituent humans. If tomorrow all humans decided to overthrow the government, or abolish capitalism, they would succeed. But once the key decisions that perpetuate major social incentive systems are no longer made by persuadable humans, but instead automatically implemented by computer systems, change might become very difficult.</p><p>Since our metrics are flawed, the long-term outcome is likely to be less than ideal. You can try to imagine what a society run by clever AI systems trained to optimise purely for their company\u2019s profit looks like. Or a world of media giants run by AIs which spin increasingly convincing false narratives about the state of the world, designed to make us&nbsp;<i>feel&nbsp;</i>more informed rather than actually telling us the truth.</p><p>Remember also, as discussed previously, that there are solid reasons to think that influence-seeking and deceptive behaviours seem likely in sufficiently-powerful AI systems. If the ML systems that increasingly run important institutions exhibit such behaviour, then the above \u201cgoing out with a whimper\u201d scenario might acquire extra nastiness and speed. This is something Paul Christiano explores in the&nbsp;<a href=\"https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like\"><u>same article</u></a> linked above.</p><p>A popular misconception about AI risk is that the arguments for doing something are based on a tiny risk of giant catastrophe. The giant catastrophe part is correct. The miniscule risk part, as best as anyone in the field can tell, is not. As mentioned above, the average ML researcher - generally an engineering-minded person not prone to grandiose futuristic speculation - gives&nbsp;<a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/\"><u>a 5% chance of civilisation-ending disaster from AI</u></a>. The ML researchers who grapple with the safety issues as part of their job are clearly not an unbiased randomly-selected sample, but generally give numbers in the 5-50% range, and some (in our opinion too alarmist people) think it\u2019s over 90%. As the above arguments hopefully emphasise, some type of catastrophe seems like the&nbsp;<i>default outcome&nbsp;</i>from the types of AI advances that we are likely to encounter in the coming decades, and the main reason for thinking we won\u2019t is the (justifiable but uncertain) hope that someone somewhere invents solutions.</p><p>It might seem forced or cliche that AI risk scenarios so frequently end with something like \u201cand then the humans no longer have control of their future and the future is dark\u201d or even \u201cand then everyone literally dies\u201d. But consider the type of event that AGI represents and the available comparisons. The computer revolution reshaped the world in a few decades by giving us machines that can do a&nbsp;<i>narrow</i> range of intellectual tasks. The industrial revolution let us automate large parts of&nbsp;<i>manual</i> labour, and also set the world off on an unprecedented rate of economic growth and political change. The evolution of humans is plausibly the most important event in the planet\u2019s history since at least the dinosaurs died out 66 million years ago, and it took on the exact form of \u201csomething smarter than anything else on the planet appeared, and now suddenly they\u2019re firmly in charge of everything\u201d.</p><p>AI is a big deal, and we need to get it right. How we might do so is the topic for <a href=\"https://www.strataoftheworld.com/2022/09/ai-risk-intro-2-solving-problem.html\">part 2</a>.</p><p>&nbsp;</p><p><i>We give full permission for this to be crossposted to other places and shared in any way, assuming references to the original post or the original authors are included.</i></p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq3swhl89ils\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq3swhl89ils\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is obviously an oversimplification. For instance, it seems unlikely you could get an unintelligent agent with a highly complex goal, because it would seem to take some degree of intelligence to represent the goal in the first place. The key message here is that you could in theory get highly capable agents pursuing arbitrary goals.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn03hyp0cdl0is\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref03hyp0cdl0is\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Anthropomorphising evolution in language here is dangerous, and should just be seen as a shorthand here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni2akuq0z6e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi2akuq0z6e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For a more detailed discussion of this point, see&nbsp;<a href=\"https://www.openphilanthropy.org/research/new-report-on-how-much-computational-power-it-takes-to-match-the-human-brain/\"><u>Joseph Carlsmith\u2019s report</u></a> on the computational power of the human brain.</p></div></li></ol>", "user": {"username": "LRudL"}}, {"_id": "MBAGRqjcjtXGpeERw", "title": "Reflections for Introductions to Altruism", "postedAt": "2022-09-11T10:23:02.612Z", "htmlBody": "<p>I though I'd share my reflection on Effective Altruism having just completed the Introduction to EA course. &nbsp; It is commonly observed in technology start-ups that the culture of a newish organisation reflects the strengths and weaknesses of the dominant founder's personality. &nbsp;It seems this is also true of EA which &nbsp;reflects the academic philosophy origins of the organisation. &nbsp;There are many positives associated with this (deep thinking with logic and a profound acceptance of debate) but there are also negatives. &nbsp;I wanted to focus on 3 closely related points that I think EA needs to address if it is to continue grow so as to have more impact on our world.</p><ol><li>&nbsp;EA is excessively focus on direct results and tends to neglect indirect effects. &nbsp;I think this arises from the thought experiment where the focus is on a single point. &nbsp;For example the transplant challenge where killing 1 person to harvest their organs would save 5 lives hence should be supported if we are serious about consequentialist utilitarianism. &nbsp;Of course people object with the focus on rights but I challenge the basic hypothesis because of the indirect effects. &nbsp;If we lived in a society that did such things, then we would live in fear of being chosen as the sacrifice and this would do harm to a huge number of people so you could not support the principle even with extreme utilitarianism. &nbsp;Even if you said it was only volunteers who would be sacrificed then it would still do harm to many who would feel pressured to volunteer or guilty about not volunteering. &nbsp; I see this problem as fundamental to EA and needs much more thought. &nbsp;What are the indirect consequences of giving money directly to the poor if it was really adopted on a huge scale? &nbsp;We know that there are valid criticisms of free loading in European social safety net so how would this apply ? &nbsp;What level of inflation would be caused if the gifting became significant in scale which are not &nbsp;apparent in small scale test cases. &nbsp;Can such direct giving of cash ever be sustainable on a large scale given the selfishness of so many people ? &nbsp;There are innumerable cases of small cases studies which failed to scale - scaling is really a major challenge. &nbsp;Also the details of implementation can make a huge difference. &nbsp;Giving deworming pills either works or does not work entirely dependent on how it is done and the very cheap approach of just giving out the pills does not work as shown by many studies. &nbsp; This also goes the other way; if I build a factory in a poor country which is internationally competitive, train the local workforce effectively &nbsp;and pay decent wages by local standards then at least at first, the cost effectiveness from an altruistic assessment will be low. &nbsp;But by creating real long term sustainable &nbsp;jobs and a viable local economy, the long term implications just multiply and if you consider these indirect effects this could be the very best type of project for EA to support. &nbsp;Of course, the implementation is absolutely critical and to develop an internationally competitive factory in a poor country is not at all easy. &nbsp; But I still say lets us move our focus and consider the indirect effects just as much as the direct effects.</li><li>&nbsp;A specific example of the first point is investing in technology and capabilities that have huge long term potential is being under-rated in EA. &nbsp;The donation of malarial bed nets is highly rated and yet surely the elimination of malaria is a much more effective strategy and there is little doubt that this will be achieved in the next few decades. &nbsp;So EA should focus on supporting the most promising approach to eliminating malaria (probably through genetic manipulation of mosquito populations) and not the band aid solution of bed nets. &nbsp;Same with Climate Change; there are so many areas were technology will make a huge difference and investment in these areas should be our priority. &nbsp;The only solution to climate change is approaching net zero but this requires an effective carbon offset credit system which does not exist today but which could be enabled through satellite Earth Observation technology and drone monitoring technology. Without such a system, carbon offset credits are often fraudulent or just mis-guided - we need the feedback that they work. &nbsp;This is crucial to combatting climate change so this is where EA should be investing together with the numerous technologies developing alternatives to fossil fuels, concrete etc and improving farming methods such as suppressing cattle methane using sea weed in cattle feed or kelp farming in general. &nbsp;EA needs to change its perspective as to what works by recognising that our world is shaped, for good and ill, by technology and hence we must invest in the right technology as the top priority. &nbsp;Such investments will also provide a direct financial return &nbsp;that can be reinvested to enable further giving and investments. &nbsp;</li><li>&nbsp;EA needs to raise its game in terms of influencing our society and the academic approach is a turn-off to many, particularly the excessive focus on AI risk. Other than technology, the main influence on our world is decisions made by governments and leaders and we need to start working to improve our governmental systems. &nbsp;In western democracies this means influencing the masses so they stop voting for populist idiots and idiocies. In poor countries, it means effectively tackling corruption (and the money laundering enabled by rich countries) and putting limits on dictators. &nbsp;This is an area that is very difficult with no easy solutions but we need to generate a system that will be viewed sympathetically which means it must be diverse (not just race and gender but wealth, power and lived experience) and it must be approachable and not overly elitist, intellectual and theoretical. &nbsp;Perhaps we need a separate EA splinter to enable the core EA thinking, which is alienating to many, while also developing an &nbsp;effective influencing network. &nbsp;It is clear we need to plan for long term gradual growth by converting people to our cause as has been achieved many times in the past by religions and political movements. &nbsp;We need EA to become &nbsp;as powerful in the 21st century as communism was in the 20th century and we need to continually learn and adapt as we go. &nbsp;I'd suggest this is the path to EA doing the greatest good.</li></ol>", "user": {"username": "Philip Windred"}}, {"_id": "qBAbiuiWJvhb9uZPT", "title": "Translating EA Online Content: Motivation and Learnings from a German Project", "postedAt": "2022-09-11T08:52:24.612Z", "htmlBody": "<h2>TL;DR</h2><p>I think translations are valuable because they improve public perception of EA in the local context, make community building easier, and potentially make people discover EA by chance more often. I've run a project for translating key longtermist content into German for three months about ten hours per week with both translation services and an EA translator. I recommend experimenting with different translators at the start of the project. Proofreading is the most time-consuming work that can only be done EA-internally. I think publishing content well is extremely important but have not done so yet.</p><p>If you are interested in running a similar project, please get in touch with me or <a href=\"https://forum.effectivealtruism.org/posts/CqYHPLCJaFdLn8gp6/open-phil-is-seeking-bilingual-people-to-help-translate-ea\">apply for funding at Open Philanthropy</a>.</p><h2>Epistemic status</h2><p>I don't have any formal background in translations. While I've spent the last four months thinking about this topic on the side, it's possible that there is some aspect I've so far overlooked and I encourage others to investigate the topic more thoroughly. I also expect this to differ considerably across languages.</p><h2>The general motivation for translating EA content</h2><p>I think there are three potential theories of change for translating content:</p><h3>1 Shaping the public perception of EA in the local context</h3><p>By public perception, I mean reports about EA in popular media which in turn influence the general attitude towards EA topics e.g. in academic circles or politics. Having high-quality content in the native language decreases the risk of negative or low-fidelity press coverage and provides a reference when EA topics are discussed.</p><p>Currently, upon searching for \u201cLongtermism\u201d on the German web, there is no EA-related resource explaining the concept. Instead, you come across two articles in major German newspapers that portray the idea as far from reality and potentially dangerous. I have heard similar reports from other languages. Having high-quality texts about longtermism available won\u2019t be a guarantee for positive media coverage but I think clarifying misconceptions by translating a good introductory text and&nbsp;<a href=\"https://longtermism.com/faq\"><u>longtermism.com\u2019s FAQ</u></a> will make it much more likely.</p><h3>2 Having more promising people discover EA content independently</h3><p>Since many people usually browse the web in their native language, it is unlikely that they will come across EA content in English. Providing content about important EA topics such as biosecurity, nuclear risk or AI safety makes it possible to discover EA through an interest in these topics.</p><p>I remain uncertain about how impactful this is because:</p><ol><li>The most impactful EA work requires strong English skills.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefljp34nwivw\"><sup><a href=\"#fnljp34nwivw\">[1]</a></sup></span>&nbsp;Those individuals who possess these skills are more likely to browse the web in English. I don\u2019t have any data here to see how much Germans (or others) browse the web in their native language vs English.&nbsp;</li><li>A brief investigation into typical search volumes in German showed that \u201cdoing good with your career\u201d or \u201cmost important problems\u201d hardly ever happen. I did not check this with different prompts such as \u201cglobal pandemic\u201d or \u201cAI risk\u201d which could be important. I expect to get more data on this once the translated texts are published and I can look at website traffic.</li></ol><h3>3 Making community building easier</h3><p>Having good EA content at hand makes it a lot easier to reach out to people, introduce them to EA and facilitate discussions.</p><p>I think it\u2019s likely that people engage more with content in their native language because:</p><ul><li>People have&nbsp;<a href=\"https://www.cambridge.org/core/journals/applied-psycholinguistics/article/abs/second-language-reading-in-fluent-bilinguals/64401E6774F15D1C950A6E45D7FC29F4\"><u>higher reading speeds</u></a> in their native language, which means it takes less time and effort to read texts. It also seems likely that comprehension is higher in your native language although I did not find any studies on this.</li><li>The content is perceived as less foreign/ more approachable, especially if examples are adapted to the local context</li><li>People with lesser English skills can participate and engage. I remain uncertain how important this is in the German context as most educated individuals speak English fairly well. However, this looks very different in other countries such as France or Japan.</li></ul><p>Overall, I\u2019m most enthusiastic about 1. while I believe that 2. and 3. need more empirical evidence to determine how promising this is.</p><h2>The specific motivation for my project</h2><p>Despite currently being <a href=\"https://forum.effectivealtruism.org/s/YLudF7wvkjALvAgni/p/zMKxgK4wbSywnkFrn#Global_Distribution_of_EAs\">the biggest non-English EA community</a>, very few resources exist in German. While effective giving is covered to some extent, no longtermism-related content exists. I believe that longtermism is one of the most important ideas developed by EA and that it deserves much more attention than it is currently receiving. Additionally, current media coverage of longtermism is mostly negative and it seems important to establish a pool of reliable, high-fidelity texts for the broader German public.</p><p>I initially planned this project as an experiment to discover how well translations work because I had a bigger project in mind. See the section \u201c<i>Would an international organization to translate content into many languages be high-impact?\u201d</i></p><h2>The texts I translated and my reasoning behind it</h2><p>To provide a good introduction to longtermism I translated the following pieces:&nbsp;</p><ul><li>An&nbsp;<a href=\"https://longtermism.com/introduction\"><u>introduction</u></a> to longtermism from longtermism.com</li><li>The&nbsp;<a href=\"https://ourworldindata.org/longtermism\"><u>Our World in Data article</u></a> on longtermism which describes how vast our future could be</li><li>The&nbsp;<a href=\"https://longtermism.com/faq\"><u>FAQ</u></a> from longtermism.org<ul><li>Hints at common misconceptions, quite important for the media</li></ul></li><li>There already exists an article on&nbsp;<a href=\"https://futureoflife.org/background/existential-risk/\"><u>x-risk</u></a> by FLI that is also translated to&nbsp;<a href=\"https://futureoflife.org/background/existential-risk-german/\"><u>German</u></a> (though I prefer the term \u201cexistentielle Gefahr\u201d)</li></ul><p>I then moved on to introduce the most important longtermist cause areas. For most of them, I found the 80k article to be the most thorough introduction.</p><ul><li>80k\u2019s article on&nbsp;<a href=\"https://80000hours.org/problem-profiles/global-priorities-research/\"><u>global priorities research</u></a></li><li>80k\u2019s new article on&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\"><u>AI risk</u></a>&nbsp;</li><li>80k\u2019s article on&nbsp;<a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/full-report/\"><u>GCBR</u></a> + additional information on the best GCBR careers in Germany and the EU from German biosecurity EAs <a href=\"https://forum.effectivealtruism.org/posts/k5jDuH2uyB65Fkcro/careers-concerning-global-catastrophic-biological-risks\">that has also been posted here</a>.</li><li>80k\u2019s article on&nbsp;<a href=\"https://80000hours.org/problem-profiles/nuclear-security/\"><u>nuclear risk</u></a>&nbsp;</li><li><a href=\"https://www.givingwhatwecan.org/cause-areas/long-term-future/climate-change\"><u>Climate change</u></a>, a report by GWWC&nbsp;<ul><li>Here, I decided against 80k as I think their climate article is more likely to scare people off than to attract them to EA (though I think the content is true). This is mainly because it argues that climate change is less important compared to other longtermist causes and that it\u2019s likely not an x-risk.</li></ul></li></ul><p>As an introduction to why you should think more about your career:</p><ul><li><a href=\"https://80000hours.org/make-a-difference-with-your-career/#no-time-to-read-right-now\"><u>80k: most important decision</u></a>&nbsp;</li></ul><p>I thought about translating some EA fundamentals as well and ended up with:</p><ul><li><a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism\"><u>CEA\u2019s introduction to EA</u></a></li><li><a href=\"https://www.lesswrong.com/posts/2ftJ38y9SRBCBsCzy/scope-insensitivity\"><u>Scope insensitivity</u></a>&nbsp;</li><li><a href=\"https://www.effectivealtruism.org/articles/moral-progress-and-cause-x\"><u>Cause X</u></a>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/jo7tM4s5ApPgv2DPC/expected-value-theory\"><u>Expected Value</u></a>&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology\"><u>EA is a Question (not an ideology)</u></a>&nbsp;</li></ul><p>My conclusion here was that it\u2019s probably better to write original German texts about EA fundamentals and I encourage anyone with a solid EA philosophy education to do so. I also think it is valuable to have more native-language content in general, as original texts can incorporate the nuances of a culture much better than translations.</p><p>I also got some articles from the Most Important Century series translated but have not managed to proofread all of them:</p><p>Proofread:</p><ul><li><a href=\"https://www.cold-takes.com/most-important-century/\"><u>Most important century: Summary</u></a>&nbsp;</li><li><a href=\"https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/\"><u>All possible futures are wild</u></a>&nbsp;</li></ul><p>Not proofread: (Reach out if you have the capacity to do this)</p><ul><li><a href=\"https://www.cold-takes.com/this-cant-go-on\"><u>This Can't Go On</u></a></li><li><a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\"><u>Forecasting Transformative AI, Part 1: What Kind of AI?</u></a></li><li><a href=\"https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/\"><u>Why AI Alignment Could Be Hard With Modern Deep Learning</u></a></li><li><a href=\"https://www.cold-takes.com/forecasting-transformative-ai-whats-the-burden-of-proof/\"><u>Forecasting Transformative AI: What's The Burden Of Proof?</u></a></li><li><a href=\"https://www.cold-takes.com/are-we-trending-toward-transformative-ai-how-would-we-know/\"><u>Forecasting Transformative AI: Are We \"Trending Toward\" Transformative AI?</u></a></li><li><a href=\"https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/\"><u>Forecasting Transformative AI: The \"Biological Anchors\" Method In A Nutshell</u></a></li><li><a href=\"https://www.cold-takes.com/where-ai-forecasting-stands-today/\"><u>AI Timelines: Where The Arguments, And The \"Experts,\" Stand</u></a></li><li><a href=\"https://www.cold-takes.com/making-the-best-of-the-most-important-century/\"><u>How To Make The Best Of The Most Important Century?</u></a>&nbsp;</li></ul><h3>My approach</h3><ol><li>I wrote down my reasoning behind the project and why I thought I would be a good fit. I then reached out to several funding sources and was funded by Open Philanthropy. They are now&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CqYHPLCJaFdLn8gp6/open-phil-is-seeking-bilingual-people-to-help-translate-ea\"><u>openly looking</u></a> for more people interested in translations. If you are unsure if your project is valuable, reach out and we can discuss it.</li><li>I identified the texts I thought to be most important (see my reasoning above) and got feedback from others in the German community. I then asked the authors of the texts for permission to translate them.</li><li>I held <a href=\"https://forum.effectivealtruism.org/posts/iLbXBRwoc2ytpyqgq/contest-250eur-for-translation-of-longtermism-to-german\">a small competition</a> to find good German translations of the term \u201clongtermism\u201d and conducted <a href=\"https://forum.effectivealtruism.org/posts/35D5NxW8uzDQkHWH8/germans-opinions-on-translations-of-longtermism-survey\">a survey</a> on MTurk to evaluate which term was preferred.</li><li>I had already looked for potential translators and met an EA who had a background in translations that was interested in working on the project. Since they had limited capacity I decided to also use professional translation services. I contracted several services and evaluated them. Quality differed considerably and I finally used \u201cIbidem\u201d for most of the texts.</li><li>After the initial translation, I proofread all the texts (both from the EA translator as well as from the service). I had to spend a significant amount of time improving the texts, mostly to make the style appropriate.</li></ol><p>The texts are not yet published and I thus don\u2019t have any experience to share here. However, I think this is a crucial step and should ideally be done by someone with knowledge of SEO and web design as well as a good understanding of the topics. (If this could be you and you speak German, please get in touch with me.)</p><h2>My learnings</h2><h3>Can you just use AI translations?</h3><p>I ran a small test where I compared a machine translation with one that was done with the AI + the translator and the latter one was easily recognizable and much better. I thus think having a translator involved reduces the time needed for proofreading significantly and strongly advise using trained translators. However, this might vary from one language to another.</p><h3>Use of EA\u2019s with a background in translation vs professional translation services</h3><p>I think texts about scientific topics should ideally be translated (or at least proofread) by someone with a background in the topic. I noticed this most strongly when translating 80k\u2019s article on&nbsp;<a href=\"https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/full-report/\"><u>GCBR</u></a> that contains many scientific phrases. Since my translator did not have a background in life sciences, I had to change most parts of the text significantly. It might thus be useful to rely on translation services that have specialized in scientific texts.<br>Texts that talk mainly about EA ideas likewise benefit from translators that are familiar with the ideas. However, I think that professional translators are able to understand and convey these ideas well in most cases.</p><p>I think my main takeaway is that it\u2019s worth experimenting a lot at the beginning of the project, i.e. test different services and have translators do a small test translation. This somewhat supports&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CqYHPLCJaFdLn8gp6/open-phil-is-seeking-bilingual-people-to-help-translate-ea#How_do_EAs_compare_to_professional_translators_on_the_quality_of_the_products_they_produce_\"><u>Eli Rose\u2019s view</u></a> that the difference between EA-adjacent translators and translation services is small.</p><p><strong>In either case, I strongly suggest either paying professional EA-adjacent translators or using translation services instead of having volunteers translate material, as this requires much less managing, is more reliable and the final output is of higher quality.</strong></p><p>If you would like to pay for professional services,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CqYHPLCJaFdLn8gp6/open-phil-is-seeking-bilingual-people-to-help-translate-ea\"><u>I recommend applying to Open Phil.</u></a></p><h3>Would an international organization translating content into many languages be high-impact?</h3><p>Earlier this year&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MBDHjwDvhDnqisyW2/awards-for-the-future-fund-s-project-ideas-competition#Konstantin_Pilz___EA_content_translation_service\"><u>I proposed the following idea</u></a> for the FTX Future Fund Ideas Competition:</p><p><i>EA-related texts are often using academic language needed to convey complex concepts. For non-native speakers reading and understanding those texts takes a lot more time than reading about the same topic in their native language would. Furthermore, today many educated people in important positions, especially in non-western countries, do not speak or only poorly speak English. (This is likely part of the reason that EA currently mainly exists in English-speaking countries and almost exclusively consists of people speaking English well.)</i></p><p><i><strong>To make EA widely known and easy to understand there needs to be a translation service enabling e.g. 80k, important Forum posts or the Precipice to be read in different languages.</strong> This would not only make EA easier to understand - and thus spread ideas further - but also likely increase epistemic diversity of the community by making EA more international.</i></p><p>The idea was one of the winning entries, however,&nbsp;<a href=\"https://ftxfuturefund.org/projects/translating-key-content-into-spanish-mandarin-and-other-languages/\"><u>on their website</u></a> they published only parts of the idea, focusing mostly on translations in general and not claiming a centralized organization would be valuable.</p><p>I used the German project as an experiment to get information on whether such an organization to translate texts into several languages would be valuable. Here are some takeaways:</p><ul><li>Most of the work required to produce good translations has to be done by native speakers:<ul><li>Finding good translators</li><li>(Translations themselves - though you can easily use professional services to outsource this)</li><li>Proof-reading the texts</li><li><strong>Publishing the texts well</strong></li></ul></li><li>Some work can be done centrally:<ul><li>Asking the authors for permission to translate the texts</li><li>Applying for funding</li></ul></li><li>Work that can be done centrally, though would be preferable to be done from within the local context:<ul><li>Choosing which texts to translate</li><li>Communicating with others in the local context to get feedback and develop a good strategy</li><li>Coordinating translators and proof-readers</li></ul></li></ul><p>Since most of the work is best done from within the local context, and a central organization seems to have more of the downsides I list in the next section, I am currently less enthusiastic about a more centralized organization. However, I think some centralized communication would be valuable:</p><ol><li>Compiling a list of texts that are valuable to be translated (<a href=\"https://forum.effectivealtruism.org/posts/CqYHPLCJaFdLn8gp6/open-phil-is-seeking-bilingual-people-to-help-translate-ea#What_content_is_Open_Phil_interested_in_having_translated_\"><u>See Open Phil\u2019s recommendations</u></a> or mine above)</li><li>Compiling a list of already existing translations (There is one&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1F_ioM0H0KRRvcl0cf4QBuuKDI7AOVc40LEj9OfZMUsM/edit#gid=0\"><u>here</u></a>)</li><li>Having an overview of ongoing projects so people can cooperate</li><li>Getting author permissions to translate texts</li></ol><p>There\u2019s a #non-english-speaking-groups channel on the EA Groups Slack, where discussions on translation often take place. Laura Gonz\u00e1lez has been organizing some coordination efforts there and is working on improving the current permissions system.</p><h2>Downsides</h2><p>I think there are some risks with translating content, though most of them can be avoided by acting carefully.</p><h3>Lock-in &amp; loss of the opportunity to experiment</h3><p>Approaches</p><p>I think&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Z95TxtkjHGPq4TAqY/why-not-to-rush-to-translate-effective-altruism-into-other\"><u>Ben Todd\u2019s article</u></a> on the topic is still worth reading, although being written back in 2018 and focusing mostly on China.</p><p>Translating content means you transfer ideas from one cultural and academic context to another. If counterfactually similar ideas would have been developed from inside the country this may lead to a loss of novel perspectives on EA topics.</p><p>It also means that you copy the framings and approaches to EA, which has an opportunity cost, as other framings and approaches can not be explored. This is especially important for places where EA is still in a fledgling state.</p><p>I thus strongly recommend thinking about experimenting with new presentations of EA before you commit to translating already existing content.</p><p>Words &amp; Phrases</p><p>The first translation of a key idea may lead to very long-lasting downstream effects. I think this should be done carefully and with a lot of feedback from other EAs in the community. I think it\u2019s worth reviewing relevant literature in the target language and using existing phrases. For the German project, I held a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/iLbXBRwoc2ytpyqgq/contest-250eur-for-translation-of-longtermism-to-german\"><u>small competition</u></a> for the best translations of the word \u201clongtermism\u201d and conducted a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/35D5NxW8uzDQkHWH8/germans-opinions-on-translations-of-longtermism-survey\"><u>small survey</u></a> to get an insight into the German public\u2019s preference.</p><p>Laura Gonz\u00e1lez shared&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1Z4j-TZpjakEKisVZ3rtrBaWob1ghcVqH-Ijw0M589eU/edit#gid=0\"><u>this glossary</u></a> of important EA-related words and phrases. I personally think you don\u2019t have to come up with a good translation of every one of these words (and for many, a translation likely already exists), thinking about the top 25 EA words such as \u201clongtermism\u201d, \u201ccause neutrality\u201d or \u201cAI alignment\u201d is probably sufficient.</p><h3>Being perceived as foreign</h3><p>If translated content is not adapted to the cultural context well, it can easily be perceived as foreign and is likely to be opposed. I think this can be mitigated by exchanging examples, using more familiar framings, and connecting to ideas from the local context.</p><p>However, I think that introductory material is best written from scratch in the target language, especially in non-western contexts, as the audience\u2019s demographics are likely very different. In general, I think it makes sense to favor original content over translated content whenever possible, with the exception of more scientific texts such as 80k\u2019s cause profiles.</p><h3>Attracting attention</h3><p>While this is to some extent desired, it can also be a problem. If popular media becomes interested in EA, it is important to have people the media can reach out to. It helps to be well connected to and communicate with other EAs in your language. Ideally, you would develop a plan for what to do once you attract more attention.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnljp34nwivw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefljp34nwivw\">^</a></strong></sup></span><ol><li>It's much harder to update your worldview via discussions with other EAs if you're not fluent in English. This seems crucial to find a high-impact career.</li><li>Most EA causes rely on academic and scientific texts that require at least decent English skills to comprehend. Almost all the intellectual output of EA is in English and so are the discussions of the topics.</li><li>EA organizations operate almost exclusively in English, they are also mostly situated in English-speaking countries. High-impact jobs outside of EA orgs often also rely heavily on English, e.g. campaigning, government jobs, and consulting.</li><li>Movement building seems to be one exception, though I'd like to argue that a good connection to the international community &amp; keeping up to date with ongoing discussions seem very important and require at least decent English skills.</li></ol></li></ol>", "user": {"username": "Konstantin Pilz"}}, {"_id": "k3mTQQ6SWoe92zCa5", "title": "The Multiple Butterfly Effect", "postedAt": "2022-09-11T04:19:40.281Z", "htmlBody": "<p>I recently discovered a web site called \u201cMaking a Huge difference\u201d, that\u2019s proclaimed mission is to make the world a better place. It sounded great, a group of like minded people that I could team up with and really make a difference to this absurd world.</p><p>As Usual, reality was disappointing.</p><p>What I found was lots of posts by academic &nbsp;Don Quixote types, with titles that I didn\u2019t really understand or desire to read. A few were about the sites problems and one gem about exclusion, but most were vanity essays on trivial concerns. The main objective of most essays was not really about making the world a better place, but trying to show how clever the author was and rack up as many positive comments as possible. The impression I got (based on limited time browsing, so could be completely cynical and wrong) was that the site had been set up by someone wanting to sell a book or two and promote educational courses, but it had ended up totally out of control as every one pitched in &nbsp;a totally unstructured manner..</p><p>I posted a few of my thoughts, received the expected ticking off and decided to leave them to it for a couple of weeks, to see what develops.</p><p>Why was I disappointed. My view is that, Ideally, a forum should be a criticism free space where anyone can come up with an idea, however wacky, and then it gets debated, examined, thrashed about and developed, to see if it is good idea. If it gets through this phase it should be put on a list of items for implementation, in priority order (not ease of achievability). My basic priority list would be, but yours may differ:-</p><p>1) Reducing fossil fuel usage<br>2) Preventing wars&nbsp;<br>3) Promoting human rights<br>4) Promoting good Governance</p><p>Once &nbsp;priories have been set, then they could be put into sub groups for a plan of action to be developed, and once this is completed, the actions could be implemented, by all of the web sites users. Thus all users contribute, develop a plan, action and share the successes. Much happiness ensues.</p><p>As \u201cMaking a Hugh Difference\u201d didn\u2019t seem to have any chance of achieving anything useful, except selling a few books, I have decided to continue to pester all and sundry with my correspondence and keep posting most on my blog under my own name. &nbsp;I use my own name because before I post or submit anything, I ask myself, what is its purpose? would I say this to their face? &nbsp;and will I be proud of what I have written in a few weeks time? Hopefully this cuts out me added more nonsense, that is all too common on the internet.</p><p>Whilst I fully expect my efforts have little impact on a world of 7 billion people, most of whom are also tapping away into their computers, I put my faith in the butterfly effect, that a single butterfly may one day create a great storm. The probability is extremely low, but Chaos theory says the probability isn\u2019t zero, Which It would be if I did nothing, so I keep on, keeping on. It gives meaning to my pathetic insignificant little life!</p><p>However, what would happen if our single butterfly, flapped its wings in unison with another? The chance would double. And if it flapped in unison with 10 others, 1000 others, 7 billion others\u2026\u2026 wouldn\u2018t the storm be much more likely to occur?. The more butterflies the better the odds, and then it\u2019s time to place your bets for real change.</p><p>Percy Shelley. wrote the following in The Masque of Anarchy in 1819 (I stress I\u2019m not an anarchist, we still need competent, selfless leaders, so the rest of us can enjoy ourselves).</p><p>Rise, like lions after slumber,<br>In unvanquishable number,<br>Shake your chains to earth, like dew,<br>Which in sleep hath fallen upon you,<br>Ye are many - they are few.</p><p>No one would publish this or his manifesto, it was too dangerous to the powers that be to print. But if Shelley were alive today, or there was someone of his equal alive, she or he could self publish on the internet and soon his many would be working together against the few. Butterflies, all flapping their wings in unison, not to create chaos or a storm, but to make the world a better place. Wouldn\u2019t that be effective?.</p><p>Where are the modern day Shelley\u2019s? Like Diogenes with his lamp, I\u2019m still searching.</p><p><br><br>&nbsp;</p>", "user": {"username": "Trev Prew"}}, {"_id": "P7DFEJbrRsrFxHNBw", "title": "What are the biggest EA Questions in American Politics?", "postedAt": "2022-09-11T03:59:23.958Z", "htmlBody": "<p>Hi folks, I hope all is well. This coming Saturday (September 17th, 2022), I will be on a roundtable discussing <a href=\"https://tinyurl.com/y7drwtnb\">Effective Altruism and Political Science</a> at the 2022 American Political Science Association (APSA) Meeting. One of the things I will be discussing is how research in American politics and in EA can be mutually beneficial to each other. I have my own thoughts about this question, but I'm sure that like everyone, I have my blind spots.&nbsp;</p><p><strong>So the questions I ask folks here are, what American politics research questions are most important in EA, and what EA research questions are most important in American politics? I look forward to your comments to reduce my blind spots.&nbsp;</strong></p><p>If you are interested in learning more about Research in Effective Altruism and Political Science (<a href=\"https://www.reaps.info\">REAPS</a>), I recommend you visit its <a href=\"https://www.reaps.info\">website</a>.</p>", "user": {"username": "Mahendra Prasad"}}, {"_id": "apAfqZTGwp6JkxPAy", "title": "Most harmful people in history?", "postedAt": "2022-09-11T03:04:46.937Z", "htmlBody": "<p>Who was someone who did a lot of harm?</p>\n<p>What was the harm they did, according to you? Was it the creation of direct suffering, or altering the course of history?</p>\n<p>For bonus points:</p>\n<ul>\n<li>What were the causes/reasons this person did so much harm?</li>\n<li>What was the obvious counterfactual to this harm? What was the ideal counterfactual?</li>\n<li>What could an ambitious, altruistic, and talented person at the time realistically have done to mitigate this harm? More hypothetically, what could an EA-like community have done?</li>\n</ul>\n<p>(If this question does well, I'd like to follow it up with a question about current harm, with the obvious warning regarding information hazards)</p>\n", "user": {"username": "SiebeRozendal"}}, {"_id": "frikkgunKP7oZxLLo", "title": "Ascended Civillians Movement", "postedAt": "2022-09-11T13:39:14.298Z", "htmlBody": "<p>I want your help for me to create an organization and a social movement by the titular name\nThat will maximize the good and empowerment of the common people. I believe it is within every individual to unite and optimize the good they do for humanity, to overcome cynicism and make their dreams of systemic and individual change possible. To make dreams of decreasing poverty and eliminating war and climate change reality. All it takes is for everyone to take responsibility for their actions. I want everyone who can to follow EAs teachings, stop buying slave made products, and to discover any other causes of possible solution. I want your help to spread the ideology \"Your actions have consequences, make them good.\", And \"To never keep your head low, fearing you may lose your comfortable, easy, meaningless life for nothing by hand of the elites.\" Whether it be through advertisements, a nonprofit, or both, or  other ideas.</p>\n<p>Our central ideology is that every person must take responsibility for their contributions\nTo unjust systems, spend their time wisely on good causes, and will be empowered to make worldwide and systemic change. I already have a forum, and I would greatly appreciate any other suggestions or help getting people to follow this idea and the ideas of EA.</p>\n", "user": {"username": "Good Evening Fellas"}}, {"_id": "icFeXbGWGhTPfZbbQ", "title": "Aversion to Happiness Across Cultures: A Review of Where and Why People are Averse to Happiness", "postedAt": "2022-09-11T01:11:38.314Z", "htmlBody": "<h3>Context</h3><p><i>This is a paper I just randomly found again in my saved reading list that seems relevant to the EA Forum.</i></p><p><br><strong>It talks about the concept of '</strong><i><strong>aversion to happiness</strong></i><strong>' which I have never heard of before in EA circles. This paper might thus be interesting to EAs who consider some form of happiness/good qualia intrinsically valuable and certain measures of happiness as their core unit of intrinsic value (of which there are many in my experience):</strong></p><h3>Abstract</h3><p>\"A common view in contemporary Western culture is that personal happiness is one of the most important values in life. For example, in American culture it is believed that failing to appear happy is cause for concern. These cultural notions are also echoed in contemporary Western psychology (including positive psychology and much of the research on subjective well-being). However, some important (often culturally-based) facts about happiness have tended to be overlooked in the psychological research on the topic. <strong>One of these cultural phenomena is that, for some individuals, happiness is not a supreme value. In fact, some individuals across cultures are averse to various kinds of happiness for several different reasons</strong>. This article presents the first review of the concept of aversion to happiness. Implications of the outcomes are discussed, as are directions for further research.\"</p><h3>Rough Review and Summary</h3><p>I have skimmed some of the paper, here are my thoughts:</p><p>A lot of it focuses on the effects of outward expression of happiness on other people's happiness or the fact that embracing happiness might lead to more subsequent suffering, which is still compatible with happiness being a core unit of value/morality. I thus do not think these claims are very relevant to EA.</p><p>However, the paper also claims that <strong>in some cultures, individuals do not consider being happy as morally desirable or even consider being happy as bad </strong>- which would be incompatible with EA notions of considering happiness intrinsically valuable:</p><p>\"People aren\u2019t just averse to happiness because it might lead to [subsequent unhappiness], however; some individuals and some cultures tend to believe that happiness is worthy of aversion because being happy can make someone a worse person (both morally and otherwise). Again, we found evidence for this belief in both non-Western and Western cultures. First we discuss beliefs that happiness is worthy of aversion because it can make someone a morally worse person, and then we discuss beliefs that happiness is worthy of aversion because it can make someone less creative.\"</p><p>It gives a few examples of that and later goes on to summarize and conclude:</p><p>\"<strong>It should be noted that [this paper] casts little doubt on the intrinsic value of most kinds of happiness</strong>. Indeed, while happiness and the pursuit of certain kinds of happiness are widely believed to have negative effects for some people in some cases, happiness is, in and of itself, still a positive experience for most people and according to most of the common conceptions of happiness.</p><p>[...]</p><p><strong>Nevertheless, it should not be in doubt that many individuals and cultures do tend to be averse to some forms of happiness, especially when taken to the extreme, for many different reasons.</strong></p><p>[...]</p><p><strong>These considerations show that equating happiness with </strong><i><strong>the</strong></i><strong> supreme universal good is dangerous unless each culture (or individual!) were to create and be assessed by its own definition of happiness</strong>.\"</p><p>It also makes a few interesting points about international comparisons of happiness across nations being potentially flawed due to response bias caused by aversion to happiness.</p>", "user": {"username": "timfarkas"}}, {"_id": "acyfmFTN3cNgwnYw6", "title": "\"Agency\" needs nuance", "postedAt": "2022-09-12T16:52:16.134Z", "htmlBody": "<h1>Summary</h1><ul><li>Recently, \u201cagency\u201d has become heavily encouraged and socially rewarded within EA, and I am concerned about the unintended consequences.</li><li>The un-nuanced encouragement of \u201csocial agency\u201d could be harmful for the community; it encourages violating social norms and social dominance. At the extreme, it can feel parasitic, with dominant individuals monopolising resources.</li><li>\u201cAgency\u201d being a high status buzzword incentivises&nbsp;<a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\"><u>Goodharting</u></a>\u2013 where big and bold actions are encouraged (at the expense of actions that actually achieve an individual's goals).</li><li>I have an impression that, sometimes, people say&nbsp;<i>\u201cagency\u201d&nbsp;</i>and others hear&nbsp;<i>\u201cwork more hours; be more ambitious; make bolder moves! Why haven\u2019t you started a project already?!\u201d</i> \u201cAgency\u201d doesn\u2019t entail \u201chustling hard\u201d \u2013 it entails acting intentionally, and these are importantly different. Agency is not a tradeoff against rest; in fact, doing things to high standards and achieving your goals often requires tons of <a href=\"https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\">slack</a>.</li><li>My experience and mistakes<ul><li>If you feel uncomfortable when violating a social norm or taking a bold move, take this feeling into consideration, there is probably a reason for this feeling.</li><li>It\u2019s not always low cost for others to say no to requests; failing to model this can make others uncomfortable and lead to a relative overconsumption of resources.</li><li>Making authorities upset with you is (emotionally and instrumentally) costly. Don\u2019t ignore this cost when taking big and bold moves.</li><li>Being too willing to ask for help made me worse at solo problem-solving.&nbsp;</li></ul></li></ul><h1>Introduction</h1><p>Recently, \u201cagency\u201d has been heavily encouraged within EA. I often feel like it\u2019s become both a status symbol and an unquestionable good.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjoo2s4m55m\"><sup><a href=\"#fnjoo2s4m55m\">[1]</a></sup></span></p><p>I wrote a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Pc3CFbYxPXgyjoDpB/seven-ways-to-become-unstoppably-agentic\"><u>post on agency</u></a> a few months ago, which I now think missed important caveats. While I broadly think that the world (and EA) could do with an injection of proactiveness and intentionality, I\u2019m concerned about some of the unintended consequences.</p><p>In this post, I lay out some concerns, some unexpected costs of taking agentic actions for me (and mistakes I\u2019ve made), and things I\u2019ve changed my mind on since&nbsp;my last post.</p><h3>Important caveats</h3><ol><li>Most people don\u2019t ask for help enough, and this post is applicable to a minority of people;&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl253hcr4vh\"><sup><a href=\"#fnl253hcr4vh\">[2]</a></sup></span></li><li>This post is many layers of abstraction away from object-level issues that improve the world.<strong> I\u2019m worried about </strong><a href=\"https://forum.effectivealtruism.org/posts/25LDrpZqaBQso8zoS/evie-cottrell-s-shortform?commentId=jwqhNdFzoibmgagDJ\"><strong>meta-conversations</strong></a><strong> (like this) taking away attention from more important and relevant topics.&nbsp;If you\u2019ve read the summary, I\u2019m not sure how much benefit you\u2019ll get from the rest of the post. </strong>Consider not reading it.</li></ol><h1>\u201cSocial Agency\u201d in moderation</h1><p>I\u2019m going to define \u201csocial agency\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrwpn1g9mzi\"><sup><a href=\"#fnrwpn1g9mzi\">[3]</a></sup></span>&nbsp;as&nbsp;<strong>a willingness to make bold social moves, leverage social capital, and make trades within your social network to achieve your goals.</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqx61gqvgqr\"><sup><a href=\"#fnqx61gqvgqr\">[4]</a></sup></span></p><p>My previous post mostly focused on encouraging social agency \u2013 \u201cask people for help,\u201d \u201cnetwork online,\u201d and \u201cdon\u2019t be too constrained by social boundaries,\u201d are some key themes. I am now concerned that this lacked nuance.</p><p>Social agency is a small part of doing good. Having a network and mentors and friends-in-high-places is not enough to actually do meaningful work in the world. The other part of agency is about&nbsp;<strong>Actually Doing Things</strong>: the nitty-gritty, engagement with reality that actually makes things happen. Taking&nbsp;<a href=\"https://www.lesswrong.com/tag/heroic-responsibility\"><u>heroic responsibility</u></a>; learning about the technical, object level details of important problems; intentionally building models of the world; noticing gaps in existing strategies; developing a plan to solve bottlenecks; executing those plans and&nbsp;<a href=\"https://mindingourway.com/stop-trying-to-try-and-try/\"><u>actually trying</u></a>; developing feedback loops; rapidly iterating your existing plans.</p><p>The un-nuanced encouragement of social agency could be harmful for the community; it encourages individuals to violate social norms to achieve their goals, be socially dominant, and not constrain their use of finite resources (e.g. the time and attention of senior people). Taken to its extreme, agency can feel parasitic, with grasping, \u201cagentic\u201d individuals monopolising resources at the expense of others. I worry that operating with high \u201csocial agency\u201d pattern-matches onto climbing a status ladder, with little regard for the consequences on others.</p><p>Don\u2019t get me wrong \u2013 social agency is important and often necessary for achieving your goals. And the flip side is too much \u201cActually Doing Things,\u201d where a lone wolf fails to benefit from cooperation, mentorship, and feedback.</p><h1>Goodharting agency</h1><p>\"Agency\" simply refers to the ability to achieve your goals, which is always instrumentally useful, but low-resolution versions can be harmful.</p><p>I claim that the following traits are being rewarded due to the \u201cagency is high status and always good\u201d meme:</p><ul><li>Willingness to violate social norms to achieve your goals;</li><li>Being a confident go-getter;</li><li>Having a low bar for making requests of others (and assuming that others will say no to your requests if they want to);</li><li>Willingness to be socially dominant and \u201ctake up space\u201d;</li><li>Not attempting to limit your consumption of finite resources (e.g. in-demand person\u2019s time; the attention of a teacher in a class);</li><li>A \u201chustle hard\u201d attitude.</li></ul><p>Most of these are helpful in moderation. But in a community where \u201cagency\u201d becomes an increasingly virtuous trait (and&nbsp;<i>\u201cyou\u2019re so agentic!!\u201d</i> is highly desired praise), we may see a lot of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\"><u>Goodharting</u></a>.</p><p>I\u2019m concerned that this meme incentivises big and bold actions \u2013 that pattern-match to \u201cbeing agentic\u201d \u2013 instead of the actions that actually achieve your goal.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2gfgtverpn2\"><sup><a href=\"#fn2gfgtverpn2\">[5]</a></sup></span></p><p>It\u2019s important not to take actions that look agentic for their own sake; use agency as a vehicle to get to the things you actually care about. Agency isn\u2019t a helpful terminal goal, but like productivity or ambition, it is instrumentally useful.</p><h1>\u201cAgency\u201d \u2260 \u201chustle hard\u201d</h1><p>I am concerned about the agency meme having Hustle Culture undertones.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefoew11z51um9\"><sup><a href=\"#fnoew11z51um9\">[6]</a></sup></span>&nbsp;There seems to be a caricature of agency that: founds a start-up; sends an absurd number of cold emails;&nbsp;<a href=\"https://www.lesswrong.com/posts/qHpazCw3ryvBojGSa/my-fear-heuristic\"><u>does</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/iA25AvZqAr6G8mAXR/break-your-habits-be-more-empirical\"><u>weird</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/mbHHE8e5442YJR4JS/self-experiment-does-working-more-hours-increase-my-output\"><u>and</u></a>&nbsp;<a href=\"https://www.neelnanda.io/blog/43-making-friends\"><u>unconventional</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/AksEurnb73RCg9Dse/one-year-of-pomodoros-1\"><u>experiments</u></a>; writes lots of blog posts; has&nbsp;strong <a href=\"https://forum.effectivealtruism.org/topics/inside-vs-outside-view\"><u>inside views</u></a> which overrule the outside view.<br>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995532/mirroredImages/acyfmFTN3cNgwnYw6/wsh5bpp22hmphwnet4jv.png\"></p><p>I prefer a version of agency that advocates for acting intentionally. Agency does not entail working long hours and having an ambitious to-do list, and it is not a tradeoff against rest; in fact, doing things to high standards and achieving your goals often requires tons of&nbsp;<a href=\"https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\"><u>slack</u></a>.</p><p>We all (probably) have goals that do not feel like hustling and striving, for example:&nbsp;</p><ul><li>make sure I can get 8-10 hours of sleep a night;&nbsp;</li><li>create room for&nbsp;<a href=\"https://www.lesswrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\"><u>slack</u></a> in my life;&nbsp;</li><li>have meaningful and emotionally close relationships;&nbsp;</li><li>call my mum regularly;&nbsp;</li><li>be a caring friend.</li></ul><p>Intentionally taking steps to achieve these goals are just as agentic as reaching out to a potential mentor or upskilling at [virtuous technical skill].&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref729qknmykzf\"><sup><a href=\"#fn729qknmykzf\">[7]</a></sup></span></p><h1>Ways that \u201cagency\u201d has been costly for me (and mistakes I\u2019ve made)</h1><p>Most people are not proactive enough, so the&nbsp;<a href=\"https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/\"><u>law of equal and opposite advice</u></a> applies throughout all the following examples.&nbsp;</p><p>However, the following might especially apply to people who: have spent time in an environment where agency is socially rewarded; aren\u2019t afraid to \u201ctake up space\u201d socially; or have deliberately worked on the skill of becoming more agentic.</p><h2>Ignoring a gut feeling that I was overstepping a boundary</h2><p>Sometimes it is \u201cworth it\u201d to violate social norms to achieve your goals.&nbsp;Sometimes it is not.</p><p>Last year, I attended the&nbsp;<a href=\"https://espr-camp.org/\"><u>European Summer Program on Rationality</u></a> (ESPR), where agency was a core theme and was socially encouraged. A common joke was to enthusiastically yell \u201c<i>AGENCY!\u201d&nbsp;</i>whenever anyone did anything weird/ bold/ unconventional.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc3ipe170pb5\"><sup><a href=\"#fnc3ipe170pb5\">[8]</a></sup></span>&nbsp;This was useful and it helped me overcome fears that others would perceive me as obnoxious or annoying if I \u201ctook up space\u201d socially \u2013 something that was seriously holding me back at the time.&nbsp;</p><p>But afterwards, whenever I had a voice of doubt in my head <i>(</i>\u201c<i>you might be overstepping a boundary! You might be being \u201ctoo much\u201d! You might be making it harder for others to participate in this conversation! Are you sure it\u2019s good for you to ask for this?\u201d)</i>, I would crush it with the memory of an instructor yelling&nbsp;<i>\u201cAGENCY!\u201d&nbsp;</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpicxngudmh\"><sup><a href=\"#fnpicxngudmh\">[9]</a></sup></span></p><p>Unfortunately, this voice in my head was serving a very real purpose. It was trying to make sure that others around me were comfortable, and that I wasn\u2019t socially domineering.&nbsp;</p><p>I am concerned that a takeaway from the agency meme could be:&nbsp;<i>\u201cdo big and bold things, even if you\u2019re uncomfortable and scared! Don\u2019t let fear hold you back,\u201d </i>and I want to give a reminder that discomfort is serving a purpose.</p><h2>Assuming that it is low-cost for others to say \u201cno\u201d to requests</h2><p>I have become increasingly <a href=\"https://www.lesswrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess\">ask culture</a> over the past year. I have developed a low bar to asking for things from others and a (mostly) thick skin for people saying \u201cno\u201d.</p><p>I recently realised that I had been expecting others to know and enforce their own boundaries, and overestimating how easy it is to say \u201cno.\" I now think that this is an unrealistic expectation to have.</p><p>Someone recently told me that I was \u201cterrible at <a href=\"https://www.lesswrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess\">guess culture</a>.\u201d This was helpful honest feedback, so I polled some friends on whether they agreed.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3wugisgva3\"><sup><a href=\"#fn3wugisgva3\">[10]</a></sup></span>&nbsp;After pondering the evidence for a while (and considering whether I was too neurodiverse to be good at silly social games like guess culture<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflnv3iamyb8q\"><sup><a href=\"#fnlnv3iamyb8q\">[11]</a></sup></span>), I realised that I thought it wasn\u2019t my responsibility to participate in guess culture \u2013 I implicitly believed it was dumb.</p><p>My implicit belief was something like:</p><blockquote><p><i>\"If someone doesn't want to agree to something I ask, then it\u2019s their responsibility to say no! We\u2019re all agents with decision making capacity here! I want to take people at their word \u2013 if they agree to something I ask, then I\u2019ll believe them. It would be patronising to second guess them. I don\u2019t want to be tracking subtext and implicit social cues to see whether they actually mean what they say.&nbsp;</i></p><p><i>Of course, I want to deliberately make it easy for people to say no to me. I only want them to agree if they actually want to. I don\u2019t want to exert pressure on people at all. But I also want to be careful to not micromanage their emotions and wrap them in a blanket \u2013 they can enforce their own boundaries.\"</i></p></blockquote><p>I still endorse much of the sentiment here.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd1qzddymg1u\"><sup><a href=\"#fnd1qzddymg1u\">[12]</a></sup></span>&nbsp;But I now want to incorporate the reality that some people actually just find it hard to say no, and <strong>this is fine</strong>, and I don\u2019t want to put them in uncomfortable situations.&nbsp;<i>(Very big caveat that it is not unvirtuous to find it hard to say no to things! I do not want that to be the subtext of this section.)</i></p><p>Now, I\u2019m making more of an effort to track how people respond when I make a request (e.g. body language, tone of voice, eye contact). I am also trying to build models of whether I expect a given person to feel comfortable saying no. For people who might find it harder to say no, I will usually: make fewer requests; be very clear that it\u2019s fine if they don\u2019t respond (and I want them not to respond if that\u2019s the right call); give them space (and time) to think about whether they actually want to say yes; and generally approach the conversation with more tact.</p><p>Ideally, we would implicitly reward decisions not to respond when it\u2019s the right call (but I\u2019m not really sure what this looks like in practice.)</p><h2>Not regulating my consumption of finite resources (e.g. the time of others)</h2><p>I now realise that I used to model resources (in EA) as a free-market. For example, I thought that:</p><ul><li>I can ask people for their time, because if it\u2019s not worth it, they will say no;</li><li>I can ask lots of questions in a classroom, because the instructor will answer fewer of my questions if they aren\u2019t useful for the whole class;</li><li>I can send emails to whoever I want, because they will ignore them if they don\u2019t want to answer.</li></ul><p>I now think this model is flawed, and want to regulate my consumption of resources within the community more. Especially with regard to people\u2019s time, which is valuable and finite.</p><p>(<i>Again, this will not apply to most people reading this.)</i></p><h2>Not cooperating with authority systems</h2><p>For two months last year, I was studying for an important, difficult university entrance exam. Unfortunately, I had to be in school for seven hours a day, where the working conditions were unideal. There wasn\u2019t a quiet place where I could reliably work uninterrupted; it was costly to take my textbooks to school every day; my school mandated that everyone attend sessions that I didn\u2019t find useful (for example, about apprenticeships and applying to university).&nbsp;</p><p>I decided to be absent from school for three weeks before the exam (which, combined with a school break, meant that I didn\u2019t attend school for five weeks straight). At home, I had control over my sleep schedule, working routine, and eating times \u2013 which I optimised the hell out of. I also gained the useful data that I enjoyed self-studying.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn0niuodsad\"><sup><a href=\"#fnn0niuodsad\">[13]</a></sup></span></p><p>Unfortunately, my poorly explained absence made my teachers and peers unhappy with me. My mum had told the school that I was unwell, but this was suspected to be false.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref38oriy4klux\"><sup><a href=\"#fn38oriy4klux\">[14]</a></sup></span>&nbsp;I believe that my teachers felt betrayed and confused: my actions had been uncooperative. It was perceived as me arrogantly thinking I was above the rules. This burned through a lot of trust and social capital with my teachers \u2013 which had been slowly built up over years. One day, I was taken out of class and scolded, which felt really bad and shame-provoking. I also felt more distant from my peers upon returning. The strained relationships with my teachers and peers made school a much more unpleasant and difficult environment for me to be in.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefb9m1zqrqy7j\"><sup><a href=\"#fnb9m1zqrqy7j\">[15]</a></sup></span></p><p>To be clear, I endorse having taken time off school to study. This decision meaningfully increased my exam success (and therefore the likelihood that I could attend my top-choice university). But,&nbsp;<strong>there were major costs to not cooperating with my school authorities</strong>. For one, I care about the feelings of people around me, and this situation left others feeling undermined, which I wish I could have avoided. For another,&nbsp;<strong>life is much more enjoyable when you\u2019re playing cooperatively with other agents in your environment</strong>. Life is much more enjoyable when <i>you're regarded as a \"nice person\"</i>&nbsp;<i>who others like and trust.&nbsp;</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmcuzljsv1v8\"><sup><a href=\"#fnmcuzljsv1v8\">[16]</a></sup></span></p><p>I\u2019m honestly not sure what I would have done differently here.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbq2tuppyabc\"><sup><a href=\"#fnbq2tuppyabc\">[17]</a></sup></span>&nbsp;My closing point is just that actions have costs \u2013 and that big, \u201cagentic,\u201d unconventional actions have bigger costs.</p><h2>Learned helplessness from leaning too far into social agency</h2><p>I suspect that leaning heavily into \u201csocial agency\u201d can make people worse at \u201cActually Doing Things agency.\u201d If you\u2019re in a headspace of readily seeking help from others, you might temporarily develop a helplessness around doing things for yourself.</p><p>I experienced a period of reduced sense of self-sufficiency recently. When I encountered a problem, my default thought process was often<i>\u201cwho can help me?\u201d&nbsp;</i>instead of&nbsp;<i>\u2018how can I solve this problem myself?\u201d&nbsp;</i></p><p>Examples:</p><ul><li>My bike chain fell off my bike and I didn\u2019t really consider putting it back on myself (even though I would be perfectly capable of figuring out how to do that). Instead, some guy on the street helped.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkeyrctigjt\"><sup><a href=\"#fnkeyrctigjt\">[18]</a></sup></span></li><li>When I feel sad, my default is to message friends and talk about it with them. This can be a very good default \u2013 definitely better than feeling too ashamed to my friends about what\u2019s bothering me. But my friends aren\u2019t always available and it\u2019s important to me that I don\u2019t need to depend on them to feel better. If my friends aren\u2019t available, I often feel stuck in a rut until I can speak to them.&nbsp;</li><li>Occasionally, my Anki flashcards build up and I develop an ugh-field around reviewing my flashcards. When this happens, I tend to avoid it until I am stressed enough to tell a friend about it. Then, the friend usually helps me overcome it (by coworking, or encouragement, or a monetary fine if I don\u2019t do it). Until I tell the friend, I feel helpless and defeated. I then rely on my friend to overcome it.</li><li>I rarely&nbsp;<a href=\"https://www.lesswrong.com/tag/debugging\"><u>debug</u></a> anything by myself, instead defaulting to problem-solving with friends.&nbsp;</li></ul><p>This is a clear case where the laws of&nbsp;<a href=\"https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/\"><u>equal and opposite advice</u></a> apply; all of these examples are completely fine in moderation, but I could certainly do with developing my solo-problem-solving skills more than my asking-for-help skills.</p><h1>Things I\u2019ve changed my mind on since my last post</h1><p>I think my <a href=\"https://forum.effectivealtruism.org/posts/Pc3CFbYxPXgyjoDpB/seven-ways-to-become-unstoppably-agentic\">previous post</a> (perhaps unhelpfully) contributed to the \u201cagency is high status and always good\u201d meme.</p><p>I have changed my mind the following things since then:</p><ul><li>The title of my previous post on agency was meant to be ironic, but I was glorifying the idea of being \u201cunstoppable,\u201d in a way that I no longer endorse. \u201cUnstoppable\u201d has some of the \u201cparasite-like\u201d undertones.</li><li>In the post, I advocate for having a very low bar for asking for help, but I no longer think that this is universally good. Some people could certainly do with asking for help more. But, it\u2019s possible for dominant individuals to monopolise resources \u2013 in a way that is net harmful to the community.</li><li>I said in a <a href=\"https://forum.effectivealtruism.org/posts/Pc3CFbYxPXgyjoDpB/seven-ways-to-become-unstoppably-agentic?commentId=AM3rixvsf8XahyD7A\">comment</a>: <i>\"it seems strictly good for EAs to be socially domineering in non-EA contexts. Like\u2026 I want young EAs to out-compete non-EAs for internship or opportunities that will help them skill build. (This framing has a bad aesthetic, but I can\u2019t think of a nicer way to say it.)\" &nbsp;</i>I now disagree with this; it feels selfish in a way that's hard to imagine is good, deontologically.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkhe4ytbrhjc\"><sup><a href=\"#fnkhe4ytbrhjc\">[19]</a></sup></span></li><li>It\u2019s less clear to me that Twitter is a good use of a given person\u2019s time.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref320org1prje\"><sup><a href=\"#fn320org1prje\">[20]</a></sup></span></li></ul><p>&nbsp;I really like&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Pc3CFbYxPXgyjoDpB/seven-ways-to-become-unstoppably-agentic?commentId=NPGQXvBiJ7hRev3EA\"><u>Owen\u2019s comments</u></a> on the post.</p><blockquote><p><i>\u201cI feel uneasy about is the reinforcement of the message \"the things you need are in other people's gift\", and the way the post (especially #1) kind of presents \"agency\" as primarily a social thing (later points do this less but I think it's too late to impact the implicit takeaway.</i></p><p><i>Sometimes social agency is good, but I'm not sure I want to generically increase it in society, and I'm not sure I want EA associated with it. I'm especially worried about people getting social agency without having something like \"groundedness\".</i></p></blockquote><blockquote><p><i>... At the very least I think there's something like a \"missing mood\" of ~sadness here about pushing for EAs to do lots of &nbsp;[being socially dominant]. The attitude I want EAs to be adopting is more like \"obviously in an ideal world this wouldn't be rewarded, but in the world we live in it is, and the moral purity of avoiding this generally isn't worth the foregone benefits\". If we don't have that sadness I worry that (a) it's more likely that we forget our fundamentals and this becomes part of the culture unthinkingly, and (b) a bunch of conscientious people who intuit the costs of people turning this dial up see the attitudes towards it and decide that EA isn't for them.\"</i></p></blockquote><p>&nbsp;</p><p><i>I'm grateful for the conversations that prompted much of this post, and for helpful feedback from friends on a draft.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjoo2s4m55m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjoo2s4m55m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\"Agency\" refers to a range of proactive, ambitious, deliberate, goal-directed traits and habits. See the beginning of&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Pc3CFbYxPXgyjoDpB/seven-ways-to-become-unstoppably-agentic\"><u>this post</u></a> for a less abstract definition.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl253hcr4vh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl253hcr4vh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>A friend commented that this post could be an info-hazard for people who aren't proactive enough.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrwpn1g9mzi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrwpn1g9mzi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This was coined by Owen Cotton-Barratt in a <a href=\"https://forum.effectivealtruism.org/posts/Pc3CFbYxPXgyjoDpB/seven-ways-to-become-unstoppably-agentic?commentId=NPGQXvBiJ7hRev3EA\">comment on the original post</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqx61gqvgqr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqx61gqvgqr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example: reaching out to someone you admire online and asking to have a call with them; organising social events; asking for help on a research project from someone more senior; asking your peers for feedback; organising a coworking session with a friend where you both overcome your ugh-fields.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2gfgtverpn2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2gfgtverpn2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For example, maybe the best action for a given individual is quiet and simple, like studying hard in their room. But maybe they are incentivised to do other things like organise events and write blog posts, because these actions look agentic.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnoew11z51um9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefoew11z51um9\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.monster.com/career-advice/article/what-is-hustle-culture#:~:text=Also%20known%20as%20burnout%20culture,pursuit%20of%20their%20professional%20goals.\">This website</a> defines Hustle Culture: \"Also known as burnout culture and grind culture, hustle culture refers to the mentality that one must work all day every day in pursuit of their professional goals.\"</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn729qknmykzf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref729qknmykzf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear, I mean \"virtuous\" in a tongue in cheek way here.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc3ipe170pb5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc3ipe170pb5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Examples: yell to a room to be quiet and listen; give an impromptu lightning talk; jump in a river at night; ask for more food at dinner; seek out instructors and ask them for a 1-1.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpicxngudmh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpicxngudmh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I mainly used this example because I find it funny, and I am not blaming ESPR or any staff member. I am responsible for my own actions .</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3wugisgva3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3wugisgva3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>My close friends all felt moderately to strongly positively towards my communication style \u2013 which is to be expected because of the extremely strong selection bias. Someone who I know less well said that they have sometimes found it hard to say no t0 me, which was very helpful feedback.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlnv3iamyb8q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflnv3iamyb8q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is a joke; I don't actually think guess culture is silly. I think it serves a valuable purpose, socially.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd1qzddymg1u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd1qzddymg1u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Of treating people as agents and not trying to enforce their boundaries for them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn0niuodsad\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn0niuodsad\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that I found self-studying&nbsp;<strong>much</strong> harder when I did it for five months this year.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn38oriy4klux\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref38oriy4klux\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Towards the end, she told the school that I was staying home to study for the exam.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnb9m1zqrqy7j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefb9m1zqrqy7j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I had been struggling in school before then, but my absence certainly amplified it.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmcuzljsv1v8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmcuzljsv1v8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Obviously, my goal isn't to be regarded as a nice person, it's to be kind and act with care towards the emotional wellbeing of others (and the two are importantly different).&nbsp;</p><p>But<strong> it does feel bad (for me at least) when others don't think of you as a \"nice person,\"</strong> and this is a cost worth tracking, even when taking actions that you overall endorse.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbq2tuppyabc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbq2tuppyabc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Maybe I would have discussed it with my teachers beforehand. Maybe I would have told the school that I was taking time off to study, instead of ill. I don\u2019t know; I\u2019m doubtful that either of those solutions would have led to better outcomes.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkeyrctigjt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkeyrctigjt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This happened again recently and I put the chain back on my bike by myself! Win.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkhe4ytbrhjc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkhe4ytbrhjc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Multiplied, this strategy destroys everything and eats up all the resources.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn320org1prje\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref320org1prje\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I am concerned about an implicit assumption that engaging with the EA community on Twitter makes the world better. I also feel uncomfortable that an implicit goal of Twitter is to climb the Twitter status ladder. My rough thoughts/feelings now are probably: \u201cif you\u2019re bottlenecked by your network, consider using Twitter to grow your network. But seriously consider the possibility that Twitter could be a time and attention sink.\u201d The above is conditional on using Twitter for impact and networking. But if you find Twitter fun and exciting (which is great), it might be worth it.</p><p>I personally find Twitter too addictive if I use it regularly, and it's better for my mental health if I'm on social media less (but this is just my experience).</p></div></li></ol>", "user": {"username": "Evie Cottrell"}}, {"_id": "ooG5EJrhfS6MCGt7e", "title": "Should Community Builders Experiment More?", "postedAt": "2022-09-11T11:52:06.114Z", "htmlBody": "<p><i>[Epistemic status: a shower thought]</i></p><p><strong>Main question</strong>: Do we have a list of most promising interventions for local group organizers and other community builders to try out? If not, why not?</p><h2>What\u2019s the issue?</h2><p>As widely discussed in EA, the impact of efforts to do good seems to be fat-tailed - most of the donations in EA come from a few billionaires; the best global health interventions can be 60 times more cost-effective than the median <a href=\"https://www.files.ethz.ch/isn/162329/1427016_file_moral_imperative_cost_effectiveness.pdf\"><u>[1]</u></a>; the wide differences in the number of citations indicate that some research has much more impact than other etc. Likely, the same distribution applies to the cost-effectiveness of community building interventions, which seems to have important implications for strategy.</p><p>Despite this, it seems like there has been little talk about exeprimentation and efforts to find the few most impactful opportunities in the field of community building. Is this not a missed opportunity?</p><h2>What could be done about it?</h2><h3><strong>Finding ideas</strong></h3><p>How do we find the most cost-effective intervention ideas in community building?</p><ul><li>We could incentivise community builders to run new experiments that they think could be cost-effective.</li><li>We could run a competition to generate ideas.</li><li>We could make a list of high-potential community building interventions.<ul><li>Either find or combine existing lists.</li></ul></li></ul><p>Or have a small team create one themselves. They could copy the methods Charity Entrepreneuship uses to look for most promising charity ideas <a href=\"https://www.charityentrepreneurship.com/research\"><u>[3]</u></a>.</p><p>&nbsp;</p><p>What do you think?</p>", "user": {"username": "rannilo"}}, {"_id": "zDveJHXMpD6pgtj5v", "title": "Who Encouraged Or Inspired You?", "postedAt": "2022-09-12T07:17:07.685Z", "htmlBody": "<p>The Peek behind the Curtain interview series includes interviews with eleven people I thought were particularly successful, relatable, or productive. We cover topics ranging from productivity to career exploration to self-care.&nbsp;</p><p>This eighth post covers thoughts on mentors, colleagues, and feedback.</p><p>You can view bios of my guests and other posts in the series <a href=\"https://lynettebye.com/blog/2022/5/25/a-peek-behind-the-curtain-interview-series\">here</a>. This post is cross posted on my <a href=\"https://lynettebye.com/blog/2022/9/8/who-encouraged-or-inspired-you\">blog</a>.</p><h2>How did you develop your most meaningful mentors? Who encouraged or inspired you?&nbsp;</h2><h3>Admit your vulnerability and ask&nbsp;</h3><blockquote><p>I\u2019ve developed great mentorships by asking questions, and sometimes coming clean about my total ignorance about a topic, tactfully. Without seeming incompetent, you can definitely talk about your vulnerability and knowledge gaps. For example, after some big meeting, I can tell that there's some background information that I'm missing, maybe an ulterior motive someone seems to exhibit. To know the whole picture, I'll privately ask a trusted higher-up for the things between the lines. People are often glad to teach others the ropes.</p><p>Abi Olvera</p></blockquote><h3>Having role models can open doors&nbsp;</h3><blockquote><p>Setting up a charity feels like this big intimidating thing that someone coming straight out of university would have no idea how to do.</p><p>Charity Entrepreneurship is saying, \"Hey, you too can do this. Here are some things that you need, you need to be a smart person and have a good idea and stuff like that. Hey, we're going to vet people and figure out if you seem like actually a good fit for this. Also, we're going to describe some projects that you could do that are worth trying. Also, we're going to model it. Look, here are a couple of charities that have gone really well. You too can do this.\"</p><p>I think that kind of thing is also pretty useful because as an undergrad, I just hadn't really come across anyone who was setting up companies or charities or anything. I had no idea that you would be able to do this without too much work. The kind of thing that made it feel way more viable was talking to some people who had done it and were just like, \"Yes, no, this isn't that hard. Here are the kinds of things you do. Also, here are some other people you can ask for help from,\" that kind of thing.</p><p>I think putting more information out there, I guess including the kind of thing that you're doing with your interview series, can help people realize that these are the kinds of things that they could have a go at as well.</p><p>Michelle Hutchinson</p></blockquote><h3>Learn by working with senior people</h3><blockquote><p>It's important to hear what more senior people think, who\u2019ve sort of gone through this process. Especially because senior people tend to be more plugged in and have a lot of accumulated knowledge. So trying to pick their brains on things tends to be more valuable. You can also learn a lot through collaborating with people. I'm working with this guy right now, Stefano DellaVigna, who's fantastic. And it's been a great learning experience for me to see how he works on grant applications and how he phrases things. It's been really wonderful working with him. So some of that is also just learning by working with other people.</p><p>Eva Vivalt</p></blockquote><h3>Work with people who can mentor you</h3><blockquote><p>I think because the field that I'm in is very relationship-based, relationships have been pretty important. Two really obvious examples have been the two main bosses I've worked with, Holden Karnofsky and Jason Matheny. Just trying to have good relationships with them, trying to do good work so that they trust me in my work. I think they both have supported me a lot. That's been very, very important.&nbsp;</p><p>Helen Toner</p></blockquote><h3>Encouragement is sometimes necessary&nbsp;</h3><blockquote><p>I tended not to find it easy to come up with ideas and just decide to execute on them. It meant that it was really useful for me to be surrounded by people who were ambitious and had ideas.</p><p>In particular, I'm very motivated by helping the people around me. As soon as there was someone doing some ambitious project and they were like, \"We want to do this thing. Do you think you can do this part of it or something?\" that immediately feels way more appealing to me than coming up with that idea and deciding to do it. Things like running Giving What We Can, I just definitely don't think I would have, on my own steam, decided that I was ready for. Whereas having someone be like, \"Nope, I think you are ready for it, and you're going to do it, go get them\" made all the difference to me.</p><p>I think some of that can come somewhat quickly. I think part of what advising does often is, look at someone's CV, talk to them for half an hour and be like, \"Yeah no, I just think you can do this, maybe won't go well, but I'm pretty sure you can do it. You certainly can try. No one will think you're ridiculous for trying,\" that kind of thing.</p><p>I think it doesn't require many months of getting to know someone super well, you can just be like, \"You're the kind of profile for whom this is reasonable.\"&nbsp;</p><p>Michelle Hutchinson&nbsp;</p></blockquote><p>&nbsp;</p><h2>How do you elicit useful feedback? How important is being surrounded with or working with other great people?&nbsp;</h2><h3>Use objections you\u2019ve already heard to elicit better feedback</h3><blockquote><p>Trying to anticipate objections and listing objections that you've already heard and asking people to say if they agree with them or if that sparks any thoughts, is pretty helpful and will get you a lot more responses.</p><p>Ajeya Cotra</p></blockquote><h3>Being surrounded by great people seems important for research</h3><blockquote><p>I think I've definitely benefited from having people to follow or at least people to try to join in their projects. I've benefited from that a lot. As far as research ideas goes, this definitely feels true to me. It does seem really important to have people that bounce ideas off.</p><p>Some people also seem to just go off on their own, do a bunch of really good stuff. I don't know how that would work.</p><p>Daniel Ziegler</p></blockquote><h3>Get feedback opportunistically or go where it\u2019s easy</h3><blockquote><p>The main way I get feedback is actually by talking with particular individuals. Like there\u2019s some person who is in my field, and I reach out to them directly and try to chat with them. It can also be a little bit more opportunistic\u2014so it's maybe somebody who's not quite in my field, but you know, maybe they're in my department or I run into them in the hallway or see them at a conference or something like that. So your environment also matters.&nbsp;</p><p>Eva Vivalt</p></blockquote><h3>Ask observant, kind people for feedback&nbsp;</h3><blockquote><p>I think if you can identify people who are both observant and reasonably kind, and try to seek out feedback. One of my coworkers in that first social work job just gave me some kind feedback about like, \"Here's some things you're doing that are annoying people.\" She did it in this gentle way and I'm just like, \"Oh my gosh, I wish you had told me that six months ago.\u201d That was very helpful.</p><p>I think if I had said, \"Hey, you've been in this field a lot longer than I have. If you had three tips to give me, what would they be?\" then I wouldn't have been so embarrassed when I realized, \"Oh my gosh, I've just been screwing up these things,\" and not knowing it because my manager was too busy until like three months in to be like, \"Here are all the things that annoy your coworkers.\"</p><p>Julia Wise&nbsp;</p></blockquote><h3>Pick people\u2019s brains about areas they know well&nbsp;</h3><blockquote><p>People can really point you to the things that you should be paying attention to, which is really useful. It's not only the case that you can come to them with specific questions that are useful for your work, but also that they can notice other things that they know about that are useful for your work until you can really find the most useful things. I find the EA community great for this just very disproportionately if you write to a person like, \"Hey, 'I have some questions about your specific job,'' they're happy to chat.&nbsp;</p><p>Then I prepare beforehand to make sure that I have specific questions. For example, during advising sessions, I'll try to write down afterward whatever came up that I didn't know the answer to and that way, when I'm talking to someone if I notice like, \"Hey, I feel like I have a few finance questions,\" I can reach out to someone in finance and be like, \"Hey, do you want to chat?\" and then go through my questions.</p><p>Michelle Hutchinson</p></blockquote><h3>Try copying beliefs to build models</h3><blockquote><p>I think an example of this is when I was trying to build my models of how AI will work.&nbsp;</p><p>One of the major things I did was \u201cCan I take some person who's already working in AI safety and has written a decent amount about their views? Can I inhabit that person's perspective and explain why they have these views?\u201d Frequently, I couldn't, but some of the times I could. I think even when I couldn't, the act of trying gave me a lot of information.&nbsp;</p><p>Imagine that is like giant haystack of possible things you could think about AI safety, and then there's this one needle that's Eliezer Yudkowsky. Then you're like, \"Okay, well, I don't really know exactly where Eliezer's needle is in the entire haystack, but I know it's in this tiny little portion over here based on things that he's written.\"&nbsp;</p><p>A lot of the things, that already narrows it down a lot, and then I can think about, \"Okay, well, it can't be this particular view because that makes no sense. Maybe it's this other thing.\" It narrows down your search space a lot. Rather than sifting through the whole haystack, you're like, here are these 10 different spots where it could be out of millions of possible spots, and search only those 10.</p><p>Rohin Shah</p></blockquote><p><i>Enjoying the interview? </i><a href=\"http://eepurl.com/gnWbln\"><i><u>Subscribe to Lynette\u2019s newsletter</u></i></a><i> to get more posts delivered to you.</i></p>", "user": {"username": "lynettebye"}}, {"_id": "NbWeRmEsBEknNHqZP", "title": "Longterm cost-effectiveness of Founders Pledge's Climate Change Fund", "postedAt": "2022-09-14T15:11:22.959Z", "htmlBody": "<h1>Summary</h1><ul><li>This analysis estimates the cost-effectiveness of reducing <a href=\"https://forum.effectivealtruism.org/topics/existential-risk\">existential risk</a> via the <a href=\"https://founderspledge.com/funds/climate-change-fund\">Climate Change Fund</a> (CCF) of <a href=\"https://founderspledge.com/\">Founders Pledge</a> (FP) (see <a href=\"https://forum.effectivealtruism.org/posts/5C8B8fHEohSGF5vKr/cost-effectiveness-of-founders-pledge-s-climate-change-fund#Methodology\">Methodology</a>).</li><li>The results were obtained with <a href=\"https://colab.research.google.com/drive/1I9wXk7awV-lOqXEp5ppwUaKm9sua3JJV?usp=sharing\">this</a> Colab, and the key ones are summarised in the table below<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefj7h4i4i0sxc\"><sup><a href=\"#fnj7h4i4i0sxc\">[1]</a></sup></span>&nbsp;(for more, see <a href=\"https://forum.effectivealtruism.org/posts/5C8B8fHEohSGF5vKr/cost-effectiveness-of-founders-pledge-s-climate-change-fund#Results\">Results</a> and <a href=\"https://forum.effectivealtruism.org/posts/5C8B8fHEohSGF5vKr/cost-effectiveness-of-founders-pledge-s-climate-change-fund#Discussion\">Discussion</a>). Comments about how to interpret them are welcome.</li><li>The cost-effectiveness bar for reducing existential risk (8 kt/$) is estimated to be 3 times as high as (an overestimate for?) the cost-effectiveness of CCF (2 kt/$) (see <a href=\"https://forum.effectivealtruism.org/posts/5C8B8fHEohSGF5vKr/cost-effectiveness-of-founders-pledge-s-climate-change-fund#Discussion\">Discussion</a>). This suggests the best interventions to fight climate change are not amongst the most effective ways of reducing existential risk.</li></ul><figure class=\"table\" style=\"height:327.2px;width:617.308px\"><table style=\"border:1px double rgb(179, 179, 179)\"><thead><tr><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Result</strong></th><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>Mean</strong></th></tr></thead><tbody><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Existential risk due to climate change (bp)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">1.00</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cumulative GHG emissions between 2020 and 2100 (Tt)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">3.76</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Existential risk reduction caused by removing GHG emissions (bp/Tt)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.273</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cost-effectiveness of removing GHG emissions via CCF (t/$)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">2.34 k</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cost-effectiveness of CCF (bp/G$)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.640</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cost-effectiveness bar for reducing existentiat risk (bp/G$)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">2.17</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cost-effectiveness bar for removing GHG emissions (t/$)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">7.94 k</td></tr></tbody></table></figure><h2>Acknowledgements</h2><p>Thanks to Alexey Turchin, Anonymous Person, David Denkenberger, Johannes Ackva, Luke Kemp, and Nu\u00f1o Sempere.</p><h1>Introduction</h1><p>Removing greenhouse gas (GHG) emissions decreases heat-related deaths: <a href=\"https://www.nature.com/articles/s41467-021-24487-w\">Bressler 2021</a> estimated the 2020 mortality cost of carbon to be 2.26*10^-4 life/t. However, most of the benefits of reducing GHG emissions respect the decrease in existential risk due to climate change.&nbsp;</p><p>This analysis estimates the existential risk reduction caused by removing GHG emissions, and the cost-effectiveness of CCF, which is compared with an estimate for the cost-effectiveness bar for reducing existential risk. Nonetheless, it should be noted that, according to <a href=\"https://forum.effectivealtruism.org/posts/NbWeRmEsBEknNHqZP/longterm-cost-effectiveness-of-founders-pledge-s-climate?commentId=pQmcndSe2c33Wshta\">this</a> comment from Johannes Ackva<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreff9ihxcqstt\"><sup><a href=\"#fnf9ihxcqstt\">[2]</a></sup></span>:</p><blockquote><p>The goal of the Climate [Change] Fund is to optimize spending that is not cause-neutral, to have as big as possible a positive impact given constraints, it is not intended or marketed as \"the top bet on reducing existential risk\" and we are careful to not crowd in resources that would otherwise go to areas we think of as higher marginal priority.</p></blockquote><p>I encourage the readers to check this <a href=\"https://forum.effectivealtruism.org/posts/NbWeRmEsBEknNHqZP/longterm-cost-effectiveness-of-founders-pledge-s-climate?commentId=66EDAHD9QdfPf4rpQ\">comment</a> from Matt Lerner, FP's research director, to better understand FP's mission:</p><blockquote><p>To be absolutely clear, <strong>FP's goal is to do the maximum possible amount of good</strong>, and to do so in a cause-neutral way.</p></blockquote><h1>Methodology</h1><p>The (marginal) cost-effectiveness of CCF was estimated from the product between:</p><ul><li>The existential risk reduction caused by removing GHG emissions, which was estimated from the ratio between:<ul><li>The existential risk due to climate change.</li><li>The cumulative GHG emissions between 2020 and 2100 assuming current climate policies<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5npmd7klibn\"><sup><a href=\"#fn5npmd7klibn\">[3]</a></sup></span>.</li></ul></li><li>The cost-effectiveness of removing GHG emissions via CCF, which was estimated from:<ul><li>The reciprocal of the cost to remove GHG emissions via CCF.</li></ul></li></ul><p>The assumption of current climate policies regarding the cumulative GHG emissions is consistent with the current <a href=\"https://www.metaculus.com/questions/\">Metaculus</a>' cummunity prediction of 2.6 \u00baC for <a href=\"https://www.metaculus.com/questions/605/how-much-global-warming-by-2100/\">how much greater (in \u02daC) will the average global temperature in 2100 be than the average global temperature in 1880</a>. According to the <a href=\"https://climateactiontracker.org/global/temperatures/\">Climate Action Tracker</a> (CAT), current climate policies are predicted to result in a global warming between 2.5 \u00baC and 2.9 \u00baC (interval which contains 2.6 \u00baC). The modelling of the cumulative GHG emissions would ideally consider more scenarios. The exclusion of&nbsp;the emissions after 2100 until net zero, after which the existential risk due to climate change is arguably negligible, tends to overestimate the cost-effectiveness.</p><h2>Existential risk due to climate change</h2><p>The existential risk due to climate change was modelled as a <a href=\"https://en.wikipedia.org/wiki/Beta_distribution\">beta distribution</a> with:</p><ul><li>Mean equal to 0.01 %, which was determined from the geometric mean between:<ul><li>The 0.1 % guessed by Toby Ord in <a href=\"https://theprecipice.com/\">The Precipice</a> for the next 100 years (2021 to 2120).</li><li>The upper bound of 0.01 % guessed by 80,000 Hours <a href=\"https://80000hours.org/problem-profiles/climate-change/#summing-up-how-climate-change-makes-global-catastrophic-risks-worse\">here</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw0z84tphn8s\"><sup><a href=\"#fnw0z84tphn8s\">[4]</a></sup></span>.</li><li>The 0.001 % respecting the John Halstead's best guess presented <a href=\"https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report\">here</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4iei7w1wu3q\"><sup><a href=\"#fn4iei7w1wu3q\">[5]</a></sup></span>.</li></ul></li><li>Ratio between the mode and mean equal to that of the total existential risk considered in <a href=\"https://allfed.info/images/pdfs/Long_term_cost_effectiveness_of_resilien.pdf\">Denkenberger 2022</a>, which is defined as a beta distribution with parameters alpha and beta of 1.5 and 8 (see section 2.3).</li></ul><p>This led to parameters alpha and beta having values of 1.73 and 17.3 k<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpcjd4i4af9\"><sup><a href=\"#fnpcjd4i4af9\">[6]</a></sup></span>.&nbsp;</p><p>The distribution defined here should not be considered <a href=\"https://forum.effectivealtruism.org/posts/fPu5eWJagwDvqxiGY/terminate-deliberation-based-on-resilience-not-certainty\">resilient</a>. As discussed in <a href=\"https://www.pnas.org/doi/full/10.1073/pnas.2108146119\">Kemp 2022</a>, bad-to-worst-case scenarios of climate change are underexplored.</p><h2>Cumulative GHG emissions between 2020 and 2100</h2><p>The cumulative GHG emissions between 2020 and 2100 assuming current climate policies were modelled as a lognormal distribution with 25th and 75th percentiles equal to 3.333 Tt and 4.136 Tt, which are the lower and upper estimates of <a href=\"https://climateactiontracker.org/global/temperatures/\">CAT</a>. This led to values in Tt for the mean and standard deviation of 3.73 and 0.595.</p><h2>Cost to remove GHG emissions via CCF</h2><p>The cost to remove GHG emissions via CCF was modelled as a lognormal distribution with 2.5th and 97.5th percentiles equal to the lower and upper bound of the 95 % confidence interval provided by Johannes Ackva&nbsp;(via personal communation): in $/t, 10^-4 to 10. These should be intended as informed guesses, not resilient estimates. FP is working to produce a more robust cost-effectiveness distribution.</p><p>The guesses were assumed to account for the value of removing emissions as a function of global warming. For example, assuming existential risk due to climate change increases quadratically with global warming, and that this increases linearly with cumulative emissions, removing 1 t at 4 \u00baC of global warming would be twice as valuable as removing 1 t at 2 \u00baC of global warming.</p><h1>Results</h1><p>The results are presented below for a Monte Carlo simulation with 10 M samples. I encourage the readers to make a copy of the Colab <a href=\"https://colab.research.google.com/drive/1I9wXk7awV-lOqXEp5ppwUaKm9sua3JJV?usp=sharing\">model</a>, and select their preferred parameters. The model is fully commented (the inputs section is at the top), and could be run in 6 s for 10 M samples.</p><figure class=\"table\" style=\"height:436.75px;width:682px\"><table style=\"border:1px double rgb(179, 179, 179)\"><thead><tr><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Result</strong></th><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>Mean</strong></th><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>Standard deviation</strong></th><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>5th percentile</strong></th><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>Median</strong></th><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>95th percentile</strong></th></tr></thead><tbody><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Existential risk due to climate change (bp)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">1.00</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.760</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.146</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.815</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">2.48</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cumulative GHG emissions between 2020 and 2100 (Tt)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">3.76</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.606</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">2.85</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">3.71</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">4.83</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Existential risk reduction caused by removing GHG emissions (bp/Tt)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.273</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.215</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.0387</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.219</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.691</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cost-effectiveness of removing GHG emissions via CCF (t/$)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">2.34 k</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">90.0 k</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.252</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">31.6</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">3.97 k</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Cost-effectiveness of CCF (bp/G$)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.640</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">34.3</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">39.1&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mu\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">\u03bc</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">6.26 m</td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.953</td></tr></tbody></table></figure><h1>Discussion</h1><p>The mean cost-effectiveness of removing GHG emissions via CCF of 2.34 kt/$ appears to be an overestimate:</p><ul><li>According to Table 2 of <a href=\"https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.32.4.53\">Gillingham 2018</a>, it is<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3nw3y9tkzon\"><sup><a href=\"#fn3nw3y9tkzon\">[7]</a></sup></span>:<ul><li>7 k times as high as the cost-effectiveness of 0.3 t/$ for \"reforestation\".</li><li>50 k times as high as the cost-effectiveness of 0.04 t/$ for \"wind energy subsidies\".</li><li>200 k times as high as the cost-effectiveness of 0.01 t/$ for \"concentrating solar power expansion (China &amp; India)\", \"renewable fuel subsidies\", and \"livestock management policies\".</li><li>1 M times as high as the cost-effectiveness of 0.002 t/$ for \"solar photovoltaics subsidies\".</li></ul></li><li>It is 2 k times as high as 1 t/$, which is arguably the bar FP considers.</li><li>It is 70 times as high as the \"optimistic\" cost-effectiveness of 31.4 t/$ estimated by FP <a href=\"https://docs.google.com/spreadsheets/d/1q6srpmt5VkdXLGfYzqHqkU3hvGUwPKjA67uxqYI0Upw/edit#gid=200949301&amp;range=F31\">here</a> for the future projects of <a href=\"https://www.catf.us/\">Clean Air Task Force</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjoga5wgzg4\"><sup><a href=\"#fnjoga5wgzg4\">[8]</a></sup></span>&nbsp;(CATF). However, Johannes thinks the 2018 cost-effectiveness analysis which produced this estimate \"radically underestimated the real uncertainty in both directions\".</li></ul><p>However, the estimated mean cost-effectiveness of CCF is still smaller than most of the cost-effectiveness bars for reducing existential risk. These are summarised in the table below, and were taken from the answers to <a href=\"https://forum.effectivealtruism.org/posts/cKPkimztzKoCkZ75r/how-many-ea-2021-usds-would-you-trade-off-against-a-0-01\">this</a> question from Linchuan Zhang<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwucsei7n7d\"><sup><a href=\"#fnwucsei7n7d\">[9]</a></sup></span>, or provided via personal message<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs56z62wzcm\"><sup><a href=\"#fns56z62wzcm\">[10]</a></sup></span>.</p><figure class=\"table\" style=\"height:333.6px;width:457.233px\"><table style=\"border:1px double rgb(179, 179, 179)\"><thead><tr><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Answer</strong></th><th style=\"background-color:rgb(250, 250, 250);border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\"><strong>Cost-effectiveness bar (bp/G$)</strong></th></tr></thead><tbody><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Open Philanthropy (OP)</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">0.05<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk4ov7h2edj\"><sup><a href=\"#fnk4ov7h2edj\">[11]</a></sup></span></td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Anonymous Person</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">1<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefobufo2xkwk9\"><sup><a href=\"#fnobufo2xkwk9\">[12]</a></sup></span></td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Oliver Habryka</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">1</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Linchuan Zhang</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">3.33<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq0nw513vtck\"><sup><a href=\"#fnq0nw513vtck\">[13]</a></sup></span></td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Simon Skade</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">6</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>William Kiely</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">10</td></tr><tr><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em\"><strong>Median</strong></td><td style=\"border:1px double rgb(217, 217, 217);padding:0.4em;text-align:center\">2.17</td></tr></tbody></table></figure><p>The mean cost-effectiveness of CCF of 0.6 bp/G$ only exceeds OP's conservative (lower) bar. The median cost-effectiveness bar of 2.17 bp/G$ is equivalent to 7.94 kt/$ (= 2.17/0.273), which is 3 times (= 2.17/0.640) as high as the estimated cost-effectiveness of CCF, and 8 k times as high as 1 t/$.&nbsp;</p><p>Assuming interventions funded by CCF are amongst the best opportunities to remove GHG emissions, the above suggests interventions to fight climate change are not amongst the most effective ways of reducing existential risk.</p><p>In addition, it should be noted there seem to be opportunities whose cost-effectiveness is above the bar of 2 bp/G$. <a href=\"https://eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-021-00178-z\">Denkenberger 2021</a> and <a href=\"https://www.sciencedirect.com/science/article/pii/S2212420922000176#:~:text=One%20version%20of%20the%20model,at%20the%20margin%20now%2C%20respectively.\">Denkerberger 2022</a> estimate the following 5th and 95th percentiles<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzcxt96zfrv\"><sup><a href=\"#fnzcxt96zfrv\">[14]</a></sup></span>&nbsp;(in bp/G$):</p><ul><li><a href=\"https://eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-021-00178-z\">Denkenberger 2021</a> (see Table 2):<ul><li>\"Far future potential increase per $ due to loss of industry preparation average over ~\u2009$30 million model 1\": 4 and 30 k.</li><li>\"Far future potential increase per $ due to loss of industry preparation average over ~\u2009$50 million model 2\": 1 and 80.</li><li>\"Far future potential increase per $ AGI safety research at the $3 billion margin (same for both models)\": 0.08 and 50.</li></ul></li><li><a href=\"https://www.sciencedirect.com/science/article/pii/S2212420922000176#:~:text=One%20version%20of%20the%20model,at%20the%20margin%20now%2C%20respectively.\">Denkerberger 2022</a> (see Table 3):<ul><li>\"Far future potential increase per $ due to resilient foods average over \u223c$100 million S model\": 20 and 20 k.</li><li>\"Far future potential increase per $ due to resilient foods average over \u223c$100 million E model\": 30 and 80 k.</li><li>\"Far future potential increase per $ AGI safety research at the $3 billion margin (same for both models)\": 0.2 and 100.</li></ul></li></ul><p>This conclusion would hardly change due to including effects of removing GHG emissions which do not lead to <a href=\"https://forum.effectivealtruism.org/topics/trajectory-change\">trajectory changes</a>. For example, the direct benefits of reducing existential risk due to climate change which result from removing GHG emissions may well be over 100 k times as large as those from decreasing temperature-related mortality between 2020 and 2100, supposing:</p><ul><li>The benefits of reducing existential risk are 27.5 life/t, based on the following assumptions:<ul><li>Existential risk reduction caused by removing GHG emissions of 0.275 bp/Tt (see <a href=\"https://forum.effectivealtruism.org/posts/5C8B8fHEohSGF5vKr/cost-effectiveness-of-founders-pledge-s-climate-change-fund#Results\">Results</a>).</li><li>Population size of 10 G (currently, it is 8 G).</li><li>Existence of 10 Gyear, which is given as a lower bound in <a href=\"https://rucore.libraries.rutgers.edu/rutgers-lib/40469/PDF/1/play/\">Beckstead 2013</a> (search for \u201cexpected years of civilization ahead of us\u201d).</li><li>Life expectancy of 100 year/life (currently, it is 70 year/life).</li></ul></li><li>The benefits of decreasing temperature-related mortality between 2020 and 2100 are 2.26*10^-4 life/t, as given by the mortality cost of carbon estimated in <a href=\"https://www.nature.com/articles/s41467-021-24487-w\">Bressler 2021</a>.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnj7h4i4i0sxc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefj7h4i4i0sxc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>bp stands for basis point (0.01 <a href=\"https://en.wikipedia.org/wiki/Percentage_point\">pp</a>), t for tonne of CO2e, k for thousand, G for billion, and T for trillion.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnf9ihxcqstt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreff9ihxcqstt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Johannes leads FP's climate team, and is the first author of FP's latest climate <a href=\"https://assets.ctfassets.net/x5sq5djrgbwu/7eEpX4UcKNEy6LUDhf2B05/735518c277987ad5ad91f096b1fdc2a7/A_guide_to_the_changing_landscape_of_high-impact_climate_philanthropy.pdf\">report</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5npmd7klibn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5npmd7klibn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In other words, the more GHG emissions are required to reach a given level of global warming, the smaller is their longterm impact.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw0z84tphn8s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw0z84tphn8s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In the <a href=\"https://80000hours.org/problem-profiles/climate-change/\">climate change problem profile</a> from <a href=\"https://80000hours.org/\">80,000 Hours</a>, Benjamin Hilton writes:</p><blockquote><p>That said, we [80,000 Hours] still think this risk is relatively low. If climate change poses something like a 1 in 1,000,000 risk of extinction by itself, our guess is that its contribution to other existential risks is at most a few orders of magnitude higher \u2014 so something like 1 in 10,000.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4iei7w1wu3q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4iei7w1wu3q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>John Halstead writes:</p><blockquote><p>With those caveats in my mind, my best guess estimate is that the indirect risk of existential catastrophe due to climate change is on the order of 1 in 100,000, and I struggle to get the risk above 1 in 1,000. Working directly on US-China, US-Russia, India-China, or India-Pakistan relations seems like a better way to reduce the risk of Great Power War than working on climate change.</p></blockquote></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpcjd4i4af9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpcjd4i4af9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Based on the formulas for the mean (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\mu\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">\u03bc</span></span></span></span></span></span></span>) and mode (<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"m\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span></span></span></span></span></span>) provided in <a href=\"https://en.wikipedia.org/wiki/Beta_distribution\">Wikipedia</a>, the parameters of the beta distribution are:&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\alpha = \\left(1 - 2\\,m\\right)/\\left(1-m\\,/\\,\\mu\\right)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u03b1</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mrow MJXc-space3\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-texatom MJXc-space1\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mrow MJXc-space1\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">m</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">\u03bc</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span></span>;&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\beta = \\left(1 - 1\\,/\\,\\mu\\right)\\,\\alpha\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;\">\u03b2</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mrow MJXc-space3\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">\u2212</span></span><span class=\"mjx-mn MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">\u03bc</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">\u03b1</span></span></span></span></span></span></span>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3nw3y9tkzon\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3nw3y9tkzon\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The cost-effectiveness estimates were calculated from the reciprocal of the point estimates or the geometric mean of the lower and upper bounds of the values in $/t presented in <a href=\"https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.32.4.53\">Gillingham 2018</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjoga5wgzg4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjoga5wgzg4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>CATF has received two grants from the CCF (see table <a href=\"https://founderspledge.com/funds/climate-change-fund\">here</a>): 850 k$ in December 2020; and 2 M$ in November 2021.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwucsei7n7d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwucsei7n7d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The estimates of Kirsten Horton and Nu\u00f1o Sempere were not included, as they were seemingly supposed to be underestimates of their cost-effectiveness bars.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns56z62wzcm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs56z62wzcm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Anonymous Person's estimate was the only provided via personal message.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk4ov7h2edj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk4ov7h2edj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This refers to OP's longtermist projects, and is based on the estimate of \"$200 trillion per world saved\" provided by Ajeya Cotra in <a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/#last-dollar-project-022528\">this</a> section of <a href=\"https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/\">episode 90</a> of The 80,000 Hours Podcast. It concerns \"meta R&amp;D to make responses to new pathogens faster\", and \"[Open Philanthropy] were aiming for this to be conservative\", meaning the actual cost-effectiveness bar is higher.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnobufo2xkwk9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefobufo2xkwk9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Geometric mean between the lower and upper bound provided. \"I currently think it's probably somewhere between $1-100 trillion per existential catastrophe averted (I find this framing more intuitive than bp/G$)\".</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq0nw513vtck\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq0nw513vtck\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is the median cost-effectiveness bar provided by Linchuan. According to \"[Linchuan's] very fragile thoughts as of 2021/11/27\": \"I feel pretty bullish and comfortable saying that we should fund interventions that we have resilient estimates of reducing x-risk ~0.01% at a cost of ~$100M\" (10 bp/G$); \"I think for time-sensitive grants of an otherwise similar nature, I'd also lean optimistic about grants costing ~$300M/0.01% of xrisk, but if it's not time-sensitive I'd like to see more estimates and research done first\" (3.33 bp/G$); \"for work where we currently estimate ~$1B/0.01% of xrisk, I'd weakly lean against funding them right now, but think research and seed funding into those interventions is warranted\" (1 bp/G$).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzcxt96zfrv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzcxt96zfrv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The quantiles are expressed in 1/$ in the articles, but in bp/G$ here. However, for loss of industry preparation and resilent foods, they do not apply (as accurately) to investments of 1 G$, as they were computed for investments between 30 M$ and 100 M$.</p></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "bZPzDgF9nEXNnGgct", "title": "The (Allegedly) Best Business Books", "postedAt": "2022-09-12T00:38:31.762Z", "htmlBody": "<p><i>Epistemic Status: Mostly my own subjective opinions. You should read this as though each statement has a hidden \"in my opinion\" or \"I think\" attached to it.</i></p><p><strong>TLDR</strong>: I've read a lot of business books, and here I share my impressions<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz5344sfzxnp\"><sup><a href=\"#fnz5344sfzxnp\">[1]</a></sup></span>&nbsp;of some of the most popular ones. Some are definitely worth reading, depending on what you are looking for and what you already know. There are also several which I recommend avoiding because reading them would be a waste of time.</p><h1>Intro</h1><p>I read a lot, and I often find myself interested in business books. While these books are sometimes directly related to business and can help you be a better manager/leader/employee, some also can help you with self-control, decision making, or other areas of life. This is especially true if you are interested in increasing your competency in some area of life that can be applied to work. Thus, regardless of whether you want to be effective in altruism, in egoism, or in something else, these books might be helpful for you.</p><p>(I put this together fairly quickly without having anyone review it, so I wouldn't be surprised if there are some typos or other simple errors in it. Please do let me know if you notice something wrong.)</p><p>\"Business book\" is an odd genre. There are several different types:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefksk5na140h\"><sup><a href=\"#fnksk5na140h\">[2]</a></sup></span></p><ul><li>Sometimes it is a sort of curated memoir or biography, either of a company or of a person (like <a href=\"https://www.goodreads.com/book/show/27220736-shoe-dog\">Shoe Dog</a>, or <a href=\"https://www.goodreads.com/book/show/44525305-the-ride-of-a-lifetime?from_search=true&amp;from_srp=true&amp;qid=XPOBiRf4up&amp;rank=1\">The Ride of a Lifetime</a>)</li><li>Sometimes it is an attempt to distill and portray the underlying ideas behind a person's success or failure (such as <a href=\"https://forum.effectivealtruism.org/posts/zqJkKHA6tBCRbHBHe/Winning\">Winning</a>, or <a href=\"https://www.goodreads.com/book/show/54502643-billion-dollar-loser?from_search=true&amp;from_srp=true&amp;qid=AVHnGswAhJ&amp;rank=1\">Billion Dollar Loser</a>)</li><li>Sometimes it is moderately good information but it could have been a 5- to 15-page essay instead of a 200-300 page book (<a href=\"https://www.goodreads.com/book/show/6667514-the-checklist-manifesto?ac=1&amp;from_search=true&amp;qid=LcdmObQlnE&amp;rank=1\">The Checklist Manifesto</a> and <a href=\"https://www.goodreads.com/book/show/37559166-measure-what-matters?from_search=true&amp;from_srp=true&amp;qid=8zzuKwKKzN&amp;rank=1\">Measure What Matters</a> both spring to mind)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu3wcqnrutj\"><sup><a href=\"#fnu3wcqnrutj\">[3]</a></sup></span></li><li>Sometimes it is a slightly more objective/valid/rigorous attempt to share best practices, either as a result of research or experience (<a href=\"https://www.goodreads.com/book/show/50937.First_Break_All_the_Rules\">First, Break All the Rules</a> is a decent example, as in <a href=\"https://forum.effectivealtruism.org/posts/zqJkKHA6tBCRbHBHe/Work%20Rules!:%20Insights%20from%20Inside%20Google%20That%20Will%20Transform%20How%20You%20Live%20and%20Lead\">Work Rules!</a>)</li><li>Sometimes it is more of a general self-help or self-improvement book that happens to be applicable in some professional contexts (such as <a href=\"https://www.goodreads.com/book/show/84525.What_Got_You_Here_Won_t_Get_You_There\">What Got You Here Won't Get You There</a>, and <a href=\"https://www.goodreads.com/book/show/84699.Never_Eat_Alone\">Never Eat Alone</a>).</li><li>Sometimes it is a summary of research, taking years or decades of scientific investigation and making it palatable/digestible to a popular audience (like <a href=\"https://www.goodreads.com/book/show/28815.Influence\">Influence: The Psychology of Persuasion</a>, and <a href=\"https://www.goodreads.com/book/show/12609433-the-power-of-habit\">The Power of Habit: Why We Do What We Do in Life and Business</a>)</li></ul><p>Many of the business books I've read didn't provide me with much value.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrl48rzg5jbq\"><sup><a href=\"#fnrl48rzg5jbq\">[4]</a></sup></span>&nbsp;So what should I do? Well, I'm going to offer my perspectives on some of the most popular and well-known business books so that the people reading this can <strong>avoid the bad ones and steer toward the good ones</strong>. I'll only write a few sentences about each book, but if you have specific questions about any of the books, feel free to let me know and I'll try to elaborate a bit more.</p><p>I'm not being particularly scientific about this. I've found a few lists of \"best business books\" on Goodreads, looked at which ones I've read, and shared my opinions on them.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7t5twwfnysa\"><sup><a href=\"#fn7t5twwfnysa\">[5]</a></sup></span>&nbsp;I'll exclude a few that aren't very related to business<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefse6am84u8f8\"><sup><a href=\"#fnse6am84u8f8\">[6]</a></sup></span>, but I'll generally try to cast a fairly wide net. I also should note that I am not really doing research for these beyond skimming some reviews to refresh my memory, so most of what I write is from memory, from my notes, or from cursory Google searches.</p><p>I hate that I feel the need to include a caveat, but this is the internet, so here it goes: just because I dislike a book and you like it, does not mean that you need to get angry and write impolitely. If you loved a book that I hated, that is okay. If you hated a book that I enjoyed, that is okay, too. If you have useful or interesting perspectives to share, please share them.</p><h1>The Books</h1><h2>Very Good</h2><ul><li>Turn the Ship Around!: A True Story of Turning Followers into Leaders<ul><li>This book is about nurturing initiative in your team, and the system that the author has seems to be pretty good. He went on to make <a href=\"https://www.youtube.com/c/LeadershipNudges/videos?view=0&amp;sort=p&amp;flow=grid\">a bunch of short YouTube videos</a> if you want more exposure to his ideas, but the rough idea is that you should increase the level of control you give to an employee in proportion to the level of competence that the employee has, shifting the employee from someone who says \"tell me what to do\" toward someone that says \"I intend to...\" He has also packaged the concepts in ways that are fairly visual and easily understood, such as a \"ladder of leadership.\" I enjoyed this because it is very much about how a leader\u2019s shaping of the culture affects employee behavior.<ul><li>EDIT: <a href=\"https://www.centreforeffectivealtruism.org/\">CEA</a> uses the phrase \"unless otherwise directed...,\" (shortened to UNODIR) to the same effect as \"I intend to...\"</li></ul></li></ul></li><li>Getting Things Done: The Art of Stress-Free Productivity<ul><li>This is related to any aspect of getting things done in life, not just at work, and is basically a system so that you don't forget tasks. That's it. There are people who are really enthusiastic about GTD and you can easily find summaries online. If you are already well-organized, if you don't get overwhelmed by having multiple distinct places where you receive to-dos, and if having tasks fall through the cracks isn't a problem for you, then you likely don't need this. But I suspect that most people would have less stress and more productivity if they adopted at least some of the GTD practices. The basics are to 1) consolidate everything into a single \"inbox\" (I like Asana and Google Calendar), and 2) make sure that when you <i>process tasks</i> you put tasks into one of the following categories: do, delay, delegate, delete. There is more that you can dig into, but even just these two things can make life easier.</li><li>EDIT: <a href=\"https://docs.google.com/document/d/1BMlExxWtUgMXBNR0Ox4W9wpZW5CJWDe1AtaMMvc6Nv0/edit#\">a great summary is available here</a>.</li></ul></li><li>Thinking, Fast and Slow<ul><li>This isn't so related to business or work life in general. The main reason and I felt justified including it here is that knowing how to avoid or mitigate mistakes is valuable for anyone who makes decisions, and reading this book really helped to humble me. A big part of being a good leader or a good manager is being able to make good decisions. I don't know much about how to train people to make good decisions, but awareness of how easy it is for our brains to fool us has made me a better thinker. Prior to reading this I had the vaguest of inklings about how our brains trick us. Many confident people would benefit from being more skeptical of their own thoughts. People who are in a management/leadership position should especially be more aware of their own failings. This book should help you increase that aspect of self-awareness.</li></ul></li><li>The Advantage: Why Organizational Health Trumps Everything Else in Business<ul><li>This book provides a structure for how to organize your leadership team, and I imagine that it would be quite helpful to anyone who is starting a new organization or who is in charge of a young organization. It is basically a checklist for how to build a team and make sure departments are aligned. I have to admit, that doesn't sound too special. But the structure of an organization is massively impactful in shaping the behavior of employees. If you aren't going to design a team or an organization, then you probably wouldn't get much from this book, and it doesn't really have practical tips outside of the realm of designing and running an organization.</li></ul></li></ul><h2>Good</h2><ul><li>What Got You Here Won't Get You There: How Successful People Become Even More Successful<ul><li>This is targeted at people who want to better themselves, and the author has a list of 20 behavior issues/character flaws that often prevent people from being as successful as they like. Each of those 20 items really could be a little essay on it's own. I view this book as vaguely like coaching for being a better person, with emphasis on professional roles.</li></ul></li><li>High Output Management<ul><li>A decent introduction to operations management, as well as a few principles illustrated by situations from Andy Grove's own career. You won't learn much if you are an experienced manager or if you are well-read, but if you are relatively junior in your career or if you are new to reading business books you might gain some nice perspectives. I especially enjoyed his description of a breakfast restaurant as a nice introduction to the basics of operations management.</li></ul></li><li>First, Break All the Rules: What the World's Greatest Managers Do Differently<ul><li>If you want a highly abridged version of this book, you can read the article <a href=\"https://www.gallup.com/workplace/231593/why-great-managers-rare.aspx\">Why Great Managers Are So Rare</a>.</li><li>It has been several years since I've read this, but my memories are that it refers to actual research, which is a rarity in popular business books. One of the most valuable takeaways from this book has simply been the checklist of 12 questions for evaluating how well a manager is doing his/her job.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref00q3nl9jcxoqp\"><sup><a href=\"#fn00q3nl9jcxoqp\">[7]</a></sup></span>&nbsp;Like any measure, it is subject to being manipulated or gamed or <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">Goodharted</a>, but that even a simple checklist like this is a big improvement over the minimal or non-existent systems for evaluating managers at most of the companies I've observed.</li></ul></li><li>The Goal: A Process of Ongoing Improvement<ul><li>This book falls into the category of parable/allegory/fable: it is a fictional story used to illustrate some business principles. This format is somewhat common among business books, and while I don't enjoy business fables, that it actually works quite well in The Goal. I imagine it would be hard to write a book about operations management without it feeling very dull and textbook-like, but The Goal does a pretty decent job. This book is fairly weak in terms of literature, but very good in terms of feeding operations management concepts in bite-sized chunks. It is considered somewhat of a classic among operations managers, especially for understanding <a href=\"https://en.wikipedia.org/wiki/Theory_of_constraints\"><u>the theory of constraints</u></a>. Due to the accessible style, even if you are only 18 years old and have no work experience, you should still be able to understand most of the ideas in this book. If you are studying business administration, you might end up having this book as assigned reading. In my first full-time white collar job my boss assigned the book to our team, and I am really glad that she did.</li></ul></li><li>The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses<ul><li>My memory is a bit fuzzy on this book, but what I took away from it was the idea that if you are trying to figure out what customers find valuable, you need to iterate a lot, which allows you to learn, and then you can take those learnings and apply them in order to get a better product. A big part of this involves getting feedback from your customers. \"Startup people\"/\"Silicon Valley speak\" refer to this as <i>product-market fit</i>. If you are already familiar with the idea of a minimum viable product, the importance of iteration, and similar ideas, then you probably won't get much from this book. But if you are new to the business world and haven't read much yet, then you might learn some general stuff.</li></ul></li><li>Dan Ariely's books (Predictably Irrational, The Upside of Irrationality, The Honest Truth About Dishonesty)<ul><li>I read all of Dan Ariely's books around 2012-2014 or so (although he has published a few additional books since then) as well as other on what I consider to be part of <a href=\"https://www.goodreads.com/genres/behavioral-economics\">the standard popular behavioral economics reading list</a>, and they lined up closely with my interest in how our brains deceive us. I was surprised to see it on lists of business books, since it is really a psychology book. However, most people need to make decisions in their professional life, and if you are a leader or any type then you are probably expected to have good judgment with your decisions. If you want to make good decisions, then at the very least you should be familiar with some of the basic findings of behavioral economics, and preferably you should read a few books about how we tend to fool ourselves.</li></ul></li></ul><h2>Passable</h2><ul><li>Winning, by Jack Welch<ul><li>The premise of this book is more or less \"I was successful, so here are the things that will make you successful.\" There is sensible advice, but like with so much sensible advice, in order to be useful it has to be true and novel rather than merely true. I'll take candor and lack of candor as an example. This book draws some attention to how lack of candor causes problems, and how candor is beneficial. This is true, but it also strikes me as obvious: in a culture that avoids healthy conflict and where people avoid bringing up bad news, isn't it obvious that this is less effective than candor? Like other \"this is how I did it\" business books, some things may not be applicable in different contexts. To the extent that my working career will closely reflect being a manager at General Electric during the 80s and 90s, there may be a lot of useful insights here. But there is also a lot of content which is basically just backed up by his opinion. <a href=\"https://en.wikipedia.org/wiki/Vitality_curve\">Rank-and-yank performance management</a> is something that I have my own feelings on, but I haven't yet read any research about it, so I can't really say with any confidence to what extent it is or isn't effective.</li></ul></li><li>Influence: The Psychology of Persuasion<ul><li>If you are already decently knowledgeable about behavior economics and the related aspects of psychology, then you might not learn much from this book. It was full of applicable content, and the examples make the content much more vivid. Some of these things might be obvious things that you haven't really thought of before (such as if you give someone a gift, then they are more likely to comply with your subsequent requests). But mostly I view a book like this as drawing our focus toward things that exist all around us that we generally don't pay attention to (power of social proof, believing in commitment because their reputation is on the line, deferring to authority, etc.).</li></ul></li><li>The Essential Drucker<ul><li>If you like Peter Drucker and if you are a nerd for business writing, then this will probably be a good book. It is a bunch of opinions and statements. There isn't really any evidence provided to support these opinions, and the fact that it is basically an anthology of various articles and speeches means that there isn't any strong unifying theme. But Drucker seemed so influential on the practice of management in the 20th century that it is worth being familiar with him.</li></ul></li><li>The 4 Disciplines of Execution: Achieving Your Wildly Important Goals<ul><li>I liked the phrase of \u201cwhirlwind\u201d used to describe the activity and energy that\u2019s necessary to keep the operation going on a day-to-day basis. I also liked the bifurcation of work into <i>whirlwind</i> and <i>important goals</i>. The concepts of acting on lead measures rather than lag measures, of keeping a scoreboard, and of having a cadence are all useful, if a bit obvious. That said, my primary complaints are that<ul><li>if you are an organized person who is able to differentiate between what is important and what isn\u2019t, then you won\u2019t gain much from this book. If you are trained in project management (or if you are naturally the type to do project management), then this book seems superfluous.</li><li>A large proportion of the book seems to consist of proclaiming how great his method is, how Company A and Company B benefited so much from his method, and anecdotes about his method being very important to the success of various people and organizations.</li></ul></li></ul></li><li>Radical Candor: Be a Kickass Boss Without Losing Your Humanity<ul><li>This book didn\u2019t seem particularly radical. It was just basic advice on general management. How to get feedback, how to run a weekly staff meeting, how to do hiring and firing, etc. Most of the advice seemed fairly standard, and I\u2019ve met plenty of people working in positions of authority who would benefit from following the advice. Overall I found it good, but nothing too special.</li></ul></li><li>The Personal MBA: Master the Art of Business<ul><li>The book is a list of concepts and definitions, with a few examples thrown in. If you are new to the world of business and you want to understand some of the terminology, then you might benefit from reading this book. I\u2019ve not done an MBA, nor (at the time of reading this book) had I ever taken any courses in business. I didn\u2019t learn anything from reading this book because the concepts it provided were all things that I had picked up from other books, or from popular press.</li></ul></li><li>Start with Why: How Great Leaders Inspire Everyone to Take Action<ul><li>This book should have been an article. The TED talk was nice, but there simply doesn\u2019t seem to be enough content to justify a book. You don\u2019t get anything more from the book than you do from the TED talk. And the core concepts aren\u2019t particularly complex, either.</li></ul></li><li>Rework<ul><li>This is sort of a manifesto. It is a mixture of the author\u2019s philosophy of how business should be done, with a good portion of his own personal experiences about what worked for him. There is nothing too surprising or amazing; mostly just stuff that I understand to be true. I didn\u2019t gain much, but maybe if I had read it at age 18 I would have.</li></ul></li><li>Drive: The Surprising Truth About What Motivates Us<ul><li>Here is what you will learn from this book:<ul><li>the difference between intrinsic and extrinsic motivation</li><li>the importance of autonomy, mastery, and purpose</li></ul></li><li>That\u2019s it. That\u2019s all there is here. It is just another business book that filters years of other people\u2019s research into broad talking points. The ideas are true and useful, but unless you are brand new to working with people then a lot of it probably will be stuff you already know.</li></ul></li><li>The E-Myth Revisited: Why Most Small Businesses Don't Work and What to Do About It<ul><li>The core idea here is about delegation and documentation. Define a role well and make sure that the standards are clear so that when you start to withdraw the new employee keeps doing the job properly. This seems to me mainly targeted at people who are running their own businesses, but it has a few useful tidbits for anyone who is managing people, a team, a business unit, etc.</li></ul></li><li>James Collins's books (Great by Choice, Built to Last, and Good to Great)<ul><li>I\u2019m clumping these three books together. They sold well and they are smoothly written. There are some nice concepts that are made memorable through description (such as <a href=\"https://www.jimcollins.com/concepts/fire-bullets-then-cannonballs.html\"><i><u>fire bullets, then cannonballs</u></i></a> as a sort of <a href=\"https://www.scotthyoung.com/blog/2019/09/17/explore-exploit/\"><u>explore-exploit</u></a> problem), but similarly to many of the business books that I don\u2019t think are particularly worthwhile, if you\u2019ve done some reading/learning before then it is likely that you won\u2019t gain too much from these.</li></ul></li><li>The Power of Habit: Why We Do What We Do in Life and Business<ul><li>Again, it filters years of other people\u2019s research into broad talking points. There are a lot of anecdotes (which is a common trend in business books). There was a lot of buzz about this book, and I\u2019m guessing that is mainly because it is fairly digestible and because the author is a journalist and writes quite smoothly. But this could have been an article: habits are a loop of cues, routines, and rewards, and you are able to tweak these individual elements.</li></ul></li><li>The 4-Hour Workweek<ul><li>A nice introduction to the concept of automation of a business, and perhaps quite interesting to a person who has never realized that alternatives to the mainstream (full-time job, working 30-60 hours per week) exist. I read this when I was 19 or so, and I thought it was really cool. Now that I am a bit older I am not quite as impressed. Nonetheless, there is some helpful advice about concrete things, such as sending clear if-then emails and not being constantly available for messages.</li></ul></li><li>The Five Dysfunctions of a Team: A Leadership Fable<ul><li>I hate management fables, but this did manage to get the point across. There are communication habits that people have which sabotage their ability to achieve goals. His five dysfunctions are: lack of trust, fear of conflict, lack of commitment, avoidance of accountability, and inattention to results. Perhaps useful if you need to review what kinds of behaviors help or hinder success.</li></ul></li></ul><h2>Bad</h2><ul><li>The 7 Habits of Highly Effective People<ul><li>These ideas strike me as common sense, but they have great marketing and packaging. The ideas themselves aren\u2019t bad. They are quite reasonable. But they aren\u2019t novel either: it seems like a repackaging of the protestant work ethic. There are too many analogies, and the whole book is pervaded with a \u201cdogmatic air of absolute certainty.\u201d I enjoyed <a href=\"https://www.goodreads.com/review/show/191787469\"><u>this review</u></a> of it.</li></ul></li><li>How to Win Friends and Influence People<ul><li>While the core ideas seem to have <i>some </i>merit to them, the presentation is little more than a series of bland anecdotes. I wouldn\u2019t really want to behave in a way that this book describes: it advises you to be agreeable to everyone, to compliment and flatter people very frequently, and generally to act like a people pleaser. I can see how it might be useful for salespeople in certain situations. What is good from it? Well, the idea of \u201cThe only way to get the best of an argument is to avoid it\u201d seems fairly true, but it also seems situational, and fairly obvious.</li></ul></li><li>Never Eat Alone: And Other Secrets to Success, One Relationship at a Time<ul><li>I strongly disliked the tone of this book, and it was a struggle to make myself finish reading it. A large portion of the book seems to consist of the author name-dropping all the famous people he has met. The advice is fairly bland/obvious (throw a dinner party and invite people, then ask people to invite an interesting person they know).</li></ul></li><li>The One Minute Manager<ul><li>This is a simple fable (or maybe an allegory?) about a wise teacher who passes knowledge to a young \u201chero.\u201d The knowledge consists of a series of step for one-minute goal setting, one-minute praising, one-minute reprimand, and so on. It seems to be written at about a 5th grade level.</li></ul></li><li>Delivering Happiness: A Path to Profits, Passion, and Purpose<ul><li>I don\u2019t know why this book got onto lists of best business books. It is just a memoir of a guy who built a business. It is vaguely interesting, but there aren\u2019t really any lessons or takeaways. It is just a guy that happened to do the right things, in the right place, at the right time. There isn\u2019t anything repeatable here, or any transferable lessons.</li></ul></li><li>Lean In: Women, Work, and the Will to Lead<ul><li>This was a type of \"corporate feminism,\" which has an extremely narrow focus, and thereby excludes most women. There were two main things that made me dislike this book. The first is a problem of tone: the author\u2019s blindness to her own privilege. The second is the assumption throughout the whole book that the issues holding women back are primarily individual issues that each woman is responsible for changing on her own rather than structural/systemic issues with how our culture and organizations are formed.</li></ul></li><li>Who Moved My Cheese?<ul><li>This is another business fable. The summary is that people should be ready to change, because (if you are a rat in a maze) you never know when your cheese will be moved. When change happens, you have to be capable of going out and finding new cheese. Described as \u201cthe classic of downsizing propaganda.\u201d</li></ul></li></ul><h1>If you are just starting out</h1><p>If you are new to the professional world and you want to learn a bunch of generally applicable skills, where to start? I started from the <a href=\"https://www.manager-tools.com/all-podcasts?field_content_domain_tid=4\">Manager Tools podcast</a>, first subscribing, then going through their archive to download and listen to anything that looked interesting or relevant.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjvpa49mf7i\"><sup><a href=\"#fnjvpa49mf7i\">[8]</a></sup></span>&nbsp;I spent a year or two listen to Manager Tools on my commute, and it was a pretty good starting point for me. I'm basing this recommendation on my rough perception of what would be most helpful for a college student who hasn't yet had any full-time work experience and who wants to learn about the professional world.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefizyjw80lrmf\"><sup><a href=\"#fnizyjw80lrmf\">[9]</a></sup></span>&nbsp;</p><p>If you have a business book you are considering reading and you aren't sure whether or not it is worth your time, let me know. If it is decently well-known, then there is a good chance that I have read it and I can tell you my impressions of it, including whether or not it would be helpful for you if you tell me about your situation.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz5344sfzxnp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz5344sfzxnp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Whether or not you gain something from reading a business book will depend a lot on what you already know. There are certain books that I would have gained a lot from if I had read them at age 18, which seem so obvious as to be useless at age 35. <a href=\"https://dictionary.cambridge.org/us/dictionary/english/your-mileage-may-vary\">YMMV</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnksk5na140h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefksk5na140h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear, these are just my own casual descriptions off the top of my head, not anything formal or widely used. Books can fall into multiple categories; this is not a <a href=\"https://en.wikipedia.org/wiki/MECE_principle\">MECE</a> categorization.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu3wcqnrutj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu3wcqnrutj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I assume that these authors write an essay/blog post/article, get the idea that they could be a more-or-less bestselling author, and then add lots of fluff to get enough content to justify printing a book. Sometimes the extra examples and anecdotes are helpful (I loved all the examples in <a href=\"https://www.goodreads.com/en/book/show/18114120\">Thanks for the Feedback</a>), but usually they don't add much value.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrl48rzg5jbq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrl48rzg5jbq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There can be many reasons for this, but the most common few are that</p><p>&nbsp;\u2022 I already know the thing the book is trying to teach</p><p>&nbsp;\u2022&nbsp;the book's author falls prey to the fundamental attribution error, claiming that his (it is almost always a written by a man) actions are responsible for his success, ignoring the other factors that contributed to his success</p><p>&nbsp;\u2022&nbsp;the signal to noise ratio is bad: there is some good information, but it is mixed in with a lot of rubbish.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7t5twwfnysa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7t5twwfnysa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear, <strong>this is not a list of what I think the best business books are</strong>. This is a list of my opinions on the books that other people think are the best business books. My list of best business books looks quite different.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnse6am84u8f8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefse6am84u8f8\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.goodreads.com/book/show/10534.The_Art_of_War\">The Art of War</a> shows up on a few lists, but it isn't so relevant to business. <a href=\"https://www.goodreads.com/book/show/1911.The_World_Is_Flat\">The World Is Flat: A Brief History of the Twenty-first Century</a> also showed up on a few, and I don't view it as very relevant.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn00q3nl9jcxoqp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref00q3nl9jcxoqp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The questions can be found on various blogs and articles, but I've put them here for your convenience:<br>1. Do I know what is expected of me at work?<br>2. Do I have the materials and equipment I need to do my work right?<br>3. At work, do I have the opportunity to do what I do best every day?<br>4. In the last seven days, have I received recognition or praise for doing good work?<br>5. Does my supervisor, or someone at work, seem to care about me as a person?<br>6. Is there someone at work who encourages my development?<br>7. At work, do my opinions count?<br>8. Does the mission/purpose of my company make me feel my job is important?<br>9. Are my co-workers committed to doing quality work?<br>10. Do I have a best friend at work?<br>11. In the last six months, has someone at work talked to me about my progress?<br>12. This last year, have I had opportunities at work to learn and grow?</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjvpa49mf7i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjvpa49mf7i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The <a href=\"https://www.manager-tools.com/manager-tools-basics\">Manager Tools Basics</a> are a good place to start, and from there you can use the <a href=\"https://www.manager-tools.com/map-of-the-universe\">Map of the Universe</a> to explore topics.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnizyjw80lrmf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefizyjw80lrmf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There is all kinds of classism wrapped up in this, as you could easily work full-time in a job where many of these ideas are less applicable, especially as a young person without wealth or family connections. I can't think of a label that described the type of work I am talking about without using phrases like \"white collar\" or \"professional class\" or other phrases that are quite loaded with classist meanings. I'm sorry about that. I feel bad that I don't have better words to describe what I want to talk about, but if someone has recommendations for better terminology I'd be very happy to hear them.</p></div></li></ol>", "user": {"username": "jlemien"}}, {"_id": "J6QCmkQmuRaP7skje", "title": "Differential technology development: preprint on the concept", "postedAt": "2022-09-12T13:52:50.852Z", "htmlBody": "<p><strong>Summary</strong></p><p>We have published a&nbsp;<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4213670\"><u>preprint</u></a> of a paper describing the concept of differential technology development and exploring when and how its implementation may be viable. This manuscript has been in the works at FHI for a while, so we would love your feedback!</p><p>Authors: Jonas B. Sandbrink, Hamish Hobbs, Jacob L. Swett, Allan Dafoe, Anders Sandberg</p><p>&nbsp;</p><p><strong>Further details</strong></p><p>Nick Bostrom articulated the concept of Differential Technological Development in Superintelligence. This concept, and the concept of differential progress more broadly, has already been fairly widely discussed and written about, but within a limited community.&nbsp; This includes&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/differential-progress\"><u>here on the EA Forum</u></a>. The concept is typically used to argue that it would be beneficial to accelerate risk-reducing technological progress and retard risk-increasing technological progress, to manage potentially catastrophic or existential technological risks such as those from advanced AI systems and biotechnologies.</p><p>With this paper, we attempt to thoroughly explore the concept, make it accessible to the broader research and policy community, and explore the contexts in which its implementation is likely to be viable. To our knowledge, this is the first academic article seeking to comprehensively articulate a principle of differential technology development.</p><p>In the paper we argue that:</p><ul><li>Responsible innovation efforts to date have largely focussed on shaping individual technologies.&nbsp;</li><li>However, as demonstrated by the preferential advancement of low-emission technologies, certain technologies reduce risks from other technologies or constitute low-risk substitutes.&nbsp;</li><li>Governments and other relevant actors may leverage risk-reducing interactions across technology portfolios to mitigate risks beyond climate change.&nbsp;</li><li>We adapt Nick Bostrom\u2019s original concept to propose a responsible innovation principle of \u201cdifferential technology development\u201d, which calls relevant actors to leverage risk-reducing interactions between technologies by affecting their relative timing.&nbsp;</li><li>Thus, it may be beneficial to delay risk-increasing technologies and preferentially advance risk-reducing defensive, safety, or substitute technologies.&nbsp;</li><li>Implementing differential technology development requires the ability to anticipate or identify impacts and intervene in the relative timing of technologies.&nbsp;</li><li>We find that both are sometimes viable and that differential technology development may still be usefully applied even late in the diffusion of some harmful technologies.&nbsp;</li><li>A principle of differential technology development may inform government research funding priorities and technology regulation, as well as philanthropic research and development funders and corporate social responsibility measures.&nbsp;</li><li>Differential technology development may be particularly promising to mitigate potential catastrophic risks from emerging technologies like synthetic biology and artificial intelligence.</li></ul><p>While writing the paper, we considered alternative terms to describe the concept of differential technology development. Our leading alternatives were \u201cresponsible innovation sequencing\u201d or \u201cdifferential progress\u201d. However, in the end we decided that differential technological development was the best fit for integrating this article into the responsible innovation literature. That said, we do see merit to either defining technology broadly to include \u201cnot only gadgets but also methods, techniques and institution design principles\u201d or to extending the concept to include forms of differential development beyond technology development.&nbsp;</p><p>Amongst other things, paper outlines four relevant categories of technologies:</p><ul><li><strong>Risk increasing technologies</strong> cause negative societal impacts by causing insidious harm or through their potential to cause a catastrophe.</li><li><strong>Safety technologies</strong> reduce or prevent negative societal impacts by modifying risk-increasing technologies.</li><li><strong>Defensive technologies</strong> decrease risks from risk-increasing technologies without modifying these technologies.</li><li><strong>Substitute technologies</strong> achieve similar benefits to a risk-increasing technology while featuring less risk.</li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995129/mirroredImages/J6QCmkQmuRaP7skje/fugb6itkjbch4whqxsio.png\"></p><p>We hope that this paper can provide a useful framing for discussions and future research about differential technology development, as well as generating ideas for how it may be best implemented in practice.</p><p>&nbsp;</p><p><strong>Preprint link:</strong> https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4213670</p><p>&nbsp;</p><p><strong>Acknowledgements</strong></p><p>We are grateful to Michael Aird, Markus Anderljung, Jan Ole Ernst, Ben Garfinkel, Sihao Huang, Matthijs Maas, Cassidy Nelson, and James Wagstaff for useful discussions and comments on the manuscript. Furthermore, we are also grateful for feedback from participants of work-in-progress meetings of the Future of Humanity Institute and Centre for the Governance of AI. We thank Shrestha Rath for help with formatting and organising references. Jonas B. Sandbrink\u2019s doctoral research is funded by Open Philanthropy. Hamish Hobbs\u2019 contribution to the paper largely occurred while funded as a Research Scholar at the Future of Humanity Institute.<br>&nbsp;</p>", "user": {"username": "Hamish_Hobbs"}}, {"_id": "EPhDMkovGquHtFq3h", "title": "An experiment eliciting relative estimates for Open Philanthropy\u2019s 2018 AI safety grants", "postedAt": "2022-09-12T11:19:20.383Z", "htmlBody": "<h2>Summary</h2><p>I present the design and results of an experiment eliciting relative values from six different researchers for the nine large AI safety grants Open Philanthropy made in 2018.&nbsp;</p><p>The specific elicitation procedures I used might be usable for&nbsp;<strong>rapid evaluation setups</strong>, for going from zero to some evaluation, or for identifying disagreements. For weighty decisions, I would recommend more time-intensive approaches, like explicitly modelling the pathways to impact.</p><h2>Background and motivation</h2><p>This experiment follows up on past work around relative values (<a href=\"https://forum.effectivealtruism.org/posts/9hQFfmbEiAoodstDA/simple-comparison-polling-to-create-utility-functions\"><u>1</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hrdxf5qdKmCZNWTvs/valuing-research-works-by-eliciting-comparisons-from-ea\"><u>2</u></a>,&nbsp;<a href=\"https://utility-function-extractor.quantifieduncertainty.org/\"><u>3</u></a>) and more generally on work to better&nbsp;<a href=\"https://forum.effectivealtruism.org/s/AbrRsXM2PrCrPShuZ\"><u>estimate values</u></a>. The aim of this research direction is to explore a possibly scalable way of producing estimates and evaluations. If successful, this would bring utilitarianism and/or longtermism closer to producing practical guidance around more topics, which has been a recurring thread in my work in the last few years.&nbsp;</p><h2>Methodology</h2><p>My methodology was as follows:</p><ol><li>I selected a group of participants whose judgment I consider to be good.</li><li>I selected a number of grants which I thought would be suitable for testing purposes.</li><li>Participants familiarized themselves with the grants and with what exactly they ought to be estimating.</li><li>Participants made their own initial estimates using two different methods:<ol><li>Method 1: Using a utility function extractor app.</li><li>Method 2: Making a \u201chierarchical tree\u201d of estimates.</li></ol></li><li>For each participant, I aggregated and/or showed their two estimates side by side, and asked them to make a best guess estimate.</li><li>I took their best guess estimates, and held a discussion going through each grant, making participants discuss their viewpoints when they had some disagreements.</li><li>After holding the discussion, I asked participants to make new estimates.</li></ol><p>Overall, the participants took about&nbsp;<a href=\"https://www.squiggle-language.com/playground#code=eNqrVirOyC8PLs3NTSyqVLIqKSpN1QELuaZkluQXwUQy8zJLMhNzggtLM9PTc1KDS4oy89KVrJQ0NAwNFEryFYwNNBW0FTSMwBwTCMcUIWFmgCygqW9moFQLAMYeI4o%3D\"><u>two to three hours</u></a> each to complete this process, roughly divided as follows:</p><ol><li>10 to 30 mins to familiarize themselves with the estimation target and to re-familiarize themselves with the grants</li><li>20 to 40 mins to do the two initial estimates</li><li>5 to 30 mins to give their first best guess estimate after seeing the result of the two different methods</li><li>1h to hold a discussion</li><li>5 to 30 mins to give their resulting best guess estimate&nbsp;</li></ol><p>The rest of this section goes through these steps individually.</p><h3>Selection of participants</h3><p>I selected participants by asking friends or colleagues whose judgment I trust, and who had some expertise or knowledge of AI safety. In particular, I selected participants who would be somewhat familiar with Open Philanthropy grants, because otherwise the time required for research would have been too onerous.</p><p>The participants were Gavin Leech, Misha Yagudin, Ozzie Gooen, Jaime Sevilla, Daniel Filan and another participant who prefers to remain anonymous. Note that one participant didn\u2019t participate in all the rounds, which is why some summaries contain only five datapoints.&nbsp;</p><h3>Selection of grants</h3><p>The grants I selected were:</p><ul><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-impacts-general-support-2018\"><u>AI Impacts \u2014 General Support (2018)</u></a>: $100,000</li><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/machine-intelligence-research-institute-ai-safety-retraining-program\"><u>Machine Intelligence Research Institute \u2014 AI Safety Retraining Program</u></a>: $150,000</li><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-phil-ai-fellowship-2018-class\"><u>Open Phil AI Fellowship \u2014 2018 Class</u></a>: $1,135,000</li><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ought-general-support\"><u>Ought \u2014 General Support (2018)</u></a>: $525,000</li><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/oxford-university-global-politics-of-ai-dafoe\"><u>Oxford University \u2014 Research on the Global Politics of AI</u></a>: $429,770</li><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/stanford-university-machine-learning-security-research-dan-boneh-florian-tramer\"><u>Stanford University \u2014 Machine Learning Security Research Led by Dan Boneh and Florian Tramer</u></a>: $100,000</li><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/university-of-california-berkeley-artificial-intelligence-safety-research-2018\"><u>UC Berkeley \u2014 AI Safety Research (2018)</u></a>: $1,145,000</li><li><a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/wilson-center-ai-policy-seminar-series\"><u>Wilson Center \u2014 AI Policy Seminar Series</u></a>: $400,000</li></ul><p>These are all the grants that Open Philanthropy made to reduce AI risk in 2018 above a threshold of $10k, according to their&nbsp;<a href=\"https://www.openphilanthropy.org/grants/?q=&amp;focus-area=potential-risks-advanced-ai&amp;yr=2018\"><u>database</u></a>. The year these grants were made is long enough ago that we have some information about their success.</p><p>I shared a&nbsp;<a href=\"https://docs.google.com/document/d/1sTCwFUA7_G46YzUp4p4U_OvpYd9tdmq7D8IRdL63BeA/edit#heading=h.tf7bismm62hi\"><u>briefing</u></a> with the participants summarizing the nine Open Philanthropy grants above, with the idea that it might speed the process along.&nbsp;</p><p>In hindsight, this was suboptimal, and might have led to some anchoring bias. Some participants complained that the summaries had some subjective component. These participants said they used the source links but did not pay that much attention to these opinions.</p><p>On the other hand, other participants said they found the subjective estimates useful. And because the briefing was written in good faith, I am personally not particularly worried about it. Even if there are anchoring issues, we may not necessarily care about it if we think that the output is accurate, in the same way that we may not care about forecasters anchoring on the base rate.</p><p>If I were redoing this experiment, I would probably limit myself even more to expressing only factual claims and finding sources. A better scheme may have been share a&nbsp;writeup with a minimal subjective component, then&nbsp; strongly encouraging participants to make their own judgments before looking at a separate writeup with more subjective summaries, which they can optionally use to adjust their estimates</p><h3>Estimation target</h3><p>I asked participants to estimate \u201c<i>the probability distribution of the relative ex-post counterfactual values of Open Philanthropy\u2019s grants\u201d</i>.&nbsp;</p><ul><li><i><strong>the distribution</strong></i>: inputs are distributions, using Guesstimate-like syntax, like \u201c1 to 10\u201d, which represents a lognormal distribution with its 90% confidence interval ranging from 1 to 10.</li><li>estimates are&nbsp;<i><strong>relative</strong></i>: we don\u2019t necessarily have an absolute set comparison point, like percentage points of reduction in x-risk. This means that estimates were expressed in the form \u201cgrant A is x to y times more valuable than grant B\u201d.</li><li>estimates are&nbsp;<i><strong>ex-post</strong></i> (after the fact) because estimating ex-ante expected values of something that already has happened is a) more complicated, and b) amenable to falling prey to hindsight bias.</li><li>estimates are of the&nbsp;<i><strong>counterfactual value</strong></i> because estimating the Shapley value is a headache. And if we want to arrive at cost-effectiveness, we can just divide by the grant cost, which is known.</li><li>estimates are about the value&nbsp;<i><strong>of the grants</strong></i>, as opposed to the value of the projects, because some of the projects could have gotten funding elsewhere. And so the value of the grants might be small, lie in OpenPhil acquiring influence, or have more to do with seeding a field than with the project themselves.&nbsp;</li></ul><p>More detailed instructions to participants can be seen&nbsp;<a href=\"https://docs.google.com/document/d/1VNnFtKKoMqJcqMD_4XFy9-86LJkv3p_rIGF3VDWRTac/edit#\"><u>here</u></a>. In elicitation setups such as this, I think that specifying the exact subject of discussion is valuable, so that participants are talking about the same thing.</p><p>Still, there were some things I wasn\u2019t explicit about:</p><ul><li>Participants were not intended to consider the counterfactual cost of capital. So for example, a neutral grant that didn\u2019t have further effects on the world should have been rated as having a value of 0. However, I wasn\u2019t particularly explicit about this, so it\u2019s possible that participants were thinking something else.</li><li>I don\u2019t remember being clear about whether participants should have estimated relative values or relative&nbsp;<i>expected</i> values. Looking at the intervals below, they are pretty narrow, which might be explained by participants thinking about expected value instead.</li></ul><h3>Elicitation method #1: Utility function extractor application</h3><p>The first method was a \u201cutility function extractor\u201d, the app for which can be found&nbsp;<a href=\"https://utility-function-extractor.quantifieduncertainty.org/\"><u>here</u></a>. The idea here is to make possibly inconsistent pairwise comparisons between pairs of grants, and extract a utility function from this. Past prior work and explanations can be found&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/hrdxf5qdKmCZNWTvs/valuing-research-works-by-eliciting-comparisons-from-ea\"><u>here</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9hQFfmbEiAoodstDA/simple-comparison-polling-to-create-utility-functions\"><u>here</u></a>.&nbsp;</p><p>An example of the results for one user looks like this:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995028/mirroredImages/EPhDMkovGquHtFq3h/ditolzx8q5duf7unwzhf.png\"></p><p>I first processed each participant\u2019s utility function extractor results into a table like this one:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/j9rwdzjowapac9tzswts.png\"></p><p>and then processed it into proper distributional aggregates using&nbsp;<a href=\"https://github.com/quantified-uncertainty/utility-function-extractor/tree/master/packages/utility-tools\"><u>this package</u></a>. One difficulty I ran into is that I didn\u2019t consider that some of the estimates could be negative, because I was using the geometric mean as an aggregation method. This wrought havoc into distributional aggregates, particularly when some of the estimates for one particular element were sometimes positive and sometimes negative.</p><h3>Elicitation method #2: Hierarchical tree estimates</h3><p>The second method involved creating a hierarchical tree of estimates, using&nbsp;<a href=\"https://observablehq.com/@nunosempere/relative-value-comparisons-within-clusters-public\"><u>this Observable document</u></a>. The idea here is to express relationships between the grants using a \u201chierarchical model\u201d, where grants belonging to a category are compared to a reference grant, and reference grants are then compared to a greater reference element (\u201cone year of Paul Christiano's work\u201d).</p><p>The interface I asked participants to use looked as follows:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/q7a4flbpx1htfn8gpcwn.png\"></p><p>A participant mentioned that this part was painful to fill. Using a visualization scheme which the participants didn\u2019t have access to at the time, participant results can be displayed as:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/zbvqlmm0fhpaqm9ow33a.png\"></p><p>In this case, the top-most element is \u201cpercentage reduction in x-risk\u201d. I asked some participants for their best guess for this number, and the one displayed gave 0.03% per year of Paul Christiano\u2019s work.</p><h3>Elicitation method #3:&nbsp; Individual aggregate estimates</h3><p>After presenting participants with their estimates from the two different methods, I asked the participants to give their pointwise first guesses after reflection. Their answers, normalized to add up to 100, can be summarized as follows:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/hdpief7c6bjuaw4br5cl.png\"><figcaption>Researcher #6 only reported his estimates using one method (the utility function extractor), and then participated on the discussion round, which is why he isn\u2019t shown in this table.</figcaption></figure><p>So for example, researcher #4 is saying that the first grant, to research on the Global Politics of AI at the University of Oxford (GovAI), was the most valuable grant. In particular, the estimate is saying that it has 71% of the total value of the grants. The estimate is also saying that the grant to GovAI is 71/21.2 = 3.3 times as valuable as the next most valuable grant, to Michael Cohen and Dmitri Krasheninnikov.</p><h3>Elicitation method #4: Discussion and new individual estimates</h3><p>After holding a discussion round for an hour, participants\u2019 estimates shifted to the following<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpmfo0q7i4di\"><sup><a href=\"#fnpmfo0q7i4di\">[1]</a></sup></span>:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/1be79cebdb6804692bc906b81530ff838ccf95bfef48ca44.png/w_1364 1364w\"></figure><p>To elicit these estimates, I asked participants to divide approximately 100 units of value between the different grants. Some participants found this elicitation method more convenient and less painful than the previous pairwise comparisons.&nbsp;</p><h2>Observations and reflections</h2><h3>Initial estimates from the same researcher using two different methods did not tend to overlap</h3><p>Consider two estimates, expressed as 90% confidence intervals:</p><ul><li>10 to 100</li><li>500 to 1000</li></ul><p>These estimates do not overlap. That is, the highest end of the first estimate is smaller than the lower end of the second estimate.</p><p>When analyzing the results, I was very surprised to see that in many cases, estimates made by the same participant about the same grant using the first two methods\u2014the utility function extractor and hierarchical tree\u2014did not overlap:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/knhj1ma2nolswdilfjxd.png\"></p><p>In the table above, for example, the first light red \u201cFALSE\u201d square under \u201cResearcher 1\u201d and to the side of \u201cOxford University\u2026\u201d indicates that the 90% estimates initial produced by researcher 1 about that grant do not overlap.</p><h3>Estimates between participants after holding a discussion round were mostly in agreement</h3><p>The final estimates made by the participants after discussion were fairly concordant<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqbjzronh3oi\"><sup><a href=\"#fnqbjzronh3oi\">[2]</a></sup></span>:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/4c11643ddec43ac73c480a75c6ebb57fdf93df3258ecc303.png/w_1364 1364w\"></figure><p>For instance, if we look at the first row, the 90% confidence intervals<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefacizl98aof\"><sup><a href=\"#fnacizl98aof\">[3]</a></sup></span>&nbsp;of the normalized estimates are 0.1 to 1000, 48 to 90, -16 to 54, 41 to 124, 23 to 233, and 20 to 180. These all overlap! If we visualize these 90% confidence intervals as lognormals or loguniforms, they would look as follows<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefclvpudp11e\"><sup><a href=\"#fnclvpudp11e\">[4]</a></sup></span>:&nbsp;</p><p><i><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/tgxf2ui1jfkxvmegb0al.png\"></i></p><h3>Discussion of the shape of the results</h3><p>Many researchers assigned most of the expected impact to one grant, similar to a power law or an 80/20 Pareto distribution, though a bit flatter. There was a tail of grants widely perceived to be close to worthless. There was also disagreement about the extent to which grants could have negative value.</p><p>The estimates generally seem to me to have been too narrow. In many cases they span merely an order of magnitude. This can maybe be partially explained by some ambiguity about whether participants were estimating relative expected values or the actual values.</p><h3>Thoughts on accuracy</h3><p>The fact that the estimates end up clustering together could be a result of:</p><ul><li>Participants rationally coming to agree as a result of acquiring the same knowledge.</li><li>Social pressure, group-think, human biases, or other effects. Not all of these might be negative: for example, if the group correctly identifies the most knowledgeable person about each grant and then defers to them, this could make the estimates better.</li></ul><p>Overall I think that convergence is a weak and positive signal of accuracy. For example, per Aumann\u2019s agreement theorem, participants shouldn\u2019t expect to \u201cagree to disagree\u201d, so to the extent that irrational disagreement is not happening, convergence is good.</p><p>One way to find out whether this aggregate is converging to something like the truth would be to have a separate group, or a separate person known to have good judgment, make their own estimates independently, and then compare them with these estimates. This would require an additional time investment.</p><h3>What was the role of Squiggle?</h3><p>I used Squiggle in the utility function extractor and in the hierarchical method, interpreting distributions using Squiggle syntax. I then also used it for aggregating the estimates, both to aggregate the many estimates made by one participant, and to arrive at an aggregate of all participants\u2019 estimates.</p><h3>Thoughts on scaling up this type of estimation up</h3><p>I\u2019m estimating that the experiment took 20 to 40 hours:</p><pre><code>hours_per_participant = 2 to 5\nparticipants = 5 to 6\nparticipant_hours = hours_per_participant * participants\norganizer_hours = (2 to 4) + (2)&nbsp; + (0.3 to 2) + (4 to 15) + (0.2 to 0.5) // preparation + hosting + nagging + writeup + paying\nparticipant_hours + organizer_hours</code></pre><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/icrkvfpigftczjhchqjo.png\"></p><p>So for 9 grants, this is 2.6 to 4.9 hours per grant. Perhaps continued investment could bring this down to one hour per grant. I also think that time might scale roughly linearly with the number of grants, because grants can be divided into buckets, and then we can apply the relative value method to each bucket. Then we can compare buckets at a small additional cost\u2014e.g., by comparing the best grants from each bucket.</p><p>I\u2019m not actually sure how many grants the EA ecosystem has, but I\u2019m guessing something like 300 to 1000 grants per year<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6fjejaxnj27\"><sup><a href=\"#fn6fjejaxnj27\">[5]</a></sup></span>. Given this, it would take half to two FTEs (full-time equivalents) to evaluate all grants, which was lower than I suspected:</p><pre><code>hours_per_participant = 2 to 5\nparticipants = 5 to 6\nparticipant_hours = hours_per_participant * participants\norganizer_hours = (2 to 4) + (2)&nbsp; + (0.3 to 2) + (4 to 15) + (0.2 to 0.5) // preparation + hosting + nagging + writeup + paying\n\nhours_per_grant = (participant_hours + organizer_hours) /&nbsp; 9\ngrants_per_year = 300 to 1000\nhours_per_person_per_year = (30 to 50) * 52\nftes_to_evaluate_all_grants = grants_per_year * hours_per_grant / hours_per_person_per_year\n\nftes_to_evaluate_all_grants\n\n</code></pre><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/irrgfe9ytdt1jmliryt0.png\"></p><p>~1 FTE per year seems low enough that it seems doable. However, note that this would be spread amongst many people, which would have extra costs, because of attention/context-switching and coordination costs.</p><h3>Relative estimates as an elicitation method vs as an output format</h3><p>There is a difference between relative estimates as an elicitation method (as presented here) and relative estimates as an output format (where we have the relative values of projects, and transformations between these and reference units, like QALYs, fractions of the future, etc.)\\. It\u2019s possible that relative values as an output format remain promising even as relative values as a (rapid) elicitation method remain less so.&nbsp;</p><h3>Relative estimates of value seem a bit more resilient to shifts in what we care about</h3><p>One advantage of relative values as a format might be that they are more resilient to shifts in what we care about (sometimes called \u201c<a href=\"https://www.lesswrong.com/tag/ontological-crisis\"><u>ontological crisis</u></a>\u201d). Thanks to Ozzie Gooen for this point. For instance, raw estimates of value may change as we switch from DALYs, to QALYs, to fractions of the future, to other units, or as we realize that the future is larger or smaller than we thought. But relative values would perhaps remain more stable.</p><h3>Thoughts on alternative value estimation methods</h3><p>The main alternative to relative values that I\u2019m considering is estimates made directly in a unit of interest, such as percentage or basis points of existential risk reduction, or QALYs saved. In particular, I\u2019m thinking of setups which decompose impact into various steps and then estimate the value or probability of each step.</p><p><strong>A concrete example</strong></p><p>For instance, per&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact#Prioritization_and_Theory_of_Impact\"><u>AI Governance: Opportunity and Theory of Impact</u></a>, the pathway to impact for the GovAI center would be something like this:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/ngunxtsdyblfyqqdcvjv.png\"><figcaption>Higher quality image <a href=\"https://i.imgur.com/CS1mg13.png\">here</a>.</figcaption></figure><p>Giving some&nbsp;<i>very</i> quick numbers to this, say:</p><ul><li>a 12% chance of AGI being built before 2030,&nbsp;</li><li>a 30% of it being built in Britain by then if so,</li><li>a 90% of it being built by DeepMind if so,&nbsp;</li><li>an initial 50% chance of it going well if so</li><li>GovAI efforts shift the probability of it going well from 50% to 55%.&nbsp;</li></ul><p>Punching those numbers into a calculator, a rough estimate is that GovAI reduces existential risk by around 0.081%, or 8.1&nbsp;<a href=\"https://en.wikipedia.org/wiki/Basis_point\"><u>basis points</u></a>.&nbsp;</p><p>The key number here is the 5% improvement (from 50% to 55%). I\u2019m getting this estimate mostly because I think that Allan Dafoe being the \u201cHead of Long-term Strategy and Governance\u201d at DeepMind seems like a promising signal. It nicely corresponds to the \u201chaving people in places to implement safety strategies\u201d part of GovAI\u2019s pathway to impact. But that estimation strategy is very crude, and I could imagine a better estimate ranging from &lt;0.5% to more than 5%.</p><p>To avoid the class of problems around using point estimates rather than distributions that&nbsp;<a href=\"https://arxiv.org/abs/1806.02404\"><u>Dissolving the Fermi Paradox</u></a> points out, we can rewrite these point estimates into distributional probabilities:</p><pre><code>t(d) = truncateLeft(truncateRight(d, 1), 0)\nagi_before_2030 = t(0.01 to 0.3) // should really be using beta distributions, though\nagi_in_britain_if_agi_before_2030 = t(0.1 to 0.5)\nagi_by_deepmind_if_agi_in_britain = t(0.8 to 1)\nincreased_survival_probability = t(0.001 to 0.1) // changed my mind while putting a distributional estimate\nvalue_of_govai = t(agi_before_2030 * agi_in_britain_if_agi_before_2030 * agi_by_deepmind_if_agi_in_britain * increased_survival_probability)\nvalue_of_govai_in_percentage_points = value_of_govai * 100\nvalue_of_govai_in_percentage_points\n\n<img src=\"http://res.cloudinary.com/cea/image/upload/v1667995027/mirroredImages/EPhDMkovGquHtFq3h/uagwwuiu2w5e08nexzz9.png\"></code></pre><p>This produces an estimate of 0.52% of the future, or 52 basis points, which is around 6x higher than our initial estimate of 8.1 basis points. But we shouldn\u2019t be particularly surprised to see these estimates vary by ~1 order of magnitude.</p><p>We could make a more granular estimate by thinking about how many people would be involved in that decision, how many would have been influenced by GovAI, etc.&nbsp;</p><p>In any case, in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cKPkimztzKoCkZ75r/how-many-ea-2021-usds-would-you-trade-off-against-a-0-01\"><u>this post</u></a>, Linch estimates that we should be prepared to pay&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cKPkimztzKoCkZ75r/how-many-ea-2021-usds-would-you-trade-off-against-a-0-01?commentId=ooEuFiZKQwwacb7MJ\"><u>$100M to $1B</u></a> for a 0.01% reduction in existential risk, or $7.2B to $72B for the existential risk reduction of 0.72% that I quickly estimated GovAI to produce. Because GovAI\u2019s budget is much lower, it seems like an outstanding opportunity, conditional on that estimate being correct.</p><p><strong>How does that example differ from the relative estimates method?</strong></p><p>In this case, both the relative values method and the explicit pathway to impact method end up concluding that GovAI is an outstanding opportunity, but the explicit estimate method seems much more legible, because its moving parts are explicit and thus can more easily be scrutinized and challenged.&nbsp;</p><p>Note that GovAI has a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact#Prioritization_and_Theory_of_Impact\"><u>very clearly written</u></a> explanation of its theory of impact, which other interventions may not have. And producing a clear theory of impact, of the sort which could be used for estimation, might be too time-consuming for any given small grant. But I am optimistic that we could have templates which we could then reuse.</p><h3>Future work</h3><p>Future work directions might involve:</p><ul><li>Finding more convenient and scalable ways to produce these kinds of estimates</li><li>Finding better ways to&nbsp;<strong>visualize</strong>, present and interrogate these estimates</li><li>Checking whether these estimates align with expert intuition</li><li>Applying these estimation methods to regimes where there was previously very estimation being done</li><li>Further experimenting with more in-depth and high-quality estimation methods than the one used here</li><li>Using relative estimates as a way to identify disagreements</li></ul><p>I still think relative values are meaningful for creating units, such as \u201cquality-adjusted sentient life year\u201d. But otherwise, I\u2019m most excited about purely relative estimates as a better method for aiding relatively low-level decisions, and estimates based on the pathway to impact as a more expensive estimation option for more important decisions.</p><p>One reason for this view is that I have become more convinced that direct estimates of variables of interest (like basis points of existential risk reduction) can be meaningfully estimated, although at some expense. Previously, I thought that producing endline estimates might end up being too expensive.</p><p>It\u2019s possible that relative value estimates could also be used for other use cases, such as to create evaluations of grants in cases where there previously were none, or to align the intuitions of senior and junior grantmakers. But I don\u2019t consider this particularly likely, maybe because people who could be doing this kind of thing would have more valuable projects to implement.</p><h2>Acknowledgements</h2><figure class=\"image image_resized\" style=\"width:21.74%\"><img src=\"https://i.imgur.com/7yuRrge.png\"></figure><p>Thanks to Gavin Leech, Misha Yagudin, Ozzie Gooen, Jaime Sevilla, Daniel Filan and another other anonymous participant for participating in this experiment. Thanks to them and to Eli Lifland for their comments and suggestions throughout and afterwards. Thanks to Hauke Hillebrandt, Ozzie Gooen and Nick Beckstead for encouragement around this research direction.</p><p>This post is a project by the<a href=\"https://quantifieduncertainty.org/\"><u> Quantified Uncertainty Research Institute</u></a> (QURI). The language used to express probability distributions used throughout the post is&nbsp;<a href=\"https://www.squiggle-language.com/\"><u>Squiggle,</u></a> which is being developed by QURI.</p><h2>Appendix: More details</h2><p>You can find more detailed estimates in&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1ivaFYy_07X5JDZ0qj8kkgTr790gPDJ8Asc2I7UVLjHA/edit?usp=sharing\"><u>this Google Sheet</u></a>. For each participant, their sheet shows:</p><ul><li>The results for each method</li><li>The results for an aggregate of both methods</li><li>The best guess of the participant after seeing the results for each method and an aggregate</li><li>The best guess of the participant after discussing with other participants</li></ul><p>You can also find more detailed aggregates in&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/13inKETvESvcOu8UX2uyM7nlUvUNbECEugt3ec_YqnoY/edit#gid=253364323\"><u>this Google Sheet</u></a>, which include the individual distributions and the medians in the table in the last section.</p><p>Note that there are various methodological inelegancies:</p><ul><li>Researcher #2 did not participate in the discussion, and only read the notes</li><li>Researcher #6 only used the utility function extractor method</li><li>Various researchers at times gave idiosyncratic estimate types, like 80% confidence intervals, or medians instead of distributions.</li></ul><p>In part because the initial estimates were not congruent, I procrastinated in hosting the discussion session, which was held around a month after the initial experiment, if I recall correctly. If I were redoing the experiment, I would hold the different parts of this experiment closer together.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpmfo0q7i4di\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpmfo0q7i4di\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that in the first case, I am displaying the mean, and in the other, the medians. This is because a) means of very wide distributions are fairly counterintuitive, and in various occasions, I don't think that participants thought much about this, and b) because of a methodological accident, participants provided means in the first case and medians in the second.<br><br>Note also&nbsp;that medians are a pretty terrible aggregation method.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqbjzronh3oi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqbjzronh3oi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that the distributions aren't necessarily lognormally distributed, hence why the medians may look off. See <a href=\"https://docs.google.com/spreadsheets/d/13inKETvESvcOu8UX2uyM7nlUvUNbECEugt3ec_YqnoY/edit?usp=sharing\">this spreadsheet</a> for details.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnacizl98aof\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefacizl98aof\">^</a></strong></sup></span><div class=\"footnote-content\"><p>80% for researcher #5, because of idiosyncratic reasons.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnclvpudp11e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefclvpudp11e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Squiggle model <a href=\"https://www.squiggle-language.com/playground/#code=eNqdkMFOwzAQRH9l5VMiBZQ4BRVLHPmCHDGKAnWTFYkNa5sWRfl34gJqi5Dcdk6r8WqfZ0ZmO7Op%2FDA09MmEI6%2BynfWwQmfo10GNDpu%2BevfYtr2qHKFumWArtPP47B0abWteO1Nb3MI9jFLDrKN3AY9Sf%2FtB434M0s2gBEhGyqqGXjpF4DZGsgyO9w5PClgswRm4y%2Fc7U3YWoiOlYpCr4jZQbhaXUtbGUzRJERgFvxgyFx9j8HzHWP6p66wo%2BBHti5cBw8vy%2Fyinw4yOstT2LfEa14aGpDdtkl8XaQZhKvLXNE0PvvBz50nqKWRm0xfkbtQi\">here</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6fjejaxnj27\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6fjejaxnj27\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Open Philanthropy grants for 2021: 216, Long-term future fund grants for 2021: 46, FTX Future fund public grants and regrants: 113 so far, so an expected ~170 by the end of the year. In total this is 375 grants, and I'd wager it will be growing year by year.</p></div></li></ol>", "user": {"username": "NunoSempere"}}, {"_id": "GxiebzonnBAijpcEm", "title": "Key takeaways from Famine, Affluence, and Morality", "postedAt": "2022-09-14T21:02:02.747Z", "htmlBody": "<p>The goal of this post is not to summarize <a href=\"https://www.amazon.com/Famine-Affluence-Morality-Peter-Singer/dp/0190219203/ref=sr_1_1?crid=2BIWT3L2UMABC&amp;keywords=famine%2C+affluence+and+morality&amp;qid=1661121886&amp;sprefix=famine%2C+afflu%2Caps%2C290&amp;sr=8-1\">\"Famine, Affluence, and Morality\"</a> by Peter Singer as a whole but to underline key lessons and perspectives from the essay that I consider relevant to current effective altruist efforts.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq1wz46dzm2\"><sup><a href=\"#fnq1wz46dzm2\">[1]</a></sup></span>&nbsp;As one of the foremost foundational works to inspire effective altruism, I intend for these explanations to provide someone without much prior knowledge of Singer or effective altruism to enter into the material and its implications. I highly encourage reading <a href=\"https://personal.lse.ac.uk/robert49/teaching/mm/articles/Singer_1972Famine.pdf\">the whole essay</a> if you haven't before (it's both brief and relatively easy to understand).</p><p>If there are concepts I did not include that you think would be beneficial to someone who is new to Singer, please put them in the comments!</p><h1>tl;dr</h1><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_170 170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_340 340w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_510 510w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_680 680w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_850 850w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_1020 1020w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_1190 1190w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_1360 1360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_1530 1530w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/40a49187db4aeaddbc9cd84b8d82bb0ad9b99614d0aa6326.png/w_1640 1640w\"></figure><p>Singer encourages these particular mindsets and ideas in this landmark essay:</p><ul><li>Choose the rational perspective when you're looking at a problem and try to counteract biases you have towards those close to you or problems you're most familiar with</li><li>Assess your ability to help as accurately as possible and consider marginal benefits in situations where you might help</li><li>Consider people equally across distance and geography</li><li>Don't be a bystander; actively help the problems you see and create a moral norm of generosity</li><li>Pursue actions that will be in harmony with the philosophies you hold</li></ul><h1>Context</h1><p>Peter Singer is a moral philosopher and professor of applied ethics best known for his writing on animal welfare and global poverty.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefymtdnw5xck9\"><sup><a href=\"#fnymtdnw5xck9\">[2]</a></sup></span>&nbsp;In 1971, a refugee crisis caused by military activity in East Pakistan prompted Singer to consider the lack of help from affluent nations to those in need in the situation.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrqu9bv3icyr\"><sup><a href=\"#fnrqu9bv3icyr\">[3]</a></sup></span>&nbsp;The essay posits two main arguments: that those in relatively affluent countries should feel a moral obligation to help those in need around the world; and that \"if it is in our power to prevent something bad from happening, without thereby sacrificing anything of comparable moral importance, we ought morally, to do it.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefiohic7aiwah\"><sup><a href=\"#fniohic7aiwah\">[4]</a></sup></span></p><p>On Twitter or in other spaces of effective altruism, you may have seen references to the analogy of saving a child drowning in a pond. In \"Famine, Affluence, and Morality,\" Singer likens the moral obligation to try to save a child you can see drowning in a pond to the moral obligation to save a life through effective giving\u2014regardless of the person's geographical proximity to you. Singer uses this analogy to explore and explain the concepts I detail below.</p><h1>Key lessons and perspectives</h1><h2>Choose the right mode</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_1920 1920w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_2560 2560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_3200 3200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_3840 3840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_4480 4480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_5120 5120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_5760 5760w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f17f19bfe66fea0c52ad2fc85bfe6620dc717cda172c2502.PNG/w_6400 6400w\"><figcaption>The illustrations provided are original to the author. Please cite or tag me if you share them anywhere.</figcaption></figure><p>One of the reasons this essay is easy to understand is Singer's use of metaphors to explain his key arguments. Alongside the analogy of the child in the pond, Singer uses the analogy of our evolutionary instincts of survival and reproduction versus our intentional use of reason to different modes of a camera.</p><p>In the preface to the 2016 edition of the essay, Singer sympathizes with our natural inability to help those far away from us; he acknowledges that helping \"distant strangers\" does not seem logical to our evolutionary instinct that cares about our individual survival and that of our direct kin.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcaizhzsq99l\"><sup><a href=\"#fncaizhzsq99l\">[5]</a></sup></span>&nbsp;He categorizes this as the \"point-and-shoot' response,\" where we naturally care more about those close to us versus those far away from us.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcaizhzsq99l\"><sup><a href=\"#fncaizhzsq99l\">[5]</a></sup></span>&nbsp;</p><p>Instead of this, Singer encourages us to switch to \"manual\" mode as the more accurate way of thinking. When we choose to look beyond our evolutionary instincts, we see that a life lived in a different country matters just as much as a life lived in our own city or region. In choosing \"manual\" mode, we're able to see more clearly and accurately\u2014enabling us to choose the more humane way of thinking, which values all lives equally.</p><p>Another aspect of manual mode for Singer is being able to assess when your actions would be truly helpful in reducing suffering. As part of seeing and thinking accurately, Singer advocates for assessing your marginal benefit in a situation: spend the effort to reduce suffering until the effort you would have to spend would equal the reduction in suffering.</p><h2>Equality across distance: expand your moral circle</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_1920 1920w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_2560 2560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_3200 3200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_3840 3840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_4480 4480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_5120 5120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_5760 5760w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b754aaed33e4bfb7eb26c901c2393526414481ac16b1f0d9.PNG/w_6400 6400w\"></figure><p>&nbsp;</p><p>Note: you can read more about this particular concept in Singer's work <a href=\"https://www.amazon.com/Expanding-Circle-Ethics-Evolution-Progress/dp/0691150699/ref=sr_1_1?crid=2297UTG94R0IO&amp;keywords=the+expanding+moral+circle&amp;qid=1663095953&amp;sprefix=the+expanding+moral+circl%2Caps%2C340&amp;sr=8-1\"><i>The Expanding Circle: Ethics, Evolution, and Moral Progress</i></a><i>, </i>originally published in 1981.&nbsp;</p><p>In <i>The Expanding Circle </i>and in <i>Famine, Affluence, and Morality</i>, Singer argues that we should equally consider all people\u2014regardless of their distance from us. If we believe in equality and want equality in our own country or city, we have to extend this belief to all people. If we would feel compelled to help a charitable cause in our area, we need to consider it alongside the causes and concerns of people everywhere.</p><p>An astute part of Singer's argument is the observation that the world has developed into a \"global village.\" With the ability to know about suffering in other parts of the world and the potential to address it, Singer argues that we no longer have any \"possible justification for discriminating [in our altruistic efforts] on geographical grounds.\"<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefex1j60yyimb\"><sup><a href=\"#fnex1j60yyimb\">[6]</a></sup></span>&nbsp; We can't turn a blind eye to suffering in other countries or pretend we aren't aware of a global tragedy.</p><p>Another note: while many people might accept equality as an intrinsic value, many people struggle with the argument that we should weigh local, familial, or national concerns against global concerns or those of people in other places. For more reading and arguments on the ethics of moral circle expansion, <a href=\"https://forum.effectivealtruism.org/topics/moral-circle-expansion-1\">click here.</a></p><h2>Don't be a bystander</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_1920 1920w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_2560 2560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_3200 3200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_3840 3840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_4480 4480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_5120 5120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_5760 5760w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f8d00d44095224bf5879a4168856248d7c0dceed21662dd3.PNG/w_6400 6400w\"></figure><p>In assessing whether or not we should donate our resources, Singer argues that we probably overestimate the number of people that are helping in any given situation. We shouldn't assume that other individuals are already addressing the problem, nor should we assume that the government will address it. Singer puts the onus of action on the reader to act and not be a bystander to the world's problems. Even if we overestimated the need for help and too many resources were thrown at a problem, Singer estimates this would still be better than people doing less than they ought to do.&nbsp;</p><p>Another aspect of this mindset is being harsher towards those who don't help others. As Singer puts it, \"The charitable man may be praised, but the man who is not charitable is not condemned.\"&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrc0j3y6fax\"><sup><a href=\"#fnrc0j3y6fax\">[7]</a></sup></span>&nbsp;Singer advocates for new moral standards of giving that view generosity as the norm, judging and criticizing those who aren't giving as outside of that moral norm.</p><h2>Harmony in thought and action</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_1920 1920w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_2560 2560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_3200 3200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_3840 3840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_4480 4480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_5120 5120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_5760 5760w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/28c4bafdfdd8a95ce99ae117c11657d09277dd29e724421e.PNG/w_6400 6400w\"></figure><p>Singer finishes the essay by calling out his fellow philosophers and those who have more money than they need to support themselves and their dependents. He calls on philosophers in particular to take their ideas seriously by living them out into the world. As he puts it, if you believe that people matter equally and you are capable of giving, \"taking our conclusion seriously means acting upon it.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefcu1dwht9jl5\"><sup><a href=\"#fncu1dwht9jl5\">[8]</a></sup></span>&nbsp;To put it another way, Singer advocates for an active moral philosophy that harmonizes thoughts, beliefs, or values with individual actions.</p><p>Another aspect of this harmony that he describes is self-awareness and honesty with yourself. In another essay of his titled \"The Singer Solution to World Poverty,\" he lists the phone numbers for Oxfam America and UNICEF. Then he challenges his reader: \"Now you, too, have the information you need to save a child's life. How should you judge yourself if you don't do it?\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref91nuzgzmhpu\"><sup><a href=\"#fn91nuzgzmhpu\">[9]</a></sup></span>&nbsp;Singer encourages us to reflect on our own ability to help others and let our beliefs guide our actions.</p><p>&nbsp;</p><p>&nbsp;</p><p><i>This post is dedicated to the 1st Richter, without whom I would not be writing here!</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq1wz46dzm2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq1wz46dzm2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\"Famine, Affluence, and Morality\" was originally published in Philosophy and Public Affairs 1, no. 3 (Spring 1972): 229-43. Throughout this post, I cite a recent publication of a few of Singer's essays titled <i>Famine, Affluence, and Morality (</i>New York, Oxford University Press, 2016).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnymtdnw5xck9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefymtdnw5xck9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer has a great website on himself and his work <a href=\"https://petersinger.info/\">here</a> that links to his books and articles if you'd like to learn more about him. <a href=\"https://uchv.princeton.edu/people/peter-singer\">His academic bio</a> is also available on Princeton's website, where he teaches.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrqu9bv3icyr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrqu9bv3icyr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer, <i>Famine, Affluence, and Morality (</i>New York, Oxford University Press, 2016), ix.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fniohic7aiwah\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefiohic7aiwah\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer, <i>Famine, Affluence, and Morality, </i>6.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncaizhzsq99l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcaizhzsq99l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer, <i>Famine, Affluence, and Morality</i>, xxvii.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnex1j60yyimb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefex1j60yyimb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer, <i>Famine, Affluence, and Morality, </i>8.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrc0j3y6fax\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrc0j3y6fax\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer, <i>Famine, Affluence, and Morality</i>, 14.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fncu1dwht9jl5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefcu1dwht9jl5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer, <i>Famine, Affluence, and Morality, </i>32.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn91nuzgzmhpu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref91nuzgzmhpu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Singer, \"The Singer Solution to World Poverty,\" in <i>Famine, Affluence and Morality, </i>40.</p></div></li></ol>", "user": {"username": "2ndRichter"}}, {"_id": "RkbgTQFviF5r3KDae", "title": "Request books be placed into your (university) library", "postedAt": "2022-09-13T14:17:15.564Z", "htmlBody": "<p>Summary: It's a good idea to request books relevant to Effective Altruism to be placed into your library, particularly for university group organisers. Go and do it!</p><p>This is something I've done at my university library, as well as with a local library, and think it's a pretty good thing for everyone to do.&nbsp;</p><h1>Benefits</h1><p>In order of importance:</p><ul><li>It usually does not take very much effort.</li><li>It demonstrates trustworthiness of the book, generally.<ul><li>This can be especially true within a university context. The library is a key source of information for all university students, so books may seem more trustworthy.</li><li>It's easier to recommend someone read a book that's inside of your university library.</li></ul></li><li>The library will provide more information about the book (e.g. ISBN, date, author etc.) that you can link to, for group organiser purposes.&nbsp;</li></ul><p>Obvious:</p><ul><li>People can read those books for free.&nbsp;</li><li><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>Once inside a library, there's the potential for many people to read it.</li></ul><h1>Process</h1><p>There should be libraries in your local community and at universities that you may be attending. If you're not a member of your library, you can (and should) probably register for one.&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span></p><p>Search for a book purchase request form at your library.&nbsp;It may be called something different, such as a resource suggestion form. If there's not a specific form for it, you can send an enquiry to the library to request a book.&nbsp;</p><p>Please ensure you include the book's title, author, and year/edition, as well as any other information requested by the library. If you're a university group organiser, it's likely helpful to note that you're with a university student group.</p><p>Note that <a href=\"https://www.thelifeyoucansave.org.au/the-book/\">Life You Can Save</a> and <a href=\"https://80000hours.org/book/\">80,000 Hours</a> are freely downloadable on those websites, so ensure you request for a physical copy for the library.&nbsp;</p><p><strong>University groups</strong></p><p>As your library may not buy more than one or two books, this probably won't replace having a stock of books within a university student group. This is especially true for some 'introductory' books such as Doing Good Better, The Precipice, and Scout Mindset, which are quite 'core' books and do not have free digital replacements.<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span></p><h1>In practice</h1><p>The books that I've requested and that are (being) placed by Macquarie University (MQ) library are: 80,000 hours; The Precipice; The Scout Mindset; The Alignment Problem; Algorithms To Live By; Superintelligence<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span>; Oxford Handbook of Thinking and Reasoning; What We Owe The Future.</p><p>Here's an EA MQ reading list that includes links to the MQ library, made in Canva (two sided A4 flyer), that you can view <a href=\"https://www.canva.com/design/DAFLCM2pypo/yNX8OmWKmv1QFsRx9dnBQw/view?utm_content=DAFLCM2pypo&amp;utm_campaign=designshare&amp;utm_medium=link&amp;utm_source=publishsharelink \">here</a>. &nbsp;If you'd like to use this as a template, you can do that <a href=\"https://www.canva.com/design/DAFLCM2pypo/yNX8OmWKmv1QFsRx9dnBQw/view?utm_content=DAFLCM2pypo&amp;utm_campaign=designshare&amp;utm_medium=link&amp;utm_source=publishsharelink&amp;mode=preview\">here</a>. I used book<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span></span>&nbsp;suggestions from a few places, including my general knowledge, my fellow organisers, and Forum posts about <a href=\"https://forum.effectivealtruism.org/posts/KNZLGbGevnjStgzHt/i-scraped-all-public-effective-altruists-goodreads-reading\">GoodReads lists</a>, <a href=\"https://forum.effectivealtruism.org/posts/TzooJmtZvxtK2kBQi/co-creation-of-the-library-of-effective-altruism-information\">Information Design</a>, and <a href=\"https://forum.effectivealtruism.org/posts/iAowzcZm87wNrTQCb/a-biosecurity-and-biorisk-reading-list\">Biosecurity</a>.</p>", "user": {"username": "richie2154"}}]