[{"_id": "b9ADTuADnGjwY8CAw", "postedAt": "2022-09-28T18:38:11.352Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Strong upvote - I found your perspective really fresh:<br>\"The most likely case to me is that if AI x-risk is solved or turns out not to be a serious issue, and we just keep facing x-risks in proportion to how strong our technology gets, forever. Eventually we draw a black ball and all die.\"</p><p>Lots of us are considering a career pivot into AI safety. Is it...actually tractable at all? How hopeful should we be about it? No idea.</p>", "parentCommentId": null, "user": {"username": "Emily Dardaman"}}, {"_id": "BeNiKz5KDwqLPSDHd", "postedAt": "2022-09-28T18:47:31.222Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>I think you should be substantially more optimistic about the effects of aligned AGI. &nbsp;Once we have aligned AGI, this basically means high end cognitive labor becomes very cheap, as once an AI system is trained, it is relatively cheap to deploy it en masse. &nbsp;Some of these AI scientists would presumably work on making AI's at least cheaper if not more capable, which limits to a functionally infinite supply of high end scientists. &nbsp;Given a functionally infinite supply of high end scientists, we will quickly discover basically everything that can be discovered through parallelizable scientific labor which is, if not everything, I think at least quite a few things (e. g. I have pretty high confidence that we could solve aging, develop extremely good vaccines to prevent against biorisk, etc.). &nbsp;Moreover, this is only a lower bound; I think AGI will probably relatively quickly become significantly smarter than the smartest human, so we will probably do even better than the aforementioned scenario.</p>", "parentCommentId": null, "user": {"username": "NickGabs"}}, {"_id": "NRh6aX76aNY3To2wK", "postedAt": "2022-09-28T18:48:29.323Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>It's hard to say. Considering there are fewer than 300 people estimated working on AI Safety and it's still just starting to gain traction, I wouldn't expect us to know a ton about it yet.&nbsp;</p><p>Even in established fields people are expected to usually take years or even decades before they can produce truly great research.&nbsp;<br><br>Psychology was still using lobotomies until 55 years ago. We've learned a lot since then and there's still much more to learn. It took a similar amount of time for AI capabilities to get to where they are now. AI Safety is much newer and could look completely different in 10 years. Or, if nobody works on it or the people working on it are unable to make progress, it could look relatively similar.&nbsp;</p>", "parentCommentId": "b9ADTuADnGjwY8CAw", "user": {"username": "Peter Gebauer"}}, {"_id": "P7Suc6GtSAZpLa3Nm", "postedAt": "2022-09-28T19:30:19.777Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<blockquote><p>One of the negative things in my file was that someone had said I was \"a bit of a downer\". Much like with my technical competency, maybe so. But it's worth mentioning that in my day to day life, my coworkers generally think I'm <i>weirdly positive</i>, and often comment that my outlook is shockingly sanguine.</p></blockquote><p>I wonder how much of this is an EA thing vs idiosyncrasies of the org you trialed at, or for that matter, West Coast American culture overall. Fwiw, my own experience is that I worked at three non-EA tech companies (Epic, Impossible Foods, and Google), &nbsp;and broadly people seemed more positive/confident in the organization than people I know in EA orgs. Certainly EA <i>funders</i> seem more pessimistic (though I've never talked to top VCs).</p><blockquote><p>I had a two week work trial with a prominent EA org. There were some red flags. Nobody would tell me the projected salary, despite the job opportunity taking place across the country and in one of the most expensive cities on Earth. But whatever. I quit my job and flew over.</p></blockquote><p>This seems quite bad and I'm sorry you had to go through that. The org's actions feels rather unprofessional to me tbh.</p>", "parentCommentId": null, "user": {"username": "Linch"}}, {"_id": "sSxGTfekMPdEmicPJ", "postedAt": "2022-09-28T19:34:44.073Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Thanks for posting! I'm sympathetic to the broad intuition that any one person being at the sweet spot where they make a decisive impact seems unlikely , but I'm not sold on most of the specific arguments given here.</p>\n<blockquote>\n<p>Recall that there are decent reasons to think goal alignment is impossible - in other words, it's not a priori obvious that there's any way to declare a goal and have some other agent pursue that goal exactly as you mean it.</p>\n</blockquote>\n<p>I don't see why this is the relevant standard. \"Just\" avoiding egregiously unintended behavior seems sufficient for avoiding the worst accidents (and is clearly possible, since humans do it often).</p>\n<p>Also, I don't think I've heard these decent reasons--what are they?</p>\n<blockquote>\n<p>Recall that engineering ideas very, very rarely work on the first try, and that if we only have one chance at anything, failure is very likely.</p>\n</blockquote>\n<p>It's also unclear that we only have one chance at this. Optimistically (but not <em>that</em> optimistically?), incremental progress and failsafes can allow for effectively multiple chances. (The main argument against seems to involve assumptions of very discontinuous or abrupt AI progress, but I haven't seen very strong arguments for expecting that.)</p>\n<blockquote>\n<p>Recall that getting \"humanity\" to agree on a good spec for ethical behavior is extremely difficult: some places are against gene drives to reduce mosquito populations, for example, despite this saving many lives in expectation.</p>\n</blockquote>\n<p>Agree, but also unclear why this is the relevant standard. A smaller set of actors agreeing on a more limited goal might be enough to help.</p>\n<blockquote>\n<p>Recall that there is a gigantic economic incentive to keep pushing AI capabilities up, and referenda to reduce animal suffering in exchange for more expensive meat tend to fail.</p>\n</blockquote>\n<p>Yup, though we should make sure not to double-count this, since this point was also included earlier (which isn't to say you're necessarily double-counting).</p>\n<blockquote>\n<p>Recall that we have to implement any solution in a way that appeals to the cultural sensibilities of all major and technically savvy governments on the planet, plus major tech companies, plus, under certain circumstances, idiosyncratic ultra-talented individual hackers.</p>\n</blockquote>\n<p>This also seems like an unnecessarily high standard, since regulations have been passed and enforced before without unanimous support from affected companies.</p>\n<p>Also, getting acceptance from all major governments does seem very hard but not quite as hard as the above quotes makes it sound. After all, many major governments (developed Western ones) have relatively similar cultural sensibilities, and ambitious efforts to prevent unilateral actions have previously gotten very broad acceptance (e.g. many actors could have made and launched nukes, done large-scale human germline editing, or maybe done large-scale climate engineering, but to my knowledge none of those have happened).</p>\n<blockquote>\n<p>The we-only-get-one-shot idea applies on this stage too.</p>\n</blockquote>\n<p>Yup, though this is also potential double-counting.</p>\n", "parentCommentId": null, "user": {"username": "Mauricio"}}, {"_id": "jZcyWiACcpteKLC7e", "postedAt": "2022-09-28T21:54:12.532Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>double comment</p>", "parentCommentId": "bWHdoGXgiN9FdB3Dx", "user": {"username": "Charles He"}}, {"_id": "cuxTRwFpDrmikShxX", "postedAt": "2022-09-29T00:14:29.535Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Yeah I think a lot of it is West Coast American culture! I imagine EA would have super different vibes if it were mostly centered in New York.</p>", "parentCommentId": "P7Suc6GtSAZpLa3Nm", "user": {"username": "Justis"}}, {"_id": "gJsMtbfJLcgpPvgxB", "postedAt": "2022-09-29T00:23:07.096Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Yeah, I share the view that the \"Recalls\" are the weakest part -- I mostly was trying to get my fuzzy, accumulated-over-many-years vague sense of \"whoa no we're being way too confident about this\" into a more postable form. Seeing your criticisms I think the main issue is a little bit of a Motte-and-Bailey sort of thing where I'm kind of responding to a Yudkowskian model, but smuggling in a more moderate perspective's odds (ie. Yudkowsky thinks we need to get it right on the first try, but Grace and MacAskill may be agnostic there).</p><p>I may think more about this! I do think there's something there sort of <i>between </i>the parts you're quoting, by which I mean yes, we could get agreement to a narrower standard than solving ethics, but even just <i>making ethical progress at all</i>, or coming up with standards that go anywhere good/predictable politically seems hard. Like, the political dimension and the technical/problem specification dimensions both seem super hard in a way where we'd have to trust ourselves to be extremely competent across both dimensions, and our actual testable experiments against either outcome are mostly a wash (ie. we can't get a US congressperson elected yet, or get affordable lab-grown meat on grocery store shelves, so doing harder versions of both at once seems...I dunno, might hedge my portfolio far beyond that!).</p>", "parentCommentId": "sSxGTfekMPdEmicPJ", "user": {"username": "Justis"}}, {"_id": "JexicDjGjZGQsoN2G", "postedAt": "2022-09-29T00:25:40.370Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>To me, \"aligned\" does a lot of work here. Like yes, if it's <i>perfectly </i>aligned and totally general, the benefits are mind boggling. But maybe we just get a bunch of AI that are mostly generating pretty good/safe outputs, but a few outputs here and there lower the threshold required for random small groups to wreak mass destruction, and then at least one of those groups blows up the biome.</p><p>But yeah given the premise we get AGI that mostly does what we tell it to, and we don't immediately tell it &nbsp;to do anything stupid, I do think it's &nbsp;very hard to predict what will happen but it's gonna be <i>wild</i> (and indeed possibly really good).</p>", "parentCommentId": "BeNiKz5KDwqLPSDHd", "user": {"username": "Justis"}}, {"_id": "YCAtBXCxnZzBnjzN5", "postedAt": "2022-09-29T00:27:37.242Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Thank you! My perspective is: \"figuring out if it's tractable is at least tractable enough that it's worth a lot more time/attention going there than is currently\", but not necessarily \"working on it is far and away the best use of time/money/attention for altruistic purposes\", and almost certainly not \"working on it is the best use of time/money/attention under a wide variety of ethical frameworks and it should dominate a healthy moral parliament\".</p>", "parentCommentId": "b9ADTuADnGjwY8CAw", "user": {"username": "Justis"}}, {"_id": "wvf5rCmQRga94nEAk", "postedAt": "2022-09-29T00:35:58.397Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>(Comment to flag that I looked back over this and just totally pretended 4,000 was equal to 1,000. Whoops. Don't think it affects the argument very strongly, but I have multiplied the relevant dollar figures by 4.)</p>", "parentCommentId": null, "user": {"username": "Justis"}}, {"_id": "EWxdHYnxgtdeNCWaG", "postedAt": "2022-09-29T01:37:37.444Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>For a contrasting opinion by Kat Woods and Amber Dawn, here's this post: Two reasons we might be closer to solving alignment than it seems.</p>\n<p>Link below:</p>\n<p><a href=\"https://forum.effectivealtruism.org/posts/RkpdA8763yGtEovj9/two-reasons-we-might-be-closer-to-solving-alignment-than-it\">https://forum.effectivealtruism.org/posts/RkpdA8763yGtEovj9/two-reasons-we-might-be-closer-to-solving-alignment-than-it</a></p>\n", "parentCommentId": null, "user": {"username": "Sharmake"}}, {"_id": "sGgSxmqmMsiLyS8Ad", "postedAt": "2022-09-29T07:05:40.369Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Justis,do you, as someone involved in AI safety research, think that AI safety researchers would mostly dislike the total termination of AI research (assuming they all found great alternative jobs, etc)?</p>", "parentCommentId": null, "user": {"username": "Noah Scales"}}, {"_id": "dof9h8gNQov8Kiqg3", "postedAt": "2022-09-29T17:03:20.906Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Data point: I wasn't there for this but Justis is a friend of mine, and on an interpersonal level he's one of the chillest, highest-contentment-set-point people I know. He doesn't brim over with cheerleading or American dynamism, but my default assumption is if someone calls him a downer they can't mean interpersonal affect.</p>", "parentCommentId": null, "user": {"username": "Elizabeth"}}, {"_id": "uLghB2jx9iHWCQo6o", "postedAt": "2022-09-29T18:07:51.607Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Thanks!</p>", "parentCommentId": "jZcyWiACcpteKLC7e", "user": {"username": "Linch"}}, {"_id": "zBnPaF2JKkwX75DsZ", "postedAt": "2022-09-29T22:11:00.571Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<blockquote><p>\"EA has a strong cultural bias in favor of believing arbitrary problems are solvable\".</p></blockquote><p>I think you're pointing to a real phenomenon here (though I might not call it an \"optimism bias\"\u2014EAs also tend to be unusually pessimistic about some things).</p><p>I have pretty strong disagreements with a lot of the more concrete points in the post though, I've tried to focus on the most important ones below.</p><blockquote><p><strong>Conclusion One: </strong>Pursuing the basic plan entailed in premises 1-4 saves, in expectation, at least 4.8 million lives (800,000 * 0.06 * 0.1 * 0.1).&nbsp;</p></blockquote><p>(I think you may have missed the factor of 0.01, the relative risk reduction you postulated? I get 8 billion * 0.06 * 0.01 * 0.1 * 0.1 = 48,000. So AI safety would look worse by a factor of 100 compared to your numbers.)</p><p>But anyway, I strongly disagree with those numbers, and I'm pretty confused as to what kind of model generates them. Specifically, you seem to be extremely confident that we can't solve AI X-risk (&lt; 1/10,000 chance if we multiply together the 1% relative reduction with your two 10% chances). On the other hand, you think we'll most likely be fine by default (94%). So you seem to be saying that there probably isn't any problem in the first place, but if there is, then we should be extremely certain that it's basically intractable. This seems weird to me. Why are you so sure that there isn't a problem which would lead to catastrophe by default, but which could be solved by e.g. 1,000 AI safety researchers working for 10 years? To get to your level of certainty (&lt;1/10,000 is a lot!), you'd need a very detailed model of AI X-risk IMO, more detailed than I think anyone has written about. A lot of the uncertainty people tend to have about AI X-risk comes specifically from the fact that we're unsure what the main sources of risk are etc., so it's unclear how you'd exclude the possibility that there are significant sources of risk that are reasonably easy to address.</p><p>As to why I'm not convinced by the argument that leads you to the &lt;1/10,000 chance: the methodology of \"split my claim into a conjunction of subclaims, then assign reasonable-sounding probabilities to each, then multiply\" often just doesn't work well (there are exceptions, but this certainly isn't one of them IMO). You can get basically arbitrary result by splitting up the claim in different ways, since what probabilities are \"reasonable-sounding\" isn't very consistent in humans.</p><blockquote><p>Okay, a longtermist might say. Maybe the odds are really slim that we thread this needle, and then also the subsequent needles required to create an interstellar civilization spanning billions of years. But the value of that scenario is so high that if you shut up and multiply, it's worth putting a lot of resources in that direction.</p></blockquote><p>I can't speak for all longtermists of course, but that is decidedly <i>not</i> an argument I want to make (and FWIW, my impression is that this is not the key objection most longtermists would raise). If you convinced me that our chances of preventing an AI existential catastrophe were &lt;1/10,000, and that additionally we'd very likely die in a few centuries anyway (not sure just how likely you think that is?), then I would probably throw the expected value calculations out the window and start from scratch trying to figure out what's important. Basically for exactly the reasons you mention: at some point this starts feeling like a Pascal's mugging, and that seems fishy and confusing.</p><p>But I think the actual chances we prevent an AI existential catastrophe are way higher than 1/10,000 (more like 1/10 in terms of the order of magnitude). And I think conditioned on that, our chances of surviving for billions of years are pretty decent (very spontaneous take: &gt;=50%). Those feel like cruxes to me way more than whether we should blindly do expected value calculations with tiny probabilities, because my probabilities aren't tiny.</p><p>&nbsp;</p><blockquote><p><strong>Scenario Two: </strong>Same as scenario one, but there's a black hole/alien invasion/unstoppable asteroid/solar flare/some other astronomical event we don't know about yet that unavoidably destroys the planet in the next millennium or two. (I don't think this scenario is likely, but it is possible.)</p></blockquote><p>I agree it's <i>possible</i> in a very weak sense, but I think we can say something stronger about just how unlikely this is (over the next millennium or two): Nothing like this has happened over the past 65 million years (where I'm counting the asteroid back then as \"unstoppable\" even though I think we could stop that soon after AGI). So unless you think that alien invasions are reasonably likely to happen soon (but were't likely before we sent out radio waves, for example), this scenario seems to be firmly in the \"not really worth thinking about\" category.</p><p>This may seem really nitpicky, but I think it's important when we talk about <i>how likely</i> it is that we'll continue living for billions of years. You give several scenarios for how things could go badly, but it would be just as easy to list scenarios for how things could go well. Listing very unlikely scenarios, especially just on one side, actively makes our impression of the overall probabilities worse.</p>", "parentCommentId": null, "user": {"username": "Erik Jenner"}}, {"_id": "Gav5AfkpRbAZKiKgb", "postedAt": "2022-09-29T22:59:34.247Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Ah yeah, you're right - I think basically I put in the percent rather than the probability. So it would indeed be <i>very </i>expensive to be competitive with AMF. Though so is everything else, so that's not hugely surprising.</p><p>As for the numbers, yeah, it does just strike me as really, really unlikely that we can solve AI x-risk right now. 1/10,000 does feel about right to me. I certainly wouldn't expect everyone else to agree though! I think some people would put the odds much higher, and others (like Tyler Cowen maybe?) would put them a bit lower. Probably the 1% step is the step I'm least confident in - wouldn't surprise me if the (hard to find, hard to execute) solutions that are findable would reduce risk significantly more.</p><p>EDIT: tried to fix the math and switched the \"relative risk reduction term\" to 10%. I feel like among findable, executable interventions there's probably a lot of variance, and it's plausible some of the best ones do reduce risk by 10% or so. And 1/1000 feels about as plausible as 1/10000 to me. So, somewhere in there.</p>", "parentCommentId": "zBnPaF2JKkwX75DsZ", "user": {"username": "Justis"}}, {"_id": "NdtCBybFmWtZyCnXY", "postedAt": "2022-09-29T23:20:57.437Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Hmm. I think reactions to that would vary really widely between researchers, and be super sensitive to when it happened, why, whether it was permanent, and other considerations.</p>", "parentCommentId": "sGgSxmqmMsiLyS8Ad", "user": {"username": "Justis"}}, {"_id": "up5dZgNsEaeG7d9iB", "postedAt": "2022-10-02T02:32:15.066Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>I wonder if they are truly against AGI or ASI, or if they just want the safe versions? I am not sure if there are really two positions here (one for AI, one against), or really just one with caveats.</p>", "parentCommentId": "NdtCBybFmWtZyCnXY", "user": {"username": "Noah Scales"}}, {"_id": "dfJeYvzfFvxz3isiH", "postedAt": "2022-10-03T03:46:18.652Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<p>Re optimism bias</p>\n<p>Towards the top of the post I think you made a claim that EAs are often very optimistic (particularly agentic one\u2019s doing ambitious things or in \u2018elitist\u2019 positions).</p>\n<p>I just wanted to flag that this isn\u2019t my impression of many EAs who I think are doing ambitious projects, I think a disproportionate number of agentic people I know in EA are pretty pessimistic in general.</p>\n<p>I think the optimism thing and something like desire to try hard/motivation/ enthusiasm for projects are getting a bit confused here, but low confidence.</p>\n", "parentCommentId": null, "user": {"username": "calebp"}}, {"_id": "YrEMAXuWouKPMJRWM", "postedAt": "2022-10-12T15:45:04.245Z", "postId": "LjBYatyXkce5EiLDo", "htmlBody": "<blockquote><p>it does just strike me as really, really unlikely that we can solve AI x-risk right now</p></blockquote><p>I think Erik wasn't commenting so much on this number, but rather its combination with the assumption that there is a 94% chance things are fine by default.</p><p>I.e. you are assuming that there is a 94% chance it's trivially easy, and 6% chance it's insanely hard.</p><p>Very few problems have such a bimodal nature, and I also would be interested to understand what's generating it for you.</p>", "parentCommentId": "Gav5AfkpRbAZKiKgb", "user": {"username": "Ben_West"}}]