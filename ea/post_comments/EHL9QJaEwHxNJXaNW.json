[{"_id": "zgD87qpPF5RKLAXFr", "postedAt": "2024-01-19T06:53:13.657Z", "postId": "EHL9QJaEwHxNJXaNW", "htmlBody": "<p>Dear Joe,</p><p>I am reading the series, and I want to point out that to some extent there was a polar opposite to Lewis that was Olaf Stapledon. Lewis admired him and his Christinity &nbsp;was probably influenced by the perfect depiction of cosmic indifference that Stapledon gave us. For me \"<a href=\"https://freeditorial.com/es/books/last-and-first-men\">First and Last Men</a>\" is still an absolute masterpiece.</p><p>An additional comment is that I wrote a piece on <a href=\"https://www.lesswrong.com/posts/nY7oAdy5odfGqE7mQ/freedom-under-naturalistic-dualism\">freedom under naturalistic dualism</a> that probably you could find interesting (still looking how to publish it in a more formal way).</p>", "parentCommentId": null, "user": {"username": "Arturo Macias"}}, {"_id": "u68KRByhoiBYqS9vM", "postedAt": "2024-01-19T14:02:17.636Z", "postId": "EHL9QJaEwHxNJXaNW", "htmlBody": "<p><strong>Executive summary</strong>: The essay discusses concerns that the AI alignment discourse aspires to exert inappropriate control over future values, arguing this is not necessarily the case even without an objective \"Tao\" to guide choices.</p><p><strong>Key points</strong>:</p><ol><li>Lewis argues influencing future generations' values without believing in an objective morality makes one a \"tyrant\", but the essay disputes this, arguing influence can be ethical under moral anti-realism.</li><li>The essay claims naturalists can have rich values and relationships despite viewing values as fully natural, countering Lewis's association of naturalism with instrumentalism.</li><li>Even without an objective Tao, the essay argues it's possible to influence others' values non-tyrannically by respecting consent, freedom to not participate, ethical norms against coercion, etc.</li><li>Letting non-agential Nature determine all future values is not obviously ethically superior to intentional steering grounded in human values. We are part of Nature too.</li><li>The essay concludes shaping future values requires wisdom, cooperativeness, respect for boundaries and learning from yin, not just rejecting Nature as valueless.</li></ol><p>&nbsp;</p><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "KSJgfDkZRKZCewqcq", "postedAt": "2024-01-24T23:14:20.684Z", "postId": "EHL9QJaEwHxNJXaNW", "htmlBody": "<p>I may not have understood all of you what you said, but I was left with a few thoughts after finishing this.</p><p>1. Creating Bob to have values: if Bob is created to be able to understand that he was created to have values, and to be able to then, himself, reject those values and choose his own, then I say he is probably more free than if he wasn't. &nbsp;But, having chosen his own values, he now has to live in society, a society possibly largely determined by an AI. &nbsp;If society is out of tune with him, he will have limited ability to live out his values, and the cognitive dissonance of not being able to live out his values will wear away at his ability to hold his freely-chosen values. &nbsp;But society has to be a certain way, and it might not be compatible with whatever Bob comes up with (unless maybe each person lives in a simulation that is their society, that can be engineered to agree with them). &nbsp;</p><p>Other than the engineered-solipsism option, it seems like it's unavoidable to limit freedom to some extent. &nbsp;(Or maybe even then: what if people can understand that they are in engineered-solipsism and rebel?) &nbsp;But we could design a government (a world-ruling AI) that fails to decide for other people as much as possible and actively fosters people's ability to make their own decisions, to minimize this. &nbsp;At least, a concern one might have about AI alignment is that AI will consume decision-making opportunities in an unprecedented way, leading one to try to prevent that from happening, or even reduce the level of decision-making hoarding that currently exists.</p><p>2. Brainwashing: If I make art, that's a bit of brainwashing (in a sense). &nbsp;But then, someone else can make art, and people can just ignore my art, or their art. It's more a case of there being a \"fair fight\", than if someone locks me in a room and plays propaganda tapes 24/7, or if they just disable the \"I can see that I have been programmed and can rebel against that programming\" part of my brain. &nbsp;This \"fair fight\" scenario could maybe be better than it is (like there could be an AI that actively empowers each person to make or ignore art to be able to counteract some brainwashing artist). &nbsp;Our current world has a lot of brainwashing in it, where some people are more psychologically powerful than others.</p><p>3. \"Hinge of History\"ness: we could actively try to defer decisionmaking as much as possible to future generations, giving each generation the ability to make its own decisions and revoke the past as much as possible (if one generation revokes the past, they can't impede the next from revoking their values, as one limitation on that), and design/align AI that does the same. &nbsp;In other words, actively try to reduce the \"hingeyness\" of our century.</p>", "parentCommentId": null, "user": {"username": "James_Banks"}}]