[{"_id": "jCgQ4ZyoNSbGbNt8d", "postedAt": "2024-03-24T18:25:55.519Z", "postId": "4xwWDLfMenw48TR8c", "htmlBody": "<blockquote>\n<p>Ideally, I would include at this point some readings on how aggregation might work for building a utopia, since this seems like an obvious and important point. For instance, should the light cone be divided such that every person (or every moral patient more broadly, perhaps with the division taking moral weight into account) gets to live in a sliver of the light cone that\u2019s optimized to fit their preferences? Should everybody\u2019s preferences be aggregated somehow, so that everyone can live together happily in the overall light cone? Something else? However, I was unable to find any real discussion of this point. Let me know in the comments if there are writings I\u2019m missing. For now, I\u2019ll include the most relevant thing I could find as well as a more run-of-the-mill reading on preference aggregation theory.</p>\n</blockquote>\n<p>It would probably be worth if for someone to write out the ethical\nimplications of K-complexity-weighted utilitarianism/<a href=\"https://joecarlsmith.com/2021/11/28/anthropics-and-the-universal-distribution\">UDASSA</a> on how to think about far-future ethics.</p>\n<p>A few things that come to mind about this question (these are all ~hunches and maybe only semi-related, sorry for the braindump):</p>\n<ul>\n<li>The description length of earlier states of the universe is probably shorter, which means that the \"claw\" that locates minds earlier in a simple universe is also shorter. This implies that lives earlier in time in the universe would be more important, and that we <a href=\"https://www.lesswrong.com/posts/tDkYdyJSqe3DddtK4/alexander-gietelink-oldenziel-s-shortform?commentId=CCruSpcTvkAq8AFJg\">don't have to care about exact copies as much</a>.\n<ul>\n<li>This is similar to the reasons why not to care too much about Boltzmann brains.</li>\n</ul>\n</li>\n<li>We might have to aggregate preferences of agents with different beliefs (<a href=\"https://acritch.com/media/servant-of-many-masters.pdf\">possible</a>) and different ontologies/metaphysical stances (not sure about this), probably across <a href=\"https://intelligence.org/files/OntologicalCrises.pdf\">ontological crises</a>.\n<ul>\n<li>I have some preliminary writings on this, but nothing publishable yet.</li>\n</ul>\n</li>\n<li>The outcomes of UDASSA is dependent on the choice of Turing machine. (People say it's only up to a constant, but that constant can be pretty big).\n<ul>\n<li>So we either find a way of classifying Turing machines by simplicity without relying on a single Turing machine to give us that notion, or we start out with some probability distribution over Turing machines and do some \"2-level-Solomonoff induction\", where we update both the probability of each Turing machine <em>and</em> the probabilities of each hypothesis for Turing machine.</li>\n<li>This leads to selfishness for whoever is computing Solomonoff induction, because the Turing machine where the empty program just outputs their observations receives the highest posterior probability.</li>\n</ul>\n</li>\n<li>If we use UDASSA/K-ultilitarianism to weigh minds there's a pressure/tradeoff to simplify one's preferences to be simpler.</li>\n<li>If we endorse some kind of total utilitarianism, and there are increasing marginal returns to energymatter or spacetime investment into minds with respect to degree of moral patienthood then we'd expect to end up with very few large minds, if there are decreasing marginal returns we end up with many small minds.</li>\n<li>Theorems like <a href=\"https://en.wikipedia.org/wiki/Gibbard%E2%80%93Satterthwaite_theorem\">Gibbard-Satterthwaite</a> and <a href=\"https://warwick.ac.uk/fac/soc/economics/staff/bdutta/publications/cardinalrev7.pdf\">Hylland</a> imply that robust preference aggregation that resists manipulation is really hard. You can circumvent this by randomly selecting a dictator, but I think this would become unnecesary if we operate in an <a href=\"https://www.lesswrong.com/tag/open-source-game-theory\">open-source game theory</a> context, where algorithms can inspect each others' reasons for a vote.</li>\n<li>I'm surprised you didn't mention <a href=\"https://en.wikipedia.org/wiki/Reflective_equilibrium\">reflective equilibrium</a>! Formalising reflective equilibrium and <a href=\"https://www.lesswrong.com/posts/kmpNkeqEGvFue7AvA/value-formation-an-overarching-model\">value formation</a> with meta-preferences would be major steps in a long reflection.</li>\n<li>I have the intuition that <a href=\"https://www.goodreads.com/book/show/42275384-grand-futures\">Grand Futures</a> talks about this problem <em>somewhere</em><sup class=\"footnote-ref\"><a href=\"#fn-S3rNpvtuWaKr5dDYp-1\" id=\"fnref-S3rNpvtuWaKr5dDYp-1\">[1]</a></sup>, but I don't remember/know where.</li>\n</ul>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-S3rNpvtuWaKr5dDYp-1\" class=\"footnote-item\"><p>Which, given its length, isn't that out there. <a href=\"#fnref-S3rNpvtuWaKr5dDYp-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "parentCommentId": null, "user": {"username": "niplav"}}, {"_id": "SsfZwANGRzPeEmnAA", "postedAt": "2024-03-25T10:56:20.372Z", "postId": "4xwWDLfMenw48TR8c", "htmlBody": "<p>Many of those posts in the list seem really relevant to me for the cluster of things you're pointing at!</p><p>On some of the philosophical background assumptions, I would consider adding my ambitiously-titled post <a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated\">The Moral Uncertainty Rabbit Hole, Fully Excavated</a>. (It's the last post in my metaethics/anti-realism sequence.)</p><p>Since the post is long and it says that it doesn't work maximally well as a standalone piece (without two other posts from earlier in my sequence), it didn't get much engagement when I published it, so I feel like I should do some advertizing for it here.</p><p>As the title indicates, I'm trying to answer questions in that post that many EAs don't ask themselves because they think about moral uncertainty or moral reflection in an IMO somewhat lazy way.</p><p>The post starts with a conundrum for the concept of moral uncertainty:&nbsp;</p><blockquote><p>In an <a href=\"https://forum.effectivealtruism.org/posts/SotZAFkGbgBEFBnQX/moral-uncertainty-and-moral-realism-are-in-tension\">earlier post</a>, I argued that moral uncertainty and confident moral realism don\u2019t go together. Accordingly, if we\u2019re morally uncertain, we must either endorse moral anti-realism or at least put significant credence on it.</p></blockquote><p>This insight has implications because we're now conflating a few different things under the \"moral uncertainty\" label:&nbsp;</p><ul><li>Metaethical uncertainty (i.e., our remaining probability on moral realism) and the strength of possible wagers for acting as though moral realism is true even if our probability in it is low.</li><li>Uncertainty over the values we'd choose after long reflection (our \"idealized values\", which most people would be motivated to act upon even if moral realism is false).</li><li>Related to how we'd get to idealized values, the possibility of having <i>under-defined values</i>, i.e., the possibility that, because moral realism is false, even idealized moral reflection may lead to different endpoints based on very small changes to the procedure, or that a person's reflection doesn't \"terminate\" because their <i>subjective feeling of</i> uncertainty never goes away inside the envisioned reflection procedure.</li></ul><p>My post is all about further elaborating on these distinctions and spelling out their implications for effective altruists.<br><br>I start out by introducing the notion of a <strong>moral reflection procedure </strong>to explain what moral reflection in an idealized setting could look like<strong>:</strong></p><blockquote><p>To specify the meaning of \u201cperfectly wise and informed,\u201d we can envision a suitable procedure for moral reflection that a person would hypothetically undergo. Such a <strong>reflection procedure</strong> comprises a <strong>reflection environment</strong> and a <strong>reflection strategy</strong>. The reflection environment describes the <i>options</i> at one\u2019s disposal; the reflection strategy describes how a person would <i>use</i> those options.</p><p>Here\u2019s one example of a reflection environment:</p><ul><li><i><strong>My favorite thinking environment:</strong></i> Imagine a comfortable environment tailored for creative intellectual pursuits (e.g., a Google campus or a cozy mansion on a scenic lake in the forest). At your disposal, you find a well-intentioned, superintelligent AI advisor fluent in various schools of philosophy and programmed to advise in a value-neutral fashion. (Insofar as that\u2019s possible \u2013 since one cannot do philosophy without a specific methodology, the advisor must already endorse certain <a href=\"https://www.alignmentforum.org/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty\">metaphilosophical</a> commitments.) Besides answering questions, they can help set up experiments in virtual reality, such as ones with <a href=\"https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">emulations</a> of your brain or with modeled copies of your younger self. For instance, you can design experiments for learning what you'd value if you first encountered the EA community in San Francisco rather than in Oxford or started reading Derek Parfit or Peter Singer <i>after</i> the blog Lesswrong, instead of the other way around.<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-2\"><sup>[2]</sup></a> You can simulate conversations with select people (e.g., famous historical figures or contemporary philosophers). You can study how other people\u2019s reflection concludes and how their moral views depend on their life circumstances. In the virtual-reality environment, you can augment your copy\u2019s cognition or alter its perceptions to have it experience new types of emotions. You can test yourself for biases by simulating life as someone born with another gender(-orientation), ethnicity, or into a family with a different socioeconomic status. At the end of an experiment, your (near-)copies can produce write-ups of their insights, giving you inputs for your final moral deliberations. You can hand over authority about choosing your values to one of the simulated (near-)copies (if you trust the experimental setup and consider it too difficult to convey particular insights or experiences via text). Eventually, the person with the designated authority has to provide to your AI assistant a precise specification of values (the format \u2013 e.g., whether it\u2019s a utility function or something else \u2013 is up to you to decide on). Those values then serve as your idealized values after moral reflection.</li></ul><p>(Two other, more rigorously specified reflection procedures are <a href=\"https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\">indirect normativity</a> and <a href=\"https://ai-alignment.com/strong-hch-bedb0dc08d4e\">HCH</a>.<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-3\"><sup>[3]</sup></a> Indirect normativity outputs a utility function whereas HCH attempts to formalize \u201cidealized judgment,\u201d which we could then consult for all kinds of tasks or situations.)<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-4\"><sup>[4]</sup></a></p><p>\u201cMy favorite thinking environment\u201d leaves you in charge as much as possible while providing flexible assistance. Any other structure is for you to specify: you decide the reflection strategy.<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-5\"><sup>[5]</sup></a> This includes what questions to ask the AI assistant, what experiments to do (if any), and when to conclude the reflection.</p></blockquote><p>For reflection strategies (how to behave inside a reflection procedure), I discuss a continuum from \"conservative\" to \"open-minded\" reflection strategies.</p><blockquote><p>Someone with a <strong>conservative</strong> reflection strategy is steadfast in their moral reasoning framework. ((What I mean by \u201cmoral-reasoning framework\u201d is similar to what Wei Dai calls \u201cmetaphilosophy\u201d \u2013 it implies having confidence in a particular metaphilosophical stance and using that stance to form convictions about one\u2019s reasoning methodology or object-level moral views.)) They guard their opinions, which turns these into <i>convictions</i> (\u201cconvictions\u201d being opinions that one safeguards against goal drift). At its extreme, someone with a <i>maximally conservative</i> reflection strategy has made up their mind and no longer benefits from any moral reflection. People can have moderately conservative reflection strategies where they have formed convictions on some issues but not others.</p><p>By contrast, people with <strong>open-minded</strong> moral reflection strategies are uncertain about either their moral reasoning framework or (at least) their object-level moral views. As the defining feature, they take a passive (\u201copen-minded\u201d) reflection approach focused on learning as much as possible without privileging specific views<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-7\"><sup>[7]</sup></a> and without (yet) entering a mode where they form convictions.</p><p>That said, \u201cforming convictions\u201d is not an entirely voluntary process \u2013 sometimes, we can\u2019t help but feel confident about something after learning the details of a particular debate. As I\u2019ll elaborate below, it is partly for this reason that I think no reflection strategy is inherently superior.</p></blockquote><p>Comparing these two reflection strategies is a core theme of the post, and one takeaway I get to is that none of the two ends of the spectrum is superior to the other. Instead, I see moral reflection as a bit of an art, and we just have to find our personal point on the spectrum.</p><p>Relatedly, there's also the question of \"What's the benefit of reflection now\" vs. \"how much do we want to just leave things to future selves or hypothetical future selves in a reflection procedure.\" (The point being that it is is not by-default obvious that moral reflection has to be postponed!)</p><blockquote><p>Reflection procedures are thinking-and-acting sequences we'd undergo if we had unlimited time and resources. While we cannot properly run a moral reflection procedure right now in everyday life, we can still narrow down our uncertainty over the hypothetical reflection outcome. Spending time on that endeavor is worth it if the value of information \u2013 gaining clarity on one\u2019s values \u2013 outweighs the opportunity cost from acting under one\u2019s current (less certain) state of knowledge.</p><p>Gaining clarity on our values is easier for those who would employ a more conservative reflection strategy in their moral reflection procedure. After all, that means their strategy involves guarding some pre-existing convictions, which gives them advance knowledge of the direction of their moral reflection.<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-9\"><sup>[9]</sup></a></p><p>By contrast, people who would employ more open-minded reflection strategies may not easily be able to move past specific layers of indecision. Because they may be uncertain how to approach moral reasoning in the first place, they can be \u201cstuck\u201d in their uncertainty. (Their hope is to get unstuck once they are <i>inside</i> the reflection procedure, once it becomes clearer how to proceed.)</p><p>[...]</p><p><strong>If moral realism were true,</strong> the <i>timing</i> of that transition (\u201cthe reflection strategy becoming increasingly conservative as the person forms more convictions\u201d) is obvious. It would happen once the person knows enough to <i>see the correct answers</i>, once they see the correct way of narrowing down their reflection or (eventually) the correct values to adopt at the very end of it.</p><p>In the moral realist picture, expressions like \u201csafeguarding opinions\u201d or \u201cforming convictions\u201d (which I use interchangeably) seem out of place.&nbsp;Obviously, the idea is to \u201cform convictions\u201d about only the <i>correct</i> principles!</p><p>However, as I\u2019ve argued in previous posts, moral realism is likely false.</p></blockquote><p>This is then followed by a discussion on whether \"idealized values\" are chosen or discovered.</p><blockquote><p>Under moral anti-realism, there are two empirical possibilities<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-10\"><sup>[10]</sup></a> for \u201cWhen is someone ready to form convictions?.\u201d In the first possibility, things work similarly to naturalist moral realism but on a personal/subjectivist basis. We can describe this option as \u201cMy idealized values are here for me to discover.\u201d By this, I mean that, at any given moment, there\u2019s a fact of the matter to \u201cWhat I\u2019d conclude with open-minded moral reflection.\u201d (Specifically, a <i>unique</i> fact \u2013 it cannot be that I would conclude vastly different things in different runs of the reflection procedure or that I would find myself indifferent about a whole range of options.)</p><p>The second option is that my idealized values <i>aren\u2019t</i> \u201chere for me to discover.\u201d In this view, open-minded reflection is too passive \u2013 therefore, we have to create our values actively. Arguments for this view include that (too) open-minded reflection doesn\u2019t reliably terminate; instead, one must bring normative convictions to the table. \u201cForming convictions,\u201d according to this second option, is about making a particular moral view/outlook a part of one\u2019s identity as a morality-inspired actor. Finding one\u2019s values, then, is <i>not just</i> about intellectual insights.</p><p>I will argue that the truth is somewhere in between.</p></blockquote><p>Why do I think this? There's more in my post, but here are some of the interesting bits, which seem especially relevant to the topic of \"long reflection\":</p><blockquote><p>There are two reasons why I think open-minded reflection isn\u2019t automatically best:</p><ol><li>We have to make judgment calls about how to structure our reflection strategy. Making those judgment calls already gets us in the business of forming convictions. So, if we are qualified to do that (in \u201cpre-reflection mode,\u201d setting up our reflection procedure), why can\u2019t we also form <i>other</i> convictions similarly early?</li><li>Reflection procedures come with an overwhelming array of options, and they can be risky (in the sense of having pitfalls \u2013 see later in this section).&nbsp;Arguably, we are closer (in the sense of our intuitions being more accustomed and therefore, arguably, more reliable) to many of the fundamental issues in moral philosophy than to matters like \u201ccarefully setting up a sequence of virtual reality thought experiments to aid an open-minded process of moral reflection.\u201d Therefore, it seems reasonable/defensible to think of oneself as better positioned to form convictions about object-level morality (in places where we deem it safe enough).</li></ol><h2><strong>Reflection strategies require judgment calls</strong></h2><p>In this section, I\u2019ll elaborate on how specifying reflection strategies requires many judgment calls. The following are some dimensions alongside which judgment calls are required (many of these categories are interrelated/overlapping):</p><ul><li><strong>Social distortions:</strong> Spending years alone in the reflection environment could induce loneliness and boredom, which may have undesired effects on the reflection outcome. You could add other people to the reflection environment, but who you add is likely to influence your reflection (e.g., because of social signaling or via the added sympathy you may experience for the values of loved ones).</li><li><strong>Transformative changes:</strong> Faced with questions like whether to augment your reasoning or capacity to experience things, there\u2019s always the question \u201cWould I still trust the judgment of this newly created version of myself?\u201d</li><li><strong>Distortions from (lack of) competition:</strong> As Wei Dai points out in <a href=\"https://www.lesswrong.com/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition?commentId=SaYyRB3g2bMbFNomZ\">this Lesswrong comment</a>: \u201cCurrent human deliberation and discourse are strongly tied up with a kind of resource gathering and competition.\u201d By competition, he means things like \u201cthe need to signal intelligence, loyalty, wealth, or other \u2018positive\u2019 attributes.\u201d Within some reflection procedures (and possibly depending on your reflection strategy), you may not have much of an incentive to compete. On the one hand, a lack of competition or status considerations could lead to \u201cpurer\u201d or more careful reflection. On the other hand, perhaps competition functions as a safeguard, preventing people from adopting values where they cannot summon sufficient motivation under everyday circumstances. Without competition, people\u2019s values could become decoupled from what ordinarily motivates them and more susceptible to idiosyncratic influences, perhaps becoming more extreme.</li><li><strong>Lack of morally urgent causes:</strong> In the blogpost <a href=\"https://mindingourway.com/on-caring/\"><i>On Caring</i></a>, Nate Soares writes: \u201cIt's not enough to think you <i>should</i> change the world \u2014 you also need the sort of desperation that comes from realizing that you would dedicate your entire life to solving the world's 100th biggest problem if you could, but you can't, because there are 99 bigger problems you have to address first.\u201d<br>In that passage, Soares points out that desperation can strongly motivate why some people develop an identity around effective altruism. Interestingly enough, in some reflection environments (including \u201cMy favorite thinking environment\u201d), the outside world is on pause. As a result, the phenomenology of \u201cdesperation\u201d that Soares described would be out of place. If you suffered from poverty, illnesses, or abuse, these hardships are no longer an issue. Also, there are no <i>other</i> people to lift out of poverty and no factory farms to shut down. You\u2019re no longer in a race against time to prevent bad things from happening, seeking friends and allies while trying to defend your cause against corrosion from influence seekers. This constitutes a massive change in your \u201csituation in the world.\u201d Without morally urgent causes, you arguably become less likely to go all-out by adopting an identity around solving a class of problems you\u2019d deem urgent in the real world but which don\u2019t appear pressing inside the reflection procedure. Reflection inside the reflection procedure may feel more like writing that novel you\u2019ve always wanted to write \u2013 it has less the feel of a \u201cmission\u201d and more of \u201cdoing justice to your long-term dream.\u201d<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-11\"><sup>[11]</sup></a></li><li><strong>Ordering effects:</strong> The order in which you learn new considerations can influence your reflection outcome. (See page 7 in <a href=\"https://intelligence.org/2016/02/29/new-paper-defining-human-values-for-value-learners/\">this paper</a>. Consider a model of internal deliberation where your attachment to moral principles strengthens whenever you reach reflective equilibrium given everything you already know/endorse.)</li><li><strong>Persuasion and framing effects:</strong> Even with an AI assistant designed to give you \u201cvalue-neutral\u201d advice, there will be free parameters in the AI\u2019s reasoning that affect its guidance and how it words things. Framing effects may also play a role when interacting with other humans (e.g., epistemic peers, expert philosophers, friends, and loved ones).</li></ul><h2><strong>Pitfalls of reflection procedures</strong></h2><p>There are also pitfalls to avoid when picking a reflection strategy. The failure modes I list below are avoidable in theory,<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-12\"><sup>[12]</sup></a> but they could be difficult to avoid in practice:</p><ul><li><strong>Going off the rails:</strong> Moral reflection environments could be unintentionally alienating (enormous option space; time spent reflecting could be unusually long). Failure modes related to the strangeness of the moral reflection environment include existential breakdown and impulsively deciding to lock in specific values to be done with it.</li><li><strong>Issues with motivation and compliance:</strong> When you set up experiments in virtual reality, the people in them (including copies of you) may not always want to play along.</li><li><strong>Value attacks:</strong> Attackers could simulate people\u2019s reflection environments in the hope of influencing their reflection outcomes.</li><li><strong>Addiction traps:</strong> Superstimuli in the reflection environment could cause you to lose track of your goals. For instance, imagine you started asking your AI assistant for an experiment in virtual reality to learn about pleasure-pain tradeoffs or different types of pleasures. Then, next thing you know, you\u2019ve spent centuries in pleasure simulations and have forgotten many of your lofty ideals.</li><li><strong>Unfairly persuasive arguments:</strong> Some arguments may appeal to people because they exploit design features of our minds rather than because they tell us&nbsp; \u201cWhat humans truly want.\u201d Reflection procedures with argument search (e.g., asking the AI assistant for arguments that are persuasive to lots of people) could run into these unfairly compelling arguments. For illustration, imagine a story like \u201cAtlas Shrugged\u201d but highly persuasive to most people. We can also think of \u201carguments\u201d as sequences of experiences: Inspired by the <a href=\"https://narnia.fandom.com/wiki/Turkish_Delight\">Narnia story</a>, perhaps there exists a sensation of eating a piece of candy so delicious that many people become willing to sell out all their other values for eating more of it. Internally, this may feel like becoming convinced of some candy-focused morality, but looking at it from the outside, we\u2019ll feel like there\u2019s something problematic about how the moral update came about.)</li><li><strong>Subtle pressures exerted by AI assistants:</strong> AI assistants trained to be \u201cmaximally helpful in a value-neutral fashion\u201d may not be fully neutral, after all. (Complete) value-neutrality may be an illusory notion, and if the AI assistants mistakenly think they know our values better than we do, their advice could lead us astray. (See Wei Dai\u2019s comments in <a href=\"https://www.lesswrong.com/posts/T5ZyNq3fzN59aQG5y/the-limits-of-corrigibility?commentId=hE9CRFhEqLNkvW6bX#comments\">this thread</a> for more discussion and analysis.)</li></ul></blockquote><blockquote><h2><strong>Conclusion: \u201cOne has to actively create oneself\u201d</strong></h2><p>\u201cMoral reflection\u201d sounds straightforward \u2013 naively, one might think that the right path of reflection will somehow reveal itself. However, as we think of the complexities of setting up a suitable reflection environment and how we\u2019d proceed inside it, what it would be like and how many judgment calls we\u2019d have to make, we see that things can get tricky.</p><p>Joe Carlsmith summarized it as follows in an <a href=\"https://forum.effectivealtruism.org/posts/qQK3ZLBdsMf3j5zdh/on-the-limits-of-idealized-values\">excellent post</a> (what Carlsmith calls \u201cidealizing subjectivism\u201d corresponds to what I call \u201cdeferring to moral reflection\u201d):</p><p>&gt;My current overall take is that especially absent certain strong empirical assumptions, &gt;idealizing subjectivism is ill-suited to the role some hope it can play: namely, providing &gt;a privileged and authoritative (even if subjective) standard of value. Rather, the &gt;version of the view I favor mostly reduces to the following (mundane) observations:</p><ul><li>If you already value X, it\u2019s possible to make instrumental mistakes relative to X.</li><li>You can choose to treat the outputs of various processes, and the attitudes of various hypothetical beings, as authoritative to different degrees.</li></ul><p>&gt;This isn\u2019t necessarily a problem. To me, though, it speaks against treating your &gt;\u201cidealized values\u201d the way a <a href=\"https://handsandcities.com/2021/01/03/the-despair-of-normative-realism-bot/\">robust meta-ethical realist</a> treats the \u201ctrue values.\u201d That is, &gt;you cannot forever aim to approximate the self you \u201cwould become\u201d; you must actively &gt;create yourself, often in the here and now. Just as the world can\u2019t tell you what to &gt;value, neither can your various hypothetical selves \u2014 unless you choose to let them. Ultimately, it\u2019s on you.</p><p>In my ((Lukas's)) words, the difficulty with deferring to moral reflection too much is that the benefits of reflection procedures (having more information and more time to think; having access to augmented selves, etc.) don\u2019t change what it feels like, fundamentally, to contemplate what to value. For all we know, many people would continue to feel apprehensive about doing their moral reasoning \u201cthe wrong way\u201d since they\u2019d have to make judgment calls left and right. Plausibly, no \u201ccorrect answers\u201d would suddenly appear to us. To avoid leaving our views under-defined, we have to \u2013 at some point \u2013 form convictions by committing to certain principles or ways of reasoning. As Carslmith describes it, one has to \u2013 at some point \u2013 \u201cactively create oneself.\u201d (The alternative is to accept the possibility that one\u2019s reflection outcome may be under-defined.)</p><p>It is <i>possible</i> to delay the moment of \u201cactively creating oneself\u201d to a time within the reflection procedure. (This would correspond to an open-minded reflection strategy; there are strong arguments to keep one\u2019s reflection strategy at least moderately open-minded.) However, note that, in doing so, one \u201cactively creates oneself\u201d as someone who trusts the reflection procedure more than one\u2019s object-level moral intuitions or reasoning principles. This may be true for some people, but it isn\u2019t true for everyone. Alternatively, it could be true for someone in some domains but not others.<a href=\"https://forum.effectivealtruism.org/posts/6STzb6XBAyu3Xxxka/the-moral-uncertainty-rabbit-hole-fully-excavated#fn-3RbphKB2djKSeg8Tx-13\"><sup>[13]</sup></a></p></blockquote><p>I further discuss the notion of \"having under-defined values.\" This happens if someone defers to moral reflection with the expectation that it'll terminate with a specific answer, but they're pre-disposed to following reflection strategies that are open-ended enough so that the reflection will, in practice, have under-defined outcomes.</p><p>Having under-defined values isn't necessarily a problem \u2013 I discuss the pros and cons of it in the post.</p><p>Towards the end of the post, there's a section where I discuss the IMO most sophisticated wager for \"acting as though moral realism is true\" (the wager for <i>naturalist</i> moral realism, rather than the one for non-naturalist/irreducible-normativity-based moral realism which I discussed earlier in my sequence). In that discussion, I conclude that this naturalist moral realism wager actually often doesn't overpower what we'd do anyway under anti-realism. (The reasoning here is that naturalist moral realism feels somewhat watered down compared to non-naturalist moral realism, so that it's actually \"built on the same currency\" as how we'd anyway structure our reasoning under moral anti-realism. Consequently, whether naturalist moral realism is true isn't too different from the question of whether idealized values are chosen or discovered \u2013 it's just that now we're also asking about the degree of moral convergence between different people's reflection.)<br><br>Anyway, that section is hard to summarize, so I recommend just reading it in full in the post (it has pictures and a fun \"mountain analogy.\")<br><br>Lastly, I end the post with some condensed takeaways in the form of <i>advice for someone's moral reflection</i>:</p><blockquote><h1><strong>Selected takeaways: good vs. bad reasons for deferring to (more) moral reflection</strong></h1><p>To list a few takeaways from this post, I made a list of good and bad reasons for deferring (more) to moral reflection. (Note, again, that deferring to moral reflection comes on a spectrum.)</p><p>In this context, it\u2019s important to note that deferring to moral reflection would be wise if moral realism is true or if idealized values are ((on the far end of the spectrum of)) \u201chere for us to discover.\u201d In this sequence, I argued that neither of those is true \u2013&nbsp;but some (many?) readers may disagree.</p><p><strong>Assuming that I\u2019m right about the flavor of moral anti-realism I\u2019ve advocated for in this sequence,</strong> below are my \u201cgood and bad reasons for deferring to moral reflection.\u201d</p><p>(Note that this is not an exhaustive list, and it\u2019s pretty subjective. Moral reflection feels more like an art than a science.)</p><p><strong>Bad reasons for deferring strongly to moral reflection:</strong></p><ul><li>You haven\u2019t contemplated the possibility that the feeling of \u201ceverything feels a bit arbitrary; I hope I\u2019m not somehow doing moral reasoning the wrong way\u201d may never go away unless you get into a habit of forming your own views. Therefore, you never practiced the steps that could lead to you forming convictions. Because you haven\u2019t practiced those steps, you assume you\u2019re far from understanding the option space well enough, which only reinforces your belief that it\u2019s too early for you to form convictions.</li><li>You observe that other people\u2019s fundamental intuitions about morality differ from yours. You consider that an argument for trusting your reasoning and your intuitions less than you otherwise would. As a result, you lack enough trust in your reasoning to form convictions early.</li><li>You have an unreflected belief that things don\u2019t matter if moral anti-realism is true. You want to defer strongly to moral reflection because there\u2019s a possibility that moral realism is true. However, you haven\u2019t thought about the argument that naturalist moral realism and moral anti-realism use the same currency, i.e., that the moral views you\u2019d adopt if moral anti-realism were true might matter just as much to you.</li></ul><p><strong>Good reasons for deferring strongly to moral reflection:</strong></p><ul><li>You don\u2019t endorse any of the bad reasons, and you still feel drawn to deferring to moral reflection. For instance, you feel genuinely unsure how to reason about moral views or what to think about a specific debate (despite having tried to form opinions).</li><li>You think your present way of visualizing the moral option space is unlikely to be a sound basis for forming convictions. You suspect that it is likely to be highly incomplete or even misguided compared to how you\u2019d frame your options after learning more science and philosophy inside an ideal reflection environment.</li></ul><p><strong>Bad reasons for forming some convictions early:</strong></p><ul><li>You think moral anti-realism means there\u2019s no for-you-relevant sense in which you can be wrong about your values.</li><li>You think of yourself as a rational agent, and you believe rational agents must have well-specified \u201cutility functions.\u201d Hence, ending up with under-defined values (which is a possible side-effect of deferring strongly to moral reflection) seems irrational/unacceptable to you.</li></ul><p><strong>Good reasons for forming some convictions early:</strong></p><ul><li>You can\u2019t help it, and you think you have a solid grasp of the moral option space (e.g., you\u2019re likely to pass <a href=\"https://www.econlib.org/archives/2011/06/the_ideological.html\">Ideological Turing tests</a> of some prominent reasoners who conceptualize it differently).</li><li>You distrust your ability to guard yourself against unwanted opinion drift inside moral reflection procedures ((if you were to follow a more open-minded reflection strategy)), and the views you already hold feel too important to expose to that risk.</li></ul></blockquote>", "parentCommentId": null, "user": {"username": "Lukas_Gloor"}}, {"_id": "xG8eyx3H7QfwMmEYW", "postedAt": "2024-03-26T06:08:42.505Z", "postId": "4xwWDLfMenw48TR8c", "htmlBody": "<p>Are there any readings about how a long reflection could be realistically and concretely achieved?</p>", "parentCommentId": null, "user": {"username": "michaelchen"}}, {"_id": "jcrmtJBDqF5B7rzeH", "postedAt": "2024-03-28T00:41:49.514Z", "postId": "4xwWDLfMenw48TR8c", "htmlBody": "<p>Thanks, lots of interesting articles in this list that I missed despite my interest in this area.</p>\n<p>One suggestion I have is to add some studies of failed attempts at building/reforming institutions, otherwise one might get a skewed view of the topic. (Unfortunately I don't have specific readings to suggest.)</p>\n<p>A related topic you don't mention here (maybe due to lack of writings on it?) is maybe humanity should pause AI development and have a long (or even short!) reflection about what it wants to do next, e.g. resume AI development or do something else like subsidize intelligence enhancement (e.g. embryo selection) for everyone who wants it so more people can meaningfully participate in deciding the fate of  our world. (I note that many topics on this reading list are impossible for most humans to fully understand, perhaps even with AI assistance.)</p>\n<blockquote>\n<p>I claim that this area outscores regular AI safety on importance while being significantly more neglected</p>\n</blockquote>\n<p>This neglect is itself perhaps one of the most important puzzles of our time. With AGI very plausibly just a few years away, why aren't more people throwing money or time/effort at this cluster of problems just out of self interest? Why isn't there more intellectual/academic interest in these topics, many of which seem so intrinsically interesting to me?</p>\n", "parentCommentId": null, "user": {"username": "Wei_Dai"}}, {"_id": "piwSFt5rzt9jgFLcL", "postedAt": "2024-03-28T15:21:10.470Z", "postId": "4xwWDLfMenw48TR8c", "htmlBody": "<p>Great resource, thanks for putting this together!</p>", "parentCommentId": null, "user": {"username": "Iyngkarran Kumar"}}, {"_id": "Lh4Xi2qf7RtEQdpFe", "postedAt": "2024-03-28T17:00:51.920Z", "postId": "4xwWDLfMenw48TR8c", "htmlBody": "<blockquote><p>This neglect is itself perhaps one of the most important puzzles of our time. With AGI very plausibly just a few years away, why aren't more people throwing money or time/effort at this cluster of problems just out of self interest? Why isn't there more intellectual/academic interest in these topics, many of which seem so intrinsically interesting to me?</p></blockquote><p>I think all of:</p><ul><li>Many people seem to believe in something like \"AI will be a big deal, but the singularity is much further off (or will never happen)\".</li><li>People treat the singularity in far mode even if they admit belief.</li><li>Previously commited people (especially academics) don't shift their interests or research areas much based on events in the world, though they do rebrand their prior interests. It requires new people entering fields to actually latch onto new areas and there hasn't been enough time for this.</li><li>People who approach these topics from an altruistic perspective often come away with the view \"probably we can mostly let the AIs/future figure this out, other topics seems more pressing and more possible to make progress on.</li><li>There aren't clear shovel ready projects.</li></ul>", "parentCommentId": "jcrmtJBDqF5B7rzeH", "user": {"username": "Ryan Greenblatt"}}]