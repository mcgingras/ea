[{"_id": "KsF2FmvepKXtXyn2x", "postedAt": "2024-01-05T01:43:57.310Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I appreciate the pivot to a better-devised and merely pessimistic strategy on MIRI's part, as opposed to a deceptively dignified and misrepresentative resignation to death.</p>\n", "parentCommentId": null, "user": {"username": "Evan_Gaensbauer"}}, {"_id": "szEDtXc5RxJxukRaS", "postedAt": "2024-01-05T08:37:38.285Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I appreciate the impressive epistemic humility it must have taken for one of the original and most prestigious alignment research orgs to decide that right now prioritising policy and communications work over research might be the best course to follow. I would imagine that might be a somewhat painful decision for technical people who have devoted their life to finding a technical solution. Nice one!</p>\n<p>\"Although we plan to pursue all three of these priorities, it\u2019s likely that policy and communications will be a higher priority for MIRI than research going forward.\"</p>\n", "parentCommentId": null, "user": {"username": "NickLaing"}}, {"_id": "MdGboNkRbLcZsTi6B", "postedAt": "2024-01-05T11:01:23.194Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Could you unpack (1) how you plan to work towards \"Increase the probability that the major governments of the world end up coming to some international agreement\" and (2) how confident you are a foundational research org can transition into and make a difference in the policy space?</p>\n", "parentCommentId": null, "user": {"username": "maxime"}}, {"_id": "Y7NSvtnBGarcSg7DB", "postedAt": "2024-01-05T15:49:31.264Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I\u2019m curious, since it sounds like MIRI folks may have thought about this, if you have takes on how best to allocate marginal effort between pushing for cooperation-to-halt-AI-progress on the one hand, and accelerating cognitive enhancement (e.g., mind uploading) on the other?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefho16lse8va\"><sup><a href=\"#fnho16lse8va\">[1]</a></sup></span></p><p>Like, I see that you list promoting cooperation as a priority, but to me, based on your footnote 3, it doesn\u2019t seem obvious that promoting cooperation to buy ourselves time is a better strategy at the margin than simply working on mind uploading.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvijb35x83tm\"><sup><a href=\"#fnvijb35x83tm\">[2]</a></sup></span>&nbsp;(At least, I don\u2019t see this being obviously true for people-trying-to-reduce-AI-risk at large, and I\u2019d be interested in your\u2014or others\u2019\u2014thoughts here, in case there\u2019s something I\u2019m missing. It may well be clearly true for MIRI given your comparative advantages; I\u2019m asking this question from the perspective of overall AI risk reduction strategy.) Here\u2019s that footnote 3:</p><blockquote><p>Nate and Eliezer both believe that humanity should not be attempting technical alignment at its current level of cognitive ability, and should instead pursue human cognitive enhancement (e.g., via uploading), and then having smarter (trans)humans figure out alignment.</p></blockquote><hr><p>Related recent discussion:</p><ul><li>\u201c<a href=\"https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work\"><u>Does davidad's uploading moonshot work?</u></a>\u201d<ul><li>Context: David Dalrymple (aka davidad) recently outlined a concrete plan for mind uploading by 2040.</li></ul></li></ul><p>Related prediction markets:</p><ul><li>Eliezer\u2019s Manifold market, \u201c<a href=\"https://manifold.markets/EliezerYudkowsky/if-artificial-general-intelligence\"><u>If Artificial General Intelligence has an okay outcome, what will be the reason?</u></a>\u201d<ul><li>At present, the leading answer is: \u201cHumanity successfully coordinates worldwide to prevent the creation of powerful AGIs for long enough to develop human intelligence augmentation, uploading, or some other pathway into transcending humanity's window of fragility.\u201d</li></ul></li><li>My Metaculus question, \u201c<a href=\"https://www.metaculus.com/questions/18839/mind-uploading-before-agi/\"><u>Will mind uploading happen before AGI?</u></a>\u201d<ul><li>The current community prediction is 1%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpd1ag96cs18\"><sup><a href=\"#fnpd1ag96cs18\">[3]</a></sup></span></li></ul></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnho16lse8va\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefho16lse8va\">^</a></strong></sup></span><div class=\"footnote-content\"><p>ETA: I\u2019ve just noticed that earlier today, another Forum user posted a quick take on a similar theme, asking why there\u2019s been no EA funding for cognitive enhancement projects. <a href=\"https://forum.effectivealtruism.org/posts/4H4me8R8cWuJ5Zu5F/nicholaskross-s-quick-takes?commentId=pD5dmuqhcLDyJMKFN\">See here</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvijb35x83tm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvijb35x83tm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The immediate lines of reasoning I can think of for why \u201cput all marginal effort towards pausing AI\u201d is the best strategy right now are: i) uploading is intractable given AGI timelines, and ii) future, just-before-the-pause models\u2014GPT-7, say\u2014could help significantly with mind uploading R&amp;D. But then, assuming that uploading is our best bet for getting alignment right, I think ii just shifts the discussion to things like \u201cwhere is the best place to pause (with respect to the tradeoff between powerful automation of uploading R&amp;D versus not pausing too late)?\u201d and \u201care there ways to push for differential progress in models\u2019 capabilities? (e.g., narrow superhuman ability in neuroscience research).\u201d</p><p>What\u2019s more, as counters to i: Firstly, <a href=\"https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-fall-within-a-100x-tractability-range-under#:~:text=I%20think%20with%201000x%20the%20resources%2C%20humanity%20could%20have%20way%20better%20than%2010%25%20chance%20of%20starting%20a%20Mars%20colony%5B3%5D%2C%20solving%20the%20Riemann%20hypothesis%2C%20and%20doing%20other%20really%20difficult%20things.\">most problems fall within a 100x tractability range</a>. Secondly, even if cooperation+pause efforts are clearly higher impact right now than <i>object-level</i> uploading work, I think there\u2019s still the argument that <i>field-building</i> for mind uploading should start now, rather than once the pause is in place. Because if field-building starts now, then with luck there\u2019ll be a body of uploading researchers ready to make the most of a future pause. (This argument doesn\u2019t go through if the pause lasts indefinitely, because in that case there\u2019s time to build up the mind uploading field from scratch in the pause. But it does go through if the pause is limited or fragile, which I tentatively believe are more likely possibilities. See also Scott Alexander\u2019s&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/7WfMYzLfcTyDtD6Gn/pause-for-thought-the-ai-pause-debate\"><u>taxonomy of AI pauses</u></a>.)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpd1ag96cs18\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpd1ag96cs18\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Taken together, these two prediction markets arguably paint a grim picture. Namely, the trades on Eliezer\u2019s question imply that mind uploading is the most likely way that AGI goes well for humanity, but the forecasts on my question imply that we\u2019re very unlikely to get mind uploading before AGI.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Will Aldred"}}, {"_id": "qfzFEJN6sozyG9d5d", "postedAt": "2024-01-05T18:31:43.988Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Strong downvoted. This isn't a laughing matter.</p><p>I understand what it's like to think of a really funny joke and not want to waste it. But this isn't an appropriate environment to substitute charisma for substance.</p><p>If EA grows by, say, 30% per year, then that means at any given time there's going to be a large number of people on the forum who will see this, think it's normal, and upvote it (reinforcing that behavior). Even if professional norms hold strong, it will still make the onboarding process that much harder and more confusing for the new people, as they are misled into making serious social-status-damaging faux passes, and that reputation might follow them around in the community for years regardless of how talented or valuable they become.</p>", "parentCommentId": "szEDtXc5RxJxukRaS", "user": {"username": "trevorw96"}}, {"_id": "u2xsSLNneZFeRK2dP", "postedAt": "2024-01-05T18:58:52.137Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I assumed Nick was being sincere?&nbsp;</p>", "parentCommentId": "qfzFEJN6sozyG9d5d", "user": {"username": "Larks"}}, {"_id": "Jjwd45gBPNvJgbv6L", "postedAt": "2024-01-05T19:00:30.263Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I'm confused. The comment reads as sincere to me? What part of it did you think was a joke?</p>", "parentCommentId": "qfzFEJN6sozyG9d5d", "user": {"username": "titotal"}}, {"_id": "DdabQ5ANyGsHPA4Z7", "postedAt": "2024-01-05T19:21:34.945Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Will - we seem to be many decades away from being able to do 'mind uploading' or serious levels of cognitive enhancement, but we're probably only a few years away from extremely dangerous AI.&nbsp;</p><p>I don't think that betting on mind uploading or cognitive enhancement is a winning strategy, compared to pausing, heavily regulating, and morally stigmatizing AI development.</p><p>(Yes, given a few generations of iterated embryo selection for cognitive ability, we could probably breed much smarter people within a century or two. But they'd still run a million times slower than machine intelligences. As for mind uploading, we have nowhere near the brain imaging abilities required to do whole-brain emulations of the sort envisioned by Robin Hanson)</p>", "parentCommentId": "Y7NSvtnBGarcSg7DB", "user": {"username": "geoffreymiller"}}, {"_id": "EYwhvpeiCXbveDPg7", "postedAt": "2024-01-05T19:27:50.981Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Malo - bravo on this pivot in MIRI's strategy and priorities. Honestly it's what I've hoped MIRI would do for a while. It seems rational, timely, humble, and very useful! I'm excited about this.</p><p>I agree that we're very unlikely to solve 'technical alignment' challenges fast enough to keep AI safe, given the breakneck rate of progress in AI capabilities. If we can't speed up alignment work, we have to slow down capabilities work.&nbsp;</p><p>I guess the big organizational challenge for MIRI will be whether its current staff, who may have been recruited largely for their technical AI knowledge, general rationality, and optimism about solving alignment, can pivot towards this more policy-focused and outreach-focused agenda -- which may require quite different skill sets.&nbsp;</p><p>Let me know if there's anything I can do to help, and best of luck with this new strategy!</p>", "parentCommentId": null, "user": {"username": "geoffreymiller"}}, {"_id": "zZxztYZEEZhnwgGKv", "postedAt": "2024-01-05T20:42:03.749Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Yes I was being sincere. I might have missed some meta thing here as obviously I'm not steeped in AI alignment. Perhaps Trevor intended to reply on another comment but mistakenly replied here?</p>", "parentCommentId": "u2xsSLNneZFeRK2dP", "user": {"username": "NickLaing"}}, {"_id": "ANRyxfJmAZx6RDJJp", "postedAt": "2024-01-05T20:52:17.956Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Oops! I'm off my groove today, sorry. I'm going to go read up on some of the <a href=\"https://forum.effectivealtruism.org/topics/conflict-theory-vs-mistake-theory\">conflict theory vs. mistake theory</a> literature on my backlog in order to figure out what went wrong and how to prevent it (e.g. how human variation and inferential distance causes very strange mistakes due to miscommunication).</p>", "parentCommentId": "zZxztYZEEZhnwgGKv", "user": {"username": "trevorw96"}}, {"_id": "kQRLpFbqHvyMZyBH9", "postedAt": "2024-01-05T23:02:26.856Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>How does the choice to publish MIRI's main views as LessWrong posts rather than, say, articles in peer-reviewed journals or more pieces in the media, square with the need to convince a much broader audience (including decision-makers in particular)?</p>\n", "parentCommentId": null, "user": {"username": "Guy Raveh"}}, {"_id": "z7HeoRHQH55sDH7F3", "postedAt": "2024-01-06T02:25:29.743Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>There is no button you can press on demand to publish an article in either a peer-reviewed journal or a mainstream media outlet.</p><p>Publishing pieces in the media (with minimal 3rd-party editing) is at least tractable on the scale of weeks, if you have a friendly journalist. &nbsp;The academic game is one to two orders of magnitude slower than that. &nbsp;If you want to communicate your views in real-time, you need to stick to platforms which allow that.</p><p>I do think media comms is a complementary strategy to direct comms (which MIRI has been using, to some degree). &nbsp;But it's difficult to escape the fact that information posted on LW, the EA forum, or Twitter (by certain accounts) makes its way down the grapevine to relevant decision-makers surprisingly often, given how little overhead is involved.</p>", "parentCommentId": "kQRLpFbqHvyMZyBH9", "user": {"username": "T3t"}}, {"_id": "aZncoQddhdHLzYWfH", "postedAt": "2024-01-06T04:33:14.285Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Every aspect of that summary of how MIRI's strategy has shifted seems misleading or inaccurate to me.</p>", "parentCommentId": "KsF2FmvepKXtXyn2x", "user": {"username": "RobBensinger"}}, {"_id": "YuQLSaSKCwrufXSpm", "postedAt": "2024-01-06T13:08:51.790Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<blockquote>\n<p>Publishing pieces in the media (with minimal 3rd-party editing) is at least tractable on the scale of weeks, if you have a friendly journalist.  The academic game is one to two orders of magnitude slower than that.</p>\n</blockquote>\n<p>Given that MIRI has held these views for decades, I don't quite see how the timeline for academic publication is of issue here.</p>\n", "parentCommentId": "z7HeoRHQH55sDH7F3", "user": {"username": "Guy Raveh"}}, {"_id": "EJJwtpEFeADBGdokW", "postedAt": "2024-01-06T13:40:37.324Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<blockquote><p>But it's difficult to escape the fact that information posted on LW, the EA forum, or Twitter (by certain accounts) makes its way down the grapevine to relevant decision-makers surprisingly often, given how little overhead is involved.</p></blockquote><p><br>This isn't necessarily a good thing, if the information being passed down is flawed or incorrect, due to the lack of rigor involved.&nbsp;</p><p>The judges of quality for peer reviewed papers are domain level experts who contribute their relevant expertise. The judges of quality for blog posts are a collection of random people on the internet, often few of which have relevant expertise and who are often unable to distinguish between actual truth and convincing sounding BS.&nbsp;</p><p>The ideal situation would be to write peer reviewed papers and then communicate their results on blogs, but this won't be a good fit for a lot of things, given that some fields are not well established and some points are too small or obvious to be worth writing up academically.&nbsp;</p>", "parentCommentId": "z7HeoRHQH55sDH7F3", "user": {"username": "titotal"}}, {"_id": "64ixNK6DCfWHXhD5X", "postedAt": "2024-01-08T22:01:02.168Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>This was an acerbic and bitter comment I made as a reference to the fake MIRI strategy update in 2022 from Eliezer, the notorious \"Dying with Dignity.\" I've thought about this for a few days and I'm sorry I made that nasty comment.</p>\n<p>I was considering deleting or retracting it, though I've decided against that. The fact my comment has a significantly net negative karma score seems like punishment enough. Retracting the comment now probably wouldn't change that anyway.</p>\n<p>I've decided against deleting or retracting this comment because its reception seems like a useful signal for MIRI to receive. At least as of the time I'm writing this reply, my original comment has received more agreement than disagreement. It's valid for you or whoever from MIRI disagrees with the perception I snarkily expressed as wrong or unserious. I expect it's still worth MIRI being aware that almost as many people still distrust as trust MIRI as being sufficiently honest in its public communications.</p>\n", "parentCommentId": "aZncoQddhdHLzYWfH", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "xkpCARE8SszGGvmLy", "postedAt": "2024-01-09T12:11:41.922Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I fully agree with the shift away from research and toward policy. With how close we are to what you termed smarter-than-human AI (also called AGI or ASI, but your term is much more precise, so I'll use it going forward), research is not where efforts are best placed. We could be looking at a human extinction scenario (or an equally bad outcome, such as the permanent limiting of human potential) within 5-20 years. That's an emergency situation as far as I'm considered. Once the necessary laws and procedures are in place, research an continue.&nbsp;<br><br>I can't speak for all EAs, but my ultimate goal is to see a world without smarter-than-human AI until humanity outgrows its tendencies to wage war and seek personal gain over the flourishing of all sentient beings. This would likely place ASI development somewhere between 100 years AP* and never, and probably closer to the \"never\" end of that timescale. This is something we have to accept--especially those of us with tech-loving tendencies.&nbsp;</p><p>I'm under no illusions that Silicon Valley would ever accept this, but in a democratic society they aren't the ones calling the shots. A democratic government can ban agents / generative / smarter-than-human AI, and the actors I mentioned previously would simply have to accept it. We need the US, EU, Canada, Taiwan, and Japan to adopt MIRI guidelines on AI safety, security, and non-proliferation--and these conversations <i>must</i> begin at the local level.&nbsp;</p><p>If we are looking to shift the Overton window, we have to target our communications toward \"ordinary people\" and policymakers, not tech geeks and data wonks. This will be my top priority going forward, along with animal welfare activism.</p><p>*AP = after present</p>", "parentCommentId": null, "user": {"username": "Hayven Jackson"}}, {"_id": "na8hQGMt2gFRNuAfA", "postedAt": "2024-01-09T12:31:21.235Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>This is why I don't think the goal should be to grow the movement. Movements that grow by seeking converts usually end up drifting far from their original mission <i>and</i> taking on negative, irrational aspects of the societies they emerge from. Religious and political history provide dozens of examples of this process taking place.&nbsp;<br><br>EA should be about quality over quantity just in my opinion, and \"social status\" is both figuratively and literally worthless in the face of extinction.&nbsp;</p>", "parentCommentId": "qfzFEJN6sozyG9d5d", "user": {"username": "Hayven Jackson"}}, {"_id": "j3AbNWfQ3via4Bawn", "postedAt": "2024-01-09T12:41:37.516Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Thank you for this well-sourced comment. I'm not affiliated with MIRI, so I can't answer the questions directed to the OP. With that said, I did have a small question to ask you. What would be your issue with simply accepting human fragility and limits? Does the fact that we don't and can't know everything, live no more than a century, and are at risk for disease and early death mean that we should fundamentally alter our nature?&nbsp;<br><br>I think the best antidote to the present moment's dangerous dance with AI isn't mind uploading or transhumanism, but acceptance. We can accept that we are animals, that we will not live forever, and that any ultimate bliss or salvation won't come via silicon. We can design policies that ensure these principles are always upheld.&nbsp;</p>", "parentCommentId": "Y7NSvtnBGarcSg7DB", "user": {"username": "Hayven Jackson"}}, {"_id": "waaF5S9pXkYgyggs7", "postedAt": "2024-01-09T12:56:14.642Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Agreed, but&nbsp;as I said earlier, acceptance seems to be the answer. We are limited, biological beings, who aren't capable of understanding everything about ourselves or the universe. We're animals. I understand this leads to anxiety and disquiet for a lot of people. Recognizing the danger of AI and the impossibility of transhumanism and mind uploading, I think the best possible path forward is to just accept our limited state, rationally stagnate our technology, and focus on social harmony and environmental protection as the way forward.&nbsp;</p><p>As for the despair this could cause to some, I'm not sure what the answer is. EA has taken a lot of its organizational structure and methods of moral encouragement from philosophies like Confucianism, religions, universities, etc. Maybe an EA-led philosophical research project into human ultimate hope (in the absence of techno-salvation) would be fruitful.&nbsp;</p>", "parentCommentId": "DdabQ5ANyGsHPA4Z7", "user": {"username": "Hayven Jackson"}}, {"_id": "RuSzykQhdrJC7Ruiu", "postedAt": "2024-01-10T00:46:50.342Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I just want to share that I think you did an excellent job explaining the arguments on the <a href=\"https://podcasts.apple.com/gh/podcast/humans-are-not-some-peak-of-cognitive-ability/id1500970749?i=1000638292624\">recent Politico Tech podcast</a>, in a way that I think comes across as very grounded and reasonable, which makes me more optimistic that MIRI can make this shift. I also hope that you can nudge Eliezer more towards this style of communication, which I think would make his audience more receptive. (I thought the tone of the TIME piece didn't seem professional enough). This seems especially important if Eliezer will also focus on communications and policy instead of research.</p>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "iJiBeSjGbeJnxf3up", "postedAt": "2024-01-10T02:19:18.629Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Hayven - there's a huge, huge middle ground between reckless e/acc ASI accelerationism on the one hand, and stagnation on the other hand.</p><p>I can imagine a moratorium on further AGI research that still allows awesome progress on all kinds of wonderful technologies such as longevity, (local) space colonization, geoengineering, etc -- none of which require AGI.&nbsp;</p>", "parentCommentId": "waaF5S9pXkYgyggs7", "user": {"username": "geoffreymiller"}}, {"_id": "tKvHYXZN3E3FeaGd2", "postedAt": "2024-01-10T03:59:33.257Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>We can certainly research those things, but using purely human efforts (no AI) progress will likely take many decades to see even modest gains. From a longtermist perspective that's not a problem of course, but it's a difficult thing to sell to someone not excited about living what is essentially a 20th century life so we can make progress long after they are gone. A ban on AI should come with a cultural shift toward a much less individualistic, less present-oriented value set.</p>", "parentCommentId": "iJiBeSjGbeJnxf3up", "user": {"username": "Hayven Jackson"}}, {"_id": "FtvCFakqkNThvGayB", "postedAt": "2024-01-10T13:39:14.500Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Congratulations on a great prioritization!<br><br>Perhaps the research that we (Existential Risk Observatory) and others (e.g. <a href=\"https://forum.effectivealtruism.org/users/nik-samoylov-1?mention=user\">@Nik Samoylov</a>, <a href=\"https://forum.effectivealtruism.org/users/koenschoen?mention=user\">@KoenSchoen</a>) have done on effectively communicating AI xrisk, could be something to build on. Here's our first <a href=\"https://existentialriskobservatory.org/papers_and_reports/The_Effectiveness_of_AI_Existential_Risk_Communication_to_the_American_and_Dutch_Public.pdf\">paper</a> and <a href=\"https://forum.effectivealtruism.org/posts/fqXLT7NHZGsLmjH4o/paper-summary-the-effectiveness-of-ai-existential-risk\">three</a> <a href=\"https://forum.effectivealtruism.org/posts/YweBjDwgdco669H72/ai-x-risk-in-the-news-how-effective-are-recent-media-items\">blog</a> <a href=\"https://forum.effectivealtruism.org/posts/EoqeJCBiuJbMTKfPZ/unveiling-the-american-public-opinion-on-ai-moratorium-and\">posts</a> (the second includes measurement of Eliezer's TIME article effectiveness - its numbers are actually pretty good!). We're currently working on a base rate public awareness update and further research.<br><br>Best of luck and we'd love to cooperate!</p>", "parentCommentId": null, "user": {"username": "Otto"}}, {"_id": "DbdzsGdpvap5GSKiZ", "postedAt": "2024-01-10T17:27:40.114Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I think there is an unstated assumption here that uploading is safe. And by safe, I mean existentially safe for humanity<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9vc6bmtkxom\"><sup><a href=\"#fn9vc6bmtkxom\">[1]</a></sup></span>. If in addition to being uploaded, a human is uplifted to superintelligence, would they -- indeed <i>any</i> given human in such a state -- be aligned <i>enough</i> with humanity as a whole to not cause an existential disaster? Arguably humans right now are only relatively existentially safe because power imbalances between them are limited.<br><br>Even the nicest human could accidentally obliterate the rest of us if uplifted to superintelligence and left running for subjective millions of years (years of our time). \"Whoops, I didn't expect that to happen from my little physics experiment\"; \"Uploading everyone into a hive mind is what my extrapolations suggested was for the best (and it was just so boring talking to you all at one word per week of my time)\".</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9vc6bmtkxom\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9vc6bmtkxom\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although safety for the individual being uploaded would be far from guaranteed either.&nbsp;</p></div></li></ol>", "parentCommentId": "Y7NSvtnBGarcSg7DB", "user": {"username": "Greg_Colbourn"}}, {"_id": "2osSLhgZuadbCwJd7", "postedAt": "2024-01-10T19:39:29.516Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Yes, this is a fair point; Holden has discussed these dangers a little in \u201c<a href=\"https://forum.effectivealtruism.org/posts/AKxKR4CeakyBsGFoH/digital-people-would-be-an-even-bigger-deal#Would_these_impacts_be_a_good_or_bad_thing_\"><u>Digital People Would Be An Even Bigger Deal</u></a>\u201d. My bottom-line belief, though, is that mind uploads are still significantly more likely&nbsp;to be safe than ML-derived ASI, since uploaded minds would presumably work, and act, much more similarly to (biological) human minds. My impression is that <a href=\"https://forum.effectivealtruism.org/posts/ySCqffZTKtZp97JFB/digital-people-could-make-ai-safer?commentId=dFn7egYxiboW9A4dr\">others also hold this view</a>? I\u2019d be interested if you disagree.</p><p>To be clear, I rank moratorium &gt; mind uploads &gt; ML-derived ASI, but I think it\u2019s plausible that our strategy portfolio should include mind uploading R&amp;D alongside pushing for a moratorium.</p>", "parentCommentId": "DbdzsGdpvap5GSKiZ", "user": {"username": "Will Aldred"}}, {"_id": "YM8LvbKNF7K79xGkk", "postedAt": "2024-01-10T20:20:20.794Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>I agree that they would most likely be saf<i>er </i>than ML-derived ASI. What I'm saying is that they still won't be safe <i>enough </i>to prevent an existential catastrophe. It might buy us a bit more time (if uploads happen before ASI), but that might only be measured in years. Moratorium &gt;&gt; mind uploads &gt; ML-derived ASI.</p>", "parentCommentId": "2osSLhgZuadbCwJd7", "user": {"username": "Greg_Colbourn"}}, {"_id": "Q82aDNWXzhC2h2HpR", "postedAt": "2024-01-10T20:43:16.941Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Why do you expect an existential catastrophe from augmented mind uploads?</p>\n", "parentCommentId": "YM8LvbKNF7K79xGkk", "user": {"username": "MichaelStJules"}}, {"_id": "jJYo7Sz9jZkA9pTPn", "postedAt": "2024-01-10T20:46:19.141Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>We could upload many minds, trying to represent some (sub)distribution of human values (EDIT: and psychological traits), and augment them all slowly, limiting power imbalances between them along the way.</p>\n", "parentCommentId": "DbdzsGdpvap5GSKiZ", "user": {"username": "MichaelStJules"}}, {"_id": "zrfssECJ8wsqya6mC", "postedAt": "2024-01-10T21:44:44.792Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Because of the crazy high power differential, and propensity for accidents (can a human really not mess up on an existential scale if acting for millions of years subjectively at superhuman capability levels?). As I say in my <a href=\"https://forum.effectivealtruism.org/posts/Lvd2DFaHKfuveaCyQ/miri-2024-mission-and-strategy-update?commentId=DbdzsGdpvap5GSKiZ\">comment above</a>:</p><blockquote><p>Even the nicest human could accidentally obliterate the rest of us if uplifted to superintelligence and left running for subjective millions of years (years of our time). \"Whoops, I didn't expect that to happen from my little physics experiment\"; \"Uploading everyone into a hive mind is what my extrapolations suggested was for the best (and it was just so boring talking to you all at one word per week of my time)\".</p></blockquote>", "parentCommentId": "Q82aDNWXzhC2h2HpR", "user": {"username": "Greg_Colbourn"}}, {"_id": "v7NreDSer2EXy7jvZ", "postedAt": "2024-01-10T21:47:29.480Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Perhaps. But remember they will be smarter than us, so controlling them might not be so easy (especially if they gain access to enough computer power to speed themselves up massively. And they need not be hostile, just curious, to accidentally doom us.)</p>", "parentCommentId": "jJYo7Sz9jZkA9pTPn", "user": {"username": "Greg_Colbourn"}}, {"_id": "hvwdEPKkfKupdcpDb", "postedAt": "2024-01-10T23:28:08.930Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>This doesn\u2019t seem like a strong enough argument to justify a high probability of existential catastrophe (if that's what you intended?).</p>\n<p>At vastly superhuman capabilities (including intelligence and rationality), it should be easier to reduce existential-level mistakes to tiny levels. They would have vastly more capability for assessing and mitigating risks and for moral reflection (not that this would converge to some moral truth; I don\u2019t think there is any).</p>\n<p>If you think this has a low chance of success (if we could delay AGI long enough to actually do it), then alignment seems pretty hopeless to me on that view, and a temporary pause only delays the inevitable doom.</p>\n<p>I do think we could do better (for upside-focused views) by ensuring more value pluralism and preventing particular values from dominating, e.g. by uploading and augmenting multiple minds.</p>\n", "parentCommentId": "zrfssECJ8wsqya6mC", "user": {"username": "MichaelStJules"}}, {"_id": "tEKLCf7x9fk9hPkAN", "postedAt": "2024-01-11T15:37:31.746Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<blockquote><p>At vastly superhuman capabilities (including intelligence and rationality), it should be easier to reduce existential-level mistakes to tiny levels. They would have vastly more capability for assessing and mitigating risks and for moral reflection</p></blockquote><p>They are still human though, and humans are famous for making mistakes, even the most intelligent and rational of us. It's even regarded by many as part of what being human is - being fallible. That's not (too much of) a problem at current power differentials, but it is when we're talking of solar-system-rearranging powers for millions of subjective years without catastrophic error...</p><blockquote><p>a temporary pause only delays the inevitable doom.</p></blockquote><p>Yes. The pause should be indefinite, or at least until global consensus to proceed, with democratic acceptance of whatever risk remains.</p>", "parentCommentId": "hvwdEPKkfKupdcpDb", "user": {"username": "Greg_Colbourn"}}, {"_id": "Ys6v6jA3n3BSt7x7Y", "postedAt": "2024-01-12T15:02:16.130Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Nonsensical cheems. Go for a walk. Have a drink. Lighten up.</p>", "parentCommentId": null, "user": {"username": "Bob"}}, {"_id": "hFiZZd7LiDvNHkHpP", "postedAt": "2024-02-14T22:59:49.430Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>We\u2019ve also been doing media and we\u2019re working on building capacity and gaining expertise to do more of it more effectively.</p><p>Publishing research in more traditional venues is also something we\u2019ve been chatting about internally.</p>", "parentCommentId": "kQRLpFbqHvyMZyBH9", "user": {"username": "malo"}}, {"_id": "z8fDD4x6WF44jrfSB", "postedAt": "2024-02-14T23:11:48.967Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<blockquote><p>I expect it's still worth MIRI being aware that almost as many people still distrust as trust MIRI as being sufficiently honest in its public communications.</p></blockquote><p>FWIW, I found this last bit confusing. In my experience chatting with folk, regardless of how much they agree with or like MIRI, they usually think MIRI is quite candid an honest in it\u2019s communication.&nbsp;</p><p>(TBC, I do think the \u201cDeath with Dignity\u201d post was needlessly confusing, but that\u2019s not the same thing as dishonest.)</p>", "parentCommentId": "64ixNK6DCfWHXhD5X", "user": {"username": "malo"}}, {"_id": "vrpsGkibjH4MQXaxX", "postedAt": "2024-03-02T21:11:54.356Z", "postId": "Lvd2DFaHKfuveaCyQ", "htmlBody": "<p>Judging from all the comments in agreement, from people who probably have no political power to actually implement these things, but who might have been useful toward actually solving the problem, this pivot is probably a net negative. You will probably fail at having much of a political influence, but succeed at dissuading people from doing technical research.</p>", "parentCommentId": null, "user": {"username": "Akmon Ra"}}]