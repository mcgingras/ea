[{"_id": "CzdRcaJWm3tTvPeDJ", "postedAt": "2023-05-02T17:19:05.050Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>To push back a bit on the fast software-driven takeoff (i.e. a fast takeoff driven primarily by innovations in software):&nbsp;</p><blockquote><p>Common objections&nbsp;to this narrative [of a fast software-driven takeoff] are that there won\u2019t be enough compute, or data, for this to happen. These don\u2019t hold water after a cursory examination of our situation. We are nowhere near close to the <a href=\"https://en.wikipedia.org/wiki/Limits_of_computation\"><u>physical limits to computation</u></a> ...</p></blockquote><p>While we're nowhere near the physical limits to computation, it's still true that hardware progress has slowed down considerably on <a href=\"https://forum.effectivealtruism.org/posts/7cCr6vAmN4Xi3yzR5/two-contrasting-models-of-intelligence-and-future-growth#The_fastest_supercomputers_and_other_hardware_measures\">various measures</a>. I think the steelman of the compute-based argument against a fast software-driven takeoff is not that the ultimate limits to computation are near, but rather that the <i>pace</i> of hardware progress is unlikely to be explosively fast (e.g. in light of recent trends that <a href=\"https://forum.effectivealtruism.org/posts/7cCr6vAmN4Xi3yzR5/two-contrasting-models-of-intelligence-and-future-growth#Superhuman_software_capabilities_increasingly_advance_technological_development___yet_growth_rates_appear_to_be_declining\">arguably</a> point in the opposite direction, and because software progress per se seems insufficient for driving explosive hardware progress).</p><blockquote><p>We're on ~10^18 <a href=\"https://en.wikipedia.org/wiki/FLOPS#:~:text=floating%20point%20operations%20per%20second\"><u>FLOPS</u></a>&nbsp;for large clusters at <a href=\"https://epochai.org/trends#:~:text=Plausible-,2e25%20FLOP,-Total%20training%20compute\"><u>the</u></a> <a href=\"https://www.lesswrong.com/posts/iQx2eeHKLwgBYdWPZ/retrospective-on-gpt-4-predictions-after-the-release-of-gpt#:~:text=than%20I%20expected.-,Training%20time,suggests%20that%20GPT%2D4%20was%20trained%20for%20about%204%2D7%20months.,-I%20originally%20predicted\"><u>moment</u></a><a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#fnn5hudcqszgo\"><sup>[14]</sup></a>, but there are likely enough GPUs available for 100 times this<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#fnampu9lp5m9b\"><sup>[15]</sup></a>. And the cost of this \u2013 ~<a href=\"https://en.wikipedia.org/wiki/GPT-4#:~:text=the%20cost%20of%20training%20GPT%2D4%20was%20more%20than%20%24100%20million\"><u>$10B</u></a>&nbsp;- is affordable for many <a href=\"https://companiesmarketcap.com/software/largest-software-companies-by-market-cap/\"><u>large tech companies</u></a>&nbsp;and <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_government_budget\"><u>national governments</u></a>, and even <a href=\"https://www.bloomberg.com/billionaires/\"><u>individuals</u></a>!</p></blockquote><p>That actors can afford to create this next generation of AIs does not imply that those AIs will in turn lead to a hard takeoff in capabilities. From my perspective at least, that seems like an unargued assumption here.</p><blockquote><p>Data is not a show-stopper either. Sure, ~all the text on the internet might\u2019ve already been digested, but Google could&nbsp;readily record more words per <i>day</i>&nbsp;via <a href=\"https://nordvpn.com/blog/is-my-phone-listening-to-me/\"><u>phone mics</u></a><a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#fn5znedjfxl6\"><sup>[16]</sup></a>&nbsp;than the number used to train GPT-4<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#fn3e7xci5t6mi\"><sup>[17]</sup></a>. These <a href=\"https://twitter.com/GregAttilaKiss/status/1650555251462746119\"><u>may or may not</u></a>&nbsp;be as high&nbsp;quality as text, but 1000x as many as all the text on the internet could be gathered within months<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#fnhhjzwfe2vsm\"><sup>[18]</sup></a>. Then there are all the billions of high-res video cameras (<a href=\"https://www.bankmycell.com/blog/how-many-phones-are-in-the-world#:~:text=How%20Many%20People%20Have%20Smartphones%20In%20The%20World%3F&amp;text=According%20to%20Statista%2C%20in%202023,world's%20population%20owns%20a%20smartphone.\"><u>phones</u></a>&nbsp;and <a href=\"https://www.visualcapitalist.com/ranked-the-worlds-most-surveilled-cities/#:~:text=IHS%20Markit%20estimates%20that%20as,billion%20surveillance%20cameras%20installed%20worldwide.\"><u>CCTV</u></a>), and <a href=\"https://www.statista.com/statistics/728541/sensors-and-controllers-market-size-worldwide/\"><u>sensors</u></a>&nbsp;in the world<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#fnvpiohsvnh3\"><sup>[19]</sup></a>. And if that is not enough, there is already a fast-growing <a href=\"https://www.grandviewresearch.com/industry-analysis/synthetic-data-generation-market-report#:~:text=The%20global%20synthetic%20data%20generation,has%20spurred%20the%20industry's%20growth.\"><u>synthetic data industry</u></a>&nbsp;serving the ML community\u2019s ever growing thirst for data to train their models on.</p></blockquote><p>A key question is whether this extra data would be all that valuable to the main tasks of concern. For example, it seems unclear whether low-quality data from phone conversations, video cameras, etc. would give that much of a boost to a model's ability to write code. So I don't think the point made above, as it stands, is a strong rebuttal to the claim that data will soon be a limiting bottleneck to significant capability gains. (<a href=\"https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset\">Some</a> <a href=\"https://jacobbuckman.com/2022-06-14-an-actually-good-argument-against-naive-ai-scaling/\">related</a> <a href=\"https://jacobbuckman.substack.com/p/we-arent-close-to-creating-a-rapidly\">posts</a>.)</p><blockquote><p>The time for talking politely to (and working with) big AI is over.</p></blockquote><p>This is another claim I would push back against. For instance, from a perspective <a href=\"https://twitter.com/joe_zimmerman/status/1639403125969915905\">concerned</a> with the reduction of s-risks, one could argue that talking politely to, and working with, leading AI companies is in fact the most responsible thing to do, and that taking a less cooperative stance is unduly risky and irresponsible. To be clear, I'm not saying that this is obviously the case, but I'm trying to say that it's not clear-cut either way. Good arguments can be made for a different approach, and this seems true for a wide range of altruistic values.</p>", "parentCommentId": null, "user": {"username": "MagnusVinding"}}, {"_id": "8FZTq48JrmfsspPyk", "postedAt": "2023-05-02T20:41:45.837Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Current scaling \"laws\" are not laws of nature. And there are already worrying signs that things like dataset optimization/pruning, curriculum learning and synthetic data might well break them - It seems likely to me that LLMs will be useful in all three. I would still be worried even if LLMs prove useless in enhancing architecture search.</p>", "parentCommentId": "CzdRcaJWm3tTvPeDJ", "user": {"username": "Bjartur T\u00f3mas"}}, {"_id": "avPMDBF9QQcr6sufX", "postedAt": "2023-05-02T23:23:45.008Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<blockquote><p>Common objections&nbsp;to this narrative are that there won\u2019t be enough compute, or data, for this to happen. These don\u2019t hold water after a cursory examination of our situation. We are nowhere near close to the <a href=\"https://en.wikipedia.org/wiki/Limits_of_computation\"><u>physical limits to computation</u></a>&nbsp;(the theoretical limit set by physics is, to a first approximation, as many orders of magnitude above current infrastructure as current infrastructure is above the <a href=\"https://www.computerhistory.org/babbage/\"><u>babbage engine</u></a>).</p></blockquote><p>And we are nowhere, nowhere, nowhere, <i>nowhere</i> near being able to even begin approaching them. We're closer to nanotech. We're closer to dyson spheres. I can't emphasize this enough. It's like an iron-age armorsmith worrying about the physical limits of tensile strength.</p><p>Hardware overhang is a legitimate concern but you hurt your argument quite badly by mixing it with this stuff. In the whole history of the universe no one will ever be hit in the head with <strong>an entire ringworld. </strong>It's <i>steel</i> you have to worry about.</p>", "parentCommentId": null, "user": {"username": "yefreitor"}}, {"_id": "HzBRXccHmnM8oJm2o", "postedAt": "2023-05-03T04:29:20.973Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Thanks for writing this. I appreciate the effort and sentiment. My quick and unpolished thoughts are below. I wrote this very quickly, so feel free to critique.<br><br>The TLDR is that I think that this is good with some caveats but also that we need more work on our ecosystem to be able to do outreach (and everything else) better.</p><p><strong>I think we need a better AI Safety movement to properly do and benefit from outreach work.</strong> Otherwise, this and similar posts for outreach/action are somewhat like a call to arms without the strategy, weapons and logistics structure needed to support them.&nbsp;<br><br>Doing the things you mention is&nbsp;<i>probably</i> better than doing nothing (some of these more than others), but it's far what is possible in terms or risk minimisation and expected impact.&nbsp;<br>&nbsp;</p><p><strong>What do we need for the AI Safety movement to properly do and benefit from outreach work?</strong></p><p>I think that doing effective collective outreach will require us to be more centralised and coordinated.&nbsp;</p><p>Right now, we have people like you who seem to believe that we need to act urgently to engage people and raise awareness, in opposition to other influential people like Rohin Shah, Oliver Harbynka, who seem to oppose movement building (though this may just be the recruitment element).<br><br>The polarisation and uncertainty promotes inaction.</p><p>I therefore don't think that we will get anything close to maximally effective awareness raising about AI risk until we have a related strategy and operational plan that has enough support from key stakeholders or is led by one key stakeholder (e.g., Will/Holden/Paul) and actioned by those who trust that person's takes.</p><p>Here are my related (low confidence) intuitions (based on<a href=\"https://forum.effectivealtruism.org/s/RwtygELTfbRJzcvwD\">&nbsp;<u>this</u></a><u>&nbsp;</u>and related conversations mainly) for what to do next:</p><p><i>We need to find/fund/choose some/more people/process to drive overall strategy and operation for the mainstream AI Safety community.&nbsp;</i>For instance, we could just have some sort of survey/voting system to capture community preferences/elect someone. I don't know what makes sense now, but it's worth thinking about.&nbsp;</p><p><i>When we know what the community/representatives see as the strategy and supporting operation, we need someone/some process to figure out who is responsible for executing the overall strategy and parts of the operations and communicating them to relevant people. </i>We need behaviour level statements for 'who needs to do what differently'.</p><p><i>When we know 'who needs to do what differently' we need to determine and address the blockers and enablers to scale and sustain the strategy and operation</i> (e.g., we likely need researchers to find what communication works with different audiences, communicators to write things, connect with, and win over, influential/powerful people, recruiters to recruit the human resources, developers and designers to make persuasive digital media, managers to manage these groups, entrepreneurs to start and scale the project, and funders to support the whole thing etc).</p><p>It's a big ask, but it might be our best shot.</p><p>&nbsp;</p><p><strong>Why hasn't somebody done this already?</strong></p><p>As I see it, the main reason for all of the above is <a href=\"https://forum.effectivealtruism.org/posts/8XZmu8BM5JBtSnHiP/a-proposed-approach-for-ai-safety-movement-building-projects#There_is_uncertainty_about_what_AI_Safety_Movement_Building_is__and_how_to_do_it_helpfully\">a lack of shared language and understanding which merged because of how the AI safety community developed</a>.&nbsp;</p><p>Movement building/field building mean different things to different people and no-one knows what the community collectively support or oppose in this regard. This uncertainty reduces attempts to do anything on behalf of the community or the chances of success if anyone tries.&nbsp;</p><p>Perhaps because of this no-one who could curate preferences and set a direction (e.g., Will/Holden/Paul) feels confident to do so.&nbsp;</p><p>It's potentially a chicken and egg or coincidence of wants problem where most people would like someone like Holden to drive the agenda, but he doesn't know or thinks someone would be better suited (and they don\u2019t know). Or the people who could lead somehow know that the community doesn\u2019t want anyone to lead it in this way, but haven't communicated this, so I don\u2019t know that yet.</p><p>&nbsp;</p><p><strong>What happens if we keep going as we are?</strong></p><p>I think that the EA community (with some exceptions) will mostly continue to function like a decentralised group of activists, posting conflicting opinions in different forums and social media channels, while doing high quality, but small scale, AI safety governance, technical and strategy work that is mostly known and respected in the communities it is produced in.</p><p>Various other more centralised groups with leaders like Sam Altman, Tristan Harris, Tina Gebru etc will drive the conversations and changes. That might be for the best, but I suspect not.</p><p>&nbsp;</p><p><strong>Urgent, unplanned communication by EA acting insolation poses many risks.&nbsp;</strong>If lots of people who don't know what works for changing people's minds and behaviours post lots of things about how they feel this could be bad.&nbsp;</p><p>These people could very well end up in isolated communities (e.g., just like many vegan activists I see who are mainly just reaching vegan followers on social media).&nbsp;</p><p>They could poison the well and make people associate AI safety with poorly informed and overconfident pessimists.&nbsp;</p><p>If people engage in civil disobedience we could end being feared and hated and subsequently excluded from consideration and conversation.</p><p>Our actions could create abiding associations that will damage later attempts to persuade by more persuasive sources.</p><p>This could be the unilateralist's curse brought to life.<br>&nbsp;</p><p><strong>Other thoughts/suggestions</strong></p><p>Test the communication in small scale (e.g., with a small sample of people on mechanical turk or with friends) before you do large scale outreach&nbsp;</p><p>Think about taking steps back to prioritise between the behaviour to rule out the ones with more downside risk (so better to write letters to representatives than posts to large audiences on social media if unsure what is persuasive).</p><p>Don\u2019t do civil disobedience unless you have read the literature about when and where it works (and maybe just don\u2019t do it - that could backfire badly).<br>&nbsp;</p><p><i>Think about the AI Safety ecosystem and indirect ways to get more of what you want by influencing/aiding people or processes within it:</i></p><p>For instance, I'd like for progress on questions like:</p><p>- what are the main arguments for and against doing certain things (e.g., the AI pause/public awareness raising), what is the expert consensus on whether a strategy/action would be a good idea or not (e.g., what do superforcasters/AI orgs recommend)?</p><p>- When we have evidence for a strategy/action, then: Who needs to do what differently? Who do we need to communicate to, and what do we know is persuasive to them/how can we test that?</p><p>- Which current AI safety (e.g., technical, strategy, movement building) projects are the ones that are worth prioritising the allocation of resources (funding, time, advocacy) to etc?&nbsp;What do experts think?<br>- What voices/messages can we amplify when we communicate? It's much easier to share something good from an expert than write it.</p><p>- Who could work with others for mutual benefit but doesn\u2019t realise it yet?</p><p><br>I am thinking about, and doing, a little of some of these things, but have other obligations for 3-6 months and some uncertainty about whether I am well suited to do them.</p>", "parentCommentId": null, "user": {"username": "Peterslattery"}}, {"_id": "3bAEsegXS7uHKSfvv", "postedAt": "2023-05-03T04:32:15.118Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Thanks for writing this - it was useful to read the pushbacks!&nbsp;<br><br>As I said below, I want more synthesis of these sorts of arguments. I know that some academic groups are preparing literature reviews of the key arguments for and against AGI risk.<br><br>I really think that we should be doing that for ourselves as a community and to make sure that we are able to present busy smart people with more compelling content than a range of arguments spread across many different forum posts.&nbsp;<br><br>I don't think that that is going to cut it for many people in the policy space.</p>", "parentCommentId": "CzdRcaJWm3tTvPeDJ", "user": {"username": "Peterslattery"}}, {"_id": "DKKQMAfrFJiCmzBLn", "postedAt": "2023-05-03T07:02:42.678Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>I'm convinced. Raising public awareness is the most important thing to do now. Is there a ready-made presentation I can use to communicate more effectively to the general public? A presentation that explains AGI risks from the ground up?</p>\n", "parentCommentId": null, "user": {"username": "RagingAgainstDarkness"}}, {"_id": "fYAdBomKLXtyEQphS", "postedAt": "2023-05-03T10:17:18.913Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<blockquote><p>hardware progress has slowed down considerably on <a href=\"https://forum.effectivealtruism.org/posts/7cCr6vAmN4Xi3yzR5/two-contrasting-models-of-intelligence-and-future-growth#The_fastest_supercomputers_and_other_hardware_measures\">various measures</a></p></blockquote><p>I don't think this matters, as per the next point about there <i>already</i> being enough compute for doom [Edit: I've relegated the \"nowhere near close to the physical limits to computation\" sentence to a footnote and added Magnus' reference on slowdown to it].</p><blockquote><p>That actors can afford to create this next generation of AIs does not imply that those AIs will in turn lead to a hard takeoff in capabilities. From my perspective at least, that seems like an unargued assumption here.</p></blockquote><p>I think the <a href=\"https://forum.effectivealtruism.org/posts/fk9pNpJbhTAEbQHNs/shifting-the-burden-of-proof-companies-should-prove-that\">burden of proof here needs to shift</a> to those willing to gamble on the safety of 100x larger systems. All I'm really saying here is that the risk is <i>way too high for comfort</i> (given the jumps in capabilities we've seen so far going from GPT-3-&gt;GPT3.5-&gt;GPT-4).<br><br><i>[Meta: would appreciate separate points being made in separate comments]</i>.&nbsp;<br>Will look into your links re data and respond later.</p><blockquote><p>from a perspective <a href=\"https://twitter.com/joe_zimmerman/status/1639403125969915905\">concerned</a> with the reduction of s-risks, one could argue that talking politely to, and working with, leading AI companies is in fact the most responsible thing to do, and that taking a less cooperative stance is unduly risky and irresponsible.</p></blockquote><p>I'm not sure what you are saying here? Do you think there is a risk of AI companies <i>deliberately causing</i> s-risks (e.g. releasing a basilisk) if we don't play nice!? They may be crazy in a sense of being reckless with the fate of billions of people's lives, but I don't think they are <i>that</i> crazy (in a sense of being sadistically malicious and spiteful toward their opponents)!</p>", "parentCommentId": "CzdRcaJWm3tTvPeDJ", "user": {"username": "Greg_Colbourn"}}, {"_id": "Bbe5acCuouzGjaSqK", "postedAt": "2023-05-03T10:21:06.846Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Agree. But at the same time, we need to do this <i>fast</i>! The typical academic paper review cycle is far too slow for this. We probably need groups like <a href=\"https://www.gov.uk/government/organisations/scientific-advisory-group-for-emergencies\">SAGE</a> (and <a href=\"https://www.independentsage.org/\">Independent SAGE</a>?) to step in. In fact, I'll try and get hold of them.. (they are for \"emergencies\" in general, not just Covid<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefun42psoy42\"><sup><a href=\"#fnun42psoy42\">[1]</a></sup></span>)</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnun42psoy42\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefun42psoy42\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Although it looks like they are highly specialised on viral threats. They would need totally new teams to be formed for AI. Maybe Hinton should chair?</p></div></li></ol>", "parentCommentId": "3bAEsegXS7uHKSfvv", "user": {"username": "Greg_Colbourn"}}, {"_id": "tYAQw7LLrjQHt82Lw", "postedAt": "2023-05-03T10:50:54.883Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Noted. It's the sentence after that (below) that is the more important one, so perhaps that should go first (and the rest as a footnote [Edit: I've now made these changes to OP]).</p><blockquote><p>We're on ~10^18 <a href=\"https://en.wikipedia.org/wiki/FLOPS#:~:text=floating%20point%20operations%20per%20second\"><u>FLOPS</u></a>&nbsp;for large clusters at <a href=\"https://epochai.org/trends#:~:text=Plausible-,2e25%20FLOP,-Total%20training%20compute\"><u>the</u></a> <a href=\"https://www.lesswrong.com/posts/iQx2eeHKLwgBYdWPZ/retrospective-on-gpt-4-predictions-after-the-release-of-gpt#:~:text=than%20I%20expected.-,Training%20time,suggests%20that%20GPT%2D4%20was%20trained%20for%20about%204%2D7%20months.,-I%20originally%20predicted\"><u>moment</u></a><a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and?commentId=3bAEsegXS7uHKSfvv#fnn5hudcqszgo\"><sup>[14]</sup></a>, but there are likely enough GPUs available for 100 times this<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and?commentId=3bAEsegXS7uHKSfvv#fnampu9lp5m9b\"><sup>[15]</sup></a>. And the cost of this \u2013 ~<a href=\"https://en.wikipedia.org/wiki/GPT-4#:~:text=the%20cost%20of%20training%20GPT%2D4%20was%20more%20than%20%24100%20million\"><u>$10B</u></a>&nbsp;- is affordable for many <a href=\"https://companiesmarketcap.com/software/largest-software-companies-by-market-cap/\"><u>large tech companies</u></a>&nbsp;and <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_government_budget\"><u>national governments</u></a>, and even <a href=\"https://www.bloomberg.com/billionaires/\"><u>individuals</u></a>!</p></blockquote>", "parentCommentId": "avPMDBF9QQcr6sufX", "user": {"username": "Greg_Colbourn"}}, {"_id": "KifoXYiNsiPo8TujB", "postedAt": "2023-05-03T12:12:43.763Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>These all seem like good suggestions, if we still had <i>years</i>. But what if we really do only have months (to get a global AGI moratorium in place)? In some sense the \"fog of war\" may already be upon us (there are already too many further things for me to read and synthesise, and analysis paralysis seems like a great path toward death). How did action on Covid unfold? Did all these kinds of things happen first before we got to lockdowns?&nbsp;</p><blockquote><p>vegan activists</p></blockquote><p>This is quite different. It's about personal survival of each and every person on Earth, and their families. (No concern for other people or animals is needed!)</p><blockquote><p>This could be the unilateralist's curse brought to life.</p></blockquote><p>Could it possibly be worse than what's already happened with DeepMind, OpenAI and Anthropic (and now Musk's X.ai)?</p><blockquote><p>have other obligations for 3-6 months</p></blockquote><p>Is there any way you can get out of those other obligations? Time really is of the essence here. Things are <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">already</a> <a href=\"https://www.telegraph.co.uk/politics/2023/04/30/artificial-intelligence-safety-darren-jones-rishi-sunak/\">moving</a> <a href=\"https://twitter.com/amanpour/status/1653452034463367168\">fast</a>, whether EA/the AI Safety community is coordinated and onboard or not.</p>", "parentCommentId": "HzBRXccHmnM8oJm2o", "user": {"username": "Greg_Colbourn"}}, {"_id": "FLoDmJYGqPxWcvWYJ", "postedAt": "2023-05-03T13:38:28.243Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Thanks for your reply, Greg :)</p><blockquote><p>I don't think this matters, as per the next point about there <i>already</i> being enough compute for doom</p></blockquote><p>That is what I did not find adequately justified or argued for in the post.</p><blockquote><p>I think the <a href=\"https://forum.effectivealtruism.org/posts/fk9pNpJbhTAEbQHNs/shifting-the-burden-of-proof-companies-should-prove-that\">burden of proof here needs to shift</a> to those willing to gamble on the safety of 100x larger systems.</p></blockquote><p>I suspect that a different framing might be more realistic and more apt from our perspective. In terms of helpful actions we can take, I more see the choice before us as one between trying to slow down development vs. trying to steer future development in better (or less bad) directions conditional on the current pace of development continuing (of course, one could dedicate resources to both, but one would still need to prioritize between them). Both of those choices (as well as graded allocations between them) seem to come with a lot of risks, and they both strike me as gambles with potentially serious downsides. I don't think there's really a \"safe\" choice here.</p><blockquote><p>All I'm really saying here is that the risk is <i>way too high for comfort</i></p></blockquote><p>I'd agree with that, but that seems different from saying that a fast software-driven takeoff is the most likely scenario, or that trying to slow down development is the most important or effective thing to do (e.g. compared to the alternative option mentioned above).</p>", "parentCommentId": "fYAdBomKLXtyEQphS", "user": {"username": "MagnusVinding"}}, {"_id": "vbekPFLCLem2LPZPp", "postedAt": "2023-05-03T13:39:54.452Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<blockquote><p>I'm not sure what you are saying here? Do you think there is a risk of AI companies <i>deliberately causing</i> s-risks (e.g. releasing a basilisk) if we don't play nice!?</p></blockquote><p>No, I didn't mean anything like that (although such crazy unlikely risks might also be marginally better reduced through cooperation with these actors). I was simply suggesting that cooperation could be a more effective way to reduce risks of worst-case outcomes that might occur in the absence of cooperative work to prevent them, i.e. work of the directional kind gestured at in my other comment (e.g. because ensuring the inclusion of certain measures to avoid worst-case outcomes has higher EV than does work to slow down AI). Again, I'm not saying that this is definitely the case, but it could well be. It's fairly unclear, in my view.</p>", "parentCommentId": "fYAdBomKLXtyEQphS", "user": {"username": "MagnusVinding"}}, {"_id": "v9aJjpi5t8cripYxY", "postedAt": "2023-05-03T14:19:49.060Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<blockquote><p>both strike me as gambles with potentially serious downsides.</p></blockquote><p>What are the downsides from slowing down? Things like not curing diseases and ageing? Eliminating wild animal suffering? I address that <a href=\"https://forum.effectivealtruism.org/posts/THogLaytmj3n8oGbD/p-doom-or-agi-is-high-why-the-default-outcome-of-agi-is-doom#comments:~:text=it%E2%80%99s%20a%20rather,I%20hope%20so.\">here</a>: <i>\"it\u2019s a rather depressing thought. We may be far closer to the </i><a href=\"https://dune.fandom.com/wiki/Butlerian_Jihad\"><i><u>Dune universe</u></i></a><i>&nbsp;than the </i><a href=\"https://en.wikipedia.org/wiki/Culture_series\"><i><u>Culture</u></i></a><i>&nbsp;one (the worry driving a future Butlerian Jihad will be the advancement of AGI algorithms to the point of individual laptops and phones being able to end the world). For those who may worry about the loss of the \u201cglorious transhumanist future\u201d, and in particular, radical life extension and cryonic&nbsp;reanimation (I\u2019m in favour of these things), I think there is some consolation in thinking that if a really strong </i><a href=\"https://forum.effectivealtruism.org/posts/i6btyefRRX23yCpnP/what-ai-companies-can-do-today-to-help-with-the-most?commentId=D5FA7R8t4iBfdFzET\"><i><u>taboo</u></i></a><i>&nbsp;emerges around AGI, to the point of stopping all algorithm advancement, we can still achieve these ends using standard supercomputers, bioinformatics and human scientists. I hope so.\"</i><br><br>To be clear, I'll also say that it's far too late to <i>only</i> steer future development better. For that, Alignment needs to be 10 years ahead of where it is now!</p><blockquote><p>a fast software-driven takeoff is the most likely scenario</p></blockquote><p>I don't think you need to believe this to want to be slamming on the brakes now. As <a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and?commentId=FLoDmJYGqPxWcvWYJ#:~:text=Even%20if%2C%20after,and%20cybersecurity%20domains.\">mentioned in the OP</a>, is the prospect of mere imminent global catastrophe not enough?</p>", "parentCommentId": "FLoDmJYGqPxWcvWYJ", "user": {"username": "Greg_Colbourn"}}, {"_id": "xvjgECNbjPRvsftAm", "postedAt": "2023-05-03T14:26:15.251Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Ok. I don't put much weight on s-risks being a likely outcome. Far more likely seems to be just that the solar system (and beyond) will be arranged in some (to us) arbitrary way, and all carbon-based life will be lost as collateral damage.&nbsp;<br><br>Although I guess if you are looking a bit nearer term, then s-risk from <a href=\"https://www.youtube.com/watch?v=ps_CCGvgLS8&amp;t=24m35s\">misuse</a> could be quite high. But I don't think any of the major players (OpenAI, Deepmind, Anthropic) are even really working on trying to prevent misuse at all as part of their strategy (their core AI Alignment work is on aligning the AIs, rather than the humans using them!) So actually, this is just another reason to shut it all down.</p>", "parentCommentId": "vbekPFLCLem2LPZPp", "user": {"username": "Greg_Colbourn"}}, {"_id": "MgSwmjAs8KM2YGsKg", "postedAt": "2023-05-03T14:50:55.323Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<blockquote><p>What are the downsides from slowing down?</p></blockquote><p>I'd again prefer to frame the issue as \"what are the downsides from spending marginal resources on efforts to slow down?\" I think the main downside, from this marginal perspective, is opportunity costs in terms of other efforts to reduce future risks, e.g. trying to implement \"<a href=\"https://longtermrisk.org/files/fail-safe-ai.pdf\">fail-safe measures</a>\"/\"<a href=\"https://arbital.com/p/hyperexistential_separation/\">separation from hyperexistential risk</a>\" in case a slowdown is insufficiently likely to be successful. There are <a href=\"https://longtermrisk.org/research-agenda\">various ideas</a> that one could try to implement.</p><p>In other words, a serious downside of betting chiefly on efforts to slow down over these alternative options could be that these s-risks/hyperexistential risks would end up being significantly greater in counterfactual terms (again, not saying this is clearly the case, but, FWIW, I doubt that efforts to slow down are among the most effective ways to reduce risks like these).</p><blockquote><blockquote><p>a fast software-driven takeoff is the most likely scenario</p></blockquote><p>I don't think you need to believe this to want to be slamming on the breaks on now.</p></blockquote><p>Didn't mean to say that that's a necessary condition for wanting to slow down. But again, I still think it's highly unclear whether efforts that push for slower progress are more beneficial than alternative efforts.</p>", "parentCommentId": "v9aJjpi5t8cripYxY", "user": {"username": "MagnusVinding"}}, {"_id": "cmZQt7XwYs2KPGyha", "postedAt": "2023-05-03T20:38:57.131Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Great post!</p><blockquote><p>\"All that is stopping them being even more powerful is spending on compute. Google &amp; Microsoft are worth <a href=\"https://companiesmarketcap.com/assets-by-market-cap/\"><u>$1-2T each</u></a>, and $10B can buy ~<a href=\"https://en.wikipedia.org/wiki/GPT-4#:~:text=the%20cost%20of%20training%20GPT%2D4%20was%20more%20than%20%24100%20million\"><u>100x</u></a>&nbsp;the compute&nbsp;used for GPT-4. Think about this: it means we are already well into &nbsp;<a href=\"https://www.lesswrong.com/tag/computing-overhang\"><u>hardware overhang</u></a>&nbsp;territory<a href=\"https://forum.effectivealtruism.org/posts/8YXFaM9yHbhiJTPqp/agi-rising-why-we-are-in-a-new-era-of-acute-risk-and#fnkei1iniqmpq\"><sup>[5]</sup></a>.\"</p></blockquote><p><br>I broadly agree with the point that compute could be scaled up significantly, but I want to add a few notes about the claim that $10B buys 100x the compute of GPT-4.<br><br>Altman said \"more\" when asked if GPT-4 had cost $100M to train. We don't know how much more. But PaLM seems to have only cost <a href=\"https://blog.heim.xyz/palm-training-cost/\">$9M-$23M</a> so $100M is probably reasonable.<br><br>&nbsp;If OpenAI was buying up 100x the compute of GPT-4, maybe that would be a big enough spike in demand for GPUs that they would become more expensive. I'm pretty uncertain about what to expect there, but I <a href=\"https://wiki.aiimpacts.org/doku.php?id=ai_timelines:hardware_and_ai_timelines:computing_capacity_of_all_gpus_and_tpus&amp;s[]=gpu\">estimated</a> that PaLM used the equivalent of 0.01% of the world's current GPU/TPU computing capacity for 2 months. GPT-4 seems to be bigger than PaLM, so 100x the compute used for it might be the equivalent of more than 1% of the world's existing GPU/TPU computing capacity for 2 months.</p>", "parentCommentId": null, "user": {"username": "Harlan"}}, {"_id": "9emitSm5cSpCx3noa", "postedAt": "2023-05-03T21:04:21.689Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>I think it's a very hard sell to try and get people to sacrifice themselves (and the whole world) for the sake of preventing \"fates worse than death\". At that point most people would probably just be pretty nihilistic. It also feels like it's not far off basically just giving up hope: the future is, at best, non-existence for sentient life; but we should still focus our efforts on avoiding hell. Nope. We should be doing all we can now to avoid having to face such a predicament! Global moratorium on AGI, now.</p>", "parentCommentId": "MgSwmjAs8KM2YGsKg", "user": {"username": "Greg_Colbourn"}}, {"_id": "CzM4JLxK2jnQMYe7w", "postedAt": "2023-05-03T21:13:02.921Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Thanks! Interested in getting more stats on global GPU/TPU capacity (there seems to be a dearth of good stats available). In your AI Impacts report, you mention paywalled reports. How much are they to access? (DM me and I can pay).<br><br>Epoch AI estimate that GPT-4 costs only <a href=\"https://epochai.org/trends#:~:text=Plausible-,%2440%20million,-Training%20cost%20of\">$40M</a> (not sure if that is today's price, with cost improvements since last year?)</p><blockquote><p>100x the compute used for it might be the equivalent of more than 1% of the world's existing GPU/TPU computing capacity for 2 months.</p></blockquote><p>1% seems well within reach for the biggest players :(</p>", "parentCommentId": "cmZQt7XwYs2KPGyha", "user": {"username": "Greg_Colbourn"}}, {"_id": "gASxRkP8oQAwj4Fja", "postedAt": "2023-05-04T08:42:16.705Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<blockquote><p>I think it's a very hard sell to try and get people to sacrifice themselves (and the whole world) for the sake of preventing \"fates worse than death\".</p></blockquote><p>I'm not talking about people sacrificing themselves or the whole world. Even if we were to adopt a purely survivalist perspective, I think it's still far from obvious that trying to slow things down is more effective than is focusing on other aims. After all, the space of alternative aims that one could focus on is vast, and trying to slow things down comes with non-trivial risks of its own (e.g. risks of backlash from tech-accelerationists). Again, I'm <i>not</i> saying it's clear; I'm saying that it seems to me unclear either way.</p><blockquote><p>We should be doing all we can now to avoid having to face such a predicament!</p></blockquote><p>But, as I see it, what's at issue is what the best way is to avoid such a predicament/how to best navigate given our current all-too risky predicament.</p><p>FWIW, I think that a lot of the discussion around this issue appears strongly fear-driven, to such an extent that it seems to get in the way of sober and helpful analysis. This is, to be sure, extremely understandable. But I also suspect that it is not the optimal way to figure out how to best achieve our aims, nor an effective way to persuade readers on this forum. Likewise, I suspect that rallying calls along the lines of \"Global moratorium on AGI, now\" might generally be received less well than would, say, a deeper analysis of the reasons for and against attempts to institute that policy.</p>", "parentCommentId": "9emitSm5cSpCx3noa", "user": {"username": "MagnusVinding"}}, {"_id": "57SsW3FRR6zBJ3Rfn", "postedAt": "2023-05-04T09:56:54.180Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>I don't know if everyone should drop everything else right now, but I do agree that raising awareness about AI xrisks should be a major cause area. That's why I quit my work on the energy transition about two years ago to found the Existential Risk Observatory, and this is what we've been doing since (resulting in about ten articles in leading Dutch newspapers, <a href=\"https://time.com/6258483/uncontrollable-ai-agi-risks/\">this one in TIME</a>, <a href=\"https://forum.effectivealtruism.org/posts/fqXLT7NHZGsLmjH4o/paper-summary-the-effectiveness-of-ai-existential-risk\">perhaps the first comms research</a>, a <a href=\"https://forum.effectivealtruism.org/posts/J3ribNjvPRtHCK7bC/organizing-a-debate-with-experts-and-mps-to-raise-ai-xrisk\">sold out debate</a>, and a passed <a href=\"https://www.tweedekamer.nl/kamerstukken/moties/detail?id=2023Z05424&amp;did=2023D12809\">parliamentary motion</a> in the Netherlands).<br><br>I miss two significant things on the list of what people can do to help:</p><p>1) Please, technical people, work on AI Pause regulation proposals! There is basically <a href=\"https://arxiv.org/abs/2303.11341\">one paper</a> now, possibly because everyone else thought a pause was too far outside the Overton window. Now we're discussing a pause anyway and I personally think it might be implemented at some point, but we don't have proper AI Pause regulation proposals, which is a really bad situation. Researchers (both policy and technical), please fix that, fix it publicly, and fix it soon!<br>2) You can start institutes or projects that aim to inform the societal debate about AI existential risk. We've done that and I would say it worked pretty well so far. Others could do the same thing. Funders should be able to choose from a range of AI xrisk communication projects to spend their money most effectively. This is currently really not the case.</p>", "parentCommentId": null, "user": {"username": "Otto"}}, {"_id": "uo3XctLbu6CAdR3aW", "postedAt": "2023-05-04T14:25:32.452Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Do you not trust Ilya when he says they have plenty more data?</p>\n<p><a href=\"https://youtu.be/Yf1o0TQzry8?t=656\">https://youtu.be/Yf1o0TQzry8?t=656</a></p>\n", "parentCommentId": "CzdRcaJWm3tTvPeDJ", "user": {"username": "Matt Brooks"}}, {"_id": "86yyir9pvcH3NYzwC", "postedAt": "2023-05-04T14:58:21.446Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>I feel like I'm one of the main characters in the film <a href=\"https://time.com/6273743/thinking-that-could-doom-us-with-ai/\"><i>Don't Look Up</i></a> here.</p><blockquote><p>the space of alternative aims that one could focus on is vast</p></blockquote><p>Please can you name 10? The way I see it is - either alignment is solved in time with business as usual<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefydvbtyojwvn\"><sup><a href=\"#fnydvbtyojwvn\">[1]</a></sup></span>, or we Pause to allow time for alignment to be solved (or establish it's impossibility). It is not a complicated situation. No need to be worrying about \"fates worse than death\" at <i>this</i> juncture.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnydvbtyojwvn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefydvbtyojwvn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>seems highly unlikely, but please say if you think there are promising solutions here</p></div></li></ol>", "parentCommentId": "gASxRkP8oQAwj4Fja", "user": {"username": "Greg_Colbourn"}}, {"_id": "QoZwsgdEHWREFtLdo", "postedAt": "2023-05-04T15:03:32.443Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>I didn't claim that there isn't plenty more data. But a relevant question is: plenty more data for what? He says that the data situation looks pretty good, which I trust is true in many domains (e.g. video data), and that data would probably in turn improve performance in those domains. But I don't see him claiming that the data situation looks good in terms of ensuring significant performance gains across all domains, which would be a more specific and stronger claim.</p><p>Moreover, the deference question could be posed in the other direction as well, e.g. do you not trust the <a href=\"https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset\">careful data collection and projections</a> of Epoch? (Though again, Ilya saying that the data situation looks pretty good is arguably not in conflict with Epoch's projections \u2014 nor with any claim I made above \u2014 mostly because his brief \"pretty good\" remark is quite vague.)</p><p>Note also that, at least in some domains, OpenAI could end up having <a href=\"https://www.youtube.com/watch?v=ivexBzomPv4&amp;ab_channel=AIExplained\"><i>less</i></a> data to train their models with going forward, as they might have been using data illegally.</p>", "parentCommentId": "uo3XctLbu6CAdR3aW", "user": {"username": "MagnusVinding"}}, {"_id": "KKkidnmmmGXuPzrLJ", "postedAt": "2023-05-04T15:07:46.119Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Great work you are doing with Existential Risk Observatory, Otto! Fully agree with your points too - have added them to the post. The Shavit paper is great btw, and the ideas should be further developed as a matter of priority (we need to have working mechanisms ready to implement).</p>", "parentCommentId": "57SsW3FRR6zBJ3Rfn", "user": {"username": "Greg_Colbourn"}}, {"_id": "x5nMEXqbwuJC4nzz9", "postedAt": "2023-05-04T15:12:13.865Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Let's hope that OpenAI is forced to pull GPT-4 over the illegal data harvesting used to create it.</p>", "parentCommentId": "QoZwsgdEHWREFtLdo", "user": {"username": "Greg_Colbourn"}}, {"_id": "jDhSoPDzPkQJzPmH8", "postedAt": "2023-05-06T15:22:11.173Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>FYI, I'm working on a book about the risks of AGI/ASI for a general and I hope to get it out within 6 months. It likely won't be as alarmist as your post but will try to communicate the key messages, the importance, the risks, and the urgency. Happy to have more help.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Darren McKee"}}, {"_id": "harymD4TmeYamnBCx", "postedAt": "2023-05-09T10:23:34.808Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p><a href=\"https://pauseai.info/\">Pauseai.info</a> is now up. Not sure if this is exactly what you are looking for, but covers the basic arguments.</p>", "parentCommentId": "DKKQMAfrFJiCmzBLn", "user": {"username": "Greg_Colbourn"}}, {"_id": "xgZoz5eJu3SiMr8vw", "postedAt": "2023-05-09T10:28:08.404Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Cool, but a lot is likely to happen in the next 6 months! Maybe consider putting it online and updating it as you go? I feel like this post I wrote is already in need of updating (with mention of the H100 \"summoning portals\" already in the pipeline, CthuluGPT, Stability.ai, discussion at Zuzalu last week, <a href=\"https://pauseai.info/\">Pause AI</a>.)</p>", "parentCommentId": "jDhSoPDzPkQJzPmH8", "user": {"username": "Greg_Colbourn"}}, {"_id": "9dJ4g8dzpwTADx8m2", "postedAt": "2023-05-10T11:31:43.046Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>I'm definitely aware of that complication but I don't think that is the best way to broader impact. Uncertainty abounds. &nbsp;If I can get it out in 3 months, I will.&nbsp;</p>", "parentCommentId": "xgZoz5eJu3SiMr8vw", "user": {"username": "Darren McKee"}}, {"_id": "oZsRjkfrsCM8H2Evt", "postedAt": "2023-05-15T20:28:24.095Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Peter -- good post; these all seem reasonable as comments.</p><p>However, let me offer a counter-point, based on my pretty <a href=\"https://twitter.com/primalpoly\">active engagement</a> on Twitter about AI X-risk over the last few weeks: it's often <i>very</i> hard to predict which public outreach strategies, messages, memes, and points will resonate with the public, until we try them out. I've often been very surprised about which ideas really get traction, and which don't. I've been surprised that meme accounts such as <a href=\"https://twitter.com/AISafetyMemes\">@AISafetyMemes</a> have been pretty influential. I've also been amazed at how (unwittingly) effective <a href=\"https://twitter.com/ylecun\">Yann LeCun</a>'s recklessly anti-safety tweets have been at making people wary of the AI industry and its hubris.</p><p>This unpredictability of public responses might seriously limit the benefits of carefully planned, centrally organized activism about AI risk. It might be best just to encourage everybody who's interested to try out some public arguments, get feedback, pay attention to what works, identify common misunderstandings and pain points, share tactics with like-minded others, and iterate.</p><p>Also, lack of formal central organization limits many of the reputational risks of social media activism. If I say something embarrassing or stupid as my Twitter persona @primalpoly, that's just a reflection on that persona (and to some extent, me), not on any formal organization. Whereas if I was the grand high vice-invigilator (or whatever) in some AI safety group, my bad tweets could tarnish the whole safety group.&nbsp;</p><p>My hunch is that a fast, agile, grassroots, decentralized campaign of raising AI X risk awareness could be much more effective than the kind of carefully-constructed, clearly-missioned, reputationally-paranoid organizations that EAs have traditionally favored.</p>", "parentCommentId": "HzBRXccHmnM8oJm2o", "user": {"username": "geoffreymiller"}}, {"_id": "q9qgJpswBS6oYobAJ", "postedAt": "2023-05-16T12:59:11.394Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Yes exactly!</p>", "parentCommentId": "KKkidnmmmGXuPzrLJ", "user": {"username": "Otto"}}, {"_id": "WW3KeCnBpRPii5TAN", "postedAt": "2023-05-16T22:45:08.815Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<blockquote><p>Current scaling \"laws\" are not laws of nature. And there are already worrying signs that things like dataset optimization/pruning, curriculum learning and synthetic data might well break them</p></blockquote><p>Interesting -- can you provide some citations?</p>", "parentCommentId": "8FZTq48JrmfsspPyk", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "Hmrxdd8APvrd4b5A5", "postedAt": "2023-06-01T01:29:48.632Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Coming back to the point about data. Whilst Epoch gathered some data showing that the stock high quality text data might soon be exhausted, their overall&nbsp;<a href=\"https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset#:~:text=All%20in%20all%2C%20we%20believe%20that%20there%20is%20about%20a%2020%252%20chance%20that%20the%20scaling%20(as%20measured%20in%20training%20compute)%20of%20ML%20models%20will%20significantly%20slow%20down%20by%202040%20due%20to%20a%20lack%20of%20training%20data.\"><u>conclusion</u></a> is that there is only a \u201c20% chance that the scaling (as measured in training compute) of ML models will significantly slow down by 2040 due to a lack of training data.\u201d. Regarding Jacob Buckman's point about <a href=\"https://jacobbuckman.com/2022-06-14-an-actually-good-argument-against-naive-ai-scaling/\">chess</a>, he actually outlines a way around that (<a href=\"https://jacobbuckman.com/2022-06-14-an-actually-good-argument-against-naive-ai-scaling/#:~:text=moves%20are%20selected,5000%2DELO%20level!\">training data provided by narrow AI</a>). As a counter to the wider point about the need for active learning, see DeepMind's <a href=\"https://sites.google.com/view/adaptive-agent/\">Adaptive Agent</a> and the <a href=\"https://voyager.minedojo.org/\">Voyager</a> \"lifelong learning\" Minecraft agent, both of which seem like impressive steps in this direction.</p><p><br>&nbsp;</p>", "parentCommentId": "CzdRcaJWm3tTvPeDJ", "user": {"username": "Greg_Colbourn"}}, {"_id": "5uA2BuDPQwfJpzbCa", "postedAt": "2023-06-13T08:11:45.716Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>I submitted this to the OpenPhil AI Worldviews Contest on 31st May with a few additions and edits -<a href=\"https://www.dropbox.com/s/4qs7w9sqtql0cyq/%5BOpenPhil%20AI%20Worldviews%20Contest%5D%20AGI%20rising_%20why%20we%20are%20in%20a%20new%20era%20of%20acute%20risk%20and%20public%20awareness%2C%20and%20what%20to%20do%20now%20-%20Google%20Docs.pdf?dl=0\"> this pdf version</a> is most up to date.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "mY8f8fFmRdTT4Zkhf", "postedAt": "2023-06-14T11:16:29.245Z", "postId": "8YXFaM9yHbhiJTPqp", "htmlBody": "<p>Matthew Barnett's <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of\">compute-based framework for thinking about the future of AI</a> corroborates my view that <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/#Part_2__A_compute_centered_theory_of_AI_automation:~:text=In%20the%20last%20year%2C%20we%E2%80%99ve%20seen%20predictions%20that%20the%20total%20amount%20of%20data%20available%20on%20the%20internet%20could%20constrain%20AI%20progress%20in%20the%20near%20future.%20However%2C%20researchers%20at%20Epoch%20believe%20there%20are%20a%20number%20of%20strong%20reasons%20to%20doubt%20these%20predictions.\">data is not likely to be a bottleneck</a>. Also, contrary to the section \"against very short timelines\", I <a href=\"https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of?commentId=BgqW8dSDv8rZhYEEg\">argue</a> that in fact, the data/framework used is enough to make one even more worried than I am in the OP. 1 OOM FLOP more than I previously said (\"100x the compute used for GPT-4\") is likely available basically now to some actors; or 4 OOM including algorithmic improvements that come \"for free\" with compute scaling! This (10^28-10^31 FLOP) means AGI is possible this year.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}]