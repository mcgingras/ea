[{"_id": "KSC6s6T7KD6pkpGLK", "postedAt": "2014-08-13T23:12:00.000Z", "postId": "SZbA2AkZR8PKz2tWS", "htmlBody": "<p>It's great you're thinking about these issues.</p><p>I agree that AGI safety is plausibly the dominating consideration regarding takeoff speed. Thus, whether one wants a faster or slower takeoff depends on whether one wants safe AGI (which is not a completely trivial question, <a href=\"http://foundational-research.org/robots-ai-intelligence-explosion/#Would_a_human_inspired_AI_or_rogue_AI_cause_more_suffering\">http://foundational-research.org/robots-ai-intelligence-explosion/#Would_a_human_inspired_AI_or_rogue_AI_cause_more_suffering</a> , though I think it's likely safe AI is better for most human values).</p><p></p><p>And yes, neuromorphic AGI seems likely to be safer both because it may be associated with a slow takeoff but also because we understand how humans work, how to balance power with them, and so on. Arbitrary AGIs with alien motivational and behavioral systems are more unpredictable. In the long run, if you want goal preservation, you probably need AGI that's different from the human brain, but goal preservation is arguably less of a concern in the short run; knowledge of how to do goal preservation will come with greater intelligence. In any case, neuromorphic AGIs are much more likely to have human-like values than arbitrary AGIs. We don't worry that much about goal preservation with subsequent generations of humans because they're pretty similar to us (though old conservatives are often upset with the moral degeneration of society caused by young people).</p><p>\nI agree that multipolar power dynamics could be bad, because this might lead to arms races and conflict relative to a quick monopoly by one group. On the other hand, it might allow for more representation by different parties.</p><p>\nOverall, I think the odds of a fast takeoff are sufficiently low that I'm not convinced it makes sense to focus on fast-takeoff work (even if some such exploration is worthwhile). There may be important first-mover advantages to shaping how society approaches slow takeoffs, and if slow takeoff is sufficiently probable, those may dominate in impact. In any case, the fast-slow distinction is not binary, and maybe the best place to focus is on scenarios where human-level AI takes over on a time scale of a few years. (Timescales of months, days, or hours strike me as pretty improbable, unless, say, Skynet gets control of nuclear weapons.)</p>", "parentCommentId": null, "user": {"username": "Brian_Tomasik"}}, {"_id": "bP9ycvH6Wus3PSxhc", "postedAt": "2014-08-15T18:16:00.000Z", "postId": "SZbA2AkZR8PKz2tWS", "htmlBody": "<p>Thanks, good comments.</p><p>\nWhich kind of work it's better to focus depends on the relative leverage you think you have in either case, combined with the likelihoods of the different scenarios. I plan to try a more quantitative analysis, which investigates what ranges of empirical beliefs about these factors correspond to what kind of work now. We could then try to gather some data on estimates (and variance in estimates) of these key values.</p>", "parentCommentId": "KSC6s6T7KD6pkpGLK", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "Kbj4taMvusLGWFHyp", "postedAt": "2017-02-12T03:43:35.029Z", "postId": "SZbA2AkZR8PKz2tWS", "htmlBody": "<blockquote>\n<p>Second, we should generally focus safety research today on fast takeoff scenarios. Since there will be much less safety work in total in these scenarios, extra work is likely to have a much larger marginal effect.</p>\n</blockquote>\n<p>Does this assumption depend on how pessimistic/optimistic one is about our chances of achieving alignment in different take-off scenarios, i.e. what our position on a <a href=\"http://i.imgur.com/8ZYes6d.png\">curve something like this</a> is expected to be for a given takeoff scenario?</p>\n", "parentCommentId": null, "user": {"username": "CalebWithers"}}, {"_id": "KeTX98C2HMTikSRrm", "postedAt": "2017-02-12T10:45:52.023Z", "postId": "SZbA2AkZR8PKz2tWS", "htmlBody": "<p>I think you get an adjustment from that, but that it should be modest. None of the arguments we have so far about how difficult to expect the problem to be seem very robust, so I think it's appropriate to have a <a href=\"https://www.fhi.ox.ac.uk/how-to-treat-problems-of-unknown-difficulty/\">somewhat broad prior over possible difficulties</a>.</p>\n<p>I think the picture you link to is plausible if the horizontal axis is interpreted as a log scale. But this changes the calculation of marginal impact quite a lot, so that you probably get more marginal impact towards the left than in the middle of the curve. (I think it's conceivable to end up with well-founded beliefs that look like that curve on a linear scale, but that this requires (a) very good understanding of what the problem actually is, &amp; (b) justified confidence that you have the correct understanding.)</p>\n", "parentCommentId": "Kbj4taMvusLGWFHyp", "user": {"username": "Owen_Cotton-Barratt"}}]