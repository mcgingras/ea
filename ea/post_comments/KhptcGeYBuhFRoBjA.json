[{"_id": "kA4ZizyQ9nXgFDFBM", "postedAt": "2023-10-25T08:04:15.008Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>I feel like this post is doing something I really don't like, which I'd categorize as something like \"instead of trying to persuade with arguments, using rhetorical tricks to define terms in such a way that the other side is stuck defending a loaded concept and has an unjustified uphill battle.\"<br><br>For instance:</p><blockquote><p>let us be clear: <strong>hiding your beliefs</strong>, in ways that predictably leads people to believe false things, <strong>is lying</strong>. This is the case regardless of your intentions, and regardless of how it feels.</p></blockquote><p>I mean, no, that's just not how the term is usually used. It's <i>misleading </i>to hide your beliefs in that way, and you could argue it's <i>dishonest</i>, but it's not generally what people would call a \"lie\" (or if they did, they'd use the phrase \"lie by omission\"). One could argue that lies by omission are no less bad than lies by commission, but I think this is at least nonobvious, and also a view that I'm pretty sure most people don't hold. You could have written this post with words like \"mislead\" or \"act coyly about true beliefs\" instead of \"lie\", and I think that would have made this post substantially better.</p><p>I also feel like the piece weirdly implies that it's dishonest to advocate for a policy that you think is second best. Like, this just doesn't follow \u2013 someone could, for instance, want a $20/hr minimum wage, and advocate for a $15/hr minimum wage based on the idea that it's more politically feasible, and this isn't remotely dishonest <i>unless they're being dishonest about their preference for $20/hr in other communications</i>. You say:</p><blockquote><p>many AI Safety people being much more vocal about their endorsement of RSPs than their private belief that in a saner world, all AGI progress should stop right now.</p></blockquote><p>but this simply isn't contradictory \u2013 you could think a perfect society would pause but that RSPs are still good and make more sense to advocate for given the political reality of our society.</p>", "parentCommentId": null, "user": {"username": "Daniel_Eth"}}, {"_id": "d6pqPpvdCQ2oDDSkZ", "postedAt": "2023-10-25T10:48:14.419Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<blockquote><ul><li>ARC &amp; Open Philanthropy state in a press release \u201c<strong>In a sane world, all AGI progress should stop. If we don\u2019t, there\u2019s more than a 10% chance we will all die.</strong>\u201d</li><li>People at AGI labs working in the safety teams echo this message publicly.</li></ul></blockquote><p><br>Genuine question: Do the majority of people in open Phil, or at AGI safety labs, actually believe the statement above?&nbsp;</p><p>I'm all for honesty, but it seems like an alternate explanation for a lot of people not saying the statement above is that they don't believe that the statement is true. I worry that setting this kind of standard will just lead to accusations that genuine disagreement is secret dishonesty. &nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "dNacyuzcRYszoE8nb", "postedAt": "2023-10-25T13:59:22.238Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>I think this is tantamount to saying that we shouldn't engage within the political system, compromise, or meet people where they are coming from in our advocacy. I don't think other social movements would have got anywhere with this kind of attitude, and this seems especially tricky with something very detail orientated like a AI safety.</p><p><a href=\"https://paxfauna.org/inside-outside-strategy/\">Inside game approaches</a> (versus outside game approaches like this is describing) are going to require engaging in things this post says that no one should do. Boldly stating exactly the ideal situation you are after could have its role, but I'd need to see and much more detailed argument about why that should be the only game in town when it comes to AI.</p><p>I think that as AI safety turns more into an advocacy project it needs engage more with the existing literature on the subject including what has worked for past social movements.</p><p>Also, importantly, this isn't lying (as Daniel's comment explains).&nbsp;</p>", "parentCommentId": null, "user": {"username": "Max_Carpendale"}}, {"_id": "uKyAE7uq9avoktPfg", "postedAt": "2023-10-25T14:53:49.305Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>'There are very few people that we have consistently seen publicly call for a <strong>stop</strong> to AGI progress. The clearest ones are Eliezer\u2019s \u201c<a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\"><u>Shut it All Down</u></a>\u201d and Nate\u2019s \u201c<a href=\"https://twitter.com/So8res/status/1715380167911067878\"><u>Fucking stop</u></a>\u201d.</p><p>The loudest silence is from Paul Christiano, whose RSPs are being used to safety-wash scaling.'<br><br>I'm not blaming the authors for this, as they couldn't know, but literally today, on this forum, Paul Christiano has publicly expressed clear beliefs about whether a pause would be a good idea, and why he's not advocating for one directly: <a href=\"https://forum.effectivealtruism.org/posts/cKW4db8u2uFEAHewg/thoughts-on-responsible-scaling-policies-and-regulation\">https://forum.effectivealtruism.org/posts/cKW4db8u2uFEAHewg/thoughts-on-responsible-scaling-policies-and-regulation</a><br><br>Christiano: \"If the world were unified around the priority of minimizing global catastrophic risk, I think that we could reduce risk significantly further by implementing a global, long-lasting, and effectively enforced pause on frontier AI development\u2014including a moratorium on the development and production of some types of computing hardware. The world is not unified around this goal; this policy would come with other significant costs and currently seems unlikely to be implemented without much clearer evidence of serious risk.&nbsp;</p><p>A unilateral pause on large AI training runs in the West, without a pause on new computing hardware, would have more ambiguous impacts on global catastrophic risk. The primary negative effects on risk are leading to faster catch-up growth in a later period with more hardware and driving AI development into laxer jurisdictions.</p><p>However, if governments shared my perspective on risk then I think they should already be implementing domestic policies that will often lead to temporary pauses or slowdowns in practice. For example, they might require frontier AI developers to implement additional protective measures before training larger models than those that exist today, and some of those protective measures may take a fairly long time (such as major improvements in risk evaluations or information security). Or governments might aim to limit the rate at which effective training compute of frontier models grows, in order to provide a smoother ramp for society to adapt to AI and to limit the risk of surprises.\"<br><br>&nbsp;</p>", "parentCommentId": null, "user": {"username": "Dr. David Mathers"}}, {"_id": "cik8E8LwoRCEwwcLv", "postedAt": "2023-10-25T15:46:12.232Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>Isn\u2019t your point a little bit pedantic here in the sense that you seem to be perfectly able to understand the key point the post was trying to make, find that point somewhat objectionable or controversial, and thus point to some issues regarding \u201eframing\u201c rather than really engage deeply with the key points?</p>\n<p>Of course, every post could be better written, more thoughtful, etc. but let\u2019s be honest, we are here to make progress on important issues and not to win \u201eargument style points.\u201c In particular, I find it disturbing that this technique of criticizing style of argument seems to be used quite often to discredit or not engage with \u201eniche\u201c viewpoints that criticize prevailing \u201emainstream\u201c opinions in the EA community. Happened to me as well, when I was suggesting we should look more into whether there are maybe alternatives to purely for profit/closed sourced driven business models for AI ventures. Some people where bending over backwards to argue some concerns that were only tangentially related to the proposal I made (e.g., government can't be trusted and is incompetent so anything involving regulation could never ever work, etc.). Another case was a post on engagement with \"post growth\" concepts. There I witnessed something like a wholesale character assassination of the post growth community for whatever reasons. Not saying this happened here but I am simply trying to show a pattern of dismissal of niche viewpoints for spurious, tangential reasons without really engaging with them.</p>\n<p>Altogether, wouldn\u2019t it be more productive to have more open minded discussions and practice more of what we preach to the normies out there ourselves (e.g., steel-manning instead of straw-manning)?  Critiquing style is fine and has its place but maybe let\u2019s do substance first and style second?</p>\n", "parentCommentId": "kA4ZizyQ9nXgFDFBM", "user": {"username": "alexherwix"}}, {"_id": "F4tikhNjfPtLyFLts", "postedAt": "2023-10-25T15:58:59.938Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>Your question reminded me of the following quote:</p>\n<blockquote>\n<p>It Is Difficult to Get a Man to Understand Something When His Salary Depends Upon His Not Understanding It</p>\n</blockquote>\n<p>Maybe here we are talking about an alternative version of this:</p>\n<blockquote>\n<p>It Is Difficult to Get a Man to Say Something When His Salary (or Relevance, Power, Influence, Status) Depends Upon Him Not Saying It</p>\n</blockquote>\n", "parentCommentId": "d6pqPpvdCQ2oDDSkZ", "user": {"username": "alexherwix"}}, {"_id": "d4JATfLdvDzcZfPBM", "postedAt": "2023-10-25T20:21:54.348Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p><strong>note - everything i say here is based on my own interpretation of the AI scene. I have no special knowledge, so this may be inaccurate, out-of-date, or both</strong></p><p>So this post has provoked a lot of reaction, and I'm not going to defend all of it. I do think, however, that debating what is lying/white lying/lying by omission/being misleading is not as important as the other issues. (I think the tone is overly combative and hurts its cause, for instance). But from the perspective of someone like Connor, I can see why they may think that the AI Safety Community (especially those at the larger labs) are deliberately falsifying their beliefs.</p><p>On the May 16th Senate Judiciary Committee hearing where the witnesses are <i>under oath</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc0hhbxlnale\"><sup><a href=\"#fnc0hhbxlnale\">[1]</a></sup></span>, Blumenthal asks Sam Altman about his quote that <i>\"development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity\"</i> and leads on to talk about the effect of AI on jobs and employment. Sam answers this by also talking about jobs, but Gary Marcus then calls him out by saying <i>\"Sam's worst fear I do not think is employment and he never told us what his worst fear actually is\". </i>When asked again, Sam says <i>\"my worst fears are that...we cause significant harm to the world\"</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm7ux1xhtlcm\"><sup><a href=\"#fnm7ux1xhtlcm\">[2]</a></sup></span></p><p>I think that's a lie of omission - Sam's worst fear is the extinction of humanity or perhaps some S-risk scenario, and he did not say that. Does this violate <a href=\"https://www.govinfo.gov/content/pkg/USCODE-2014-title18/pdf/USCODE-2014-title18-partI-chap47-sec1001.pdf\">Section 1001 of the United States Code</a>? I don't know. I think it'd be good to get more concrete estimates from Sam of what he thinks the risks are, and I think as the CEO of the world's leading AI company, it'd be something the US Senate should interested in.</p><p>In a recent podcast, Dario Amodei implied his p(doom) was between 10-25%, though it's not a clear prediction with a timeframe, resolution criteria, or whether it's conditional or unconditional on Anthropic's actions (<a href=\"https://www.youtube.com/watch?v=gAaCqj6j5sQ&amp;t=5883s\">listen and judge for yourself</a>). But Dario's been aware of xRisk for a long time, so this isn't a new belief, but it didn't make it into <a href=\"https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-principles-for-regulation\">his senate testimony either</a>. If my p(doom) was that high, and I was leading one of the world's top AI labs, I'd be making <i>a lot more noise.</i> But, instead, they've recently announced partnerships with and taken money from Google and <a href=\"https://press.aboutamazon.com/2023/9/amazon-and-anthropic-announce-strategic-collaboration-to-advance-generative-ai\">Amazon</a> with the intent to develop frontier AI and, presumably, commercialise it.</p><p>Now, many people see this and go \"that doesnt' add up\". It's what leads many critics of AI x-risk to go \"this is just a galaxy brained marketing strategy\", and I hope this comment has given some clarification as to why they think so.</p><p>Connor, on the other hand, has been consistently against scaling up AI systems and has argued for this in public appearances on the news, on podcasts, and in public hearings. I don't agree with his pessimism or framing of the positions, but he has authenticity regarding his position. It's what people expect people to act like <i>if they think the industry they work in has a significant chance of destroying the world</i>, whereas I can't make sense of Amodei's position, for example, without guessing additional beliefs or information which he may or may not actually have.</p><p>So, tying it back to the intense scepticism of RSPs from the Connor/ctrl.ai/Conjecture side of the AI Safety space is because given a track record like the above <i>they don't trust that RSPs are actually a mechanism that will reduce the risk from developing unaligned AGI. </i>And I can see why they'd be sceptical - if I'd just accepted the promise of $4 billion from Amazon, that's a big incentive to put your thumb on the scale and say \"we're not at ASL-3 yet\", for example.</p><p>The alternative might be titotal's explanation that Sam/Dario have much lower p(doom) than Connor, and I think that's probably true. But that might not be true for every employee at these organisations, and even then I think 10% is a high enough p(doom) that one ought to report that estimate fully and faithfully to your democratically elected government if they ask you about it.</p><p>I hope this makes the 'RSP sceptical' position more clear to people, though I don't hold it myself (at least to the same extreme as Connor and Gabe do here), and I'm open to corrections from either Connor or Gabe if you think I've interpreted what you're trying to say incorrectly.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc0hhbxlnale\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc0hhbxlnale\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I can't find a recording or transcript of what the oath was, but I assume it included a promise to tell the whole truth</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm7ux1xhtlcm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm7ux1xhtlcm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The hearing is <a href=\"https://www.youtube.com/watch?v=TO0J2Yw7usM\">here</a>, this exchange kicks off around 35:38</p></div></li></ol>", "parentCommentId": null, "user": {"username": "JWS"}}, {"_id": "cAKMu9cmbxRKJegcL", "postedAt": "2023-10-25T22:11:38.636Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>Thanks Alex. In general I agree with you, if viewpoints are expressed that are outside of what most EAs think, they do sometimes get strawmanned and voted down without good reason (like you say ideas like handing more power to governments and post-growth concepts). In this case though I think the original poster was fairly aggressive with rhetorical tricks, as a pretty large part of making their argument - so I think Daniel's criticism was reasonable.</p>", "parentCommentId": "cik8E8LwoRCEwwcLv", "user": {"username": "NickLaing"}}, {"_id": "REtuqLCQLYhFJEowR", "postedAt": "2023-10-26T09:04:45.052Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>My problem with the post wasn't that it used subpar prose or \"could be written better\", it's that it uses rhetorical techniques that make actual exchange of ideas and truth-seeking harder. This isn't about \"argument style points\", it's about cultivating norms in the community that make it easier for us to converge on truth, even on hard topics.</p><p>The reason I didn't personally engage with the object level is I didn't feel like I had anything particularly valuable to say on the topic. I didn't <i>avoid</i> saying my object-level views (if he had written a similar post with a style I didn't take issue with, I wouldn't have responded at all), and I don't want other people in the community to avoid engaging with the ideas either.</p>", "parentCommentId": "cik8E8LwoRCEwwcLv", "user": {"username": "Daniel_Eth"}}, {"_id": "33aGyaq5EsqRHCmoA", "postedAt": "2023-10-26T13:59:43.343Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>I think there is something to be said for the <a href=\"https://forum.effectivealtruism.org/posts/LXj4cs5dLqDHwJynp/radical-tactics-can-increase-support-for-more-moderate\">radical flank effect</a>, and Connor and Gabe are providing a somewhat radical flank (even though I actually think the \"<a href=\"https://twitter.com/So8res/status/1715380167911067878\">fucking stop</a>!\" position is the most reasonable, moderate one, given the urgency and the stakes!).</p>", "parentCommentId": "dNacyuzcRYszoE8nb", "user": {"username": "Greg_Colbourn"}}, {"_id": "SChZFaFPgkLMzdEqo", "postedAt": "2023-10-26T14:03:19.221Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<blockquote><p>what has worked for past social movements</p></blockquote><p>This is fundamentally different imo, because we aren't asking for people to right injustices, stick up for marginalised groups, care about future generations, or do good of any kind; we're asking people not to kill literally everyone, including ourselves, and for those who would do (however unintentionally) to be stopped by governments. It's a matter of survival above all else.</p>", "parentCommentId": "dNacyuzcRYszoE8nb", "user": {"username": "Greg_Colbourn"}}, {"_id": "JfD8LJxJ2bzFXfo3w", "postedAt": "2023-10-26T14:09:39.719Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>Yes, would be good to hear more from them directly. I'm disappointed that OpenPhil have not given any public update on their credences on the <a href=\"https://forum.effectivealtruism.org/posts/eSZuJcLGd7BacjWGi/announcing-the-winners-of-the-2023-open-philanthropy-ai?commentId=APJBaECtGv9eWmfpt\">two main questions</a> their AI Worldviews Contest sought to address.</p>", "parentCommentId": "d6pqPpvdCQ2oDDSkZ", "user": {"username": "Greg_Colbourn"}}, {"_id": "Wj6ePJ8CJZo93Goky", "postedAt": "2023-10-27T11:03:00.657Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>I don't think the scale or expected value affects this strategy question directly. You still just use a strategy that is going to be most likely to achieve the goal.</p><p>If the goal is something you have really widespread agreement on, that probably leans you towards an uncompromising, radical ask approach. Seems like things might be going pretty well for AI safety in that respect, though I don't know if it's been established that people are buying into the high probability of doom arguments that much. I suspect that we are much less far along than the climate change movement in that respect, for example. And even if support were much greater, I wouldn't agree with a lot of this post.</p><p>Oh, my expertise is in animal advocacy, not AI safety FYI</p>", "parentCommentId": "SChZFaFPgkLMzdEqo", "user": {"username": "Max_Carpendale"}}, {"_id": "jmGoLDfZ4xnpoBcRz", "postedAt": "2023-10-27T11:27:11.141Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>Hey Nick,</p>\n<p>thanks for your reply. I didn\u2019t mean to say that Daniel didn\u2019t have a point. It\u2019s a reasonable argument to make. I just wanted to highlight that this shouldn\u2019t be the only angle to look at such posts. If you look, his comment is by far the most upvoted and it only addresses a point tangential to the problem at hand. Of course, getting upvoted is not his \u201efault\u201c. I just felt compelled to highlight that overly focusing on this kind of angle only brings us so far.</p>\n<p>Hope that makes it clearer :)</p>\n", "parentCommentId": "cAKMu9cmbxRKJegcL", "user": {"username": "alexherwix"}}, {"_id": "FadMR6c4sQqJjCfze", "postedAt": "2023-10-27T11:42:32.626Z", "postId": "KhptcGeYBuhFRoBjA", "htmlBody": "<p>Hey Daniel,</p>\n<p>as I also stated in another reply to Nick, I didn\u2019t really mean to diminish the point you raised but to highlight that this is really more of a \u201emeta point\u201c that\u2019s only tangential to the matter of the issue outlined. My critical reaction was not meant to be against you or the point you raised but the more general community practice / trend of focusing on those points at the expense of engaging the subject matter itself, in particular, when the topic is against mainstream thinking. This I think is somewhat demonstrated by the fact that your comment is by far the most upvoted on an issue that would have far reaching implications if accepted as having some merit.</p>\n<p>Hope this makes it clearer. Don\u2019t mean to criticize the object level of your argument, it\u2019s just coincidental that I picked out your comment to illustrate a problematic development that I see.</p>\n<p>P.S.: There is also some irony in me posting a meta critique of a meta critique to argue for more object level engagement but that\u2019s life I guess.</p>\n", "parentCommentId": "REtuqLCQLYhFJEowR", "user": {"username": "alexherwix"}}]