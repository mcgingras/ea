[{"_id": "3BsMgzNXpNXe887wb", "postedAt": "2022-08-27T21:31:52.860Z", "postId": "bQb6TyBib2ucxPwT8", "htmlBody": "<p>This sounds like an absolutely amazing idea which I bet will get you feedback that will make you way over 10x more efficient with how you learn.</p><p>&nbsp;</p><p>If you're worried about PR risks - you can do something like this in a more-closed group (like a forum post that you keep replying to?), or as a middle-ground you can avoid using buzzwords like \"Effective Altruism\". If you want a more official answer about PR, I recommend messaging <a href=\"https://forum.effectivealtruism.org/users/lizka\">Lizka</a>, and she'll know who's in charge of that stuff</p>", "parentCommentId": null, "user": {"username": "hibukki"}}, {"_id": "NsFSiF2qGmT28bqqX", "postedAt": "2022-08-27T22:39:12.461Z", "postId": "bQb6TyBib2ucxPwT8", "htmlBody": "<p>I did this with nuclear weapons, I made a big website full of basic knowledge so that I would have to learn the basic knowledge. &nbsp; I could have just quietly read the information, but engaging my web building creativity, and pretending I was a teacher :-) seemed to help things along.</p><p>&nbsp;A bachelor's in Engineering and Philosophy sounds quite promising. &nbsp; I love the idea of those two together. &nbsp;&nbsp;</p><p>I've been arguing that existential risk is at it's heart largely a philosophical problem, more than a technical one. &nbsp; In short, the \"more is better\" relationship with knowledge which is driving the technology is an outdated, simplistic and increasingly dangerous philosophy left over from the 19th century and earlier. &nbsp;It was a great philosophy in the long era of knowledge scarcity, but we no longer live in that era. &nbsp; &nbsp; &nbsp;</p><p>Existential risk is largely a failure to adapt our knowledge philosophy to the new conditions created by the success of the knowledge explosion. &nbsp; Revolutionary new conditions, same old philosophy.</p><p>For more...</p><p>https://forum.effectivealtruism.org/posts/kbfdeZbdoFXT8nuM6/our-relationship-with-knowledge</p>", "parentCommentId": null, "user": {"username": "Phil Tanny"}}, {"_id": "datfDrS76kjWG84jJ", "postedAt": "2022-08-27T23:00:37.000Z", "postId": "bQb6TyBib2ucxPwT8", "htmlBody": "<p>Hi there,&nbsp;</p><p>Interesting idea. I think there's a lot of possible commentary or answers, so I'll provide some quick thoughts based on ~1 month of reading/upskilling in AI/Bio-related X-risk, which I began since I received an FTX Future Fund Regrant.&nbsp;</p><blockquote><p>Does anyone have any advice before I start this project?</p></blockquote><p>Do experiments. These can inform your estimates of how valuable a podcast would be to others, how useful it would be to you and how much effort it would require. This post is a great experiment also, so kudos! &nbsp;</p><blockquote><p>In particular, are there any resources you recommend for teaching myself about machine learning, genomics, or politics?</p></blockquote><p>There are lots of different materials online for learning about these general topics. I would highly suggest you start with having a thorough understanding of the relevant x-risk cause areas without getting into technical details first, followed by learning about these technical topics if/when they appear to be most appropriate.&nbsp;</p><p>I'm interested in whether this particular piece of advice in the previous paragraph is contentious (with the other perspective being \"go learn lots of general skills before getting more context on x-risks\"). Still, I think that might be a costly approach involving spending lots of time learning extraneous detail with no apparent payoff.&nbsp;</p><p>For AI:</p><p>&nbsp;I think the best place to start is the <a href=\"https://www.eacambridge.org/agi-safety-fundamentals\">Cambridge AGI Safety Fundamentals </a>course (which has technical and governance variations). You don't need a lot of Deep Learning expertise to do the course, and the materials are available online until they run one.&nbsp;</p><p>For Bio:&nbsp;</p><p>Tessa curated <a href=\"https://forum.effectivealtruism.org/posts/iAowzcZm87wNrTQCb/a-biosecurity-and-biorisk-reading-list\">A Biosecurity and Biorisk Reading+ List,</a> which covers several domains, including genomics.</p><blockquote><p>And are there any hidden risks I'm not considering that might make this idea worse than it seems?</p></blockquote><p>Other than not achieving your goals, or being costly, mitigated by starting small and doing experiments, the most significant potential risk is some <a href=\"https://forum.effectivealtruism.org/topics/information-hazard\">information hazard</a>. If you focus on pre-requisite skills, then info hazards might be less likely. There are dangers in being too careful around info hazards, so maybe the best action is to share podcasts with a small group of info hazard-aware community members first to check.&nbsp;</p><p>Good luck! And please feel free to reach out if you'd like to discuss this further.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Joseph Bloom"}}, {"_id": "gPhck4mhRdWKy5FQM", "postedAt": "2022-08-28T01:22:04.262Z", "postId": "bQb6TyBib2ucxPwT8", "htmlBody": "<p>If you think there's a risk you start but do not continue the project, you could just publish the transcripts rather than record the podcast.</p>\n", "parentCommentId": null, "user": {"username": "vaidehi_agarwalla"}}]