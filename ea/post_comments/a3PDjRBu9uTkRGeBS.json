[{"_id": "A2xJTXM9z8mKSv2id", "postedAt": "2015-08-11T16:46:47.189Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>I think you are short selling Matthews on Pascal's Mugging. I don't think his point was that you must throw up your hands because of the uncertainty, but that he believes friendly AI researchers have approximately the same amount of evidence that AI research done today will have a 10^-15 chance of saving the existence of future humanity as any infinitesimal but positive chance.</p>\n<p>Anyone feel free to correct me, but I believe in such a scenario spreading your prior evenly over all possible outcomes wouldn't arbitrarily just include spitting the difference between 10^-15 or 10^-50 but spreading your belief over all positive outcomes below some reasonable barrier and (potentially) above another* (and this isn't taking into account the non-zero, even if unlikely, probability that despite caution AI research is indeed speeding up our doom). What those numbers are is very difficult to tell but if the estimation of those boundaries is off, and given the record of future predictions of technology it's not implausible, then all current donations could end up doing basically nothing. In other words, his critique is not that we must give up in the face of uncertainty but that the the justification of AI risk reduction being valuable right now depends on a number of assumptions with rather large error bars. </p>\n<p>Despite what appeared to him to be this large uncertainty, he seemed to encounter many people who brushed aside, or seemingly belittled, all other possible cause areas and this rubbed him the wrong way. I believe that was his point about Pascal's Mugging. And while you criticized him for not acknowledging MIRI does not support Pascal's Mugging reasoning to support AI research, he never said they did in the article. He said many people at the conference replied to him with that type of reasoning (and as a fellow attendee, I can attest to a similar experience).</p>\n<p>*Normally, I believe, it would be all logically possible outcomes but obviously it's unreasonable to believe a $1000 donation, which was his example, has, say, a 25% chance of success given everything we know about how much such work costs, etc. However, where the lower bound is on this estimation is far less clear.</p>\n", "parentCommentId": null, "user": {"username": "Marcus_A_Davis"}}, {"_id": "8K6X5SgCL9wucN3Ye", "postedAt": "2015-08-11T20:25:30.421Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>Whether this is right or wrong - and Ryan is certainly correct that Dylan Matthews' piece didn't offer a knock-down argument against focusing on AI risk, which I doubt it was intended to do - it's worth noting that <a href=\"http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">the article in question</a> wasn't only about this issue. It focused primarily on Matthews' worries about the EA movement as a whole, following EA Global San Francisco. These included a lack of diversity, the risks of the focus on meta-charities and movement building (which can of course be valuable, but can also lead to self-servingness and self-congratulation), and the <em>attitude</em> of some of those focused on x-risks to people focused on global poverty. On this last, here was my comment from Facebook:</p>\n<blockquote>\n<p>This article's been doing the rounds (e.g. <a href=\"http://on.fb.me/1gYKJN6\">http://on.fb.me/1gYKJN6</a> &amp; <a href=\"http://on.fb.me/1MZnGia\">http://on.fb.me/1MZnGia</a> ). I think it's correct in many ways, and points to very serious problems with EA.</p>\n<p>Global poverty genuinely <em>is</em> increasingly marginalised and dismissed as an EA cause. Some people here may be being misled by the fact that poverty is frequently placed front and centre in the rhetoric. But the rhetoric is often just that, explicitly intended as a device to recruit new batches of EAs who can then be directed towards supporting x-risk or meta causes. Sometimes this strategy is public, and that may indeed be unusually common in the Bay Area, but it\u2019s widespread among people and organisations elsewhere. (As a matter of fixed policy I\u2019m not going to pick them out because that\u2019d be counterproductive, and probably the wrong thing to do!)</p>\n<p>I've heard many people express this perspective. To take one example, Sasha Cooper noted on the EA Forum that \u201cthose committed to poverty [...] often seem to be looked on as incomplete or fledgling EAs\u201d and in <a href=\"http://on.fb.me/1gYKJN6\">http://on.fb.me/1gYKJN6</a> that there\u2019s also a related but distinct disagreement between \u201cquantifiers\u201d and \u201cspeculators\u201d (with &quot;quanitifiers&quot; often but not always supporting global poverty charities), which is fairly open and occasionally hostile. I perceive the hostility/dismissiveness as mainly coming from supporters of speculative causes, but I'm sure it sometimes goes the other way.</p>\n<p>Disclaimer 1: I unfortunately couldn\u2019t attend EA Global SF because my visa makes it impractical to leave Canada for a while, so I don\u2019t know first hand what the tilt of the conference was. I heard it was very x-risk and meta heavy, but that was only second hand.</p>\n<p>Disclaimer 2: I obviously say this as someone who leans towards more quantifiable and less speculative approaches, and thinks that global poverty is (probably) the best ultimate cause to donate to. But I intellectually respect many other EA cause areas (where \u2018respect\u2019 means something genuine and meaningful, rather than something automatically handed out to anyone\u2019s view).</p>\n</blockquote>\n", "parentCommentId": null, "user": {"username": "Tom_Ash"}}, {"_id": "9DSBY39vmrcS5znxP", "postedAt": "2015-08-12T15:16:17.388Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<blockquote>\n<p>Anyone feel free to correct me, but I believe in such a scenario spreading your prior evenly over all possible outcomes wouldn't arbitrarily just include spitting the difference between 10^-15 or 10^-50 but spreading your belief over all positive outcomes below some reasonable barrier and (potentially) above another* (and this isn't taking into account the non-zero, even if unlikely, probability that despite caution AI research is indeed speeding up our doom).</p>\n</blockquote>\n<p>It's complicated, but I don't think it makes sense to have a probability distribution over probability distributions, because it collapses. We should just have a probability distribution over outcomes. We choose our prior estimate for chance of success based on other cases of people attempting to make safer tech.</p>\n<blockquote>\n<p>Despite what appeared to him to be this large uncertainty, he seemed to encounter many people who brushed aside, or seemingly belittled, all other possible cause areas and this rubbed him the wrong way.</p>\n</blockquote>\n<p>In fairness, for people who adhere to expected value thinking to the fullest extent (some of whom would have turned out to the conference), arguments purely on the basis of scope of potential impact would be persuasive. But if it's even annoying folks at EA Global, then probably people ought to stop using them.</p>\n", "parentCommentId": "A2xJTXM9z8mKSv2id", "user": {"username": "RyanCarey"}}, {"_id": "dGa9MMZS7JP3ngrtw", "postedAt": "2015-08-12T23:54:21.468Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<blockquote>\n<p>It's complicated, but I don't think it makes sense to have a probability distribution over probability distributions, because it collapses. We should just have a probability distribution over outcomes.</p>\n</blockquote>\n<p>I did mean over outcomes. I was referring to this: </p>\n<blockquote>\n<p>If we're uncertain about Matthews propositions, we ought to place our guesses somewhere closer to 50%. To do otherwise would be to mistake our deep uncertainty deep scepticism.</p>\n</blockquote>\n<p>That seems mistaken to me but it could be because I'm misinterpreting it. I was reading it as saying we should split the difference between the two probabilities of success Matthews proposed. However, I thought he was suggesting, and believe it is correct, that we shouldn't just pick the median between the two because the smaller number was just an example. His real point being that any tiny probability of success seems equally as reasonable from the vantage point of now. If true we'd then have to split our prior evenly over that range instead of picking the median between 10^-15 and 10^-50. And given it's very difficult to put a lower bound on the reasonable range but a $1000 donation being a good investment depends on a specific lower bound higher than he believes can be justified with evidence, some people came across as unduly confident. </p>\n<blockquote>\n<p>But if it's even annoying folks at EA Global, then probably people ought to stop using them.</p>\n</blockquote>\n<p>Let me be very clear, I was not annoyed by them, even if I disagree, but people definitely used this reasoning. However, as I often point out, extrapolating from me to other humans is not a good idea even within the EA community.</p>\n", "parentCommentId": "9DSBY39vmrcS5znxP", "user": {"username": "Marcus_A_Davis"}}, {"_id": "sH73cS5shGss2yHXu", "postedAt": "2015-08-13T16:08:54.824Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>I think it's very good Matthews brought this point up so the movement can make sure we remain tolerant and inclusive of people mostly on our side but differing in a few small points. Especially those focused on x-risk, if he finds them to be most aggressive, but really I think it should apply to all of us.</p>\n<p>That being said, I wish he had himself refrained from being divisive with allegations that x-risk is self-serving for those in CS. Your point about CS concentrators being &quot;damned if you do, damned if you don't&quot; is great. Similarly, the point (you made on facebook?) about many people converting from other areas into computer science as they realize the risk is a VERY strong counterargument to his. But more generally, it seems like he is applying asymmetric standards here. It seems the x-risk crowd no more deserves his label of biased and self-serving as the animal rights crowd, or the global poverty crowd; many of the people in those subsets also began there, so any rebuttal could label them as self-serving for promoting their favored cause if we wanted. Ad hominem is a dangerous road to go down, and I wish he would refrain from critiquing the people and stick to critiquing the arguments (which actually promotes good discussion from people like you and <a href=\"http://slatestarcodex.com/2015/08/12/stop-adding-zeroes/\">Scott Alexander in regards to his pseudo-probability calculation</a>, even if we've been down this road before).</p>\n", "parentCommentId": null, "user": {"username": "cflexman"}}, {"_id": "FrWzhXDBCSsyufZMr", "postedAt": "2015-08-13T16:53:06.561Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>I haven't explored the debate over AI risk in the EA movement in depth, so I'm not informed enough to take a strong position. But Kosta's comment gets at one of the things that has puzzled me -- as basically an interested outsider -- about the concern for x-risk in EA. A very strong fear of human extinction seems to treat humanity as innately important. But in a hedonic utilitarian framework, humanity is only contingently important to the extent that the continuation of humanity improves overall utility. If an AI or AIs could improve overall utility by destroying humanity (perhaps after determining that humans feel more suffering than pleasure overall, or that humans cause more suffering than pleasure overall, or that AIs feel more pleasure and less suffering than humans and so should use all space and resources to sustain as many AIs as possible), then hedonic utilitarians (and EAs, to the extent that they are hedonic utilitarians) should presumably want AIs to do this. </p>\n<p>I'm sure there are arguments that an AI that destroys humanity would end up lowering utility, but I don't get the impression that x-risk-centered EAs only oppose the destruction of humanity if it turns out humanity adds more pleasure to the aggregate. I would have expected to see EAs arguing something more like, &quot;Let's make sure an AI only destroys us if destroying us turns out to raise aggregate good,&quot; but instead the x-risk EAs seem to be saying something more like, &quot;Let's make sure an AI doesn't destroy us.&quot;</p>\n<p>But maybe the x-risk-centered EAs aren't hedonic utilitarians, or they mostly tend to think an AI destroying humanity would lower overall utility and that's why they oppose it, or there's something else that I'm missing \u2013 which is probably the case, since I haven't investigated the debate in detail. </p>\n", "parentCommentId": null, "user": {"username": "RhysSouthan"}}, {"_id": "Xpzubj2cGn9iPLmff", "postedAt": "2015-08-13T17:09:30.400Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>Cautious support of giving an AI control is not opposed to x-risk reduction. Reduction of x-risk is defined as curtailing the potential of Earth-originating life. Turning civ over to AIs or ems might be inevitable, but would still be safety-critical. </p>\n<p>A non-careful transition to AI is bad for utilitarians and many others because of its irreversibility. Once you codify values (a definition of happiness and whatever else) in an AI, they're stuck, unless you've programmed the AI a way for it to reflect on its values. When combined with Bostrom's argument in Astronomical Waste, that the eventual awesomeness of a technologically mature civilisation is more important than when it is achived, this gives a strong reason for caution.</p>\n", "parentCommentId": "FrWzhXDBCSsyufZMr", "user": {"username": "RyanCarey"}}, {"_id": "uKJSp6yGh63YbeNcn", "postedAt": "2015-08-13T17:42:09.033Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>I forgot to mention that your post did help to clarify points and alleviate some of my confusion. Particularly the idea that an ultra-powerful AI tool (which may or may not be sentient) &quot;would still permit one human to wield power over all others.&quot; </p>\n<p>The hypothetical of an AI wiping out all of humanity because it figures out (or thinks it figures out) that it will increase overall utility by doing so is just one extreme possibility. There must be a lot of credible seeming scenarios opposed to this one in which an AI could be used to increase overall suffering. (Unless the assumption was that a super intelligent being or device couldn't help but come around to a utilitarian perspective, no matter how it was initially programmed!) </p>\n<p>Also, like Scott Alexander wrote on his post about this, x-risk reduction is not all about AI. </p>\n<p>Still, from a utilitarian perspective, it seems like talking about &quot;AI friendliness&quot; should mean friendliness to overall utility, which won't automatically mean friendliness to humanity or human rights. But again, I imagine plenty of EAs do make that distinction, and I'm just not aware of it because I haven't looked that far into it. And anyway, that's not a critique of AI risk being a concern for EAs; at most, it's a critique of some instances of rhetoric.  </p>\n", "parentCommentId": "Xpzubj2cGn9iPLmff", "user": {"username": "RhysSouthan"}}, {"_id": "iQAPnE7krY6qhqHGF", "postedAt": "2015-08-13T18:15:49.155Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>If you're a hedonic utilitarian, you might retain some uncertainty over this, and think it's best to at least hold off on destroying humanity for a while out of deference to other moral theories, and because of the option value.</p>\n<p>Even if someone took the view you describe, though, it's not clear that it would be a helpful one to communicate, because talking about &quot;AI destroying humanity&quot; does a good job of successfully communicating concern about the scenarios you're worried about (where AI destroys humanity without this being a good outcome) to other people. As the exceptions are things people generally won't even think of, caveating might well cause more confusion than clarity.</p>\n", "parentCommentId": "FrWzhXDBCSsyufZMr", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "gTbcjcDwjxsYfavAy", "postedAt": "2015-08-13T21:42:29.822Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<blockquote>\n<p>The median AI researcher estimates even odds of human-level AI between 2035 and 2050 so the prospect that AI is possible and achievable within decades is large enough to worry about.</p>\n</blockquote>\n<p>Predictions about when we will achieve human-level AI have been wildly inaccurate in the past[1]. I don't think the predictions of current AI researchers is particularly useful data point.</p>\n<p>Assuming that we do in fact achieve human-level AI at some point, then if we're going to avoid Pascal's Mugging you need to present compelling evidence that the path from human-level AI -&gt; superintelligent/singularity/end of humanity level AI is (a) a thing which is likely (i.e. p &gt;&gt; 10^-50), (b) a thing that is likely to be bad for humanity, and (c) that we have a credible chance of altering the outcome in a way that benefits us.</p>\n<p>I've seen some good arguments for (b), much less so for (a) and (c). Are there good arguments for these, or am I missing some other line of reasoning (very possible) that makes a non-Pascal's Mugging argument for AI research?</p>\n<p>[1] <a href=\"https://intelligence.org/files/PredictingAI.pdf\">https://intelligence.org/files/PredictingAI.pdf</a></p>\n", "parentCommentId": null, "user": {"username": "rjshade"}}, {"_id": "TLGJR7KujPi7iWuiM", "postedAt": "2015-08-13T22:34:31.743Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<blockquote>\n<p>Predictions about when we will achieve human-level AI have been wildly inaccurate in the past[1]. I don't think the predictions of current AI researchers is particularly useful data point.</p>\n</blockquote>\n<p>I agree that they're not reliable, but there's not much better. We're basically citing the same body of surveys. On a compromise reading I suppose they suggest that AI will likely happen anywhen between this decade and a few centuries, with most of the weight in this one, which sounds right to me.</p>\n<p>a) AI is already superhuman in a bunch of domains that are increasingly complex, starting from backgammon and chess to Jeopardy, driving, and image recognition. Computing power is still increasing, though less quickly than before, and in a more parallel direction. Algorithms are also getting better. There's also a parallel path to superintelligence through brain emulation. So AI getting superhuman in some domains is science fact already.</p>\n<p>Once AI gets more intelligent than one human in certain important domains, such as i) information security ii) trading iii) manipulating military hardware, or iv) persuasion, it will have significant power. See Scott's <a href=\"http://slatestarcodex.com/2015/04/07/no-physical-substrate-no-problem/\">colourful descriptions</a>. So p &lt; 10^-2 cannot be right here</p>\n<p>c) This is harder. The best achievements of the AI community so far are making some interesting theoretical discoveries re cooperation and decision theory (MIRI), attracting millions in donations from eccentric billionaires (FLI), convening dozens of supportive AI experts (FLI), writing a popular book (Bostrom) and meeting with high levels of government in UK (FHI) and Germany (CSER). This is great, though none yet shows the path to friendly AI. There are suggestions for how to make friendly AI. Even if there weren't, there'd be a nontrivial chance that this emerging x-risk-aware apparatus would find them, given that it it young and quickly gaining momentum. MIRI's approach would require a lot more technical exploration, while a brain emulation approach would require a heap more resources, as well as progress in hardware and brain-scanning technology. I think this is the substance that has to be engaged with to push this discussion forward, and potentially also to improve AI safety efforts.</p>\n", "parentCommentId": "gTbcjcDwjxsYfavAy", "user": {"username": "RyanCarey"}}, {"_id": "9zd4sdjavsDSpHPpn", "postedAt": "2015-08-15T08:08:46.967Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>While Ryan's below rebuttal to A sways me a little bit, I have reservations until I learn more about counterarguments to predictions of an intelligence explosion\nThe biggest bottleneck put here is C, which gets to the heart of why I wouldn't be ready to focus primarily on A.I. safety research among all causes right now. Thanks to you and Ryan for illustrating my concerns also, ones I can point to in the future when discussing A.I. safety research.</p>\n", "parentCommentId": "gTbcjcDwjxsYfavAy", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "bNpiBsJFmg6ysYbuY", "postedAt": "2015-08-15T08:22:58.013Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>Brian Tomasik is ab self-described &quot;negative-leaning&quot; hedonic utilitarian who is a prominent thinker for effective altruism. He's written about how humanity might have values which lead us to generating much suffering in the future, but also worries a machine superintelligence might end up doing the same. They're myriad reasons he thinks this I can't do justice to here. I believe right now he thinks the best course of action is to try steering values of present-day humanity, as much of it or as crucially an influential subset as possible, towards neglecting suffering less. He also believes doing foundational research into ascertaining better the chances of a singleton to promulgate suffering throughout space in the future. To this end he both does research with and funds colleagues at the Foundational Research Institute. </p>\n<p>His whole body of work concerning future suffering is referred to as &quot;astronomical suffering&quot; considerations, sort of complementary utilitarian consideration to Dr\n Bostrom's astronomical waste argument. You can read more of Mr.\n Tomasik's work on the far future and related topics <a href=\"http://www.utilitarian-essays.com\">here</a>. Note some of it is advanced and may require you to read beforehand to understand all premises in some of his essays, but he also usually provides citations for all this.</p>\n", "parentCommentId": "uKJSp6yGh63YbeNcn", "user": {"username": "Evan_Gaensbauer"}}, {"_id": "QQgF4QBHSpWs6iXZj", "postedAt": "2015-08-17T19:46:57.403Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>An 'option value' argument assumes that (a) the AI wouldn't take that uncertainty into account and (b) the AI wouldn't be able to recreate humanity at some later point if it decided that this was in fact the correct maximisation course. Even if it set us back by fully 10,000 years (very roughly the time from the dawn of civilisation up to now) it wouldn't be obviously that bad in the long run. Indeed, for all we know this could have already happened...</p>\n<p>In other words, in the context of an ultra-powerful ultra-well-resourced ultra-smart AI, there are few things in this world that are truly irreversible, and I see little need to give special 'option value' to humanity's or even civilisation's existence.</p>\n<p>Agree with the rest of your post re. rhetoric, and that's generally what I've assumed is going on here when this has puzzled me also.</p>\n", "parentCommentId": "iQAPnE7krY6qhqHGF", "user": {"username": "AGB"}}, {"_id": "WarGajMGS24rKAgzt", "postedAt": "2015-08-17T20:40:45.957Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>Agree with this. I was being a bit vague about what the option value was, but I was thinking of something like the value of not locking in a value set that on reflection we would disagree with. I think this covers some but not all of the scenarios Rhys was discussing.</p>\n", "parentCommentId": "QQgF4QBHSpWs6iXZj", "user": {"username": "Owen_Cotton-Barratt"}}, {"_id": "opLXKTyJ9dY7Fakh6", "postedAt": "2015-08-19T12:39:11.637Z", "postId": "a3PDjRBu9uTkRGeBS", "htmlBody": "<p>Worth noting that the negative-learning position is pretty fringe though, especially in mainstream philosophy. Personally, I avoid it.</p>\n", "parentCommentId": "bNpiBsJFmg6ysYbuY", "user": {"username": "RyanCarey"}}]