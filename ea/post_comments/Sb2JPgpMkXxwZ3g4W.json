[{"_id": "HzgQ3ksbLe48sjY9R", "postedAt": "2023-02-24T23:57:08.316Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I also watched the video and was also pleasantly surprised by how fair it ended up feeling.<br><br>For what it's worth, I didn't find the EA and systemic change section to be that interesting, but that might just be because it's a critique I've spent time reading about previously. My guess is that most other forum readers won't find much new in that section relative to existing discussions around the issue. And Thorn doesn't mention anything about tradeoffs or opportunity costs in making that critique, which makes it feel like it's really missing something. Because for practical purposes, the systemic change argument she's making requires arguing that it's worth letting a substantial number of people die from preventable diseases (plus letting a substantial number of people suffer from lack of mental healthcare, letting a substantial number of animals be subject to terrible conditions on factory farms etc.) in the short run in order to bring about systemic change that will do more to save and improve lives in the long run. It's possible that's right, but I think making that case really requires a clear understanding of what those opportunity costs are and a justification of why they would be worth accepting.&nbsp;</p>", "parentCommentId": null, "user": {"username": "MHR"}}, {"_id": "ncywLghtsu76Af7eu", "postedAt": "2023-02-25T00:00:46.623Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Thanks for the summary. I hope to make it through the video. I like thorn and fully expect her to be one of EA's higher quality outside critics.&nbsp;</p><p>I'm going to briefly jot down an answer to a (rhetorical?) question of hers. (epistemic status: far left for about 7 years)&nbsp;</p><blockquote><p>Whose vision of the future gets listened to?</p></blockquote><p>It's a great question, and as far as I know EAs outperform any overly prioritarian standpoint theorist at facing it. <a href=\"https://arbital.obormot.net/page/value_cosmopolitan.html\">I think an old arbital article (probably Eliezer?) did the best job at distilling and walking you through the exercise of generalizing cosmopolitanism</a>. But maybe <a href=\"https://forum.effectivealtruism.org/posts/wkWG4tYEgF6Ko49bH/don-t-leave-your-fingerprints-on-the-future\">Soares' version is a little more to the point</a>, and do also see <a href=\"https://forum.effectivealtruism.org/posts/r5GbSZ7dcb6nbuWch/quinn-s-shortform?commentId=pvXtqvGfjATkJq7N2\">my shortform about how I think negative longtermism dominates positive longtermism</a>. At the same time <a href=\"https://www.alignmentforum.org/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1#Computational_Social_Choice__CSC_\">Critch has been trying to get the alignment community to pay attention to social choice theory</a>. Feeling a little \"yeah, we thought of that\" and that lack of enthusiasm for something like Doing EA Better's \"indigenous ways of knowing\" remark is a feature not a bug.&nbsp;</p><p>It's a problem that terrifies me, I fear its intractability, but at least EAs will share the terror with me and understand where I'm coming from. Leftists (or more precisely prioritarian standpoint theorists) tend to be extremely confident about everything, that we'd all see how right they were if we just gave them power, etc. I don't see any reasonable way of expecting them to be more trustworthy than us about \"who's vision of the future gets listened to?\"</p>", "parentCommentId": null, "user": {"username": "quinn"}}, {"_id": "EFFZTCPTZyvyiLw5a", "postedAt": "2023-02-25T01:15:38.266Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Also, I found the lack of discussion of animal welfare frustrating. That's one of the three big cause areas within EA (or one of four if you count community building)!</p>", "parentCommentId": "HzgQ3ksbLe48sjY9R", "user": {"username": "MHR"}}, {"_id": "fjvM6xHBs8LnC875g", "postedAt": "2023-02-25T08:53:11.832Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think this question is more centred about elitism and EA being mostly western, educated, industrialized, rich and democratic (<a href=\"https://en.wikipedia.org/wiki/The_WEIRDest_People_in_the_World\">WEIRD</a>) than about the culture war between left and right.</p>", "parentCommentId": "ncywLghtsu76Af7eu", "user": {"username": "Felix Wolf"}}, {"_id": "2rr6Nkto6ereiw4Gg", "postedAt": "2023-02-25T09:56:02.961Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>The main point I took from video was that Abigail is kinda asking the question: \"How can a movement that wants to change the world be so apolitical?\" This is also a criticism I have of many EA structures and people. I even have come across people who view EA and themselves as not political, even as they are arguing for longtermism. The video also highlights this.</p><p>When you are quantifying something you don't become objective all over sudden. You cannot quantify everything, so you have to make a choice on what you want to quantify. And this is a political choice. There is not objective source of truth that tells you that for example quality adjusted life years are the best objective measure. People choose what makes the most sense to them given their background. But you could easily switch it to something else. There is only your subjective choice on what you want to focus. And I would really appreciate if this would be highlighted more in EA.</p><p>Right now the vibe is often \"We have objectively compared such and such and therefore the obvious choice is this intervention or cause.\" But this just frames personal preferences on what is important as an objective truth about the world. Would be great if this subjectivity would be acknowledged more.</p><p>And one final point the video also hints at: In EA basically all modern philosophy outside of consequentialism is ignored. Even though much of that philosophy is explicitly developed to crticise pure reason and consequentialism. But if you read EA material you get the impression to only notable philosophers of the 20th century are Peter Singer and Derek Parfit.</p>", "parentCommentId": null, "user": {"username": "FJehn"}}, {"_id": "zcZtnEq5jpFXmWN3v", "postedAt": "2023-02-25T09:58:05.514Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I'd like to add a thought on the last point:</p><p>EA appears to largely ignore the developments of modern and post-modern philosophy, making EA appear like a genuinely new idea/movement. Which it is not. That means that there is a lot to learn from past instances of EA-like movements. EA-like meaning Western rich people trying to do good with Rationality. 20th century philosophy is brimming with very valid critiques of Rationality, but somehow EA seems to jump from Bentham/Mills to Singer/Parfit without batting an eye.</p><p>Abigail leaves open how we should do good, whether we want to pursue systemic change or work within the system, or even how we shall define what \"good\" is. I am sure this is intentionally put at the end of the video. She warns people who consider joining EA to do so with open eyes. I deeply agree with this. If you are thinking about making EA your political movement of choice, be very careful, as with any political movement. EA claims to be open to different moral standpoints, but it most certainly not. There are unchecked power dynamics at play, demographic bias, \"thought leaders\", the primacy of Rationality. If I had any advice for anyone in EA, I would recommend they go and spend a year or more <strong>learning about all the philosophy that came AFTER utilitarianism</strong>*. Otherwise, EA will be lacking context, and could even appear as The Truth. You will be tempted to buy into the opinion of a small number of apparently smart people saying apparently smart things, and by that, hand over your moral decisions to them.</p><p><br>* (for a start, Philosophize This is a nice podcast that deals at length with a lot of these topics)</p>", "parentCommentId": "2rr6Nkto6ereiw4Gg", "user": {"username": "Ekaterina_Ilin"}}, {"_id": "cqdDAB4KgsZWnzwdZ", "postedAt": "2023-02-25T10:56:29.199Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I agree that the choices we make are in some sense political. But they\u2019re not political in the sense that they involve party or partisan politics. Perhaps it would be good for EAs to get involved in that kind of politics (and we sometimes do, usually in an individual capacity), but I personally don\u2019t think it would be fruitful at an institutional level and it\u2019s a position that has to be argued for.</p>\n<p>Many EAs would also disagree with your assumption that there aren\u2019t any objective moral truths. And many EAs who don\u2019t endorse moral realism would agree that we shouldn\u2019t make the mistake of assuming that all choices are equally valid, and that the only reason anyone makes decisions is due to our personal background.</p>\n<p>Without wishing to be too self-congratulatory, when you look at the beings that most EAs consider to be potential moral patients (nonhuman animals including shrimp and insects, potential future people, digital beings), it\u2019s hard to argue that EAs haven\u2019t made more of an effort than most to escape their personal biases.</p>\n", "parentCommentId": "2rr6Nkto6ereiw4Gg", "user": {"username": "JBentham"}}, {"_id": "5aHFYfWaFx6Yktcfk", "postedAt": "2023-02-25T11:04:31.594Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>EA is a movement that aims to use reason and evidence to do the most good, so the centrality of \u201crationality\u201d (broadly speaking) shouldn\u2019t be too surprising. Many EAs are also deeply familiar with alternatives to utilitarianism. While most (according to the surveys) are utilitarians, some are non-utilitarian consequentialists or pluralists.</p>\n<p>I suspect that the movement is dominated by utilitarians and utilitarian-leaning people because while all effective altruists shouldn\u2019t necessarily be utilitarians, all utilitarians should be effective altruists. In contrast, it\u2019s hard to see why a pure deontologist or virtue ethicist should, as a matter of philosophical consistency, be an effective altruist. It\u2019s also difficult to see how a pure deontologist or virtue ethicist could engage in cause prioritisation decisions without ultimately appealing to consequences.</p>\n", "parentCommentId": "zcZtnEq5jpFXmWN3v", "user": {"username": "JBentham"}}, {"_id": "7HYEcfH6iELWTX6cm", "postedAt": "2023-02-25T11:27:42.039Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think it's fairly unsurprising that EA is mostly consequentialists or utilitarians. But often it goes way beyond that, into very specific niches that are not all a requirement for trying to \"do good effectively\".&nbsp;</p><p>For example, a disproportionate amount of people here are are capital R \"Rationalists\", referring to the subculture built around fans of the \"sequences\" blogposts on Lesswrong written by Yudkowsky. I think this subgroup in particular suffers from \"<a href=\"https://en.wikipedia.org/wiki/Not_invented_here\">not invented here</a>\" syndrome, where philosophical ideas that haven't been translated into rationalist jargon are not engaged with seriously.&nbsp;</p>", "parentCommentId": "5aHFYfWaFx6Yktcfk", "user": {"username": "titotal"}}, {"_id": "eF6JxextCLjAyhvEm", "postedAt": "2023-02-25T12:02:14.213Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<blockquote><p>The main point I took from video was that Abigail is kinda asking the question: \"How can a movement that wants to change the world be so apolitical?\" This is also a criticism I have of many EA structures and people.</p></blockquote><p>I think it's <i>surprising</i> that EA is so apolitical, but I'm not convinced it's wrong to make some effort to avoid issues that are politically hot. Three reasons to avoid such things: 1) they're often not the areas where the most impact can be had, even ignoring constraints imposed by them being hot political topics 2) being hot political topics makes it even harder to make significant progress on these issues and 3) if EAs routinely took strong stands on such things, I'm confident it would lead to significant fragmentation of the community.</p><p>EA does take some political stances, although they're often not on standard hot topics: they're strongly in favour of animal rights and animal welfare, and were involved in lobbying for a very substantial piece of legislation recently introduced in Europe. Also, a reasonable number of EAs are becoming substantially more \"political\" on the question of how quickly the frontier of AI capabilities should be advanced.</p>", "parentCommentId": "2rr6Nkto6ereiw4Gg", "user": {"username": "David Johnston"}}, {"_id": "tAGcszrqetLuWatLq", "postedAt": "2023-02-25T12:07:26.820Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<blockquote><p>I agree that the choices we make are in some sense political. But they\u2019re not political in the sense that they involve party or partisan politics.</p></blockquote><p>&nbsp;</p><p>I disagree. Counter-examples: Sam Bankman-Fried was one of the largest donors to Joe Biden's presidential campaign. Voting and electoral reform has often been a topic on the EA Forum and appeared on the 80000h podcast. I know several EAs who are or have been actively involved in party politics in Germany. The All-Party Parliamentary Group in the UK says on its website that it \"aims to create space for cross-party dialogue\". I would put these people and organizations squarely in the EA space. The choices these people and organizations made directly involve political parties*.<br><br><br>* or their abolition, in the case of some proposed electoral reforms, I believe.</p>", "parentCommentId": "cqdDAB4KgsZWnzwdZ", "user": {"username": "Ekaterina_Ilin"}}, {"_id": "dm7dRsepbpRCGAY2E", "postedAt": "2023-02-25T12:30:23.918Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I want to clarify that I do specifically mean philosophical movements like existentialism, structuralism, post-structuralism, the ethics behind communism and fascism -- which all were influential in the 20th century. I would also argue that the grouping into consequentialism/virtue ethics/deontology does not capture the perspectives brought up in the aforementioned movements. I would love to see EAs engage with more modern ideas about ethics because they specifically shed light on the flexibility and impermanence of the terms '<i>reason</i>' and '<i>evidence</i>' over the decades.&nbsp;<br><br>Sure, you have to choose some model at some point to act, or else you'll be paralyzed. But I really wish that people who make <strong>significant life changes based on </strong><i><strong>reason</strong></i><strong> and </strong><i><strong>evidence</strong></i><strong> take a close look at how these terms are defined within their political movement, and by whom.</strong></p>", "parentCommentId": "5aHFYfWaFx6Yktcfk", "user": {"username": "Ekaterina_Ilin"}}, {"_id": "hgcy5FLQjWihbJMwP", "postedAt": "2023-02-25T13:01:22.790Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I don\u2019t quite see how existentialism, structuralism, post-structuralism and fascism are going to help us be more effectively altruistic, or how they\u2019re going to help us prioritise causes. Communism is a different case as in some formats it\u2019s a potential altruistic cause area that people may choose to prioritise.</p>\n<p>I also don\u2019t think that these ideas are more \u201cmodern\u201d than utilitarianism, or that their supposed novelty is a point in their favour. Fascism, just to take one of these movements, has been thoroughly discredited and is pretty much the antithesis of altruism. These movements are movements in their own right, and I don\u2019t think they\u2019d want EAs to turn them into something they\u2019re not. The same is true in the opposite direction.</p>\n<p>By all means, make an argument in favour of these movements or their relevance to EA. But claiming that EAs haven\u2019t considered these movements (I have, and think they\u2019re false) isn\u2019t likely to change much.</p>\n", "parentCommentId": "dm7dRsepbpRCGAY2E", "user": {"username": "JBentham"}}, {"_id": "vYYR94u9kkCxgyb74", "postedAt": "2023-02-25T13:12:30.803Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>My comment mainly referred to the causes we\u2019ve generally decided to prioritise. When we engage in cause prioritisation decisions, we don\u2019t ask ourselves whether they\u2019re a \u201cleftist\u201d or \u201crightist\u201d cause area.</p>\n<p>I did say that EAs may engage in party politics in an individual or group capacity. But they\u2019re still often doing so in order to advocate for causes that EAs care about, and which people from various standard political ideologies can get on board with. Bankman-Fried also donated to Republican candidates who he thought were good on EA issues, for example. And the name of the \u201call-party\u201d parliamentary group clearly distinguishes it from just advocating for a standard political ideology or party.</p>\n", "parentCommentId": "tAGcszrqetLuWatLq", "user": {"username": "JBentham"}}, {"_id": "oK3HLYne4CAzydyuA", "postedAt": "2023-02-25T13:26:42.730Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I\u2019m sure Thorn does do this (I haven\u2019t watched the video in full yet), but it seems more productive to criticise the \u201cEA vision of the future\u201d than to ask where it comes from (and there were EA-like ideas in China, India, Ancient Greece and the Islamic world long before Bentham).</p>\n<p>MacAskill, Ord and others seem to me to have advocated a highly pluralistic future in which humanity is able to reflect on its values. Clearly, some people don\u2019t like what they think is the \u201cEA vision of the future\u201d and want their vision to prevail instead. The question seems to imply, though, that EAs are the only ones who are excluding others\u2019 visions of the future from their thinking. Actually, everyone is doing that, otherwise they wouldn\u2019t have a specific vision.</p>\n", "parentCommentId": "fjvM6xHBs8LnC875g", "user": {"username": "JBentham"}}, {"_id": "PvyJ8E8dNgpAAWGdF", "postedAt": "2023-02-25T14:05:22.046Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Just regarding your last sentence: I disagree that it has any bearing whatsoever whether everyone else is excluding other's visions of the future or not.<br>No matter if everyone else is great or terrible - I want EA to be as good as it possibly can, and if it fails on some metric it should be criticised and changed in that regard, no matter if everyone else fails on the same metric too, or not, or whatever.</p>", "parentCommentId": "oK3HLYne4CAzydyuA", "user": {"username": "Moya Schiller"}}, {"_id": "WuWGBHNftqTAZti23", "postedAt": "2023-02-25T15:06:17.208Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>It seems to me that we are talking about different definitions about what political means. I agree that in some situations it can make sense to not chip in political discussions, to not get pushed to one side. &nbsp;I also see that there are some political issues where EA has taken a stance like animal welfare. However, when I say political I mean what are the reason for us doing things and how do we convince other people of it? In EA there are often arguments that something is not political, because there has been an \"objective\" calculation of value. However, there is almost never a justification why something was deemed important, even though when you want to change the world in a different way, this is the important part. Or on a more practical level why are QUALYs seen as the best way to measure outcomes in many cases? Using this and not another measure is choice which has to be justified.&nbsp;</p>", "parentCommentId": "eF6JxextCLjAyhvEm", "user": {"username": "FJehn"}}, {"_id": "c3qLHHsFjjYu6JW9H", "postedAt": "2023-02-25T15:18:39.916Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Surely, they are more modern than utilitarianism. Utilitarianism has been developed in the 19th century, while all the other ones mentioned are from the 20th century. And it is not their \"novelty\" which is interesting, but that they are a direct follow up and criticism of things like utilitarianism. &nbsp;Also, I don't think that post above was an endorsement of using fascism, but instead a call to understand the idea why people even started with fascism in the first place.&nbsp;</p><p>The main contribution of the above mentioned fields of ideas to EA is that they highlight that reason is not a strong tool, as many EA think it is. You can easily bring yourself into bad situation, even if you follow reason all the way. Reason is not something objective, but born from your standpoint in the world and the culture you grow up in.&nbsp;</p><p>And if EA (or you) have considered things like existentialism, structuralism, post-structuralism I'd love to see those arguments why it is not important to EA. Never seen anything in this regard.&nbsp;</p>", "parentCommentId": "hgcy5FLQjWihbJMwP", "user": {"username": "FJehn"}}, {"_id": "DdXNY3HHiohNGGC6Y", "postedAt": "2023-02-25T15:40:42.989Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think reason is as close to an objective tool as we\u2019re likely to get and often isn\u2019t born from our standpoint in the world or the culture we grow up in. That\u2019s why people from many different cultures have often reached similar conclusions, and why almost everyone (regardless of their background) can recognise logical and mathematical truths. It\u2019s also why most people agree that the sun will rise the next morning and that attempting to leave your house from your upper floor window is a bad idea.</p>\n<p>I think the onus is on advocates of these movements to explain their relevance to \u201cdoing the most good\u201d. As for the various 20th Century criticisms of utilitarianism, my sense is that they\u2019ve been parried rather successfully by other philosophers. Finally, my point about utilitarianism being just as modern is that it hasn\u2019t in any way been superseded by these other movements \u2014 it\u2019s still practiced and used today.</p>\n", "parentCommentId": "c3qLHHsFjjYu6JW9H", "user": {"username": "JBentham"}}, {"_id": "h4qpzY3yoWkkDHw9H", "postedAt": "2023-02-25T16:04:44.944Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Just regarding this bit: \"MacAskill, Ord and others seem to me to have advocated a highly pluralistic future in which humanity is able to reflect on its values.\"</p><p>I have posited, multiple times, in different EA spaces, that EAs should learn more languages in order to be better able to think, better able to understand perspectives further removed from that which they were raised in, healthier (trilinguals are massively protected against dementia, Alzheimer's, etc), etc.&nbsp;</p><p>And the response I have received has been broadly \"eh\" or at best \"this is interesting but I don't know if it's worth EAs time\".</p><p>I have not seen any EA \"world literature\" circles based around trying to expand their horizons to perspectives as far from their own as possible. I have not seen any EA language learning groups. I have not seen any effort put towards using the EA community (that is <i>so important to build</i>!) in order to enable individual EAs to become better at understanding radically different perspectives, etc.</p><p>So like... Iunno, I don't buy the \"it's not a problem we're mostly wealthy white guys\" argument. It seems to me like a lot of EAs don't know what they don't know, and don't realize the axes along which they could not-know-things on top. They don't behave the way people who are genuinely invested in a more pluralistic vision of the future would behave. And they don't react positively to proposals that aim to improve that.</p>", "parentCommentId": "oK3HLYne4CAzydyuA", "user": {"username": "O Carciente"}}, {"_id": "mh76t3SoQyZfR5WBK", "postedAt": "2023-02-25T16:07:43.133Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Alternatives to QALYs (such as WELLBYs) have been put forward from within the EA movement. But if we\u2019re trying to help others, it seems plausible that we should do it in ways that they care about. Most people care about their quality of life or well-being, as well as the amount of time they\u2019ll have to experience or realise that well-being.</p>\n<p>I\u2019m sure there are people who would say they are most effectively helping others by \u201csaving their souls\u201d or promoting their \u201cnatural rights\u201d. They\u2019re free to act as they wish. But the reason that EAs (and not just EAs, because QALYs are widely used in health economics and resource allocation) have settled on quality of life and length of life is frankly because they\u2019re the most plausible (or least implausible) ways of measuring the extent to which we\u2019ve helped others.</p>\n", "parentCommentId": "WuWGBHNftqTAZti23", "user": {"username": "JBentham"}}, {"_id": "jtEByhsymn7DoT876", "postedAt": "2023-02-25T16:14:10.528Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Thanks for your reply! I\u2019m not saying that EA should be able to exclude others\u2019 visions because others are doing so. I\u2019m claiming that it\u2019s impossible not to exclude others\u2019 visions of the future. Let\u2019s take the pluralistic vision of the future that appeals to MacAskill and Ord. There will be many people in the world (fascists, Islamists, evangelical Christians) who disagree with such a vision. MacAskill and Ord are thus excluding those visions of the future. Is this a bad thing? I will let the reader decide.</p>\n", "parentCommentId": "PvyJ8E8dNgpAAWGdF", "user": {"username": "JBentham"}}, {"_id": "LEsqKC5ycthW22bRM", "postedAt": "2023-02-25T16:29:10.293Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Thanks for your reply! Firstly, there will many EAs (particularly from the non-Anglosphere West and non-Western countries) who do understand multiple languages. I imagine there are also many EAs who have read world literature.</p>\n<p>When we say that EAs \u201cmostly\u201d have a certain demographic background, we should remember that this still means there are hundreds of EAs that don\u2019t fit that background at all and they shouldn\u2019t be forgotten. Relatedly, I (somewhat ironically) think critics of EA could do with studying world history because it would show them that EA-like ideas haven\u2019t just popped up in the West by any means.</p>\n<p>I also don\u2019t think one needs to understand radically different perspectives to want a world in which those perspectives can survive and flourish into the future. There are so many worldviews out there that you have to ultimately draw a line somewhere, and many of those perspectives will just be diametrically opposed to core EA principles, so it would be odd to promote them at the community level. Should people try to expand their intellectual horizons as a personal project? Possibly!</p>\n", "parentCommentId": "h4qpzY3yoWkkDHw9H", "user": {"username": "JBentham"}}, {"_id": "vcbjPmbxCuBpKkeuC", "postedAt": "2023-02-25T17:54:10.667Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think you might have misunderstood my comment.&nbsp;</p><p>I, as someone who is at least trying to be an EA, and who can speak two languages fluently and survive in 3 more, would \"count\" as an EA who is not from \"Anglosphere West\", and who has read world literature. So yes, I know I exist.</p><p>My point is that EA, as a <i>community</i>, should encourage that kind of thing among its members. And it really doesn't. Yes, people can do it as a personal project, but I think EA generally puts a lot of stock on people doing what are ultimately fairly difficult things (like, self-directed study of AI) without providing a consistent community with accountability that would help them achieve those things. And I think that the WEIRD / Anglosphere West / etc. demographic bias of EA is part of the reason why this seems to be the case.&nbsp;</p><p>Yes, it is possible to want a perspective to survive in the future without being particularly well-versed in it. I theoretically would not want Hinduism to go extinct in 50 years and can want that without knowing a whole lot about Hinduism.&nbsp;</p><p>That said, in order to know <i>what</i> will allow certain worldviews, and certain populations to thrive, you need to understand them at least a little. And if you're going to try to maximize the good you do for people, which would include a LOT of people who are not Anglosphere West. If I genuinely thought that Hinduism was under threat of extinction and wanted to do something about it, trying to do that without learning anything about Hinduism would be really short-sighted of me.&nbsp;</p><p>Given that most human beings for most of history have not been WEIRD in the Heinrich sense, and that a lot of currently WEIRD people are becoming less so (increase in antidemocratic sentiment, the affordability crisis and rising inequality) it is reasonable to believe that the future people EA is so concerned with will not be particularly WEIRD. And if you want to do what is best for that population, there should be more effort put into ensuring they <i>will</i> be WEIRD in some fashion<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3mex9gvj5se\"><sup><a href=\"#fn3mex9gvj5se\">[1]</a></sup></span>&nbsp;<strong>or</strong> into ensuring that EA interventions will help non-WEIRD people a meaningful amount in ways that they will value. Which is more than just malaria nets. &nbsp;</p><p>And like... I haven't seen that conversation.&nbsp;</p><p>I've seen allusions to it. But I haven't really seen it. Nor have I seen EA engage with the \"a bunch of philosophers and computer scientists got together and determined that the most important thing you can be is a philosopher or computer scientist\" critique particularly well, nor have I seen EA engage very well with the question of lowering the barriers of entry (which I also received a fairly unhelpful response to when I posited it, which boiled down to \"well <i>you</i> understand all of the EA projects that you're not involved in and create lower barriers of entry for all of them\", which again comes back to the problem that EA creates a community and then doesn't seem to actually use it to do the things communities are good for..?).&nbsp;</p><p>So I think it's kind of a copout to just say \"well, you can care in this theoretical way about perspectives you don't understand\", given that part of the plan of EA, and the success condition is to <i>affect those people's lives meaningfully</i>.&nbsp;</p><p>Not to mention the question of \"promoting\" vs \"understanding\".&nbsp;</p><p>Should EA promote, iunno, fascism, on a community level? Obviously not.&nbsp;</p><p>Should EA seek to understand fascism, and authoritarianism more broadly, as a concerning potential threat that has arisen multiple times and could arise yet again with greater technological and military force in the future? Fucking definitely.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3mex9gvj5se\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3mex9gvj5se\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The closest thing to this is the \"liberal norms\" political career path as far as I'm aware, but I think both paths should be taken concurrently and that OR is inclusive, yet the second is largely neglected.</p></div></li></ol>", "parentCommentId": "LEsqKC5ycthW22bRM", "user": {"username": "O Carciente"}}, {"_id": "9XXgG73SwHEcoNuHk", "postedAt": "2023-02-25T19:45:54.699Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Excellent reply. I cheaply agree-voted but am not agree-voting in a costly manner because that would require me backing up my cosmopolitan values by learning a language.&nbsp;</p><p>Skeptical that language learning is actually the most pivotal part of learning about wild (to you) perspectives, but it's not obviously wrong.&nbsp;</p>", "parentCommentId": "h4qpzY3yoWkkDHw9H", "user": {"username": "quinn"}}, {"_id": "qLPrLcYx2MKYfmBDY", "postedAt": "2023-02-25T19:51:02.651Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<blockquote><p>it is reasonable to believe that the future people EA is so concerned with will not be particularly WEIRD&nbsp;</p></blockquote><p>There's a half-joking take that some people in longtermism bring up sometimes that roughly looks like</p><blockquote><p>according to demographic models, there's not a sense in which longtermism isn't just a flavor of afrofuturism</p></blockquote><p>(i.e. predictions that most new humans will be born in africa)&nbsp;</p>", "parentCommentId": "vcbjPmbxCuBpKkeuC", "user": {"username": "quinn"}}, {"_id": "vMPfHXPhXw2NndXty", "postedAt": "2023-02-25T19:56:03.753Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>What are the beliefs of prioritarian standpoint theorists?</p>\n", "parentCommentId": "ncywLghtsu76Af7eu", "user": {"username": "robirahman"}}, {"_id": "8qRsw7xjyjHsKbu2g", "postedAt": "2023-02-25T20:03:33.837Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Prioritarianism is a flavor of utilitarianism that tries to increase impact by starting with the oppressed or unprivileged.</p><p>Standpoint theory or standpoint epistemology is about advantages and disadvantages to gaining knowledge based on demographic membership.&nbsp;</p><p>Leftist culture is deeply exposed to both of these views, occasionally to the point of them being invisible/commonsensical assumptions.&nbsp;</p><p>My internal gpt completion / simulation of someone like Thorn assumed that her rhetorical question was gesturing toward \"these EA folks seem to be underrating at least one of prioritarianism or standpoint epistemology\"&nbsp;</p>", "parentCommentId": "vMPfHXPhXw2NndXty", "user": {"username": "quinn"}}, {"_id": "GfktADbu3nZXpofr6", "postedAt": "2023-02-25T20:14:37.027Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Great comment, thanks for clarifying your position. To be clear, I\u2019m not particularly concerned about the survival of most particular worldviews as long as they decline organically. I just want to ensure that there\u2019s a marketplace in which different worldviews can compete, rather than some kind of irreversible \u2018lock-in\u2019 scenario.</p>\n<p>I have some issues with the entire \u2018WEIRD\u2019 concept and certainly wouldn\u2019t want humanity to lock in \u2018WEIRD\u2019 values (which are typically speciesist). Within that marketplace, I do want to promote moral circle expansion and a broadly utilitarian outlook as a whole. I wouldn\u2019t say this is as neglected as you claim it is \u2014 MacAskill discusses the value of the future (not just whether there is a future) extensively in his recent book, and there are EA organisations devoted to moral values spreading. It\u2019s also partly why \u201cphilosopher\u201d is recommended as a career in some cases, too.</p>\n<p>If we want to spread those values, I agree with you that learning about competitor philosophies, ideologies, cultures and perspectives (I personally spend a fair bit of time on this) would be important, and that lowering language barriers could be helpful.</p>\n<p>It could also be useful to explore whether there are interventions in cultures that we\u2019re less familiar with that could improve people\u2019s well-being even more than the typical global health interventions that are currently recommended. Perhaps there\u2019s something about a particular culture which, if promoted more effectively, would really improve people\u2019s lives. But maybe not: children dying of malaria is really, really bad, and that\u2019s not a culture-specific phenomenon.</p>\n<p>Needless to say, none of the above applies to the vast majority of moral patients on the planet, whether they\u2019re factory farmed land animals, fishes or shrimps. (Though if we want to improve, say, shrimp welfare in Asia, learning local languages could help us work and recruit more effectively as well as spread values.)</p>\n", "parentCommentId": "vcbjPmbxCuBpKkeuC", "user": {"username": "JBentham"}}, {"_id": "iEWnAhNnNha8fhhKD", "postedAt": "2023-02-25T22:51:33.928Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>TBH I think that half-joking take should probably be engaged with more seriously (maybe say, pursuing more translations of EA works into Igbo or something), and I'm glad to hear it.&nbsp;</p>", "parentCommentId": "qLPrLcYx2MKYfmBDY", "user": {"username": "O Carciente"}}, {"_id": "LnAqsFfcguk2WsRfQ", "postedAt": "2023-02-25T23:54:16.808Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Thank you! I don't think it's necessarily the most pivotal&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsxeour278qp\"><sup><a href=\"#fnsxeour278qp\">[1]</a></sup></span>&nbsp;but it is one part that has recently begun having its barrier of entry lowered&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbqxlm25zo5m\"><sup><a href=\"#fnbqxlm25zo5m\">[2]</a></sup></span>. Additionally, while reading broadly&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv5gc6rsgf09\"><sup><a href=\"#fnv5gc6rsgf09\">[3]</a></sup></span>could also help, the reason why language-learning looks so good in my eyes is because of the stones-to-birds ratio.&nbsp;</p><p>If you read very broadly and travel a lot, you may gain more \"learning about wild(to you) perspective\" benefits. But if you learn a language&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc1kr9ddhqeh\"><sup><a href=\"#fnc1kr9ddhqeh\">[4]</a></sup></span>you are:&nbsp;</p><p>1) benefitting your brain,&nbsp;</p><p>2) increasing the amount of people in the world you can talk to, and whose work you can learn from,&nbsp;</p><p>3) absorb new ideas you may not have otherwise been able to absorb,&nbsp;</p><p>4) acquire new intuitions&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref54x6euu3pj9\"><sup><a href=\"#fn54x6euu3pj9\">[5]</a></sup></span>.&nbsp;</p><p>You can separately do things that will fulfill all four of those things (and even fulfill some of the other benefits that language learning can provide for you) without learning another language. But I am very bad at executive skills, and juggling 4+ different habits, so I generally don't find the idea of say...&nbsp;</p><ul><li>doing 2 crosswords, 2 4x4x4 sudoku a day, and other brain teasers +&nbsp;</li><li>taking dance classes or learning a new instrument +&nbsp;</li><li>taking communications classes and reading books about public speaking and active listening +&nbsp;</li><li>engaging in comparative-translation reading +</li><li>ingratiating myself to radically different communities in order to cultivate those modes of thought&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5fzx6q8i2m2\"><sup><a href=\"#fn5fzx6q8i2m2\">[6]</a></sup></span>&nbsp;</li></ul><p>...to be less onerous than learning a new language. &nbsp;Especially since language-learning can help and be done concurrently with these alternatives&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7klan69g3nr\"><sup><a href=\"#fn7klan69g3nr\">[7]</a></sup></span>.</p><p>Language learning is also something that can help with community bonding, which would probably be helpful to the substantial-seeming portion of EAs who are kind of lonely and depressed. &nbsp;It can also help you remember what it is like to suck at something, which I think a lot of people in Rationalist spaces would benefit from more broadly, since so many of them were gifted kids who now have anxiety, and becoming comfortable with failure and iteration is also good for you and your ability to do things in general.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsxeour278qp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsxeour278qp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Travelling broadly will probably provide better results to most people, but it also costs a lot of money, even more if you need to hire a translator.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbqxlm25zo5m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbqxlm25zo5m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Especially with Duolingo offering endangered languages now.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv5gc6rsgf09\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv5gc6rsgf09\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Say, reading a national award-winning book from every nation in the world.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc1kr9ddhqeh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc1kr9ddhqeh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Or, preferrably, if you learn 2, given that the greatest benefits are found in trilinguals+.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn54x6euu3pj9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref54x6euu3pj9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I find that personally, I am more socially conservative in Spanish and more progressive in English, which has allowed me to test ideas against my own brain in a way that most monolinguals I talk to seem to find somewhat alien and much more effortful. Conversely, in French, I am not very capable, and I find that quite useful because it allows me to force myself to simplify my ideas on the grounds that I am literally unable to express the complex version.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5fzx6q8i2m2\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5fzx6q8i2m2\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;+ [whatever else I haven't thought of yet that would help obtain these benefits]</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7klan69g3nr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7klan69g3nr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Music terminology is often in French or Italian, learning languages will just broaden your vocabulary for crossword puzzles, knowing another language is a gateway to communities that were previously closed to you, and you can engage in reading different translations of something more easily if you can also just read it in the original language. &nbsp;</p></div></li></ol>", "parentCommentId": "9XXgG73SwHEcoNuHk", "user": {"username": "O Carciente"}}, {"_id": "m7JAJKMk9LeLdd4qR", "postedAt": "2023-02-26T00:03:33.091Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<blockquote><p>If we want to spread those values, I agree with you that learning about competitor philosophies, ideologies, cultures and perspectives (I personally spend a fair bit of time on this) would be important, and that lowering language barriers could be helpful.</p></blockquote><p>Wonderful! What specific actions could we take to make that easier for you (and others like you for whom this would be a worthwhile pursuit)?</p><p>Maybe a reading group that meets every week (or month). Or an asynchronous thread in which people provide reviews of philosophical articles or world literature. Or a group of Duolingo \"friends\" (or some other language-learning app of people's choice, I have a variety of thoughts on which languages should be prioritized, but starting with <i>something</i> would be good, and Spanish-language EAs seem to be growing in number and organization).&nbsp;</p><blockquote><p>It could also be useful to explore whether there are interventions in cultures that we\u2019re less familiar with that could improve people\u2019s well-being even more than the typical global health interventions that are currently recommended. Perhaps there\u2019s something about a particular culture which, if promoted more effectively, would really improve people\u2019s lives.&nbsp;</p></blockquote><p>Bhutan's notion of Gross Domestic Happiness, Denmark's \"hygge\", whatever it is that makes certain people with schizophrenia from <a href=\"https://news.stanford.edu/2014/07/16/voices-culture-luhrmann-071614/\">Africa get the voices to say nice things to them</a>, indigenous practices of farming and sustainable hunting, and maybe the practice of \"insulting the meat\" just off the top of my head, would probably be good things to make more broadly understood and build into certain institutions. Not to mention that knowledge of cultural features that need to be avoided or handled somewhat (for example, overtly strict beauty standards which harm people in a variety of different cultures).&nbsp;</p><blockquote><p>(Though if we want to improve, say, shrimp welfare in Asia, learning local languages could help us work and recruit more effectively as well as spread values.)</p></blockquote><p>And, very importantly, it could allow you to discover new things to value, new frameworks, new ways of approaching a problem. Every language you learn comes with new intuition pumps, new frames upon which you can hang your thoughts.&nbsp;</p><p>Even if you think the vast majority of moral patients are non-human and our priorities should reflect that, there are ways of thinking about animals and their welfare that have been cultivated for centuries by less WEIRD populations that could prove illuminating to you. I don't know about them, because I have my own areas of ignorance. But that's the kind of thing that EA could benefit from aggregating somewhere.&nbsp;</p><p>I would be very interested in working on a project like that, of aggregating non-EA perspectives in various packages for the convenience of individual EAs who may want to learn about perspectives that are underrepresented in the community and may offer interesting insights.&nbsp;</p>", "parentCommentId": "GfktADbu3nZXpofr6", "user": {"username": "O Carciente"}}, {"_id": "qspEiogm48n526iiG", "postedAt": "2023-02-26T00:35:17.498Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think the note on Not Invented Here syndrome is actually amazing and I'm very happy you introduced that concept into this discussion.</p>", "parentCommentId": "7HYEcfH6iELWTX6cm", "user": {"username": "O Carciente"}}, {"_id": "BfNyeBDLBpPZKo3Tz", "postedAt": "2023-02-26T09:38:11.851Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I want to note that both Philosophy Tube's and Sabine Hossenfelder's sceptism against AI-risk stemmed from AGI's reliance on extraordinary hardware capacities. They both believe it will be very difficult for an AGI to copy itself because there won't be suitable hardware in the world. Therefore AGI will be physically bound, limited in number and easier to deal with. I think introductory resources should address this more often. For example, there isn't a mention of this criticism in <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">80000 Hours' problem</a> profile on this topic.</p>", "parentCommentId": null, "user": {"username": "emre kaplan"}}, {"_id": "7qcYjzFr8hhd3LLnE", "postedAt": "2023-02-26T10:12:31.901Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>\"There is not objective source of truth that tells you that for example quality adjusted life years are the best objective measure.\"</p>\n<p>There's no objective source of truth telling humans to value what we value; on some level it's just a brute fact that we have certain values. But given a set of values, some metrics will do better vs. worse at describing the values.</p>\n<p>Or in other words: Facts about how much people prefer one thing relative to other things are \"subjective\" in the weak sense that all psychological facts are subjective: they're about subjects / minds. But psychology facts aren't \"subjective\" in a sense like \"there are no facts of the matter about minds\". Minds are just as real a part of the world as chairs, electrons, and zebras.</p>\n<p>Consider, for example, a measure that says \"a sunburn is 1/2 as bad as a migraine\" versus one that says \"a sunburn is a billion times as bad as a migraine\". We can decompose this into a factual claim about the relative preferences of some group of agents, plus a normative claim that calls the things that group dislikes \"bad\".</p>\n<p>For practical purposes, the important contribution of welfare metrics isn't \"telling us that the things we dislike are bad\"; realists are already happy to run with this, and anti-realists are happy to play along with the basic behavioral take-aways in practice.</p>\n<p>Instead, the important contribution is the factual claim about what a group prefers, which is as objective/subjective as any other psych claim. Viewed through that lens, even if neither of the claims above is perfectly accurate, it seems clear that the \"1/2 as bad\" claim is a lot closer to the psychological truth.</p>\n", "parentCommentId": "2rr6Nkto6ereiw4Gg", "user": {"username": "RobBensinger"}}, {"_id": "R9Lx2iNLbBKLC3sZC", "postedAt": "2023-02-26T10:34:05.349Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<blockquote><p>EA heavily draws from Rationalism, which views reason as the chief source of knowledge; specifically, EA heavily prioritises quantitative analysis over qualitative analysis.</p></blockquote><p>This misunderstands what rationalism is (in the context of EA, LW, etc.).</p><p>You're thinking \"reason, rationality, deliberation, explicit thought, etc. as opposed to emotion, intuition, implicit thought, etc.\" LessWrong-style \"rationality\" is instead about <i>truth as opposed to falsehood</i> (\"epistemic rationality\") and <i>succeeding in your goals as opposed to failing</i> (\"instrumental rationality\").</p><p>Classic LW content like <a href=\"https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality-1\">What Do We Mean By \"Rationality\"?</a>, <a href=\"https://www.lesswrong.com/posts/zuJmtSqt3TsnBTYyu/communicating-rationality-to-the-public-julia-galef-s-the\">The Straw Vulcan</a>, <a href=\"https://www.lesswrong.com/posts/SqF8cHjJv43mvJJzx/feeling-rational\">Feeling Rational</a>, and <a href=\"https://www.lesswrong.com/posts/AJ9dX59QXokZb35fk/when-not-to-use-probabilities\">When (Not) To Use Probabilities</a> talk about how different these concepts are: using deliberative, explicit, quantitative, etc. thought is rational (in the LW sense) if it <i>helps you better understand things</i> or <i>helps you succeed in life</i>, but emotions, hunches, intuitions, etc. are also a crucial part of understanding the world and achieving your goals.</p>", "parentCommentId": null, "user": {"username": "RobBensinger"}}, {"_id": "5y7rW93ec6fbYEjeS", "postedAt": "2023-02-26T16:44:19.708Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Also I think \"reason as the chief source of knowledge\" is not quite it, right? I think \"observation is the chief source of knowledge\" would pass an ideological turing test a bit better.&nbsp;</p>", "parentCommentId": "R9Lx2iNLbBKLC3sZC", "user": {"username": "quinn"}}, {"_id": "u6SckhWGAtp9Qsekf", "postedAt": "2023-02-27T12:53:27.543Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>This was an interesting video, many important points made in an entertaining manner.</p><p>Though I don't strongly agree with all the points mentioned but I agree that&nbsp;<br>The precipice &gt; what we owe the future&nbsp;<br>(but they are both great books nonetheless)</p>", "parentCommentId": null, "user": {"username": "Joyce Alvino"}}, {"_id": "hvcaBe7vqgMtGcLE6", "postedAt": "2023-02-27T13:57:02.127Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>\"Observation is the chief source of knowledge\" falls under the <a href=\"https://en.wikipedia.org/wiki/Epistemology#Schools_of_thought_in_epistemology\">Empiricism</a> school of thought in epistemology, as opposed to Rationalism, which is perhaps where my misunderstanding came about.<br><br>(A minor gripe I have about LW, and EA by extension, is that words with a specific meaning in philosophy are misused and therefore take on a different meaning \u2013 take \"epistemic status\", which has grown out of its original intended meaning of how confident one is in one's claim and is now used more to describe someone's background and raise general caveats and flags for where someone might have blind spots.)<br><br>In general, I'd agree that using different tools to help you better understand the world and succeed in life is a good thing; however, my point here is that LW and the Rationality community in general view certain tools and ways of looking at the world as \"better\" (or are<a href=\"https://en.wikipedia.org/wiki/Epistemology#Schools_of_thought_in_epistemology\"> only exposed to these tools and ways of looking at the world</a>, and therefore don't come across other methods). I have further thoughts on this that I might write a post about in the future, but in short, I think that this leads to the Rationality community (and EA to some extent) to tend to be biased in certain ways that could be mitigated by increasing recognition of the value of other tools and worldviews, and maybe even some reading of academic philosophy (although I recognise not everyone has the time for this).</p>", "parentCommentId": "5y7rW93ec6fbYEjeS", "user": {"username": "Jessica Wen"}}, {"_id": "aFnwfhNYhfF4gjqD3", "postedAt": "2023-02-27T14:23:05.100Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think the point is that our subculture's \"rationalism\" and a <a href=\"https://en.wikipedia.org/wiki/Rationalism\">historian of philosophy's \"rationalism\"</a> are homonyms.</p>", "parentCommentId": "hvcaBe7vqgMtGcLE6", "user": {"username": "quinn"}}, {"_id": "h6NvRK2NChwv9ALM2", "postedAt": "2023-02-27T14:27:38.968Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think lesswrong and EA are <strong>gluttonous</strong> and <strong>appropriative</strong> and good at looting the useful stuff from a breadth of academic fields, but <a href=\"https://forum.effectivealtruism.org/posts/NzPwFfzJur5bMmHTg/diversity-takes#Take_3__intellectual_homogeneity_is_often_a_feature__not_a_bug\">excluding continental philosophy is a deeply correct move that we have made and will continue to make</a>.&nbsp;</p>", "parentCommentId": "hvcaBe7vqgMtGcLE6", "user": {"username": "quinn"}}, {"_id": "8oukrEmLocnnFo46a", "postedAt": "2023-02-28T02:43:24.391Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<blockquote><p>A minor gripe I have about LW, and EA by extension, is that words with a specific meaning in philosophy are misused and therefore take on a different meaning</p></blockquote><p>The version of \"rationalist\" you're talking about is a common usage, but:</p><ul><li>The <a href=\"https://www.lesswrong.com/posts/DtcbfwSrcewFubjxp/the-rationalists-of-the-1950s-and-before-also-called?commentId=cLcm9qZoxed8QKfvX\">oldest meaning</a> of \"rationalist\" is about truth, science, inquiry, and good epistemics rather than about \"observation matters less than abstract thought\".</li><li>Rationalists' conception of \"rationality\" isn't our invention: we're just using the standard conception from cognitive science.</li><li><a href=\"https://www.lesswrong.com/posts/DtcbfwSrcewFubjxp/the-rationalists-of-the-1950s-and-before-also-called\">Lots of groups</a> have called themselves \"rationalist\" in a LW-like sense prior to LessWrong. It's one of the more common terms humanists, secularists, atheists, materialists, etc. historically used to distinguish themselves from religionists, purveyors of pseudoscience, and the like.</li></ul><p>Also, the rationalist vs. empiricist debate in philosophy is mostly of historical interest; it's not clear to me that it should matter much to non-historians nowadays.</p><blockquote><p>take \"epistemic status\"</p></blockquote><p>\"Epistemic status\" isn't philosophy jargon, is it?</p><p>I took it to be riffing on early LiveJournal posts that began with text like \"status: bored\" or \"current mood: busy\", adding the qualifier \"epistemic\" as a cute variation.</p>", "parentCommentId": "hvcaBe7vqgMtGcLE6", "user": {"username": "RobBensinger"}}, {"_id": "rEcqdLMuFncyv6zWw", "postedAt": "2023-02-28T05:50:30.903Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p><a href=\"https://iep.utm.edu/defeaters-in-epistemology/\">Epistemic status is 100% philosophy jargon</a>. Hell, the word \"epistemic\" or the word \"epistemology\" is itself philosophy jargon. I only ever hear it from LW people/EAs and people in philosophy departments.&nbsp;</p>", "parentCommentId": "8oukrEmLocnnFo46a", "user": {"username": "O Carciente"}}, {"_id": "mdc5Lyok2ofiHZB6H", "postedAt": "2023-02-28T05:51:44.881Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I think you are 100% correct and would be interested in helping you with a post about this if you wanted.</p>", "parentCommentId": "hvcaBe7vqgMtGcLE6", "user": {"username": "O Carciente"}}, {"_id": "QWcqSwvHbTdX6v3kA", "postedAt": "2023-03-02T19:31:56.199Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Language is a mess of  exaptations build upon ever more exaptations until you find a reference to something physical at the bottom of it all. Consider what \"the mouth the river\" means if you believe you live in a world where everything is/has a spirit. Definition discussions are useful for communication, but adjusting definitions to make progress is deeply necessary,  because new ideas need to build upon old foundations and going for some sort of new word creates more confusion than it is worth.</p>\n", "parentCommentId": "hvcaBe7vqgMtGcLE6", "user": {"username": "Jens Brandt"}}, {"_id": "vFfmnEDgGGNDGmnSp", "postedAt": "2023-03-08T00:55:13.556Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>The word \"epistemic\" is philosophy jargon. The phrase \"epistemic status\" in the link you gave isn't a separate piece of jargon, it's just the normal word \"status\" modified by the word \"epistemic\".</p>\n<p>The original comment I was replying to said:</p>\n<p>\"A minor gripe I have about LW, and EA by extension, is that words with a specific meaning in philosophy are misused and therefore take on a different meaning \u2013 take \"epistemic status\", which has grown out of its original intended meaning of how confident one is in one's claim and is now used more to describe someone's background and raise general caveats and flags for where someone might have blind spots.\"</p>\n<p>If the claim is that rationalists are misusing the word \"epistemic\", not some specific unfamiliar-to-me new piece of jargon (\"epistemic status\"), then the claim is based on a misunderstanding of the word \"epistemic\". Epistemic in philosophy means \"pertaining to knowledge (belief justification, reliability, accuracy, reasonableness, warrant, etc.)\", not \"pertaining to confidence level\".</p>\n<p>Someone's \"epistemic status\" includes what they believe and how strongly they believe it, but it also includes anything that's relevant to how justified, reasonable, supported, based-on-reliable-processes, etc. your beliefs are. Like, \"epistemic status: I wrote this whole hungry, which often makes people irritable and causes them to have more brain farts, which reduces the expected reliability and justifiedness of the stuff I wrote\" is totally legit. And if people have the background knowledge to understand why you might want to flag that you were hungry, it's completely fine to write \"epistemic status: written while hungry\" as a shorthand.</p>\n<p>(I do think rationalists sometimes put other stuff under \"epistemic status\" as a joke, but \"rationalists joke too much\" is a different criticism than \"rationalists have their own nonstandard meaning for the word 'epistemic'\".)</p>\n", "parentCommentId": "rEcqdLMuFncyv6zWw", "user": {"username": "RobBensinger"}}, {"_id": "TcdcyzMxaPbCdDFyn", "postedAt": "2023-03-22T13:57:31.256Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>Sort of related to this, I started to design an <a href=\"http://ungglish.loyc.net/\">easier dialect of English</a> because I think English is too hard and that (1) it would be easier to learn it in stages and (2) two people who have learned the easier dialect could speak it among themselves. This would be nice in reverse; I married a Filipino but found it difficult to learn Tagalog because of the lack of available Tagalog courses and the fact that my wife doesn't understand and cannot explain the grammar of her language. I wish I could learn an intentionally-designed pidgeon/simplified version of the language before tackling the whole thing. Hearing the language spoken in the house for several years hasn't helped.</p><p>It would be good for EAs to learn other languages, but it's hard. I studied Spanish in my free time for four years, but I remained <i>terrible</i> at it, my vocabulary is still small and I usually can't understand what Spanish people are saying. If I moved to Mexico I'm sure I would learn better. But I have various reasons not to.</p>", "parentCommentId": "vcbjPmbxCuBpKkeuC", "user": {"username": "dpiepgrass"}}, {"_id": "kdghamCbgLcfPHbDJ", "postedAt": "2023-04-03T12:12:35.924Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<blockquote><p>measurability bias</p></blockquote><p>Side note from main discussion: I really <i>really</i> dislike this phrase. It seems to crop up whenever anyone in the EA/rationality-adjacent space wants to handwave that their pet cause area is underappreciated but can't provide any good reason for the claim - which is exactly what you might imagine a priori that such a phrase should get used for.&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/Sb2JPgpMkXxwZ3g4W/on-philosophy-tube-s-video-on-effective-altruism#EA_and__The_System_\">EA and ~The System~</a> is a perfect case in point. Leftists think EA should aim to change the system to be more left, rightists think EA should change it to be more right, or at least actively resist leftist change, and Scott Alexander <a href=\"https://slatestarcodex.com/2015/09/22/beware-systemic-change/\">observes</a> (correctly, IMO) that 'if everyone gave 10% of their income to effective charity, it would be more than enough to end world poverty, cure several major diseases, and start a cultural and scientific renaissance. If everyone became very interested in systemic change, we would probably have a civil war.'</p><p>Putting the word 'bias' after a concept is not a reasonable way of criticising that concept.</p>", "parentCommentId": null, "user": {"username": "Arepo"}}, {"_id": "wcobefGjcQhX8TTws", "postedAt": "2023-04-23T10:28:56.968Z", "postId": "Sb2JPgpMkXxwZ3g4W", "htmlBody": "<p>I personally don't see how Thorn's analysis of effective altruism was 'fair' when she dedicates nearly a third of the video to longtermism and Sam Bankman-Fried, misleadingly mentions Elon Musk, while not discussing effective altruist animal advocacy or key concepts like the ITN framework and cause neutrality.</p><blockquote><p>She paints EA in a slightly simplistic light (can\u2019t expect much more from a 40-min video on a huge movement that\u2019s over a decade old)</p></blockquote><p>Actually, I do think you could expect a bit more nuance in a video that is nearly half as long as a college lecture. Thorn touches on the (leftist) critique of philanthropy, but doesn't elaborate on it - a missed opportunity in my opinion.</p><blockquote><p>Thorn says: \u201cMacAskill and Ord write a lot about progress and humanity\u2019s potential, but they say almost nothing about who gets to define those concepts. Who gets seen as an expert? Who decides what counts as evidence? Whose vision of the future gets listened to? In my opinion, those aren\u2019t side-questions to hide in the footnotes. They\u2019re core to the whole project.\u201d</p></blockquote><p>I think this was one of the best parts of the video. Unfortunately, again, Thorn doesn't really go much in-depth here. Overall I think this video is not bad but it could have been far better if Thorn had put more effort into research and balanced representation of views and debates.</p>", "parentCommentId": null, "user": {"username": "Maxim Vandaele"}}]