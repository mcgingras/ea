[{"_id": "HYoo94dsMyWD4Jy3F", "title": "William MacAskill - The Daily Show", "postedAt": "2022-09-28T19:31:22.207Z", "htmlBody": "<p>Will appeared on The Daily Show with Trevor Noah yesterday (2022-09-27). &nbsp;This is, to my knowledge, the most prominent US television show to feature an EA guest. &nbsp;He primarily discusses ideas of most impactful charities, billionaire donors, and obligations to future people using the phrase \"effective altruism\" a bunch of times. &nbsp;</p><p>It was neat to see these ideas in a different context and I thought it went really well!</p>", "user": {"username": "Tyner"}}, {"_id": "LjBYatyXkce5EiLDo", "title": "Optimism, AI risk, and EA blind spots", "postedAt": "2022-09-28T17:21:17.347Z", "htmlBody": "<h3>Preface</h3><p>I'm going to start this post with a personal story, in part because people tend to enjoy writing that does that. If you don't want the gossip, just skip section one, the takeaway of which is: \"EA has a strong cultural bias in favor of believing arbitrary problems are solvable\".</p><p>The gossip - and this takeaway - are not the only insight I'm trying to communicate. I don't mean for this post to be a \"community\" post overall, but rather one that is action relevant to doing good on the object level.</p><h2>N=1</h2><p>I had a two week work trial with a prominent EA org. There were some red flags. Nobody would tell me the projected salary, despite the job opportunity taking place across the country and in one of the most expensive cities on Earth. But whatever. I quit my job and flew over.</p><p>It didn't work out. My best guess is that this was for cultural reasons. My would-be manager didn't think I'd been making fast enough progress understanding a technical framework, but the jobs I've had since have involved that framework, and I've received overwhelmingly positive feedback, working on products dramatically more complicated than the job opportunity called for. C'est la vie.</p><p>Much later, I was told some of the things in my file for that organization. I was told by the organization's leader in a totally open way - nothing sneaky or \"here's the dirt\", just some feedback to help me improve. I appreciate this, and welcomed it. But here's the part relevant to the post:</p><p>One of the negative things in my file was that someone had said I was \"a bit of a downer\". Much like with my technical competency, maybe so. But it's worth mentioning that in my day to day life, my coworkers generally think I'm <i>weirdly positive</i>, and often comment that my outlook is shockingly sanguine.</p><p>I believe that both are true. I'm unusually optimistic. But professional EA culture is much, much more so.</p><p>That's not a bad thing (he said, optimistically). But it's also not all good.</p><h2>(Why) is there an optimism bias?</h2><p>If you want to complete an ambitious project, it's extremely useful to presume that (almost) any challenge can be met. This is a big part of being \"agentic\", a much-celebrated and indeed valuable virtue within the EA community. (And also within elite circles more generally.) The high-end professional world has lots of upside opportunities and relatively little downside risk (you will probably always find a pretty great job as a fallback), so it's rational to make lots of bets on long odds and try to find holy grails.</p><p>Therefore, people who are flagged as \"ambitious\", \"impressive\", \"agentic\", will both be selected for and encouraged to further cultivate a mindset where you never say a problem is insurmountable, merely challenging or, if you truly must, \"not a top priority right now\".</p><p>But yeah. No odds are too long to be worth a shot!</p><h2>How is this action relevant?</h2><p>To avoid burying the lede, it's a major part of my reasoning to donate my 10% pledge to the Against Malaria Foundation, rather than x-risk reduction efforts. I'll trace out the argument, then pile on the caveats.</p><p>On the 80,000 Hours Podcast, Will MacAskill put the odds of a misaligned AI takeover around 3%. Many community figures put the odds much higher, but I feel pretty comfortable anchoring on a combination of Will and Katja Grace, who put the odds at 7% that AI destroys the world. Low to mid single digits. Okay.</p><p>So here's a valid argument, given its premises:</p><p><strong>Premise One: </strong>There is at least a 6% chance that AI destroys the world, or removes all humans from it.</p><p><strong>Premise Two: </strong>There exist interventions that can reliably reduce the risk we face by at least 10% (of the risk, not of the total - so 6% would turn into 5.4%, not -4%/0%).</p><p><strong>Premise Three: </strong>We can identify these interventions with at least 10% probability.</p><p><strong>Premise Four: </strong>We can pursue these interventions, and have at least 10% odds of succeeding, provided we've found the right ones.</p><p><strong>Premise Five: </strong>If the world ends, about 8 billion people die.</p><p><strong>Conclusion One: </strong>Pursuing the basic plan entailed in premises 1-4 saves, in expectation, at least 480,000 lives (800,000 * 0.06 * 0.1 * 0.1 * 0.1).&nbsp;</p><p>Let's take that as an anchor point and add two further premises.</p><p><strong>Premise Six: </strong>The (next) best opportunity to save human lives is the Against Malaria Foundation, and saving lives through AMF costs approximately $4,000 per life.</p><p><strong>Premise Seven: </strong>We want to save as many lives as possible in expectation.</p><p><strong>Conclusion Two: </strong>We should pursue AI x-risk mitigation if strategies in line with the above premises cost $1.92B or less.</p><p>This is simplified in many ways. I could see arguments to challenge every premise in every direction. And, of course, longtermist arguments massively change the calculus by counting all future generations on the ledger (and guessing that there's some chance there will be a truly staggering number of such generations).</p><p>We'll come back to longtermism, which deserves its own section. But first let's focus on premises three and four.</p><h3>Are we overoptimistic about key premises?</h3><p>Yes, I think so.</p><blockquote><p><strong>Premise Three: </strong>We can identify these interventions with at least 10% probability.</p></blockquote><p>I won't dispute that there exist interventions that would reduce risk. There exist actions that achieve nearly anything. But can we find them?&nbsp;</p><p>Recall that there are decent reasons to think goal alignment is impossible - in other words, it's not a priori obvious that there's <i>any </i>way to declare a goal and have some other agent pursue that goal exactly as you mean it.</p><p>Recall that engineering ideas very, very rarely work on the first try, and that if we only have one chance at <i>anything</i>,<i> </i>failure is very likely.</p><p>Recall that interventions that slow technological development in the face of strong economic pressures seem extremely hard to find, to the degree that I'm not sure one has ever worked (and if one had, I'd guess it had been in a dictatorship rather than a liberal democracy).</p><p>10% seems significantly too high to me, even to reduce the relative risk by 1%. As it looks to me, there are broad swathes of possible worlds where the problem either basically never comes up or solves itself, and broad swathes of possible worlds where our trajectory is already set in stone and we're going down.</p><p>The presumption that we live in the sweet spot and need merely roll up our sleeves strikes me as an example of powerful optimism bias.</p><blockquote><p><strong>Premise Four: </strong>We can pursue these interventions, and have at least 10% odds of succeeding, provided we've found the right ones.</p></blockquote><p>Recall that getting \"humanity\" to agree on a good spec for ethical behavior is extremely difficult: some places are against gene drives to reduce mosquito populations, for example, despite this saving many lives in expectation.</p><p>Recall that there is a gigantic economic incentive to keep pushing AI capabilities up, and referenda to reduce animal suffering in exchange for more expensive meat tend to fail.</p><p>Recall that we have to implement any solution in a way that appeals to the cultural sensibilities of all major and technically savvy governments on the planet, plus major tech companies, plus, under certain circumstances, idiosyncratic ultra-talented individual hackers.</p><p>The we-only-get-one-shot idea applies on this stage too.</p><p>So again, 10% strikes me as really optimistic. It's worth mentioning here, too, that I don't tend to see these premises valued at 10% in most analyses, or even part of the calculation. Most often it's taken as a given that levers exist to reduce risk by at least 1% (or much more), and that we're competent to push those levers.</p><p>$1.92B to save 480,000 lives in expectation is a great deal. But it seems really, really rosy to think we can accomplish simultaneous extremely difficult and currently poorly specified political, philosophical, and technical tasks on a global scale at that price point. Heck, we've been working on figuring out how good deworming is for a decade. This stuff is <i>hard</i>.&nbsp;</p><p>So, presuming shorttermism, AMF looks like the better option. Of course, we shouldn't presume shorttermism. Let's get into that.</p><h3>The promised longtermism section</h3><p>Thank you for your characteristic patience, longtermists.</p><p>I am going to risk looking stupid here, because longtermist arguments are often strong and generally really complicated. Please presume I know what I'm talking about and will get to relevant objections, then judge me extra if I in fact leave a big one out.</p><p>Here's a longtermist argument as I understand it:</p><p><strong>Premise One: </strong>If you do the math on certain x-risk reduction initiatives with the proposed benefit as being \"save 8 billion lives\", it may or may not be cost effective.</p><p><strong>Premise Two: </strong>However, extinction is a <i>really big deal beyond that</i>, because we lose not only the 8 billion lives, but also however many lives there would have been counterfactually.</p><p>These premises are both totally solid. I am not nearly so arrogant as to pick a fight with the legendary Derek Parfit.</p><p><strong>Premise Three: </strong>In expectation, there are extremely many people in the long-term future, such that we should model x-risk reduction initiatives as saving orders of magnitude more lives than a mere 8 billion.</p><p>I am not as sold on premise three.</p><p><strong>Some basic (and troubling) anchor points</strong></p><p>Recall that until nuclear weapons were developed, humanity had no realistic shot at accidentally messing up the biome we live in so badly that our survival as a species was at risk.</p><p>Recall that bioweapons - including engineered pathogens that could kill almost everyone - are possible and protections against biological attack are spectacularly underfunded (for the record, I am <i>very in favor </i>of improving this state of affairs and think people are doing incredible work here).</p><p><strong>Scenarios if we pass the (potential) AI sieve</strong></p><p>Imagine that we create well-aligned AGI. There are several things this could mean.</p><p><strong>Scenario One: </strong>The AGI does global surveillance good enough to prevent rogue actors from destroying the planet, no matter how powerful technology gets. Everyone's cool with this level of surveillance, and it doesn't cause permanent rule by some bad ideology or other. Also, no cosmic threats happen to manifest.</p><p><strong>Scenario Two: </strong>Same as scenario one, but there's a black hole/alien invasion/unstoppable asteroid/solar flare/some other astronomical event we don't know about yet that unavoidably destroys the planet in the next millennium or two. (I don't think this scenario is likely, but it is possible.)</p><p><strong>Scenario Three: </strong>The AGI causes massive technological progress, but there are actually lots of AGIs basically at once. None of them is trying to kill us all, but none of them is given permission to surveil everyone all the time. We have many more \"make sure the world doesn't get destroyed\" sieves to get through as a species, and over time it gets easier and easier for rogue actors or industrial accidents to kill us all. Eventually, and probably in under 1,000 years, one does.</p><p><strong>Scenario Four: </strong>The AGI causes massive technological progress, but less than we currently imagine a \"singularity\" to look like, due to limits to returns from intelligence far above what our species can reach, but far short of godlike powers. Same problems as scenario three, but slower.</p><p>And many others! The most likely case to me is that if AI x-risk is solved or turns out not to be a serious issue, and we just keep facing x-risks in proportion to how strong our technology gets, forever. Eventually we draw a black ball and all die. If technology keeps improving really fast, that's likely in the next 500 years or so. The second most likely case to me is stuff just gets so weird as to be unrecognizable, but it's not straightforwardly catastrophic.</p><p><strong>Key Longtermist Objection: Use expected value</strong></p><p>Okay, a longtermist might say. Maybe the odds are really slim that we thread this needle, and then also the subsequent needles required to create an interstellar civilization spanning billions of years. But the value of that scenario is so high that if you shut up and multiply, it's worth putting a lot of resources in that direction.</p><p>To which I say... man, I dunno, this starts to feel like Pascal's Mugging. There are a lot of unknown unknowns, interstellar travel seems really hard and like no particular generation has a strong incentive to bear the huge sacrifices to make it happen, and it's just very suspicious to suppose we're at the precipice of the one sieve that matters most and all further ones are comfortably manageable by our descendants. We're just getting into territory that feels roughly analogous to assigning some probability mass to each fundamentalist major religion being true. I can't easily put into words why I don't want to do that, but I really, really don't, and I feel like digging deeper into it will make me less sane rather than more sane.</p><p><strong>End of longtermist section</strong></p><p>So okay, am I anti-longtermism? No, I don't think so. I think Will MacAskill's argument that we're dramatically underfunding the long term future is just straightforwardly right, even if that future only extends 500 years in expectation. On net we should move the needle up.</p><p>But that doesn't mean the most cost effective interventions are longtermist-motivated x-risk reduction, just that longtermist organizations and projects are way better than the baseline in terms of projects that exist and are funded/well regarded.</p><h2>So, scrap AI x-risk projects?</h2><p>No! Actually, this is awkward, because I actually spend a bunch of my own time professionally working on those. I find AI really interesting, and I think that much like with longtermism, more resources should go toward AI safety rather than fewer, and we as a planet should take AI risk more seriously.</p><p>But much like Tyler Cowen's view on EA as a whole, I think something can be overrated locally and (badly) underrated globally. Buy your local AI safety researcher a coffee. Heck, buy your local AI safety research editor (that's me) a coffee, while you're at it.</p><p>But does it beat AMF? Is it clearly the highest value altruistic project available? Is it a slam dunk that reducing x-risk from AI should be our North Star?</p><p>I don't think so. I think the assumptions behind that conclusion are biased severely - maybe irrecoverably - by a culture of intense optimism. When I put on my AI x-risk hat, I myself prefer to have that optimism. Conditional on having selected that as a project to work on, it's probably good.</p><p>But when I'm deciding where to donate, and I do the math, I genuinely think the most effectively altruistic option is just saving little children from malaria.</p>", "user": {"username": "Justis"}}, {"_id": "DEicZi5APTDGjS9py", "title": "AI Safety Endgame Stories", "postedAt": "2022-09-28T17:12:33.005Z", "htmlBody": "", "user": {"username": "IvanVendrov"}}, {"_id": "HsDMguLtdhFP46GQ8", "title": "How Open Source Machine Learning Software Shapes AI", "postedAt": "2022-09-28T17:49:07.469Z", "htmlBody": "<p><strong>Acknowledgements</strong></p><p>We would like to thank many different people for contributing to the research that comprised this post. Tim Hwang and Teddy Collins at CSET for originally sparking this research question, Helen Toner for her early encouragement and feedback on research framing, Tuomas Oikarinen for sharpening our arguments, Tan Zhi Xuan and Serena Booth for providing several helpful resources.</p><h2>Summary&nbsp;</h2><p>This post is authored by Max Langenkamp, who did this research during his MEng at MIT\u2019s <a href=\"https://algorithmicalignment.csail.mit.edu/\">Algorithmic Alignment Group</a>, and by&nbsp;<a href=\"http://people.csail.mit.edu/dhm/\"><u>Dylan Hadfield-Menell</u></a>, associate professor and Max\u2019s supervisor.</p><p>This post is a summary of Max\u2019s Master\u2019s research (<a href=\"https://maxlangenkamp.me/AIES_paper.pdf\"><u>AIES conference paper</u></a>,&nbsp;<a href=\"https://maxlangenkamp.me/how_MLOSS_shapes_AI.pdf\"><u>full thesis</u></a>) which was also done in collaboration with&nbsp;<a href=\"https://www.hbs.edu/faculty/Pages/profile.aspx?facId=1160365\"><u>Daniel Yue</u></a>, a PhD student at Harvard Business School.</p><p>Although it is more object-level than the typical EA Forum post, we believe that this research is of interest to the EA community because (1) it examines an important factor of AI development that we believe can also serve as a lever for AI governance, and (2) it provides qualitative research on AI, which we consider a neglected but important angle for understanding how to wisely govern AI systems.</p><p><i>TL;DR</i></p><p>Machine Learning Open Source Software (MLOSS) is an important, yet neglected factor shaping the trajectory of AI. Our contributions:</p><ol><li>Using qualitative data, we claim that MLOSS shapes AI through standardization, enabling newer and faster forms of experimentation, and by creating online communities.&nbsp;</li><li>We suggest that a dependency graph of technology capabilities (Wardley map) provide a helpful alternative to the canonical AI production function.</li><li>We examine the economic incentives, sociotechnical factors, and ideology shaping MLOSS</li><li>We suggest that the future of MLOSS involves decreased emphasis on deep learning frameworks and increased focus on model and data tooling (probabilistic predictions can be found at the end). At the end, we also briefly examine the risks implied by these trends.</li></ol><p>&nbsp;</p><h3>MLOSS is important<br>&nbsp;</h3><p>This should be obvious to anyone who\u2019s built an ML model in the last decade, but everyone \u2014 whether a research engineer at Deepmind or a high schooler in India \u2014 uses open source software to build models. All of the two dozen ML practitioners we interviewed told us that MLOSS tools were a crucial part of their workflow.</p><p>That everyone uses the same freely available tools implies that these tools play a large role in shaping AI development. Yet few researchers have examined MLOSS\u2019 role in shaping AI development.</p><h3>MLOSS is neglected</h3><p>Hitherto, much of the discussion of the factors that shape AI has focused on the role of computation, with some consideration of the role of algorithms and data. For instance, Allan Dafoe suggests that \u201c[p]lausibly the key inputs to AI are computing power (compute), talent, data, insight, and money.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc1wbrwk9haa\"><sup><a href=\"#fnc1wbrwk9haa\">[1]</a></sup></span>&nbsp;Hwang (2018) examines how the hardware supply chain shapes machine learning development. Rosenfeld (2019) and Hestness (2017) examine how dataset size relates to model accuracy in AI. Both are part of a growing literature that aims to more explicitly model the relationships between inputs and predictive error in AI. To the best of our knowledge, however, there have been no detailed examinations of how MLOSS generally shapes AI development.</p><p>Insofar as we care about how factors such as data or compute shape the trajectory of AI, we should also care about the role of MLOSS.</p><h3>MLOSS and the AI production function<br>&nbsp;</h3><p>One question that appeared early in our research was \u201cwhat is the relationship between the data, compute, MLOSS, and the other factors that shape AI production?\u201d Articulating the relationship between these factors is key to understanding the default trajectory of AI system development.</p><p>&nbsp;</p><p>One common characterization within economics is the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Cobb%E2%80%93Douglas_production_function\"><u>Cobb-Douglas production function</u></a>. It models variables, such as capital and raw materials, that parameterize a function that maps inputs to outputs.</p><p>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995123/mirroredImages/HsDMguLtdhFP46GQ8/wsbeixroklg4ozbvxvp2.png\"></p><p><i>The functional form of the CD production function</i></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995124/mirroredImages/HsDMguLtdhFP46GQ8/mc8izlotpj2tejbnfc3k.png\"></p><p><i>The implied form of the AI production function</i><br>&nbsp;</p><p>Allan Dafoe applies this in the context of AI governance with the \u2018AI production function\u2019. He proposes that this production function depends on \u201ccompute, talent, data, investment, time, and indicators such as prior progress and achievements\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc1wbrwk9haa\"><sup><a href=\"#fnc1wbrwk9haa\">[1]</a></sup></span>. Dafoe\u2019s discussion of \u2018AI progress\u2019, along with discussions found in similar work, attempts to be agnostic to the particular paradigm of AI. In practice, this equates \u2018deep learning\u2019 and \u2018AI\u2019. While this choice can be motivated, it is important to recognize that there are alternative paradigms with different capabilities. For example, probabilistic programs, which can more easily incorporate explicit world knowledge, can depend far less on the availability of large datasets.</p><p>&nbsp;</p><p>While a production function can help to explicitly separate the factors that influence deep-learning development, it also has its limits. In particular, the production function, which is typically conceived of as the product of independent variables, doesn\u2019t account for shared dependencies between factors of production and can hide contextual information about each factor.&nbsp;</p><p>There is another helpful way of articulating the factors that shape AI production. Using an ordered dependency graph of capabilities, also known as a&nbsp;<i>Wardley map</i>, we can account for shared dependencies among factors (e.g. intermediate model representations depend on both compute infrastructure and the MLOSS frameworks).&nbsp;<br>&nbsp;</p><h2>Wardley Maps provide a helpful alternative to the AI production function</h2><p>Wardley maps have been used for everything from&nbsp;<a href=\"https://www.kda.zone/post/would-you-survive-the-loss-of-your-phone\"><u>preparing for survival without a phone</u></a> to&nbsp;<a href=\"https://joapen.com/blog/2022/08/29/how-will-cars-be-refueled-by-2032/\"><u>forecasting the trajectory of electric cars</u></a>. There is also a&nbsp;<a href=\"https://learnwardleymapping.com/book/\"><u>whole book</u></a> on the theory of Wardley maps. Nonetheless, we have provided a simple example below to explore the role of MLOSS in the AI ecosystem.</p><p>There are three main steps in constructing a Wardley map: describing a&nbsp;<strong>use case</strong>,&nbsp;<strong>defining the</strong>&nbsp;<strong>technological capabilities</strong> needed to address the use case, and&nbsp;<strong>ordering the capabilities</strong> on a map.<br>&nbsp;</p><p>Here, the use case is \u2018building deep learning models\u2019. That goes at the top. The capabilities we\u2019ll focus on are frameworks, pretrained models, data, and hardware. These are the primary capabilities. Each capability itself depends on other capabilities. For instance, ML frameworks (e.g. PyTorch) depend on framework compilation software (e.g. Glow compiler), which depends on the intermediate representation (e.g. ONNX), which depends on the hardware (e.g. NVIDIA GPU).</p><p>At this stage, we are not aiming to be comprehensive, but rather to articulate some key capabilities that ML frameworks (one prominent instance of MLOSS) interacts with.&nbsp;</p><p>&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995124/mirroredImages/HsDMguLtdhFP46GQ8/e4duznh8xssfy9wmawle.png\"><br>&nbsp;</p><p><i>Fig 1.</i> <a href=\"https://onlinewardleymaps.com/#IWYMZMdHMpkWz02WaT\">Basic Wardley Map of Deep Learning Models</a>&nbsp;<br>&nbsp;</p><p>The map allows us to more clearly specify the relationship between capabilities shaping deep learning research. It also allows us to reason about which capabilities are likely to become a point of focus in the future. We will discuss this more in the final section, \u201cThe Future of MLOSS\u201d.<br><br>We will use the Wardley Map here for discussion of the future of AI later on.</p><p>&nbsp;</p><h2>MLOSS shapes AI research through standardization, facilitating experimentation, and creating communities<br>&nbsp;</h2><p>After doing qualitative interviews with 23 participants, we identified three main effects that MLOSS has on the AI ecosystem.<br>&nbsp;</p><h3>Standardization</h3><p>By standardization we mean the widespread adoption of a single technology or technique. Participants discussed the impacts of standardization through three mechanisms: the development of standardized model types; coordination on frameworks, and the creation of a common user experience for developers.</p><p>Standardization of model types is most prominent with practitioners that work with large neural networks. A decade ago, it was a substantial engineering feat to work with a 1 million parameter model. Now, however, any researcher with a connection to the internet and the right hardware can freely download&nbsp;<a href=\"https://web.archive.org/web/20220919192829/https://huggingface.co/blog/bloom\"><u>a model with over 170 billion parameters</u></a> or else&nbsp;<a href=\"https://web.archive.org/web/20220919193056/https://huggingface.co/bigscience/bloom\"><u>use it for inference online</u></a>. Consequently, a larger fraction of machine learning work now entails large neural networks. This was made possible by the proliferation of MLOSS tools, as well as the advances in hardware and performance engineering.</p><p>In terms of deep learning frameworks, we also see a high level of standardization: while in 2016 several different frameworks (e.g. MXNet, Theano, TensorFlow, Caffe2, Torch) had large market share, practitioners in the west have collectively settled on PyTorch, JAX, and TensorFlow as the three dominant frameworks. All of our participants used one of PyTorch, JAX, or TensorFlow. As of June 2022, Paperswithcode found that PyTorch, TensorFlow, and JAX accounted for 62%, 7%, and 1% respectively of publicly accessible paper implementations. However, because of endorsements such as DeepMind\u2019s&nbsp;<a href=\"https://web.archive.org/web/20220926192027/https://www.deepmind.com/blog/using-jax-to-accelerate-our-research\"><u>public adoption of JAX</u></a>, we believe that Paperswithcode\u2019s statistic for JAX use does not reflect the increased traction for JAX.</p><p>We can also see convergence at the level of user experience within the frameworks. Several of our interviewees noted that TensorFlow\u2019s previous default graph-based execution was counter-intuitive, which made it harder to learn as a beginner. They explained that this led them to PyTorch, with its more intuitive imperative style model specification. Notably, because of this pressure, TensorFlow 2.0 adopted PyTorch\u2019s interface and now feel very similar to use.<br>&nbsp;</p><h3>Enabling Experimentation<br>&nbsp;</h3><p>Enabling experimentation means not only are ideas faster to materialize, but also provides new ways of thinking about problems. PyTorch Lightning saves a researcher hours of model debugging by providing a&nbsp;<a href=\"https://pytorch-lightning.readthedocs.io/en/latest/debug/debugging_basic.html\"><u>module</u></a> that summarizes matrix weights. However, Torch\u2019s imperative style programming allows a researcher to think in new ways. This means that novel architectures like Tree-LSTMs previously unimaginable with graph-based model specification, became conceivable.<br>&nbsp;</p><h3>Creation of Community</h3><p>One important aspect of MLOSS, similar to OSS ecosystems in general, is that it creates opportunities for technology contributors and users to interact. This&nbsp;<i>community creation</i> has many benefits. It allows users to become contributors, provides a large amount of feedback, naturally creates educational materials, and can provide job opportunities to MLOSS volunteers.&nbsp;</p><p>Users becoming contributors is a more specific instance of how open source software forums can provide a new space for connections to be made. We came across several anecdotes of how major contributors to MLOSS projects began as users, only to later be hired by the organization sponsoring the project. Although we have not explicitly measured it, we have the general sense that several job opportunities are found through engaging with MLOSS forums and communities.</p><p>The online community interacts in complex ways with the goals of the organization sponsoring the project. Soumith Chintala, co-creator of PyTorch, has for instance publicly discussed how closely the public PyTorch community has shaped his early development of the tool. In open source software development more broadly, public consensus is crucial for a successful project; perhaps unsurprisingly, it is very difficult to make an unpopular modification to a project. For a powerful illustration of this dynamic, we invite the reader to examine the&nbsp;<a href=\"https://www.zdnet.com/article/facebook-relicenses-react-in-the-face-of-open-source-dev-backlash/\"><u>unsuccessful attempt</u></a> of Facebook to change the licensing terms of React.<br>&nbsp;</p><h2>Economic Incentives, Sociotechnical Factors, and Ideology shape MLOSS</h2><h3>Funding</h3><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995124/mirroredImages/HsDMguLtdhFP46GQ8/qilequ1eitsqigwldmgx.png\"></p><p><i>Fig 2.</i> A proposal for how business incentives shape MLOSS<br>&nbsp;</p><p>From a combination of case studies and interviewee suggestions, we found that business incentives differ between large companies and startups. Big companies fund MLOSS because it allows for talent acquisition, indirect control over the ecosystem, and enhances existing capabilities. Talent acquisition is obvious; many developers who work on the open source tools were once users. Indirect control over the ecosystem is more difficult to articulate. This can show up in how defaults are set (e.g. what types of hardware PyTorch is compatible with) or more broadly what direction the project is heading in (e.g. whether HuggingFace prioritizes compatibility with Graphcore\u2019s IPUs or Google\u2019s TPUs). Finally, whether through increasing demand for computation (benefiting digital cloud providers) or improving the existing stack, more MLOSS usage often increases the value of existing capabilities. Google\u2019s Colab plausibly has significantly increased adoption because of the widespread use of TensorFlow; Facebook/Meta\u2019s face recognition and image captioning capabilities have significantly improved as a result of sharing PyTorch.<br>&nbsp;</p><p>Startups provide MLOSS for community and to complement existing or future products. The community, while not an end in and of itself, serves as a generally powerful moat that helps exert cultural influence, improve products, provide routes to monetization, and grants&nbsp;<a href=\"https://marksaroufim.substack.com/p/huggingface?s=r\"><u>a sense of collective meaning</u></a>. Hugging Face, which provides the most popular large language models package, is one example of this. After open sourcing powerful and accessible packages for using language models, Hugging Face garnered a dedicated community of followers. In 2022, the&nbsp;<a href=\"https://venturebeat.com/ai/hugging-face-triples-investment-in-open-source-machine-learning-models/#:~:text=the%20company%20has%20been%20cash%2Dpositive%20in%20the%20first%20months%20of%202021\"><u>company became cash flow positive</u></a> by charging to train or perform inference on models, and also by consulting for large enterprise customers. The community has allowed them to hire talent, improve existing products, and find customers for their services.<br>&nbsp;</p><h3>Sociotechnical forces</h3><p>Outside of funding and ideology, there were a number of sociotechnical forces that clearly shaped MLOSS. Below are three of the most prominent.</p><p><strong>1. The most usable software tends to win</strong></p><p>The usability of a tool effectively acts as a selection pressure, favoring the tools with the most intuitive user interface. Notably, PyTorch\u2019s imperative-style specification of neural networks was eventually adopted by TensorFlow. This is despite the initial lack of support for imperative approaches to production systems. The competition between TensorFlow\u2019s graph-based execution and PyTorch\u2019s eager execution recalls the famous Lisp vs C, or \u2018the right thing\u2019 vs \u2018<a href=\"https://web.archive.org/web/20220920224037/https://dreamsongs.com/RiseOfWorseIsBetter.html\"><u>worse is better</u></a>\u2019. Just as the simpler, less complete, \u2018worse\u2019 C programming language prevailed in adoption over the more complicated, complete, \u2018right\u2019 Lisp language, so too did PyTorch\u2019s simpler approach prevail over TensorFlow\u2019s more efficient graph-based model.</p><p><strong>2. Some code wants to be standardized</strong></p><p>Consider two common tasks in machine learning: dataset manipulation and matrix differentiation. Dataset manipulation varies highly between datasets but requires few steps. Matrix differentiation, in contrast, is very similar between models, but requires many steps. The tedium and similarity of matrix differentiation meant that it was one of the first tasks to be standardized in software. In general, tasks that are modular, homogenous, and detailed, will be standardized first.</p><p><strong>3. Research community interest</strong></p><p>MLOSS is also significantly shaped by the prevailing paradigm within machine learning. The predominant model type for machine learning is currently deep learning. This then drives the creation of deep learning (as opposed to alternate ML paradigms such as probabilistic programming or automated planning) MLOSS tools.</p><p>Of course, the prevailing paradigm within machine learning is itself a product of a large number of different factors. The state of hardware, the most prestigious benchmarks, the commercial applicability, are all important in shaping the dominant approach within machine learning. For a thoughtful consideration of this topic, we point the reader to Dotan and Milli (2019).</p><h3>Ideology</h3><p>A final, critical, aspect of MLOSS development is the role of ideology. Most of the key figures in MLOSS are motivated by particular visions of the world. These visions may be religious in nature or the consequence of strongly held values. These values tend to either be about helping improve the experience of other fellow developers, or else furthering the state of AI.</p><p>Travis Oliphant, the creator of NumPy, one of the most commonly used libraries in Python, tells the story of creating NumPy as an act of public service against the wishes of his advisors and peers at Brigham Young University<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9c87t73w60r\"><sup><a href=\"#fn9c87t73w60r\">[2]</a></sup></span>.</p><p>Other developers believe in the pure good of furthering AI. In a podcast interview, when asked for the reason Facebook sponsors PyTorch, Soumith Chintala&nbsp;<a href=\"https://www.youtube.com/watch?v=am895oU6mmY\"><u>explains</u></a> that \u201cwe have a single point agenda at [Facebook AI Research], which is to solve AI\u201d which involves empowering others to work on the problem. The implication seems to be that \u2018solving AI\u2019 would lead to enormous upside for society, whether by allowing new drugs to be discovered or by proving new theorems. Similarly, H2O.ai and Hugging Face both refer to \u2018democratizing AI\u2019 as a central motivation for open sourcing their products.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefheij7p05gs9\"><sup><a href=\"#fnheij7p05gs9\">[3]</a></sup></span></p><p>&nbsp;</p><h2>Self-reinforcing feedback loops in deep learning<br>&nbsp;</h2><p>In this section, we\u2019ll discuss the ways in which the proliferation of MLOSS may have selectively favored one type of machine learning (deep learning) over other types.</p><h3>Alternatives to the Deep Learning Paradigm in AI</h3><p>Beyond deep learning, other paradigms in AI research include probabilistic machine learning<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmbfgju8o7l\"><sup><a href=\"#fnmbfgju8o7l\">[4]</a></sup></span>, rule-based expert systems<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7tpozw6jycb\"><sup><a href=\"#fn7tpozw6jycb\">[5]</a></sup></span>, and automated planning<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaty8om5crfe\"><sup><a href=\"#fnaty8om5crfe\">[6]</a></sup></span>. As some of our interviewees pointed out,&nbsp;<strong>although Deep Learning is the dominant paradigm in ML/AL research, it is hard to disentangle the reasons for deep learning's progress. Clearly, there is some degree of technological advantage, however, there is also substantial&nbsp;engineering advantage as the MLOSS ecosystem has progressed.</strong><i> </i><strong>At best, it is difficult to evaluate the counterfactual of how technology would have progressed if alternative approaches had similar resources. At worst, deep learning has created a self-reinforcing dynamic where the ecosystem effects are more important than the advantages of the underlying technology.</strong></p><p>For one example where deep learning is not clearly superior, consider verifying flight software. The stakes of buggy flight software are very high and require a large degree of certainty. In such domains, engineers opt to use theorem provers, which are closely linked to automated planning, to formally verify that the software is free of bugs. This is a task that deep learning algorithms are currently ill-suited for because they cannot provide formal guarantees.&nbsp;</p><h3>Better support for deep learning tools reinforces deep learning</h3><p>There are two ways that current MLOSS tools favor deep learning: decreasing developer friction and shifting researcher incentives.&nbsp;</p><p>The two most popular open source tools in deep learning and in automated planning are, respectively, <a href=\"https://pytorch.org/\">PyTorch</a> and <a href=\"https://www.fast-downward.org/\">FastDownward</a>. As a tool developed largely by the Facebook AI Research term, PyTorch is incredibly well supported. A user can typically resolve technical issues with a single search engine query, which parses tens of thousands of posts and active users. There are dozens of helpful snippets of code detailing how to, for instance, debug tensors with mismatched dimensions.</p><p>Now consider FastDownward. Installation is non-trivial and requires basic knowledge of operating systems to download a compressed bundle of files (tarball) and to manually configure installation.It\u2019s difficult to get immediate support if a user runs into technical issues: In most of the bug queries we tried, a straightforward search via a search engine did not yield answers, and we had to turn to their custom forum. This is not to disparage FastDownward, but to point out the large differences in user experience that a team of full-time engineers can make.&nbsp;</p><p>Over the course of the interviews, we saw how small differences in the friction surrounding tool use drove the movement to PyTorch over TensorFlow. We believe that ease of tool use also significantly affects the problems that researchers choose to focus on. In this way, the current most popular MLOSS tools (PyTorch, JAX, Hugging Face \u2026) have fostered work on deep learning that is not a direct consequence of the scientific merit of deep learning as an AI paradigm.</p><p>Another way that MLOSS has reinforced deep learning is that, because of the strong ecosystem support, deep learning tools have become productionized, making deep learning engineering a highly desirable skillset. Because deep learning tools are so easy to use, reliable, and supported by a large user community, many companies can use deep learning. Companies have en masse begun to apply deep learning via tools like TensorFlow, which has resulted in many more jobs in industry for experts in deep learning than in other paradigms<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0n6gnofr36p\"><sup><a href=\"#fn0n6gnofr36p\">[7]</a></sup></span>. There is also much higher demand in industry for jobs that involve deep learning. On Indeed.com, there were over 15k jobs including the description \u2018deep learning\u2019. For \u2018probabilistic programming\u2019 and \u2018automated planning\u2019, there were 40 and 8 jobs, respectively<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0pso15x1b0gq\"><sup><a href=\"#fn0pso15x1b0gq\">[8]</a></sup></span>.<br>&nbsp;</p><h2>The Future of MLOSS</h2><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_190 190w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_380 380w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_570 570w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_760 760w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_950 950w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_1140 1140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_1330 1330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_1520 1520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_1710 1710w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/89a6cbb649922ee6b6d7ee0b50fbf61d0f02be40d5cb97dd.png/w_1887 1887w\"></p><p><i>Fig 3.</i> Expanded Wardley map of deep learning models<br>&nbsp;</p><p>The Wardley map, as introduced earlier, can give some intuition for the trajectory of technological development. In general, capabilities to the left (i.e. less developed) become the bottleneck and attention in the ecosystem will turn to addressing these relatively neglected features. This map, when combined with our qualitative evidence, informs these next four trends we identify.<br>&nbsp;</p><h3>Trend 1: Attention shifts away from deep learning frameworks</h3><p>Despite the history of intense competition among deep learning frameworks (PyTorch and TensorFlow, there was Chainer, Theano, Torch; Huawei recently released MindSpore), expressing deep learning models using frameworks is no longer a bottleneck. Soumith Chintala, one of the creators of PyTorch, provides a suggestive quote: \u201cWith PyTorch and TensorFlow, you\u2019ve seen the frameworks sort of converge... the next war is compilers for the frameworks \u2014 XLA, TVM, PyTorch has Glow, a lot of innovation is waiting to happen.\u201d<br>&nbsp;</p><h3>Trend 2: More tooling for large pretrained models</h3><p>Many recent notable projects have iterated on large pretrained models. Github\u2019s Copilot , the large model designed to assist with Python code-writing, was a finetuned version of GPT-3[12]. At the most basic level, this would involve infrastructure for serving pretrained models. The current large language models are too large to run on a single computer, and we already see services like OpenAI\u2019s GPT-3 API and Hugging Face\u2019s serving infrastructure meeting this need. Later tools could address allow for managing different model versions, meta-frameworks for composing large distinct pretrained models, and tools for incorporating different modalities (e.g. vision, sound, text) into pretrained models.<br>&nbsp;</p><h3>Trend 3: Greater variety of (potentially closed-source) data tooling</h3><p>Many startups are trying to address the current ad-hoc nature of working with data, but there has yet to be a consolidation on a single tool. More broadly, with movements like Andrew Ng\u2019s \u2018data-centric AI\u2019, many researchers feel that data has been neglected as a focus for tooling and research. If they are correct, then tools for data inspection and production will become especially important.</p><p>Whether these future tools will be open sourced depends on the scale of the task. Systems that require petabytes of data, such as Tesla\u2019s self-driving car pipeline, are far less likely to be open sourced than the gigabyte-sized experiments at a university or small startup.&nbsp; One plausible scenario is a bifurcation between large and small scale tools. The large scale tools are provided by proprietary platforms like Scale whereas some fraction of the small scale data tools are open source and freely available to researchers.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995123/mirroredImages/HsDMguLtdhFP46GQ8/vvcm41ctxlhvqusmf5zd.png\"><br><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Table 1. </i>Different types of data tools</p><h3>A brief reflection on risks</h3><p>What do trends in data tooling imply for medium term risks from machine learning? It seems that the trends within data tooling, similar to the trends in computation, point towards increasing concentration of capabilities into a small number of firms. This may allow for easier regulation; governments have in the past successfully demonstrated the ability to regulate monopolies emerging from general purpose technologies like electricity. However, it may be much more concerning that these capabilities are developing far more rapidly than our wisdom of how to control our technologies.&nbsp;</p><p>Community norms within MLOSS also have significant bearing on potential risks. Because many communities within MLOSS are quite strong, if an influential community disregards safety concerns about releasing powerful AI systems, we believe that developers of high risk models (e.g. deadly virus generation) are much more likely to release their model publicly.&nbsp; <strong>Since AI systems are often composable, we might expect nonlinear increases in risk with model proliferation. Because a bad actor can combine models from different modalities, doubling the number of openly available models is likely more than doubling the risk of harm. If we take this line of reasoning seriously, it is crucial to foster cautious norms around model release early on.</strong></p><h3><br>Predictions</h3><p>In the service of trying to make our research more accountable, we provide a number of predictions about the future landscape. Each prediction is accompanied with an event probability:&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</p><ol><li>PyTorch, and JAX will be two of top three frameworks for deep learning ac- cording to Paperswithcode (outside of China) as of January 2027. 0.75<br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</li><li>Python will be the most popular language for machine learning in 2027. 0.90<br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</li><li>ONNX will become accepted as the dominant intermediate representation framework. 0.70<br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</li><li>Between 2023-2027, none of the top publicly disclosed 5 largest language models will be open sourced. 0.8<br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</li><li>As of 2027, the three most popular platforms that provide data tooling are largely proprietary/do not open source a crucial part of their stack. 0.7<br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</li></ol><p>These prediction questions are not without ambiguity, and are mostly based on our intuition. Nonetheless, we believe these rough predictions are more useful to provide than omit, so we have included them here.</p><p>&nbsp;&nbsp;&nbsp;</p><h2>Questions for further investigation</h2><ol><li>What properties are shared/different between MLOSS and OSS within other domains (say compilers like LLVM)?&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</li><li>How do we expect the evolution of data tooling to be different from framework tooling?&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</li><li>How different are the capabilities required for scaling up probabilistic programming versus deep learning (specifically compute)?&nbsp;&nbsp;&nbsp;&nbsp;</li><li>How does MLOSS in China differ from those in the U.S? What does this imply about the diffusion of knowledge about AI research?</li><li>How are the incentives for open sourcing data tools different than the incentives for other MLOSS (especially frameworks)?</li></ol><p><br><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc1wbrwk9haa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc1wbrwk9haa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Allan Dafoe. AI Governance: A Research Agenda. University of Oxford, August 2018.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9c87t73w60r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9c87t73w60r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Travis oliphant: NumPy, SciPy, anaconda, python &amp; scientific programming | lex fridman podcast #224 - YouTube https://www.youtube.com/watch?v= gFEE3w7F0ww&amp;t=3089s.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnheij7p05gs9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefheij7p05gs9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For further discussion of the role of ideology in AI, see also How AI Fails Us</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmbfgju8o7l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmbfgju8o7l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452\u2013459, May 2015. Number: 7553 Publisher: Nature Publishing Group.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7tpozw6jycb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7tpozw6jycb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Randall Davis and Jonathan J King. The origin of rule-based systems in AI. Rule-based expert systems : The MYCIN experiments of the Stanford heuristic programming project / Edited by Bruce G. Buchanan and Edward H. Shortliffe, 1984. Place: S.l. Publisher: s.n. OCLC: 848324685</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaty8om5crfe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaty8om5crfe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Richard E. Fikes and Nils J. Nilsson. Strips: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2(3):189\u2013208, December 1971</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0n6gnofr36p\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0n6gnofr36p\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Google AI. Case Studies and Mentions - TensorFlow https://web. archive.org/web/20220228134129/https://www.tensorflow.org/about/ case-studies, 2022</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0pso15x1b0gq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0pso15x1b0gq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Indeed.com. Deep Learning job query. https://www.indeed.com/jobs?q=Deep\\ %20Learning&amp;l&amp;vjk=87d0802a025023b4. Website, 2022.; Indeed.com. Probabilistic programming job query. https://www.indeed.com/ jobs?q=Probabilistic\\%20Programming&amp;l&amp;vjk=4ac08ba6112c10e2. Website, 2022.</p></div></li></ol>", "user": {"username": "Max Langenkamp"}}, {"_id": "2h2E448uqCY6uGbAg", "title": "The missing link to AGI", "postedAt": "2022-09-28T16:37:51.892Z", "htmlBody": "<p>Current AI stuff is OK but we will never get to AGI by making it bigger and better because some important things are missing from its foundation. One of the \u201cfathers\u201d of Deep Learning, Chief AI Scientist at Meta Yann LeCun recently claimed that \"We're not to the point where our intelligent machines have as much common sense as a cat. So, why don't we start there?\"</p><p>My meta research of the history of science in relevant fields shows clearly that in order to reach AGI we need to start from bacteria and a living cell, on one hand, and from theoretical physics and cosmology, on the other.</p><p>Starting with the \u201cfathers\u201d of psychology as a science Edward Thorndike and Ivan Pavlov scientists for more than a century know that there is a basic mechanism of universal learning installed in all living creatures including human beings. For most of the time this mechanism has been neglected by the mainstream of psychologists and neuroscientists as primitive, slow and inefficient. AI inherited that neglectance and magnified it to the extreme.</p><p>However, I have identified some scientists, theories and experiments which have reached such significant advances in the understanding of the universal learning mechanism that, in my opinion, AGI may emerge with high probability within a decade from now on.</p><p>Most probably, it will evolve from something like Xenobots, tiny synthetic creatures made&nbsp; by Michael Levin and his team from single cells of a frog by programming them according to mathematical models based on Karl Friston\u2019s fundamental free energy principle. At a later stage mathematical models of these small but clever minds will be infused into humongous but stupid AI models making them smart and, even more importantly, alive.&nbsp;</p><p>It will be impossible to handle risks arising from the emergence of huge smart living machines after that shift happens. So we need to mitigate those risks at the stage of creation of basic simple minds. The time to do it is now.&nbsp;</p><p>An overwhelming amount of information on this subject including links to the original research is available in the manuscript of my book Learning Infinity: From One to Zero. It is a part of my submission for the prize and is available by following this link. <a href=\"https://docs.google.com/document/d/1kxz_siIZVLjRK6DxxrWDsiRD2bjTua-EOPPgAd-3KTI/edit?usp=sharing\">https://docs.google.com/document/d/1kxz_siIZVLjRK6DxxrWDsiRD2bjTua-EOPPgAd-3KTI/edit?usp=sharing</a>&nbsp;</p><p>#Future Fund worldview prize</p>", "user": {"username": "Yuri Barzov"}}, {"_id": "CgDfPtS2qxKngvYg8", "title": "Will Values and Competition Decouple?", "postedAt": "2022-09-28T16:32:34.352Z", "htmlBody": "<p><em>(Cross-posted from <a href=\"https://www.lesswrong.com/posts/2XLFyhuKP7m4xpgea/will-values-and-competition-decouple-1\">LessWrong</a>.)</em></p>\n<p>There are a great many forces shaping the evolution of the universe. Among them, the values of agents -- systems which attempt to optimize, or steer the future towards certain configurations over others -- seem likely to have a dominant influence on the long-term future. The values of the agents around now have been largely determined by competitive pressures. Many people in the rationalist/EA community seem to take it for granted that this is soon going to change, and we will enter an era in which values and competition are completely decoupled; the values of the beings around at the time of this decoupling will be \"locked in\" and determine the shape of the entire future. I think is it plausible(&gt;30% probability) that they are wrong, and that competition will continue, with at least some strength, indefinitely. If this is true, it has major implications for the likely trajectory of the world and how we should go about influencing the long-term future. In this blog post I'll lay out why I think this and what the implications are.</p>\n<p><em>Epistemic status: not confident that the thesis is correct; I am confident that the community should be allocating more probability mass to this scenario than they currently are. If you like, imagine prepending every statement with \"there is at least a 30% probability that\".</em></p>\n<h2>SUMMARY</h2>\n<ol>\n<li>\n<p>I sketch three possible scenarios for what the value systems of machine intelligences might look like. In two of these scenarios, values and competition are totally decoupled; in the third, they remain partially coupled.</p>\n</li>\n<li>\n<p>I present the most basic arguments for and against the occurrence of decoupling. Briefly, the difficulty of ensuring successor alignment might generate competitive pressure towards value systems that try to accrue power to their successors in a value-agnostic way. I define autopoietic agents, systems which increase the number and influence of systems similar to themself.</p>\n</li>\n<li>\n<p>I survey some more arguments given in the EA/rationalist community for why value/competition decoupling will occur. None of them decisively refute the continuing influence of the competitive pressure outlined in section 2.</p>\n</li>\n<li>\n<p>Discussion of implications</p>\n<ol>\n<li>\n<p>Given that values remain subject to competitive pressures, alignment schemes which plan for an AI to competitively pursue its own autopoiesis while ultimately remaining in the service of human values are doomed to failure. This includes MIRI's CEV and ARC's alignment schemes.</p>\n</li>\n<li>\n<p>On the other hand, this gives us less reason to fear the destruction of all value in the universe, since fanatical <a href=\"https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy\">wrapper minds</a> like paperclip maximizers will be competitively selected against.</p>\n</li>\n</ol>\n</li>\n<li>\n<p>If values and competition remain coupled, it might seem that we can have no influence on the future; I argue instead that competition can continue in a path-dependent manner which we can affect. I discuss two ways we could influence the future: (a) attempting to create good successor AGI, whose flourishing is morally valuable from our perspective, (b) using coordination and limited AI to buy time for (a).</p>\n</li>\n<li>\n<p>Conclusion. In favor of maintaining epistemic equipoise.</p>\n</li>\n<li>\n<p>Appendix. I discuss what sorts of environments select for greater or lesser degrees of value stability, and conjecture that nearness to qualitatively novel boundaries is an important factor.</p>\n</li>\n</ol>\n<h1>1. Machine Intelligence and Value Stability: Three Scenarios</h1>\n<p>It's plausible that, sometime this century, we will see the development of artificial general intelligence, software systems with the same cognitive capabilities as humans. The ability of such systems to copy and improve themselves could lead to a great increase in their numbers, speed, and capability, and ultimately a scenario in which more and more improvement occurs in a shorter and shorter span of time until there is an <a href=\"https://www.cold-takes.com/the-duplicator/\">explosion of growth and change</a> -- a 'singularity'. In the event, the resulting AI systems could be <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">far more powerful than the combined forces of humanity</a>, and their decisions would have a decisive influence on the future of the world and ultimately the universe. Thus, it seems very important to understand what kind of values such systems might have, and how they are likely to develop -- values being defined as the properties of the universe they tend to optimize towards.</p>\n<p>Here are three possible scenarios for future AI values. I believe all are plausible, but the third has been underdiscussed in the rationalist/EA communities.</p>\n<p><strong>Utility maximizer goes <a href=\"https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\">FOOM</a></strong>: The above process of self-improvement is concentrated in the first system to attain human-level intelligence. At some point during this process, internal 'pressures' towards coherence cause the system to become a utility maximizer, and at the same time develop a mature theory of reflective agency. Using this knowledge, the AGI completes the process of self-improvement while maintaining its value system, and thereafter uses its immense cognitive abilities to optimize our future lightcone in accordance with its utility function. <a href=\"https://www.lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals?commentId=xdWq52Xg5yGoxD2dP\">Example.</a></p>\n<p><strong>Value lock-in via perfect delegation</strong>: Here there is still a process of rapidly increasing self-improvement, but spread out over the entire economy rather than concentrated in a single AI. There will be an entire ecosystem of many AI systems designing their superior future successors who in turn design <em>their</em> successors. Values, however, will become unprecedentedly stable: AI systems, freed of the foibles of biology, will be able to design successor systems which perfectly share their values. This means the initial distribution of values across AIs will become fixed and ultimately determine how the universe is optimized. <a href=\"https://rationalaltruist.com/2014/05/14/machine-intelligence-and-capital-accumulation/\">Example.</a></p>\n<p><strong>Continuing Competition</strong>: There is again a process of accelerating change distributed over an economy of virtual agents. However, here it is not assumed that AI systems are necessarily able to create successors with perfect value stability. Instead, values will continue to change over time, being partially determined by the initial distribution of values, but also random drift and competitive forces. <a href=\"http://mason.gmu.edu/~rhanson/uploads.html\">Example.</a></p>\n<p>One central factor distinguishing the third scenario from the first two is <em>value/competition decoupling</em> -- whether or not competitive forces continue to act on the dominant value systems. Whether or not this is true seems like a central factor influencing the expected goodness of the future and how we can influence it. Most alignment researchers seem to explicitly or implicitly assume that value/competition decoupling will occur -- with MIRI favoring the first scenario above and Paul Christiano and other 'prosaic' alignment researchers favoring the second. While there has been some discussion of scenarios with continued coupling, most notably Robin Hanson's ems, I believe their likelihood has been underrated and their likely implications underdiscussed.</p>\n<h1>2.  Basic Arguments for and against Decoupling</h1>\n<p>There are many different arguments and types of evidence that you can bring to bear on the question of whether values and competition will remain coupled. I think of the following as being the most basic arguments for and against the continued influence of competition on values.</p>\n<p><strong>Basic Argument for Continued Coupling</strong>: Values and competition will remain coupled because agents with certain value systems will better be able to compete and gain resources than others. For example, agents that value hard work and competition might succeed better than hedonistic agents.</p>\n<p><strong>Counter-Argument</strong>: Past a certain level of sophistication and self-control, agents will be able to recognize if pursuing their values in the short-term disadvantages them in the long-term. They can then adopt the strategies that a more competitive agent would have used, and spend the acquired resources on their values later.</p>\n<p><strong>Counter-Counter-Argument</strong>: The counter-argument assumes that agents can costlessly ensure that their future self and successors share their values. But different value systems can have an easier or harder time with this -- in particular, agents that tend to value <em>any</em> successors having power needn't worry as much about verifying their successors' value alignment.</p>\n<h2>2.1: Generality of the counter-counter-argument</h2>\n<p>At a high enough level of abstraction, this basic template covers most of the arguments for and against decoupling that I've seen; I think the CCA provides us with reason to think that continued coupling is plausible, but it's far from certain. Stated so simply, however, it might sound nitpicky -- isn't this a rather specific scenario?</p>\n<p>I instead think it's very general, because the problem of designing one's successor is a universally important one. This is clearly true even under the mundane circumstances of biological evolution and human life -- but if a 'singularity' is indeed likely to occur soon, that implies there may be an even larger competitive advantage for agents that are willing to recklessly experiment with new designs for successors.</p>\n<p>'Ensuring successor alignment' can also cover a broader range of scenarios than we would normally think of as 'designing a new successor'. A 'messy' agent like a human might fear that it will experience value drift simply from undergoing novel experiences, so agents that care less about such value drift can go about life more freely. This is actually a factor people worry about in human life -- e.g. people donating money while young because they fear losing the desire to donate, or deeply religious people who fear learning new things because they might disrupt their faith. These sorts of commitments can make it difficult to accumulate power and knowledge.</p>\n<p>Value stability is also important in deciding how broadly and freely to disperse copies of oneself. If you aren't certain that each of the copies will maintain your values, and can't establish strong coordination mechanisms, then you may be reluctant to duplicate yourself recklessly. History is filled with tales of countries whose colonies or mercenaries ultimately broke with them: and yet, some of those colonies have been <a href=\"https://en.wikipedia.org/wiki/United_States\">extremely influential</a>, and thus so have their reckless parent countries. These incentives away from value stability can also apply fractally, increasing the influence obtained by cognitive sub-processes that increase their own influence via reckless actions -- e.g. if people find that bold, risky moves pay off in certain environments, they may be more inclined to take similarly risky moves in the future, including in ways that threaten to change their overall values.</p>\n<p>Overall, I think of the CCA as pointing out a general 'force' pushing agents away from perfect value stability. Much as coherence theorems can be thought of as <a href=\"https://www.lesswrong.com/posts/DkcdXsP56g9kXyBdq/coherence-arguments-imply-a-force-for-goal-directed-behavior\">implying a force pushing towards goal-directed behavior</a>, I think the arguments above imply a force pushing agents away from monomaniacal obsession with value stability.</p>\n<h2>2.2: Autopoiesis</h2>\n<p>Here's another way of framing the discussion. Define the class of <em>autopoietic agents</em> to be beings whose actions increase(in expectation) the number and influence of beings similar to itself in the future.\nAutopoietic agents definitionally increase in power and influence. The definition is behavior; an agent successfully optimizing its successors' influence is autopoietic, but an effective paperclip maximizer could also be autopoietic; for that matter, agents with deontological or other types of value systems could be autopoietic, if their value systems lead to them making decisions that increase their influence on the future. I think autopoiesis is a useful concept to have because it is the agents that are most effectively autopoietic that will ultimately control the future -- basically by definition.</p>\n<p>Different autopoietic agents can have successors that are more or less similar to them; the above arguments re:decoupling suggests that there is a competitive pressure pushing such agents from maximal similarity -- or <em>fidelity</em> -- between themselves and their successors.</p>\n<p>In addition to this pressure, there is another pushing towards <em>greater</em> value stability. This is simply the fact that agents who create beings more similar to them, will have more-similar descendants in the future.</p>\n<p>Taken together, these pressures create an optimal level of value stability that will be selected for. This level probably varies a lot depending on the circumstances -- I discuss some of the factors that might favor a greater or lesser level of stability in an appendix. For the purposes of this post, the important point is that this optimal level is not necessarily the maximum possible</p>\n<p>If this remains the case into the far future, there will be a competitive pressure towards value systems which place a non-maximal value on stability. In particular, this implies decoupling of values and competition will not occur: both directly because of this pressure, and because non-maximal successor fidelity will lead to a proliferation of value systems which can be selected amongst.</p>\n<h1>3. Further Arguments for Decoupling</h1>\n<p>So those are some basic arguments for why values might remain subject to competitive forces. I've collected some other common arguments in favor of decoupling and responses below.</p>\n<h2>3.1: Modular goal architectures<sup class=\"footnote-ref\"><a href=\"#fn-Zodiza8P8Lt4JEsT7-1\" id=\"fnref-Zodiza8P8Lt4JEsT7-1\">[1]</a></sup></h2>\n<p><strong>Argument</strong>: Unlike messy humans, future AI systems will have a modular architecture(\"wrapper mind\") like AIXI in which there is an explicit utility function component separated from world-model and planning components. Value stability under self-modification can easily be achieved by keeping the utility function constant while the world-model and planning components are changed.</p>\n<p><strong>Response</strong>: It is far from certain that powerful AI systems will have this form. Current powerful AI systems are too messy for such a simple approach to successor fidelity; difficulties involving <a href=\"https://www.alignmentforum.org/tag/mesa-optimization\">mesa-optimizers</a>, <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\">ontology identification/ELK</a>, and <a href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\">reward not being the optimization target</a> mean that merely keeping a component of your system labeled 'utility function' constant is not guaranteed to actually preserve your values.</p>\n<h2>3.2: The Orthogonality Thesis</h2>\n<p><strong>Argument</strong>: The <a href=\"https://www.lesswrong.com/tag/orthogonality-thesis\">orthogonality</a> <a href=\"https://arbital.com/p/orthogonality/\">thesis</a> states it's possible to create minds of arbitrary capability levels pursuing arbitrary goals. Thus there exist minds able to succeed at any given level of competition while holding any values.</p>\n<p><strong>Response</strong>: Although there may <em>exist</em> minds holding arbitrary goals able to compete equally well, that does not imply that they are all equally likely to come into existence. In particular agents with some value systems may find it harder to design their successors than others.</p>\n<h2>3.3: Better AI Copying &amp; Surveillance</h2>\n<p><strong>Argument</strong>: Human values are unstable in large part due to foibles of our biology, notably mortality(causing value churn when people holding given values die) and our inability to read others' minds(making it more difficult to ensure others share our values). AI systems will not be constrained in this way and so <a href=\"https://intelligence.org/files/WBE-Superorgs.pdf\">will have a much higher level of value stability</a>.</p>\n<p><strong>Response</strong>: Human value instability is not purely caused by biological quirks. Societies differ in how strongly they attempt to impart their values on their members, e.g. more authoritarian governments attempt to control what their subjects are allowed to say to each other in order to suppress dissent. Despite this, the most powerful human societies of today are not those that most stringently attempt to ensure their own stability, suggesting that their are competitive pressures acting against value stability in humans, not just biological limits. AIs may also have a <em>harder</em> time ensuring value stability in some respects, due to e.g. a much greater ability to alter their mind architecture.</p>\n<h2>3.4: Coherence Theorems</h2>\n<p><strong>Argument</strong>: <a href=\"https://www.lesswrong.com/tag/coherence-arguments\">Coherence theorems</a> dictate that any agent is either an expected utility maximizer(which will theorefore attempt to ensure the value alignment of its successors) or is predictably throwing away resources(and is therefore noncompetitive)</p>\n<p><strong>Response</strong>: Agents that attempt to accrue power to themselves and their successors should not be exploitable if they are competent. An agent that autopoietically values empowering its successors can be seen as maximizing a utility function -- one that assigns higher value to trajectories of the universe where its successors are plentiful and have influence. The arguments given above imply that some utility functions, those that assign value to a broad range of successors, can be more easily maximized than others.</p>\n<h2>3.5: Unipolarity</h2>\n<p><strong>Argument</strong>: The world has seen a trend towards greater centralization of political power. If this trend continues, or if some other force causes the creation of a singleton(e.g. a single AI greatly increasing in power in a FOOM scenario) then the singleton can prevent the occurrence of further competition.</p>\n<p><strong>Response</strong>: Even a singleton will likely still have competitive pressures of some sort acting on its sub-components. A singleton that totally suppresses internal competition faces the risk of <a href=\"https://www.overcomingbias.com/2021/11/will-world-government-rot.html\">rotting</a> and ultimately being destroyed. Furthermore, it's not clear if the creation of a singleton is likely to occur. Scenarios where a single agent suddenly gains massively in power are seen as implausible by many. The historical trend towards political centralization might be a side effect of being confined to the Earth: in a future where our civilization has become interstellar, it may be difficult to enforce governance over breakaways who <a href=\"https://mason.gmu.edu/~rhanson/filluniv.pdf\">expand into space at close to light-speed</a>.</p>\n<h2>3.6: Improved Representations</h2>\n<p>I've never seen anyone explicitly make this argument, but I think it's interesting and might be an underlying driver of intuitions.</p>\n<p><strong>Argument</strong>: Considering past autopoietic systems, we have seen a continual increase in the quality and comprehensiveness of their representations of the world. While simple baceteria can only track chemical gradients in their immediate environment, humans have rich, accurate representations of the world and its workings stretching to the limits of the reachable universe. Past value instability was largely caused by agents making decisions based on faulty or incomplete representations of the world, so we should expect values to stabilize as representations become closer to accurately reflecting the world.</p>\n<p><strong>Response</strong>: Just as there is a competitive pressure away from maximal value stability, there is also one against maximally accurate representations -- basically, if your representation is <em>good enough</em> then you may be put at a competitive disadvantage if you spend time improving it further rather than using it to seize resources. There also may be G\u00f6delian obstacles to having a fully accurate representation of systems as cognitively powerful as one's self, such as one would need to ensure perfect value stability of successors.</p>\n<h1>4. Implications</h1>\n<p>As mentioned above, I'm not certain that values and competition will remain coupled. For the rest of this piece, however, I'm going to assume that they will, and analyze what the implications would be for the likely outcome of AGI development and policy decisions.</p>\n<h2>4.1: Indefinitely Scalable Alignment Schemes</h2>\n<p>One prominent class of alignment schemes proposes that we might achieve <em>competitive, scalable</em> alignment -- that is, we might create agents whose goal is to empower humanity, and which can scale to arbitrarily high capability levels while remaining competitive with arbitrary unaligned AI. In a multipolar singularity, such agents could optimize human values by first undergoing autopoietic expansion to gain control of resources, later using these resources to optimize human values. In strong forms, this doesn't require human-controlled AI to prevent the creation of unaligned AI -- they could fight or negotiate with such AI instead, and(by the competitiveness assumption) should in principle succeed about as well as the unaligned AI. The ELK report mentions <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.3y1okszgtslx\">one such alignment scheme in an appendix</a>, defining a utility function for an AI via an elaborate hypothetical process of delegation. <a href=\"https://intelligence.org/files/CEV.pdf\">CEV</a> is another example of a utility function that we could give to a fixed-goal-optimizing AGI, although MIRI usually envisions a unipolar singularity.</p>\n<p>If values remain subject to competitive pressure indefinitely, this class of schemes cannot work -- at least in their strongest form. This is because such schemes require agents that are capable of maintaining their goal of maximizing human values while undergoing a series of extreme self-modifications, in total representing an amount of change and growth comparable to all that has occurred in Earth's history, all while competing with other equally powerful beings doing the same. Clearly this requires an extreme degree of value stability on the part of the human-values-optimizing AI, so if there is a competitive advantage to agents/sub-processes with more labile value systems, the human-values-optimizing AI has little hope of effectively gaining power while maintaining allegiance to human values.</p>\n<p>So, \"aligning\" AI in this strong sense is more difficult in a world with value/competition coupling. Of course, more limited forms of alignment could still be possible, such as MIRI's <a href=\"https://arbital.com/p/task_agi/\">\"Task AI\"</a> intended to be superintelligent in a particular domain but not more broadly, or <a href=\"https://ai-alignment.com/act-based-agents-8ec926c79e9c\">act-based agents</a> with limited capabilities.</p>\n<h2>4.2: Likelihood of all Value in the Universe being Destroyed</h2>\n<p>Given this difficulty, does continued value/competition coupling imply that all value in the universe(from our perspective) is doomed to be destroyed?</p>\n<p>I don't think this is necessarily the case. While value/competition coupling does make alignment harder, it also makes unaligned AI less bad in expectation. In particular, it means that we are not as likely to create <a href=\"https://www.lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy\">wrapper minds</a> that fanatically re-shape the future according to whatever arbitrary values they are initialized with.</p>\n<p>If future AI systems are not wrapper-mind-like, what sort of motivational system will they have? It's impossible to say in any detail. But if they exist in a world full of continuing competition and value diversification, in some ways resembling the evolutionary process that produced us, I think it's morally reasonable to think of them as somewhat like an alien species. While obviously I wouldn't be happy about humanity being disempowered and replaced by an unknown alien species, in expectation it's better than paperclips. I'd estimate that the value of a future controlled by such an 'alien species' is in expectation 10% as good as one in which humans remain in control. Furthermore, as I'll discuss in the next section, we could improve that number by deliberately creating AIs whose autopoiesis we would regard as valuable.</p>\n<h1>5. Policy</h1>\n<h2>5.1 Possibility of Influencing the Future</h2>\n<p>In a world with continued value/competition coupling, you might wonder whether having a lasting influence on the long-term future is even possible, since competitive forces will push the dominant value system towards whatever is globally optimal anyway.</p>\n<p>However, that <em>some</em> competition persists indefinitely does not imply that there is a single global optimum we are doomed to be sucked into. Most of the competitive landscape faced by future agents consists of other agents: there can be many different stable Nash equilibria. At the extreme, this simply recovers decoupling, but it's also possible for some path-dependence to co-exist with some competition. This is what we've seen historically: we still carry the idiosyncratic genetic legacy and many behavioral traits of organisms from hundreds of millions of years ago, although there has been fairly harsh competition during this entire period.</p>\n<p>The difference between this sort of path-dependence and locked-in value stability is that, while we can anticipate that our descendants will share many features and values inherited from us, we can't predict ahead of time that any particular feature will remain perfectly stable. Compared to aligning a fixed-goal-AGI, this feels like a much more robust way of passing on our values: like valuing people because you think they are intrinsically good, VS. valuing a sociopath who you have trained or incentivized to pursue what you regard as good.</p>\n<p>One way of thinking about the future in non-decoupled worlds is as a continuation of regular history, just at a faster tempo. When thinking about the singularity, there is a tendency to see it, in <a href=\"https://www.overcomingbias.com/2010/06/near-far-summary.html\">far mode</a>, as a simple process that will produce a simple outcome, e.g. a utility-maximizing AGI. It might be better to think of it as a vast stretch of time, full of all the complications and twists of regular history, that happens to be compressed into a smaller number of cycles around the Sun than usual. Designing our AGI successors in such a world is similar to passing on control to our children: we can't anticipate every possible future challenge they will face, but what we can hope to do is pass on our values and knowledge, to give them the best shot possible at navigating whatever future challenges come up, including the challenges of managing future competition and value drift. The big difference is that we can't rely on biology to pass on our implict values as we usually do: instead we will need to figure out what sorts of AGIs we can create that we would be happy to see flourishing on their own terms: a <a href=\"https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable\">good successor AI</a>, rather than an aligned one.</p>\n<h2>5.2 Good Successors</h2>\n<p>So how <em>could</em> we create a good successor AI? Are there any such things?</p>\n<p>One example of AIs that would count as good successors: <a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\">ems</a>. Creating a society of highly-accurate human brain emulations would constitute a good successor AI, since they would by definition share human values, and would be in a far better position than baseline humans to navigate the singularity, due to their ability to rapidly copy and alter themselves.<sup class=\"footnote-ref\"><a href=\"#fn-Zodiza8P8Lt4JEsT7-2\" id=\"fnref-Zodiza8P8Lt4JEsT7-2\">[2]</a></sup> Unfortunately it doesn't seem likely that we're going to be able to make ems before the advent of human-level AI.</p>\n<p>As an alternative, we could instead create AI that is similar <em>enough</em> to the brain that it retains moral value from our perspective. There are lots of features of human brains that are pretty idiosyncratic to our biology and we would be fine with losing; on a larger scale, I suspect most mammal species would produce a civilization we would regard as morally valuable, if upgraded in intelligence and uploaded. The big question is how complex are the features of human/mammal brain that are most important for being morally valuable.</p>\n<p>There are currently a few research agendas attempting to reverse-engineer how human values actually work on a neurological level, for instance Steve Byrnes' <a href=\"https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8\">model of brain-like AGI</a> and <a href=\"https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview\">Shard Theory</a>. Optimistically, if they succeed and find that our value system is algorithmically simple, creating good successor AI might be as simple as copying that algorithm to silicon.<sup class=\"footnote-ref\"><a href=\"#fn-Zodiza8P8Lt4JEsT7-3\" id=\"fnref-Zodiza8P8Lt4JEsT7-3\">[3]</a></sup></p>\n<p><a href=\"https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable\">This earlier-linked post by Paul</a> contains another proposal for how we might create good successor AI, by simulating alien evolution(and presenting the aliens with a recursive copy of the same scenario). This seems like it might be difficult to pull off in full detail before HLAI arrives, but less ambitious versions of the same proposal could still be a useful tool in obtaining a good successor AI. \"Sympathy with other value systems\" also might be a key desideratum for any potential good successor.</p>\n<h2>5.3 Delay</h2>\n<p>In worlds where competition continues to influence values, our main route for affecting the singularity and beyond is developing good successor AI. But this doesn't mean that direct research on such AI is the only worthwhile thing we can do -- we can also extend the time which we have for deliberation by delaying AGI deployment. A lot of this depends on the details of geopolitical policy and is beyond the scope of this essay, so my remarks here will be somewhat brief.</p>\n<p>Coordination is obviously crucial. Developing better, more rigorous versions of arguments for AI risk could be quite helpful here, as could spreading awareness of existing arguments among influential people and the broader public.</p>\n<p>Limited AI systems could also be helpful. The above-mentioned Task AGI, or act-based agents, could be deployed to detect and counteract the emergence of unaligned general AGI. Such systems could also be useful for consuming the 'free energy'(h/t Paul) that an unaligned AI would use to expand, such as by running ML models designed to find and patch holes in computer security.</p>\n<p>If value/competition coupling continues to hold, then there is a limit to how long we can delay without incurring a competitive disadvantage or rotting. The optimal amount of time to delay will depend on the details of the geopolitical situation and AI development, and will likely have to be worked out as we go.</p>\n<h1>6. Conclusion</h1>\n<p>In closing, I again emphasize that I am not <em>certain</em> that value/competition coupling will continue. However, reflecting on all the arguments and evidence above, my overall feeling is that it is (at least) comparably likely to the alternatives. In some ways the picture of the singularity thus painted might seem a bit less urgent than the typical arguments suggest: it is harder for us to permanently lock in our current values, but also less likely that all value(from our perspective) will be permanently destroyed. The stakes are only mildly less apocalyptic, however -- it is still the case that a massive rupture in the normal line of succession may be coming soon, with little time for us to prepare.</p>\n<p>In the face of such an event, urgency is appropriate. Urgency is not all that is needed, though -- what is equally important is epistemic equipoise, the ability to carefully track what you do know and what you don't. Maintaining this equipoise is likely to be a necessity if we are to navigate the <a href=\"https://www.cold-takes.com/most-important-century/\">most important century</a> successfully. My hope is that by bringing attention to some neglected arguments, this essay can help the rationalist/EA community track more possible futures and be ready for whatever may happen.</p>\n<p><em>( The time spent writing this post was sponsored by the FTX Future Fund regranting program. Thanks to Simeon Campos for discussion and encouragement and Justis Mills from the LW team for help with editing)</em></p>\n<h1>Appendix: Value Stability and Boundaries</h1>\n<p><em>Epistemic status: pure, unbridled speculation</em></p>\n<p>The <em>optimal level of value stability</em> plays a crucial role in the analysis above. What features of the environment and agents affect this optimal level? I conjecture <em>being near a complex or novel boundary, either in physical or conceptual space, makes the optimal level of value stability lower; being far from complex, novel boundaries makes the optimal level higher</em>.</p>\n<p>By \"being near a boundary\" I mean having access to relatively unclaimed/virgin/unoptimized resources. In physical space this would be gaining access to some previously unoccupied area of space; for example a spacefaring civilization expanding into untouched solar systems. In conceptual space this is coming up with a novel class of useful ideas, for instance new processor designs or neural net architectures. By \"qualitatively novel boundary\" I mean a boundary that is not just adjacent to new resources/ideas, but resources/ideas configured in a different way from previous boundaries that the agents in question have encountered.</p>\n<p>When near a boundary, fresh resources are plentiful, so agents there can, <em>on average</em>, gain in power/number of descendants. In places far from boundaries, where there is a fixed supply of resources, the average increase in power/descendants of a population of agents must be equal to one. Hence, agents near boundaries have more to gain from reckless expansion. Agents which quickly grab a lot of the new resources are selected for.</p>\n<p>\"Qualitatively novel\" boundaries provide an additional pressure away from value stability in that their novelty makes it difficult to rigorously verify the behaviour of successors across them. A completely new class of mind architecture might promise great gains in capability, but make proving alignment harder. It may be harder for successors to coordinate in totally uncharted &amp; unknown territory.</p>\n<p>The property of \"being a novel boundary\" is not binary. The physical and conceptual landscapes are fractal, containing nested sub-divisions with their own boundaries. Agents will differ in what they consider to be 'uncharted territory' -- territory that has only been lightly exploited by one class of agents might appear optimal for expansion to a more sophisticated class. It seems plausible that the future will contain enough novel boundaries in conceptual and physical space to incentivize non-maximal value stability for a long subjective time.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-Zodiza8P8Lt4JEsT7-1\" class=\"footnote-item\"><p>TBF I'm not sure if I've seen anyone make this exact argument, at least in such a simple-minded way; nevertheless I think it's an important background driver of intuitions so I'm including it <a href=\"#fnref-Zodiza8P8Lt4JEsT7-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Zodiza8P8Lt4JEsT7-2\" class=\"footnote-item\"><p>You might dispute that since ems share human values, they are in fact aligned with humanity, not just good successors. Here by aligned I mean \"aligned with their human operators\", so a society of ems would not qualify if they decided to pursue their own interests rather than those of their operators. <a href=\"#fnref-Zodiza8P8Lt4JEsT7-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Zodiza8P8Lt4JEsT7-3\" class=\"footnote-item\"><p>This is not to say that either research agenda is only useful for creating good successor AI -- the same insights could be useful for creating 'traditional' aligned AI as well. <a href=\"#fnref-Zodiza8P8Lt4JEsT7-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "interstice"}}, {"_id": "tcs98gDYdGwMKgKZh", "title": "Reacting to failure: how funders can encourage more ambitious projects", "postedAt": "2022-09-28T16:37:46.556Z", "htmlBody": "<p><i>Tl:dr; someone close to me did something ambitious and made mistakes. The feedback they got was significantly to harsh: not because it is false, but because it discourages trying again.</i></p><p>This summer I had one of my most disheartening experiences with the EA community and I think this is really important criticism. So, if you are a grant-maker or know someone who is, please let them know: how you react after a high-risk, high-reward project panned out not to work, will determine how likely it is the people involved will try again. Fact is, we are lacking ambitious entrepreneurs who can start large projects. Any action that could lead to less of those should be handled with care and the feedback from funders after a project is one of them.</p><p>I won't give more details and the person in question has really good support, so I think this specific instance is fine. But if the pattern of response remains the same, I can imagine many highly promising people being burned or becoming disillusioned. Be empathetic, try to model how it must feel to try something big for the first time - especially if the grant recipients are young - and then react accordingly. Criticism is important, but the wording is too. And if something really was harmful, then at least have the courtesy to wait a while till emotions have calmed down and the people involved had time to process what they did on their own.</p>", "user": {"username": "blob"}}, {"_id": "WDih4fmisdmFRMALs", "title": "The sense of a start", "postedAt": "2022-09-28T13:37:44.782Z", "htmlBody": "<p><i>Some&nbsp;attempts to feel the size of the future:</i><br>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://chronozoom.com/\"><u>ChronoZoom</u></a><u>:</u></p><h2><a href=\"http://gleech.org/img/humans.gif\">[woosh]</a></h2><p>&nbsp;</p><p>Will Macaskill:</p><blockquote><p>Imagine living, in order of birth, through the life of every human being who has ever lived... Your life lasts for almost four trillion years in total. For a tenth of that time, you\u2019re a hunter-gatherer, and for 60 percent you\u2019re an agriculturalist. You spend a full 20 percent of your life raising children, a further 20 percent farming, and almost 2 percent taking part in religious rituals. For over 1 percent of your life you are afflicted with malaria or smallpox. You spend 1.5 billion years having sex and 250 million giving birth. You drink forty-four trillion cups of coffee. As a colonizer, you invade new lands; as the colonized, you suffer your lands taken from you. You feel the rage of the abuser and the pain of the abused. For about 10 percent of your life you are a slaveholder; for about the same length of time, you are enslaved.</p></blockquote><blockquote><p>But now imagine that you live all future lives, too. Your life, we hope, would be just beginning. Even if humanity lasts only as long as the typical mammalian species (one million years), and even if the world population falls to a tenth of its current size, 99.5 percent of your life would still be ahead of you. On the scale of a typical human life, you in the present would be just five months old. And if humanity survived longer than a typical mammalian species\u2014for the hundreds of millions of years remaining until the earth is no longer habitable, or the tens of trillions remaining until the last stars burn out \u2014 your four trillion years of life would be like the first blinking seconds out of the womb.</p></blockquote><p>&nbsp;</p><p><a href=\"http://palaeos.com/time/cosmic_calendar.html\">Carl Sagan</a>:</p><blockquote><p>If history were a football field, all of human history would occupy an area the size of my hand.</p></blockquote><p>&nbsp;</p><p><br><a href=\"https://books.google.cz/books?id=p3KYDwAAQBAJ&amp;pg=PA2&amp;lpg=PA2&amp;dq=%22There+are+always+some+human+beings+who+live+to+be+a+hundred%22&amp;source=bl&amp;ots=ldw5sncNWN&amp;sig=ACfU3U2WzXuQdF1Y5fxrD9t3TsHeH_HXNA&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjvkY6juLf6AhXES_EDHfyCCSEQ6AF6BAgEEAM#v=onepage&amp;q=%22There%20are%20always%20some%20human%20beings%20who%20live%20to%20be%20a%20hundred%22&amp;f=false\">Bryan Magee</a>:</p><blockquote><p>There are always some human beings who live to be a hundred. More do so today than ever before, but there have always been some... It comes as a shock to realise that the whole of civilisation has occurred within the successive lifetimes of sixty people -- which is the number of friends I squeeze into my living room when I have a drinks party. Twenty people take us back to Jesus, twenty-one to Julius Caesar.</p></blockquote><p>&nbsp;</p><p><a href=\"https://www.wired.com/2013/11/touch-generations-and-1000-years/\">Samuel Arbesman</a>:</p><ol><li>\"Me (b. 1981)</li><li>Harold C. Urey, American chemist, Nobel Prize laureate (b. 1893)</li><li>Lucius Quintus Cincinnatus Lamar, U.S. Supreme Court justice (b. 1825)</li><li>William Waldegrave, 1st Baron Radstock, Governor of Newfoundland (b. 1753)</li><li>George Berkeley, Irish philosopher (b. 1685)</li><li>King Charles II of England, Scotland and Ireland (b. 1630)</li><li>Johannes Kepler, German astronomer (b. 1571)</li><li>Shimazu Takahisa, Japanese samurai and warlord (b. 1514)</li><li>Donato Bramante, Italian architect (b. 1444)</li><li>Leonardo Bruni, Italian humanist (b. 1374)</li><li>Petrarch, Italian poet (b. 1304)</li><li>Emperor Go-Fukakusa of Japan (b. 1243)</li><li>Hubert de Burgh, 1st Earl of Kent (b. circa 1160)</li><li>Eric Jedvardsson, king of Sweden since 1156 (b. c. 1120)</li><li>Gerard Thom (The Blessed Gerard), founder of the Knights Hospitaller (b. c. 1040)</li><li>King Duncan I of Scotland (b. 1001)\"</li></ol><p>&nbsp;</p><p><a href=\"https://www.goodreads.com/book/show/51025650-on-time-and-water\">Andri Sn\u00e6r Magnason</a>:</p><blockquote><p>262 years. That\u2019s the length of time you connect across. You\u2019ll know the people who span this time. Your time is the time of the people you know and love, the time that molds you, and your time is the time of the people you will know and love, the time that you will shape. You can touch 262 years with your bare hands. Your great grandma taught you, you will teach your great granddaughter, you can have a direct impact on the future right up to the year 2186.</p></blockquote><p>&nbsp;</p><p><a href=\"https://www.washingtonpost.com/history/2020/07/27/slave-son-racism-george-floyd/\">Sydney Trent</a>:</p><blockquote><p>The whipping post. The lynching tree. The wagon wheel. They were the stories of slavery, an inheritance of fear and dread, passed down from father to son. The boy, barely 5, would listen, awed, as his father spoke of life in Virginia, where he had been born into bondage on a plantation during the Civil War and suffered as a child laborer afterward. &lt;br&gt;&lt;br&gt; As unlikely as it might seem, that boy, Daniel Smith, is still alive at 88, a member of an almost vanished demographic: The child of someone once considered a piece of property instead of a human being.</p></blockquote><p><br><a href=\"https://www.dailykos.com/stories/2007/03/09/310038/-Science-Friday-Sixty-Men-from-Ur\">Mark Sumner</a>:</p><blockquote><p>Arthur M. Schlesinger, Jr. died in 2007. &nbsp;He saw the Internet rise to become a new means of communication and worked alongside his son as a blogger. &nbsp;He saw the Soviet Union collapse. &nbsp;He saw the folly of Vietnam... &nbsp;He saw the end of World War II and the beginning of Soviet domination of Eastern Europe. &nbsp;He saw television begin... He saw the Great Depression. &nbsp;He saw the dust bowl, and the migration of workers to the west. &nbsp;He was two when... the 19th amendment -- women's suffrage -- was passed, millions died in a worldwide flu epidemic, and the first radio station went on the air... William Frederick Cody, also know as \"Buffalo Bill,\" died.</p></blockquote><blockquote><p>William Cody died in a country that had forty eight states, a telephone system, and nearly ten million Ford Model T's... At the turn of the twentieth century, he was the most recognized celebrity on the globe. &nbsp;He toured London and met the Pope... &nbsp;He cheered the news of the first plane and incorporated the Spanish-American War into his show... &nbsp;The first skyscraper -- ten stories tall -- was built in Chicago when Cody was forty. &nbsp;He read in the newspaper of the first light bulb... service in the Pony Express. &nbsp;He was born into a United States that had only twenty nine states, and where half those states supported the owning of slaves. &nbsp; In the year of his birth... thousands died in Irish Potato Famine while thousands more fled to America, the first woman doctor in the United States was awarded her degree, and John Quincy Adams, sixth president of the United States, died...&nbsp;</p></blockquote><blockquote><p>It was Adams who represented the interest of Africans aboard the Spanish slave ship, Amistad. &nbsp;A few years before that another ship, the H. M. S. Beagle, returned home from a long, fruitful voyage. The first train service began in England. &nbsp;The Cree were removed from their lands in Georgia and forced west into Indian Territory, despite Adam's efforts in nullifying an abusive treaty... He was born in the colony of Massachusetts in the same year that Daniel Boone first crossed through the Cumberland Gap to reach Kentucky.<br><br>Arthur M. Schlesinger, Jr., one the United States' great historians, is less than two lifetimes removed from a world where the United States did not exist. &nbsp;Through Mr. Schlesinger, you're no more than three away yourself. &nbsp;That's how short the history of our nation really is. &nbsp;<br><br>Not impressed? &nbsp;It's only two more life spans to William Shakespeare. &nbsp; Two more beyond that, and the only Europeans to see America are those who sailed from Greenland. &nbsp;You're ten lifetimes from the occupation of Damietta during the fifth crusade. &nbsp;Twenty from the founding of Great Zimbabwe and the Visigoth sack of Rome. &nbsp;Make it forty, and Theseus, king of Athens, is held captive on Crete by King Minos, the Olmecs are building the first cities in Mexico, and the New Kingdom collapses in Egypt. &nbsp;<br><br>Sixty life times ago, a man named Abram left Ur of the Chaldees and took his family into Canaan... A few lifetimes before that, and you've come out the bottom of that dime. &nbsp;You're that close to it.</p></blockquote><p>&nbsp;</p><p><a href=\"http://web.archive.org/web/20220928130448/http://208.106.253.109/essays/the-next-one-thousand-years-of-christianity.aspx\">Kevin Kelly</a>:</p><blockquote><p>I recently constructed such a personal generational chain back 1,000 years. I searched for a notable person who died shortly after I was born. A few minutes on Wikipedia turned up the explorer Sven Hedin. I then found a notable person who was born shortly before Hedin died. And then someone born before he died, and so on. With little effort I can arrange a chain of only 13 people that will reach back 1,000 years...</p></blockquote><blockquote><p>If anyone takes a generational concern for the future of mankind, it should be Christians. If not the followers of Jesus, then who will contemplate that place we are headed? It is written in Joel 1:3: \u201cTell your children of it, and let your children tell their children, and their children to another generation.\u201d The future always begins right now.</p></blockquote><p><br><br><a href=\"https://www.ams.org/notices/199701/comm-rota.pdf\">Giancarlo Rota</a>:</p><blockquote><p>Running overtime is the one unforgivable error a lecturer can make. After fifty minutes (one microcentury as von Neumann used to say) everybody's attention will turn elsewhere even if we are trying to prove the Riemann hypothesis. One minute overtime can destroy the best of lectures.</p></blockquote><p>&nbsp;</p><p><a href=\"https://www.wired.com/2013/08/one-billion-seconds/\">Gigasecond</a>:</p><blockquote><p>A billion seconds sounds like a long time: After all, a billion is a pretty big number. But as we can see, it's actually quite manageable, depending on how you look at it. And this is only one of the many quantities we are confronted with from our universe that can be viewed on the human scale, from the size of a neutron star (it could fit comfortable in the Boston metro area) to the frequency of supernovas in our galaxy (about once every hundred years). These are intersections of the cosmic with the human...</p></blockquote><h2><br>See also</h2><ul><li><a href=\"https://en.wikipedia.org/wiki/Big_History\">Big History</a></li><li><a href=\"https://kottke.org/tag/The%20Great%20Span\">The Great Span</a></li></ul>", "user": {"username": "technicalities"}}, {"_id": "i9RJjun327SnT3vW8", "title": "Reasoning Transparency", "postedAt": "2022-09-28T12:22:00.465Z", "htmlBody": "<p><i>I think \u201creasoning transparency\u201d (and/or&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\"><i><u>epistemic legibility</u></i></a><i>) is a key value of effective altruism.&nbsp;</i></p><p><i>As far as I can tell, the key piece of writing about it is&nbsp;</i><a href=\"https://www.openphilanthropy.org/research/reasoning-transparency/\"><i><u>this Open Philanthropy blog post by Luke Muehlhauser</u></i></a><i>, which I\u2019m cross-posting it to the Forum with permission. We also have&nbsp;</i><a href=\"https://forum.effectivealtruism.org/topics/reasoning-transparency\"><i><u>a topic page for \"reasoning transparency\"</u></i></a><i> \u2014 you can see some related posts there.</i></p><hr><p><i>Published: December 01, 2017 | by&nbsp;</i><a href=\"https://www.openphilanthropy.org/about/team/luke-muehlhauser\"><i><u>Luke Muehlhauser</u></i></a></p><p>We at the Open Philanthropy Project value analyses which exhibit strong \u201creasoning transparency.\u201d This document explains what we mean by \u201creasoning transparency,\u201d and provides some tips for how to efficiently write documents that exhibit greater reasoning transparency than is standard in many domains.</p><p>In short, our top recommendations are to:</p><ul><li>Open with a linked summary of key takeaways. [<a href=\"https://forum.effectivealtruism.org/posts/i9RJjun327SnT3vW8/reasoning-transparency#3_1_Open_with_a_linked_summary_of_key_takeaways\"><u>more</u></a>]</li><li>Throughout a document, indicate which considerations are most important to your key takeaways. [<a href=\"https://forum.effectivealtruism.org/posts/i9RJjun327SnT3vW8/reasoning-transparency#3_2_Indicate_which_considerations_are_most_important\"><u>more</u></a>]</li><li>Throughout a document, indicate how confident you are in major claims, and what support you have for them. [<a href=\"https://forum.effectivealtruism.org/posts/i9RJjun327SnT3vW8/reasoning-transparency#3_3_Indicate_how_confident_you_are_in_major_claims__and_what_support_you_have_for_them\"><u>more</u></a>]</li></ul><h1><strong>1 Motivation</strong></h1><p>When reading an analysis \u2014 e.g. a scientific paper, or some other collection of arguments and evidence for some conclusions \u2014 we want to know: \u201cHow should I update my view in response to this?\u201d In particular, we want to know things like:</p><ul><li>Has the author presented a fair or biased presentation of evidence and arguments on this topic?</li><li>How much expertise does the author have in this area?</li><li>How trustworthy is the author in general? What are their biases and conflicts of interest?</li><li>What was the research process that led to this analysis? What shortcuts were taken?</li><li>What rough level of confidence does the author have in each of their substantive claims?</li><li>What support does the author think they have for each of their substantive claims?</li><li>What does the author think are the most important takeaways, and what could change the author\u2019s mind about those takeaways?</li><li>If the analysis includes some data analysis, how were the data collected, which analyses were done, and can I access the data myself?</li></ul><p>Many scientific communication norms are aimed at making it easier for a reader to answer questions like these, e.g. norms for \u2018related literature\u2019 sections and \u2018methods\u2019 sections, open data and code, reporting standards, pre-registration, conflict of interest statements, and so on.</p><p>In other ways, typical scientific communication norms lack some aspects of reasoning transparency that we value. For example, many scientific papers say little about roughly how confident the authors are in different claims throughout the paper, or they might cite a series of papers (or even entire books!) in support of specific claims without giving any page numbers.</p><p>Below, I (Luke Muehlhauser) offer some tips for how to write analyses that (I suspect, and in my experience) make it easier for the reader to answer the question, \u201cHow should I update my views in response to this?\u201d</p><h1><strong>2 Example of GiveWell charity reviews</strong></h1><p>I\u2019ll use a GiveWell charity review to illustrate a relatively \u201cextreme\u201d model of reasoning transparency, one that is probably more costly than it\u2019s worth for most analysts. Later, I\u2019ll give some tips for how to improve an analysis\u2019 reasoning transparency without paying as high a cost for it as GiveWell does.</p><p>Consider GiveWell\u2019s&nbsp;<a href=\"https://www.givewell.org/charities/against-malaria-foundation\"><u>review of Against Malaria Foundation</u></a> (AMF). This review\u2026</p><ul><li>\u2026includes a summary of the most important points of the review, each linked to a longer section that elaborates those points and the evidence for them in some detail.</li><li>\u2026provides detailed responses to major questions that bear on the likely cost-effectiveness of marginal donations to AMF, e.g. \u201cAre LLINs targeted at people who do not already have them?\u201d, \u201cDo LLINs reach intended destinations?\u201d, \u201cIs there room for more funding?\u201d, and \u201cHow generally effective is AMF as an organization?\u201d</li><li>\u2026provides a summary of the research process GiveWell used to evaluate AMF\u2019s cost-effectiveness.</li><li>\u2026provides an endnote, link to another section or page, or other indication of reasoning/sources for nearly every substantive claim. There are 125 endnotes, and in general, the endnote provides the support for the corresponding claim, e.g. a quote from a scientific paper, or a link to a series of calculations in a spreadsheet, or a quote from a written summary of an interview with an expert. (There are some claims that do not have such support, but these still tend to clearly signal what the basis for the claim is; e.g. \u201cGiven that countries and other funders have some discretion over how funds will be used, it is likely that some portion of AMF\u2019s funding has displaced other funding into other malaria interventions and into other uses.\u201d)</li><li>\u2026provides a comprehensive table of sources, including archived copies of most sources in case some of the original links break at some point.</li><li>\u2026includes a list of remaining open questions about AMF\u2019s likely cost-effectiveness, plus comments throughout the report on which claims about AMF GiveWell is more or less confident in, and why.</li><li>\u2026links to a separate summary of the scientific evidence for the effectiveness of the intervention AMF performs, namely the&nbsp;<a href=\"https://www.givewell.org/international/technical/programs/insecticide-treated-nets\"><u>mass distribution of long-lasting insecticide-treated nets</u></a> (LLINs), which itself exhibits all the features listed above.</li><li>\u2026plus much more</li></ul><h1><strong>3 Most important recommendations</strong></h1><p>Most analysts and publishers, including the Open Philanthropy Project,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref18su08wsc42\"><sup><a href=\"#fn18su08wsc42\">[1]</a></sup></span>&nbsp;don\u2019t (and shouldn\u2019t) invest as much effort as GiveWell does to achieve improved reasoning transparency. How can you improve an analysis\u2019 reasoning transparency cheaply and efficiently? Below are some tips, with examples in the text and in footnotes.</p><h2><strong>3.1 Open with a linked summary of key takeaways</strong></h2><p>Many GiveWell and Open Philanthropy Project analyses open with a summary of key takeaways, with links to later sections that elaborate and argue for each of those key takeaways at greater length (examples in footnote<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7ib3ch830l5\"><sup><a href=\"#fn7ib3ch830l5\">[2]</a></sup></span>). This makes it easy for the reader to understand the key takeaways and examine particular takeaways in more depth.</p><h2><strong>3.2 Indicate which considerations are most important</strong></h2><p>Which arguments or pieces of evidence are most critical for your key takeaways? Ideally, this should be made clear early in the document, or at least early in the section discussing each key takeaway.</p><p>Some of my earlier Open Philanthropy Project reports don\u2019t do this well. E.g. my&nbsp;<a href=\"https://www.openphilanthropy.org/research/the-carbs-obesity-hypothesis/\"><u>carbs-obesity report</u></a> doesn\u2019t make it clear that the evidence from randomized controlled trials (RCTs) played the largest role in my overall conclusions.</p><p>Some examples that do a better job of this include:</p><ol><li>My report on behavioral treatments for insomnia&nbsp;<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/#my-overall-tentative-conclusion\"><u>makes clear</u></a> that my conclusions are based almost entirely on RCT evidence, and in particular on (1) the inconclusive or small results from RCTs that \u201ctested [behavioral treatments] against a neutral control at \u22651mo follow-up using objective measures of [total sleep time or sleep efficiency],\u201d (2) the apparent lack of any \u201chigh-quality, highly pragmatic\u201d RCTs on the question, and (3) my general reasons for distrusting self-report measures of sleep quality.</li><li>After surveying a huge variety of evidence, my report on consciousness and moral patienthood provides an 8-point&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/#high-level-summary\"><u>high-level summary</u></a> that makes it (somewhat) clear how I\u2019ve integrated those diverse types of evidence into an overall conclusion, and which kinds of evidence play which roles in that conclusion.</li><li>The introduction of Holden\u2019s blog post on&nbsp;<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>worldview diversification</u></a> makes it clear which factors seem to be most important in the case one could make for or against using a worldview diversification strategy, and the rest of the post elaborates each of those points.</li><li>Holden\u2019s&nbsp;<a href=\"https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/\"><u>Potential Risks from Advanced Artificial Intelligence: The Philanthropic Opportunity</u></a> does the same.</li></ol><h2><strong>3.3 Indicate how confident you are in major claims, and what support you have for them</strong></h2><p>For most substantive claims, or at least for \u201cmajor\u201d claims that are especially critical for your conclusions, try to give some indication of how confident you are in each claim, and what support you think you have for it.</p><h3><strong>3.3.1 Expressing degrees of confidence</strong></h3><p>Confidence in a claim can be expressed roughly using words such as \u201cplausible,\u201d \u201clikely,\u201d \u201cunlikely,\u201d \u201cvery likely,\u201d and so on. When it\u2019s worth the effort, in some cases you might want to express your confidence as a probability or a confidence interval, in part because terms like \u201cplausible\u201d can be&nbsp;<a href=\"https://en.wikipedia.org/wiki/Words_of_estimative_probability\"><u>interpreted differently by different readers</u></a>.</p><p>Below are examples that illustrate the diversity of options available for expressing degrees of confidence with varying precision and varying amounts of effort:<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7u1myvd4tzt\"><sup><a href=\"#fn7u1myvd4tzt\">[3]</a></sup></span></p><ol><li>\u201cI think there is a nontrivial likelihood (at least 10% with moderate robustness, and at least 1% with high robustness) of transformative AI within the next 20 years\u201d (<a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/\"><u>source</u></a>). This is a key premise in the case for making&nbsp;<a href=\"https://www.openphilanthropy.org/focus/potential-risks-advanced-ai/\"><u>potential risks from advanced AI</u></a> an Open Philanthropy Project priority, so we thought hard about how confident we should be that transformative AI would be created in the next couple decades, and decided to state our confidence using probabilities.</li><li>In a&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/#my-current-probabilities\"><u>table</u></a>, I reported specific probabilities that various species (e.g. cows, chickens) are phenomenally conscious, because such estimates were a major goal of the investigation. However, I also made clear that my probabilities are hard to justify and may be unstable, using nearby statements such as \u201cI don\u2019t have much reason to believe my judgments about consciousness are well-calibrated\u201d and \u201cI have limited introspective access to the reasons why my brain has produced these probabilities rather than others\u201d and \u201cThere are many different kinds of uncertainty, and I\u2019m not sure how to act given uncertainty of this kind.\u201d (The last of these is followed by a footnote with links to sources explaining what I mean by \u201cmany kinds of uncertainty.\u201d)</li><li>\u201c\u2026my own 70% confidence interval for \u2018years to [high-level machine intelligence]\u2019 is something like 10\u2013120 years, though that estimate is unstable and uncertain\u201d (<a href=\"https://www.openphilanthropy.org/research/what-do-we-know-about-ai-timelines/\"><u>source</u></a>). This is the conclusion to a report about AI timelines, so it seemed worthwhile to conclude the report with a probabilistic statement about my forecast \u2014 in this case, in terms of a 70% confidence interval \u2014 but I also indicate that this estimate is \u201cunstable and uncertain\u201d (with a footnote explaining what I mean by that).</li><li>\u201cIt is widely believed, and seems likely, that regular, high-quality sleep is important for personal performance and well-being, as well as for public safety and other important outcomes\u201d (<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>). This claim was not a major focus of the report, so I simply said \u201cseems likely,\u201d to indicate that I think the probability of my statement being true is &gt;50%, while also indicating that I haven\u2019t investigated the evidence in detail and haven\u2019t tried to acquire a more precise probabilistic estimate.</li><li>\u201cCBT-I appears to be the most commonly-discussed [behavioral treatment for insomnia] in the research literature, and is plausibly the most common [behavioral treatment for insomnia] in clinical practice\u201d (<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>). I probably could have done 1-3 hours of research and become more confident about whether CBT-I is the most common behavioral treatment for insomnia in clinical practice, but this claim wasn\u2019t central to my report, so instead I just reported my rough impression after reading some literature, and thus reported my level of confidence as \u201cplausible.\u201d</li><li>\u201cThe rise of bioethics seems to be a case study in the transfer of authority over a domain (medical ethics) from one group (doctors) to another (bioethicists), in large part due to the first group\u2019s relative neglect of that domain\u201d (<a href=\"https://www.openphilanthropy.org/research/some-case-studies-in-early-field-growth/\"><u>source</u></a>). Here, I use the phrase \u201cseems to be\u201d to indicate that I\u2019m fairly uncertain even about this major takeaway, and the context makes clear that this uncertainty is (at least in part) due to the fact that my study of the history of bioethics was fairly quick and shallow.</li><li>In my report on&nbsp;<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>behavioral treatments for insomnia</u></a>, I expressed some key claims in colloquial terms in the main text, but in a footnote provided a precise, probabilistic statement of my claim. For example, I claimed in the main text that \u201cI found ~70 such [systematic reviews], and I think this is a fairly complete list,\u201d and my footnote stated: \u201cTo be more precise: I\u2019m 70% confident there are fewer than 5 [systematic reviews] on this topic, that I did not find, published online before October 2015, which include at least 5 [randomized controlled trials] testing the effectiveness of one or more treatments for insomnia.\u201d</li><li>\u201cI have raised my best estimate of the chance of a really big storm, like the storied one of 1859, from 0.33% to 0.70% per decade. And I have expanded my 95% confidence interval for this estimate from 0.0\u20134.0% to 0.0\u201311.6% per decade\u201d (<a href=\"https://www.openphilanthropy.org/research/updating-my-risk-estimate-for-the-geomagnetic-big-one/\"><u>source</u></a>). In this case, the author (David Roodman) expressed his confidences precisely, since his estimates are the output of a statistical model.</li></ol><h3><strong>3.3.2 Indicating kinds of support</strong></h3><p>Given limited resources, you cannot systematically and carefully examine every argument and piece of evidence relevant to every claim in your analysis. Nor can you explain in detail what kind(s) of support you think you have for every claim you make. Nevertheless, you can quickly give the reader some indication of what kind(s) of support you have for different claims, and you can explain in relatively more detail the kind(s) of support you think you have for some key claims.</p><p>Here are some different kinds of support you might have for a claim:</p><ul><li>another detailed analysis you wrote</li><li>careful examination of one or more studies you feel qualified to assess</li><li>careful examination of one or more studies you feel only weakly able to assess</li><li>shallow skimming of one or more studies you feel qualified to assess</li><li>shallow skimming of one or more studies you feel only weakly able to assess</li><li>verifiable facts you can easily provide sources for</li><li>verifiable facts you can\u2019t easily provide sources for</li><li>expert opinion you feel comfortable assessing</li><li>expert opinion you can\u2019t can\u2019t easily assess</li><li>a vague impression you have based on reading various sources, or talking to various experts, or something else</li><li>a general intuition you have about how the world works</li><li>a simple argument that seems robust to you</li><li>a simple argument that seems questionable to you</li><li>a complex argument that nevertheless seems strong to you</li><li>a complex argument that seems questionable to you</li><li>the claim seems to follow logically from other supported claims plus general background knowledge</li><li>a source you can\u2019t remember, except that you remember thinking at the time it was a trustworthy source, and you think it would be easy to verify the claim if one tried<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvlq1ka946dj\"><sup><a href=\"#fnvlq1ka946dj\">[4]</a></sup></span></li><li>a combination of any of the above</li></ul><p>Below, I give a series of examples for how to indicate different kinds of support for different claims, and I comment briefly on each of them.</p><p>Here\u2019s the first example:</p><blockquote><p>\u201cAs stated above, my view is based on a large number of undocumented conversations, such that I don\u2019t think it is realistic to aim for being highly convincing in this post. Instead, I have attempted to lay out the general structure of the inputs into my thinking. For further clarification, I will now briefly go over which parts of my argument I believe are well-supported and/or should be uncontroversial, vs. which parts rely crucially on information I haven\u2019t been able to fully share\u2026\u201d [<a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/\"><u>source</u></a>]</p></blockquote><p>In some cases, you can\u2019t provide much of the reasoning for your view, and it\u2019s most transparent to simply say so.</p><p>Next example:</p><blockquote><p>\u201cIt is widely believed, and seems likely, that regular, high-quality sleep is important for personal performance and well-being, as well as for public safety and other important outcomes.\u201d [<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>]</p></blockquote><p>This claim isn\u2019t the focus of the report, so I didn\u2019t review the literature on the topic, and my phrasing makes it clear that I believe this claim merely because it is \u201cwidely believed\u201d and \u201cseems likely,\u201d not because I have carefully reviewed the relevant literature.</p><blockquote><p>\u201c[CBT-I] can be delivered on an individual basis or in a group setting, via self-help (with or without phone support), and via computerized delivery (with or without phone support).\u201d [<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>]</p></blockquote><p>This claim is easily verifiable, e.g. by Googling \u201ccomputerized CBT-I\u201d or \u201cself-help CBT-I,\u201d so I didn\u2019t bother to explain what kind of support I have for it \u2014 other than to say, in a footnote, \u201cI found many or several RCTs testing each of these types of CBT-I.\u201d</p><blockquote><p>\u201cPSG is widely considered the \u2018gold standard\u2019 measure of sleep, but it has several disadvantages. It is expensive, complicated to interpret, requires some adaptation by the patient (people aren\u2019t used to sleeping with wires attached to them), and is usually (but not always) administered at a sleep lab rather than at home.\u201d [<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>]</p></blockquote><p>These claims are substantive but&nbsp;<i>non-controversial</i>, so as support I merely quote (in a footnote) some example narrative reviews which back up my claims.</p><blockquote><p>\u201cSupposedly (I haven\u2019t checked), [actigraphy] correlates well with PSG on at least two key variables\u2026\u201d [<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>]</p></blockquote><p>Here, I quote from narrative reviews in a footnote, but because the \u201ccorrelates well with\u201d claim is more substantive and potentially questionable/controversial than the earlier claims about PSG, I flag both my uncertainty, and the fact that I haven\u2019t checked the primary studies, by saying \u201cSupposedly (I haven\u2019t checked)\u2026\u201d This is a good example of a phrasing that helps improve a document\u2019s reasoning transparency, but would rarely be found in e.g. a scientific paper.</p><blockquote><p>\u201cthese seven trials were only moderately pragmatic in design.\u201d [<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>]</p></blockquote><p>To elaborate on this claim, in a footnote I make a prediction about the results of a very specific test for \u201cpragmaticness.\u201d The footnote both makes it clear what I mean by \u201cmoderately pragmatic in design,\u201d and provides a way for someone to check whether my statement about the studies is accurate. Again, the idea isn\u2019t that the reader can assume my predictions are well-calibrated, but rather that I\u2019m being clear about what I\u2019m claiming and what kind of support I think I have for it. (In this case, my support is that I skimmed the papers and I came away with an intuition that they wouldn\u2019t score very highly on a certain test of study pragmaticness.)</p><p>Also, I couldn\u2019t find anything succinct that clearly explained what I meant by \u201cpragmatic,\u201d and the concept of pragmaticness was fairly important to my overall conclusions in the report, so I took the time to write a&nbsp;<a href=\"https://www.openphilanthropy.org/explanatory-and-pragmatic-research/\"><u>separate page</u></a> on that concept, and linked to that page from this report.</p><blockquote><p>\u201cI have very little sense of how much these things would cost. My guess is that if a better measure of sleep (of the sort I described) can be developed, it could be developed for $2M-$20M. I would guess that the \u201crelatively small\u201d RCTs I suggested might cost $1M-$5M each, whereas I would guess that a large, pragmatic RCT of the sort I described could cost $20M-$50M. But these numbers are just pulled from vague memories of conversations I\u2019ve had with people about how much certain kinds of product development and RCT implementation cost, and my estimates could easily be off by a large factor, and maybe even an order of magnitude.\u201d [<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>]</p></blockquote><p>Here, I\u2019m very clear that I have no basis whatsoever for the cost estimates I provide.</p><blockquote><p>\u201cboth&nbsp;<i>a priori</i> reasoning about self-report measures and empirical reviews of the accuracy of self-report measures (across multiple domains) lead me to be suspicious of self-reported measures of sleep.\u201d [<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>source</u></a>]</p></blockquote><p>In this case, I hadn\u2019t finished my review of the literature on self-report measures, and I didn\u2019t have time to be highly transparent about the reasoning behind my skepticism of the accuracy of self-report measures, so in a footnote I simply said: \u201cI am still researching the accuracy of self-report measures across multiple domains, and might or might not produce a separate report on the topic. In the meantime, I only have time to point to some of the sources that have informed my preliminary judgments on this question, without further comment or argument at this time. [Long list of sources.] Please keep in mind that this is only a preliminary list of sources: I have not evaluated any of them closely, they may be unrepresentative of the literature on self-report as a whole, and I can imagine having a different impression of the typical accuracy of self-report measures if and when I complete my report on the accuracy of self-report measures. My uncertainty about the eventual outcome that investigation is accounted for in the predictions I have made in other footnotes in this report\u2026\u201d This is another example of a footnote that improves the reasoning transparency of the report, but is a paragraph you\u2019d be unlikely to read in a journal article.</p><blockquote><p>\u201cI will, for this report, make four key assumptions about the nature of consciousness. It is beyond the scope of this report to survey and engage with the arguments for or against these assumptions; instead, I merely report what my assumptions are, and provide links to the relevant scholarly debates. My purpose here isn\u2019t to contribute to these debates, but merely to explain \u2018where I\u2019m coming from.\u2019\u201d [<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>source</u></a>]</p></blockquote><p>Here, I\u2019m admitting that I didn\u2019t take the time to explain the support I think I have for some key assumptions of the report.</p><blockquote><p>\u201cAs far as we know, the vast majority of human cognitive processing is unconscious, including a large amount of fairly complex, \u2018sophisticated\u2019 processing. This suggests that consciousness is the result of some particular kinds of information processing, not just any information processing. Assuming a relatively complex account of consciousness, I find it intuitively hard to imagine how (e.g.) the 302 neurons of&nbsp;<i>C. elegans</i> could support cognitive algorithms which instantiate consciousness. However, it is more intuitive to me that the ~100,000 neurons of the Gazami crab might support cognitive algorithms which instantiate consciousness. But I can also imagine it being the case that not even a chimpanzee happens to have the right organization of cognitive processing to have conscious experiences.\u201d [<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>source</u></a>]</p></blockquote><p>Here, I make it clear that my claims are merely intuitions.</p><blockquote><p>\u201cBy the time I began this investigation, I had already found persuasive my four key assumptions about the nature of consciousness: physicalism, functionalism, illusionism, and fuzziness. During this investigation I studied the arguments for and against these views more deeply than I had in the past, and came away more convinced of them than I was before. Perhaps that is because the arguments for these views are stronger than the arguments against them, or perhaps it is because I am roughly just as subject to confirmation bias as nearly all people seem to be (including those who, like me, know about confirmation bias and actively try to mitigate it). In any case: as you consider how to update your own views based on this report, keep in mind that I began this investigation as a physicalist functionalist illusionist who thought consciousness was likely a very fuzzy concept.\u201d [<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>source</u></a>]</p></blockquote><p>This example makes it clear that the reader shouldn\u2019t conclude that my investigation&nbsp;<i>led</i> me to these four assumptions, but instead that I already had those assumptions before I began.</p><blockquote><p>\u201cOur understanding is that it is not clearly possible to create an advanced artificial intelligence agent that avoids all challenges of this sort. [footnote:] Our reasoning behind this judgment cannot be easily summarized, and is based on reading about the problem and having many informal conversations. Bostrom\u2019s&nbsp;<i>Superintelligence</i> discusses many possible strategies for solving this problem, but identifies substantial potential challenges for essentially all of them, and the interested reader could read the book for more evidence on this point.\u201d [<a href=\"https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence/\"><u>source</u></a>]</p></blockquote><p>This is another example of simply saying \u201cit would be too costly to summarize our reasoning behind this judgment, which is based on many hours of reading about the topic and discussing it with others.\u201d</p><p>Often, a good way to be transparent about the kind of support you think you have for a claim is to summarize the research process that led to the conclusion. Examples:</p><ol><li>\u201cHere is a table showing how the animals I ranked compare on these factors (according to my own quick, non-expert judgments)\u2026 But let me be clear about my process: I did not decide on some particular combination rule for these four factors, assign values to each factor for each species, and then compute a resulting probability of consciousness for each taxon. Instead, I used my intuitions to generate my probabilities, then reflected on what factors seemed to be affecting my intuitive probabilities, and then filled out this table\u201d (<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>source</u></a>).</li><li>\u201cI spent less than one hour on this rapid review. Given this limitation, I looked only for systematic reviews released by the Cochrane Collaboration\u2026, a good source of reliably high-quality systematic reviews of intervention effectiveness evidence. I also conducted a few Google Scholar keyword searches to see whether I could find compelling articles challenging the Cochrane reviews\u2019 conclusions, but I did not quickly find any such articles\u201d (<a href=\"https://www.openphilanthropy.org/research/evidence-for-common-oral-hygiene-practices/\"><u>source</u></a>).</li><li>\u201cI did not conduct any literature searches to produce this report. I have been following the small field of HLMI forecasting closely since 2011, and I felt comfortable that I already knew where to find most of the best recent HLMI forecasting work\u201d (<a href=\"https://www.openphilanthropy.org/research/what-do-we-know-about-ai-timelines/\"><u>source</u></a>).</li><li>\u201cTo investigate the nature of past AI predictions and cycles of optimism and pessimism in the history of the field, I read or skim-read several histories of AI and tracked down the original sources for many published AI predictions so I could read them in context. I also considered how I might have responded to hype or pessimism/criticism about AI at various times in its history, if I had been around at the time and had been trying to make my own predictions about the future of AI\u2026 I can\u2019t easily summarize all the evidence I encountered that left me with these impressions, but I have tried to collect many of the important quotes and other data below\u201d (<a href=\"https://www.openphilanthropy.org/research/what-should-we-learn-from-past-ai-forecasts/\"><u>source</u></a>).</li><li>\u201cTo find potential case studies on philanthropic field-building, I surveyed our earlier work on the history of philanthropy, skimmed through the many additional case studies collected in&nbsp;<i>The Almanac of American Philanthropy</i>, asked staff for additional suggestions, and drew upon my own knowledge of the history of some fields. My choices about which case studies to look at more closely were based mostly on some combination of (1) the apparent similarity of the case study to our mid-2016 perception of the state of the nascent field of research addressing potential risks from advanced AI (the current focus area of ours where the relevant fields seem most nascent, and where we\u2019re most likely to apply lessons from this investigation in the short term), and (2) the apparent availability and helpfulness of sources covering the history of the case study. I read and/or skimmed the sources listed in the annotated bibliography below, taking notes as I went. I then wrote up my impressions (based on these notes) of how the relevant fields developed, what role (if any) philanthropy seemed to play, and anything else I found interesting. After a fairly thorough look at bioethics, I did quicker and more impressionistic investigations and write-ups on a number of other fields\u201d (<a href=\"https://www.openphilanthropy.org/research/some-case-studies-in-early-field-growth/\"><u>source</u></a>).</li></ol><p>For further examples, see the longer research process explanations in&nbsp;<a href=\"https://www.openphilanthropy.org/research/reasonable-doubt-a-new-look-at-whether-prison-growth-cuts-crime/\"><u>David Roodman\u2019s series on the effects of incarceration on crime</u></a>; my reports on&nbsp;<a href=\"https://www.openphilanthropy.org/research/the-carbs-obesity-hypothesis/\"><u>the carbs-obesity hypothesis</u></a> and&nbsp;<a href=\"https://www.openphilanthropy.org/research/behavioral-treatments-for-insomnia/\"><u>behavioral treatments for insomnia</u></a>; the \u201cour process\u201d sections of most Open Philanthropy Project&nbsp;<a href=\"https://www.openphilanthropy.org/research/?content-type=cause-investigations&amp;view-list=true#categories\"><u>cause reports</u></a> and GiveWell&nbsp;<a href=\"https://www.givewell.org/charities/top-charities\"><u>top charity reviews</u></a> and&nbsp;<a href=\"https://www.givewell.org/research/intervention-reports\"><u>intervention reports</u></a>; and a variety of other reports.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref243gzqrkl7l\"><sup><a href=\"#fn243gzqrkl7l\">[5]</a></sup></span></p><h1><strong>4 Secondary recommendations</strong></h1><h2><strong>4.1 Provide quotes and page numbers when possible</strong></h2><p>When citing some support for a claim, provide a page number if possible. Even better if you can directly quote the most relevant passage, so the reader doesn\u2019t need to track down the source to get a sense of what kind of support for the claim the source provides.</p><p>Especially if your report is published online, there are essentially no space constraints barring you from including dozens or hundreds of potentially lengthy quotes from primary sources in footnotes and elsewhere. E.g. see the many long quotes in the footnotes of my&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>report</u></a> on consciousness and moral patienthood, or the dozens of quotes in the footnotes of GiveWell&nbsp;<a href=\"https://www.givewell.org/research/intervention-reports\"><u>intervention reports</u></a>.</p><h2><strong>4.2 Provide data and code when possible</strong></h2><p>Both GiveWell and Open Philanthropy Project provide underlying data, calculations, and code when possible.</p><p>In some cases these supplementary materials are fairly large, as with the 800mb of data and code for David Roodman\u2019s&nbsp;<a href=\"https://www.openphilanthropy.org/research/reasonable-doubt-a-new-look-at-whether-prison-growth-cuts-crime/\"><u>investigation</u></a> into the impact of incarceration on crime, or the 11-sheet&nbsp;<a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models\"><u>cost-effectiveness models</u></a> (<a href=\"https://docs.google.com/spreadsheets/d/1TB0YD2yRvzPel9-CJ77gjvb_nW2lq3tJQQfmJQLIbAA/edit#gid=1537947274\"><u>v4</u></a>) for GiveWell\u2019s top charities.</p><p>In other cases they can be quite small, for example a 27-item&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1zaV3fK4yPkjMKWCeiyZSRwljYMUGksl7ENq6o0J69Uo/edit#gid=0\"><u>spreadsheet</u></a> of case studies I considered examining for&nbsp;<a href=\"https://www.openphilanthropy.org/research/some-case-studies-in-early-field-growth/\"><u>Some Case Studies in Early Field Growth</u></a>, or a 14-item&nbsp;<a href=\"https://docs.google.com/spreadsheets/d/1KeoKVYspULcMXtfoNCaedkATc1wmHlqFyz5Nt0BZ4BQ/edit#gid=1273928110\"><u>spreadsheet</u></a> of global catastrophic risks.</p><h2><strong>4.3 Provide archived copies of sources when possible</strong></h2><p>Both GiveWell and Open Philanthropy Project provide archived copies of sources when possible,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefeb4ki6k0g5\"><sup><a href=\"#fneb4ki6k0g5\">[6]</a></sup></span>&nbsp;since web links can break over time.</p><h2><strong>4.4 Provide transcripts or summaries of conversations when possible</strong></h2><p>For many investigations, interviews with domain experts will be a key source of information alongside written materials. Hence when possible, it will help improve the reasoning transparency of a report if those conversations (or at least the most important ones) can be made available to the reader, either as a transcript or a summary.</p><p>But in many cases this is too time-costly to be worth doing, and in many cases a domain expert will only be willing to speak frankly with you anonymously, or will only be willing to be quoted/cited on particular points.</p><p><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn18su08wsc42\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref18su08wsc42\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For more on openness and transparency at the Open Philanthropy Project, see&nbsp;<a href=\"https://www.openphilanthropy.org/grantmaking-process/\"><u>What \u201cOpen\u201d Means to Us</u></a>&nbsp;and&nbsp;<a href=\"https://www.openphilanthropy.org/research/update-on-how-were-thinking-about-openness-and-information-sharing/\"><u>Update on How We\u2019re Thinking about Openness and Information Sharing</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7ib3ch830l5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7ib3ch830l5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Or, if the summary itself doesn\u2019t link to elaborations on the key takeaways/arguments, then the summary is immediately followed by a linked table of contents that does so, as is the case for this document.&nbsp;<i>Open Philanthropy Project examples</i>: every&nbsp;<a href=\"https://www.openphilanthropy.org/research/?content-type=cause-investigations&amp;view-list=true#categories\"><u>cause report</u></a>, plus many other reports and blog posts, e.g.&nbsp;<a href=\"https://www.openphilanthropy.org/research/how-will-hen-welfare-be-impacted-by-the-transition-to-cage-free-housing/\"><u>How Will Hen Welfare Be Impacted by the Transition to Cage-Free Housing?</u></a>,&nbsp;<a href=\"https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/\"><u>2017 Report on Consciousness and Moral Patienthood</u></a>,&nbsp;<a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>Worldview Diversification</u></a>,&nbsp;<a href=\"https://www.openphilanthropy.org/research/three-key-issues-ive-changed-my-mind-about/\"><u>Three Key Issues I\u2019ve Changed My Mind About</u></a>,&nbsp;<a href=\"https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/\"><u>Potential Risks from Advanced Artificial Intelligence: The Philanthropic Opportunity</u></a>,&nbsp;<a href=\"https://www.openphilanthropy.org/research/hits-based-giving/\"><u>Hits-based Giving</u></a>,&nbsp;<a href=\"https://www.openphilanthropy.org/research/initial-grants-to-support-corporate-cage-free-reforms/\"><u>Initial Grants to Support Corporate Cage-free Reforms</u></a>.&nbsp;<i>GiveWell examples</i>: Every&nbsp;<a href=\"https://www.givewell.org/charities/top-charities\"><u>top charity</u></a>&nbsp;review, most completed&nbsp;<a href=\"https://www.givewell.org/research/intervention-reports\"><u>intervention reports</u></a>, plus some other reports and blog posts, e.g.&nbsp;<a href=\"https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/comparing-moral-weights\"><u>Approaches to Moral Weights: How GiveWell Compares to Other Actors</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7u1myvd4tzt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7u1myvd4tzt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Examples in the main text of this document are typically numbered simply for ease of reference.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvlq1ka946dj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvlq1ka946dj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;In such a case, you might say something like: \u201cWe do not recall our source for this information but believe it would be straightforward to verify.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn243gzqrkl7l\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref243gzqrkl7l\">^</a></strong></sup></span><div class=\"footnote-content\"><p>E.g.&nbsp;<a href=\"https://www.openphilanthropy.org/research/how-will-hen-welfare-be-impacted-by-the-transition-to-cage-free-housing/\"><u>How Will Hen Welfare Be Impacted by the Transition to Cage-Free Housing?</u></a>&nbsp;and the \u201cprocess and findings\u201d&nbsp;<a href=\"http://files.givewell.org/files/GiveWell%20Final%20Project%20Review.pdf\"><u>document</u></a> for Ben Soskis\u2019 bibliography for the history of American philanthropy.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fneb4ki6k0g5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefeb4ki6k0g5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>See the table of sources at the bottom of most large reports by GiveWell or the Open Philanthropy Project.</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "DrjyBXvcqGwmLWNqr", "title": "Linkpost for various recent essays on suffering-focused ethics, priorities, and more", "postedAt": "2022-09-28T08:58:58.627Z", "htmlBody": "<p>The following are (links to) various essays that I have published over the last few months. Some of the essays have been published on the website of the <a href=\"https://centerforreducingsuffering.org/\">Center for Reducing Suffering</a> (CRS), and some of them have been published on my <a href=\"https://magnusvinding.com/\">own blog</a>.</p><h1>CRS essays</h1><h2><a href=\"https://centerforreducingsuffering.org/phenomenological-argument/\">A phenomenological argument against a positive counterpart to suffering</a></h2><blockquote><p>Various views deny that suffering has a positive counterpart. Proponents of such views often pursue a line of argument that focuses on the prevalence of subtle frustrations and bothersome sensations. That is, when we typically think that we are in a neutral state, and we claim that some pleasure takes us above that neutral state, what we are experiencing is really a subtly bothered and unsatisfied state that becomes (somewhat) relieved of its commonly overlooked unpleasant features (see e.g. Sherman, <a href=\"https://ore.exeter.ac.uk/repository/bitstream/handle/10871/32103/ShermanT.pdf?sequence=1\">2017</a>, pp. 103-107; Gloor, <a href=\"https://longtermrisk.org/tranquilism/\">2017</a>, sec. 2.1; Knutsson, <a href=\"https://www.simonknutsson.com/undisturbedness-as-the-hedonic-ceiling/\">2022</a>, sec. 6).</p><p>This essay will pursue a different line of argument. Rather than focusing on unpleasant states, and arguing for their subtle omnipresence, my aim here is instead to zoom in on the purportedly positive side. I will argue that purportedly positive experiences do not possess any property that renders them genuine opposites of painful and uncomfortable experiences, neither in phenomenological nor axiological terms.</p></blockquote><h2><a href=\"https://centerforreducingsuffering.org/reply-to-the-evolutionary-asymmetry-objection/\">Reply to the \u201cevolutionary asymmetry objection\u201d against suffering-focused ethics</a></h2><blockquote><p>An objection that is sometimes raised against suffering-focused ethics is that our intuitions about the relative value of suffering and happiness are skewed toward the negative for evolutionary reasons, and hence we cannot trust our intuition that says that the reduction of suffering is more valuable and more morally important than the creation of happiness. My aim in this post is to reply to this objection.</p></blockquote><h2><a href=\"https://centerforreducingsuffering.org/reply-to-the-scope-neglect-objection-against-value-lexicality/\">Reply to the scope neglect objection against value lexicality</a></h2><blockquote><p>Some views hold that no amount of mild discomfort can be worse than a single instance of extreme suffering (i.e. they endorse <a href=\"https://www.simonknutsson.com/value-lexicality\">value lexicality</a> between extreme suffering and mild discomfort). An <a href=\"https://www.lesswrong.com/posts/3wYTFWY3LKQCnAptN/torture-vs-dust-specks?commentId=XkDoPE3wCtwByTjLh\">objection</a> to such views is that they are biased by <a href=\"https://www.animal-ethics.org/scope-insensitivity-failing-to-appreciate-the-numbers-of-those-who-need-our-help/\">scope neglect</a> \u2014 our tendency to disregard the number of affected beings in our evaluations of a problem. Since we cannot comprehend the badness of a vast amount of mild discomfort, the objection goes, we cannot trust our intuitive assessment that extreme suffering is worse than any amount of mild discomfort. My aim in this brief post is to reply to this objection.</p></blockquote><h2><a href=\"https://centerforreducingsuffering.org/comments-on-the-weight-of-suffering/\">Comments on Mogensen\u2019s \u201cThe weight of suffering\u201d</a></h2><blockquote><p>Andreas Mogensen\u2019s paper \u201c<a href=\"https://globalprioritiesinstitute.org/the-weight-of-suffering-andreas-mogensen-global-priorities-institute-university-of-oxford/\">The weight of suffering</a>\u201d presents an interesting argument in favor of the axiological position that \u201cthere exists some depth of suffering that cannot be compensated for by any measure of well-being\u201d \u2014 a position he calls \u201cLTNU\u201d (Mogensen, 2022, abstract). Mogensen then proceeds to explore how one might respond to that argument and thereby reject LTNU.</p><p>My aim in this post is to raise some critical points in response to this paper. As a preliminary note, I should say that I commend Mogensen for taking up this crucial issue regarding the weight of suffering, and for exploring it in an open-ended manner.</p></blockquote><h2><a href=\"https://centerforreducingsuffering.org/reply-to-chappells-rethinking-the-asymmetry/\">Reply to Chappell\u2019s \u201cRethinking the Asymmetry\u201d</a></h2><blockquote><p>My aim in this post is to respond to the arguments presented in Richard Yetter Chappell\u2019s \u201c<a href=\"https://eprints.whiterose.ac.uk/116418/1/Chappell_Asymmetry.pdf\">Rethinking the Asymmetry</a>\u201d. Chappell argues against the <a href=\"https://en.wikipedia.org/wiki/Asymmetry_(population_ethics)\">Asymmetry</a> in population ethics, which roughly holds that the addition of bad lives makes the world worse, whereas the addition of good lives does not make the world better (other things being equal).</p></blockquote><h2><a href=\"https://centerforreducingsuffering.org/a-thought-experiment-that-questions-the-moral-importance-of-creating-happy-lives/\">A thought experiment that questions the moral importance of creating happy lives</a></h2><blockquote><p>Many people have the intuition that extinction would be bad. A problem, however, is that the term \u201cextinction\u201d carries many different connotations, and extinction may be considered bad for many different reasons. For instance, an extinction scenario might be considered bad because it involves frustrated preferences, violations of consent, or lethal violence. Yet extinction scenarios need not involve any of these elements in principle. By considering thought experiments that involve extinction without involving any of the elements listed above, we can get a better sense of what might explain the intuition that extinction would be bad. In this post, I will present a thought experiment that casts doubt on the notion that extinction would be bad or morally objectionable because it would prevent the creation of future happy lives.</p></blockquote><h2><a href=\"https://centerforreducingsuffering.org/lexical-priority-in-practice/\">Lexical priority to extreme suffering \u2014 in practice</a></h2><blockquote><p>Some ethical views grant a <a href=\"https://www.simonknutsson.com/value-lexicality\">lexical priority</a> to the prevention of extreme suffering over mild forms of suffering, meaning that the prevention of extreme suffering takes precedence over the prevention of mild suffering.</p><p>Such views have been claimed to have implausible practical implications. For instance, one objection is that such a lexical priority implies that we should neglect all endeavors that do not aim directly at the reduction of extreme suffering. My goal in this post is to reply to a couple of these objections, and to clarify some key aspects regarding how one might think about prioritization in light of lexical views.</p></blockquote><h1>Personal blog essays</h1><h2><a href=\"https://magnusvinding.com/2022/09/05/reasons-to-include-insects-in-animal-advocacy/\">Reasons to include insects in animal advocacy</a></h2><blockquote><p>I have seen some people claim that animal activists should primarily be concerned with certain groups of numerous vertebrates, such as chickens and fish, whereas we should not be concerned much, if at all, with insects and other small invertebrates. (See e.g. <a href=\"http://www.mattball.org/2019/04/why-i-am-not-utilitarian.html\">here</a>.) I think there are indeed good arguments in favor of emphasizing <a href=\"https://www.onestepforanimals.org/about.html\">chickens</a> and <a href=\"https://reducing-suffering.org/one-trillion-fish/\">fish</a> in animal advocacy, yet I think those same arguments tend to support a strong emphasis on helping insects as well. My aim in this post is to argue that we have compelling reasons to include insects and other small vertebrates in animal advocacy.</p></blockquote><h2><a href=\"https://magnusvinding.com/2022/09/10/the-catastrophic-rise-of-insect-farming/\">The catastrophic rise of insect farming and its implications for future efforts to reduce suffering</a></h2><blockquote><p>On the 17th of August 2021, the EU <a href=\"https://www.euractiv.com/section/agriculture-food/news/insects-on-the-menu-for-pigs-poultry-after-eu-approval/\">authorized</a> the use of insects as feed for farmed animals such as chickens and pigs. This was a disastrous decision for sentient beings, as it may <a href=\"https://reducing-suffering.org/why-i-dont-support-eating-insects/\">greatly increase</a> the number of beings who will suffer in animal agriculture. Sadly, this was just one in a series of disastrous decisions that the EU has made regarding insect farming in the last couple of years. Most recently, in February 2022, they <a href=\"https://food.ec.europa.eu/safety/novel-food/authorisations/approval-insect-novel-food_en\">authorized</a> the farming of house crickets for human consumption, after having made similar decisions for the farming of mealworms and migratory locusts in 2021.</p><p>Many such catastrophic decisions probably lie ahead, seeing that the EU is currently reviewing applications for the farming of nine additional kinds of insects. This brief posts reviews some reflections and potential lessons in light of these harmful legislative decisions.</p></blockquote><h2><a href=\"https://magnusvinding.com/2022/09/09/beware-underestimating-bad-outcomes/\">Beware underestimating the probability of very bad outcomes: Historical examples against future optimism</a></h2><blockquote><p>It may be tempting to view history through a progressive lens that sees humanity as climbing toward ever greater moral progress and wisdom. As the famous <a href=\"https://quoteinvestigator.com/2012/11/15/arc-of-universe/\">quote</a> popularized by Martin Luther King Jr. goes: \u201cThe arc of the moral universe is long, but it bends toward justice.\u201d</p><p>Yet while we may <i>hope</i> that this is true, and do our best to increase the probability that it will be, we should also keep in mind that there are reasons to doubt this optimistic narrative. For some, the recent rise of right-wing populism is a salient reason to be less confident about humanity\u2019s supposed path toward ever more compassionate and universal values. But it seems that we find even stronger reasons to be skeptical if we look further back in history. My aim in this post is to present a few historical examples that in my view speak against confident optimism regarding humanity\u2019s future.</p></blockquote><h2><a href=\"https://magnusvinding.com/2022/09/07/strategic-uncertainty/\">Radical uncertainty about outcomes need not imply (similarly) radical uncertainty about strategies</a></h2><blockquote><p>Our uncertainty about how the future will unfold is <a href=\"https://longtermrisk.org/charity-cost-effectiveness-in-an-uncertain-world/\">vast</a>, especially on long timescales. In light of this uncertainty, it may be natural to think that our uncertainty about strategies must be equally vast and intractable. My aim in this brief post is to argue that this is not the case.</p></blockquote><h2><a href=\"https://magnusvinding.com/2022/09/06/what-does-a-future-dominated-by-ai-imply/\">What does a future dominated by AI&nbsp;imply?</a></h2><blockquote><p>Among altruists working to reduce risks of bad outcomes due to AI, I sometimes get the impression that there is a rather quick step from the premise <strong>\u201cthe future will be dominated by AI\u201d</strong> to a practical position that roughly holds that <strong>\u201ctechnical AI safety research aimed at reducing risks associated with fast takeoff scenarios is the best way to prevent bad AI outcomes\u201d</strong>.</p><p>I am not saying that this is the most common view among those who work to prevent bad outcomes due to AI. Nor am I saying that the practical position outlined above is necessarily an unreasonable one. But I think I have seen (something like) this sentiment assumed often enough for it to be worthy of a critique. My aim in this post is to argue that there are many other practical positions that one could reasonably adopt based on that same starting premise.</p></blockquote><h2><a href=\"https://magnusvinding.com/2022/07/07/why-i-dont-prioritize-consciousness-research/\">Why I don\u2019t prioritize consciousness&nbsp;research</a></h2><blockquote><p>For altruists trying to reduce suffering, there is much to be said in favor of gaining a better understanding of consciousness. Not only may it lead to therapies that can mitigate suffering in the near term, but it may also help us in our large-scale prioritization efforts. For instance, clarifying which beings can feel pain is important for determining which causes and interventions we should be working on to best reduce suffering.</p><p>These points notwithstanding, my own view is that advancing consciousness research is not among the best uses of marginal resources for those seeking to reduce suffering. My aim in this post is to briefly explain why I hold this view.</p></blockquote>", "user": {"username": "MagnusVinding"}}, {"_id": "HAfRsn27uGwxdzHcj", "title": "Review of Examine.com\u2019s vitamin write-ups", "postedAt": "2022-09-28T07:48:30.878Z", "htmlBody": "", "user": {"username": "Elizabeth"}}, {"_id": "z2aJn6uQbvnncvJdo", "title": "What should Future Fund regrantors fund?", "postedAt": "2022-09-28T05:48:01.408Z", "htmlBody": "<p>The Future Fund regrantors are approaching their deadline (September 31st) to fund projects and I'm guessing some of them are scrambling to find stuff to fund due to deadline pressure, so I'm posting this question as a public good for them to look for opportunities. Post individual suggestions as top-level comments if you want people to upvote them, or make it a list if you have a lot of suggestions. I recommend taking a minute to think through your social network (did you meet anyone at EAG?) who has a project that could be evaluated for funding.</p>", "user": {"username": "DonyChristie"}}, {"_id": "kRNLsBLoCryMMipoJ", "title": "Why I think strong general AI is coming soon", "postedAt": "2022-09-28T06:55:23.401Z", "htmlBody": "", "user": {"username": "porby"}}, {"_id": "NTuTSEEn2Ka54nwFC", "title": "Offering FAANG-style mock interviews", "postedAt": "2022-09-28T03:09:32.288Z", "htmlBody": "<h1>My background</h1><p>I'm a senior software engineer at Waymo, and previously worked at Google. I've conducted around 150 technical interviews and been part of a couple hiring committees.</p><h1>Who can this probably help</h1><p>People who want to work as software engineers at FAANG companies (or companies with similar interviews), and want to get more feedback than a leetcode/Hackerrank will give. For example, how many questions to ask the interviewer? How to talk about your background? What exactly is the hiring bar for a recent college grad? And other things that are otherwise hard to learn about.</p><p>I expect this to be most helpful for folks early in their career (or pre-career!), but I'm happy to give a mock interview for anyone.<br>&nbsp;</p><p>Also more generally, if you want to practice interviews but find real interviews stressful, I\u2019m friendly!</p><p><img src=\"https://res.cloudinary.com/cea/image/upload/v1676166210/mirroredImages/NTuTSEEn2Ka54nwFC/iixxikno7m5jrtpdyoe7.png\"></p><p><br>&nbsp;</p><h1>Contact me</h1><p>Email:&nbsp;<a href=\"mailto:dantweinand@gmail.com\"><u>dantweinand@gmail.com</u></a></p><p>Title: EA FAANG mock interview</p><p>Body: Hey, could we set up a mock software interview?</p><p><br>&nbsp;</p><p>Anything else you write is a bonus. We'll figure out a compatible time over email.<br><br>&nbsp;[edit: I sometimes get asked if these are still ongoing. I hereby commit to updating this page if I stop doing these.</p><p>I've also had someone offer to share some interviewing.io interviews that they have built up, so if you are interested feel free to ask about that and I'll get you in touch with them.<br><br>After having done a dozen or so of these practice interviews, there are also some common themes that come up pretty frequently. These are:<br><br>1) It's important to be &nbsp;familiar with the major datastructures in the programming language you intend to interview in. <a href=\"https://www.bigocheatsheet.com/\">Big-O cheatsheet</a> has a good summary of the operations that various structures support efficiently. The most important structures for interviews are IMO: arrays, linked lists (for stacks/queues), hash maps, balanced binary search trees, heaps, and tries.<br><br>2) It's good to feel comfortable with the syntax of the language and basic implementation. Leetcode/Neetcode/Hackerrank are great for this. Project Euler is excellent for more algorithmically heavy problems, although I would recommend against going beyond the 100 most solved problems (unless you love number theory).<br><br>3) Ask the interviewer clarifying questions and for examples of terms. It's really helpful to be on the same page and prevent accidentally going off on a different problem.]</p>", "user": {"username": "dan.pandori"}}, {"_id": "jZnZuNhF4C7zrALsu", "title": "7 traps that (we think) new alignment researchers often fall into", "postedAt": "2022-09-27T23:13:46.722Z", "htmlBody": "", "user": {"username": "Akash"}}, {"_id": "baXRSujvPFXXPZjru", "title": "Effective Altruist bingo card", "postedAt": "2022-09-28T06:55:17.962Z", "htmlBody": "<p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/effb21179783bbc11c385c6b1800566d981941540b0dd507.png/w_1080 1080w\"></p>", "user": {"username": "dEAsign"}}, {"_id": "7XHCGwfWJdPufr5A7", "title": "Predict which posts will win the Criticism and Red Teaming Contest!", "postedAt": "2022-09-27T22:46:39.806Z", "htmlBody": "<p>If you've been enjoying the <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming\">Criticism and Red Teaming contest submissions</a>, you can now predict which ones will win on Manifold Markets.</p><ul><li>Each market tracks the probability that the specific essay will win any prize.</li><li>If you think the market is incorrect (rating an essay as too likely or not likely enough), trade in that direction to win M$.</li><li>All M$ on Manifold can be sent to <a href=\"https://manifold.markets/charity\">your favorite charity</a>!</li></ul><p>Start predicting here: <a href=\"https://manifold.markets/group/cart-contest\">https://manifold.markets/group/cart-contest</a></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_1620 1620w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f1455e8d611fab92a6d6f5d8bf1656f491120c7ae3ca8eb2.png/w_1742 1742w\"></figure>", "user": {"username": "akrolsmir"}}, {"_id": "5ZyLZjgJzyZdDLFrh", "title": "Likelihood of an anti-AI backlash: Results from a preliminary Twitter poll", "postedAt": "2022-09-27T22:01:30.671Z", "htmlBody": "<p>I ran a little Twitter <a href=\"https://twitter.com/primalpoly/status/1574494678216425473\">poll</a> yesterday asking about the likelihood of a strong, organized backlash against Artificial Intelligence (AI) in the next 20 years. The results from 1,203 votes are below; I'm curious about your reactions to these results.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_88 88w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_168 168w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_248 248w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_328 328w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_408 408w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_488 488w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_568 568w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/141e95b42128667445e3fe8deaaacd7c909a5816b9804e31.png/w_648 648w\"></figure><p>Caveats about poll sampling bias: Only about 1% of my 124k followers voted (which is pretty typical for polls). As far as I can tell, most of my followers seem to be based in the US, with some in the UK, Europe, etc.; most are politically centrist, conservative, libertarian, or slightly Lefty; most are male, college-educated, and somewhat ornery. These results should not be taken seriously as a globally, demographically, or politically representative poll; more research is needed.</p><p>Nonetheless: out of the 1,106 people who voted for one of the likelihood options, about 69% expected more than a 20% likelihood of a strong anti-AI backlash. That's much higher than I expected (given that most people carry around dozens of narrow AI systems every day, arguably, in the form of apps on their smartphones.)</p><p>We've seen timelines predicting when AGI will be developed. Has anyone developed any timelines about if/when an anti-AI backlash might develop, and/or considered how such a backlash might delay (or stop) AI research and development?&nbsp;</p><p>I'm familiar with arguments that there are irresistible geopolitical and corporate incentives to develop AI as fast as possible, and that formal government regulation of AI would be unlikely to slow that pace. However, the arguments I've seen so far don't seem to take seriously the many ways that informal shifts in social values could stigmatize AI research, analogous to the ways that the BLM movement, MeToo movement, anti-GMO movement, anti-vax movement, etc have stigmatized some previously accepted behaviors and values -- even those that had strong government and corporate support.&nbsp;</p><p>PS I ran two other related polls that might be of interest: <a href=\"https://twitter.com/primalpoly/status/1574502701320110080\">one</a> on general attitudes towards AI researchers (N=731 votes; slightly positive attitudes overall, but mixed), and <a href=\"https://twitter.com/primalpoly/status/1574504605060243456\">one</a> on whether people would believe AI experts who claim to have develop theorems proving some reassuring AI safety and alignment results (N=972 votes; overwhelming 'no' votes, indicating high skepticism about formal alignment claims).</p>", "user": {"username": "geoffreymiller"}}, {"_id": "iBeWbfQLA9EKfsdhu", "title": "Why we're not founding a human-data-for-alignment org", "postedAt": "2022-09-27T20:14:00.547Z", "htmlBody": "<h1>TL;DR</h1><p><strong>One-paragraph summary:&nbsp;</strong>we (two recent graduates) spent about half of the summer exploring the idea of starting an organisation producing custom human-generated datasets for AI alignment research. Most of our time was spent on customer interviews with alignment researchers to determine if they have a pressing need for such a service. We decided not to continue with this idea, because there doesn\u2019t seem to be a human-generated data niche (unfilled by existing services like Surge) that alignment teams would want outsourced.</p><p>&nbsp;</p><p><strong>In more detail</strong>: The idea of a human datasets organisation was&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MBDHjwDvhDnqisyW2/awards-for-the-future-fund-s-project-ideas-competition\"><u>one of the winners of the Future Fund project ideas competition</u>,&nbsp;</a>still figures on their&nbsp;<a href=\"https://ftxfuturefund.org/projects/high-quality-human-data-for-ai-alignment-nbsp/\"><u>list</u></a> of project ideas, and had been advocated before then by some people, including Beth Barnes. Even though we ended up deciding against, we think this was a reasonable and high-expected-value idea for these groups to advocate at the time.</p><p>Human-generated data is often needed for ML projects or benchmarks if a suitable dataset cannot be e.g. scraped from the web, or if human feedback is required. Alignment researchers conduct such ML experiments, but sometimes have different data requirements than standard capabilities researchers. As a result, it seemed plausible that there was some niche unfilled by the market to help alignment researchers solve problems related to human-generated datasets. In particular, we thought - and to some extent confirmed - that the most likely such niche is human data generation that requires particularly competent or high-skill humans. We will refer to this as&nbsp;<strong>high-skill (human) data</strong>.</p><p>We (Matt &amp; Rudolf) went through <a href=\"https://forum.effectivealtruism.org/posts/8QfQcFyj6aGNM78kz/learning-from-matching-co-founders-for-an-ai-alignment\">an informal co-founder matching process along with four other people</a> and were chosen as the co-founder pair to explore this idea. In line with standard startup advice, our first step was to explore whether or not there is a concrete current need for this product by conducting interviews with potential customers. We talked to about 15 alignment researchers, most of them selected on the basis of doing work that requires human data. A secondary goal of these interviews was to build better models for the future importance and role of human feedback in alignment.</p><p>Getting human-generated data does indeed cost many of these researchers significant time and effort. However, we think to a large extent this is because dealing with humans is inherently messy, rather than existing providers doing a bad job. Surge AI in particular seems to offer a pretty good and likely improving service. Furthermore, many companies have in-house data-gathering teams or are in the process of building them.</p><p>Hence we have decided to not further pursue this idea.</p><p>Other projects in the human data generation space may still be valuable, especially if the importance of human feedback in ML continues to increase, as we expect. This might include people specializing on human data as a career.</p><p>The types of factors that are most important for doing human dataset provision well include: high-skill contractors, fast iteration, and high bandwidth communication and shared understanding between the research team, the provider organisation and the contractors.</p><p>We are keen to hear other people\u2019s thoughts, and would be happy to talk or to share more notes and thoughts with anyone interested in working on this idea or a similar one in the future.</p><p><br>&nbsp;</p><h1>Theory of Change</h1><p>A major part of AI alignment research requires doing machine learning (ML) research, and ML research in turn requires training ML models. This involves expertise and execution ability in three broad categories: algorithms, compute, and data, the last of which is very neglected by EAs.</p><p>We expect training on data from human feedback to become an increasingly popular and very powerful tool in mainstream ML (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org#Will_human_feedback_become_a_much_bigger_deal__Is_this_a_very_quickly_growing_industry_\"><u>below</u></a>). Furthermore, many proposals for alignment (for example: reinforcement learning from human feedback (RLHF) and variants like recursive reward modelling, iterated amplification, and safety via debate) would require lots of human interaction or datasets based on human-generated data.</p><p>While many services (most notably Surge) exist for finding labour to work on data generation for ML models, it seems plausible that an EA-aligned company could add significant value because:</p><ul><li>Markets may not be efficient enough to fill small niches that are more important to alignment researchers than other customers; high-skill human data that requires very competent crowdworkers may be one such example. If alignment researchers can get it at all, it might be very expensive.</li><li>We have a better understanding of alignment research agendas, and this might help. This may allow us to make better-informed decisions on many implementation details with less handholding, thereby saving researchers time.</li><li>We would have a shared goal with our customers: reducing AI x-risk. Though profit motives already provide decent incentives to offer a good service, mission alignment helps avoid adversarial dynamics, increases trust, and reduces friction in collaboration.</li><li>An EA-led company may be more willing to make certain strategic moves that go against its profit incentives; e.g. investing heavily into detecting a model\u2019s potential attempts to deceive the crowdworkers, even when it\u2019s hard for outsiders to tell whether such monitoring efforts are sincere and effective (and thus customers may not be willing to pay for it). Given that crowdworkers might provide a reward signal, they could be a key target for deceptive AIs.</li></ul><p>Therefore, there is a chance that an EA-led&nbsp; human data service that abstracts out some subset of dataset-related problems (e.g. contractor finding, instruction writing/testing, UI and pipeline design/coding, experimentation to figure out best practices and accumulate that knowledge in one place) would:</p><ol><li>save the time of alignment researchers, letting them make more progress on alignment; and</li><li>reduce the cost (in terms of time and annoying work) required to run alignment-relevant ML experiments, and therefore bring more of them below the bar at which it makes sense to run them, and thus increasing the number of such experiments that are run.</li></ol><p>In the longer run, benefits of such an organisation might include:</p><ul><li>There is some chance that we could simply outcompete existing ML data generation companies and be better even in the cases where they do provide a service; this is especially plausible for relatively niche services. In this scenario we\u2019d be able to exert some marginal influence over the direction of the AI field, for example by only taking alignment-oriented customers. This would amount to differential development of safety over capabilities. Beyond only working with teams that prioritise safety, we could also pick among self-proclaimed \u201csafety researchers\u201d. It is common for proclaimed safety efforts to be accused of helping more with capabilities than alignment by other members of the community.</li><li>There are plausibly critical actions that might need to be taken for alignment, possibly quickly during \u201ccrunch-time\u201d, that involve a major (in quality or scale) data-gathering project (or something like large-scale human-requiring interpretability work, that makes use of similar assets, like a large contractor pool). At such a time it might be very valuable to have an organisation committed to x-risk minimisation with the competence to carry out any such project.</li></ul><p>Furthermore, if future AIs will learn human values from human feedback, then higher data quality will be equivalent to a training signal that points more accurately at human values. In other words, higher quality data may directly help with outer alignment (though we're not claiming that it could realistically solve it on its own). In discussions, it seemed that Matt gave this argument slightly more weight than Rudolf.</p><p>While these points are potentially high-impact, we think that there are significant problems with starting an organisation mainly to build capacity to be useful only at some hypothetical future moment. In particular, we think it is hard to know exactly what sort of capacity to build (and the size of the target in type-of-capacity space might be quite small), and there would be little feedback that the organisation could improve or course-correct based on.&nbsp;</p><p>More generally, both of us believe that EA is right now partly bottlenecked by people who can start and scale high-impact organisations, which is a key reason why we\u2019re considering entrepreneurship. This seems particularly likely given the large growth of the movement.&nbsp;<br>&nbsp;</p><h1>What an org in this space may look like</h1><h2>Providing human datasets</h2><p>The concept we most seriously considered was a for-profit that would specialise in meeting the specific needs of alignment researchers, probably by focusing on very high-skill human data. Since this niche is quite small, the company could offer a very custom-tailored service. At least for the first couple years, this would probably mean both of us having a detailed understanding of the research projects and motivations of our customers. That way, we could get a lot of small decisions right, without the researchers having to spend much time on it. We might be especially good at that compared to competitors, given our greater understanding of alignment.</p><h2>Researching enhanced human feedback</h2><p>An alternative we considered was founding a non-profit that would research how to enhance human feedback. See this&nbsp;<a href=\"https://www.lesswrong.com/posts/ybThg9nA7u6f8qfZZ/techniques-for-enhancing-human-feedback\"><u>post</u></a> by Ajeya Cotra for some ideas on what this kind of research could look like. The central question is whether and how you can combine several weak training signals into a stronger more accurate one. If this succeeded, maybe (enhanced) human feedback could become a more accurate (and thereby marginally safer) signal to train models on.</p><p>We decided against this for a number of reasons:</p><ul><li>Currently, neither of us has more research experience than an undergraduate research project.</li><li>We thought we could get a significant fraction of the benefits of this kind of research even if we did the for-profit version, and plausibly even more valuable expertise.<ul><li>First of all, any particular experiment that funders would have liked to see, they could have paid us to do, although we freely admit that this is very different from someone pushing forward their own research agenda.</li><li>More importantly, we thought a lot of the most valuable expertise to be gained would come in the form of&nbsp;<strong>tacit knowledge and answers to concrete boring questions</strong> that are not best answered by doing \u201cresearch\u201d on them, but rather by iterating on them while trying to offer the best product (e.g. \u201cWhere do you find the best contractors?\u201d, \u201cHow do you incentivize them?\u201d, \u201cWhat\u2019s the best way to set up communication channels?\u201d).<ul><li>It is our impression that Ought pivoted away from doing abstract research on factored cognition and toward offering a valuable product for related reasons.</li></ul></li></ul></li><li>This topic seems plausibly especially tricky to research (though some people we\u2019ve spoken to disagreed):&nbsp;<ul><li>At least some proposed such experiments would not involve ML models at all. We fear that this might make it especially easy to fool ourselves into thinking some experiment might eventually turn out to be useful when it won\u2019t. More generally, the research would be pretty far removed from the end product (very high quality human feedback). In the for-profit case on the other hand, we could easily tell whether alignment teams were willing to pay for our services and iteratively improve.&nbsp;</li></ul></li></ul><h2>For-profit vs non-profit</h2><p>We can imagine two basic funding models for this org:&nbsp;</p><ul><li>either we\u2019re a nonprofit directly funded by EA donors and offering free or subsidized services to alignment teams;</li><li>or we\u2019re a for-profit, paid by its customers (ie alignment teams).&nbsp;</li></ul><p>Either way, a lot of the money will ultimately come from EA donors (who fund alignment teams.)</p><p>The latter funding mechanism seems better; \u201ccustomers paying money for a service\u201d leads to the efficient allocation of resources by creating market structures. They have a clear incentive to spend the money well. On the other hand, \u201cfoundations deciding what services are free\u201d is more reminiscent of planned economies and distorts markets. To a first approximation, funders should give alignment orgs as much money as they judge appropriate and then alignment orgs should exchange it for services as they see fit.</p><p>A further reason is that a non-profit is legally more complicated to set up, and imposes additional constraints on the organisation.</p><h2>Should the company exclusively serve alignment researchers?</h2><p>We also considered founding a company with the ambition to become a major player in the larger space of human data provision. It would by default serve anyone willing to pay us and working on something AGI-related, rather than just alignment researchers. Conditional on us being able to successfully build a big company, this would have the following upsides:</p><ul><li>Plausibly one of the main benefits of founding a human data gathering organisation is to produce EAs and an EA org that have deep expertise in handling and producing high-skill human data in significant quantities. That might prove useful around \u201ccrunch time\u201d, e.g. when some project aims to create competitive but safe AGI and needs this expertise. Serving the entire market could scale to a much larger company enabling us to&nbsp;<strong>gain expertise at higher scales</strong>.</li><li>Operating a large company would also come with some degree of&nbsp;<strong>market power</strong>. Any company with paying customers has some amount of leverage over them: first of all just because of switching costs, but also because the product it offers might be much better than the next-best alternative. This could allow us to make some demands, e.g. once we\u2019re big and established, announce we\u2019d only work with companies that follow certain best practices.</li></ul><p>On the other hand, building a big successful company serving anyone willing to pay might come with some significant downsides as well.</p><ul><li>First, and most straightforwardly,<strong> it is probably much&nbsp;harder than filling a small niche (just meeting the specific needs of alignment researchers), making us less likely to succeed. A large number of competitors exist and as described in this&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org#Key_crux__demand_looks_questionable__Surge_seems_pretty_good\"><strong><u>section</u></strong></a><strong>, some of them (esp. Surge) seem pretty hard to beat. Since this is an already big and growing market, there is an additional efficient markets reason to assume this is true a priori.</strong></li><li>Secondly, and <strong>perhaps more importantly, such a company might&nbsp;accelerate capabilities&nbsp;(more on this&nbsp;</strong><a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org#Would_we_be_accelerating_capabilities_\"><strong><u>below</u></strong></a><strong>).</strong></li></ul><p>Furthermore, it might&nbsp;<strong>make RLHF (Reinforcement Learning from Human Feedback) in particular more attractive</strong>. Depending on one\u2019s opinions about RLHF and how it compares to other realistic alternatives, one might consider this a strong up- or downside.&nbsp;</p><h1>Approach</h1><p>The main reason companies fail is that they build a product that customers don\u2019t want. For for-profits, the signal is very clear: either customers care enough to be willing to pay hard cash for the product/service, or they don\u2019t. For non-profits, the signal is less clear, and therefore nonprofits can easily stick around in an undead state, something that is an even worse outcome than the quick death of a for-profit because of resource (mis)allocation and opportunity costs. As discussed, it is not obvious which structure we should adopt for this organisation, though for-profit may be a better choice on balance. However, in all cases it is clear that the organisation needs to solve a concrete problem or provide clear value to exist and be worth existing. This does not mean that the value proposition needs to be certain; we would be happy to take a high-risk, high-reward bet, and generally support&nbsp;<a href=\"https://www.openphilanthropy.org/research/hits-based-giving/\"><u>hits-based approaches to impact</u></a> both in general and for ourselves.</p><p>An organisation is unlikely to do something useful to its customers without being very focused on customer needs, and ideally having tight feedback cycles.&nbsp;</p><p>The shortest feedback loops are when you\u2019re making a consumer software product where you can prototype quickly (including with mockups), and watch and talk to users as they use the core features, and then see if the user actually buys the product on the spot. A datasets service differs from this ideal feedback mode in a number of ways:</p><ol><li>The product is a labour-intensive process, which means the user cannot quickly use the core features and we cannot quickly simulate them.</li><li>The actual service requires either a contractor pool or (potentially at the start) the two of us spending a number of hours per request generating data.</li><li>There is significant friction to getting users to use the core feature (providing a dataset), since it requires specification of a dataset from a user, which takes time and effort.</li></ol><p>Therefore, we relied on customer interviews with prospective customers. The goal of these interviews was to talk to alignment researchers who work with data, and figure out if external help with their dataset projects would be of major use to them.</p><p>Our approach to customer interviews was mostly based on the book&nbsp;<a href=\"https://www.amazon.com/Mom-Test-customers-business-everyone-ebook/dp/B01H4G2J1U\"><i><u>The Mom Test</u></i></a>, which is named after the idea that your customer interview questions should be concrete and factual enough that even someone as biased as your own mom shouldn\u2019t be able to give you a false signal about whether the idea is actually good. Key lessons emphasised by&nbsp;<i>The Mom Test</i> include emphasising:</p><ul><li><strong>factual</strong> questions about the past&nbsp;<strong>over hypothetical</strong> questions for the future;<ul><li>In particular, questions about concrete past and current&nbsp;<strong>efforts</strong> spent solving a problem<strong> rather than</strong> questions about current or future&nbsp;<strong>wishes</strong> for solving a problem</li></ul></li><li>questions that get at something<strong> concrete (e.g. numbers)</strong>; and</li><li>questions that prompt the customer to give information about their problems and priorities without prompting them with a solution.</li></ul><p>We wanted to avoid the failure mode where lots of people tell us something is important and valuable in the abstract, without anyone actually needing it themselves.</p><p>We prepared a set of default questions that roughly divided into:</p><ol><li>A general starting question prompting the alignment researcher to describe the biggest pain points and bottlenecks they face in their work, without us mentioning human data.</li><li>Various questions about their past and current dataset-related work, including what types of problems they encounter with datasets, how much of their time these problems take, and steps they took to address these problems.</li><li>Various questions on their past experiences using human data providers like Surge, Scale, or Upwork, and specifically about any things they were unable to accomplish because of problems with such services.</li><li>In some cases, more general questions about their views on where the bottlenecks for solving alignment are, views on the importance of human data or tractability of different data-related proposals, etc.&nbsp;</li><li>What we should\u2019ve asked but didn\u2019t, and who else we should talk to.</li></ol><p>Point 4 represents the fact that in addition to being potential customers, alignment researchers also doubled as domain experts. The weight given to the questions described in point 4 varied a lot, though in general if someone was both a potential customer and a source of data-demand-relevant alignment takes, we prioritised the customer interview questions.</p><p>In practice, we found it easy to arrange meetings with alignment researchers; they generally seemed willing to talk to people who wanted input on their alignment-relevant idea. We did customer interviews with around 15 alignment researchers, and had second meetings with a few. For each meeting, we prepared beforehand a set of questions tweaked to the particular person we were meeting with, which sometimes involved digging into papers published by alignment researchers on datasets or dataset-relevant topics (Sam Bowman in particular has worked on a lot of data-relevant papers). Though the customer interviews were by far the most important way of getting information on our cruxes, we found the literature reviews we carried out to be useful too. We are happy to share the notes from the literature reviews we carried out; please reach out if this would be helpful to you.</p><p>Though we prepared a set of questions beforehand, in many meetings - including often the most important or successful ones - we often ended up going off script fairly quickly.</p><p>Something we found very useful was that, since there were two of us, we could split the tasks during the meeting into two roles (alternating between meetings):</p><ol><li>One person who does most of the talking, and makes sure to be focused on the thread of the conversation.</li><li>One person who mostly focuses on note-taking, but also pipes in if they think of an important question to ask or want to ask for clarification.</li></ol><h1>Key crux: demand looks questionable, Surge seems pretty good</h1><p><strong>Common startup advice&nbsp;</strong>is to make sure you have identified a very&nbsp;<strong>strong signal of demand&nbsp;</strong>before you start building stuff. That should look something like someone telling you that the thing you\u2019re working on is one of their biggest bottlenecks and that they can\u2019t wait to pay you asap so you solve this problem for them. \u201cNice to have\u201d doesn\u2019t cut it. This is in part because working with young startups is inherently risky, so you need to make up for that by solving one of their most important problems.</p><p>In brief, we don\u2019t think this level of very strong demand currently exists, though there were some weaker signals that looked somewhat promising. There are many existing startups that offer human feedback already.&nbsp;<a href=\"https://www.surgehq.ai/\"><strong><u>Surge AI</u></strong></a> in particular was brought up by many people we talked to and seems to offer quite a decent service that would be&nbsp;<strong>hard to beat</strong>.</p><h2>Details about Surge</h2><p>Surge is a US-based company that offers a service very similar to what we had in mind, though they are not focused on alignment researchers exclusively. They build data-labelling and generation tools and have a workforce of crowdworkers.</p><p>They\u2019ve worked with Redwood and the OpenAI safety team, both of which had moderately good experiences with them. More recently, Ethan Perez\u2019s team have worked with Surge too; he seems to be very satisfied based <a href=\"https://twitter.com/EthanJPerez/status/1567180843231379457?t=CEdeLRWNcxBD2eeO3Hd1Iw&amp;s=07\">on this Twitter thread</a>.</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/13dcad81b5782236c25371ce3642e027fb1de521ca9b3a21.png/w_827 827w\"><br>&nbsp;</p><h3>Collaboration with Redwood</h3><p>Surge has worked with Redwood Research on their&nbsp;<a href=\"https://arxiv.org/abs/2205.01663\"><u>paper</u></a> about adversarial training. This is one of three&nbsp;<a href=\"https://www.surgehq.ai/case-study/adversarial-testing-redwood-research\"><u>case studies</u></a> on Surge\u2019s website, so we assume it\u2019s among the most interesting projects they\u2019ve done so far. The crowdworkers were tasked with coming up with prompts that would cause the model to output text in which someone got injured. Furthermore, crowdworkers also classified whether someone got injured in a given piece of text.</p><p>One person from Redwood commented that doing better than Surge seemed possible to them with \u201cprobably significant value to be created\u201d, but \u201cnot an easy task\u201d. They thought our main edge would have to be that we\u2019d specialise on fuzzy and complex tasks needed for alignment; Surge apparently did quite well with those, but still with some room for improvement. A better understanding of alignment might lower chances of miscommunication. Overall, Redwood seems quite happy with the service they received.</p><p>Initially, Surge\u2019s iteration cycle was apparently quite slow, but this improved over time and was \u201cpretty good\u201d toward the end.</p><p>Redwood told us they were quite likely to use human data again by the end of the year and more generally in the future, though they had substantial uncertainty around this. Their experience in working with human feedback overall was somewhat painful as we understood it.&nbsp; This is part of the reason they\u2019re uncertain about how much human feedback they will use for future experiments, even though it\u2019s quite a powerful tool. However, they estimated that friction in working with human feedback was mostly caused by inherent reasons (humans are inevitably slower and messier than code), rather than Surge being insufficiently competent.&nbsp;</p><h3>Collaboration with OpenAI</h3><p>OpenAI have worked with Surge in the context of their WebGPT&nbsp;<a href=\"https://arxiv.org/abs/2112.09332\"><u>paper</u></a>. In that paper, OpenAI fine-tuned their language model GPT-3 to answer long-form questions. The model is given access to the web, where it can search and navigate in a text-based environment. It\u2019s first trained with imitation learning and then optimised with human feedback.&nbsp;</p><p>Crowdworkers provided \u201cdemonstrations\u201d, where they answered questions by browsing the web. They also provided \u201ccomparisons\u201d, where they indicated which of two answers to the same question they liked better.</p><p>People from OpenAI said they had used Surge mostly for sourcing the contractors, while doing most of the project management, including building the interfaces, in-house. They were generally pretty happy with the service from Surge, though all of them did mention shortcomings.</p><p>One of the problems they told us about was that it was hard to get access to highly competent crowdworkers for consistent amounts of time. Relatedly, it often turned out that a very small fraction of crowdworkers would provide a large majority of the total data.&nbsp;</p><p>More generally, they wished there had been someone at Surge that understood their project better. Also, it might have been somewhat better if there had been more people with greater experience in ML, such that they could have more effectively anticipated OpenAI\u2019s preferences \u2014 e.g. predict accurately what examples might be interesting to researchers when doing quality evaluation. However, organisational barriers and insufficient communication were probably larger bottlenecks than ML knowledge. At least one person from OpenAI strongly expressed a desire for a service that understood their motives well and took as much off their plate as possible in terms of hiring and firing people, building the interfaces, doing quality checks and summarising findings etc. It is unclear to us to what extent Surge could have offered these things if OpenAI hadn\u2019t chosen to do a lot of these things in-house. One researcher suggested that communicating their ideas reliably was often more work than just doing it themselves. As it was, they felt that marginal quality improvement required significant time investment on their own part, i.e. could not be solved with money alone.&nbsp;</p><p>Notably, one person from OpenAI estimated that about&nbsp;<strong>60% of the WebGPT team\u2019s efforts&nbsp;</strong>were spent on various aspects of&nbsp;<strong>data collection</strong>. They also said that this figure didn\u2019t change much after weighting for talent, though in the future they expect junior people to take on more disproportionate shares of this workload.</p><p>Finally, one minor complaint that was mentioned was the lack of transparency about contractor compensation.&nbsp;</p><h3>How mission-aligned is Surge?</h3><p>Surge&nbsp;<a href=\"https://www.surgehq.ai/case-study/adversarial-testing-redwood-research\"><u>highlight</u></a> their collaboration with Redwood on their website as one of three case studies. In their blog&nbsp;<a href=\"https://www.surgehq.ai/blog/the-250k-inverse-scaling-prize-and-human-ai-alignment\"><u>post</u></a> about their collaboration with Anthropic, the first sentence reads: \u201cIn many ways, alignment \u2013 getting models to align themselves with what we want, not what they think we want \u2013 is one of the fundamental problems of AI.\u201d&nbsp;</p><p>On the one hand, they describe alignment as one of the fundamental problems of AI, which could indicate that they intrinsically cared about alignment. However, they have a big commercial incentive to say this. Note that many people would consider their half-sentence definition of alignment to be wrong (a model might know what we want, but still do something else).</p><p>We suspect that the heads of Surge have at least vaguepositive dispositions towards alignment. They definitely seem eager to work with alignment researchers, which might well be more important. We think it\u2019s mostly fine if they are not maximally intrinsically driven, though mission alignment does add value as mentioned above.</p><h2>Other competitors</h2><p>We see Surge as the most direct competitor and have researched them by far in the most detail. But besides Surge, there are a large number of other companies offering similar services.&nbsp;</p><p>First, and most obviously, Amazon&nbsp;<a href=\"https://www.mturk.com/\"><u>Mechanical Turk</u></a> offers a very low quality version of this service and is very large.&nbsp;<a href=\"https://www.upwork.com/\"><u>Upwork</u></a> specialises in sourcing humans for various tasks, without building interfaces.&nbsp;<a href=\"https://scale.com/\"><u>ScaleAI</u></a> is a startup with a $7B valuation --- they augment human feedback with various automated tools. OpenAI have worked with them. Other companies in this broad space include&nbsp;<a href=\"https://gethybrid.io/\"><u>Hybrid</u></a> (which Sam Bowman\u2019s lab has worked with) and&nbsp;<a href=\"https://www.invisible.ai/\"><u>Invisible</u></a> (who have worked with OpenAI). There are many more that we haven\u2019t listed here.</p><p>In addition, some labs have in-house teams for data gathering (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org#Is_it_more_natural_for_this_work_to_be_done_in_house_in_the_longterm__Especially_at_big_labs_companies_\"><u>here</u></a> for more).</p><h2>Data providers used by other labs</h2><p>Ethan Perez\u2019s and Sam Bowman\u2019s labs at NYU/Anthropic have historically often built their own interfaces while using contractors from Upwork or undergrads, but they have been trialing Surge over the summer and seem likely to stick with them if they have a good experience. Judging from the Twitter thread linked above and asking J\u00e9r\u00e9my Scheurer (who works on the team and built the pre-Surge data pipeline) how they\u2019ve found Surge so far, Surge is doing a good job.&nbsp;</p><p>Google has an internal team that provides a similar service, though DeepMind have used at least one external provider as well. We expect that it would be quite hard to get DeepMind to work with us, at least until we would be somewhat more established.&nbsp;</p><p>Generally, we get the impression that most people are quite happy with Surge. It\u2019s worth also considering that it\u2019s a young company that\u2019s&nbsp;<strong>likely improving its service over time</strong>. We\u2019ve heard that Surge iterates quickly, e.g. by shipping simple feature requests in two days. It\u2019s possible that some of the problems listed above may no longer apply by now or in a few months.</p><h2>Good signs for demand</h2><p>One researcher we talked to said that there were lots of projects their team didn\u2019t do, because gathering human feedback of sufficient quality was infeasible.&nbsp;</p><p>One of the examples this researcher gave was human feedback on code quality. This is implausible to do, because the time of software engineers is just too expensive. That problem is hard for a new org to solve.&nbsp;</p><p>Another example they gave seemed like it might be more feasible: for things like RLHF, they often choose to do pairwise comparisons between examples or multi-preferences. Ideally, they would want to get ratings, e.g. on a scale from 1 to 10. But they thought they didn\u2019t trust the reliability of their raters enough to do this.&nbsp;</p><p>More generally, this researcher thought there were lots of examples where if they could copy any person on their team a hundred times to provide high-skill data, they could do many experiments that they currently can\u2019t.&nbsp;</p><p>They also said that their team would be willing to pay ~3x of what they were paying currently to receive much higher-quality feedback.</p><p>Multiple other researchers we talked to expressed vaguely similar sentiments, though none quite as strong.</p><p>However, it\u2019s notable that in this particular case, the researcher hadn\u2019t worked with Surge yet.&nbsp;</p><p>The same researcher also told us about a recent project where they had spent a month on things like creating quality assurance examples, screening raters, tweaking instructions etc. They thought this could probably have been reduced a lot by an external org, maybe to as little as one day. Again, we think Surge may be able to get them a decent part of the way there.</p><h2>Labs we could have worked with</h2><p>We ended up finding three projects that we could have potentially worked on:</p><ul><li>A collaboration with Ought --- they spend about 15 hours a week on data-gathering and would have been happy to outsource that to us. If it had gone well, they might also have done more data-gathering in the longterm (since friction is lower if it doesn\u2019t require staff time). We decided not to go ahead with this project since we weren\u2019t optimistic enough about demand from other labs being bigger once we had established competence with Ought and the project itself didn\u2019t seem high upside enough.&nbsp;</li><li>Attempt to get the Visible Thoughts&nbsp;<a href=\"https://intelligence.org/2021/11/29/visible-thoughts-project-and-bounty-announcement/\"><u>bounty</u></a> by MIRI. We decided against this for a number of reasons. See more of our thinking about Visible Thoughts below.</li><li>Potentially a collaboration with Owain Evans on curated datasets for alignment.</li></ul><p>We think the alignment community is currently relatively tight-knit. e.g. researchers often knew about other alignment teams\u2019 experiences with Surge from conversations they had had with them. Hence, we were relatively optimistic that conditional on there being significant demand for this kind of service, doing a good job on one of the projects above would quickly lead to more opportunities.<br>&nbsp;</p><h3>Visible Thoughts</h3><p>In November 2021,&nbsp;<a href=\"https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement\"><u>MIRI announced the Visible Thoughts (VT) project bounty</u></a>. In many ways VT would be a good starting project for an alignment-oriented dataset provider, in particular because the bounty is large (up to $1.2M) and because it is ambitious enough that executing on it would provide a strong learning signal to us and a credible signal to other organisations we might want to work with. However, on closer examination of VT, we came to the conclusion that it is not worth it for us to work on it.</p><p>The idea of VT is to collect a dataset of 100 runs of fiction of a particular type (\u201cdungeon runs\u201d, an interactive text-based genre where one party, called the \u201cdungeon master\u201d and often an AI, offers descriptions of what is happening, and the other responds in natural language with what actions they want to take), annotated with a transcript of some of the key verbal thoughts that the dungeon master might be thinking as they decide what happens in the story world. MIRI hopes that this would be useful for training AI systems that make their thought processes legible and modifiable.</p><p>In particular, a notable feature of the VT bounty is the extreme run lengths that it asks for: to the tune of 300 000 words for each of the runs (for perspective, this is the length of&nbsp;<i>A Game of Thrones</i>, and longer than the first three&nbsp;<i>Harry Potter</i> books combined). A VT run is much less work than a comparable-length book - the equivalent of a rough unpolished first-draft (with some quality checks) would likely be sufficient - but producing one such run would still probably require at least on the order of 3 months of sequential work time from an author. We expect the pool of people willing to write such a story for 3 months is significantly smaller than the pool of people who would be willing to complete, say, a 30 000 word run, and that the high sequential time cost increases the amount of time required to generate the same number of total words. We also appear to have different ideas on how easy it is to fit a coherent story, for the relevant definition of coherent, into a given number of words. Note that to compare VT word counts to lengths of standard fiction without the written-out thoughts from the author, the VT word count should be reduced by a factor of 5-6.</p><p>Concerns about the length are raised in the comments section, to which Eliezer Yudkowksy&nbsp;<a href=\"https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement?commentId=irJCDQaWRcdT3Bnoo\"><u>responded</u></a>. His first point, that longer is easier to write per step, may be true, especially as we also learned (by email with Nate Soares and Aurelien Cabanillas) that in MIRI\u2019s experience \u201cauthors that are good at producing high quality steps are also the ones who don't mind producing many steps\u201d. In particular because of that practical experience, we think it is possible we overestimated the logistical problems caused by the length. MIRI also said they would likely accept shorter runs too if they satisfied their other criteria.</p><p>In a brief informal conversation with Rudolf during EAG SF, Eliezer emphasised the long-range coherence point in particular. However, they did not come to a shared understanding of what type of \u201clong-range coherence\u201d is meant.</p><p>Even more than these considerations, we are sceptical about the vague plans for what to do given a VT dataset. A recurring theme from talking to alignment researchers who work with datasets was that inventing and creating a good dataset is surprisingly hard, and generally involves having a clear goal of what you\u2019re going to use the dataset for. It is possible the key here is the difference in our priors for how likely a dataset idea is to be useful.</p><p>In addition, we have significant concerns about undertaking a major project based on a bounty whose only criterion is the judgement of one person (Eliezer Yudkowsky), and undertaking such a large project as our first project.</p><h1>Other cruxy considerations</h1><h2>Could we make a profit / get funding?&nbsp;</h2><p>One researcher from OpenAI told us he thought it would be hard to imagine an EA data-gathering company making a profit because costs for individual projects would always be quite high (requiring several full-time staff), and total demand was probably not all that big.</p><p>In terms of funding, both of us were able to spend time on this project because of grants from regrantors in the Future Fund regrantor program. Based on conversations with regrantors, we believe we could\u2019ve gotten funding to carry out an initial project if we had so chosen.</p><h2>Will human feedback become a much bigger deal? Is this a very quickly growing industry?</h2><p>Our best guess is yes. For example, see this&nbsp;<a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>post</u></a> by Ajeya Cotra which outlines how we could get to TAI by training on Human Feedback on Diverse Tasks (HFDT).&nbsp;</p><p>She writes: \u201cHFDT is not the only approach to developing transformative AI, and it may not work at all. But I take it very seriously, and I\u2019m aware of increasingly many executives and ML researchers at AI companies who believe something within this space could work soon.\u201d</p><p>In addition, we have also had discussions with at least one other senior AI safety researcher whom we respect and who thought human feedback was currently irrationally neglected by mainstream ML; they expected it to become much more wide-spread and to be a very powerful tool.</p><p>If that\u2019s right, then providing human feedback will likely become important and economically valuable.&nbsp;</p><p>This matters, because operating a new company in a growing industry is generally much easier and more likely to be successful. We think this is true even if profit isn\u2019t the main objective.</p><h2>Would we be accelerating capabilities?</h2><p>Our main idea was to found a company (or possibly non-profit) that served alignment researchers exclusively. That could accelerate alignment differentially.&nbsp;</p><p>One problem is that it\u2019s not clear where to draw this boundary. Some alignment researchers definitely think that other people who would also consider themselves to be alignment researchers are effectively doing capabilities work. This is particularly true of RLHF.</p><p>One mechanism worth taking seriously if we worked with big AI labs to make their models more aligned by providing higher quality data is that the models might merely appear surface-level aligned. \u201cMake the data higher quality\u201d might be a technique that scales poorly as capabilities ramp up. So it risks creating a false sense of security. It would also clearly improve the usefulness of current-day models and hence, it risks increasing investment levels too.</p><p>We don\u2019t currently think the risk of surface-level alignment is big enough to outweigh the benefits. In general, we think that a good first-order heuristic that helps the field stay grounded in reality would be that whatever improves alignment in current models is useful to explore further and invest resources into. It seems like a good prior that such things would also be valuable in the future (even if it\u2019s possible that new additional problems may arise, or such efforts aren\u2019t on the path to a future alignment solution). See Nate Soares\u2019 post about&nbsp;<a href=\"https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization\"><u>sharp left turns</u></a> to get a contradicting view on this.&nbsp;</p><h2>Is it more natural for this work to be done in-house in the longterm? Especially at big labs/companies.</h2><p>We expect that human data gathering is likely to become very important and that it benefits from understanding the relevant research agenda well. So maybe big companies will want to do this internally, instead of relying on third-party suppliers?&nbsp;</p><p>That seems quite plausible to us and to some extent it\u2019s happening already. Our understanding is that Anthropic is hiring an internal team to do human data gathering. DeepMind has access to Google\u2019s crowdworker service. OpenAI have worked with multiple companies, but they also have at least one in-house specialist for this kind of work and are advertising multiple further jobs on the human data team&nbsp;<a href=\"https://openai.com/careers/#human-data\"><u>here</u></a>. They\u2019re definitely considering moving more of this work in-house, but it\u2019s unclear to us to what extent that\u2019s going to happen and we have received somewhat contradicting signals regarding OpenAI safety team members\u2019 preferences on this.</p><p>So a new EA org would face stiff competition, not only from other external providers, but also from within companies.</p><p>Of course, smaller labs will most likely always have to rely on external providers. Hence,&nbsp;<strong>another cruxy consideration is how much small labs matter</strong>. Our intuition is that they matter much less than bigger labs (since the latter have access to the best and biggest models).</p><h2>Creating redundancy of supply and competition</h2><p>Even if existing companies are doing a pretty good job at serving the needs of alignment researchers, there\u2019s still some value in founding a competitor.&nbsp;</p><p>First,&nbsp;<strong>competition is good</strong>. Founding a competitor puts pressure on existing providers to keep service quality high, work on improving their products, and margins low. Ironically, part of the value of founding this company would thus flow through getting existing companies to try harder to offer the best product.</p><p>Second, it creates some redundancy.&nbsp;<strong>What if Surge pivots?</strong> What if their leadership changes or they become less useful for some other reason? In those worlds it might be especially useful to have a \u201cback-up\u201d company.</p><p>Both of these points have been mentioned to us as arguments in favour of founding this org. We agree that these effects are real and likely point in favour of founding the org. However,&nbsp;<strong>we don\u2019t think these factors carry very significant weight</strong> relative to our opportunity costs, especially given that there are already many start-ups working in this space.&nbsp;</p><p>Adding a marginal competitor can only affect a company\u2019s incentives so much. And in the worlds where we\u2019d be most successful such that all alignment researchers were working with us, we might cause Surge and others to pivot away from alignment researchers, instead of getting them to try harder.&nbsp;</p><p>The redundancy argument only applies in worlds in which the best provider ceases to exist; maybe that\u2019s 10% likely. And then the next best alternative is likely not all that bad. Competitors are plentiful and even doing it in-house is feasible. Hence, it seems unlikely to us that the expected benefit here is very large after factoring in the low probability of the best provider disappearing.</p><h1>Other lessons</h1><h2>Lessons on human data gathering</h2><p>In the process of talking to lots of experts about their experiences in working with human data, we learned many general lessons about data gathering. This section presents some of those lessons, in roughly decreasing order of importance.</p><h3>Iteration</h3><p>Many people emphasized to us that working with human data rarely looks like having a clean pipeline from requirements design to instruction writing to contractor finding to finished product. Rather, it more often involves a lot of iteration and testing, especially regarding what sort of data the contractors actually produce. While some of this iteration may be removed by having better contractors and better knowledge of good instruction-writing, the researchers generally view the iteration as a key part of the research process, and therefore prize&nbsp;</p><ul><li>ease of iteration (especially time to get back with a new batch of data based on updated instructions); and</li><li>high-bandwidth communication with the contractors and whoever is writing the instructions (often both are done by the researchers themselves).&nbsp;</li></ul><p>This last point holds to the point that it is somewhat questionable whether an external provider (rather than e.g. a new team member deeply enmeshed in the context of the research project) could even be a good fit for this need.</p><h3>The ideal pool of contractors</h3><p>All of the following features matter in a pool of contractors:</p><ul><li>Competence, carefulness, intelligence, etc. (sometimes expertise). It is often ideal if the contractors understand the experiment.</li><li>Number of contractors</li><li>Quick availability and therefore low latency for fulfilling requests</li><li>Consistent availability (ideally full-time)</li><li>Even distribution of contributions across contractors (ie it shouldn\u2019t be the case that 20% of the contractors provide 80% of the examples).&nbsp;</li></ul><h3>Quality often beats quantity for alignment research</h3><p>Many researchers told us that high-quality, high-skill data is usually more important and more of a bottleneck than just a high quantity of data. Some of the types of projects where current human data generation methods are most obviously deficient are cases where a dataset would need epistemically-competent people to make subtle judgments, e.g. of the form \u201chow true is this statement?\u201d or \u201chow well-constructed was this study?\u201d As an indication of reference classes where the necessary epistemic level exists, the researcher mentioned subject-matter experts in their domain, LessWrong posters, and EAs.</p><h3>A typical data gathering project needs UX-design, Ops, ML, and data science expertise&nbsp;</h3><p>These specialists might respectively focus on the following:</p><ul><li>Designing the interfaces that crowdworkers interact with. (UX-expert/front-end web developer)</li><li>Managing all operations, including hiring, paying, managing, and firing contractors, communicating with them and the researchers etc. (ops expert)</li><li>Helping the team make informed decisions about the details of the experimental design, while minimizing time costs for the customer. The people we spoke to usually emphasized ML-expertise more than alignment expertise. (ML-expert)</li><li>Meta-analysis of the data. e.g. inter-rater agreement, the distribution of how much each contractor contributed, demographics, noticing any other curious aspects of the data, etc. (data scientist)</li></ul><p>It is possible that someone in a team could have expertise in more than one of these areas, but generally this means a typical project will involve at least three people.</p><h3>Crowdworkers do not have very attractive jobs</h3><p>Usually the crowdworkers are employed as contractors. This means their jobs are inherently not maximally attractive; they probably don\u2019t offer much in the way of healthcare, employment benefits, job security, status etc. The main way that these jobs are made more attractive is through offering higher hourly rates.</p><p>If very high quality on high-skill data is going to become essential for alignment, it may be worth considering changing this, to attract more talented people.&nbsp;</p><p>However, we expect that it might be inherently very hard to offer permanent positions for this kind of work, since demand is likely variable and since different people may be valuable for different projects. This is especially true for a small organisation.&nbsp;</p><h3>What does the typical crowdworker look like?</h3><p>This varies a lot between projects and providers.</p><p>The cheapest are non-native English speakers who live outside of the US.</p><p>Some platforms, including Surge, offer the option to filter crowdworkers for things like being native English-speakers, expertise as a software engineer, background in finance, etc.</p><h2>Bottlenecks in alignment</h2><p>When asked to name the factors most holding back their progress on alignment, many alignment researchers mentioned talent bottlenecks.&nbsp;</p><p>The most common talent bottleneck seemed to be in competent ML-knowledgeable people. Some people mentioned the additional desire for these to understand and care about alignment. (Not coincidentally, Matt\u2019s next project is likely going to be about skilling people up in ML).</p><p>There were also several comments about things like good web development experience being important. For example, many data collection projects involve creating a user interface at some point, and in practice this is often handled by ML-specialised junior people at the lab, who can, with some effort and given their programming background, cobble together some type of website - often using different frameworks and libraries than the next person knows (or wants to use). (When asked about why they don\u2019t hire freelance programmers, one researcher commented that a key feature they\u2019d want is the same person working for them for a year or two, so that there\u2019s an established working relationship, clear quality assurances, and continuity with the choice of technical stack.)</p><h1>Conclusion</h1><p>After having looked into this project idea for about a month, we have decided not to found a human data gathering organisation for now.&nbsp;</p><p>This is mostly because demand for an external provider seems insufficient, as outlined in this&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org#Key_crux__demand_looks_questionable__Surge_seems_pretty_good\"><u>section</u></a>. No lab gave a clear signal that gathering human data was a key bottleneck for them, where they would have been willing to go to significant lengths to fix it urgently (especially not the ones that had tried Surge).&nbsp;</p><p>We expect that many labs would want to stick with their current providers, Surge in particular, or their in-house team, bar exceptional success on our part (even then, we\u2019d only provide so much marginal value over those alternatives).</p><p>Though we did find some opportunities for potential initial projects after looking for a month, we are hesitant about how far this company would be expected to scale. One of the main draws (from an impact perspective) of founding an organisation is that you can potentially achieve very high counterfactual impact by creating an organisation that scales to a large size and does lots of high-impact work over its existence. The absence of a plausible pathway to really outstanding outcomes from starting this organisation is a lot of what deters us.</p><p>In a world where we\u2019re more successful than expected (say 90th to 95th percentile), we could imagine that in five years from now, we\u2019d have a team of about ten good people. This team may be working with a handful of moderately big projects (about as big as WebGPT), and provide non-trivial marginal value over the next-best alternative to each one of them. Maybe one of these projects would not have been carried out without us.</p><p>A median outcome might mean failing to make great hires and remaining relatively small and insignificant: on the scale of doing projects like the ones we\u2019ve identified above, enough to keep us busy throughout the year and provide some value, but with little scaling. In that case we would probably quit the project at some point.</p><p>This distribution doesn\u2019t seem good enough to justify our opportunity cost (which includes other entrepreneurial projects or technical work among other things). Thus we have decided not to pursue this project any further for now.</p><p>We think this was a good idea to invest effort in pursuing, and we think we made the right call in choosing to investigate it. Both of us are open to, and also quite likely to, evaluate other EA-relevant entrepreneurial project ideas in the future.</p><h2>Other human data-gathering careers</h2><p>However,&nbsp;<strong>the assumption that high-quality high-skill human feedback is important and neglected by EAs has not been falsified</strong>.&nbsp;</p><p>It is still plausible to us that EAs should consider career paths that focus on building expertise at data-gathering; just probably not by founding a new company. In the short run, this could look like</p><ul><li>Contributing to&nbsp;<strong>in-house data-gathering teams</strong> (eg Anthropic, OpenAI, etc.)</li><li><strong>Joining Surge</strong> or other data-gathering startups.</li></ul><p>As we discussed above, the types of skills that seem most relevant for working in a human data generation role include: data science experience and in particular experience with natural languaga data or social science data and experiment design, front-end web development, ops and management skills, and some understanding of machine learning and alignment. 80,000 Hours recently wrote a profile which you can find&nbsp;<a href=\"https://80000hours.org/career-reviews/alignment-data-expert/\"><u>here</u></a>.</p><p>Of course, in the short term, this career path will be especially impactful if one\u2019s efforts are focussed on helping alignment researchers. But if it\u2019s true that human feedback will prove a very powerful tool for ML, then people with such expertise may become increasingly valuable going forward, such that it could easily be worth skilling up at a non-safety-focused org.&nbsp;</p><p>We think joining Surge may be a particularly great opportunity. It is common advice that joining young, rapidly growing start-ups with good execution is great for building experience; early employees can often get a lot of responsibility early on. See e.g. this&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ejaC35E5qyKEkAWn2/early-career-ea-s-should-consider-joining-fast-growing\"><u>post</u></a> by Bill Zito.</p><p>One of the hardest parts about that seems to be identifying promising startups. After talking to many of their customers, we have built reasonable confidence that Surge holds significant promise. They seem to execute well, in a space which we expect to grow. In addition to building career capital, there is clear value in helping Surge serve alignment researchers as well as possible.</p><p>From Surge\u2019s perspective, we think they could greatly benefit from hiring EAs, who are tuned in to the AI safety scene, which we would guess represents a significant fraction of their customers.&nbsp;</p><p>One senior alignment researcher told us explicitly that they would be interested in hiring people who had worked in a senior role at Surge.</p><h1>Next steps for us</h1><p>Matt is planning to run a bootcamp that will allow EAs to upskill in ML engineering. Rudolf is completing a computer science master\u2019s at Cambridge from October to June.</p><h1>Request for feedback, comments, etc.&nbsp;</h1><p>We would love to hear other people\u2019s thoughts on this matter and our reasoning. Comments are very welcome and much appreciated, including if you haven\u2019t read the whole report.&nbsp;</p>", "user": {"username": "LRudL"}}, {"_id": "Z2jTTR52fPLvaMgpX", "title": "Does participation in Effective Altruism predict success when applying to CEA?", "postedAt": "2022-09-27T18:34:13.851Z", "htmlBody": "<h1><strong>Summary</strong></h1><ul><li>We analysed recruitment data for 8 roles open in 2021 and 2022, covering 492 applicants.&nbsp;</li><li>Those who self-reported that they attended EA Global or read an EA book were somewhat more likely to advance in our recruitment processes.</li><li>However this relationship is noisy, and seems to recommend against hiring managers filtering strongly on self-reported engagement. Similarly, it suggests that people with some but limited EA engagement are not significantly disadvantaged in our hiring process.</li></ul><h1><strong>Introduction</strong></h1><p>In CEA\u2019s application form, we include a multiple choice question to ask applications about the extent to which they participate in effective altruism (EA). We are interested in whether any of these factors are good predictors of applicants progressing in the recruitment process.</p><p>We analysed recruitment data for 8 roles open in 2021 and 2022, covering 492 applicants. This post outlines these findings.</p><p><strong>In summary:</strong></p><ul><li><strong>When applicants participate in a larger quantity of EA activities, there is a slightly higher association with progression in the recruitment process</strong>, which is statistically significant (p&lt;0.001)</li><li><strong>When applicants have not participated in any of the named EA activities, they are significantly less likely to progress through any of the stages</strong>&nbsp;(p&lt;0.001).</li><li><strong>Applicants who passed an initial sift were more likely to;</strong><ul><li><strong>have read&nbsp;</strong><i><strong>Doing Good Better</strong></i><strong>,&nbsp;</strong><i><strong>The Most Good You Can Do</strong></i><strong>, or&nbsp;</strong><i><strong>The Precipice,</strong></i></li><li><strong>or have attended EA Global or another EA retreat.</strong></li><li>The associated odds ratios were 2.80 (p&lt;0.001) and 2.52 (p&lt;0.01) respectively.</li></ul></li><li><strong>Applicants who passed the screening interview and trial task were more likely to have attended EA Global or other EA retreat.&nbsp;</strong>The associated odds ratios were 2.56 (p&lt;0.05) for the screening interview and 2.26 (p&lt;0.05) for the trial task.</li><li>Surprisingly, recieving coaching from 80,000 Hours was not associated with significantly increased odds of advancement.</li></ul><h1><strong>Context</strong></h1><p>We looked at applications to the 8 roles listed in this footnote<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpe297j73xki\"><sup><a href=\"#fnpe297j73xki\">[1]</a></sup></span>. Applicants to the roles uploaded a CV and answered some basic questions. One question asked about applicants\u2019 participation in EA; they could select more than one option from the choices provided, which are detailed in this footnote<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6vx37k5ak15\"><sup><a href=\"#fn6vx37k5ak15\">[2]</a></sup></span>.&nbsp;</p><p>We have removed some records from this analysis:</p><ul><li>Some roles were open as an EOI for a short period, before being advertised fully as an open role<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyqzlwr4d9n\"><sup><a href=\"#fnyqzlwr4d9n\">[3]</a></sup></span>. The EOI form is different to the open role form; applicants to the EOI were not asked this multiple choice question about their participation in EA, so they are not included in this analysis.</li><li>We have also removed candidates whose applications were withdrawn soon after applying.</li></ul><h2>Base rates for success</h2><p>All applications first undergo a sift, in which the hiring manager chooses which applicants to invite to try further stages in the recruitment process.</p><p>In four of the roles the first stage was a screening interview and the second stage a trial task. In the other four roles these were reversed; candidates first attempted a trial task and then may have been invited to a screening interview.</p><p>When calculating base rates, we use as a denominator the number of people who participated in that stage.</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#f3f3f3;border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p><strong>Stage</strong></p><p><i>Half the roles had the screening interview first; the other half had the trial task first.</i></p></td><td style=\"background-color:#f3f3f3;border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p><strong>Number participating</strong></p></td><td style=\"background-color:#f3f3f3;border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p><strong>Number passing</strong></p></td><td style=\"background-color:#f3f3f3;border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p><strong>Success rate</strong></p></td></tr><tr><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\">Initial application sift</td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>492</p></td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>360</p></td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>44%</p></td></tr><tr><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\">Screening interview</td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>208</p></td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>86</p></td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>41%</p></td></tr><tr><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\">Trial task</td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>263</p></td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>62</p></td><td style=\"border:1pt solid #cccccc;padding:5pt;vertical-align:top\"><p>24%</p></td></tr></tbody></table></figure><h1><strong>Findings</strong></h1><p>We fitted logistic regression models to the data; with the dependent variable being whether a candidate passed a given stage, and the independent variables being their participation in each of the options outlined above.</p><p>We then calculated modelled odds ratios and probabilities associated with each \"predictor\". The results of this are shown below, with a&nbsp;data table in the appendix.</p><h2>Number of ways of participating</h2><p>We plot the number of ways in which applicants participate in EA against their outcome (1 for pass and 0 for not passing).</p><p>A logistic regression model suggests for each additional way in which applicants participate in EA, the odds ratio for success associated with each marginal way of participating in EA is:</p><ul><li>OR 1.21 for passing screening interview (p&lt;0.001)</li><li>OR 1.17 for passing the trial task (p&lt;0.001)</li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995489/mirroredImages/Z2jTTR52fPLvaMgpX/cgty3g4i3uphqokgbrwz.png\"></p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995489/mirroredImages/Z2jTTR52fPLvaMgpX/iuwtg6i4qobdv48bbak7.png\"></p><h2>Predictors for passing an initial sift</h2><p>This model predicts whether all submitted applicants (N=492) would pass an initial sift and be invited to a stage in the recruitment process, with sensitivity of 68% and specificity of 67%.</p><ul><li>Applicants were more likely to be invited to screening interview if they had;<ul><li><strong>read Doing Good Better, The Most Good You Can Do, or The Precipice</strong> (OR 2.80, p&lt;0.001)</li><li><strong>attended an EA Global conference or other EA retreat&nbsp;</strong>(OR 2.52 p&lt;0.01)</li></ul></li><li>Applicants were less likely to be invited to screening interview&nbsp;<strong>if they did not take part in any of these factors</strong>,&nbsp; as labelled by the \u201cintercept\u201d (OR 0.16, p&lt;0.001)</li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995489/mirroredImages/Z2jTTR52fPLvaMgpX/iujj7ybtiewxhnfbfawg.png\"></p><h2>Predictors for passing screening interview</h2><p>This model predicts whether all applicants who did not withdraw (N=492) would pass the screening interview. We count applicants who did not reach the screening interview as having not passed the interview either. This model has sensitivity of 79% and specificity of 66%.</p><ul><li>Applicants were more likely to pass the screening interview if they had&nbsp;<strong>attended EA Global or an EA summit retreat</strong> (OR 2.56, p&lt;0.05)</li><li>Applicants were less likely to pass the screening interview if:<ul><li><strong>they did not take part in any of these factors</strong>,&nbsp; as labelled by the \u201cintercept\u201d (OR 0.13, p&lt;0.001). The majority of this selective effect takes place before candidates are invited to the screening interview.</li><li><strong>they said they had chosen a job or degree program on the basis of EA principles&nbsp;</strong>(OR 0.43, p&lt;0.05).</li></ul></li></ul><p>Other predictors were not statistically significant.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995489/mirroredImages/Z2jTTR52fPLvaMgpX/fybytxluqt1t3t8mvszp.png\"></p><h2>Predictors for passing trial task</h2><p>This model predicts whether all applicants who did not withdraw (N=492) would pass the trial task. We count applicants rejected at screening interviews as not passing the trial task either. The model has sensitivity of 84% and specificity of 59%.</p><ul><li>Applicants were more likely to pass the trial task if they had<strong> attended EA Global or a similar EA retreat</strong> (OR 2.26, p&lt;0.05).</li><li>Applicants were less likely to pass the trial task if<ul><li><strong>they did not take part in any of these factors</strong>,&nbsp; as labelled by the \u201cintercept\u201d (OR 0.08, p&lt;0.001).</li><li><strong>they said they had chosen a job or degree program on the basis of EA principles&nbsp;</strong>(OR 0.39, p&lt;0.01).</li></ul></li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995489/mirroredImages/Z2jTTR52fPLvaMgpX/g4gqpbo24n3shevwa8gn.png\"></p><h1><strong>Commentary</strong></h1><p>By Ben West</p><ul><li>The base rates for success at trial task are imbalanced (with a low rate of success), which affects the models\u2019 sensitivity and specificity. We would advise some caution when interpreting these results.</li><li>The OR &lt; 1 and lack of significance from receiving 80,000 Hours coaching is surprising: 80,000 Hours is somewhat selective, and I (Ben) would have expected it to be a relatively strong positive predictor (if only because hiring managers falsely believe that it\u2019s a sign of the applicant\u2019s ability and therefore are biased towards advancing candidates who received coaching).&nbsp;</li><li>The OR &lt; 1 for choosing a job or degree program on the basis of EA principles fits with my anecdotal experience: people who aren't very involved in EA are likely to tick that box, whereas more highly engaged people have a higher threshold for \"on the basis of EA principles\", so this ends up having the reverse correlation that we intended.</li><li>I would be interested in a follow-up study which makes some of these metrics more precise. E.g. instead of \"I read the effective altruism forum regularly\" measure \"I read the effective altruism forum for more than one hour per week\". (Or better yet: actually measure the amount of time they spend on the forum without needing self-reported data.)</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpe297j73xki\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpe297j73xki\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We looked at applications to the following roles:<br><i>-&nbsp;</i>Community Liaison<br><i>-&nbsp;</i>EA Strategy Coordinator<br><i>-&nbsp;</i>EA Virtual Programs Operations Specialist<br><i>-&nbsp;</i>Executive Assistant and Groups Support<br><i>-&nbsp;</i>Full-Stack Engineer<br><i>-&nbsp;</i>Groups Associate (Scalable University Support)<br><i>-&nbsp;</i>Product Manager (EffectiveAltruism.org)<br><i>-&nbsp;</i>Project Coordinator</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6vx37k5ak15\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6vx37k5ak15\">^</a></strong></sup></span><div class=\"footnote-content\"><p>The question is <i>Participation in effective altruism: Do any of the following statements apply to you? These are examples of activities that some members of the effective community participate in. Feel free to add any others that may apply!</i><br><i>Choose as many as you like:</i><br><i>- I have used the 80,000 Hours career guide</i><br><i>-&nbsp;I have received 80,000 Hours 1-on-1 coaching</i><br><i>-&nbsp;I attended an EA Global conference or a similar EA summit retreat</i><br><i>-&nbsp;I donate some percentage of my income to effective charities</i><br><i>-&nbsp;I have donated or pledged to donate 10 percent of my annual income to EA causes</i><br><i>-&nbsp;I have spent 4 hours per week on EA organizing or other EA projects</i><br><i>-&nbsp;I have participated in an EA group</i><br><i>-&nbsp;I have led an EA group</i><br><i>-&nbsp;I have read Doing Good Better, The Most Good You Can Do, or The Precipice</i><br><i>-&nbsp;I have chosen a job or degree program on the basis of EA principles</i><br><i>-&nbsp;I read the Effective Altruism Forum regularly</i><br>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyqzlwr4d9n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyqzlwr4d9n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Expressions of Interest (EOIs) represent roles which CEA is interested in eventually hiring for, but are not actively focused on at present. The website signposts that these roles have a higher bar for progression, and that CEA carries out much less outreach for them, but aims to make applying as easy as possible.</p></div></li></ol>", "user": {"username": "Akara"}}, {"_id": "PkqQnGDjKNHkuu32C", "title": "Which EAs should be bigger on Twitter? Upvote those who are top priorities.", "postedAt": "2022-09-27T16:41:56.465Z", "htmlBody": "<p>Who ought to have a larger twitter account and it wouldn't be much effort for us to make it happen?<br><br>I think were this numerical, we'd do: (value of them having bigger account) * (difficulty to get there)/(GiveWell charities)</p><p>&nbsp;Use upvotes to signal &nbsp;the priority of the answer, and the agree/disagree to support the specific reasoning given by the answer.</p><p>IE the upvote ordering should be the correct ordering.<br><br>Ideally we'd not have anyone named who doesn't want to be, so if you are a bit in doubt, then ask their permission to name them.<br><br>Starter questions:</p><ul><li>Who would be easy for outsiders to engage with and understand?</li><li>Who produces great content you wish you saw more of?</li><li>Who deserves the ability to more easily influence discourse and meet with members of the cultural elite?</li><li>Who has cultural cache in a different space that would allow them to grow quickly</li></ul><p>This question is to give a sense of who ought to be supported/invested in. I think that it should be followed by a strategic look and finally funding to those individuals named. I am not authorised to do any of that.</p>", "user": {"username": "nathan"}}, {"_id": "jCgZzxsabngpnzhLQ", "title": "The Onion Test for Personal and Institutional Honesty", "postedAt": "2022-09-27T15:26:34.847Z", "htmlBody": "", "user": {"username": "ChanaMessinger"}}, {"_id": "xTNrsFLvaMBu3rFZZ", "title": "CCTV cameras in slaughterhouses: Modest benefits for animal welfare", "postedAt": "2022-09-27T14:50:31.232Z", "htmlBody": "<p><i>We conducted this research on behalf of Equalia to evaluate the impact of their campaign to establish CCTV cameras in slaughterhouses. More information about Equalia's campaign is available </i><a href=\"https://en.equaliaong.org/ley-camaras-matadero\"><i><u>here on their website</u></i></a><i>.</i></p><h2><br><strong>SUMMARY</strong></h2><p>Due to the desire to reduce animal welfare violations, <strong>CCTV cameras have been installed in slaughterhouses</strong> in a number of jurisdictions around the world. This has been driven by legal requirements (e.g. England, Israel), agreements between industry and government (e.g. the Netherlands), or retailer requirements (e.g. United States).</p><p>There have been <strong>no studies testing whether CCTV cameras actually deter violations</strong> of <strong>animal welfare regulations</strong> in slaughterhouses. Until a scientific study calculates the magnitude of the effect of CCTV on compliance with animal welfare regulations, it is difficult to be certain about the exact magnitude of CCTV's impact. However, anecdotal reports from slaughterhouse employees and government officers <strong>consistently indicate that CCTV cameras do deter animal welfare violations</strong>. This hypothesis is also supported by studies on the effect of CCTV on reducing crime in other contexts. Furthermore, animal welfare violations before and during slaughter risk causing <strong>suffering that is extreme</strong>.</p><p>The slaughterhouse only represents a <strong>very small proportion of an animal's life</strong>. Even if CCTV cameras do deter animal welfare violations, under certain moral worldviews this may only result in a <strong>small welfare improvement</strong> when considered across an animal's entire lifespan. Additionally, CCTV can only reduce existing non-compliance which does <strong>not affect every anim</strong></p><p><strong>In conclusion, we find that campaigning for CCTV in slaughterhouses is likely to do less good for animals than other campaign options</strong>, even if CCTV is likely to reduce non-compliance with animal welfare regulations.</p><h2><br><strong>BACKGROUND TO CCTV CAMERAS IN SLAUGHTERHOUSES</strong></h2><p>In many countries around the world, there are laws that aim to protect the welfare of farmed animals during slaughter. For any welfare regulation to improve the lives of animals in practice, appropriate monitoring and enforcement measures need to be put in place. To monitor the welfare of animals during slaughter, one policy that is often proposed is to install CCTV in slaughterhouses. Under this policy, it is hypothesised that CCTV can deter slaughterhouse owners and employees from violating animal welfare regulations.</p><p>There has been no scientific study on whether CCTV in slaughterhouses can deter animal welfare violations in practice. However, there have been many studies on CCTV and reducing crime in other contexts. Research on CCTV and crime in general is grounded in the rational choice perspective (1,2). This is a perspective that emphasises characteristics of the environment, rather than the individual, when an individual is evaluating whether or not to commit a crime. Here, 'rational' does not suggest that people who commit animal welfare violations are perfectly rational. Rather, the term means that potential offenders nevertheless evaluate the potential outcomes of committing a crime, no matter how rapid or bounded that evaluation may be. The rational choice perspective still applies to crimes committed in anger or other intense emotions.</p><p>The rational choice perspective describes potential offenders as making a decision based on the effort, risk of detection, potential rewards, provocations, and excuses associated with a crime opportunity (1). CCTV emphasises the \u2018risk of detection\u2019, particularly when the CCTV cameras are visible and obvious and/or when crimes are rapidly followed by a response by authorities. The other components in that list may provide insight into preventing animal welfare violations through other means (see final paragraph of report).</p><h2><br><strong>CCTV IN SLAUGHTERHOUSES AROUND THE WORLD</strong></h2><p>Around the world, there are a handful of jurisdictions that have either placed CCTV cameras in slaughterhouses or have considered doing so.</p><h2><strong>England</strong></h2><p>In the United Kingdom (UK), the welfare of farmed animals is under the jurisdiction of the devolved governments (England, Scotland, Wales, and Northern Ireland). Each of these governments has a different policy on CCTV in slaughterhouses.</p><p>In England, the use of CCTV in slaughterhouses is mandated by law. This has been the case since 2018, when the Mandatory Use of Closed Circuit Television in Slaughterhouses (England) Regulations 2018 was established. It mandates that footage must be held for 90 days, and inspectors have 'unrestricted access to footage' (3). However, a report written before the regulations were established concluded that the review of footage is likely to be 'intermittent, selective and periodic', which may reduce the value of the footage for deterring animal welfare offences (4). In England, it is accepted that CCTV offers many benefits beyond animal welfare, such as security, training, workplace safety, and auditing and evaluation of facilities (4).</p><p>In Scotland, the use of CCTV in slaughterhouses is also mandated by law. This has been the case since 2021, when the Mandatory Use of Closed Circuit Television in Slaughterhouses (Scotland) Regulations 2020 was established. Similarly to the mandate in England, footage must be held for 90 days and can be accessed by inspectors (5).</p><p>In Wales, although it is not mandatory to use CCTV cameras in slaughterhouses, many slaughterhouses voluntarily use them. 90% of red meat animals and 97% of poultry animals are killed in slaughterhouses that use CCTV (6). The use of CCTV in slaughterhouses in Wales is required by some certification schemes and is encouraged through government grants (7).</p><p>In Northern Ireland, it is also not mandatory to use CCTV cameras in slaughterhouses, but many slaughterhouses voluntarily use them and 99% of slaughtered animals are covered by CCTV (4).</p><h2><strong>Israel</strong></h2><p>In Israel, the use of CCTV in slaughterhouses is mandated by law. This requirement has been in place since the Ministry of Agriculture ordered, in late 2015, CCTV cameras to be installed in slaughterhouses throughout 2016 (8).</p><p>Israel's policy is notable in that camera footage is transmitted live to a central control room at the Ministry of Agriculture (8). This might mean that the CCTV cameras act as a stronger deterrent against violations of animal welfare laws. Indeed, in Israel, a primary justification for the Ministry's decision to install CCTV cameras was the purported ability of CCTV to deter violations of animal welfare laws.</p><h2><strong>Spain</strong></h2><p>In Spain, the use of CCTV in slaughterhouses has recently become mandated by law. There is currently a transition period in place to allow slaughterhouses to adapt to this royal decree, lasting one year for large slaughterhouses and two years for small slaughterhouses (9).<br>&nbsp;</p><p>Under the royal decree (10), slaughterhouses must install a CCTV system that covers areas where live animals are kept (with a specific clause stating that blind spots are not permitted). The footage is owned by the slaughterhouse operator and must be kept for one month. The footage is subject to reviews by government inspectors periodically, when non-compliance is detected, or when there is suspicion of non-compliance. The decree has specific clauses to ensure that the CCTV systems are compliant with EU and Spanish laws on workers' privacy.</p><h2><strong>The Netherlands</strong></h2><p>In the Netherlands, it is not mandatory to use CCTV cameras in slaughterhouses. However, the government and the slaughterhouse industry reached an agreement where all large and medium slaughterhouses installed CCTV cameras voluntarily (11). These negotiations between government and industry began in 2017, and the first warnings and fines based on camera footage were issued in 2020 (12).</p><p>In the Netherlands, CCTV is used for camera inspections on a monthly or twice-monthly basis. During a camera inspection, an inspector looks at around two hours of footage from different days and points around the slaughterhouse (12). Since the CCTV cameras are installed voluntarily rather than as a legal requirement, there are a few weaknesses with the systems. For example, inspectors can only access images during physical camera inspections and not remotely; the images remain the property of the slaughterhouses, and the slaughterhouse owners, not government officers, decide where to place the cameras (12).</p><h2><strong>Germany</strong></h2><p>In the German state of Lower Saxony in June 2020, an agreement was reached between meat and trade associations, the Lower Saxony association of cities and towns, and the Lower Saxony Ministry of Food, Agriculture and Consumer Protection (13,14). This was a voluntary agreement that set out guidelines for the use of CCTV in slaughterhouses, with no legal requirements. Some employees lodged official complaints relating to data protection (13,14), although many slaughterhouses in Lower Saxony continue to use CCTV today (15). As of 2022, stakeholders in Lower Saxony are developing recommendations for best practice methods relating to CCTV in slaughterhouses (15,16).</p><p>The coalition agreement of the federal German government, reached in 2021, states that the government aims to create a legal right for the implementation of CCTV in slaughterhouses above a particular size (17). This indicates that CCTV may become a legal requirement in slaughterhouses across Germany in the next few years.</p><h2><strong>United States</strong></h2><p>In the United States (US), CCTV is not legally required in slaughterhouses. Nevertheless, most large slaughterhouses have used CCTV systems for over a decade (18). As of 2018, Arrowsight (a company that audits CCTV footage from slaughterhouses) audited 40% of North America's chicken market, 38% of the pig market, 57% of the cow market, and 68% of the turkey market (18). Since there is likely CCTV footage that is not audited or is audited by other companies, the coverage of CCTV in US slaughterhouses may be even greater than those percentages.</p><h2><strong>France</strong></h2><p>In France, a 2018 law provided for a two-year experiment in which slaughterhouses could voluntarily install CCTV. By February 2020, only three slaughterhouses out of the 934 across France had installed CCTV (19). This voluntary experiment did follow the failure of a bill that would have made CCTV legally mandatory in slaughterhouses (20).</p><p>&nbsp;</p><h2><strong>EVIDENCE ON CCTV AND CRIME PREVENTION</strong></h2><h2><strong>CCTV and Deterrence of Crime in Slaughterhouses</strong></h2><p>There have been no high-quality, empirical studies on whether CCTV cameras improve compliance with animal welfare laws in slaughterhouses. An analysis by the UK Government in 2011 using data on compliance rates found that CCTV cameras do not improve compliance (21). However, the specifics of this analysis are not explained in detail, and the analysis appears rudimentary. The governments of the UK and the Netherlands do publish data on slaughterhouse inspection and compliance rates, but this data is not in a format suitable for conducting an analysis on the effects of CCTV. A government representative from the Netherlands told us that a pilot study found that the probability of detection is ~75 times higher in slaughterhouses with CCTV, although there is no data on whether this translates to higher compliance with animal welfare regulations. However, since probability of detection is a significant factor in the rational choice perspective of crime deterrence, using CCTV is likely to translate to higher compliance.</p><p>When asked to give anecdotal evidence, slaughterhouse employees and government officials consistently claim that CCTV cameras do seem to improve compliance with animal welfare laws in slaughterhouses. Ten slaughterhouse employees in the US surveyed by Wigham all reported that CCTV improves animal welfare at slaughter, as long as CCTV footage is audited and staff are held accountable for their actions (18). Moreover, those employees reported that 'the number of misconducts had decreased since the start of CCTV auditing', and that when the quality assurance team member (who is responsible for auditing the footage) is away from the slaughterhouse, the number of non-compliances increases (18). Similar anecdotal evidence from the US is reported by Edwards-Callaway (22). We spoke with government officials from countries where CCTV has been mandatory or widespread, and it was consistently reported that CCTV appears to deter animal welfare violations in slaughterhouses.</p><p>The UK government is required, as part of the 2018 regulations that established mandatory CCTV in slaughterhouses, to publish a review of the CCTV mandate. This review is required to be published before 4 May 2023. That review must 'assess the extent to which [the] objectives are achieved'. Therefore, there is a good chance that this review will involve analysing data on whether CCTV does deter animal welfare violations in the UK.</p><p>In the jurisdictions that have installed CCTV, officials generally state that CCTV can augment existing inspection and compliance regimes but not replace those regimes (4,6,8).</p><p>As noted above, CCTV surveillance in slaughterhouses is believed to have the greatest deterrent effect when non-compliances are rapidly detected and addressed. However, constantly monitoring the footage produced by every camera in every slaughterhouse is not feasible, so most of the footage is not viewed. Increasingly, the monitoring of slaughterhouse footage is conducted using artificial intelligence (AI)<a href=\"https://www.animalask.org/post/cctv-cameras-in-slaughterhouses-modest-benefits-for-animal-welfare#viewer-e1tfr\"><u>\u00b9</u></a>. This was the motivation behind the AI4Animals (Artificial Intelligence for Animals) system, developed jointly for use in the Netherlands by animal welfare organisations, a meat company, and Deloitte. AI4Animals uses an algorithm to detect video segments that are likely to contain animal welfare violations and flags them for review by humans (24).</p><p>CCTV may also improve animal welfare in slaughterhouses beyond the deterrence of welfare violations. CCTV can be used for training and improving processes in the slaughterhouse both proactively and after welfare incidents that do occur.</p><h2><strong>CCTV and Crime in General</strong></h2><p>When it comes to the effects of CCTV on crime in general, the strongest piece of evidence is the 2019 systematic review and meta-analysis by Piza et al (2). The meta-analysis included 76 studies conducted over 40 years and constitutes an update of previous meta-analyses. This study found that 'CCTV is associated with a significant and modest decrease in crime'. However, the effects of CCTV depended on the setting, the type of monitoring, and the type of crime. The strongest evidence for the effects of CCTV were observed in car parks and residential areas, with weaker evidence found in other settings. The largest effect was observed when CCTV cameras were actively monitored and when there were multiple interventions alongside this. However, though CCTV was associated with reductions for vehicle crime and property crime, there were no significant effects for violent crime. Notably, significant effects for violent crime have been detected in individual studies due to particular details of how CCTV is implemented (e.g. rapid responses to violations, see below).</p><p>Another useful piece of evidence is the 2017 narrative review by Alexandrie (25). Alexandrie was concerned by threats to causal validity in the literature on CCTV. As such, their review selectively focused on randomised experiments and natural experiments, and it included seven studies overall. This review also concluded that CCTV reduces crime in some settings but not others. There were reductions in crime on public streets and at urban subway stations, but no evidence of crime reductions in consumer parking facilities or suburban subway stations.</p><p>One expert we spoke to pointed out that most studies on CCTV (including the meta-analysis) focus on public settings. CCTV in private settings, such as retail stores, might be more relevant to CCTV in slaughterhouses. The strongest piece of evidence relating to CCTV in private settings is a 2011 randomised control trial by Hayes and Downs (1). This study tested the effect of two types of CCTV cameras (in-aisle CCTV public view monitors and in-aisle CCTV domes) on the rate of shoplifting of razor blades, a valuable, frequently shoplifted item. The results showed that both types of CCTV cameras caused a notable and statistically significant decrease in the rate of shoplifting. The authors suggest that this decrease occurred because the CCTV cameras increased the perceived risk of detection.</p><p>Another expert provided evidence that CCTV cameras would only serve as a deterrent if they are backed up by rapid responses to violations. The efficacy of any deterrent hinges on the certainty of punishment (26,27). (This matches the anecdotal experiences reported by US slaughterhouse employees, above.) In one randomised control trial in New Jersey, scientists tested whether rapid responses to violations observed on CCTV cameras could cause a decrease in the crime rate (27). This study involved CCTV schemes that had been implemented to address violent crime, social disorder, and narcotics activity. The study did find that rapid responses to violations caused the crime rate to decrease, although the effects were larger for violent crime and social disorder than for narcotics activity.</p><p>Overall, the general crime literature provides both empirical evidence that CCTV can prevent crime and a theoretical explanation for why it does. There is, notably, variation across studies; the strongest effects seem to be observed in CCTV interventions that are well-implemented, such as those that are accompanied by a rapid response. The theoretical explanation offered for CCTV's preventative effect (increasing the rate of perceived detection) is general, so we expect that this mechanism would also apply to the context of slaughterhouses. The crime experts we spoke to agree with this claim. As such, this body of literature offers evidence that well-designed CCTV interventions would indeed prevent animal welfare violations in slaughterhouses.</p><h2><strong>SLAUGHTERHOUSES ONLY COVER A SHORT PART OF ANIMALS' LIVES</strong></h2><p>The slaughterhouse only represents a very small proportion of an animal's life. The evidence outlined in this report does indicate that CCTV could improve animal welfare in the slaughterhouse, but even a moderate improvement in animal welfare - such as the improvement that would arise if CCTV deters animal welfare violations in the slaughterhouse - would represent only a small improvement in the welfare of an animal across their lifespan. Animal welfare improvements that instead apply for a longer period of an animal's lifespan, such as welfare reforms on the farm, may be more impactful than CCTV in slaughterhouses.</p><p>Admittedly, animal welfare violations before and during slaughter risk causing suffering that is extreme. Philosophical worldviews that focus heavily on preventing extreme suffering may therefore place a greater importance on deterring animal welfare violations using CCTV in slaughterhouses opposed to welfare reforms on the farm. However, when we are faced with decisions that pull us between different worldviews, an effective choice is one that looks robustly impactful under many philosophical worldviews. CCTV in slaughterhouses may look effective from the perspective of extreme suffering-focused worldviews, but not other worldviews. As such, campaigning for CCTV in slaughterhouses is less effective than other campaign options, including campaigns that improve welfare across animals' lifespans.</p><p>This raises the question of whether CCTV could be installed not just in slaughterhouses, but also on farms. However, we spoke to one expert who pointed out that advocating for CCTV cameras on farms may be more difficult. Farmers often live on, or have closer personal relationships with, their farms, so the proposal to install CCTV on farms may trigger more sensitive emotions within the farming industry. CCTV on farms would also result in a very high volume of data that may require either large teams or artificial intelligence to audit appropriately.</p><h2><strong>CAMPAIGNING FOR CCTV IN SLAUGHTERHOUSES</strong></h2><p>If an organisation does choose to campaign for the installation of CCTV in slaughterhouses, there are several lessons that can be drawn from the experiences of other countries.</p><p>Publishing images of cruelty from undercover investigations could be an indispensable strategy for building public and political support for CCTV in slaughterhouses. In all cases, proposals for CCTV in slaughterhouses gained political and public traction following the media publication of imagery showing animal cruelty obtained in independent investigations by animal advocacy organisations (11,28\u201331).</p><p>Encouraging some slaughterhouses to install CCTV cameras, perhaps through pressure from certification schemes or retailers, might make it easier to subsequently make CCTV a legal requirement across the industry. In England, CCTV cameras were only required by law after a large proportion of slaughterhouses had already installed them. The use of CCTV in slaughterhouses was required by most major food retailers and by RSPCA's Freedom Food certification scheme, and it was recommended by other certification schemes and associations (4). In 2010, around 7% of slaughterhouses had installed CCTV (4). This number increased gradually until 2016, when around 50% of red meat slaughterhouses and 70% of poultry slaughterhouses had adopted CCTV for animal welfare purposes (29). Requirements by certification schemes and retailers encouraged slaughterhouses, particularly those belonging to larger businesses, to adopt CCTV, which in turn may have reduced opposition to the subsequent legal regulations. In fact, when large producers voluntarily adopt new animal welfare practices, those large producers often lobby the government to make those practices legally binding to avoid other producers having an economic advantage (32).</p><p>There are also dangers that installing CCTV in slaughterhouses can rebound. Slaughterhouse owners in the Netherlands have argued that, given that CCTV cameras have been installed, companies that perform well on inspections should have fewer physical inspections (12). In contrast, stakeholders in the UK and the US maintain that CCTV cannot replace physical inspections (18,29). Separately, one expert we interviewed pointed out that slaughterhouse employees may commit deliberate acts of animal cruelty in the cameras' blind spots, and this did indeed occur in the Netherlands (33). This highlights the need to ensure that CCTV interventions are well-designed (discussed in 'CCTV and crime in general', above).</p><p>The experts we consulted spoke about the critical importance of open communication between the government and the industry. When implementing CCTV in slaughterhouses, it is essential for the government to communicate early and often with slaughterhouse owners to help address any concerns and overcome any challenges. It may also be beneficial to replicate the small grant scheme established by the government in Wales, which helped address complaints by smaller businesses that the costs of purchasing and maintaining CCTV systems would be prohibitive (6). The impact analysis conducted by the UK government could also help to address such complaints (34).</p><p>Lastly, it is important to ensure that CCTV cameras installed in slaughterhouses do not infringe upon the rights of employees in terms of data protection (13,14).</p><p>Beyond CCTV, there are other ways that animal welfare violations in slaughterhouses may be reduced. Research has found that cruelty to farm animals can be associated with poor work environments or workers who are abused themselves (35,36). Under the rational choice perspective of crime (see above), increasing surveillance is one of several avenues by which crime may be prevented. Another is reducing provocation, which may involve reducing frustrations and stress, avoiding disputes, reducing emotional arousal, neutralising peer pressure, and/or discouraging imitation (1,37). Given the link between poor work environments and animal welfare violations, improving working conditions and workers' wellbeing may be an additional way to reduce animal welfare violations in slaughterhouses.</p><p>&nbsp;</p><h2><strong>CONCLUSIONS</strong></h2><p>When it comes to deterring violations of animal welfare regulations, one policy option is to install CCTV in slaughterhouses. Although there have been no direct academic studies on whether CCTV can deter welfare violations in slaughterhouses, anecdotal evidence and studies on CCTV\u2019s effect on other types of crime do suggest that this policy may be effective in reducing non-compliance. However, the slaughterhouse only accounts for a small proportion of a farmed animal's lifespan. Unless one adopts a philosophical worldview that is focused on extreme suffering, we believe that this policy would be less impactful than welfare reforms that benefit animals throughout their lives.</p><p><br>&nbsp;</p><h2><strong>NOTES</strong></h2><p>1. Notably, the application of AI in animal agriculture presents many risks to the lives of animals - we are speculating here, but it may be the case that farmers, companies and/or governments that find value in applying AI to a particular area of farm operations may be more predisposed to applying AI to other areas of farm operations. This might contribute to the risk that AI makes animal agriculture more cost-effective and thus hinders the future success of the animal advocacy movement (23). This risk is worth flagging, but the scenario we outline is merely one possibility.</p><p><br>&nbsp;</p><h2><strong>REFERENCES</strong></h2><p>1. Hayes R, Downs DM. Controlling retail theft with CCTV domes, CCTV public view monitors, and protective containers: A randomized controlled trial. Secur J. 2011 Jul 1;24(3):237\u201350.</p><p>2. Piza EL, Welsh BC, Farrington DP, Thomas AL. CCTV surveillance for crime prevention. Criminol Public Policy. 2019 Feb;18(1):135\u201359.</p><p>3. Department for Environment, Food and Rural Affairs. CCTV becomes mandatory in all abattoirs in England [Internet]. GOV.UK. 2018. Available from: <a href=\"https://www.gov.uk/government/news/cctv-becomes-mandatory-in-all-abattoirs-in-england\">https://www.gov.uk/government/news/cctv-becomes-mandatory-in-all-abattoirs-in-england</a></p><p>4. Farm Animal Welfare Committee. Opinion on CCTV in slaughterhouses [Internet]. Farm Animal Welfare Committee; 2015. Available from: <a href=\"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/400796/Opinion_on_CCTV_in_slaughterhouses.pdf\">https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/400796/Opinion_on_CCTV_in_slaughterhouses.pdf</a></p><p>5. Scottish Government. Animal slaughter: guidance on the use of mandatory CCTV [Internet]. gov.scot. 2021. Available from: <a href=\"https://www.gov.scot/publications/animal-slaughter-guidance-on-the-use-of-mandatory-cctv/pages/power-to-inspect-and-seize/\">https://www.gov.scot/publications/animal-slaughter-guidance-on-the-use-of-mandatory-cctv/pages/power-to-inspect-and-seize/</a></p><p>6. The Safeguarding Animal Welfare at Slaughter Task and Finish Group. The Need for and Possible Implementation of a Workable System of CCTV in All Slaughterhouses in Wales [Internet]. 2016. Available from: <a href=\"https://gov.wales/sites/default/files/publications/2018-01/cctv-in-slaughterhouses-report.pdf\">https://gov.wales/sites/default/files/publications/2018-01/cctv-in-slaughterhouses-report.pdf</a></p><p>7. Welsh Government. \u00a31.1m grant aid scheme for small and medium size slaughterhouses [Internet]. Llywodraeth Cymru Welsh Government. 2018. Available from: <a href=\"https://gov.wales/ps11m-grant-aid-scheme-small-and-medium-size-slaughterhouses\">https://gov.wales/ps11m-grant-aid-scheme-small-and-medium-size-slaughterhouses</a></p><p>8. Ministry of Agriculture and Rural Development. The Ministry of Agriculture is ordering the installation of cameras in abattoirs and slaughterhouses that will broadcast live to a Ministry control room [Internet]. gov.il. 2015. Available from: <a href=\"https://www.gov.il/en/Departments/news/cameras-in-slaughterhouses\">https://www.gov.il/en/Departments/news/cameras-in-slaughterhouses</a></p><p>9. Ministerio de Consumo. Espa\u00f1a ser\u00e1 el primer pa\u00eds de la UE con sistemas de videovigilancia obligatorios en los mataderos [Spain will be the first EU country with mandatory video surveillance systems in slaughterhouses] [Internet]. Ministerio de Consumo. 2022. Available from: <a href=\"https://www.consumo.gob.es/es/carrousel/espa-ser-el-primer-pa-s-de-la-ue-con-sistemas-de-videovigilancia-obligatorios-en-los\">https://www.consumo.gob.es/es/carrousel/espa-ser-el-primer-pa-s-de-la-ue-con-sistemas-de-videovigilancia-obligatorios-en-los</a></p><p>10. Real Decreto 695/2022, de 23 de agosto, por el que se establecen medidas para el control del bienestar de los animales en los mataderos mediante la instalaci\u00f3n de sistemas de videovigilancia. [Royal Decree 695/2022, of August 23, which establishes measures to control the welfare of animals in slaughterhouses through the installation of video surveillance systems.] [Internet]. 2022. Available from: <a href=\"https://www.boe.es/eli/es/rd/2022/08/23/695\">https://www.boe.es/eli/es/rd/2022/08/23/695</a></p><p>11. Dutch News. Dutch slaughterhouses agree on camera surveillance after Belgian scandal [Internet]. Dutch News. 2017. Available from: <a href=\"https://www.dutchnews.nl/news/2017/05/dutch-slaughterhouses-reach-deal-on-camera-surveillance/\">https://www.dutchnews.nl/news/2017/05/dutch-slaughterhouses-reach-deal-on-camera-surveillance/</a></p><p>12. Nederlandse Voedsel- en Warenautoriteit. Cameratoezicht dierenwelzijn in slachthuizen [Camera surveillance of animal welfare in slaughterhouses] [Internet]. Ministerie van Landbouw, Natuur en Voedselkwaliteit; 2021. Available from: <a href=\"https://www.nvwa.nl/onderwerpen/cameratoezicht-in-slachthuizen/documenten/dier/dierenwelzijn/slachthuizen/publicatie/cameratoezicht-dierenwelzijn-in-slachthuizen\">https://www.nvwa.nl/onderwerpen/cameratoezicht-in-slachthuizen/documenten/dier/dierenwelzijn/slachthuizen/publicatie/cameratoezicht-dierenwelzijn-in-slachthuizen</a></p><p>13. Datenschutz Agentur. Video\u00fcberwachung in Schlachth\u00f6fen [Video surveillance in slaughterhouses] [Internet]. Datenschutz Agentur. 2021. Available from: <a href=\"https://datenschutz-agentur.de/blog/videoueberwachung-in-schlachthoefen/\">https://datenschutz-agentur.de/blog/videoueberwachung-in-schlachthoefen/</a></p><p>14. Grabmeier A. Schlachth\u00f6fe: Pflicht zur Video\u00fcberwachung gescheitert [Slaughterhouses: Obligation for video surveillance failed] [Internet]. Agrarheute. 2020. Available from: <a href=\"https://www.agrarheute.com/tier/schlachthoefe-pflicht-videoueberwachung-gescheitert-568100\">https://www.agrarheute.com/tier/schlachthoefe-pflicht-videoueberwachung-gescheitert-568100</a></p><p>15. Personal communication, Press Office, Lower Saxony Ministry of Agriculture. 2022.</p><p>16. Government of Niedersachsen. Projektgruppe Schlachten und T\u00f6ten [Internet]. Niedersachsen. 2022. Available from: <a href=\"https://www.ml.niedersachsen.de/startseite/themen/tiergesundheit_tierschutz/niedersachsische_nutztierstrategie_tierschutzplan_4_0/projektgruppe-schlachten-und-toten-214060.html\">https://www.ml.niedersachsen.de/startseite/themen/tiergesundheit_tierschutz/niedersachsische_nutztierstrategie_tierschutzplan_4_0/projektgruppe-schlachten-und-toten-214060.html</a></p><p>17. Sozialdemokratischen Partei Deutschlands, B\u00dcNDNIS 90 / DIE GR\u00dcNEN, and Freien Demokraten. Mehr Fortschritt wagen: B\u00fcndnis f\u00fcr Freiheit, Gerechtigkeit und Nachhaltigkeit [Internet]. 2021. Available from: <a href=\"https://www.bundesregierung.de/resource/blob/974430/1990812/04221173eef9a6720059cc353d759a2b/2021-12-10-koav2021-data.pdf?download=1\">https://www.bundesregierung.de/resource/blob/974430/1990812/04221173eef9a6720059cc353d759a2b/2021-12-10-koav2021-data.pdf?download=1</a></p><p>18. Wigham EE. The impact of Animal Welfare Training at slaughter on animal welfare, personnel attitudes and product quality [Doctor of Philosophy]. University of Bristol; 2019.</p><p>19. Saint-Sevin J. Bien-\u00eatre animal : pourquoi faut-il mettre des cam\u00e9ras dans les abattoirs ? [Animal welfare: why should cameras be installed in slaughterhouses?] [Internet]. franceinfo. 2021. Available from: <a href=\"https://france3-regions.francetvinfo.fr/normandie/calvados/bien-etre-animal-pourquoi-faut-il-mettre-des-cameras-dans-les-abattoirs-2298865.html\">https://france3-regions.francetvinfo.fr/normandie/calvados/bien-etre-animal-pourquoi-faut-il-mettre-des-cameras-dans-les-abattoirs-2298865.html</a></p><p>20. Chopin L. Question de campagne: Faut-il mettre des cam\u00e9ras dans tous les abattoirs? [Campaign Question Should cameras be installed in all slaughterhouses?] [Internet]. Lib\u00e9ration. 2018. Available from: <a href=\"https://www.liberation.fr/environnement/agriculture/faut-il-mettre-des-cameras-dans-tous-les-abattoirs-20220330_U6YTJMVAP5AU3PYCU7QGTNKXPA/?redirected=1\">https://www.liberation.fr/environnement/agriculture/faut-il-mettre-des-cameras-dans-tous-les-abattoirs-20220330_U6YTJMVAP5AU3PYCU7QGTNKXPA/?redirected=1</a></p><p>21. Rhodes A. Results of the 2011 FSA Animal Welfare Survey in Great Britain [Internet]. Food Standaards Agency; 2012. Available from: <a href=\"https://webarchive.nationalarchives.gov.uk/ukgwa/20131206121215/http://food.gov.uk/multimedia/pdfs/board/fsa120508.pdf\">https://webarchive.nationalarchives.gov.uk/ukgwa/20131206121215/http://food.gov.uk/multimedia/pdfs/board/fsa120508.pdf</a></p><p>22. Edwards-Callaway LN. 4 - Human\u2013animal interactions: Effects, challenges, and progress. In: Tucker CB, editor. Advances in Cattle Welfare. Woodhead Publishing; 2018. p. 71\u201392.</p><p>23. Singer P, Tse YF. AI ethics: the case for including animals. AI and Ethics [Internet]. 2022 Jul 6; Available from: <a href=\"https://doi.org/10.1007/s43681-022-00187-z\">https://doi.org/10.1007/s43681-022-00187-z</a></p><p>24. Deloitte. AI4Animals: Improving animal welfare through AI technology [Internet]. Deloitte. Available from: <a href=\"https://www2.deloitte.com/nl/nl/pages/consumer/solutions/ai4animals.html\">https://www2.deloitte.com/nl/nl/pages/consumer/solutions/ai4animals.html</a></p><p>25. Alexandrie G. Surveillance cameras and crime: a review of randomized and natural experiments. J Scand Stud Criminol Crime Prev. 2017 Jul 3;18(2):210\u201322.</p><p>26. Durlauf SN, Nagin DS. Imprisonment and crime. Criminol Public Policy. 2011 Feb;10(1):13\u201354.</p><p>27. Piza EL, Caplan JM, Kennedy LW. The effects of merging proactive CCTV monitoring with directed police patrol: A randomized controlled trial. Journal of Experimental [Internet]. 2015; Available from: <a href=\"https://link.springer.com/article/10.1007/s11292-014-9211-x\">https://link.springer.com/article/10.1007/s11292-014-9211-x</a></p><p>28. Udasin S. Agriculture Ministry orders installation of cameras in all slaughterhouses [Internet]. The Jerusalem Post. 2015. Available from: <a href=\"https://www.jpost.com/Business-and-Innovation/Environment/Agriculture-Ministry-orders-installation-of-cameras-in-all-slaughterhouses-438657\">https://www.jpost.com/Business-and-Innovation/Environment/Agriculture-Ministry-orders-installation-of-cameras-in-all-slaughterhouses-438657</a></p><p>29. Department for Environment, Food and Rural Affairs. Mandatory Closed Circuit Television (CCTV) recording in slaughterhouses \u2013 a consultation [Internet]. Department for Environment, Food and Rural Affairs; 2017. Available from: <a href=\"https://consult.defra.gov.uk/farm-animal-welfare/cctv-in-slaughterhouses/supporting_documents/Consultation%20on%20mandatory%20CCTV%20recording%20in%20slaughterhouses%20August%202017.pdf\">https://consult.defra.gov.uk/farm-animal-welfare/cctv-in-slaughterhouses/supporting_documents/Consultation%20on%20mandatory%20CCTV%20recording%20in%20slaughterhouses%20August%202017.pdf</a></p><p>30. Livingstone E. French parliament votes for slaughterhouse cameras [Internet]. Politico. 2017. Available from: <a href=\"https://www.politico.eu/article/french-parliament-votes-for-slaughterhouses-cameras/\">https://www.politico.eu/article/french-parliament-votes-for-slaughterhouses-cameras/</a></p><p>31. Hartstang S, Preuss D. Kamera\u00fcberwachung an Schlachth\u00f6fen: \u00dcberlegungen zu Wirkung und Angemessenheit von Transparenzma\u00dfnahmen [Camera surveillance in slaughterhouses: Reflections on effects and adequacy of transparency measures]. Berl Munch Tierarztl Wochenschr. 2020;133(5-6):291\u2013301.</p><p>32. Carey R, Parker C, Scrinis G. How free is sow stall free? Incremental regulatory reform and industry co\u2010optation of activism. Law Policy. 2020 Jul;42(3):284\u2013309.</p><p>33. The Animal Reader. Dutch slaughterhouse ordered to close after shocking animal abuse video [Internet]. The Animal Reader. 2021. Available from: <a href=\"https://www.theanimalreader.com/2021/07/02/breaking-dutch-slaughterhouse-ordered-to-close-after-shocking-animal-abuse-video/\">https://www.theanimalreader.com/2021/07/02/breaking-dutch-slaughterhouse-ordered-to-close-after-shocking-animal-abuse-video/</a></p><p>34. Department for Environment, Food and Rural Affairs. Improving animal welfare: Closed Circuit Television (CCTV) in Slaughterhouses: Impact Assessment [Internet]. Department for Environment, Food and Rural Affairs. 2017. Available from: <a href=\"https://consult.defra.gov.uk/farm-animal-welfare/cctv-in-slaughterhouses/supporting_documents/CCTV%20internal%20impact%20assessment%20%20final.pdf\">https://consult.defra.gov.uk/farm-animal-welfare/cctv-in-slaughterhouses/supporting_documents/CCTV%20internal%20impact%20assessment%20%20final.pdf</a></p><p>35. Lovell. Understanding farm animal abuse: Legal and extra-legal factors. Routledge international handbook of rural criminology [Internet]. 2016; Available from: <a href=\"https://www.taylorfrancis.com/chapters/edit/10.4324/9781315755885-16/understanding-farm-animal-abuse-jarret-lovell\">https://www.taylorfrancis.com/chapters/edit/10.4324/9781315755885-16/understanding-farm-animal-abuse-jarret-lovell</a></p><p>36. Anneberg I, Vaarst M, Sand\u0153 P. To inspect, to motivate or to do both? A dilemma for on-farm inspection of animal welfare. Anim Welf. 2013;22(2):185\u201394.</p><p>37. Cornish DB, Clarke RV. Opportunities, precipitators and criminal decisions: A reply to Wortley\u2019s critique of situational crime prevention. Prevention Studies. 2003;16:41\u201396.</p>", "user": {"username": "Animal Ask"}}, {"_id": "JFjBqeZNcuqH7hLWj", "title": "Wise Crowd & Democratic Spirit", "postedAt": "2022-09-27T16:29:01.092Z", "htmlBody": "", "user": {"username": "Hristo Zaykov"}}, {"_id": "tGBsabgWLs9KmqSem", "title": "LW4EA: A game of mattering", "postedAt": "2022-09-27T02:32:12.457Z", "htmlBody": "<p>Written by LW user <a href=\"https://www.lesswrong.com/users/katjagrace\">KatjaGrace</a>.</p><p>This is part of&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA</a>, a LessWrong repost &amp; low-commitment discussion group (inspired by&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/qtDZHBx6LpAR7ejDD/what-standalone-lesswrong-posts-would-you-recommend-to-most?commentId=ykgk75KtczWSyuLgs#comments\">this comment</a>). Each week I will revive a highly upvoted, EA-relevant post from the LessWrong Archives, more or less at random</p><p>Excerpt from the post (the first paragraph is actually a few combined sentences from further down the post that really got at the essence of a big problem for me):</p><blockquote><p>...I think a basic problem with working on a big pile of things in a big expanse of time is that if you work or not during any particular minute, it feels like it makes nearly no difference to the expectation of success... I only need to work a smidgen faster on average to get any particular amount of work done, so what does it matter if I work now or later?... Having a single specific thing to do within minutes is much more compelling: the task and the time are lined up so that my action right now matters. Slacking this minute is the difference between success and failure...</p><p>When I have an overwhelming number of things to do, and insufficient native urge to do them, I often arrange them into a kind of game for myself. The nature and appeal of this game has been relatively stable for about a year, after many years of evolution, so this seems like a reasonable time to share it. I also play it when I just want to structure my day and am in the mood for it. I currently play something like two or three times a week.</p><h1>The game</h1><p>The basic idea is to lay out the tasks in time a bit like obstacles in a platformer or steps in Dance Dance Revolution, then race through the obstacle course grabbing them under consistently high-but-doable time pressure.</p><p>Here\u2019s how to play:</p><ol><li>Draw a grid with as many rows as there are remaining hours in your hoped for productive day, and ~3 columns. Each box stands for a particular ~20 minute period (I sometimes play with 15m or 30m periods.)</li><li>Lay out the gameboard: break the stuff you want to do into appropriate units, henceforth \u2018items\u2019. An item should fit comfortably in the length of a box, and it should be easy enough to verify completion. (This can be achieved through house rules such as \u2018do x a tiny bit = do it until I have a sense that an appropriate tiny bit has been done\u2019 as long as you are happy applying them). Space items out a decent amount so that the whole course is clearly feasible. Include everything you want to do in the day, including nice or relaxing things, or break activities. Drinks, snacks, tiny bouts of exercise, looking at news sites for 5 minutes, etc. Design the track thoughtfully, with hard bouts followed by relief before the next hard bout.</li><li>To play, start in the first box, then move through the boxes according to the time of day. The goal in playing is to collect as many items as you can, as you are forced along the track by the passage of time. You can collect an item by doing the task in or before you get to the box it is in. If it isn\u2019t done by the end of the box, it gets left behind. However if you clear any box entirely, you get to move one item anywhere on the gameboard. So you can rescue something from the past, or rearrange the future to make it more feasible, or if everything is perfect, you can add an entirely new item somewhere. (<a href=\"https://www.lesswrong.com/posts/6BPqAbx9woMfpyJF5/a-game-of-mattering\">Full Post on LW</a>)</li></ol></blockquote><p>Please feel free to,</p><ul><li>Discuss in the comments</li><li>Subscribe to the&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA tag</a>&nbsp;to be notified of future posts</li><li>Tag other LessWrong reposts with&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/low-commitment-less-wrong-book-club\">LessWrong for EA</a>.</li><li>Recommend additional posts</li></ul>", "user": {"username": "captainjc"}}, {"_id": "ugJa9c8XWMrgga5Df", "title": "Democrats Veto, Republicans Coin Flip", "postedAt": "2022-09-27T02:06:31.615Z", "htmlBody": "<p>I wrote these two posts during my time at EA Global DC and realized I should share them here for feedback. Part 3 will be up tomorrow and part 4 either Wednesday or Thursday. Thanks in advance.<br><br><a href=\"https://cactus.substack.com/p/democrats-veto-republicans-coin-flip\">https://cactus.substack.com/p/democrats-veto-republicans-coin-flip</a></p><p><a href=\"https://cactus.substack.com/p/effective-altruisms-lifestyle-dilemma\">https://cactus.substack.com/p/effective-altruisms-lifestyle-dilemma</a></p>", "user": {"username": "Brian Chau"}}, {"_id": "PxwdLvYA8HvRkWXnH", "title": "Ask Me Anything about parenting as an Effective Altruist", "postedAt": "2022-09-26T23:35:24.548Z", "htmlBody": "<p>Lots of young EAs are struggling with the issue of whether, when, where, and how to have kids, and whether becoming a parent will undermine being an Effective Altruist, in terms of opportunities costs such as career, time, energy, money, focus, and values.</p><p>For whatever it's worth, I'm happy to answer any questions you might have about parenting -- its pros and cons, ethics, practicalities, etc.&nbsp;</p><p>Background: I'm a 57-year-old dad; I've raised a 26-year-old daughter and a 6-month-old baby. I've also helped raise a teenage step-son, and I come from a big, close-knit family (I have about 30 cousins.) I've lived as a parent in the US (mostly), UK, and Australia. I'm also a psychology professor who's taught courses on parenting-relevant topics such as behavior genetics, educational psychology, evolutionary psychology, human intelligence, evolutionary game theory, and decision making. I've been involved in EA for the last 6 years, and I have a pronatalist orientation, with an interest in population ethics, reproductive bioethics, gamete donation, and cognitive and moral enhancement. I'm not an expert on every practical or scientific issue about parenting, but maybe my perspective could be useful to some EAs.</p>", "user": {"username": "geoffreymiller"}}, {"_id": "hxBEJR7diDXSDe7bq", "title": "What Criteria Determines Who Gets Into EAG & EAGx?", "postedAt": "2022-09-26T22:03:27.987Z", "htmlBody": "<p>One of the members in one of my EA Virtual groups said he was in an EA group and did an EA fellowship at two elite universities, yet was not accepted into EAGx or EA Global events.</p>\n<p>This goes somewhat against my model of quat qualifies someone for these events, can someone provide a clear, transparent guide of what the selection process is for these events?</p>\n", "user": {"username": "Jordan Arel"}}, {"_id": "n9ieig8qnv2n5otG3", "title": "Fine-Grained Karma Voting", "postedAt": "2022-09-26T18:58:22.050Z", "htmlBody": "<p>A friend on the forum the other day wanted to down-vote a comment, which he sightly disapproved of, but even his weak down-vote voted the comment into negative karma (strong up/down votes are where users with high karma can choose to vote something up or down by multiple karma points, with strength of vote proportional to how much karma the up/down-voting user has themselves earned. But for some users, even their normal \u201cweak\u201d votes are worth multiple votes.) He was torn about whether he should down-vote or not. This seems to me problematic but easily fixable.</p>\n<p>What if users who have earned the ability to up/down-vote multiple karma points could click and hold the karma up/down-vote button and then select the exact amount of karma they want to add or subtract, up to their limit? I am calling this fine-grained karma voting.</p>\n<p>It seems to me this granularity would add valuable information of exactly how valuable an experienced forum user feels a post or comment is, and the extra work of users having to think a little more carefully about their votes wouldn\u2019t add too much cost, at least not proportional to the benefits of the extra information.</p>\n<p>Would love to hear other\u2019s thoughts on fine-grained karma voting, I do not have the programming knowledge to know how technically easy or difficult this is, and I am curious if there are other costs or benefits I have not thought of?</p>\n", "user": {"username": "Jordan Arel"}}, {"_id": "wne6DRmEoz5gbhxyG", "title": "For Longtermism, Employ an Earth-Based Morality", "postedAt": "2022-09-26T18:57:46.261Z", "htmlBody": "<p>In <i>What We Owe the Future,&nbsp;</i>William MacAskill asks such questions as \u201cIs It Good to Make Happy People?\u201d In his evaluations, he simplifies the analysis by ignoring the impact that any additional human might have on other humans (and implicitly, that they might have on animals or the earth).</p><p>I would propose that it would be far more accurate and useful to do such an analysis if we were to treat the earth as a living being, with all life-forms as cells of that being, just as we are living beings composed of many cells. This is a way of describing an ecological perspective that has the benefit of giving us an intuitive view of the situation.</p><p>Human beings could be considered as comparable to the brain cells of the earth. Given that, clearly the extinction of humanity would be an enormous loss. On the other hand, humans are leading to the extinction of a great many species on the planet, in part by crowding out habitat. In fact, it is not a stretch to consider humanity to be an invasive species. From this perspective, the idea that adding more humans creates an overall good might well be misguided.</p><p>Just as the idea that GDP can grow indefinitely is clearly absurd, since planetary resources are finite, so the idea that increasing the number of humans is better than having fewer is clearly true only up to a point, after which the negative consequences of having more humans than the carrying capacity of the earth outweighs any benefits those additional humans might bring. This does not preclude humanity from expanding by colonizing other planets\u2014many plants send out seeds in order to increase their presence, rather than simply growing to enormous and unsustainable size. But on the earth, there is some limit to how many humans can live well, especially taking into account the well-being of all life on earth. I do not claim to know what this number is, or whether we have surpassed it already. While it appears that we have, since we are badly fouling our own nest, it might be possible for us to learn to live more in harmony with nature so as not to emit excess greenhouse gasses, not to destroy habitat for our fellow inhabitants, and perhaps even to get along with one another.</p><p>In other arenas, MacAskill is clear that (for example) additional dollars may provide diminishing returns when contributed to improve a particular form of human suffering, he has not expressed the same insight regarding additional humans. We modern humans tend to carry the perspective that each human is separate, independent, and unrelated to others when considering their value, rather than recognizing (as many indigenous people seem to) that we are all deeply interconnected and interdependent with one another, other living beings, and the earth. I do note that MacAskill and many other people involved in EA do at times express concern for animal well-being, but this does not seem to go as far as a full ecological perspective.</p><p>I might point out that this idea about the earth being a living being has actually been believed by many people in different cultures and times. <a href=\"https://en.wikipedia.org/wiki/Anima_mundi\">Wikipedia</a> says, \u201cAlthough the concept of the <i>anima mundi&nbsp;</i>[world soul] originated in classical antiquity, similar ideas can be found in the thoughts of later European philosophers such as those of Baruch Spinoza, Gottfried Leibniz, Immanuel Kant Friedrich Schelling, and Georg W.F. Hegel (particularly in his concept of <i>Weltgeist)\u201d</i>. It further points out connections with Platonism, Gnosticism, Hermeticism, Jewish mysticism, eastern philosophy and more. In modern times, spiritual ecology and the idea of Interbeing popularized by Thich Nhat Hanh express this same concept.</p><p>But whether accepted as describing something actual or just as a useful analogy, I would propose that taking this ecological perspective is far more useful in evaluating moral good that acting as if human beings are in some way independent of, and divorced from, the earth and all other living beings.</p>", "user": {"username": "Wahhab Baldwin"}}, {"_id": "AoT98q6xrFv3K8CAk", "title": "Start a Minimal Local Group for Passive Outreach", "postedAt": "2022-09-27T11:44:53.126Z", "htmlBody": "<p>Epistemic status: thrown together relatively quickly after talking to a few people about it at EAGx Berlin. Putting my views out here for now and hoping for discussion in the comments.</p><h1>Summary</h1><ul><li>Many major (university)&nbsp;<strong>cities don\u2019t have an active EA local group</strong> or university group.&nbsp;</li><li>Unusually many&nbsp;<strong>people might be on the search for EA local groups</strong> in their city.<ul><li>With the recent&nbsp;<strong>publication of&nbsp;</strong><i><strong>What We Owe The Future</strong></i> and the corresponding marketing efforts, a lot of&nbsp;<strong>new people</strong> have been acquainted with effective altruism.</li><li>In some countries,&nbsp;<strong>a new semester / academic year</strong> is about to start or has recently started</li></ul></li><li>Setting up a \u201c<strong>minimal local group</strong>\u201d is&nbsp;<strong>easy&nbsp;</strong>&amp; potentially very&nbsp;<strong>valuable right now</strong><ul><li>People should be able to&nbsp;<strong>find</strong> the group when googling \u201ceffective altruism &lt;city&gt;\u201d</li><li>It should be easy to&nbsp;<strong>contact</strong> the group (e.g. set up a gmail address)</li><li>Ideally the group should appear&nbsp;<strong>alive&nbsp;</strong>/ non-outdated</li><li><i><strong>You can set this up even if you don\u2019t have the time to actually run a local group</strong></i></li></ul></li></ul><p>If you are:</p><ul><li>living in a reasonably large city without an active local group</li><li>you could see yourself investing 1-3 hours within the next few weeks</li></ul><p>Then please consider reading the rest of this post!</p><p>If this does not describe you but you know somebody in this position, then please consider forwarding the post to them.</p><h1>Is Your City Lacking a Local Group?</h1><p>I\u2019m primarily familiar with the situation in Germany, but am confident that my points generalize to many countries. The situation in Germany is that, while it has one of the most active EA communities, there are still many major cities with no local group at all, one that\u2019s hard to find, or one that seems very inactive from a quick search. For instance:</p><ul><li>Essen, Dortmund, Bochum, Duisburg in the Ruhr area seem to be lacking a local group despite being very close to each other and all having major universities</li><li>The remote university Hagen has almost 70k students, but no sign of a local group (which would have to coordinate mostly online, but might still be valuable)</li><li>Mainz has a reasonably large university but no local group</li><li>I found a few existing local groups that seem very inactive/outdated and could probably be superficially revived rather easily</li></ul><p>In many other, especially non-English-speaking countries there\u2019s a group in one or two larger cities (e.g. Prague, Milan, Helsinki) but seemingly little activity beyond that.</p><h1>Increased Interest in EA</h1><p>I believe that some countries are about to experience an increased interest in local EA chapters.</p><p>Firstly,&nbsp;<i>What We Owe The Future</i> was published about a month ago. At the same time, there were widely watched videos about EA topics by YouTube channels such as&nbsp;<a href=\"https://www.youtube.com/watch?v=W93XyXHI8Nw\"><u>Kurzgesagt</u></a>,&nbsp;<a href=\"https://www.youtube.com/watch?v=r6sa_fWQB_4\"><u>Primer</u></a> and&nbsp;<a href=\"https://www.youtube.com/watch?v=_uV3wP5z51U\"><u>Rational Animations</u></a>. This likely leads to an increase in interest for effective altruism, as the&nbsp;<a href=\"https://trends.google.com/trends/explore?date=today%205-y&amp;q=effective%20altruism\"><u>Google trend</u></a> for the search term \u201ceffective altruism\u201d seems to support with its peak in August:</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994907/mirroredImages/AoT98q6xrFv3K8CAk/ovbzk7k6kcplk0cq8j9q.png\"></p><p>Secondly, university students tend to look out for interesting opportunities to engage in during the beginning of their terms. I believe \u201cminimal local groups\u201d (which I\u2019ll explain in detail in the next section) should exist generally in larger cities, but the impending or very recent start of a new term in many countries (such as Germany, UK and France) adds to the relative urgency of ensuring the existence of such groups.</p><p>If interested people are actively looking for a local entry point to the EA community and don\u2019t easily find one, their enthusiasm may fade. If, however, they find a way to connect with others, they\u2019re much more likely to stick around and get more involved.</p><p>This does not necessarily mean that people now need to set up and run a group to its full extent. This would be a lot of work and responsibility, and I get that most people are not in a great position to invest such effort. However, I do believe that there\u2019s a much smaller version of \u201cstarting a local group\u201d, which most people should have the capacity to do, and which still yields a lot of value.</p><h1>Setting up a Minimal Local Group</h1><p>Actually&nbsp;<i>running&nbsp;</i>a local group might involve a lot of things, such as:</p><ul><li>Setting up an online presence</li><li>Setting up and maintaining internal channels of communication</li><li>Handling external communication</li><li>Coming up with a strategy</li><li>Planning events</li><li>Actually running events</li><li>Doing outreach</li><li>Running a fellowship</li><li>Registering as a university group</li><li>\u2026</li></ul><p>And obviously it\u2019s great if groups exist and go to such lengths to provide a thriving local community for others to be part of.</p><p>However, in this post I\u2019m not arguing for the creation of such \u201cflourishing\u201d groups but rather a much smaller version that mostly achieves one simple goal:&nbsp;<strong>being an entry point for people that come to the group on their own</strong>. So I\u2019m particularly&nbsp;<i>not&nbsp;</i>talking about active outreach or even an active group life (although at least the latter probably needs to be pursued sooner or later by somebody in order to keep people actually interested), but rather a form of <i>passive </i>outreach. This would entail setting up a \u201cminimal local group\u201d with a very limited scope.</p><p>A minimal local group should primarily do the following:</p><ul><li>Be&nbsp;<strong>easy to find online</strong>&nbsp;</li><li>Be&nbsp;<strong>easy to contact</strong>&nbsp;</li><li>Be&nbsp;<strong>responsive</strong></li><li>And ideally, if people&nbsp;<i>do</i> contact the group, they should be able to take the next steps from there (which could mean that the person who set it all up additionally organizes a group kick-off event with the interested people to make some plans and distribute responsibilities) and move the group beyond its minimal state</li></ul><p><strong>I believe that setting up such a minimal group can be done in around 2 hours</strong>, with very low maintenance cost after that. Also note that you don\u2019t have to be a student to do so, even if your city has a large university, as registering as a university group would be an optional step to be taken later, if at all.</p><p>So,&nbsp;<strong>what specifically would need to be done</strong>? This might of course depend on your particular circumstances, but I would suggest to work through the following list:</p><ol><li>Check if there is a&nbsp;<strong>group in your city</strong>/area already, or there are any remnants of one from the past</li><li>If you can find any other (formerly)&nbsp;<strong>active EAs</strong> from the area, contact them to potentially get them on board</li><li>Get access to or set up an&nbsp;<strong>email address</strong> for your local group (e.g. \u201ceffective-altruism-&lt;your_city&gt;@gmail.com\u201d)</li><li>Optionally, create a&nbsp;<strong>group&nbsp;</strong>in the&nbsp;<strong>messenger&nbsp;</strong>of your choice (such as Signal or WhatsApp) that others will be able to join</li><li>Create or update an entry for your local group on the&nbsp;<a href=\"https://forum.effectivealtruism.org/community\"><strong><u>EA Forum community page</u></strong></a>, linking to your group\u2019s email address, and possibly providing a link to join your messenger group</li><li>If there\u2019s a central&nbsp;<strong>listing of local groups</strong> in your country (such as&nbsp;<a href=\"https://www.effektiveraltruismus.de/lokalgruppen\"><u>this one</u></a> in Germany), figure out how to get your group added to the list</li><li>Optionally, create a page/group on other&nbsp;<strong>social media</strong> platforms, such as Facebook or Twitter, where you expect people might look for your group</li><li>Optionally, ask some EA group organizers from nearby cities to join your social media / messenger groups (this both helps coordination between groups, and makes your group seem not quite as empty to newcomers)</li><li>Set a recurring reminder for yourself to check the group\u2019s communication channels</li></ol><p>If you want to do more, there is of course a variety of further actions you can take, but this exceeds the scope of this post and there are&nbsp;<a href=\"https://resources.eagroups.org/start-a-group\"><u>other resources available</u></a>.</p><h2>What Happens Next?</h2><p>Once a minimal local group is set up, there are a few different ways things might pan out:</p><ul><li>A number of people contact / join your group, including one or more who have some EA experience and are willing to take over responsibility \u2192 success!</li><li>Some people contact / join your group, but they are all new to EA and prefer to be \u201cshown around\u201d rather than being responsible themselves<ul><li>If you have the capacity and it seems worth your time, you could run a few introductory events for them and see where things go.</li><li>If you have little capacity but at least one newcomer is enthusiastic enough, you could offer to advise them while they try to get the group off the ground (e.g. by that person running an introductory reading circle, or coordinating with other new people to join an&nbsp;<a href=\"https://www.effectivealtruism.org/virtual-programs/introductory-program\"><u>online virtual program</u></a>).</li><li>In the worst case, you can just be open about the fact that the group is not yet very active, but you still wanted to ensure people can get connected already, and briefly advise the new people on how to familiarize themselves with EA. Ideally make it clear that people are free to step up and kickstart the group.</li></ul></li><li>Very few or no people join your group (for now) \u2192 unfortunate, but no harm done, and this might possibly change in the future. Your minimal group is very low maintenance after all, and you can just keep it in this state for months or years if necessary.</li></ul><p><br>I think there\u2019s a significant chance that setting up a minimal local group will result in a few additional engaged EAs, and that it might even turn into an active local group over time. The potential downsides seem negligible in comparison.</p><p>&nbsp;</p><p><i>Thanks to Adrian Spierling and Matthew Esche for their feedback on this post.</i></p>", "user": {"username": "markus_over"}}, {"_id": "uPu2tbzowqCw73dbh", "title": "Project Idea: The cost of Coccidiosis on Chicken farming and if AI can help", "postedAt": "2022-09-26T16:30:22.957Z", "htmlBody": "<h2>Outlining the problem</h2><p>More than 68 billion chickens were farmed in 2018, representing a third of all meat produced globally in addition to 1.38 trillion eggs for human consumption.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefp55w17y3oi\"><sup><a href=\"#fnp55w17y3oi\">[1]</a></sup></span>&nbsp;It's probably not a controversial take to say that Chickens share of the global non-human animal suffering is the biggest of any land animal. Any welfare improvements to farm chickens would probably effect massive numbers of individuals, even if just on a country by country basis.&nbsp;</p><p>There is a common disease in chickens called Coccidiosis. The cost of Coccidiosis in chicken farming in the UK using 2016 prices is \u00a30.16/ chicken produced.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaoyitr5f52\"><sup><a href=\"#fnaoyitr5f52\">[2]</a></sup></span>&nbsp;There is a financial interest to try to get rid of the disease from farm's supply because it costs them money to have to cull chickens prior to slaughter age because of waste. Coccidiosis is also bad for broiler/egg laying chicken welfare because they suffer and die from a parasitic infection before \"normal\" slaughter age.&nbsp;</p><p>\u201cOur survey suggested that approximately 2% of UK broilers would be expected to die or (more commonly) be culled in a house affected by coccidiosis. Thus, if 31.5 million broilers were in houses affected by coccidiosis, mortality would have been 630,000. Note, the figure does not include losses due to subsequent mortality caused by other opportunistic pathogens.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaoyitr5f52\"><sup><a href=\"#fnaoyitr5f52\">[2]</a></sup></span></p><p>In 2016 in the UK at least 95% of egg laying chickens received live anticoccidial vaccination. The average cost of an anticoccidial vaccine was 8p per dose in the UK in 2016. If 95% of laying hens received vaccines, the total cost would have been \u00a34.06 million.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaoyitr5f52\"><sup><a href=\"#fnaoyitr5f52\">[2]</a></sup></span></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_101 101w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_181 181w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_261 261w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_341 341w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_421 421w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_501 501w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_581 581w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/872e88e8d6f59c9508a1b17c47752ec4741c0961d47cd206.png/w_661 661w\"></figure><p><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaoyitr5f52\"><sup><a href=\"#fnaoyitr5f52\">[2]</a></sup></span></p><p>As you can see from the table above, despite an effective and cheap vaccine the disease is still a source of pain in the industry. And in countries which don\u2019t have a strong take up of the vaccine, the percentage of affected flocks is even higher.&nbsp;</p><hr><h2>How AI could potentially help</h2><p>Luckily, the disease Coccidiosis has quite visible symptoms, as seen from the image below:<br><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a3bb2ab42fc13b6700d1b24fd45c7ecb38eae2fcc6b57333.jpg/w_1176 1176w\"><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefigu2xqc0ek\"><sup><a href=\"#fnigu2xqc0ek\">[3]</a></sup></span></p><p>[Redacted, see comments].</p><hr><h2>Uncertainties&nbsp;</h2><h3>Would spotting Coccidiosis early significantly reduce the risk/damage of an outbreak?&nbsp;</h3><p>It\u2019s unclear, but some websites seem to indicate that if it's spotted early then it can be treated easier. However farms are more likely to cull than treat according to the study<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefaoyitr5f52\"><sup><a href=\"#fnaoyitr5f52\">[2]</a></sup></span>. But this could just be because by the time the farms notice chickens are sick those chickens are too far gone. Further investigation is probably needed here in reaching out and talking to farmers.&nbsp;</p><hr><h2>Blockers to potential work / the outcome being useful</h2><ul><li>[Redacted, see comments]</li><li>It's possible the footage from said cameras would be too poor quality for the AI to gain any information from it.&nbsp;</li><li>Chicken farms might be against having cameras in their farm</li></ul><hr><h2>Next steps</h2><p>I have some experience in writing [redacted] and would be able to take a crack at creating some type of algorithm for it on my own. However, after googling I realise I have no method of collecting the training data required to test this out. So for the time being the idea is stuck. &nbsp;</p><p>Which sucks because I think this idea strikes a rare alignment of being both good for animal welfare and farmers profits (and food security too actually).</p><p>If anyone thinks this is a good idea and would like to help in some way or have connections to chicken farms who could potentially supply training data for the algorithm that would be amazing. If anyone wants to grab this idea and investigate it on their own that would be great too.&nbsp;</p><p>Feedback about whether this is in fact a good idea is very welcome.&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp55w17y3oi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp55w17y3oi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://www.fao.org/3/cb1329en/CB1329EN.pdf</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnaoyitr5f52\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefaoyitr5f52\">^</a></strong></sup></span><div class=\"footnote-content\"><p>https://veterinaryresearch.biomedcentral.com/articles/10.1186/s13567-020-00837-2#:~:text=The%20cost%20of%20broiler%20mortality,would%20have%20been%20630%2C000%20(Eq.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnigu2xqc0ek\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefigu2xqc0ek\">^</a></strong></sup></span><div class=\"footnote-content\"><p>http://www.poultrydvm.com/condition/coccidiosis</p></div></li></ol>", "user": {"username": "maxh94"}}, {"_id": "XhBFcsotZ2nC87DQW", "title": "Longtermism and Uncertainty", "postedAt": "2022-09-26T16:21:01.636Z", "htmlBody": "<p>One of the brilliant innovations of EA was the idea that a donor wishing to donate to a &nbsp;charity should focus on the outcome of a contribution to the charity, specifically by seeking measurements of the impact on the target recipients, ideally by A/B testing or other means of investigation commonly employed in scientific studies. While this idea seems obvious in retrospect, and while a few philanthropists (like Bill Gates) have pursued this for many years, it is a radical shift from how giving to charity has typically been done.</p><p>Longtermism attempts to anticipate long-term future goods or ills in order to determine a moral value of pursuing them. However, it is obvious that in general, the further in the future a potential issue lies, the less certainty we can have about it.&nbsp;</p><p>Let us imagine a person in England in the year 1500 trying to think about how she could make the world a better place 500 years in the future. I will not bother to discuss the issues going on at the time, such as enclosure of land for sheep, inflation, increased disparity between rich and poor, and changes in farming techniques. It seems clear that any ideas this person had about what she could do to make the world a better place in the year 2000 would likely be far removed from the reality of what was to come, and any effort made based on those ideas, while they might improve the well-being of humanity for years or decades, could not realistically be expected to be improving humanity's well-being in the year 2000. Well-meaning efforts to improve the world (such as the crusades to the Holy Land in the period between 1095 and 1291) could easily have long-term negative effects rather than positive ones. Morality in Europe during that time was completely based on Christian ideas and ideals, while that is not the case in Europe today.</p><p>Since 1500, the rate of change in the world has greatly increased, and the world seems increasingly unstable, in part because of our increasing ability . So it seems likely that we are even less able to predict the situation the world will be facing 500 years from now. And even more likely that we are unable to choose an intervention today, say a donation to a charity or political group or group of scientists, that we can have any level of confidence will produce a beneficial (or harmful) outcome this far in the future.</p><p>Based on this, it seems clear that it is hubris to plan some intervention for good based on populations that may or may not live thousands of year from now. My observation is that we greatly over-rate our abilities to predict the future. From time to time, <i>Scientific American </i>and other magazines run articles listing predictions from the past. Even though made by experts of the time, they tend to have at least as many errors as accurate predictions. (See \u201c<a href=\"https://www.wired.com/story/history-predicting-future/\">The History of Predicting the Future</a>\u201d for a discussion of some of the issues.)</p><p>Now admittedly, if some action has a significant risk of wiping out all of humanity within the next 50 or 100 years, this may be a realistic threat to attempt to prevent. But even there, the likelihood of humanity being made extinct by any near-term human actions seems small.</p><p>Humans are scattered far and wide over the surface of the earth. Many live in large cities, some are farmers, while others are hunter-gatherers as part of indigenous tribes. Humans are remarkably adaptable, and live from the tropics to the arctic, from below sea level to miles above sea level. So even the most devastating human-caused event we can imagine, from nuclear winter to an AGI set (for some odd reason) on eliminating humans, is unlikely to get rid of all of us. A malevolent, human-developed pathogen might kill most human, but there would doubtless be some who are immune, or some islands where the infection never reached.</p><p>In addition, unlike the dinosaurs, a great many human beings would be alerted to any potential extinction event in time to take precautions. The world is large, and no human-caused event could kill all humans immediately. We were able to develop a vaccine against Covid in &nbsp;a year; by a century or two from now, we might be able to develop one in days or weeks.</p><p>As a result, efforts in longtermism that attempt to calculate the benefit of preventing a future that leads to human extinction are, in my opinion, fundamentally flawed. Take for example the malevolent AGI. First of all, to calculate the probability that we will develop an AGI that is overall more intelligent than humans is likely an effort at throwing darts at a dartboard. Secondly, any kind of timeline on when that would occur is just a wild guess. Thirdly, the likelihood that such an AGI might undertake activities that would be inimical to humanity is a complete unknown. Fourthly, the chances that such activities could lead to the extinction of humanity (prior to humanity being made extinct for some other reason) seem very small.&nbsp;</p><p>My overall conclusion is that longtermism, at least as I am finding it discussed in the EA community, is not effective. We cannot have A/B trials of the impact of efforts on the far future. We greatly overestimate our ability to anticipate what the future will bring, or what efforts we might make to alter it &nbsp;might actually accomplish. For this reason, I feel that concern for issues like an AGI leading to human extinction should be given far less weight that either human health in impoverished areas, or topics like the climate crisis, which is already hurting the lives of millions, and about which science is fairly certain will get much worse over the coming decades.</p>", "user": {"username": "Wahhab Baldwin"}}, {"_id": "9hzGFBbtaAsLKAnRy", "title": "\"Defective Altruism\" by Nathan J. Robinson in Current Affairs", "postedAt": "2022-09-26T14:27:56.720Z", "htmlBody": "<p>Curious to see any specific rebuttals of Robinson's points here as this piece does not seem to have been posted and is one of the most thorough assessments of EA by a non-EA community member that I've seen.</p>", "user": {"username": "Peter_Layman"}}, {"_id": "hsKTgzhwyaXfQz8t3", "title": "Any recommendation for how to explain EA funds grants to friend?", "postedAt": "2022-09-26T06:54:12.400Z", "htmlBody": "<p>I have a friend who is casually engaged with EA. They went on <a href=\"https://funds.effectivealtruism.org/grants\">https://funds.effectivealtruism.org/grants</a> and was browsing some of the grants and asking me about them. I get the impression this was well-intentioned curiosity, but I was at a loss to explain the dollar amount of some of the grants my friend pointed out to me, in light of the short \"project description\" provided.&nbsp;</p><p>The last thing I want to do in this post is call anyone out-- I'm sure the rationale behind these grants was sound, and there is relevant information missing from the provided description-- but I was surprised to find out that there are grants for university group organizing in the five and six figures, and some of these are not even for an entire year. I do think this is something that will (perhaps justifiably) raise eyebrows for the average person, if they are just learning about EA as a movement very focused on cost effectiveness, and haven't yet internalized some of the expected value calculations that probably went into these grants. But also, in a couple cases, I personally am having a hard time imagining how these numbers make sense.&nbsp;</p><p>If you are reading this post and willing to comment-- could you (1) help me make sense of these grants first for myself and (2) provide any pointers on how to explain them to someone who isn't yet totally onboard with EA? I don't want to indicate specific grants, but specifically, what e.g. is the argument for a 5 or 6 figure grant for one semester of university organizing at a specific school? I don't understand how so much money could be needed. As far as I'm aware, most organizers are volunteers (but maybe that is changing?). Happy to take this to a private conversation if that would be more appropriate.</p>", "user": {"username": "maxfieldwallace"}}, {"_id": "hLJYgehJ4JLbKvgmb", "title": "NASA will re-direct an asteroid tonight as a test for planetary defence (link-post)", "postedAt": "2022-09-26T04:58:47.354Z", "htmlBody": "<p>This is a link-post for an explainer of NASA's Double Asteroid Redirection Test (DART). It may be one of the most prominent existential risk reduction activities in the public sphere (the explainer even describes the likelihood of asteroid collisions large enough to threaten civilisation). I hadn't seen much talk about it.</p><p>DART will be reaching a two asteroid system in the evening of September 26. It has been travelling for around 10 months, and is now around 11 million kilometres away. The asteroids are not a threat to Earth in any way. It will autonomously target the smaller asteroid (Dimorphos, around 160m diameter) and collide with it at a speed of around 26,000 km/hr.&nbsp;</p><p>This should inform the potential for future asteroid-redirection efforts. As noted in 'The Precipice' though, while potentially reducing the risk from asteroids, such a capability may pose a larger risk itself if used by malicious actors to target asteroids <i>towards</i> Earth.</p>", "user": {"username": "BenStewart"}}, {"_id": "JcgcK8G6mHR7khaHc", "title": "Oren's Field Guide of Bad AGI Outcomes", "postedAt": "2022-09-26T08:59:29.432Z", "htmlBody": "", "user": {"username": "Oren Montano"}}, {"_id": "yi8grNfDwxY2QCLH2", "title": "On Generality", "postedAt": "2022-09-26T08:59:29.438Z", "htmlBody": "", "user": {"username": "Oren Montano"}}, {"_id": "NyCHoZGGw5YssvDJB", "title": "Lessons from Three Mile Island for AI Warning Shots", "postedAt": "2022-09-26T02:47:08.375Z", "htmlBody": "<h1><strong>Summary&nbsp;</strong></h1><p>In 1979, a nuclear power plant in Three Mile Island (TMI), Pennsylvania experienced a meltdown.&nbsp; In response, the public became much more concerned with the safety of nuclear power and successfully demanded increased regulation.&nbsp; TMI seems potentially instructive for how the American public might respond to an AI \u201cwarning shot\u201d, i. e. a situation in which AI visibly causes significant but not catastrophic damage.&nbsp; In particular, TMI suggests that</p><ol><li>The public is capable of being quickly and strongly influenced by warning shots (95%).&nbsp;&nbsp;</li><li>The public\u2019s response to a warning shot is influenced more by pre-existing discussions about the risk (including fictional accounts), media coverage of the accident, and the public\u2019s attitude towards the actors responsible for the warning shot than by the magnitude of the accident (90%).&nbsp;&nbsp;</li><li>Confusion about the relevant technology may lead the public to fail to recognize the threat prior to the warning shot, but once the threat is clear, it may cause the public to become more afraid (50%).&nbsp;&nbsp;</li><li>Capitalizing on warning shots is much more likely if the actors who aim to do so are part of a broader political coalition (85%).</li><li>The legislative response is likely to be ineffective by default, particularly given that the technical problems involved in regulation are complicated (80%).&nbsp; However, industry may be able to respond more effectively through internal regulation in order to avoid further reputational damage.&nbsp;<br>&nbsp;</li></ol><p>These takeaways suggest that if the longtermist AI governance community (henceforth referred to as \u201cthe AI governance community\u201d) wants to effectively influence AI policy in response to an AI warning shot, it should attempt to influence the media environment prior to the warning shot (perhaps through the release of realistic seeming fiction about AI takeover scenarios), utilize/play into public suspicion towards big tech, form allies with other groups who support AI regulation, and think hard in advance about what policy responses would actually reduce AI risk.<br>&nbsp;</p><p>Epistemic Status: This post was written as part of the Stanford Existential Risks Initiative's Summer Research Fellowship, and is the product of ~50 hours of writing and research.&nbsp; Moreover, n = 1, so this post overall provides relatively weak evidence for its claims; the probabilities assigned are more based on my priors than on the evidence from Three Mile Island.&nbsp; That being said, I am quite confident in the straightforwardly historical claims made.</p><p>&nbsp;</p><p>Acknowledgements: Thank you to&nbsp;Matthew Gentzel and Ben Snyder for their feedback</p><p>&nbsp;</p><h1><strong>Introduction</strong></h1><p>Currently, one of the biggest barriers to governments taking major political action to reduce existential risk from AI is that neither voters nor government officials take the risk very seriously.&nbsp; This could possibly change in response to a \u201cwarning shot,\u201d i. e. an event in which an AI system causes a major accident but does not precipitate existential catastrophe.&nbsp; Thus, it seems important to answer the question of how the public might respond to a warning shot if it occurs, and how we might channel this response into effective political action to reduce AI risk.</p><p>In order to answer these questions, I examine a previous case of a warning shot-like event which led the public to successfully demand legislation to reduce future risks: the Three Mile Island (TMI) nuclear accident.&nbsp; In March, 1979, a nuclear power plant in Three Mile Island, Pennsylvania experienced a partial meltdown.&nbsp; While a total meltdown was ultimately averted and, according to most estimates, the damage was relatively minor, TMI prompted increased public concern about the safety of nuclear power and led to major efforts to regulate it.&nbsp; Thus, TMI plausibly serves as a good case study for what factors make a warning shot generate major public and legislative response.</p><p>&nbsp;</p><p><strong>Limitations</strong></p><p>TMI is far from a perfect analogy to an AI warning shot.&nbsp; Firstly, the risks posed by AI are very different to those posed by nuclear power.&nbsp; In particular, risk from nuclear meltdowns may be easier to understand and certainly seems more \u201crealistic\u201d to the public than risk from misaligned AI.&nbsp; Moreover, U. S. culture and politics have changed in important ways since 1979.&nbsp; For example, in the more partisan environment of US politics today, it is possible that the question of how to respond to an AI warning shot might become a partisan issue.&nbsp; Finally, particularly given that the AI industry will likely grow significantly in the next decade prior to a warning shot, it might be much more influential than the nuclear industry was at the time of TMI and effectively oppose legislation.&nbsp; Nevertheless, TMI is one of few cases of a widespread public response to a major accident involving technology in recent history.&nbsp; Thus, insofar as we want to have some kind of baseline for what responses to warning shots look like, studying the case of TMI seems valuable.&nbsp; With this in mind, I present 7 lessons that I think we can learn from the example of TMI for how the public might respond to an AI warning shot, and how we might take advantage of this response.</p><p><br><br>&nbsp;</p><h1><strong>Lessons from Three Mile Island</strong></h1><p>&nbsp;</p><p><strong>Lesson #1: Warning shot events can have large effects on public opinion</strong></p><p>Some people believe that the public is insufficiently attentive to the state of the world to respond in any strong manner to warning shots (see: COVID and pandemic preparedness measures).&nbsp; However, the case of TMI suggests that this is false.&nbsp; Prior to the incident, public opinion was generally favorable towards nuclear energy, with about 60% of the public supportive of building nuclear power plants in the US and about 30% opposed.&nbsp; Immediately after the accident, the public became about evenly split on the issue, with about 45% on each side.&nbsp; While support for nuclear power seems to have briefly rebounded after a few months, this rebound then quickly reversed partially owing to a second wave of coverage in response to the release of a public report on TMI.&nbsp; Moreover, in addition to shifting views on nuclear power, TMI seems to have caused the public to pay more attention to the issue of nuclear power in general, as the percent of people answering that they did not have an opinion about whether more nuclear power plants should be built in the US dropped from over 20% to under 10% immediately following the accident.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdqkcfw3ui4\"><sup><a href=\"#fndqkcfw3ui4\">[1]</a></sup></span>&nbsp; Finally, TMI prompted a significant legislative response.&nbsp; In particular, immediately after the accident, Jimmy Carter formed a commission to investigate the accident and to make recommendations about what could be done to reduce the probability of future accidents.&nbsp; The commission then released a report which ultimately resulted in significant reforms to the agency which regulated nuclear power, the Nuclear Regulatory Commission (NRC).&nbsp; While these reforms were not very effective, they did involve a significant restructuring of the NRC.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6lv3r7key7\"><sup><a href=\"#fn6lv3r7key7\">[2]</a></sup></span>&nbsp; Thus, TMI suggests that as irrational as the public can be, it can quickly update in response to major technological accidents and pressure public officials into taking major actions aimed at reducing future risks.</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>Lesson #2: Which warning shots grab the public\u2019s attention is not mostly explained by the objective \u201csize\u201d of the warning shot</strong></p><p>While the evidence presented above suggests that warning shot events can get the public concerned about an issue, the public\u2019s broader pattern of response to nuclear energy accidents suggests that which specific accident grabs its attention may be more a matter of chance and media coverage than of the actual size of the issue.&nbsp; TMI was not the first nuclear power accident in the US; throughout the 1950\u2019s, \u201860\u2019s, and early \u201870\u2019s, there were several other accidents involving nuclear power that failed to generate nearly as much public attention as TMI.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhkrzdujwbdw\"><sup><a href=\"#fnhkrzdujwbdw\">[3]</a></sup></span>&nbsp; &nbsp;While TMI was in important ways \u201cbigger\u201d than these other accidents, it was in other ways \u201csmaller\u201d insofar as some of the past accidents caused deaths while TMI did not immediately lead to any.&nbsp; Similarly, the differences in the public response to nuclear energy accidents between countries does not seem to be well explained by how they are actually affected by the accident.&nbsp; For example, different European countries\u2019 publics had significantly different responses to the Chernobyl nuclear meltdown.&nbsp; However, the size of these responses was unrelated to the extent to which the country in question was negatively impacted by the radiation which Chernobyl generated.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg5k78f17c9\"><sup><a href=\"#fng5k78f17c9\">[4]</a></sup></span>&nbsp; This suggests that it may be difficult to predict what specific accident or kinds of accidents will draw public attention to AI risk.</p><p><br>&nbsp;</p><p><strong>Lesson #3: Fiction and media coverage can greatly influence public response to a warning shot</strong></p><p>If the public\u2019s strong reaction to TMI was not primarily due to the nature of the accident itself, then what caused it?&nbsp; While public response to an event is determined by a huge number of variables, two key ones in the case of TMI seem to have been the prior presence of fiction which primed the public to be concerned about nuclear meltdowns and the media coverage of the event itself.&nbsp; Some have argued that a major reason for lack of public concern about AI risk is that people see AI takeover scenarios as science fiction rather than as something that could happen in real life.&nbsp; However, the case of TMI suggests that fiction can sometimes have the opposite effect, causing the public to be concerned about the technological accidents which it presents.&nbsp; In particular, the movie \u201cThe China Syndrome\u201d was released less than two weeks prior to TMI.&nbsp; The China Syndrome presented the fictional story of the near-meltdown of a nuclear power plant and the efforts of the company responsible for the accident to cover it up.&nbsp; It was widely viewed by the public, starring major actors such as Jane Fonda and Jack Lemmon, and explicitly aimed to call attention to the issue of nuclear safety, dramatizing the possible negative effects of nuclear meltdowns and portraying nuclear power companies as corrupt.&nbsp; Moreover, even before TMI, at least some commentators took the concerns raised by the movie seriously.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefitcaopq1bcp\"><sup><a href=\"#fnitcaopq1bcp\">[5]</a></sup></span>&nbsp; While other commentators viewed the concerns which the movie raised as overblown, it would prove critical to the public response to TMI.&nbsp; In their coverage of TMI, media commentators often drew parallels to The China Syndrome.&nbsp; For example, in its cover story immediately following the accident,&nbsp;<i>Time&nbsp;</i>magazine stated that the public statements given by the public spokesman for the plant sounded \u201cas if they were taken right out of the script for the film&nbsp;<i>The China Syndrome,</i>\u201d and Newsweek coverage of the event included images taken from the film.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjbmlr0rq8q\"><sup><a href=\"#fnjbmlr0rq8q\">[6]</a></sup></span>&nbsp; Confirming the importance of the film, the report issued by the NRC months after the accident stated that its effects on the public perception of TMI had been \u201cimmeasurable.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpm5airzjuam\"><sup><a href=\"#fnpm5airzjuam\">[7]</a></sup></span>&nbsp;&nbsp;</p><p>&nbsp;</p><p>A second major reason why the public seems to have reacted in the way that it did was that media coverage of the accident was extensive and dramatic.&nbsp; Partially, this was due to the fact that the media had little else to cover during the week in which TMI occurred.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6c0rmdxj1b3\"><sup><a href=\"#fn6c0rmdxj1b3\">[8]</a></sup></span>&nbsp; However, TMI also happened at a time when the media was already paying increased attention to nuclear power issues due to the release of the China Syndrome, Jimmy Carter\u2019s efforts to pass his energy policies, and generally increasing concern about the safety of nuclear power throughout the 1970\u2019s.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrku8fxekghh\"><sup><a href=\"#fnrku8fxekghh\">[9]</a></sup></span>&nbsp; As a result, TMI received lots of TV coverage, with 40% of all evening news coverage being devoted to it during the first week of the crisis.&nbsp; Moreover, this coverage was in many cases quite dramatic.&nbsp; For example, Walter Cronkite of CBS evening news stated during his coverage of TMI that \u201cThe world has never known a day quite like today,\u201d and ABC News showcased sentimental stories of mothers evacuating with their children.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4v8bzde5a1o\"><sup><a href=\"#fn4v8bzde5a1o\">[10]</a></sup></span>&nbsp; Between the release of The China Syndrome and the coverage of the event, the public was heavily influenced by media to respond fearfully to TMI.&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>Lesson #4: The public is more responsive to warning shots if it does not trust the authorities responsible for them</strong></p><p>Another major reason that the public responded strongly to TMI was that it distrusted the nuclear power industry and other authorities, partially due to the conflicting reports which they issued about the accident.&nbsp; Polls from the 1970\u2019s reveal that public skepticism towards the nuclear industry and its regulators pre-dated the crisis, perhaps owing to Americans\u2019 generally skeptical attitudes towards authorities.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref68fl3aiw1w\"><sup><a href=\"#fn68fl3aiw1w\">[11]</a></sup></span>&nbsp; However, TMI itself increased this distrust.&nbsp; During the crisis, spokespeople for the plant were confused about what was happening.&nbsp; Nevertheless, in response to media pressure, they made various reassuring positive statements which they later had to retract once they learned more information.&nbsp; For example, Jack Herbein, the vice president and spokesperson for Metropolitan Edison (the company operating the power plant), first claimed that the accident was not serious, and the utility spokesperson Dave Klucsik stated that \u201c[t]here is absolutely no risk of a meltdown.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzwwh8f4end\"><sup><a href=\"#fnzwwh8f4end\">[12]</a></sup></span>&nbsp; However, soon afterwards, the NRC declared that a meltdown was possible, and media figures revealed that they had been told by people working at the plant that the crisis was more severe than their public statements had indicated.&nbsp; Officials continued to give contradictory and confusing statements as the crisis continued.&nbsp; As a result, the public began to suspect that industry and government were both conspiring to conceal information from them.&nbsp; This lack of trust and information caused further panic, with many local residents citing anxieties over conflicting information as their reason for evacuating the area during the crisis.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1v50lyq54o\"><sup><a href=\"#fn1v50lyq54o\">[13]</a></sup></span>&nbsp; Moreover, media coverage after the event continued to push this narrative, with The Washington&nbsp;<i>Observer-Reporter&nbsp;</i>running a headline stating that \u201cThree Mile Island Meltdown Was Near\u201d and other media outlets covering accusations that the radiation released during TMI had had more damaging health effects than authorities had claimed.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefoz75knqdpod\"><sup><a href=\"#fnoz75knqdpod\">[14]</a></sup></span>&nbsp;&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>Lesson #5: Lack of understanding is not necessarily a barrier to public concern, and in fact can in some cases increase the public's level of fear</strong></p><p>One reason that some suspect that the public may not become concerned with AI risk even if a warning shot occurs is that it does not understand the technical problems involved in AI alignment.&nbsp; However, in the case of TMI, the public\u2019s lack of technical knowledge does not seem to have made it any less concerned about nuclear energy.&nbsp; Due to the inconsistency of the reporting, as well as the complex nature of nuclear power, much of the public seems to have been confused about exactly what was happening during the meltdown.&nbsp; However, this led many to infer that the situation was worse than it was in actuality.&nbsp; For example, 36% of those who responded to an NYT poll believed that the accident would produce a mushroom cloud explosion.&nbsp; Confusion about radiation seems to have particularly contributed to public panic.&nbsp; Many viewed radiation as a \u201cmysterious and dangerous\u201d force due to its ability to invisibly cause disease and death, which heightened their fear.&nbsp; Similarly, some commentators framed nuclear energy as an unnatural power which humans should not tamper with in response to TMI, invoking both Christian and classical rhetoric against playing God or opening the \u201cPandora\u2019s box\u201d of nuclear power.&nbsp; Thus, while lack of understanding can sometimes cause the public to be unconcerned about a risk, it can also lead to heightened perception of the risk once it is noticed, particularly if it seems spooky or alien.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsg7vjyo9olp\"><sup><a href=\"#fnsg7vjyo9olp\">[15]</a></sup></span></p><p>&nbsp;</p><p>&nbsp;</p><p><strong>Lesson #6: Capitalizing on warning shots is easier if you have allies</strong></p><p>While there are many factors that increase the probability of legislative action in response to a warning shot, one important one seems to be whether the actors pushing for legislative action have political allies.&nbsp; In the case of the US, the anti-nuclear energy movement made allies with social movements that were originally much larger than it, such as the broader environmental movement and the anti-nuclear weapons movement, and was organized by protestors with connections to the civil rights and women\u2019s liberation movements.&nbsp; The anti-nuclear energy movement in Finland seems to have adopted a similar strategy.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefpfaczfrs78m\"><sup><a href=\"#fnpfaczfrs78m\">[16]</a></sup></span>&nbsp; In Germany, the movement was initially coolly received by existing political parties but eventually found a major ally in the newly founded Green Party.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdhdwxaptz89\"><sup><a href=\"#fndhdwxaptz89\">[17]</a></sup></span>&nbsp; In contrast, in countries such as France, the anti-nuclear power movement was not similarly part of a larger coalition and did not have a political party supporting it.&nbsp; Even though nuclear power had similar levels of public support across the different countries, the French anti-nuclear power movement failed to halt the construction of new nuclear power plants, while it largely succeeded in Germany, Finland and the United States.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkaptvrp8yyh\"><sup><a href=\"#fnkaptvrp8yyh\">[18]</a></sup></span>&nbsp;&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>Lesson 7: The legislative response to a warning shot involving a technically complicated technology may be ineffective by default, though the private sector may respond more effectively</strong></p><p>While TMI prompted significant responses from legislators and regulators, these responses seem like they did not have actually done much to reduce the risk of nuclear meltdown.&nbsp; While the rate of development of nuclear reactors in the US slowed considerably in the 1980\u2019s, this seems to have been significantly if not entirely due to pre-existing economic trends and increased regulation during the 1960\u2019s and \u201870\u2019s rather than the legislation caused by TMI.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsm7wcucxfn\"><sup><a href=\"#fnsm7wcucxfn\">[19]</a></sup></span>&nbsp; Furthermore, internal reports by the NRC suggest that the agency was not very effective at regulation even after the reforms, as it often failed entirely to inspect plants and, when it did, lacked the technical knowledge to understand what changes were necessary.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2ix6gtndhmq\"><sup><a href=\"#fn2ix6gtndhmq\">[20]</a></sup></span>&nbsp; However, despite the lack of successful legal regulation, TMI prompted the nuclear power industry to informally regulate itself which was more effective due to the greater technical knowledge of people working in the nuclear power industry.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjxm1tvmncsj\"><sup><a href=\"#fnjxm1tvmncsj\">[21]</a></sup></span>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><h1><strong>Conclusion</strong></h1><p>Several facts about the TMI incident suggest that there are reasons to believe that, conditional on there being an AI warning shot, the public will become concerned about AI risk.&nbsp; Broadly, TMI serves as a proof of concept that the public can quickly become concerned with the safety of a technology and demand regulation of it in response to major accidents.&nbsp; Moreover, an AI warning shot seems likely to share some of the characteristics that seem to have contributed to the public\u2019s strong response to TMI.&nbsp; Firstly, just as there was increased coverage of safety problems with nuclear power in the years and months leading up to TMI, there has recently been increasing attention to the negative effects of AI such as algorithmic discrimination and polarization due to social media algorithms, which will likely only increase over time.&nbsp; Additionally, just as public concern with the safety of nuclear energy post-TMI was increased by feelings of distrust towards regulators and the nuclear power industry, the broad distrust of \u201cbig tech\u201d by the contemporary public seems likely to prime it to respond strongly to an AI warning shot.&nbsp; Finally, just like radiation, risk from AI is currently poorly understood by the public but, once it is made salient, is alien-seeming and threatening in a way that seems likely to further scare people.</p><p><br>However, even though these factors suggest that there is a substantial chance that the public becomes concerned about AI in response to a warning shot, further efforts are necessary to increase the odds that this reaction occurs and that it is translated into effective legislative action.&nbsp; The critical factor for the public's response to an AI warning shot which the AI governance community can attempt to influence is media coverage.&nbsp; If the analogy to TMI holds, both the immediate media response to an AI warning shot as well as the media environment prior to the warning shot, such as fictional accounts of AI risk and political discourse about the potential harms of AI, would be very important for how the public would react to an AI warning shot.&nbsp; Thus, aiming to influence media narratives about AI, as well as perhaps releasing fictional media such as an episode of a TV show (e. g. an episode of Black Mirror) or a movie which depicts a realistic seeming AI takeover situation might be effective pathways to increase the odds of a strong public response.&nbsp;&nbsp;</p><p><br>In addition to increasing the odds that the public becomes concerned in response to a warning shot, the AI governance community also needs to put itself in a position to actually influence legislation if the public becomes concerned about AI.&nbsp; To that end, the successes of the Finnish and US anti-nuclear movements which forged a broader coalition opposed to nuclear power are instructive.&nbsp; In the case of AI risk, forming a coalition with actors who seem like they could possibly favor stricter regulation of the AI industry such as those concerned with bias and discrimination and AI unemployment seems potentially worthwhile.&nbsp; If the AI governance community chooses to pursue political allies, then it should be mindful about who those are, as picking allies also means picking enemies.&nbsp; For example, if it were to align itself with the aforementioned movements, it would run the risk of being perceived as a left-wing movement and thus alienating conservatives.&nbsp; However, particularly given that it is probably smaller than the anti-nuclear movement was before TMI, the AI governance community seems unlikely to successfully influence policy in response to a warning shot by itself. &nbsp;</p><p>&nbsp;</p><p>Perhaps most importantly, even if put into a position of influence, the AI governance community will also need to have effective concrete policy proposals ready if the legislative response to a warning shot is to reduce AI risk. &nbsp;Without such carefully researched proposals, regulation is likely to be ineffective by default, because effective regulation of AI requires even more technical knowledge than effective regulation of nuclear energy which seems to have been beyond the capacity of the NRC. &nbsp;Indeed, given the potentially dim prospects for such regulation, it could also be wise for the AI governance community to attempt to influence AI developers to institute non-legal regulations in response to a warning shot than to influence the US government, as developers are likely to have more technical knowledge and be closer to the actual problem than regulators.</p><p><br>Zooming out, I think that major legislative action to reduce AI risk is unlikely without a warning shot, as neither the public nor politicians seem likely to be convinced by abstract arguments that AI could be very dangerous without concrete examples.&nbsp; Thus, to the extent that reducing AI risk through legislation is a viable strategy, understanding the dynamics of the public\u2019s response to a warning shot and how that response affects legislation is important.&nbsp; While this post has begun to address these questions, further research on this seemingly neglected question is necessary.</p><p><br><br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndqkcfw3ui4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdqkcfw3ui4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the various claims about public opinion, see Rosa, Eugene A., and Riley E. Dunlap. \u201cPoll Trends: Nuclear Power: Three Decades of Public Opinion.\u201d&nbsp;<i>The Public Opinion Quarterly</i>, vol. 58, no. 2, 1994, pp. 295\u2013324.&nbsp;<i>JSTOR</i>,&nbsp;<a href=\"http://www.jstor.org/stable/2749543\"><u>http://www.jstor.org/stable/2749543</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6lv3r7key7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6lv3r7key7\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Temples, James R. \u201cThe Nuclear Regulatory Commission and the Politics of Regulatory Reform: Since Three Mile Island.\u201d&nbsp;<i>Public Administration Review</i>, vol. 42, no. 4, 1982, pp. 355\u201362.&nbsp;<i>JSTOR</i>, <a href=\"https://doi.org/10.2307/975979\">https://doi.org/10.2307/975979</a>. Accessed 6 Aug. 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhkrzdujwbdw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhkrzdujwbdw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mazur, Allan. \u201cThe Journalists and Technology: Reporting about Love Canal and Three Mile Island.\u201d&nbsp;<i>Minerva</i>, vol. 22, no. 1, 1984, pp. 45\u201366.&nbsp;<i>JSTOR</i>, <a href=\"http://www.jstor.org/stable/41820553\">http://www.jstor.org/stable/41820553</a>. Accessed 10 Aug. 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng5k78f17c9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg5k78f17c9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Koopmans, Ruud, and Jan Willem Duyvendak. \u201cThe Political Construction of the Nuclear Energy Issue and Its Impact on the Mobilization of Anti-Nuclear Movements in Western Europe.\u201d&nbsp;<i>Social Problems</i>, vol. 42, no. 2, 1995, pp. 235\u201351.&nbsp;<i>JSTOR</i>, <a href=\"https://doi.org/10.2307/3096903\">https://doi.org/10.2307/3096903</a>. Accessed 8 Aug. 2022</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnitcaopq1bcp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefitcaopq1bcp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the previous claims about The China Syndrome, see &nbsp;Shaw, Tony. \u201c\u2018Rotten to the Core\u2019: Exposing America\u2019s Energy-Media Complex in \u2018The China Syndrome.\u2019\u201d&nbsp;<i>Cinema Journal</i>, vol. 52, no. 2, 2013, pp. 93\u2013113.&nbsp;<i>JSTOR</i>, <a href=\"http://www.jstor.org/stable/23360267\">http://www.jstor.org/stable/23360267</a>. Accessed 10 Aug. 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjbmlr0rq8q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjbmlr0rq8q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mazur. \"The Journalists and Technology,\" <a href=\"http://www.jstor.org/stable/41820553\">http://www.jstor.org/stable/41820553</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpm5airzjuam\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpm5airzjuam\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Shaw. \"'Rotten to the Core,'\" <a href=\"http://www.jstor.org/stable/23360267\">http://www.jstor.org/stable/23360267</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6c0rmdxj1b3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6c0rmdxj1b3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Halden, Grace. \u201cThree Mile Island: The Meltdown Crisis and Nuclear Power in American Popular Culture,\u201d p. 74</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrku8fxekghh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrku8fxekghh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Kasperson, Roger E., et al. \u201cPublic Opposition to Nuclear Energy: Retrospect and Prospect.\u201d&nbsp;<i>Science, Technology, &amp; Human Values</i>, vol. 5, no. 31, 1980, pp. 11\u201323.&nbsp;<i>JSTOR</i>, <a href=\"http://www.jstor.org/stable/689009\">http://www.jstor.org/stable/689009</a>. Accessed 8 Aug. 2022. and Shaw, \"'Rotten to the Core'\"</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4v8bzde5a1o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4v8bzde5a1o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Mazur. \"The Journalists and Technology,\" <a href=\"http://www.jstor.org/stable/41820553\">http://www.jstor.org/stable/41820553</a> and Nimmo, Dan. \u201cThe Return of Frankenstein: The Popular Media Aesthetic of Three Mile Island Coverage by ABC Evening News.\u201d&nbsp;<i>Studies in Popular Culture</i>, vol. 4, 1981, pp. 38\u201348.&nbsp;<i>JSTOR</i>, <a href=\"http://www.jstor.org/stable/45018075\">http://www.jstor.org/stable/45018075</a>. Accessed 10 Aug. 2022</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn68fl3aiw1w\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref68fl3aiw1w\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Kasperson et al. \"Public Opposition to Nuclear Energy: Retrospect and Prospect,\" and &nbsp;Cook, Earl. \u201cTHE ROLE OF HISTORY IN THE ACCEPTANCE OF NUCLEAR POWER.\u201d&nbsp;<i>Social Science Quarterly</i>, vol. 63, no. 1, 1982, pp. 3\u201315.&nbsp;<i>JSTOR</i>, <a href=\"http://www.jstor.org/stable/42861373\">http://www.jstor.org/stable/42861373</a>. Accessed 8 Aug. 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzwwh8f4end\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzwwh8f4end\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Halden. \u201cThree Mile Island,\u201d p. 75-77</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1v50lyq54o\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1v50lyq54o\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Halden. \u201cThree Mile Island,\u201d p. 77</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnoz75knqdpod\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefoz75knqdpod\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Halden. \"Three Mile Island,\" p. 78</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsg7vjyo9olp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsg7vjyo9olp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For the claims in this paragraph, see Halden. \"Three Mile Island,\" p. 81-83</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnpfaczfrs78m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefpfaczfrs78m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Litmanen, Tapio. \u201cInternational Anti-Nuclear Movements in France, Finland, and the United States.\u201d&nbsp;<i>Peace Research</i>, vol. 30, no. 4, 1998, pp. 1\u201319.&nbsp;<i>JSTOR</i>, <a href=\"http://www.jstor.org/stable/23607426\">http://www.jstor.org/stable/23607426</a>. Accessed 6 Aug. 2022 and&nbsp;Walsh, Edward J. \u201cResource Mobilization and Citizen Protest in Communities around Three Mile Island.\u201d&nbsp;<i>Social Problems</i>, vol. 29, no. 1, 1981, pp. 1\u201321.&nbsp;<i>JSTOR</i>, <a href=\"https://doi.org/10.2307/800074\">https://doi.org/10.2307/800074</a>. Accessed 8 Aug. 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndhdwxaptz89\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdhdwxaptz89\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Wiliarty, Sarah Elise. \u201cNuclear Power in Germany and France.\u201d&nbsp;<i>Polity</i>, vol. 45, no. 2, 2013, pp. 281\u201396.&nbsp;<i>JSTOR</i>, <a href=\"http://www.jstor.org/stable/24540209\">http://www.jstor.org/stable/24540209</a>. Accessed 10 Aug. 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkaptvrp8yyh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkaptvrp8yyh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Litmanen. \"International Anti-Nuclear Movements in Finland, France, and the United States,\" <a href=\"http://www.jstor.org/stable/23607426\">http://www.jstor.org/stable/23607426</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsm7wcucxfn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsm7wcucxfn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Nichols, Elizabeth. \u201cU. S. Nuclear Power And The Success Of The American Anti-Nuclear Movement.\u201d&nbsp;<i>Berkeley Journal of Sociology</i>, vol. 32, 1987, pp. 167\u201392.&nbsp;<i>JSTOR</i>,&nbsp;<a href=\"http://www.jstor.org/stable/41035364\"><u>http://www.jstor.org/stable/41035364</u></a>. Accessed 5 Aug. 2022. and <a href=\"https://constructionphysics.substack.com/p/why-are-nuclear-power-construction\">https://constructionphysics.substack.com/p/why-are-nuclear-power-construction</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2ix6gtndhmq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2ix6gtndhmq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Temples, James R. \u201cThe Nuclear Regulatory Commission and the Politics of Regulatory Reform: Since Three Mile Island.\u201d&nbsp;<i>Public Administration Review</i>, vol. 42, no. 4, 1982, pp. 355\u201362.&nbsp;<i>JSTOR</i>, https://doi.org/10.2307/975979. Accessed 6 Aug. 2022. and &nbsp;Campbell, John L. \u201cCorporations, Collective Organization, and the State: Industry Response to the Accident at Three Mile Island.\u201d&nbsp;<i>Social Science Quarterly</i>, vol. 70, no. 3, 1989, pp. 650\u201366.&nbsp;<i>JSTOR</i>,&nbsp;<a href=\"http://www.jstor.org/stable/42862627\"><u>http://www.jstor.org/stable/42862627</u></a>. Accessed 5 Aug. 2022</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjxm1tvmncsj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjxm1tvmncsj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Campbell. \"Corporations, Collective Organization, and the State,\" <a href=\"http://www.jstor.org/stable/42862627\"><u>http://www.jstor.org/stable/42862627\\</u></a></p></div></li></ol>", "user": {"username": "NickGabs"}}, {"_id": "bWSBpwkLC52u3ZRtk", "title": "Climate-contingent Finance, and A Generalized Mechanism for X-Risk Reduction Financing", "postedAt": "2022-09-26T13:23:07.462Z", "htmlBody": "<h1>Summary</h1><p>Climate adaptation (reducing vulnerability to future climate change) could yield significant benefits. However, the uncertainty of which future climate scenarios will occur decreases the feasibility of proactively adapting. Fortunately, climate adaptation projects could be underwritten by benefits paid for in the climate scenarios that each adaptation project is designed to address because other entities would like to hedge the financial risk of those scenarios.</p><p>For instance, many infrastructure projects can be built to withstand extreme climate change through upfront spending. The climate adaptation expenditures generate more climate resilience benefits under more extreme climates. Because the return on investment of many adaptation actions is a function of the level of climate change, it is optimal for the adapting entity to finance adaptation with repayment that is also a function of the climate. It is also optimal for entities with more financial downside under a more extreme climate to serve as an investing counter-party because they can obtain higher than market rates of return when they need it most.</p><p>In this way, communities, cities, and states proactively adapting would reduce the risk they over-prepare, while their investors would reduce the risk they under-prepare. This is superior to typical insurance because by investing in climate-contingent mechanisms, investors are not merely financially hedging but also outright preventing physical damage, and therefore creating economic value. Both sides of the positive-sum relationship \u2014 physical and financial hedgers \u2014 are made better off. This coordinates capital through time and place according to parties\u2019 risk reduction capabilities and financial profiles, while also providing a diversifying investment return to investors.</p><p>Governments, asset owners, and companies reduce uncertainty in components of the economy (e.g., commodities prices, credit risks, and interest rates) through trillions of dollars of derivatives positions and insurance contracts \u2013 we propose a solution to provide a similar capability in the climate context. Municipalities raise trillions of dollars of debt for infrastructure \u2013 we propose a solution to provide that type of investment flow for financing climate-aware real asset projects.</p><p><i>Climate-contingent finance is a fresh approach to addressing catastrophic risk, building a bridge between long-term funding needs and financial risk management. It can be generalized to any situation where multiple entities share exposure to a risk where they lack direct control over whether it occurs (e.g., climate change, or a natural pandemic), and one type of entity can take proactive actions to benefit from addressing the effects of the risk if it occurs (e.g., through innovating on crops that would do well under extreme climate change or vaccination technology that could address particular viruses) with funding from another type of entity that seeks a targeted financial return to ameliorate the downside if the risk unfolds. This approach can finance previously under-funded efforts to address risks to humanity\u2019s long-term flourishing, including extreme climate change, large asteroids hitting the earth, and super-volcanic eruptions.</i></p><blockquote><p><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3762733\"><strong>An article describing this mechanism in detail</strong></a><strong>, including a significant amount of computational simulation to explore different implementations, was just published by </strong><a href=\"https://www.law.berkeley.edu/library/ir/bblj/\"><strong>Berkeley Business Law Journal</strong></a><strong>. This post is primarily excerpts from that Article.</strong></p></blockquote><p>&nbsp;</p><h1>Problem</h1><blockquote><p><i>\u201c[Climate uncertainty] was marginal during previous centuries and, therefore, was often neglected in decision-making. Now, uncertainty in future climate change is so large that it makes many traditional approaches to designing infrastructure and other long-lived investments inadequate.\u201d</i><a href=\"#_ftn1\"><i><strong><sup>[1]</sup></strong></i></a></p></blockquote><p>Climate change depends on many political, social, and environmental factors.<a href=\"#_ftn2\"><sup>[2]</sup></a> A sharp reduction in greenhouse gases would be ideal; however, the development of the political, social, and technological solutions necessary remains uncertain.<a href=\"#_ftn3\"><sup>[3]</sup></a> For many scientific problems, uncertainty&nbsp;decreases&nbsp;with time. However, because of the positive feedback effects inherent in climate and sociopolitical systems,<a href=\"#_ftn4\"><sup>[4]</sup></a> uncertainty in the sensitivity of the climate to the level of emissions may <i>increase</i> over time. Uncertainty in the impact of carbon emissions on global temperature change has increased between the last release of the widely trusted global climate model ensemble and the release currently underway.<a href=\"#_ftn5\"><sup>[5]</sup></a> Fully accounting for the uncertainty in climate change significantly increases the costs of climate change and the expected benefits of adaptation.<a href=\"#_ftn6\"><sup>[6]</sup></a></p><p>Figure 1 compares projections from the U.S. National Oceanic and Atmospheric Administration (NOAA) of the number of days per year of significant flooding under low (left) and&nbsp;extreme&nbsp;(right) climate scenarios for 99 coastal U.S. cities.&nbsp;</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8a32d9f1eaf772b3bafe3040d482298f3f3bfb8eec38f059.png/w_1267 1267w\"></p><p><strong>Figure 1</strong>: Yearly projections of days per year that exceed a significant flooding threshold, under low climate scenario (left) and extreme climate scenario (right) for 99 U.S. coastal cities. Each location is a separate line in each chart.<a href=\"#_ftn7\"><sup>[7]</sup></a></p><p>&nbsp;</p><p>In&nbsp;Figure 2, we focus on one city, New York, to visualize the difference between climate assumptions over varying time periods. The long tail in the changes in the extreme scenario for NYC is even longer when we move from a five-year to&nbsp;a ten-year horizon. Within some ten-year periods, projected increases of the number of days per year of significant flooding range from zero to more than 150, from<i> nothing&nbsp;</i>to<i> catastrophic</i>.<a href=\"#_ftn8\"><sup>[8]</sup></a></p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f37afcd5cc9c1ce88ffc1a7e1d6d9ae76a1eafae0da4660.png/w_1098 1098w\"></p><p><strong>Figure 2</strong>: NOAA projections of days per year that exceed a significant flooding threshold for lower Manhattan in NYC from 2020-2100 transformed into change over <i>n</i>-years, where <i>n</i> is 5 or 10, for the low climate scenario (orange bars) and extreme climate scenario (blue bars). The extreme climate scenario has a fat right tail in its distribution. The ten-year forecasts (right chart) have a much more extreme right tail (note the x-axis range) than the five-year forecasts (left chart).<a href=\"#_ftn9\"><sup>[9]</sup></a></p><p>&nbsp;</p><p>Gavin Schmidt,&nbsp;one of the world\u2019s top climate scientists,&nbsp;said,&nbsp;</p><blockquote><p>\u201cDo we have enough information to know that sea level is rising? Yes. Do we have enough information to tell people whether to build a 1-meter wall or a 2-meter wall? The answer is no.\u201d<a href=\"#_ftn10\"><sup>[10]</sup></a>&nbsp;</p></blockquote><p>Figure 3 supports his assertion, illustrating sea-level rise uncertainty across a wide range of possible climate scenarios.</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3c6021328ffe0e4c7ccc18da3923ba61d3e0c0aaeee9dc1.jpeg/w_1240 1240w\"></p><p><strong>Figure 3</strong>: NOAA projections of sea level change.<a href=\"#_ftn11\">[11]</a></p><p>&nbsp;</p><p>Due to human-caused climate change, uncertainty in the future climate is higher than it was when the processes for designing and financing infrastructure were developed.&nbsp;A recent $14 billion upgrade to New Orleans' flood infrastructure now looks like it could be inadequate in just four years because the designs underestimated sea-level rise.<a href=\"#_ftn12\"><sup>[12]</sup></a> Going forward, is the new design going to be an over- or under-estimate?<a href=\"#_ftn13\"><sup>[13]</sup></a> Extreme but scientifically plausible climate scenarios would be devastating if not proactively addressed. However, the high uncertainty over whether an extreme scenario&nbsp;will materialize&nbsp;\u2014 and therefore the potential to undertake an overprotective project \u2014 can raise the cost of capital to prohibitive levels, or, more specifically, reduce the willingness to raise capital when the cost of that capital is not tied to the climate outcomes.<a href=\"#_ftn14\"><sup>[14]</sup></a> Louisiana\u2019s Coastal Protection and Restoration Authority developed a $50 billion plan to safeguard coastal populations, and only about $10 billion has been identified to support the plan.<a href=\"#_ftn15\"><sup>[15]</sup></a></p><p>As a report from the Hoover Institution at Stanford University puts it:<i>&nbsp;</i></p><blockquote><p>\"[Climate] uncertainty is new and distinct from risks that engineers routinely consider. It creates challenges for infrastructure planners and engineers unaccustomed to managing such ambiguities. There is a risk of over- or underbuilding, which can, in turn, transfer risks to infrastructure investors.\u201d<a href=\"#_ftn16\"><sup>[16]</sup></a>&nbsp;</p></blockquote><p>Most existing infrastructure is likely under-built, and some may be over-built, given the difficulty of making climate projections and embedding them into design processes.<a href=\"#_ftn17\"><sup>[17]</sup></a></p><p>Goldman Sachs Global Markets Institute believes that:</p><blockquote><p>[A]daptation could drive one of the largest infrastructure build-outs in history&nbsp;.&nbsp;. . Given the scale of the task, urban adaptation will likely need to draw on innovative sources of financing.&nbsp;. . .&nbsp;Cities won\u2019t want to over-commit to specific climate scenarios.&nbsp;. . .&nbsp;&nbsp;Taking an investment approach might suggest that it makes sense instead to \u201cwait and see,\u201d allowing time for new information to emerge before making any major investments. While this approach makes sense in many contexts, the case of climate change appears to be different. The most significant effects of climate change are likely to be the result of \u201ctail events,\u201d&nbsp;which are inherently unpredictable in both their timing and their severity. Waiting won\u2019t necessarily generate more information about these idiosyncratic events. Waiting may instead mean that cities run out of time to prevent severe damages.\u201d<a href=\"#_ftn18\"><sup>[18]</sup></a></p></blockquote><p>An attractive approach to&nbsp;handling&nbsp;this uncertainty is to take adaptation actions that are designed to have payoffs (by preventing harms) that are as similar as possible across as many plausible climate outcomes as possible, so called \u201cno-regrets\u201d adaptation actions (e.g., building extreme weather early-warning systems). No-regrets actions should be pursued and can likely be financed by traditional mechanisms.<a href=\"#_ftn19\"><sup>[19]</sup></a>&nbsp;</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_250 250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_1250 1250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_1500 1500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_1750 1750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_2000 2000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_2250 2250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2992927560a5f362be4345e2da7aff47f11513d60addd27d.png/w_2500 2500w\"></p><p><strong>Figure 4</strong>: Climate-aware financial analysis of an infrastructure project.&nbsp;</p><p>&nbsp;</p><p>However, truly no-regrets actions that significantly reduce physical risk are becoming rarer as more extreme scenarios are increasingly considered plausible. Some adaptation actions have to be designed and built, conditional on climate change scenarios. Yet it is difficult to design an action that would have similar payoffs across all plausible outcomes. An analysis of adaptation in the United States, the Philippines, and Britain concluded that, for the study areas, none of the flood protection projects had positive expected value for current climate conditions or a low climate change scenario, but all the strategies were economically attractive in the high climate change scenario.<a href=\"#_ftn20\"><sup>[20]</sup></a></p><p><strong>Few adaptation actions will deliver homogeneous benefits across climate scenarios.</strong><a href=\"#_ftn21\"><strong><sup>[21]</sup></strong></a><strong> Therefore, climate uncertainty translates into uncertainty in the benefits delivered by many adaptation actions. This is the core of the financing dilemma.</strong></p><p>Furthermore, adapting to future climate scenarios is not just an issue for infrastructure designed explicitly for reducing climate risks.&nbsp;It is a general problem for nearly all existing and future real assets.&nbsp;</p><blockquote><p>According to Morgan Stanley, \u201cclimate resilience is fast becoming an investment imperative in real assets.\u201d<a href=\"#_ftn22\"><sup>[22]</sup></a>&nbsp;</p></blockquote><p>Owners of real assets (infrastructure, buildings, and land) have long investment holding periods (often decades)<a href=\"#_ftn23\"><sup>[23]</sup></a> and high exposure to climate change impacts. However, they currently have no means to hedge this long-term climate uncertainty.<a href=\"#_ftn24\"><sup>[24]</sup></a> Hannah Nissan et al. point out the incongruity in how climate uncertainty is treated compared to other complex systems:</p><blockquote><p>Foresight about future exchange rates, oil prices, geopolitical disruptions, or epidemics of new diseases would be invaluable, but there is little expectation that such things can accurately be forecast beyond the short term. Despite high confidence in many aspects of present and future climate change, localized projections are highly unreliable. Where then does the unrealistic expectation come from that the future climate, among the most complex of known systems, should be predictable to the degree of precision often demanded?<a href=\"#_ftn25\"><sup>[25]</sup></a></p></blockquote><p>Long-term financial entities owning real assets will be forced into one of two groups: Adapters (\u201cA\u201d), proactively adapting; or Backers (\u201cB\u201d), absorbing impacts.<a href=\"#_ftn26\"><sup>[26]</sup></a> Both groups face obstacles that could be addressed by collaborating through a financial mechanism. Some entities are better positioned to move into Group<i>&nbsp;</i>A and reduce their physical risk now; while others will determine that reducing their financial risk without immediately reducing their physical risk is more feasible, and move into Group<i>&nbsp;</i>B for the time being.<a href=\"#_ftn27\"><sup>[27]</sup></a></p><p>Currently, most B entities are unable to effectively hedge the risk of climate outcomes. Municipal bonds have long-term climate risk. Given that there is no way to cleanly hedge municipal bond credit risk (the primary risk to these securities),<a href=\"#_ftn28\"><sup>[28]</sup></a> there is even less opportunity to cleanly hedge municipal bond climate risk. The vast majority of insurance is on a one-year time horizon, which is not helpful for locking in certainty of a hedge on a time scale relevant to climate change, because every year insurance providers can increase rates or stop providing insurance altogether.<a href=\"#_ftn29\"><sup>[29]</sup></a> Furthermore, insurance does not reduce risk in the aggregate. As a \u201crisk transfer\u201d mechanism, it merely shifts risk from one party to another.<a href=\"#_ftn30\"><sup>[30]</sup></a> If we physically reduce risk and prevent damages, we can generate more overall value and, in effect, share that value between parties. Therefore, a standard parametric insurance payout would likely provide a lower expected return than a triggered climate contract and serve as a less effective hedge.<a href=\"#_ftn31\"><sup>[31]</sup></a></p><p>&nbsp;</p><h1>Solution</h1><p>Party A proactively adapts, making physical changes that explicitly take climate change scenarios into consideration, funded by B<a href=\"#_ftn32\"><sup>[32]</sup></a> hedging financial risk of climate-induced losses (Figure 5).<a href=\"#_ftn33\"><sup>[33]</sup></a><sup> </sup>B provides upfront capital to A, who uses the proceeds for adaptation that will substantially reduce losses in more extreme climate scenarios. Under less extreme scenarios, the adaptation may be overprotective. A climate-related financial product of this nature was first proposed by Daniel Bloch and co-authors as a \u201cclimate default swap.\u201d<a href=\"#_ftn34\">[34]</a></p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_250 250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_1250 1250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_1500 1500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_1750 1750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_2000 2000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_2250 2250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/9ef0ca68b9693ebbe35d5ebf1eeb2e37b2c1ef1e4d02f886.png/w_2500 2500w\"></p><p><strong>Figure 5</strong>: Climate contract lifecycle.&nbsp;</p><p>&nbsp;</p><p>If&nbsp;the effects of&nbsp;climate change&nbsp;are&nbsp;worse than expected, A pays B back with a higher-than-market rate of return on the principal.<a href=\"#_ftn35\">[35]</a> Reducing risk of property and human health damages is likely to reduce losses (or even&nbsp;increase&nbsp;benefits) in &nbsp;more extreme climate change scenarios.<a href=\"#_ftn36\"><sup>[36]</sup></a> In addition to averting damage, adaptation measures&nbsp;reduce A\u2019s cost of capital for general operations by increasing their creditworthiness: according to BlackRock research, \u201cbonds issued by climate-resilient states and cities are likely to trade at a premium to those of vulnerable ones over time.\u201d<a href=\"#_ftn37\"><sup>[37]</sup></a><sup> </sup>A is insuring against risk of ruin by building adaptation projects and only paying for that \u201cinsurance\u201d in scenarios where those projects are most needed. The financing allows A to take steps to realize the benefits of adaptation and hedge against overprotecting while doing so.<a href=\"#_ftn38\"><sup>[38]</sup></a></p><p>Benefits of climate adaptation for a municipality include reduced insurance premiums,<a href=\"#_ftn39\"><sup>[39]</sup></a> reduced future uninsured direct damages to property and infrastructure assets, reduced future costs for rebuilding,<a href=\"#_ftn40\"><sup>[40]</sup></a> reduced potential litigation costs,<a href=\"#_ftn41\"><sup>[41]</sup></a> reduced cost of capital for borrowing,<a href=\"#_ftn42\"><sup>[42]</sup></a> maintained attractiveness of the area for outside investment and in-migration, maintained property tax, sales tax, and tourism tax revenues,<a href=\"#_ftn43\">[43]</a> avoided tail-risk scenarios of collapsing property values and business activity that could lead to a downward spiral and complete abandonment, and increased revenue for natural adaptation solutions from selling carbon credits.<a href=\"#_ftn44\"><sup>[44]</sup></a></p><p>Meanwhile, B is better off because its returns through its hedge are greater than returns from other investments in a more extreme climate outcome state of the world.<a href=\"#_ftn45\"><sup>[45]</sup></a> We explore values of the repayment rates in the \u00ad\u00ad\u00adsimulation experiments in <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3762733\">the longer-form paper</a>.<a href=\"#_ftn46\"><sup>[46]</sup></a> The rate of return required by B may be relatively low because the investment pays off specifically in states of the world with high marginal utility; in more extreme climate scenarios, a dollar is worth more than in less extreme climate scenarios.<a href=\"#_ftn47\"><sup>[47]</sup></a> Investing allows B to hedge against physical under-preparedness in a way that\u2019s directly linked to their climate exposures. B parties that are taking a \u201cwait-and-see\u201d approach to climate adaptation through participation in this investment can gain information on adaptation project outcomes \u2014 and generate capital in the triggered scenarios \u2014 to implement their own (less proactive) adaptation projects in the future.</p><p>If&nbsp;the effects of&nbsp;climate change&nbsp;are less severe than expected, A is no worse off than they would have been otherwise, and probably better off. First, they may have over-prepared (at least within the timeline of the repayment, potentially not later), but they paid less for it than a traditional bond repayment. Second, they are more prepared for a future increase in climate change that may still occur over a longer time period beyond the end of the repayment. Third, there are sometimes resilience \u201cco-benefits\u201d that the adaptation projects serve. B was repaid less than they would have been with a traditional debt investment, but their climate risk was hedged enough during the ensuing period that they were able to continue operating and borrowing at lower rates.</p><p>&nbsp;</p><h2><i>A Generalized X-Risk Reduction Financing Mechanism</i></h2><p>This post focuses on climate change, but we believe the generalized structure of this risk-contingent financing&nbsp;mechanism&nbsp;applies to any situation where multiple entities&nbsp;share&nbsp;exposure to&nbsp;a&nbsp;risk out of their direct control,<a href=\"#_ftn48\">[48]</a><i> R</i>, and one type of entity, <i>A</i>, can take proactive actions to benefit from (either through avoided losses or through absolute gains) addressing <i>R</i> if it occurs with funding from another type of entity, <i>B</i>, that&nbsp;seeks a targeted financial return to ameliorate the downside if <i>R</i> unfolds.</p><p>Examples of risks, <i>R</i>, that are appropriate for this type of financing include extreme climate change, natural pandemics, and large asteroids hitting the earth. Examples of proactive actions to benefit from addressing systemic risks include innovating on crops that would do well under extreme climate change, vaccination technologies that would address particular viruses, and mechanisms that deflect large asteroids from earth impact. The actions to mitigate <i>R<strong>&nbsp;</strong></i>yield a payoff <i>P</i> if <i>R</i> occurs (through avoided losses or through absolute gains).</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/94007f4f98b841dbe7e80f873912a070b1bc5b201d1ee34c.png/w_1431 1431w\"></p><p><strong>Figure 6a:</strong> Generalized risk-contingent financing structure.</p><p>&nbsp;</p><p>Insurance does not reduce risk, when measured in the aggregate; insurance shifts risk from one party to another.<a href=\"#_ftn49\"><sup>[49]</sup></a> If, instead, we physically reduce risk and prevent damages, we generate more overall value, which can be shared between parties. Therefore, a parametric insurance payout provides a lower expected return than a triggered contingent contract and serves as a less effective hedge.<a href=\"#_ftn50\">[50]</a> The key is recognizing the payoffs that proactive risk reduction would have under the negative states of the world, and then, in effect, \u201csecuritizing\u201d those payoffs to raise capital to fund the risk reduction.</p><p>Another important downside to traditional insurance is that it works by diversifying lowly correlated risks. If a source of risk is systemic, affecting most parties and therefore creating set of highly correlated risks, then it cannot be diversified away. To address systemic risk, it would need to be proactively reduced.</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bd72124ba08bbf2f53ab2c9ab53105566b9f75a763b6af9d.png/w_1428 1428w\"></p><p><strong>Figure 6b:</strong> Comparison of risk-contingent financing to traditional insurance.</p><p>&nbsp;</p><p><i>A</i> makes changes that explicitly aim to reduce risk, funded by <i>B</i><a href=\"#_ftn51\"><sup>[51]</sup></a> hedging financial risk. <i>B</i> provides upfront capital to <i>A</i>, who uses the proceeds to reduce their losses in more extreme negative scenarios (Figure 7). In the climate example, <i>A</i> might be building a tall seawall designed for extreme climate change, for instance. Under less extreme scenarios, the actions taken may be overprotective.</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/96d2bf46f50032948504fc30e7bd4acc05bd99eb40c53ed6.png/w_1092 1092w\"></p><p><strong>Figure 7:</strong> The generalized structure of the risk-contingent financing mechanism, which applies to any situation where multiple entities share exposure to an underlying systemic risk, and one type of entity, <i>A</i>, can take proactive actions to benefit from (either through avoided losses or through absolute gains) addressing that negative risk (if it occurs) with funding from another type of entity, <i>B</i>, that seeks a targeted financial return to ameliorate the downside (if it occurs).</p><p>&nbsp;</p><p>The risk-contingent financing mechanism provides capital from <i>B</i> to <i>A</i> at issuance in return for the obligation that <i>A</i> pay back principal and a return if the negative scenario, or one more extreme, is realized before expiration of the contract, i.e., if <i>R</i> occurs in the specified time range. The amount <i>B</i> pays <i>A</i> initially (<i>Principal</i>), the amount that <i>A</i> would pay back <i>B</i> if triggered (<i>Return</i>), the <i>Scenario</i> beyond which triggers the payback, and the length of the <i>Term</i> within which the trigger must be passed to cause payout are all specified when the contract is initially sold.</p><p>Contract specifications at the time of initialization:</p><ul><li><i>Principal</i> (e.g., $15 million)</li><li><i>Return</i> (e.g., 150%)</li><li><i>Scenario</i> (e.g., sea-level 1.5 inches above baseline for more than 1 year)</li><li><i>Term</i> (e.g., 15 years)</li></ul><p>Contract participants:</p><ul><li><i>A</i> (e.g., an airport building a seawall)</li><li><i>B</i> (e.g., a&nbsp;set&nbsp;of banks and insurance companies)</li></ul><p>This&nbsp;general construct of a risk-contingent financial \u201ccontract\u201d can be used to create single-trigger swap financial products at one time horizon (what we have described thus far), or debt-like products that have variable periodic interest rates contingent on the Scenario at multiple time horizons. The latter can be created by simply&nbsp;composing a multi-period repayment structure from a series of these contracts at different time horizons.</p><p>In the climate context, there is a spectrum spanning the extent to which repayment of principal is tied to a climate change outcome, with traditional debt at 0%, and the structure as described above at 100%. In between the two extremes, repayment could be partially tied to the climate variable; as the climate variable approaches the threshold, the repayment rate increases to a rate similar to traditional debt, and then surpasses that rate as the climate variable passes the threshold.</p><p>&nbsp;</p><p><strong><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_250 250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_1250 1250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_1500 1500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_1750 1750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_2000 2000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_2250 2250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e6222af6a76f351a879bb8c115b0e0149d7c8ee720d3cbc.png/w_2500 2500w\"></strong></p><p><strong>Figure 8</strong>: The cumulative percentage repayment of the principal for a traditional bond is not linked to the climate change that might occur during the life of the bond, rather, the amount repaid is purely a function of the interest rate&nbsp;\u2013 this is plotted as the grey dashed line. The extent to which repayment is linked to climate change and the amount of climate change both impact the amount repaid for climate contract bonds. If the climate ends up being more extreme, then the repayment schedule will be shaped like the one of the top two (red) lines. If the climate ends up being moderate, then the repayment schedule will be shaped like the one of the bottom two (blue) lines.</p><p>&nbsp;</p><p>A climate-contingent repayment structure can be applied to any climate-related variable(s)<a href=\"#_ftn52\"><sup>[52]</sup></a> or combinations thereof at any time scale to fund any adaptation projects designed to reduce risk exposure to specific climate thresholds.<a href=\"#_ftn53\">[53]</a> The potential scope of climate-contingent finance is vast. The <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3762733\">longer paper</a> focuses on sea-level rise in cities to illustrate the value that climate-contingent financing can provide.&nbsp;</p><p>Figure 9 lists types of potential participants to the climate contingent financing mechanisms. Resulting mark-to-market pricing data could guide policy-makers on market expectations of climate outcomes. If there is eventually secondary trading of underlying contracts, prices on specific contracts would reveal up-to-date estimates of specific climate risks, globally guiding public policy and planning.&nbsp;</p><p>&nbsp;</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_250 250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_1000 1000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_1250 1250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_1500 1500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_1750 1750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_2000 2000w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_2250 2250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/d7368aef0b34e7c2561568a01a6b402d4ac908076c3fd097.png/w_2500 2500w\"></p><p><strong>Figure 9</strong>: Conceptual diagram of Climate Contracts linked to thresholds on sea-level rise (SLR) and temperature anomaly (TA) values. Examples of Limited Partners in a set of commingled funds or separately managed accounts that invest in <i>B</i> positions in the climate contracts and examples of parties to the <i>A</i> side of the contracts are listed. In addition to <i>A</i> and <i>B</i> market participants hedging exposures, there will likely also be investors participating in order to achieve returns uncorrelated from traditional asset classes and potentially actors stabilizing markets. Many <i>B</i> participants will view their participation in the fund(s) as both a hedge and an opportunity for an uncorrelated real return. There would be essentially no capacity constraint on the size of the fund(s); they could invest hundreds of billions of dollars without decreasing expected returns.</p><hr><p><a href=\"#_ftnref1\"><sup>[1]</sup></a>&nbsp;St\u00e9phane Hallegatte<i>, Strategies to Adapt to an Uncertain Climate Change</i>, 19 Glob.&nbsp;Env\u2019t Change 240, 246 (May 2009), https://www.sciencedirect.com/science/article/pii/S0959378008001192.</p><p><a href=\"#_ftnref2\"><sup>[2]</sup></a> For instance, sea-level rise is a function of global emissions, the effect of emissions on temperature, and the effect of temperature on oceanographic changes, according to the <i>New York City Panel on Climate Change</i>, NYC Mayor\u2019s Off. of Climate Resiliency,&nbsp;<a href=\"https://www1.nyc.gov/site/orr/challenges/nyc-panel-on-climate-change.page\">https://www1.nyc.gov/site/orr/challenges/nyc-panel-on-climate-change.page</a>&nbsp;(last visited Dec. 26, 2021).&nbsp;For more on uncertainty in sea-level rise and its implications, see generally<i>&nbsp;</i>Robert E. Kopp et al., <i>Usable Science for Managing the Risks of Sea-Level Rise</i>, 7 Earth\u2019s Future 1235 (2019),&nbsp;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018EF001145. <i>See also</i> Marjolijn Haasnoot et al., <i>Generic Adaptation Pathways for Coastal Archetypes Under Uncertain Sea-Level Rise</i>, Env\u2019t Rsch. Commc\u2019ns 1 (2019),&nbsp;https://iopscience.iop.org/article/10.1088/2515-7620/ab1871 (\u201cAdaptation to coastal flood risk is hampered by high uncertainty in the rate and magnitude of sea-level rise. Subsequently, adaptation decisions carry strong risks of under- or over-investment, and could lead to costly retrofitting or unnecessary high margins.\u201d).</p><p><a href=\"#_ftnref3\"><sup>[3]</sup></a>&nbsp;<i>See generally</i> Hannah Nissan et al., <i>On the Use and Misuse of Climate Change Projections in International Development</i>, 10 WIREs Climate Change,&nbsp;no. 579,&nbsp;,&nbsp;https://doi.org/10.1002/wcc.579;&nbsp;<i>see generally</i> Tanya Fiedler et al., <i>Business Risk and the Emergence of Climate Analytics</i>, 11 Nature Climate Change87, 91 (2021), https://www.nature.com/articles/s41558-020-00984-6; <i>see generally&nbsp;</i>Peiran R. Liu &amp; Adrian E. Raftery, <i>Country-Based Rate of Emissions Reductions Should Increase by 80% Beyond Nationally Determined Contributions to Meet the 2 \u00baC Target</i>, 2 Commc\u2019ns Earth &amp; Env\u2019t,&nbsp;no. 29,&nbsp; 1, 6-7,&nbsp;https://www.nature.com/articles/s43247-021-00097-8.&nbsp;Even if we know the level of emissions, \u201cGCMs, although relatively consistent for global average results, exhibit large inter-model variability for regional climate projections.\u201d Lei Zhao et al., <i>Global Multi-Model Projections of Local Urban Climates</i>, 11 Nature Climate Change 152, 152 (2021),&nbsp;https://www.nature.com/articles/s41558-020-00958-8</p><p><a href=\"#_ftnref4\"><sup>[4]</sup></a>&nbsp;<i>See, e.g</i>., Zeke Hausfather &amp; Richard Betts, <i>Analysis: How \u2018Carbon-Cycle Feedbacks\u2019 Could Make Global Warming Worse</i>, Carbon Brief&nbsp;(Apr. 14, 2020),&nbsp;https://www.carbonbrief.org/analysis-how-carbon-cycle-feedbacks-could-make-global-warming-worse<s>.</s></p><p><a href=\"#_ftnref5\"><sup>[5]</sup></a> See generally&nbsp;<i>WCRP Coupled Model Intercomparison Project</i>, WCRP: World Climate Rsch. Programme&nbsp;(2021),&nbsp;https://www.wcrp-climate.org/wgcm-cmip; and Zeke Hausfather, <i>CMIP6: The Next Generation of Climate Models Explained</i>, Carbon Brief (Dec. 2, 2019), https://www.carbonbrief.org/cmip6-the-next-generation-of-climate-models-explained. Even the uncertainty in simulating historical temperature observations \u2014 in hindcasting (not forecasting) \u2014 has increased since the last release. For a comparison of the implications for sea-level rise,&nbsp;see generally Stefan Hofer et al., <i>Greater Greenland Ice Sheet Contribution to Global Sea Level Rise in CMIP6</i>, 11 Nature Commc\u2019ns, no. 6289, Dec. 15, 2020, https://www.nature.com/articles/s41467-020-20011-8</p><p><a href=\"#_ftnref6\"><sup>[6]</sup></a>&nbsp;<i>See generally&nbsp;</i>Raphael Calel et al., <i>Temperature Variability Implies Greater Economic Damages from Climate Change</i>, 11 Nature Commc\u2019ns,&nbsp;no. 5029,&nbsp;Oct. 6, 2020, at 1,&nbsp;https://www.nature.com/articles/s41467-020-18797-8/.&nbsp; &nbsp;&nbsp;</p><p><a href=\"#_ftnref7\"><sup>[7]</sup></a> Data from&nbsp;William V. Sweet et al., NOAA Technical Report NOS CO-OPS 096: Patterns and Projections of High Tide Flooding Along the U.S. Coastline Using a Common Impact Threshold 41-43 (Feb. 2018), https://tidesandcurrents.noaa.gov/publications/techrpt86_PaP_of_HTFlooding.pdf.</p><p><a href=\"#_ftnref8\"><sup>[8]</sup></a>&nbsp;The catastrophic outcomes would be due to passing \u201ctipping points\u201d in the climate. The consensus estimates of the temperature at which a tipping point could be reached continues to be moved lower as the science is better understood. Timothy M. Lenton et al., <i>Climate Tipping Points \u2013 Too Risky to Bet Against</i>, Nature, Apr. 9, 2020,&nbsp;https://www.nature.com/articles/d41586-019-03595-0.&nbsp; &nbsp;</p><p><a href=\"#_ftnref9\"><sup>[9]</sup></a> Data from&nbsp;William V. Sweet et al., NOAA Technical Report NOS CO-OPS 096: Patterns and Projections of High Tide Flooding Along the U.S. Coastline Using a Common Impact Threshold 41-43 (Feb. 2018), https://tidesandcurrents.noaa.gov/publications/techrpt86_PaP_of_HTFlooding.pdf.</p><p><a href=\"#_ftnref10\"><sup>[10]</sup></a>&nbsp;Doug Struck, <i>Gavin Schmidt: The Problem with Climate Models? People</i>., Christian Sci. Monitor (Jan. 22, 2021),&nbsp;https://www.csmonitor.com/Environment/2021/0122/Gavin-Schmidt-The-problem-with-climate-models-People.</p><p><a href=\"#_ftnref11\">[11]</a> Data from William V. Sweet et al., NOAA Technical Report NOS CO-OPS 096: Patterns and Projections of High Tide Flooding Along the U.S. Coastline Using a Common Impact Threshold (2018),&nbsp;<a href=\"https://tidesandcurrents.noaa.gov/publications/techrpt86_PaP_of_HTFlooding.pdf\">https://tidesandcurrents.noaa.gov/publications/techrpt86_PaP_of_HTFlooding.pdf</a>.</p><p><a href=\"#_ftnref12\"><sup>[12]</sup></a>&nbsp;Thomas Frank, <i>After a $14-Billion Upgrade, New Orleans\u2019 Levees Are Sinking</i>, Sci. Am. (2019),&nbsp;https://www.scientificamerican.com/article/after-a-14-billion-upgrade-new-orleans-levees-are-sinking/.&nbsp;</p><p><a href=\"#_ftnref13\"><sup>[13]</sup></a>&nbsp;Projections of sea-level rise estimated on climate model responses fall below simple extrapolation based on recent observational data, i.e., sea-level rise is even worse than the models thought it would be. <i>See generally&nbsp;</i>Aslak Grinsted &amp; Jens Hesselbjerg Christensen, <i>The Transient Sensitivity of Sea Level Rise</i>, 17 Ocean Sci. 181 (2021),&nbsp;https://os.copernicus.org/articles/17/181/2021/#:~:text=We%20define%20a%20new%20transient,temperature%20increases%20on%20this%20timescale. &nbsp;&nbsp;</p><p><a href=\"#_ftnref14\"><sup>[14]</sup></a>&nbsp;Regarding the cost of capital for climate risk investments, Pittsburgh Water &amp; Sewer Authority said: \u201cWe are constantly evaluating ways to reduce our borrowing costs.\u201d<strong>&nbsp;</strong>Executive Director of Pittsburgh Water &amp; Sewer Authority: \u201cWe need to look ahead and determine whether or not we should be thinking of a much higher degree of prevention or protection and how much can we afford.\u201d J. Dale Shoemaker, <i>How Pittsburgh is Funding the Fight Against Climate Change,</i> PublicSource (Sept. 30, 2019), https://www.publicsource.org/how-pittsburgh-is-funding-the-fight-against-climate-change/.</p><p><a href=\"#_ftnref15\"><sup>[15]</sup></a>&nbsp;<i>Financing Resilient Communities and Coastlines: How Environmental Impact Bonds Can Accelerate Wetland Restoration in Louisiana and Beyond</i>, Env\u2019t Def. Fund 16 (Aug. 2018),&nbsp;https://www.edf.org/sites/default/files/documents/EIB_Report_August2018.pdf.</p><p><a href=\"#_ftnref16\"><sup>[16]</sup></a>&nbsp;Alice Hill&nbsp;et al.,&nbsp;Ready for Tomorrow: Seven Strategies for Climate-Resilient Infrastructure 4 (Hoover Inst., 2019),&nbsp;https://www.hoover.org/research/ready-tomorrow-seven-strategies-climate-resilient-infrastructure. &nbsp;</p><p><a href=\"#_ftnref17\"><sup>[17]</sup></a> <i>See generally</i>&nbsp;B. Shane Underwood et al., <i>Past and Present Design Practices and Uncertainty in Climate Projections are Challenges for Designing Infrastructure to Future Conditions</i>, 26 J. Infrastructure Sys. (2020)&nbsp;https://ascelibrary.org/doi/abs/10.1061/%28ASCE%29IS.1943-555X.0000567. &nbsp;</p><p><a href=\"#_ftnref18\"><sup>[18]</sup></a>&nbsp;Amanda Hindlian et al., <i>Taking the Heat: Making Cities Resilient to Climate Change</i>, Goldman Sachs Glob. Mkt. Inst.,&nbsp;(Sept. 4, 2019), https://www.goldmansachs.com/insights/pages/gs-research/taking-the-heat/report.pdf.&nbsp;</p><p><a href=\"#_ftnref19\"><sup>[19]</sup></a>&nbsp;Especially when adaption actions have general economic development co-benefits. <i>See generally&nbsp;</i>John J. Nay et al., <i>A Review of Decision-Support Models for Adaptation to Climate Change in the Context of Development</i>, 6 Climate Change 357 (Feb. 10, 2014),&nbsp;https://www.tandfonline.com/doi/full/10.1080/17565529.2014.912196.&nbsp;<i>See</i> Figure 4 of this post for a visual explanation.</p><p><a href=\"#_ftnref20\"><sup>[20]</sup></a>&nbsp;Jeroen Aerts&nbsp; et al., <i>Evaluating Flood Resilience Strategies for Coastal Megacities</i>, 344 Science 473, 473-75 (May 2, 2014),&nbsp;https://www.science.org/doi/full/10.1126/science.1248222.&nbsp; &nbsp;&nbsp;</p><p><a href=\"#_ftnref21\"><sup>[21]</sup></a> S. Hallegatte et al., <i>Strengthening New Infrastructure Assets: A Cost-Benefit Analysis</i> 11 (World Bank Pol\u2019y Rsch. Working Paper No. 8896, 2019),&nbsp;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3430506 (\u201c[C]limate change makes the strengthening of infrastructure assets even more important. Without climate change, the median benefit\u2010cost ratio would be equal to 2, but it is doubled when climate change is considered. And the fraction of scenarios in which strengthening infrastructure is not profitable is decreased from 14 to 4 percent when climate change is taken into account.\u201d); C.M. Shreve &amp; I. Kelman, <i>Does Mitigation Save? Reviewing Cost-Benefit Analyses of Disaster Risk Reduction</i>, 10 Int\u2019l J. Disaster Risk Reduction 213, 231 (Dec. 2014),&nbsp;https://www.sciencedirect.com/science/article/pii/S2212420914000661 (\u201cFor changes in flood regimes, as a result of climate change as well as infrastructure development, understanding the hazard and vulnerability changes is much more challenging with larger uncertainties. DRR [disaster risk reduction] CBAs [cost-benefit analyses] might have different levels of usefulness depending on the hazard and depending on the hazard drivers, such as climate change, which are considered for analyzing CBAs in forward-looking studies\u201d); <i>See generally</i>, Borja G. Reguero et al., <i>Comparing the Cost Effectiveness of Nature-Based and Coastal Adaptation: A Case Study from the Gulf Coast of the United States</i>, 13 PlosOne 1, 8 (Apr. 11, 2018),&nbsp;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192132; <i>See generally&nbsp;</i>Audrey Baills et al., <i>Assessment of Selected Climate Change Adaptation Measures for Coastal Areas</i>, 185 Ocean &amp; Coastal Mgmt,&nbsp;Mar. 1, 2020, at1,&nbsp;https://www.sciencedirect.com/science/article/pii/S0964569119309287. &nbsp;</p><p><a href=\"#_ftnref22\"><sup>[22]</sup></a>&nbsp;Morgan Stanley, <i>Weathering the&nbsp;Storm: Integrating Climate Resilience into&nbsp;Real Assets Investing&nbsp;</i>3&nbsp;(2018),&nbsp;https://www.morganstanley.com/im/publication/insights/investment-insights/ii_weatheringthestorm_us.pdf. Adaptation actions may have been taken during previous climates as well; L. Supriya, <i>Ta\u00edno Stilt Houses May Have Been an Adaptation to Climate Change</i>, Eos (Jan. 15, 2021), https://eos.org/articles/taino-stilt-houses-may-have-been-an-adaptation-to-climate-change.&nbsp;</p><p><a href=\"#_ftnref23\"><sup>[23]</sup></a>&nbsp;E.g., coal-fired power plants are designed for 40 to 50 years of production, hydro-power infrastructure is designed for up to 100 years, and approximately 66% of U.S. city infrastructure is more than 30 years old today. Jonathan Woetzel et al., <i>Will Infrastructure Bend or Break Under Climate Stress?</i>, McKinsey Glob. Inst. (Aug. 19, 2020),&nbsp;https://www.mckinsey.com/business-functions/sustainability/our-insights/will-infrastructure-bend-or-break-under-climate-stress.</p><p><a href=\"#_ftnref24\"><sup>[24]</sup></a>&nbsp;In <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3762733\">the longer paper</a>, see Appendix A: Climate Risk Pricing, for a discussion of how climate uncertainty affects asset pricing and why real asset owners are the most vulnerable to physical climate-induced price changes.</p><p><a href=\"#_ftnref25\"><sup>[25]</sup></a> Hannah Nissan et al., <i>supra&nbsp;</i>note 6, at 6. &nbsp;</p><p><a href=\"#_ftnref26\"><sup>[26]</sup></a>&nbsp;E.g., a city, a homeowner, a mortgage lender, a state government or a transportation group such as the New York City Metropolitan Transportation Authority that runs the public transportation system.</p><p><a href=\"#_ftnref27\"><sup>[27]</sup></a>&nbsp;An entity could be an Adapter or a Backer at different time scales and geographies. There will likely be more Backer parties than Adapter&nbsp;parties, i.e., a lot of entities would like to insure at least a small amount of their climate risks but not as many can build large physical adaptation projects.</p><p><a href=\"#_ftnref28\"><sup>[28]</sup></a>&nbsp;Ming-Jie Wang, <i>Credit Default Swaps on Municipal Bonds: A Double-Edged Sword?</i>, 35 Yale J. Reg 301, 307 (2018).</p><p><a href=\"#_ftnref29\"><sup>[29]</sup></a>&nbsp;<i>See, e.g</i>., Ange Lavoipierre &amp; Stephen Smiley, <i>Could Climate Change Make it Harder to Get Insurance in Australia?</i>, The Signal News (Feb. 5, 2019),&nbsp;https://www.abc.net.au/news/2019-02-06/could-climate-change-make-australia-uninsurable/10783490. &nbsp;</p><p><a href=\"#_ftnref30\"><sup>[30]</sup></a>&nbsp;\"While current ILS instruments are useful for transferring risks, they are not designed to reduce underlying risks or build resilience to disasters.\u201d Lauren Carter, <i>Can Insurance-Linked Securities Mobilize Investment in Climate Adaptation?</i>, UNDP (Jan. 12, 2021),&nbsp;https://www.undp.org/blog/can-insurance-linked-securities-mobilize-investment-climate-adaptation. &nbsp;&nbsp;</p><p><a href=\"#_ftnref31\"><sup>[31]</sup></a>&nbsp;\u201cAs [parametric insurance policies have been taken out by municipalities,] those policies have gotten more popular, they\u2019ve started to run into serious questions\u2014like [...] whether the policies\u2019 existence allows governments to punt on harder decisions about where people live and businesses operate in the first place.\u201d Zack Colman, <i>Insurance for When FEMA Fails</i>, POLITICO (Jul. 14, 2020),&nbsp;https://www.politico.com/news/agenda/2020/07/14/climate-change-fema-insurance-341816.</p><p><a href=\"#_ftnref32\"><sup>[32]</sup></a>&nbsp;The plan is to have many Backer entities per Adapter, with a fund that has Backers as&nbsp;limited partners and invests in many climate contracts.</p><p><a href=\"#_ftnref33\"><sup>[33]</sup></a>&nbsp;<i>See generally</i> Daniel Bloch et al., <i>Applying Climate Derivatives to Flood Risk Management</i>, 56 Wilmott 88 (2012),&nbsp;https://onlinelibrary.wiley.com/doi/abs/10.1002/wilm.10058 (discussing the concept of climate change derivatives to mitigate flood risk); <i>see also&nbsp;</i>Daniel Bloch et al., <i>Cracking the Climate Change Conundrum with Derivatives</i>, 2 Wilmott J. 271 (2010),&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/wilj.41\">https://onlinelibrary.wiley.com/doi/abs/10.1002/wilj.41</a>&nbsp;(original article discussing the concept abstractly);&nbsp;See Figure 5 for a visual representation of climate derivatives.</p><p><a href=\"#_ftnref34\">[34]</a>&nbsp;Bloch et al. (2010), <i>supra</i> note 36,<i>&nbsp;</i>at 271-272.&nbsp; &nbsp;&nbsp;&nbsp;</p><p><a href=\"#_ftnref35\">[35]</a> Daniel Bloch, James Annan, &amp; Justin Bowles, <i>Cracking the Climate Change Conundrum with Derivatives</i>, 2 Wilmott J. 271 (2010),&nbsp;<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/wilj.41\">https://onlinelibrary.wiley.com/doi/abs/10.1002/wilj.41</a></p><p><a href=\"#_ftnref36\"><sup>[36]</sup></a>&nbsp;<i>NIBS Finds Investment in Resilient Design Can Pay Off by More than Sixfold</i>, ARCHITECT Magazine. (Jan. 18, 2018),&nbsp;https://www.architectmagazine.com/technology/nibs-finds-investment-in-resilient-design-can-pay-off-by-more-than-sixfold_o. According to Seung Kyum Kim, raising foundations provides 6.6% and 14.3% housing price increases in Miami-Dade and NYC, and adaptation for storm surges provides a 15.8% housing price increase in Miami-Dade. There\u2019s direct loss mitigation value in the event of a climate threshold crossing and, regardless of risk events, there is asset price appreciation due to the recognition of that resilience.&nbsp;Seung Kyum Kim,<i>&nbsp;</i>The Economic Effects of Climate Change Adaptation Measures: Evidence from Miami-Dade County and New York City 2, 24 (May 2019) (Doctor of Design dissertation fellowship working paper) (on file with the Joint Center for Housing Studies of Harvard University),&nbsp;https://www.jchs.harvard.edu/research-areas/working-papers/economic-effects-climate-change-adaptation-measures-evidence-miami. Delavane B. Diaz, <i>Estimating Global Damages from Sea Level Rise with the Coastal Impact and Adaptation Model (CIAM)</i>, 137 Climatic Change. 143, 143 (2016)&nbsp;&nbsp;(\u201c[T]here is large potential for coastal adaptation to reduce the expected impacts of SLR compared to the alternative of no adaptation, lowering global net present costs through 2100 by a factor of seven . . . .\u201d).&nbsp;St\u00e9phane Hallegatte, Jun Rentschler, &amp; Julie Rozenberg, The World Bank,&nbsp;Lifelines: The Resilient Infrastructure&nbsp;Opportunity 2 (2019) &nbsp;(\u201cIn the median [climate and economic] scenario, the net benefit of investing in more resilient infrastructure in low- and middle-income countries is $4.2 trillion, with $4 in benefit for each $1 invested. Climate change makes action on resilience even more necessary and attractive: on average, it doubles the net benefits from resilience. And because large investments in infrastructure are currently being made in low- and middle-income countries, the median cost of one decade of inaction is $1 trillion.\u201d),&nbsp;https://documents1.worldbank.org/curated/en/775891600098079887/pdf/Lifelines-The-Resilient-Infrastructure-Opportunity.pdf. For an Intermediate-High NOAA climate scenario for Miami-Dade County in Florida, the return on investment for building-level adaptation actions is estimated to be 518%, and the return on investment for community-wide adaptation actions is estimated to be 926%. Urban Land Inst. , Research Report: The Business Case for Resilience in Southeast Florida: Regional Economic Benefits of Climate Adaptation 4, 26 (2020), https://knowledge.uli.org/reports/research-reports/2020/the-business-case-for-resilience-in-southeast-florida?_gl=1*17yztlq*_ga*MjA2MDgwNTAwOC4xNjQwODk1MjE1*_ga_HB94BQ21DS*MTY0MDg5NTIxNC4xLjAuMTY0MDg5NTIxNC4w.&nbsp; A study of Ho Chi Minh City adaptation to sea-level rise finds very large spread in the benefit-cost ratios and net-present values of adaptation measures between medium and high climate change scenarios, Paolo Scussolini et al., <i>Adaptation to Sea Level Rise: A Multidisciplinary Analysis for Ho Chi Minh City, Vietnam</i>, 53 Water Res. Rsch. 10841, 10852 (2017), (\u201cThe combination of elevation\u2009+\u2009dryproofing shows the highest B/C [benefit-cost ratio], ranging from 41 in [the most probable possible future greenhouse gas concentrations (RCP4.5)] to 97 in [extremely severe possible future greenhouse gas concentrations (RCP8.5)] High\u2010end, and the highest NPV, from 514 B$ in RCP8.5 High\u2010end to 216 B$ in RCP4.5.\u201d),&nbsp;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2017WR021344. <i>See also&nbsp;</i>Hallegatte et al., &nbsp;<i>supra </i>note 23, at 11 (\u201c[C]limate change makes the strengthening of infrastructure assets even more important. Without climate change, the median benefit\u2010cost ratio would be equal to 2, but it is doubled when climate change is considered. And the fraction of scenarios in which strengthening infrastructure is not profitable is decreased from 14 to 4 percent when climate change is taken into account.\u201d); Borja G. Reguero et al., <i>supra</i> note 23, at 15. (\u201cAs sea level rises, land subsides, storms increase in frequency and intensity, and assets in the coastal zone increase, all adaptation measures become more cost-effective . . . .\u201d); Jeremy Martinich &amp; Allison Crimmins, <i>Climate Damages and Adaptation Potential Across Diverse Sectors of the United States</i>, 9 Nature Climate Change 397, 401 (2019) (\u201cProjected physical and economic damages are larger under [extremely severe possible future greenhouse gas concentrations (RCP8.5)] than under [the most probable possible future greenhouse gas concentrations (RCP4.5)] across all 22 sectors and both time periods, with only 1 exception (urban drainage adaptation costs in 2050. . . ). Damages associated with extreme weather, such as extreme temperature, heavy precipitation, drought and storm surge events, are substantially reduced under RCP4.5. For example, more than twice as many 100-year riverine inland flooding events are projected across the CONUS under RCP8.5 compared to RCP4.5 by the end of the century . . . .\u201d). &nbsp;</p><p><a href=\"#_ftnref37\"><sup>[37]</sup></a>&nbsp;Ashley Schulten et al.,&nbsp;<i>Getting Physical: Scenario Analysis for Assessing Climate-Related Risks</i>,&nbsp;BlackRock Inv. Inst. 1, 12, (2019),&nbsp;https://www.blackrock.com/ch/individual/en/insights/physical-climate-risks.<s>&nbsp;</s></p><p><a href=\"#_ftnref38\"><sup>[38]</sup></a>&nbsp;Backers<i>,</i> and any financial intermediary facilitating the transaction, has an implicit incentive to help Adapters ensure their adaptation investments are properly implemented because that increases the probability that the Adapters will be able to pay the Backers back.</p><p><a href=\"#_ftnref39\"><sup>[39]</sup></a>&nbsp;The U.S. National Flood Insurance Program Community Rating System reduces premiums to reflect the reduced flood risk resulting from a community\u2019s efforts. <i>National Flood Insurance Program Community Rating System</i>, FEMA (last accessed Nov. 16, 2021),&nbsp;https://www.fema.gov/floodplain-management/community-rating-system?web=1&amp;wdLOR=cCB3563C1-9027-8541-BD4B-80EEA92A7C56.&nbsp;</p><p><a href=\"#_ftnref40\"><sup>[40]</sup></a> Additionally, while less of a direct monetary benefit than the list in the main text, climate change damages and the reconstruction they require can create greenhouse gas emissions. In this way, by reducing damages and the need to rebuild infrastructure, adaptation can reduce greenhouse gas emissions, which has monetary benefits, especially with a potential price placed on carbon emissions. More broadly, there can be emissions mitigation co-benefits to adaptation actions. <i>See generally</i>,&nbsp;Lobell et al., <i>Climate adaptation as mitigation: the case of agricultural investments</i>, Environmental Research Letters 8 (2013), https://iopscience.iop.org/article/10.1088/1748-9326/8/1/015012/meta.</p><p><a href=\"#_ftnref41\"><sup>[41]</sup></a>&nbsp;\u201c[L]itigation could be packaged as breaches of duty, ordinary negligence, and inverse condemnation actions based on a public entity\u2019s failure to adequately plan, prepare, and invest for the inevitable effects of climate change. Any resulting unplanned expenditures due to climate change could potentially cause an inability of the municipality to honor their debt obligations [...] Plaintiffs have alleged that defendants (utilities in some cases) are guilty of wrongful acts or negligence because the defendants gave climate change impacts insufficient consideration, planning, and investment. For example, this occurred in cases related to the 2018 California wildfires, where insurance companies, citing inverse condemnation, are looking to PG&amp;E to pay for wildfire losses.\u201d <i>Insurance, Bond Ratings and Climate Risk: A Primer for Water Utilities</i>, Ass\u2019n Met. Water Agencies 3, 6&nbsp;(2019),&nbsp;https://www.amwa.net/assets/Insurance-BondRatings-ClimateRisk-Paper.pdf. &nbsp;</p><p>And for electric utilities, see Romany M. Webb, Michael Panfil, &amp; Sarah Ladin, <i>Climate Risk in the Electricity Sector: Legal Obligations to Advance Climate Resilience Planning by Electric Utilities</i>, Columbia L. Sch. Sabin Ctr. Climate Change L. (Dec. 3, 2020),&nbsp;<a href=\"https://climate.law.columbia.edu/content/climate-risk-electricity-sector-legal-obligations-advance-climate-resilience-planning\">https://climate.law.columbia.edu/content/climate-risk-electricity-sector-legal-obligations-advance-climate-resilience-planning</a>. &nbsp;</p><p><a href=\"#_ftnref42\"><sup>[42]</sup></a> Debt service costs can account for 50% of a water utility\u2019s total costs. Chapter 2 of Jeff Hughes et al., <i>Defining a Resilient Business Model for Water Utilities</i>, Water Rsch. Found. (2014),&nbsp;<a href=\"https://www.researchgate.net/publication/277477105_Defining_a_Resilient_Business_Model_for_Water_Utilities\">https://www.researchgate.net/publication/277477105_Defining_a_Resilient_Business_Model_for_Water_Utilities</a>. Those costs are higher if the credit ratings of the utilities are lower.</p><p>Pittsburgh Water &amp; Sewer Authority said: \u201cWe are constantly evaluating ways to reduce our borrowing costs.\u201d<strong>&nbsp;</strong>Executive Director of Pittsburgh Water &amp; Sewer Authority: \u201cWe need to look ahead and determine whether or not we should be thinking of a much higher degree of prevention or protection and how much can we afford.\u201d J. Dale Shoemaker, <i>How Pittsburgh is Funding the Fight Against Climate Change,</i> PublicSource (Sept. 30, 2019), https://www.publicsource.org/how-pittsburgh-is-funding-the-fight-against-climate-change/.</p><p><a href=\"#_ftnref43\">[43]</a> Maintaining, or even increasing, the market value of the privately held property could make a material difference to the property tax collected.</p><p><a href=\"#_ftnref44\"><sup>[44]</sup></a> For examples of adaptation related activities that could also have emissions reductions benefits,&nbsp;<i>see generally&nbsp;</i>Joseph E. Fargione et al., <i>Natural Climate Solutions for the United States</i>, 4 Science Advances,&nbsp;1, 1(2018).,&nbsp;https://www.science.org/doi/10.1126/sciadv.aat1869.&nbsp;<s>&nbsp;</s>&nbsp;<s>&nbsp;</s></p><p><a href=\"#_ftnref45\"><sup>[45]</sup></a>&nbsp;If an Adapter is a state or city government, it's very unlikely, and for some, illegal, to declare bankruptcy and avoid payments. However, the money needs to come from somewhere. The Adapter will need to raise general real estate and sales taxes if they are a city or state, increase fees if they are a water or energy utility, or implement taxes that are closer to directly capturing the benefits provided by the adaptation project. <i>The Virtues of Value Capture</i>, Deloitte (2019),&nbsp;https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Public-Sector/smart-cities-virtues-of-value-capture-19nov.pdf, e.g., taxes on real estate nearest a seawall investment or beach nourishment; <i>see generally</i> Megan Mullin et al., <i>Paying to Save the Beach: Effects of Local Finance Decisions on Coastal Management</i>, Climatic Change,&nbsp;275, 275&nbsp;(2018).&nbsp;https://link.springer.com/article/10.1007/s10584-018-2191-5.</p><p>Arpit Gupta discusses how infrastructure projects could be funded by more specific property taxes: \u201cOur paper demonstrates that it is technically feasible to determine how much each housing unit benefited from the new transit infrastructure, taking into account its exact location, and its unit and building characteristics. In theory, local government could levy a unit-specific property tax surcharge proportional to the value created. Such micro-targeted property tax surcharges would not only be based on objectively measurable value increases and property characteristics, and hence be fair, they could also become an important financing tool to fund future infrastructure needs.\u201d Arpit Gupta &amp; Stijn Van Nieuwerburg, <i>Take the Q Train: Value Capture of Public Infrastructure Projects&nbsp;</i>1, 39 (Nat\u2019l Bureau of Econ. Rsch. Working Paper no. 26789, 2020),&nbsp;https://www.nber.org/papers/w26789.</p><p>\u201cThe value of a property protected by storm surge by a new seawall or natural barrier typically should be higher than the value of a similarly situated property that does not have such protection. Authorities can estimate the value difference between comparable properties with protection and those without it. Some of the difference can then be \u201ccaptured\u201d through increased taxes on the benefitting properties.\u201d&nbsp;Alice Hill &amp; Leonardo Martinez-Dias, Building a Resilient Tomorrow: How to Prepare for the Coming Climate Disruption 86 (Oxford Univ. Press 2020).&nbsp;</p><p>Furthermore, Backers can better make payments if Adapters also issues a catastrophe (cat) bond. We could issue a cat bond linked to the same climate threshold in order to be guaranteed to be able to pay some portion of the outcome to Backers if the threshold is reached, reducing their counterparty risk. We would be issuing a given cat bond that covers many climate contracts, and there would be many Backers parties to a single contract, so we would be able to spread cat bond transaction costs across enough Backer parties that this would be a more attractive option to a Backer than issuing a cat bond themselves. Cat bonds have characteristics that make them well suited for backstopping or complementing a climate contract: they are 100% collateralized and so have essentially no counterparty risk, they provide time scales (multi-year) that lock in rates, and because they have more investor types than most insurance related instruments, they can more buyer demand and thus have reduced rates.</p><p>It is also worth noting that if there are acute disasters occurring, the federal government will likely be providing financial disaster aid after acute climate events.</p><p><a href=\"#_ftnref46\"><sup>[46]</sup></a>&nbsp;For an alternative pricing approach,&nbsp;see Bloch et al. (2012), <i>supra</i> note 35,<i>&nbsp;</i>at 89 (applying the logic of pricing credit derivative products to pricing climate derivatives by replacing the survival probabilities and default time densities with first-passage (of a climate threshold) distributions and first-passage time density).</p><p><a href=\"#_ftnref47\"><sup>[47]</sup></a>&nbsp;Risks that can\u2019t be diversified away (systemic risks) are those that increase the probability that an asset\u2019s value is correlated with most other global asset values, see generally Antti Ilmanen, <i>Expected Returns: An Investor\u2019s Guide to Harvesting Market Rewards </i>(2011); correlated with equity market volatility, see generally Tim Lee et al., <i>The Rise of Carry: The Dangerous Consequences of Volatility Suppression and the New Financial Order of Decaying Growth and Recurring Crisis</i> (2019). This is likely the case for the more extreme global climate risks; therefore, the marginal utility of a dollar is higher for the payoffs for strategies that hedge these risks. Stefano Giglio et al., <i>Climate Change and Long-Run Discount Rates: Evidence from Real Estate</i> (Nat\u2019l. Bureau Econ. Rsch. Working Paper no. 21767, 2015),&nbsp;https://www.nber.org/papers/w21767 at page 6. The Financial Stability Board believes that climate risk \u201cmay change \u2013 and in places, increase \u2013 the degree of co-movement between asset prices, and reduce the degree to which financial market participants were able to diversify exposure to climate-related risks. It might also reduce the efficacy of other channels through which financial market participants seek to insure against climate risks (e.g., via some derivatives markets).\u201d The Implications of Climate Change for Financial Stability 17&nbsp;(Fin. Stability Bd.&nbsp;Nov. 23, 2020),&nbsp;https://www.fsb.org/2020/11/the-implications-of-climate-change-for-financial-stability/. &nbsp;</p><p><a href=\"#_ftnref48\">[48]</a> The risk-contingent financing mechanism should be applied to situations where the parties involved have negligible direct control over whether the risk occurs.&nbsp;If the investor in the risk-contingent instrument could influence the likelihood or the risk occurring, it could lead to misaligned incentives because they may later on decide that the higher return received from the payout of the contract outweighs the downsides of the risk occurring. With large-scale risks such as extreme climate change, natural pandemics, or asteroids hitting the earth, there are no entities with material direct control over whether the risk is realized.</p><p><a href=\"#_ftnref49\"><sup>[49]</sup></a>&nbsp;\u201cWhile current ILS instruments are useful for transferring risks, they are not designed to reduce underlying risks or build resilience to disasters.\u201d Carter, <i>supra </i>note 32.</p><p><a href=\"#_ftnref50\">[50]</a> See Figure 6b.</p><p><a href=\"#_ftnref51\"><sup>[51]</sup></a> Likely many Backer entities per Adapter.</p><p><a href=\"#_ftnref52\"><sup>[52]</sup></a> For instance, e.g., temperature, sea-level, precipitation or drought. Data from the U.S. government agencies, such as NOAA (e.g., https://data.noaa.gov/datasetsearch/), can be used as a trusted source for nearly any climate-related variable that would be useful to index to.</p><p>In selecting the climate variable, it is a balance between basis risk and broad applicability. Basis risk is the difference between the conditions under which a contract pays out and the conditions that one would like to hedge. For example, global sea-level rise levels may not translate perfectly into the benefits that a seawall provides compared to local relative sea-level rise, i.e., a contract tied to levels of global sea-level rise would have higher basis risk than a contract tied to local relative sea-level rise.&nbsp;</p><p>However, there is a trade-off with how widely applicable the climate variable is: the less potential entities that are impacted by the climate variable (e.g., local levels compared to global levels), the fewer opportunities there are to connect counterparties and facilitate hedging activity.</p><p><a href=\"#_ftnref53\">[53]</a> For example: three inches of sea-level rise in 2030.</p>", "user": {"username": "johnjnay"}}, {"_id": "WHNTBDeqvinLpdhJK", "title": "Giving opportunity: $50 'thank you' notes", "postedAt": "2022-09-26T01:49:46.479Z", "htmlBody": "<p>Edit 2: Checks out</p><p><img src=\"https://pbs.twimg.com/media/FeWwcBPWIAIQTAc?format=jpg&amp;name=large\"></p><p>Edit: changing the title from \"Do NOT keep reading if your time is worth &gt;$200/hr,\" which sounded a lot funnier in my head a couple hours ago.&nbsp;</p><p>&nbsp;</p><p>The <a href=\"https://crazygoodturns.org/\">Crazy Good Turns</a> podcast is making <a href=\"https://crazygoodturns.org/episodes/thanks\">the following solicitation:</a> &nbsp;</p><blockquote><h3>Send A Thank You</h3><p>Nominate someone you know who\u2019s done a good turn for others. We\u2019ll send them a \u201cThank You\u201d note, plus a $50 gift card, just for being an example of graciousness during extraordinarily difficult times. You can send this gift at no cost to yourself, or to the person receiving it. There are no strings attached.</p></blockquote><p>It's unclear, but from context I'm guessing \"gift card\" means \"prepaid debit,\" rather than something store-specific; not quite cash, but pretty close. Also - and this might be pretty important - there seems to be no <i>explicit</i> limit to the number of people you can \"nominate\" although, quite reasonably...</p><blockquote><p>We do reserve the right to request more info, or reject requests if necessary. And due to logistical reasons, we have to <strong>limit the gifts to within the United States</strong> (for now).</p></blockquote><p>Ok, enough context; I'm writing this because I'm hoping others will follow the instructions in such a way that a bunch of money winds up in the hands of cash-constrained charities (or maybe cash-constrained EAs).&nbsp;</p><p>Do what you will with this information, but here's something I encourage:&nbsp;</p><ol><li>Pick a value-aligned EA friend who lives in the US</li><li>Propose trading nominations in exchange for each person donating the money</li><li>Fill out the form <a href=\"https://crazygoodturns.org/send-a-thank-you\">here</a> correspondingly</li><li>[Hopefully] Receive prepaid debit</li><li>Donate $50 - directly with the gift card if possible, or else however you can/like - to wherever you think it will do the most good</li><li>&nbsp;Consider repeating steps 1-5 with a different person</li></ol><p>Why $200/hour in the title? I estimate this whole thing will take you about 15 minutes.</p><p>The man behind this podcast and offer, <a href=\"https://en.wikipedia.org/wiki/Frank_Blake\">Frank Blake</a>, has a net worth of somewhere between $3M and $20M, so there's likely a reasonably hard cap on things (and one much below those figures). This isn't another crypto windfall. But hey, $50 is $50, and $50 can do a lot of good.</p>", "user": {"username": "aaronb50"}}, {"_id": "pMZFeGiT6586aFcZS", "title": "Applying data science/ML to plant-based/cultured meat", "postedAt": "2022-09-26T08:01:19.214Z", "htmlBody": "<p>Hi fellow EAs,&nbsp;</p><p>I'm a machine learning researcher in Sydney. I'm interested in applying {data science/machine learning} techniques to {plant-based/cultured} meat. Are there any research papers that you'd suggest as an introduction to the field?<br>&nbsp;</p><p>Here are some links from GFI that are relevant. Note that I'm looking for specific papers/datasets, which are not listed on GFI's pages. &nbsp;<a href=\"https://gfi.org/researchgrants/grantee-page-cell-culture-media-machine-learning-for-fish-growth-media-virginia-tech/?fbclid=IwAR01LqQaITNxurd5S8So38S-PRgFtuQVk503O1KHL2hJ9hW7H7FQQ_Fef2g\">https://gfi.org/researchgrants/grantee-page-cell-culture-media-machine-learning-for-fish-growth-media-virginia-tech/?fbclid=IwAR01LqQaITNxurd5S8So38S-PRgFtuQVk503O1KHL2hJ9hW7H7FQQ_Fef2g</a></p><p><a href=\"https://gfi.org/researchgrants/grantee-page-cell-culture-media-optimizing-media-for-chicken-cells-university-of-california-davis/?fbclid=IwAR1dN_lwFcQXrDHqkxa0TykNvFScXIf75wV1PD_rywYrSqpnMW-hJ5KPTp0\">https://gfi.org/researchgrants/grantee-page-cell-culture-media-optimizing-media-for-chicken-cells-university-of-california-davis/?fbclid=IwAR1dN_lwFcQXrDHqkxa0TykNvFScXIf75wV1PD_rywYrSqpnMW-hJ5KPTp0</a>&nbsp;</p><p><br>Thanks,</p><p>-Vinoth</p>", "user": {"username": "puratchi_vinnani"}}, {"_id": "aeZJqNht9izeLCpkm", "title": "9/26 is Petrov Day", "postedAt": "2022-09-25T23:14:32.296Z", "htmlBody": "<p>\u200bToday<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffk8gsqpyk5k\"><sup><a href=\"#fnfk8gsqpyk5k\">[1]</a></sup></span>&nbsp;is September 26th, Petrov Day, celebrated to honor the deed of Stanislav Yevgrafovich Petrov on September 26th, 1983.&nbsp; Wherever you are, whatever you're doing, take a minute to not destroy the world.</p><p>The story begins on September 1st, 1983, when Soviet jet interceptors shot down a Korean Air Lines civilian airliner after the aircraft crossed into Soviet airspace and then, for reasons still unknown, failed to respond to radio hails.&nbsp; 269 passengers and crew died, including US Congressman Lawrence McDonald.&nbsp; Ronald Reagan called it \"barbarism\", \"inhuman brutality\", \"a crime against humanity that must never be forgotten\".&nbsp; Note that this was already a very, <i>very </i>poor time for US/USSR relations.&nbsp; Andropov, the ailing Soviet leader, was half-convinced the US was planning a first strike.&nbsp; The KGB sent a flash message to its operatives warning them to prepare for possible nuclear war.</p><p>On September 26th, 1983, Lieutenant Colonel Stanislav Yevgrafovich Petrov was the officer on duty when the warning system reported a US missile launch.&nbsp; Petrov kept calm, suspecting a computer error.</p><p>Then the system reported another US missile launch.</p><p>And another, and another, and another.</p><p>What had actually happened, investigators later determined, was sunlight on high-altitude clouds aligning with the satellite view on a US missile base.</p><p>In the command post there were beeping signals, flashing lights, and officers screaming at people to remain calm.&nbsp; According to several accounts I've read, there was a large flashing screen from the automated computer system saying simply \"START\" (presumably in Russian). Afterward, when investigators asked Petrov why he hadn't written everything down in the logbook, Petrov replied,\"Because I had a phone in one hand and the intercom in the other, and I don't have a third hand.\"</p><p>The policy of the Soviet Union called for launch on warning.&nbsp; The Soviet Union's land radar could not detect missiles over the horizon, and waiting for positive identification would limit the response time to minutes.&nbsp; Petrov's report would be relayed to his military superiors, who would decide whether to start a nuclear war.</p><p>Petrov decided that, all else being equal, he would prefer not to destroy the world.&nbsp; He sent messages declaring the launch detection a false alarm, based solely on his personal belief that the US did not seem likely to start an attack using only five missiles.</p><p>Petrov was first congratulated, then extensively interrogated, then reprimanded for failing to follow procedure.&nbsp; He resigned in poor health from the military several months later.&nbsp; According to Wikipedia, he is spending his retirement in relative poverty in the town of Fryazino, on a pension of $200/month.&nbsp; In 2004, the Association of World Citizens gave Petrov a trophy and $1000.&nbsp; There is also a movie scheduled for release in 2008, entitled <i>The Red Button and the Man Who Saved the World.</i></p><p>Maybe someday, the names of people who decide not to start nuclear wars will be as well known as the name of Britney Spears.&nbsp; Looking forward to such a time, when humankind has grown a little wiser, let us celebrate, in this moment, Petrov Day.</p><hr><p><i>You can read more about Petrov </i><a href=\"https://forum.effectivealtruism.org/topics/stanislav-petrov\"><i>here</i></a><i>.&nbsp;</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfk8gsqpyk5k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffk8gsqpyk5k\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>The original post was written by Eliezer Yudkowsky in 2007. I got permission to cross-post it in full (and am doing this a few hours early, depending on where you are).&nbsp;</i></p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "haoHTwwtFp9M2NewH", "title": "Switzerland fails to ban factory farming \u2013 lessons for the pursuit of EA-inspired policies?", "postedAt": "2022-09-25T19:25:04.169Z", "htmlBody": "<p>Today, the Swiss electorate could have voted for the abolition of factory farming (25 years from now&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzr4vutwd0fe\"><sup><a href=\"#fnzr4vutwd0fe\">[1]</a></sup></span>). But, in aggregate, we did not. By a depressingly large margin. The participation (as share of the electorate) was 52.3%, of which 37.1% voted in favour of the initiative. Securing, on top of the voter majority, the cantonal majority, which would have also been required, was even further out of reach: \"<a href=\"https://www.swissinfo.ch/eng/politics/animal-welfare-and-swiss-meat-eating-habits-up-for-public-vote/47921262\">Canton Basel City was the only of the 26 regions to approve the idea.</a>\" The initiative was launched by <a href=\"https://sentience.ch/en\">Sentience Politics</a>. More info about the initiative and the results in this swissinfo.ch article (in English): <a href=\"https://www.swissinfo.ch/eng/politics/animal-welfare-and-swiss-meat-eating-habits-up-for-public-vote/47921262\">Voters reject ethical overhaul of animal farming rules</a> &nbsp;<br><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><br>In my impression, the most influential argument of the camp against the initiative was that factory farming just doesn't exist in Switzerland.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwh1hdnbqoi\"><sup><a href=\"#fnwh1hdnbqoi\">[2]</a></sup></span>&nbsp;Even if it was only one of but not <i>the</i> most influential argument, I think this speaks volumes about both the (current) debate culture and the limits of how hopeful we should be that <i>relevantly similar</i> EA-inspired policies will soon see widespread implementation .<br><br>A key question is: What does <i>relevantly similar</i> mean here? A key argument, from a purely egoistic perspective, against abolishing factory farming is that, to some extent probably hard to precisely estimate in advance with much confidence, animal products will become more expensive. 1) Maybe many future EA-inspired policies would not have such costs, 2) arguably many future EA-inspired policies won't even (need to) be voted on by the electorate (in some cases it might even be sufficient to get a handful of key individual actors on board), and 3) probably there are other reasons why I have more reason to be hopeful about EA-inspired policies than I feel right now.<br><br>But still... Rarely in the history of Switzerland has there ever been an initiative that, from an EA perspective, has been of higher moral significance and simultaneously, from an EA perspective, of lower controversiality. The fact that the electorate of Switzerland (notably roughly the richest country in the world) failed to vote to abolish factory farming 1) is a testament to just how far we, as world optimisers, still have to go and, slightly more controversially 2) serves as a reminder that we can philosophise as much as we want: to the extent that bridging the gap from global priorities research and longtermist macrostrategy to the \"real world\" turns out to present even more of a challenge than we previously thought, we would better rethink our allocation of resources along what I'd roughly conceptualise as a <i>theoretical/fundamental/\"ivory tower\" \u2013 practical/applied/\"real world\"</i> dimension.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmvj1tjtallk\"><sup><a href=\"#fnmvj1tjtallk\">[3]</a></sup></span><br><br>1) Where do/might you (dis)agree with me? Am I missing an important consideration?<br>2) What's your reaction to the initiative and results?<br>3) What (tentative) lessons from this can we draw for the future pursuit of EA-inspired policies (within and beyond non-human animal welfare)?<br>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzr4vutwd0fe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzr4vutwd0fe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\"<a href=\"https://www.swissinfo.ch/eng/politics/animal-welfare-and-swiss-meat-eating-habits-up-for-public-vote/47921262\">Should the factory farming initiative have been accepted, farmers would have had up to 25 years to adapt to the rules.</a>\"</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwh1hdnbqoi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwh1hdnbqoi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>While I admit the term <i>factory farming</i> is open to interpretation and that the extent to which fundamental interests of farm animals are systematically violated is lower in Switzerland than in most other countries, I encourage people who are skeptical as to the existence of factory farming in Switzerland to search for <a href=\"https://factory-farming.ch/arguments/\">facts</a> and videos on this topic. It may be less horrible than in most countries, but it is still horrible to be your average farm animal in Switzerland.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmvj1tjtallk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmvj1tjtallk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Which I find ironic given that research on the value of global priorities research compared to other endeavours is a key question within, well, global priorities research.</p></div></li></ol>", "user": {"username": "Dario Citrini"}}, {"_id": "HrFBf2DuSo6KPBYG5", "title": "Smart Movements Start Academic Disciplines\n", "postedAt": "2022-09-25T15:58:19.366Z", "htmlBody": "<p><i>Cross-posted from </i><a href=\"https://coldbuttonissues.substack.com/p/smart-movements-start-academic-disciplines\"><i>Cold Button Issues</i></a><i>.</i></p><p>There\u2019s a lot of academic disciplines out there. And sometimes new ones emerge. I have a semi-defensive philosopher friend who likes to explain how many of today\u2019s independent academic disciplines are just offshoots of philosophy.</p><p>Sometimes a new discipline (<a href=\"https://www.chemistryworld.com/careers/are-chemical-engineering-and-biochemistry-their-own-disciplines/3010176.article\"><u>like</u></a><a href=\"https://www.mcgill.ca/biochemistry/about-us/information/biochemistry#:~:text=Biochemistry%20is%20the%20application%20of,the%20chemistry%20of%20living%20systems.\"><u> biochemistry</u></a>) emerges out of old disciplines due to increases in knowledge and specialization. Sometimes individual departments or whole disciplines rebrand to seem more current or generally applicable- such as the transformation of many&nbsp;<a href=\"http://www.istl.org/46-supp/article7.html\"><u>forestry schools</u></a> into schools of the environment and sustainability studies.&nbsp;</p><p>Other times, new disciplines are created due to political activism. And once created they can be a huge asset to the movements or ideologies that spawned them.</p><p><strong>The Left and Its Disciplines</strong></p><p><a href=\"https://www.npr.org/templates/story/story.php?storyId=124775888\"><u>Sixty years ago</u></a> there were no Women\u2019s Studies departments in the United States. There were no&nbsp;<a href=\"https://www.chronicle.com/article/the-beginnings-of-black-studies/#:~:text=The%20first%20black%2Dstudies%20department,demanding%20a%20black%2Dstudies%20program.\"><u>Black Studies department</u></a>s either. Now they\u2019re commonplace. Other ethnic studies programs have flourished as has&nbsp;<a href=\"https://en.wikipedia.org/wiki/Queer_studies\"><u>queer studies</u></a>. While none of these are common majors, they\u2019re entrenched in both red states and blue states.&nbsp;&nbsp;</p><p>Looking at the political skew of today\u2019s colleges maybe the spread of these disciplines doesn\u2019t seem that impressive but this was not inevitable. The political climate of US campuses when such disciplines began was not as friendly to leftwing identity politics as campuses typically are today. And there are still some universities and colleges that have refused to grant these newer disciplines their own department. Harvard does not have an&nbsp;<a href=\"https://www.wbur.org/news/2019/12/13/harvard-university-ethnic-studies-department-rally\"><u>ethnic studies department</u></a>.</p><p>How did this happen? A lot of activism. The Third World Liberation Front, at UC Berkeley, led a lengthy strike, occupied offices, and organized protests until the university acceded to their demands to<a href=\"https://www.crg.berkeley.edu/research/third-world-liberation-front/\"><u> establish ethnic studies at UC Berkeley</u></a>. The establishment of the discipline of women\u2019s studies was&nbsp;<a href=\"https://www.grinnell.edu/academics/majors-concentrations/gender/noun/history\"><u>driven by the women\u2019s liberation movement</u></a> and the establishment of the first few programs depended on extensive activism, organization, and consciousness-raising. These disciplines didn\u2019t just happen, they were fought for.</p><p>Once a discipline is closely affiliated or established by an ideology and that discipline is widely established across American academia, it nearly guarantees the representation of that ideology even at institutions that are hostile to it. Departments of that discipline become commonplace, even expected.</p><p>Take women\u2019s studies or gender studies a field that is \u201c<a href=\"https://gender.indiana.edu/activism-resources/index.html#:~:text=Academics%20%2B%20Advocacy&amp;text=Our%20field%20is%20inherently%20activist,class%2C%20race%2C%20and%20immigration.\"><u>inherently activist</u></a>.\u201d&nbsp; There are women\u2019s studies or gender studies at many Christian colleges that have a reputation for ideological and theological conservatism. Calvin University- Dutch Reformed- has a&nbsp;<a href=\"https://calvin.edu/academics/departments-programs/gender-studies/\"><u>gender studies program</u></a>. Wheaton College- evangelical- offers a&nbsp;<a href=\"https://www.wheaton.edu/academics/programs/anthropology/gender-studies/\"><u>certificate in gender studies</u></a>. Baylor University- Baptist-&nbsp;<a href=\"https://genderstudies.artsandsciences.baylor.edu/\"><u>has a program.</u></a> This means that even at relatively conservative schools there are professors who receive institutional funding to conduct&nbsp;<a href=\"https://genderstudies.artsandsciences.baylor.edu/\"><u>feminist&nbsp;</u></a><a href=\"https://calvin.edu/directory/people/kristin-kobes-du-mez\"><u>scholarship</u></a>. That seems like a big win for the left!&nbsp;</p><p>&nbsp;</p><p><strong>The Right Just Has Centers</strong></p><p>In American academia, there aren\u2019t really right-wing disciplines,&nbsp;<a href=\"https://www.tandfonline.com/doi/abs/10.1080/08913810508443639?journalCode=rcri20\"><u>just less left-wing ones</u></a>. At some colleges, conservative donors and activists have created academic centers designed to champion conservative beliefs, especially on economic issues. Sometimes they have names that don\u2019t indicate a political affiliation,&nbsp;<a href=\"https://salemcenter.org/\"><u>the Salem Center at UT-Austin</u></a>. Sometimes their name makes their perspective pretty clear like the&nbsp;<a href=\"https://www.colorado.edu/center/benson/\"><u>Bruce D. Benson Center for the Study of Western Civilization at the University of Colorado</u></a>.</p><p>I assume these centers do deliver some advantages for the right. Donors fund them- they must think they provide some benefit. And I can think of impressive academics and intellectuals who work at them- which implies they think those positions are of value. These centers do things like provide jobs for embattled conservative or libertarian academics, host panels and conferences, and sometimes offer courses to students.</p><p>But centers lack many of the advantages of full-fledged departments and disciplines. They offer relatively few, if any, tenure-track positions. They don\u2019t educate a large share of students. They can\u2019t gain entry to universities where the ideological opposition is intense. They don\u2019t set the agenda for research or control prestigious peer-reviewed journals that can be cited by Wikipedia and so on. They can be derailed&nbsp;<a href=\"https://www.jamesgmartin.center/2022/07/how-ut-austin-administrators-destroyed-an-intellectual-diversity-initiative/\"><u>by hostile administrators</u></a>.</p><p>I\u2019m not trying to be mean to American conservatives. In addition to centers, they have had genuine success in organizing across at least one key discipline: law. The<a href=\"https://en.wikipedia.org/wiki/Law_and_economics\"><u> law and economics subfield</u></a> encourages the application of economic analysis to legal decisions, was heavily influenced by neoclassical economics, and was funded by conservative donors and business interests. Conferences were organized to promote this type of analysis and judges&nbsp; who attended these conferences<a href=\"https://www.nber.org/system/files/working_papers/w29788/w29788.pdf\"><u> subsequently became more conservative in their rulings on many economics issues</u></a>. It\u2019s definitely a win for the right.</p><p>An academic subfield no matter how prestigious lacks much of the autonomy of full-fledged disciplines and departments. Perhaps if the right had started decades ago, they could have achieved similar success as the left. Or maybe not. But today it seems like an impossible uphill battle.</p><p>Is there any plausible route for conservatives to establish their own academic discipline which would support conservative ideology and provide jobs and influence to sympathetic academics? Probably not. There isn\u2019t a large student constituency willing to engage in activism including civil disobedience to pressure administrators to open such a department. There are quietly and in some cases openly conservative academics, but there aren\u2019t many of them and they are scattered across disciplines so there\u2019s not going to be legions of PhDs campaigning for this.&nbsp;</p><p>Finally, it\u2019s not even clear what discipline conservatives should try to start. I can imagine more conservative versions of existing disciplines- philosophers that laugh uproariously whenever somebody invokes Rawls or sociologists who just to love to talk about the negative effects of divorce or psychologists who spend all their time defending the validity of IQ- but I\u2019m not sure of what new discipline conservatives should try to foster.&nbsp;</p><p>The best I could come up with was home economics- now rebranded \u201c<a href=\"https://coldbuttonissues.substack.com/p/smart-movements-start-academic-disciplines\"><u>Family and Co</u>nsumer Sciences</a>.\u201d It seems to primarily&nbsp;<a href=\"https://www.niche.com/colleges/search/best-colleges-with-family-studies-and-consumer-sciences/\"><u>be</u></a>&nbsp;<a href=\"https://datausa.io/profile/cip/general-family-consumer-sciences\"><u>offered</u></a> at purple or red state public colleges, or at Brigham Young University. Maybe these departments could be a beachhead of academics arguing that natalism, piety, and traditional gender roles are good but it seems unlikely.&nbsp;</p><p>Conservatives are in a bad position for organizing within academia. While many state colleges are technically controlled by Republican state legislatures and governors, Republican politicians have tended to be relatively hands off in terms of college management. Any effort to start a discipline to help the political right would face immense opposition and a disproportionately small talent pool given the paucity of conservative academics.</p><p>Starting a discipline would be easier for a movement that ,in contrast, is overrepresented among academics and is not directly opposed to the progressivism that dominates American academia.</p><p>I know just the movement.</p><p><strong>Effective Altruists Love Welfare Economics</strong></p><p>The effective altruist has a strange relationship with academia. On one hand it was more obviously birthed out of specific academic philosophies than most other social movements. Many effective altruists can name an analytical philosopher. Many effective altruists have a&nbsp;<strong>favorite</strong> philosopher or can name a specific philosophical argument that changed their life goals.</p><p>Effective altruists are disproportionately located at elite universities and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/wtQ3XCL35uxjXpwjE/ea-survey-2019-series-community-demographics-and#Education\"><u>16% of the community has a doctorate</u></a>.</p><p>On the other hand, many effective altruists view academic norms as pathological, think important disciplines and norms are effectively broken, and are more impressed with a well-written forum post than an article in a high-impact journal. There\u2019s a general \u201cvibe\u201d that if effective altruists ran a school, it would be much better.</p><p>The movement also has at least one major funder who\u2019s&nbsp;<a href=\"https://ftxfuturefund.org/projects/a-new-university-or-institute/\"><u>expressed interest in funding a new university.</u></a> I think a better approach would be for effective altruist funders to fund a new academic discipline: welfare economics, the use of economic tools to evaluate aggregate well-being.</p><p><a href=\"https://en.wikipedia.org/wiki/Welfare_economics\"><u>Welfare economics</u></a> isn\u2019t new of course. Specific welfare economists, such as&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/yew-kwang-ng\"><u>Yew-Kwang Ng</u></a>, contributed to the development of effective altruism. But it\u2019s not its own discipline. I\u2019ve looked and haven\u2019t found a single independent academic department.</p><p>Why should effective altruists think it would be valuable for welfare economics to be its own discipline, with its own professional association, disciplinary norms, and independent academic departments? First, there is the shared intellectual orientation, the belief that the welfare of other beings matters- a lot. Encouraging research on better measuring welfare, making interpersonal welfare comparisons, and applied research on improving welfare seems like a pretty good idea. Second, many of the intellectual concerns of effective altruists are interdisciplinary- drawing on computer science, economics, politics, philosophy, and so forth. Interdisciplinary work can be risky for academics and graduate students who might think it's valuable but professionally costly to pursue research that doesn\u2019t help advance their academic career. Giving these topics a disciplinary home would make working on these topics more attractive.</p><p>The field of economics is big in American academia,&nbsp;<a href=\"https://datausa.io/profile/cip/economics\"><u>awarding almost 50,000 degrees in 2020</u></a>. It\u2019s big enough that in addition to there being many schools with economics departments, there\u2019s a fair number of schools with agricultural economics departments, including&nbsp;<a href=\"https://are.berkeley.edu/\"><u>UC Berkeley</u></a>,&nbsp;<a href=\"https://ag.purdue.edu/department/agecon/\"><u>Purdue</u></a>, and&nbsp;<a href=\"https://www.canr.msu.edu/afre/\"><u>Michigan State University</u></a>.</p><p>The existence of agriculture economics as its own field and department, at least at some institutions,&nbsp;<a href=\"http://jaysonlusk.com/blog/2020/1/2/looking-for-a-faculty-position-agricultural-economics-vs-economics\"><u>owes to separate federal funding streams for agricultural research</u></a>. Most subfields don\u2019t have the funding to justify their independent existence. But if a major funder wanted to kickstart independent welfare economics departments, they might be able to pull it off.</p><p>&nbsp;Another limiting factor for a would-be discipline is the existence of students who would want to take courses in it. Here, I think welfare economics would shine. Not only is economics a popular major, degree programs that combine economics with other disciplines like politics and philosophy&nbsp;<a href=\"https://www.oswego.edu/news/story/state-approves-new-major-philosophy-politics-and-economics\"><u>are increasingly being added by American universities</u></a>.</p><p>Free-standing welfare economics departments could focus on researching the most important topics, encourage their students to&nbsp;<a href=\"https://effectivethesis.org/\"><u>write their theses on the most important topics</u></a>, and provide a home for academics who want to dedicate their scholarship and their careers to the well-being of others.</p><p><br>&nbsp;</p>", "user": {"username": "ColdButtonIssues"}}, {"_id": "DBFZmsu7DJ8BSoZMp", "title": "Assessing Cost Effectiveness: malnutrition, famine, and cause prioritization", "postedAt": "2022-09-25T15:14:22.348Z", "htmlBody": "<p><strong>Tldr: </strong>Recent events have prompted me to reflect on whether tackling malnutrition and/or famine should be relatively more prioritised as a cause area. It also seems neglected as a discussion topic on the forum. I would like to prompt discussion on:</p><ul><li><strong>whether donating to tackle current famines is cost-effective</strong>, compared to a malaria medication</li></ul><p>and to suggest that...</p><ul><li><strong>tackling malnutrition could be a major cause area</strong></li></ul><p>&nbsp;</p><p><strong>Write-up:</strong></p><p><i>The recent events:</i> A major consequence of the Ukraine war has been disrupting grain supplies, which in East Africa and other areas (e.g. Yemen) has exacerbated <strong>an already precarious problem in food production and famine</strong> (following four years of drought).<i> (Bloomberg </i><a href=\"https://www.bloomberg.com/news/articles/2022-08-17/why-east-africa-s-facing-its-worst-famine-in-decades-quicktake?leadSource=uverify%20wall\"><i>17th Aug</i></a><i>; the UN </i><a href=\"https://news.un.org/en/story/2022/07/1123132\"><i>22nd July</i></a><i>; International Rescue Committee </i><a href=\"https://www.rescue.org/article/why-millions-people-across-africa-are-facing-extreme-hunger\"><i>11th May</i></a><i>.)</i></p><p>More broadly, this is a problem of malnutrition, already a growing trend due to <a href=\"https://forum.effectivealtruism.org/posts/ynRG6JBvARS2cHu63/review-of-climate-cost-effectiveness-analyses#Malnutrition\">global warming</a>, exacerbated by a temporary crisis.</p><p>Though those links show there has been media coverage, I was unaware of the famine, and only recently became aware after a colleague of mine recently visited Somaliland, where the famine is particularly hard-hitting, working with Save the Children.</p><p>Given that, <strong>I thought the issue may be neglected </strong>(by EA/more broadly). More broadly, I hear very little coverage of this. For EA, there are very few posts at all about 'East Africa'/'Somalia' on the forum. On <a href=\"https://forum.effectivealtruism.org/posts/qh3AkRgiwrefSMXrp/risk-of-famine-in-somalia\">June 11th</a> this year, one post introduced this risk , receiving 62 karma but just 1 comment. Eight days later on <a href=\"https://forum.effectivealtruism.org/posts/KRgCck6bwdFTyR3KF/what-is-the-best-ngo-for-hunger-relief-in-somalia-asking-for\">June 19th</a>, a separate post soliciting donation suggestions for this cause received 29 karma and 1 comment. On 'malnutrition', there is similarly little discussion, mainly prior to 2020. This short <a href=\"https://forum.effectivealtruism.org/posts/3YEnCxrbFzmFDNfx5/might-targeting-malnutrition-not-undernourishment-be-an\">post</a>, with 8 karma &amp; 5 comments, makes a similar suggestion to this, but I found little else.</p><p>Edit: on 'famine' there has been more discussion, e.g. <a href=\"https://forum.effectivealtruism.org/posts/BizWCgin6fCJCrkTJ/will-there-be-an-ea-answer-to-the-predictable-famines-later\">this short post</a> from May with 111 karma prompted 17 comments that partly compared the cost-effectiveness of tackling famines vs tackling malaria.</p><p>There are two causes here. One, the famine currently occurring in East Africa and similar regions. Two, malnutrition writ large. <i><strong>The uncertainty I have is around how cost-effective donating to these causes is (famine relief/malnutrition).</strong></i></p><h3><strong>Famine, context</strong></h3><p><strong>My understanding of the situation in East Africa</strong> is largely anecdotal, from my colleague who spent a week in Somaliland working with Save the Children to produce media material. <i>Epistemic status, 4/10</i><a href=\"https://www.lesswrong.com/posts/itKvzPmatBDvtgXTZ/how-confident-should-we-be\"><i>?</i></a></p><p>He reports roughly as follows.&nbsp;</p><ul><li>The people of this region travel around with cattle and the like, the produce of which they sell at markets.</li><li>Drought has meant it is hard to feed their animals, imposing a lower and biting cap on the number they can maintain, and thus the revenue they can receive at market (and what they can consume to live).</li><li>As a consequence, they have less to spend on grain at market, which has increased in price due to supply constraints.</li><li>The men traverse the land with the animals seeking water, hoping to return with resources to sell and leaving the women and children at home.</li><li>It is particularly the children who suffer, and charities (like Save the Children and the International Rescue Committee) help by providing malnutrition kits, such as special milk formula and high-nutrient peanut paste.</li><li><i>(There are other organisations targeting this cause. Please name any effective alternatives you are aware of.)</i></li></ul><h2><strong>Assessing cost-effectiveness in famine relief and malnutrition</strong></h2><h3>Mega charities</h3><p>The most appropriate benchmark feels like like medicine to prevent malaria, which GiveWell estimates at <a href=\"https://www.givewell.org/charities/top-charities\">$5,000 per life saved</a> based on \"exceptionally strong\" evidence (nets come in at $5.5k). But cost-effectiveness decreases as the highest-priority funding opportunities are funded. (Malaria has received much (more) attention from EA, and I am interested in donations on the margin.)</p><p>Save the Children is <u>not</u> recommended by GiveWell, but this is largely because <strong>'mega charities' (budgets of $250m+) are difficult to assess </strong>(though size may also obscure ineffectiveness)<strong>.</strong> Scale has long been an obstacle to effective funding decisions and allocation (see GW's <a href=\"https://blog.givewell.org/2011/12/28/mega-charities/\">2011 explanation</a>), and Save the Children is not exempt: GiveWell noted in <a href=\"https://blog.givewell.org/2018/11/26/our-updated-top-charities-for-giving-season-2018/#:~:text=list%20above%20(Oxfam%2C-,save%20the%20children,-%2C%20or%20UNHCR).%20Historically\">Dec 2018</a> that StC was included in the 'mega charity -&gt; hard to assess -&gt; uncertainty -&gt; less likely to recommend' bucket. SoGive also rates <a href=\"https://sogive.org/#charity?charityId=the-save-the-children-fund\">Save the Children</a> far down the list on cost-effectiveness, again due to absence of information (the <a href=\"https://sogive.org/#charity?charityId=against-malaria-foundation\">AMF</a> is rated ~5.5x as effective.)</p><p>As an aside, how GiveWell values uncertainty <a href=\"https://forum.effectivealtruism.org/posts/ycLhq4Bmep8ssr4wR/quantifying-uncertainty-in-givewell-s-givedirectly-cost\">has an impact</a> on cost-effective analysis, and the lack of ratings for mega charities has been <a href=\"https://forum.effectivealtruism.org/posts/g7uHEoaYikc6xizys/impactmatters-was-acquired-by-charitynavigator-but-it-doesn\">considered problematic</a>. For example, SoGive nonetheless rates Save the Children as 'promising', so we may be missing cost-effective opportunities<strong>.</strong> <strong>I would consider this post a success if it prompted discussion on how to mitigate uncertainty or improving how we value it.</strong></p><p>Indeed, uncertainty regarding mega charities is not a sufficient reason to not donate to them at all. In Open Philanthropy's overview of their <a href=\"https://www.openphilanthropy.org/research/2021-allocation-to-givewell-top-charities-why-were-giving-more-going-forward/\">main allocations for 2021</a>, malaria and malnutrition, respectively, were the top two cause areas, and of the $27m allocated to malnutrition, $20m went to the International Rescue Committee, a mega charity (budget: <a href=\"https://www.fastcompany.com/3065447/how-a-visionary-aid-organization-is-using-technology-to-help-refugees#:~:text=annual%20operating%20budget%20of%20almost%20%24700%20million\">$700m+</a>). An equal amount ($27m) was allocated to malaria, for comparison &nbsp;of how Open Phil weighed up those cause areas (100% to the Malaria Consortium, a top-rated GiveWell charity).</p><h3>Specific programmes: GiveDirectly</h3><p>The specific programme matters as much, if not more, than the charity or the method. The attention paid by GiveDirectly to this cause in a Yemeni context prompted me to update upwards on my beliefs about how cost-effective this cause could be.</p><p>GiveDirectly currently has a banner on its <a href=\"https://www.givedirectly.org/\">homepage</a> noting that food prices have doubled in the past year in Yemen. Not East Africa, but they are directing site visitors to their <a href=\"https://community.givedirectly.org/campaigns/333120a242e385ae\">campaign</a> for unconditional cash transfers. They have raised $4.7m out of a $6.5m funding goal.</p><p><i>(GiveDirectly was one of GiveWell's top-rated charities from 2012 to Aug 2022, when it was demoted after a criteria change, \"</i><a href=\"https://blog.givewell.org/2022/08/17/changes-to-top-charity-criteria/#:~:text=givedirectly%20doesn%E2%80%99t%20fulfill%20the%20fourth%20criterion%20on%20the%20list%20above%E2%80%94having%20funding%20gaps%20that%20meet%20our%20cost-effectiveness%20bar%20(currently%2010x%20cash)%E2%80%94even%20though%20we%20consider%20it%20an%20outstanding%20program.\"><i>even though we still consider it an outstanding program</i></a><i>.\" See GiveWell's summary </i><a href=\"https://www.givewell.org/charities/give-directly/November-2020-version\"><i>here</i></a><i>.)</i></p><h3>A growing cause area?</h3><p>In Open Phil's 2021 overview (published November), GiveWell suggests that \"malnutrition is a very promising area for charitable funding in the future.\"</p><p>They clarify further: \"To give a sense of what we expect, <strong>we would not be surprised if GiveWell directs as much funding to malnutrition in the future as we have to malaria programs</strong> in recent years.\"</p><p>In their complementary <a href=\"https://forum.effectivealtruism.org/posts/ZoTQMGwbQ5SS7Amhp/our-recommendations-for-giving-in-2021#:~:text=new%20interventions%20team-,identified%20a%20number%20of%20promising%20new%20program%20areas%20to%20support.,-As%20a%20result\">post</a> on this forum (14 karma, no comments), they note malnutrition as a \"promising new area,\" suggesting that it \"<i><strong>may </strong></i>have large, cost-effective funding gaps\" (emphasis added).</p><p>&nbsp;</p><p>Opinions very welcome:</p><ul><li><strong><u>is donating to tackle current famines is cost-effective</u></strong>, compared to a malaria medication <i>(which has received more attention from EA, though not necessarily the rest of the world)?</i></li><li><strong>how should we assess the cost-effectiveness of donating on malnutrition and famine relief</strong> <i>(particularly at mega charities like the International Rescue Committee)?</i></li></ul><p>&nbsp;</p><p><i>Links:</i></p><ul><li><a href=\"https://community.givedirectly.org/campaigns/333120a242e385ae\"><i>GiveDirectly</i></a><i> (Yemen)</i></li><li><a href=\"https://www.savethechildren.org.uk/how-you-can-help/emergencies/east-africa-hunger-crisis\"><i>Save the Children</i></a><i> (East Africa)</i></li><li><a href=\"https://help.rescue-uk.org/?_gl=1*9tpuir*_ga*MTAzNTY2NDg5Mi4xNjY0MTA2NTQ4*_ga_DDZCWB8N2Y*MTY2NDEyNDE1OC4zLjEuMTY2NDEyNDE3Ni4wLjAuMA..\"><i>International Rescue Committee</i></a><i> (generic)</i></li></ul>", "user": {"username": "learko"}}, {"_id": "HytJXbmxGtF7guqnJ", "title": "Review of Givewell's Cost-effectiveness for Seasonal Malaria Chemoprevention", "postedAt": "2022-09-25T13:59:35.827Z", "htmlBody": "<p>This post is intended to be an entry to the <strong>GiveWell Change Our Mind Contest.</strong></p><p>&nbsp;</p><p><strong>&nbsp; &nbsp; &nbsp; &nbsp;0.Summary&nbsp;</strong></p><p>Givewell\u2019s cost-effectiveness model for Seasonal Malaria Chemoprevention (SMC) is clearly laid out and generally very well evidenced. I have identified two areas where I believe an improved methodology could significantly change the results.&nbsp;</p><p>Firstly, the cost estimation used by Givewell underestimates the economies of scale in delivering SMC, and so underestimates the cost-effectiveness of the intervention in future.</p><p>Secondly, Givewell utilizes disease burden estimates for malaria from the Global Burden of Disease. An alternative WHO malaria data source however finds significantly different, and generally lower, estimates of malaria burden in three of the five country estimates used by Givewell. Givewell should investigate these differences more closely and potentially conduct primary data collection to seek to validate their use of country estimates. Alternatively, cost-effectiveness using the lower WHO estimates could also be presented to show a range of cost-effectiveness.</p><ol><li><strong>Cost estimation</strong></li></ol><p>To estimate costs for a given country, Givewell takes the approach of developing a cost per SMC cycle over the 6 years of available data (2015 \u2013 2020) inclusive. Whilst defensible, I feel that this approach does not fully utilize the available information and neglects the importance of economies of scale.</p><p>I believe the analysis should aim to project forwards what the cost-effectiveness of future money directed to SMC would be, rather than simply averaging the cost of previous programs.&nbsp;</p><p>Taking the year 2020 as an example, we can see that the cost of commodities, freight, and stock management came to 41% of the overall cost (pre-management fee). Other cost buckets in the analysis (monitoring &amp; evaluation, equipment, research staff, communications, planning) are unlikely to scale linearly as the program increases.</p><p>I would argue therefore that the majority of the program cost is unlikely to scale 1:1 with an expanded program. There will be efficiencies of scale which can be realized.&nbsp;</p><p>We can see this when we plot a cross-country comparison of scale of the project (represented by total person months of coverage) and cost per SMC cycle as shown below. Each dot represents the cost of an SMC cycle for a single year across all countries. As we can see, as the scale of the project increases, the cost per SMC cycle delivered falls.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/425204cf7031b1980b99ae64d80facea08d2e1e8efdbab50.png/w_942 942w\"></figure><p>I believe it is an important message, which is missed at present \u2013 that increased donations to the SMC project are likely to have increasing returns, not constant returns.&nbsp;</p><p>The current approach also means that if the cost-effectiveness of the program changes in future, any change will be under-weighted \u2013 because the average cost over the period will only change slightly, due to the weight of previous years.</p><p>This second graph shows largely the same point \u2013 that as the total program cost has increased for SMC, the cost per SMC cycle has fallen. If we accept the trendline shown below, spending over $35m could lead to an estimated cost per SMC cycle of ~$1.5, rather than the $1.66 used in Givewell\u2019s cost-effectiveness analysis.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/c4019ab5c2a9ba489803355c3bff39767e18cd0d52686cd4.png/w_942 942w\"></figure><p>There are several ways to implement a change to address this issue.</p><p>My proposed approach would be for Givewell to work with Malaria Consortium to understand which costs are relatively fixed for future years (e.g. staffing levels) and which are variable (e.g. purchase of commodities). This could then be used to create a theoretical cost per SMC cycle based on the estimated amount of donation that Givewell anticipates giving to Malaria Consortium.&nbsp;</p><p><strong>&nbsp; &nbsp;2. Estimation &nbsp;under-5 malaria deaths&nbsp;</strong></p><p>The figures used in the cost-effectiveness model come from the IHME Global Burden of Disease. The IHME breaks out these figures by age-related groups and by country which is helpful for the analysis Givewell has conducted.</p><p>However, it is notable that the IHME estimates of malaria impact tend to be higher than the World Health Organization\u2019s estimates<a href=\"#_ftn1\">[1]</a>.</p><p>If we assume that the age distribution of deaths in the IHME and WHO estimates are equal, then adopting the WHO estimates would substantially change Givewell\u2019s estimates for cost-effectiveness in individual countries.</p><p>Across the five countries analysed in the Givewell model \u2013 the IHME estimates of malaria deaths are higher in four countries, but substantially lower in Chad. The overall proportional differences between the two models across the five countries is relatively small, this is largely driven by the fact that the estimates are very similar for Nigeria, which is a very large country.</p><p>The WHO malaria report estimates the number of child deaths from malaria according to a verbal autopsy multicause model which estimates causes of death for children aged 1\u201359 months. The resulting cause-specific estimates were adjusted, country by country, to fit the estimated 1\u201359 month mortality envelopes (excluding HIV and measles deaths) for corresponding years. Estimated malaria parasite prevalence was used as a covariate within the model.<a href=\"#_ftn2\">[2]</a> The WHO also estimates a range of malaria deaths, and in three of the five countries the IHME estimates are outside of the WHO range.&nbsp;</p><p>Recommendation: Givewell should investigate the reasons behind the WHO &amp; IHME divergence, with particular focus on Chad, Burkina Faso and Togo where the divergence is large and outside the WHO\u2019s reported confidence intervals. Givewell might wish to consider using WHO data as an alternative for any sensitivity analysis they plan.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1.0pt solid windowtext;padding:0cm 5.4pt\"><strong>Country</strong></td><td style=\"border-color:windowtext;padding:0cm 5.4pt\"><p><strong>Malaria Deaths: IHME Estimate</strong></p><p><strong>(2019)</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt\"><p><strong>Malaria Deaths: WHO Estimate</strong></p><p><strong>(2019)</strong></p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt\"><p><strong>Difference&nbsp;</strong></p><p>(As proportion of IHME estimate)</p></td><td style=\"border-color:windowtext;padding:0cm 5.4pt;vertical-align:top\"><strong>IHME result outside of WHO range?</strong></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Burkina Faso</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;26,305&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;18,813&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>-28.5%</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Yes</strong></p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Chad</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7,194&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;11,619&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>+61.5%</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Yes</strong></p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Mozambique</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20,528&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;19,189&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>-6.5%</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Nigeria</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; 191,106&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; 187,437&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>-1.9%</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>No</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">Togo</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5,436&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3,584&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>-34.1%</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p><strong>Yes</strong></p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\">&nbsp;</td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p>&nbsp;</p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\"><strong>Total</strong></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p><strong>&nbsp; &nbsp; &nbsp;250,569&nbsp;</strong></p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p><strong>&nbsp; &nbsp; &nbsp; &nbsp;240,642&nbsp;</strong></p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt\"><p><strong>-4.0%</strong></p></td><td style=\"border-bottom:1.0pt solid windowtext;border-right:1.0pt solid windowtext;padding:0cm 5.4pt;vertical-align:top\"><p>&nbsp;</p></td></tr><tr><td style=\"border-color:windowtext;padding:0cm 5.4pt\" colspan=\"5\"><p>&nbsp;</p><p>Sources&nbsp;:&nbsp;WHO Confidence intervals:</p><p><a href=\"https://apps.who.int/gho/data/view.main.MALARIAESTDEATHSvc?lang=en\">https://apps.who.int/gho/data/view.main.MALARIAESTDEATHSvc?lang=en</a>&nbsp;</p><p>All other data in table:&nbsp;<a href=\"https://ourworldindata.org/malaria#data-quality-definitions\">https://ourworldindata.org/malaria#data-quality-definitions</a>&nbsp;</p></td></tr></tbody></table></figure><p>&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p><hr><p><a href=\"#_ftnref1\">[1]</a> <a href=\"https://ourworldindata.org/malaria#data-quality-definitions\">https://ourworldindata.org/malaria#data-quality-definitions</a></p><p><a href=\"#_ftnref2\">[2]</a> <a href=\"https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4650\">https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4650</a></p>", "user": null}, {"_id": "Y2BpfsLEddxetcXrg", "title": "Prioritizing the Arts in response to AI automation", "postedAt": "2022-09-25T07:49:21.705Z", "htmlBody": "", "user": {"username": "Casey"}}, {"_id": "zkjoBzaqTTW94XS2Z", "title": "What are your favorite Forum Shortforms?", "postedAt": "2022-09-24T23:46:05.895Z", "htmlBody": "<p>I really like the idea of the Forum <a href=\"https://forum.effectivealtruism.org/shortform\">Shortform</a> (a place to publish draft-y, rough ideas), but rarely read them myself, except for a brief period when they were first introduced.&nbsp;</p><p>What am I missing out?</p><p>I'd love to know what your favorite (most interesting, valuable, informative) shortforms are (they can be your own!)</p>", "user": {"username": "vaidehi_agarwalla"}}, {"_id": "kGrLQZzG3rLcXvhjg", "title": "EA forum content might be declining in quality. Here are some possible mechanisms.", "postedAt": "2022-09-24T22:24:42.199Z", "htmlBody": "<p>Note: This was originally posted as a <a href=\"https://forum.effectivealtruism.org/posts/7mTTzXutkgkzJuM3e/thomas-kwa-s-shortform?commentId=ALEFMbGpaWrYKCvBq\">shortform</a> with the first 8 points, and I added more based on the replies to that shortform.&nbsp;</p><ol><li>Newer EAs have worse takes on average, because the current processes of recruitment and outreach<a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building#A_speculative_model_of_things_going_wrong\"><u> produce a worse distribution than the old ones</u></a></li><li>Newer EAs are too junior to have good takes yet. It's just that the growth rate has increased so there's a higher proportion of them.</li><li>People who have better thoughts get hired at EA orgs [edit: or have other better things to do] and are too busy to post. There is anticorrelation between the amount of time someone has to post on EA Forum and the expected quality of their post.</li><li><a href=\"https://forum.effectivealtruism.org/posts/vv7FBtMxBJicM9pae/democratising-risk-a-community-misled#comments\"><u>Controversial content</u></a>, rather than good content, gets the most engagement.</li><li>Although we want more object-level discussion, everyone can weigh in on meta/community stuff, whereas they only know about their own cause areas. Therefore community content, especially shallow criticism, gets upvoted more. There could be a similar effect for posts by well-known EA figures.</li><li>Contests like the<a href=\"https://forum.effectivealtruism.org/topics/criticism-and-red-teaming-contest\">&nbsp;<u>criticism contest</u></a> decrease average quality, because the type of person who would enter a contest to win money on average has worse takes than the type of person who has genuine deep criticism. There were 232 posts for the criticism contest, and 158 for the<a href=\"https://forum.effectivealtruism.org/topics/cause-exploration-prizes\">&nbsp;<u>Cause Exploration Prizes</u></a>, which combined is more top-level posts than the&nbsp;<strong>entire forum in any month except August 2022.</strong></li><li>EA Forum is turning into a place primarily optimized for people to feel welcome and talk about EA, rather than impact.</li><li>All of this is exacerbated as the most careful and rational thinkers flee somewhere else, expecting that they won't get good quality engagement on EA Forum.</li><li>(<a href=\"https://forum.effectivealtruism.org/posts/7mTTzXutkgkzJuM3e/thomas-kwa-s-shortform?commentId=DMpHRLkDr6AFJpzpF\"><u>pointed out by Larks</u></a>) \"We also seem to get a fair number of posts that make basically the same point as an earlier article, but the author presumably either didn't read the earlier one or wanted to re-iterate it.\"</li><li>(<a href=\"https://forum.effectivealtruism.org/posts/7mTTzXutkgkzJuM3e/thomas-kwa-s-shortform?commentId=4k4FYXzJijuWEZWwR\"><u>pointed out by ThomasW</u></a>): \"There are many people who have very high bars for how good something should be to post on the forum. Thus the forum becomes dominated by a few people (often people who aren't aware of or care about forum norms) who have much lower bars to posting.\"</li><li>(<a href=\"https://forum.effectivealtruism.org/posts/7mTTzXutkgkzJuM3e/thomas-kwa-s-shortform?commentId=ahoKjr93HBjda3xRC\"><u>pointed out by John_Maxwell</u></a>) \"Forum leadership encouraging people to be less intimidated and write more off-the-cuff posts -- see e.g.<a href=\"https://forum.effectivealtruism.org/s/s5zDhfyRPvrpeuRf8/p/4WxHNBf5LeM9gQneT\">&nbsp;<u>this</u></a> or<a href=\"https://forum.effectivealtruism.org/posts/6whiBq7czKJk4Bx29/a-forum-post-can-be-short\">&nbsp;<u>this</u></a>.\"</li><li>(<a href=\"https://forum.effectivealtruism.org/posts/7mTTzXutkgkzJuM3e/thomas-kwa-s-shortform?commentId=ytiET82gX2rgdL2fk\">pointed out by HaydnBelfield</a>) \"There is a lot more posted on the forum, mostly from newer/more junior people. It could well be the case that the average quality of posts has gone down. However, I'm not so sure that the quality of the best posts has gone down, and I'm not so sure that there are fewer of the best posts every month. Nevertheless, spotting the signal from the noise has become harder. \"</li><li>(I thought of this since last week) The appearance of quality decline is an illusion; people judge quality relative to their own understanding, which tends to increase. Thus even though quality stays constant, any given person's perception of quality decreases.</li><li>(edited to add) Stagnation; EA Forum content is mostly drawn from the same distribution and many of the good thoughts have already been said. Contributing factors may be people not reading/building on previous posts (see also (9)), and lack of diversity in e.g. career specialties.</li></ol>", "user": {"username": "tkwa"}}, {"_id": "THRbsELHDubrf2Amh", "title": "Orexin and the quest for more waking hours", "postedAt": "2022-09-24T20:36:00.736Z", "htmlBody": "", "user": {"username": "ChristianKleineidam"}}, {"_id": "WQ7RKE2WpFWdm5pnh", "title": "Acceptance and Commitment Therapy (ACT) 101", "postedAt": "2022-09-24T19:07:06.623Z", "htmlBody": "<p>ACT = Acceptance and Commitment Therapy (pronounced as the word \u2018act\u2019 rather than the letters)</p><p>CBT = Cognitive Behavioural Therapy</p><h1>Summary</h1><ul><li>I have found <strong>Acceptance and Commitment Therapy</strong> (ACT) much more useful as a framework than other types of CBT.&nbsp;</li><li>Traditional CBT focuses on helping you identify and <strong>change</strong> negative thoughts, whereas ACT focuses on <strong>accepting</strong> them. It holds that fighting against your emotions usually just leads to being stuck in them.</li><li>Other types of CBT often ask&nbsp;<i>\u201cis this thought true?\u201d&nbsp;</i>to encourage behaviour change, whereas ACT uses values to encourage behaviour change (explained better in the full text). <strong>ACT could be a good fit for people who are especially values-driven.</strong></li><li>ACT has a strong focus on values and mindfulness. Other key ideas:<ul><li><strong>Psychological flexibility</strong>: being in contact with the present moment, fully aware of emotions, and thoughts, welcoming them, including the undesired ones, and moving in a pattern of behaviour in the service of chosen values;</li><li>Encouraging gentle <strong>curiosity</strong> towards thoughts, feelings, and sensations, <strong>instead of trying to change them</strong>;</li><li><strong>The normal thinking process of a healthy human mind will naturally lead to psychological suffering</strong>. We have not evolved to be happy, and we are not deficient for experiencing difficult thoughts and feelings;</li><li>The goal isn\u2019t to chase happiness or otherwise try and control our internal state.</li></ul></li><li>The six principles of ACT:<ul><li>Learning the&nbsp;skill of <strong>defusing&nbsp;</strong>your thoughts and feelings, so they have much less influence over you;</li><li>Making room for unpleasant feelings instead of trying to push them away (<strong>expansion</strong>);&nbsp;</li><li><strong>Connecting&nbsp;</strong>with whatever you\u2019re doing or experiencing in the present moment<strong>;</strong></li><li>Becoming familiar with the&nbsp;\"<strong>observing self,\" </strong>the part of you that experiences, sees, touches, and doesn't judge or take responsibility;</li><li>Clarifying and connecting with your&nbsp;<strong>values</strong>;</li><li>Taking&nbsp;<strong>action&nbsp;</strong>motivated by your values.</li></ul></li><li>This leads to a formula of:&nbsp;<strong>Mindfulness + Values + Committed Action = Psychological Flexibility</strong></li><li>If you\u2019d like to learn more, I recommend reading the book&nbsp;<a href=\"https://www.amazon.co.uk/Happiness-Trap-Based-revolutionary-mindfulness-based/dp/184529825X\"><u>The Happiness Trap</u></a>, or this&nbsp;<a href=\"https://lucrolin.com/the-happiness-trap\"><u>book summary</u></a> of it, or watching this&nbsp;<a href=\"https://www.youtube.com/watch?v=o79_gmO5ppg\"><u>Tedx talk</u></a>. If you want to engage more seriously, I'd really recommend seeking a therapist who practises ACT.&nbsp;</li></ul><h1>Introduction&nbsp;</h1><p>I started exploring ACT about a month ago, and have found it has significantly increased my mental wellbeing. ACT has been much more powerful and effective for me than other types of CBT I have tried in the past (however, I have engaged with ACT to a deeper level \u2013 but this is mostly because it felt exciting and helpful from the beginning).</p><p>I haven't heard EAs talk about it publicly before, and wanted to encourage others to explore it as a framework, if any of it resonates. I have been enthusing to friends about ACT, and I\u2019m also writing this to be able to refer people to in future.</p><p><strong>My goal here isn't to provide a guide for </strong><i><strong>How To Live According To ACT</strong></i><strong>.</strong> It's to offer a brief taster, so that if the ACT framework seems like it could be helpful, you can learn more elsewhere.&nbsp;</p><h1>Some benefits I\u2019ve experienced</h1><ul><li>Struggling against my internal experience less \u2013 and instead approaching it with compassion and acceptance;</li><li>More ability to be present with my experience in the moment, by developing the ability to notice when I\u2019m \u201chooked\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7zncmkwgwn5\"><sup><a href=\"#fn7zncmkwgwn5\">[1]</a></sup></span>&nbsp;on a string of thoughts, and skills to reconnect with the present moment;</li><li>Less self-criticism and more self-compassion (in a way that feels non-coercive and authentic);&nbsp;</li><li>More accepting of the fact that it\u2019s normal to have difficult thoughts and feelings \u2013 I\u2019m not bad or deficient for experiencing them;</li><li>More clarity on my values and how I want to treat myself;</li><li>Feeling more empowered and less hopeless around mental health.&nbsp;</li></ul><h1>How is ACT different from traditional CBT?</h1><p>Both are behaviour-based therapies, but they differ primarily in their orientation to thoughts. Whereas&nbsp;<strong>CBT works by helping you identify and change negative or destructive thoughts</strong>, ACT holds that <strong>pain and discomfort are a fact of life</strong> \u2013 something to get comfortable with if we aim to live a happy, fulfilled life. For this reason,&nbsp;<strong>ACT encourages you to accept all thoughts rather than trying to change them</strong> \u2013 both the good and the bad. Note that accepting thoughts is not the same as complacency.</p><p>Traditional CBT uses empiricism to guide behaviour change, whereas ACT uses values to guide behaviour change. For example, CBT might ask&nbsp;<i>\u201cIs [anxious thought] empirically true?\u201d&nbsp;</i>If the thought is false, CBT might encourage you realise that, and change the thought pattern accordingly. For me, this approach hasn\u2019t been helpful, as <strong>I can intellectually know that a thought is false, but still feel emotionally affected by and \u201chooked\u201d on it</strong>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7zncmkwgwn5\"><sup><a href=\"#fn7zncmkwgwn5\">[1]</a></sup></span>&nbsp;Alternatively, <strong>ACT helps you clarify what you actually care about</strong> \u2013 your values \u2013 and engage in \u201ctowards moves\u201d (moves that are towards your values, as opposed to \u201caway moves,\u201d which are moves away from your values). I find the framing of&nbsp;<i>\u201cI am doing this because it\u2019s aligned with my values \u201d</i> much more powerful as a force to drive behavioural change. ACT could be a good fit for people who are especially values-driven.&nbsp;</p><h1>Some key ideas behind ACT</h1><h2>Psychological flexibility</h2><p>Psychological flexibility is defined as being in contact with the present moment, fully aware of emotions, sensations, and thoughts, welcoming them, including the undesired ones, and moving in a pattern of behaviour in the service of chosen values.&nbsp;</p><p>More simply, this means accepting our own thoughts and emotions and acting on long-term values rather than short-term impulses, thoughts, and feelings that are often linked to experiential avoidance and a way to control unwanted inner events.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnbdd2d293mb\"><sup><a href=\"#fnnbdd2d293mb\">[2]</a></sup></span></p><h2>Acceptance of our internal experience</h2><p>The objective of ACT is not elimination of difficult feelings; rather, it is to be present with what life brings and live in accordance with your values.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefs72kx44fn\"><sup><a href=\"#fns72kx44fn\">[3]</a></sup></span>&nbsp;Fighting against your emotions usually just leads to being stuck in them. Instead, ACT encourages gentle curiosity towards thoughts, feelings, and sensations.</p><h2>Happiness is not the natural state for human beings</h2><blockquote><p>\u201cAcceptance and Commitment Therapy is based on a dramatically different assumption: the normal thinking process of a healthy human mind will naturally lead to psychological suffering. You\u2019re not defective; your mind\u2019s just doing what it evolved to do\u201d.</p><p>\u2014 Dr. Russ Harris</p></blockquote><p>We\u2019ve not evolved to be happy, or to handle difficult thoughts and feelings effectively. We evolved for an ancestral environment, in which anxiety, sadness, and anger were evolutionarily useful. It takes deliberate practice to live a life where you\u2019re not perpetually struggling against your internal experience. Discomfort is inevitable in a full human life.</p><h2>The focus on values</h2><p>ACT encourages you to clarify your values, and then taking actions that support them. It means asking yourself: what is important to me? How do I want to interact with the world? How do I want to interact with others? How do I want to interact with myself?&nbsp;</p><h2>Pleasant feelings aren\u2019t the goal</h2><p>Sometimes ACT techniques can make you feel happier, but that\u2019s a pleasant side effect and not the goal. ACT is not an attempt to closely control your emotions \u2013 the goal isn\u2019t to chase happiness.</p><p>In <a href=\"https://www.amazon.co.uk/Happiness-Trap-Based-revolutionary-mindfulness-based/dp/184529825X\">The Happiness Trap</a>, Harris gives a non-standard definition of happiness: \u201cliving a rich, full, and meaningful life.\u201d This is different from the standard definition of \u201cfeeling good,\u201d because this is a feeling, and feelings don\u2019t last.</p><h1>The six principles of ACT</h1><p>The first 4 principles of ACT are mindfulness skills. The last 2 principles are values, and committed action. All this leads to a formula<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmsf7ik42zlr\"><sup><a href=\"#fnmsf7ik42zlr\">[4]</a></sup></span>&nbsp;that looks like this: <strong>Mindfulness + Values + Committed Action = Psychological Flexibility.</strong></p><h2>Defusion&nbsp;</h2><p>This skill is about learning to perceive thoughts, images, memories and other cognitions for what they are \u2013 nothing more than bits of language and images \u2013 rather than what they often appear to be: threatening events, rules that must be obeyed, or objective truths and facts.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefoj10jpposyb\"><sup><a href=\"#fnoj10jpposyb\">[5]</a></sup></span></p><p>As you learn to defuse painful and unpleasant thoughts, they often lose their ability to frighten, disturb, worry, stress, or depress you.</p><h2>Expansion</h2><p>Making room for unpleasant feelings and sensations instead of trying to suppress them or push them away. As you open up and make space for these feelings, they will likely bother you less, and \u201cmove on\u201d much more rapidly, instead of \u201changing around\u201d and disturbing you.</p><h2>Connection</h2><p>Connecting fully with whatever is happening right here, right now; focusing on and engaging in whatever you\u2019re doing or experiencing. Instead of dwelling on the past or worrying about the future, you are deeply connected with the present moment.</p><h2>The Observing Self</h2><p>The observing self is the part of you that does not change but experiences, sees, touches, thinks. The observing self does not judge, it does not take any responsibility; it helps you to become aware of what you have done; whereas the thinking self is the part of you that judges, the observing self does not generate any thoughts but simply observes them.</p><p>ACT believes that the observing self is like the sky. The thoughts and feelings are like the weather constantly changing and moving but the sky remains blue, regardless of the weather.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref391txjz9ego\"><sup><a href=\"#fn391txjz9ego\">[6]</a></sup></span></p><h2>Values</h2><p>ACT teaches that clarifying and connecting with your values is an essential step for making life meaningful. Your values are reflections of what is most important to you: what sort of person you want to be, what is significant and meaningful to you, and what you want to stand for in this life. Your values provide direction for your life and motivate you to make important changes.</p><h2>Committed Action</h2><p>A rich and meaningful life is created through taking effective action, guided by and motivated by your values. And in particular, it happens through committed action: action that you persevere with, even through difficulty.</p><h1>Useful resources</h1><ul><li>Read&nbsp;<a href=\"https://www.amazon.co.uk/Happiness-Trap-Based-revolutionary-mindfulness-based/dp/184529825X\"><u>The Happiness Trap</u></a> or listen to the&nbsp;<a href=\"https://www.audible.co.uk/pd/The-Happiness-Trap-Audiobook/B09PMVBLQ1\"><u>audiobook</u></a> (12 hours).<ul><li>A 20 min&nbsp;<a href=\"https://lucrolin.com/the-happiness-trap\"><u>book summary</u></a> (skim this if you only have a few minutes).</li></ul></li><li>A 19 min&nbsp;<a href=\"https://www.youtube.com/watch?v=o79_gmO5ppg\"><u>Tedx talk</u></a> by Steven Hayes (the creator of ACT) about psychological flexibility. Gives a narrative of his experience of panic attacks, his journey of creating ACT, and some of the core ideas of ACT. He\u2019s a captivating public speaker and I found this moving to watch.</li><li>The Happiness Trap <a href=\"https://thehappinesstrap.com/\">8 week online programme</a> (I've not tried this so can't comment on how good it is).&nbsp;</li><li>Some&nbsp;<a href=\"https://thehappinesstrap.com/free-resources/\"><u>free resources</u></a> created by Dr. Russ Harris. I\u2019m not sure how useful these will be by themselves, and think it would be better to use them alongside reading one of his books.&nbsp;</li><li>The book&nbsp;<a href=\"https://www.amazon.co.uk/Liberated-Mind-Pivot-Toward-Matters/dp/1785041185/ref=pd_lpo_1?pd_rd_i=1785041185&amp;psc=1\"><u>A Liberated Mind</u></a> is targeted at professionals, and is more advanced than The Happiness Trap (its more empirical and scientific).</li></ul><p><strong>My biggest recommendation is probably to work with a therapist who practices ACT.</strong> This is not always possible, but here\u2019s a friendly nudge to book an intro session with a therapist if you think it could help and you\u2019ve been putting it off :)</p><p>There is one therapist on the&nbsp;<a href=\"https://www.eamentalhealthnavigator.com/therapists\"><u>EA Mental Health Navigator</u></a> who is listed as practising ACT. Here is a <a href=\"https://www.psychologytoday.com/gb/counselling/england?category=acceptance-and-commitment-therapy-act\">list of ACT therapists in England</a> that I found from googling (you can do the same for other countries). If you're aware of any good ACT therapists, it would be helpful if you would share them with me or link to them in the comments.</p><p>I believe that there is sometimes funding available from the <a href=\"https://funds.effectivealtruism.org/funds/ea-community\">EA Infrastructure Fund</a>, if funding is a bottleneck to you seeking therapy and you work in an impactful career path. I\u2019m entirely sure about what\u2019s available, and if anyone knows please let me know and I'll be more specific here!</p><h3>Other mental health resources</h3><ul><li>Slate Star Codex posts <a href=\"http://slatestarcodex.com/2015/07/13/things-that-sometimes-work-if-you-have-anxiety/\"><u>Things that sometimes work if you have anxiety</u></a> and <a href=\"https://slatestarcodex.com/2014/06/16/things-that-sometimes-help-if-youre-depressed/\"><u>Things that sometimes help if you have depression</u></a>. These are summaries of the lifestyle, professional, pharmaceutical, and last resort interventions you can try.&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/by38PwJNpqNWfc43G/resources-on-mental-health-and-finding-a-therapist\">Resources On Mental Health And Finding A Therapist</a></li></ul><h1>Caveats</h1><ol><li>I am not a therapist, and I am relatively new to these ideas, so take everything I say with a pinch of salt.&nbsp;</li><li>I wrote this over the course of an afternoon, and by the end I was quite sleepy; I'm sure I will have made mistakes, and feel free to let me know if that's the case.&nbsp;</li><li>Other types of CBT probably work brilliantly for some people, and it\u2019s probably worth trying both. I wrote this because I\u2019ve not seen any EAs publicly talk about ACT, and I think it could be a better approach for some people (it definitely was for me).</li><li><strong>When I\u2019ve made statements throughout this post, I have not been making truth-claims, but instead present them as useful framings</strong>. For example, I\u2019m not asserting that the claim&nbsp;<i>\u201cdiscomfort is inevitable in a full human life,\u201d</i> is a ground truth, but that it is a useful frame that often reflects reality.</li></ol><p><i>I used definitions and ideas explained in </i><a href=\"https://lucrolin.com/the-happiness-trap\"><i>Luc Rolin\u2019s summary</i></a><i> of The Happiness Trap throughout this post, with explicit permission from Luc.</i></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7zncmkwgwn5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7zncmkwgwn5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Being \u201chooked\u201d is ACT terminology. When we\u2019re \u201chooked\u201d our thoughts seem real, true, important and wise, and are experienced as orders or as threats. Therefore, if we are \u201chooked\u201d we react to thoughts such as \u201cI am useless\u201d as if we actually are useless, and we react to thoughts like \u201cI am going to muck this up\u201d as if failure is a foregone conclusion. Often our thoughts \u201cbeat us up\u201d, telling us stories about how stupid, incompetent, and ugly we are. Getting hooked by these kinds of thoughts can lead to anxiety, low self-esteem, depression and so on.</p><p>Definition taken loosely from&nbsp;<a href=\"https://www.bridgethoggpsychology.com.au/gettingunhookedfromupsettingthought#:~:text=%E2%80%8B-,%E2%80%8B,or%20blended%20with%20our%20thoughts.\"><u>here</u></a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnbdd2d293mb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnbdd2d293mb\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://www.frontiersin.org/articles/10.3389/fpsyg.2019.01302/full#B33\">H\u00fclsheger et al., 2013</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fns72kx44fn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefs72kx44fn\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Taken from the book \"Acceptance and Commitment Therapy: The Process and Practice of Mindful Change\" (2nd ed.). Amazon link <a href=\"https://www.amazon.co.uk/Acceptance-Commitment-Therapy-Second-Practice/dp/1462528945\">here</a>.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmsf7ik42zlr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmsf7ik42zlr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Formula taken from <a href=\"https://lucrolin.com/the-happiness-trap\">here</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnoj10jpposyb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefoj10jpposyb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Parts of the definition taken from <a href=\"https://www.aipc.net.au/articles/six-principles-of-acceptance-and-commitment-therapy/\">here</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn391txjz9ego\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref391txjz9ego\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Definition taken from <a href=\"https://www.counselling-directory.org.uk/memberarticles/what-is-the-observing-self#:~:text=ACT%20or%20acceptance%20and%20commitment,hurricane%20can%20harm%20the%20sky.\">here</a>.&nbsp;</p></div></li></ol>", "user": {"username": "Evie Cottrell"}}, {"_id": "RkpdA8763yGtEovj9", "title": "Two reasons we might be closer to solving alignment than it seems", "postedAt": "2022-09-24T17:38:24.188Z", "htmlBody": "<p>I was at an AI safety retreat recently and there seemed to be two categories of researchers:</p><ol><li>Those who thought most AI safety research was useless</li><li>Those who thought&nbsp;<i>all</i> AI safety research was useless</li></ol><p>This is a darkly humurous anecdote illustrating a larger pattern of intense pessimism I\u2019ve noticed among a certain contingency of AI safety researchers.</p><p>I don\u2019t disagree with the more moderate version of this position. If things continue as they are, anywhere up to a 95% chance of doom seems defendable.&nbsp;</p><p>What I disagree with is the degree of confidence. While we certainly shouldn\u2019t be confident that everything will turn out fine, we also shouldn\u2019t feel confident that it won\u2019t. This post might have easily been titled the same as Rob Bensinger\u2019s similar post:&nbsp;<a href=\"https://www.lesswrong.com/posts/vT4tsttHgYJBoKi4n/some-abstract-non-technical-reasons-to-be-non-maximally\"><u>we shouldn\u2019t be maximally pessimistic about AI alignment.</u></a>&nbsp;</p><p>The main two reasons for not being overly confident of doom are:</p><ol><li>All of the arguments saying that it\u2019s hard to be confident that transformative AI (TAI) isn\u2019t just around the corner also apply to safety research progress.&nbsp;</li><li>It\u2019s still early days and we\u2019ve had about as much progress as you\u2019d predict given that up until recently we\u2019ve only had double-digit numbers of people working on the problem.&nbsp;</li></ol><h2>The arguments that apply to TAI potentially being closer than we think also apply to alignment</h2><p>It\u2019s really hard to predict research progress. In&nbsp;<a href=\"https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence\"><u>\u2018There\u2019s no fire alarm for artificial general intelligence\u2019,</u></a> Eliezer Yudkowsky points out that historically, \u2018it is very often the case that key technological developments still seem decades away, five years before they show up\u2019 - even to scientists who are working directly on the problem.&nbsp;</p><p>Wilbur Wright thought that heavier-than-air flight was fifty years away; two years later, he helped build the first heavier-than-air flyer. This is because&nbsp;<strong>it often&nbsp;</strong><i><strong>feels the same</strong></i><strong> when the technology is decades away and when the technology is a year away:</strong> in either case, you don\u2019t yet know how to solve the problem.&nbsp;</p><p>These arguments apply not only to TAI, but also to TAI alignment. Heavier-than-air flight&nbsp;<i>felt</i> like it was years away when it was actually round the corner. Similarly,&nbsp;<strong>researchers\u2019&nbsp;</strong><i><strong>sense</strong></i><strong> that alignment is decades away - or even that it is impossible - is consistent with the possibility that we\u2019ll solve alignment next year.&nbsp;</strong><br><br>AI safety researchers are more likely to be pessimistic about alignment than the general public because they are deeply embroiled in the weeds of the problem. They are viscerally aware, from firsthand experience, of the difficulty. They are the ones who have to feel the day-to-day confusion, frustration, and despair of bashing their heads against a problem and making inconsistent progress. But this is how it always feels to be on the cutting edge of research. If it felt easy and smooth, it wouldn\u2019t be the edge of our knowledge.&nbsp;</p><p>AI progress thus far has been highly discontinuous; there have been times of fast advancement interspersed with \u2018AI winters\u2019 where enthusiasm waned, and then&nbsp;<a href=\"https://openai.com/blog/dall-e/\"><u>several</u></a>&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/4m69jEBWxrqnjyuZp/deepmind-s-generalist-ai-gato-a-non-technical-explainer\"><u>important</u></a>&nbsp;<a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\"><u>advances&nbsp; </u></a>in the last few months. This could also be true for AI safety - even if we\u2019re in a slump now, massive progress could be around the corner.&nbsp;</p><p>&nbsp;</p><h2>It\u2019s not surprising to see this little progress when we have so few people working on it</h2><p>I understand why some people are in despair about the problem. Some have been working on alignment for decades and have still not figured it out. I can empathize. I\u2019ve dedicated my life to trying to do good for the last twelve years and I\u2019m still deeply uncertain whether I\u2019ve even been net&nbsp;<i>positive</i>. It\u2019s hard to stay optimistic and motivated in that scenario.&nbsp;</p><p>But let\u2019s take a step back: this is an&nbsp;<i>extremely complex</i> question, and&nbsp;we haven\u2019t attacked the problem with all our strength yet. Some of the earliest pioneers of the field are no doubt some of the most brilliant humans out there. Yet, they are still only a small number of people. There are currently only about one hundred and fifty people working full-time on technical AI safety, and even that is recent - ten years ago, it was more like five. <strong>We probably need more like&nbsp;</strong><i><strong>tens of thousands</strong></i><strong> of people researching this for&nbsp;</strong><i><strong>several decades</strong></i><strong>.&nbsp;</strong></p><p>I\u2019m reminded of the great bit in Harry Potter and the Methods of Rationality where Harry explains to Fred and George how to think about something. For context, Harry just asked the twins to creatively solve a problem for him:&nbsp;</p><blockquote><p>\u2018Fred and George exchanged worried glances.</p><p>\"I can't think of anything,\" said George.</p><p>\"Neither can I,\" said Fred. \"Sorry.\"</p><p>Harry stared at them.</p><p>And then Harry began to explain how you went about thinking of things.</p><p>It had been known to take longer than two seconds, said Harry.</p><p>You&nbsp;<i>never</i> called&nbsp;<i>any</i> question impossible, said Harry, until you had taken an actual clock and thought about it for five minutes, by the motion of the minute hand. Not five minutes metaphorically, five minutes by a physical clock\u2026.</p><p>So Harry was going to leave this problem to Fred and George, and they would discuss all the aspects of it and brainstorm anything they thought might be remotely relevant. And they shouldn't try to come up with an actual solution until they'd finished doing that, unless of course they&nbsp;<i>did</i> happen to randomly think of something awesome, in which case they could write it down for afterward and then go back to thinking. And he didn't want to hear back from them about any so-called&nbsp;<i>failures to think of anything</i> for at least a week. Some people spent&nbsp;<i>decades</i> trying to think of things.\u2019</p></blockquote><p>We\u2019ve definitely set a timer and thought about this for five minutes. But this is the sort of problem that won\u2019t just be solved by a small number of geniuses. We need&nbsp;<i>way</i> more \u201cquality-adjusted researcher-years\u201d if we\u2019re going to get through this.&nbsp;</p><p>This is one of if not&nbsp;<i>the</i> most difficult intellectual challenge of our time. Even&nbsp;<i>understanding</i> the problem is difficult, and to solve it we will probably require a mix of math, philosophy, programming, and a healthy dose of political acumen.&nbsp;</p><p>Think about how many scientists it took before we made progress on practically any important scientific discovery. Except for the lucky ones at the beginning of the Enlightenment period where there were few scientists and lots of low-hanging fruit, there are usually thousands to tens of thousands scientists banging their heads against walls for decades for every one who makes a significant breakthrough. And we\u2019ve got around one hundred in a field barely over a decade old!&nbsp;&nbsp;</p><p>When you look at it this way, it\u2019s no wonder we haven\u2019t made a lot of progress yet. In fact, it would be quite surprising if we had. We are a small field that\u2019s just getting started.&nbsp;</p><p>We\u2019re currently Fred and George, feeling discouraged after having pondered the world\u2019s most important and challenging question for a few metaphorical seconds. Let\u2019s be inspired by Harry to not only think about it for five minutes, but for decades, with a massive community of other people trying to do the same. Let\u2019s field-build and get thousands of people banging their head against this wicked problem.</p><p>Who knows - one of the new researchers might be just a year away from making the crucial insight that ushers in the AI alignment summer.</p><p><i><strong>Reminder that you can&nbsp;</strong></i><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\"><i><strong><u>listen to EA Forum/LessWrong posts on your podcast player</u></strong></i></a><i><strong> using&nbsp;</strong></i><a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\"><i><strong><u>The Nonlinear Library</u></strong></i></a><i><strong>.</strong></i></p><p><i>This post was written collaboratively by Kat Woods and Amber Dawn Ace as part of Nonlinear\u2019s experimental Writing Internship program. The ideas are Kat\u2019s; Kat explained them to Amber, and Amber wrote them up. We would like to offer this service to other EAs who want to share their as-yet unwritten ideas or expertise.</i></p><p><i><strong>If you would be interested in working with Amber to write up your ideas, fill out&nbsp;</strong></i><a href=\"https://forms.gle/g1dXj5CDtEt74fWx9\"><i><strong><u>this form</u></strong></i></a><i>.&nbsp;</i><br>&nbsp;</p>", "user": {"username": "katherinesavoie"}}, {"_id": "8QfQcFyj6aGNM78kz", "title": "Learning from Matching Co-Founders for an AI-Alignment Startup", "postedAt": "2022-09-24T13:11:43.430Z", "htmlBody": "<p><i>At EAG London in April 2022, Beth Barnes was looking for potential founders of a new startup that could provide better human data for AI Alignment research. Six of the people interested organised a self-run multi-step co-founder matching process over 7 weeks. It resulted in identifying 2 potential co-founders that have been exploring the founding opportunity in detail.</i></p><p><i>In this post, we, as part of the group, will describe the process of the co-founder matching and the learnings based on a survey of participants.</i></p><h1>The initial idea</h1><p>The Future Fund\u2019s Project Ideas Competition this March&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MBDHjwDvhDnqisyW2/awards-for-the-future-fund-s-project-ideas-competition#Marc_Everin_Carauleanu___Datasets_for_AI_alignment_research\"><u>awarded prizes to Marc-Everin Carauleanu and Beth Barnes</u></a> for suggestions regarding human data for AI Alignment. Beth met with several people at EAG London in April and provided a five-page Google Doc writeup after the conference to everyone who had expressed interest and had spoken with her. The email doubled as an introduction.</p><h1>Coordinating the process</h1><p>As Beth implicitly had handed over the idea to the group to self-coordinate, I (Patrick), as one of the participants, set up&nbsp;<a href=\"https://forms.gle/oEeVo3C39JUU4v2p7\"><u>a Google Form</u></a> to gauge the level of interest. Everyone was able to see the results. Additionally, I set up a channel in the EA Entrepreneur Slack for further discussion. Based on the interest expressed as well as the time zones of the participants, I scheduled a group call.</p><p>We decided to ask Charity Entrepreneurship (CE) if they could facilitate the process as they have several years of experience in finding and matching co-founders for new EA startups. In a call, Joey Savoie provided helpful guidance, sharing documents and later also inviting the team to use the CE offices for a joint weekend at the end of the process. Although they didn\u2019t have the capacity to facilitate the process, their resources were very helpful for us in setting up our own process.</p><h1>Steps in the matching process</h1><p>With the input of CE, a group of six people decided to start a matching process that would end in an in-person meeting in June, where the final decision on the co-founders would be made.&nbsp;We think the process length would have been excessive had many participants not been available before mid-June.</p><h2>Questions for co-founders</h2><p>The group took the document&nbsp;<a href=\"https://proof-assets.s3.amazonaws.com/firstround/50%20Questions%20for%20Co-Founders.pdf\"><u>50 Questions to Explore with a Potential Co-Founder</u></a>, removing some and adding additional questions. Once everybody had commented on the document, it was finalized, and everyone filled it out for themselves and shared it.</p><h2>Discussing questions for co-founders in 1:1s</h2><p>All participants then set up 1:1 calls with each of the other participants where they discussed the potential compatibility and roles each could take on in the new organisation. No co-founder pair ruled itself out after this process.</p><h2>5 weeks of online work</h2><p>In order to bridge the time until an in-person meeting was feasible, a process was set up where each participant worked with one of the other five for 45 minutes each week. As we didn\u2019t have any assignments prepared, we came up with a process where each pair would create an assignment that another pair would have to work on in the following week.</p><p>This is how the agenda looked:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\"><strong>Week</strong></td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\"><strong>Goal</strong></td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\"><strong>Workstream 1</strong></td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\"><strong>Workstream 2</strong></td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\"><strong>Workstream 3</strong></td></tr><tr><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Create assignment for week 2</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Marc, Maris</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Matt, Patrick</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Rudolf, Simeon</td></tr><tr><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">2</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Work on assignment</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Patrick, Rudolf</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Maris, Simeon</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Marc, Matt</td></tr><tr><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">3</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Create assignment for week 4</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Marc, Simeon</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Matt, Rudolf</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Maris, Patrick</td></tr><tr><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">4</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Work on assignment</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Maris, Rudolf</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Marc, Patrick</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Simeon, Matt</td></tr><tr><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">5</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Work on agenda for London meeting</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Simeon, Patrick</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Maris, Matt</td><td style=\"border:1px solid #000000;padding:2pt;vertical-align:top\">Marc, Rudolf</td></tr></tbody></table></figure><p><br>The different workstreams ensured that each pair generated or received assignments from another pair. We also found that trying to quickly brainstorm tasks that are good for testing co-founder fit was itself a good test of fit and forced us to spend solid time thinking about what makes for a good co-founder fit.</p><h2>Weekend workshop in London</h2><p>Originally, the idea was to spend several days coworking, however previous commitments reduced this to a two-day weekend. We settled on the UK as one participant was in Cambridge and the other in Oxford, whereas the rest were dispersed in Europe, Berkeley and the Bahamas. London was the easiest to reach for everyone, and Charity Entrepreneurship generously offered us to use their space.</p><p>As part of the online working process, we had proposals for the agenda but had to finalize them on the first morning. One of the participants was being held up by a delayed flight connection and would only join in the middle of the second day. In the first session, two of the group expressed that they weren\u2019t considering co-founding at the moment but would offer to join as advisors or help in other ways. One participant wanted to pursue an adjacent but different startup idea and wanted to convince the others to also consider this alternative plan.</p><p>We settled on a mix of group sessions and 1:1s where we clarified questions around the organisational setup as well as interpersonal fit. We also decided to write up our current decision concerning who we would like to co-found with most until the following day.</p><p>After spending the day together this way and going out for dinner the next morning started with the first reveal. Everyone copied their write-ups and pasted them into a new Google Doc at the same time. This gave us a first view of the current thinking of the others, although, with one missing, it was still a partial view. We then continued the 1:1s until all six were together.</p><p>For the afternoon, we agreed that we wouldn't leave the workshop without having settled on a co-founder pair. After giving the sixth participant some time for 1:1s, we settled on a time for the final reveal of preferences. Concerning the alternative startup idea, there was some discussion around spending some days exploring this, but ultimately, it was decided to go with the existing idea and make a decision the same day.</p><p>The reveal showed that three people were still interested in co-founding, with two preferring the same person as a co-founder. This person then held additional 1:1s until he decided who to select. The selection was mainly based on having worked together with the person previously and knowing each other. Both then agreed on a timeline for in-depth research into a product-market fit by doing customer interviews.</p><p>The two days ended in good spirits, with everyone joining for dinner and celebrating the successful workshop.<br>&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994823/mirroredImages/8QfQcFyj6aGNM78kz/wrv11oejfyzw4npb1emr.png\"><figcaption>vParticipants from left to right: Marc-Everin Carauleanu, Rudolf Laine*, Sim\u00e9on Campos, Mathieu Putz*, Maris Sala and Patrick Gruban (*selected co-founders)</figcaption></figure><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994821/mirroredImages/8QfQcFyj6aGNM78kz/vczfvqu8c9hpgxyw0tuo.png\"><figcaption>Goofing around between serious discussions.</figcaption></figure><h1>Summary and Learnings</h1><p>After the process, we set up a questionnaire and surveyed all of the participants with a mix of closed and open questions. The detailed answers are in the appendix.</p><p>While all participants rated the results as good, the overall process was rated lower. Most participants didn\u2019t think that an independent facilitator would have helped, and they would still have participated in the same process. Some comments noted that a shorter timeframe would have been preferred.</p><p>Of the individual interventions, the 1:1s in London and the narrowing down of the co-founders after the last reveal of preferences of the group were rated the highest (averaging 4,3 and 4,4 out of 5 points, respectively). A few comments reflected the view that meeting in person was important. The prior interventions got mixed reviews (2,3-3,8 points), with the questionnaire for co-founders rated worst (2,3). The five weeks of online work were seen as useful by 4 out of 6 with comments reflecting that it would have been useful to work on specific startup projects like customer interviews. As a general benefit of the process, comments mentioned getting to know more entrepreneurial-minded EAs and being able to spend time together.</p><p>As an alternative to this process, one commentator suggested people just informally contact each other and work together. In this case, this might have led to AI alignment researchers getting contacted by several teams in parallel for customer interviews. This could, however, also be solved by coordination without additional interventions.</p><p>Another alternative could be to have a process that more closely mirrors CE\u2019s but then would involve someone designing and managing it:</p><ul><li>Research the product idea in more depth before looking for co-founders</li><li>Screening potential entrepreneurs before accepting them into the program</li><li>Designing a program where entrepreneurs work on the company together and thereby getting to know each other</li></ul><p>In the way the idea was presented to us and how the group came together, this seemed unusual, so deriving general recommendations might not be useful. As a group, we thought spending time together in person in one place was most useful, so this might be something potential co-founders and supporters could prioritize.</p><p><a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org\">Eventually, the team pursuing this idea decided not to go through with it after spending about a month and a half on it</a> (mainly on customer interviews with alignment researchers).</p><h2>Personal Takes</h2><h3>Simeon</h3><p>I think the big mistake that we made was to make the cofounder matching process without having done almost any customer interviews and research on the topic before. I think that it was a significant mistake because it meant that most people had a very fuzzy view of what they were talking about. I&nbsp;personally realized this during the weekend brainstorming sessions with Marc and Patrick, where we did some tasks that led us to have a better view of what were the challenges of the human data field and the potential for improvement.</p><p>Thus, if I had to redo it again with a similar setup (i.e. someone comes up with a \u201cstartup idea\u201d that hasn\u2019t been tested and hands it over to other people), I would first commission someone (either someone external to the co-founder group or some people of the co-founder group) to do some customer interviews to have a clear view of what\u2019s needed to be successful in this area and what it would be like to co-found something in this area.&nbsp;</p><h3>Maris</h3><p>Eventually, it came down to who knew each other most from working together, so ideally, instead of doing test-1-on-1s, we could have done some specific projects over the course of a few weeks to see if we actually fit together. That could have been more useful than doing the 50 questions and the 1-on-1s - these were mainly an introduction, and during the weekend, when we discussed more practically work culture fit-related questions, it seemed like we could have chosen specific questions to zoom in on, instead of doing all 50.</p><h1>Acknowledgements</h1><p>We would like to thank the Long-Term Future Fund for approving the short-term request for travel funding and Joey Savoie/Charity Entrepreneurship for the support and hosting us in London. Thank you to the participants in the co-founder matching process for providing feedback and agreeing to it being openly shareable: Sim\u00e9on Campos, Marc-Everin Carauleanu, Patrick Gruban, Rudolf Laine, Mathieu Putz and Maris Sala.</p><h1>Appendix: Post-Workshop Survey</h1><h2>Overall Feedback</h2><p>We first asked some general questions on a scale from 1 to 5. The tables below show the numbers of results per answer:</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>Scale: 1 (very bad) - 5 (very good)</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>1&nbsp;</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>2</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>3</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>4</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>5</strong></td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">How would you rate the result of the process?</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">6</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr></tbody></table></figure><p><br><br>&nbsp;</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>Scale: 1 (very unlikely) - 5 (very likely)</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>1</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>2</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>3</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>4</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>5</strong></td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">If you would have known how the process would have been (without knowing the result of the weekend), would you have still participated?</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">3</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">Do you think an independent facilitator would have been good in the process?</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">3</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr></tbody></table></figure><h2>Question: What would you change in the process?</h2><h3>Answers</h3><blockquote><p>Seemed like it was both too fast and too slow. Eventually it came down to who knew each other most from working together, so ideally instead of doing test-1-on-1s we could have done some specific projects over the course of a few weeks to see if we actually fit together. That could have been more useful than doing the 50 questions and the 1-on-1s - these were mainly an introduction, and during the weekend when we discussed more practically about work culture fit related questions, it seemed like we could have chosen specific questions to zoom in on, instead of doing all 50. But that might have been an effect of having done the 50 questions and realising that a lot of the answers simply align and are thus not as useful.</p></blockquote><p>&nbsp;</p><blockquote><p>More evidence-based strategy</p></blockquote><p>&nbsp;</p><blockquote><p>I would probably not want a formal process at all. Just people who are interested in working with each other informally contacting each other.</p></blockquote><p>&nbsp;</p><blockquote><p>I would do some customer interviews before the final weekend + I would ensure that everyone has a better inside view of what the mission of the startup looks like.</p></blockquote><p>&nbsp;</p><blockquote><ul><li>Try to come up with better questions for the 1:1s</li><li>Try to narrow down earlier who would be really be interested in co-founding</li><li>Try to work on actual startup tasks</li></ul></blockquote><h2>Question: What did you like best or find most helpful?</h2><h3>Answers</h3><blockquote><p>It was good that there was some process to it, I actually found the 50 questions useful in terms of orienting myself towards the process, but any short overview of what the process would looked like would have sufficed. I liked the in-person weekend in London and how we ended up approaching it in a relaxed way - decided the plan once we were together. It seemed quite useful.</p></blockquote><p>&nbsp;</p><blockquote><p>Getting to know other entrepreneurial EAs</p></blockquote><p>&nbsp;</p><blockquote><p>Knowing who's interested.</p></blockquote><p>&nbsp;</p><blockquote><p>The in-person 1o1s were very useful. The fact of thinking about the company was also very useful. We maybe should have done more of that before the final week-end.</p></blockquote><p>&nbsp;</p><blockquote><p>Spending time together in London and getting to know people more</p></blockquote><h2>Feedback on the interventions</h2><p>Question: Please rate these elements of the process according to how useful you found them&nbsp;</p><h3>Answers</h3><figure class=\"table\"><table><tbody><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>Scale: 1 (not helpful) - 5 (very helpful)</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>1</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>2</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>3</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>4</strong></td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\"><strong>5</strong></td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">The whole process</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">3</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">The initial questionnaire where you expressed interest</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">3</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">The initial group call</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">4</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">The questionnaire of 50 questions for co-founders</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">3</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">The initial 1:1s where we talked about the 50 questions</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">4</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">The Slack channel</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">The 4-weeks process where we had 1:1s and created questions/answers</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">4</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">In London: The whole weekend</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">3</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">In London: The 1:1s</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">4</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">In London: The initial reveal Sunday morning</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">1</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td></tr><tr><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">In London: The narrowing down of founders after 4pm</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">3</td><td style=\"border:1px solid #000000;padding:5pt;vertical-align:top\">2</td></tr></tbody></table></figure><h2>Question: Anything else you would like to add?</h2><h3>Answers</h3><blockquote><p>I think I learned a lot from the process, also on a meta-level. I've never co-founded an organisation like this before and it was an interesting test of finding out which traits make for a co-founder that people like to pick as their number one, what kind of questions are useful to ask and when, how important it is to spend time in the physical space together, and whether I think I'm personally a good fit to found a company. Very grateful for the experience &lt;3</p></blockquote><p>&nbsp;</p><blockquote><ul><li>If it had not been the case that most people couldn't start until the end of the process anyways, I would've considered the process way too long. I think if running something similar with people who are ready to start sooner, the process should definitely wrap up in a tighter time schedule (perhaps by being shorter but more intense or just by having fewer hours).</li><li>Figuring out who you can work with on ambitious projects is enormously valuable. Having a group of competent people who you know how to work with / how well you'd work with is enormously valuable.</li><li>Differentiating among a group of people who are already competent impact-oriented EAs is quite difficult without either lots of time with someone, or experiences that mirror as closely as possible the actual process of doing work of the type that the project would involve.</li><li>It seems surprisingly difficult to make artificial tasks that are useful work tests, especially when a big part of the work that the task is supposed to test for is open-ended agentic execution toward complex goals in the real world.</li><li>Getting together in person to hash things out (rather than relying on scheduled remote calls) remains surprisingly effective.</li></ul></blockquote>", "user": {"username": "gruban"}}, {"_id": "2gWi89YDfwodyNqRk", "title": "Prize: \u2018What do you think the government should do to improve life in the UK?\u2019", "postedAt": "2022-09-24T11:50:05.884Z", "htmlBody": "<h1><strong>Heywood Prize (2022): \u2018What do you think the government should do to improve life in the UK?\u2019</strong></h1><p>60K prize pool. &nbsp;Link here: <a href=\"https://heywoodfoundation.com/contest/\">https://heywoodfoundation.com/contest/</a></p><p><i>With a closing date of the 31st December 2022, the Prize is seeking fresh proposals from all UK citizens who have innovative and impactful ideas with the potential to improve UK public policy.&nbsp; Entries can be in audio, video or text formats and the winning ideas will receive the Foundation\u2019s full backing for fast-tracking with UK government policymakers.</i></p><p>\"Last year\u2019s top prize went to a proposal to establish an <a href=\"https://heywoodfoundation.com/2021/06/24/a-territorial-army-for-the-nhs/\"><u>NHS Reserve Force</u></a>, a reserve \u2018army\u2019 of medically qualified volunteers to support the UK\u2019s health service, and especially in times of health emergencies. This idea was quickly shared with government and has since been implemented by NHS England. The NHS Reserve Programme, launched in March 2022, already has more than 4,100 reservists who are providing tens of thousands of hours of care in UK communities.\"</p><p><strong>Me: Could be one for UK EAs who have a good policy idea they would want to pitch. Some emphasis for those under 21, and younger generations.</strong></p><p>More from blurb:</p><p><i>Maybe you have an idea for how to reduce inflation, possibly you\u2019ve thought of a brilliant way to encourage sustainable living, or it could be that you\u2019ve thought of an idea to ensure equal access to higher education for school leavers; you might even have thoughts on how to decrease bureaucracy in the civil service \u2013 whatever your idea may be, and regardless of its complexity, we want to hear about it.</i></p><p><i>Our question is very broad but given how far reaching the government\u2019s input can be, this is your opportunity for you to tell us how the government could make life better for you, your family, your community or the population as a whole. Your answer might capture a simple policy detail or it could describe a big change in how we could live \u2013 or are living \u2013 our lives.</i></p><p><i>Credit will be given for the originality, practicality, and impact of ideas. Perhaps even more importantly, the winning submissions will be put on the desks of policymakers within the government, will attract wider debate, and perhaps will be enacted to make our world a better place.</i></p><p>Entries may be submitted from 00:01, Wednesday 7th September<br>2022 until 23:59, Saturday 31st December 2022.</p><p>Acceptable formats are:<br>\u2022 Text (up top 1500 words)<br>\u2022 Uploaded video (maximum of 3 minutes)<br>\u2022 Uploaded audio (maximum of 3 minutes)<br>\u2022 Images (maximum of 3) *may be submitted with text</p><p>Prize breakdown:<br><br>Main prize<br>\u2022 Top prize: \u00a325,000<br>\u2022 Second prize: \u00a310,000<br>\u2022 Third prize: \u00a35,000<br>\u2022 15 x runner-ups: \u00a31,000 each<br><br>Youth prize (21 and under)<br>\u2022 Top prize: \u00a35,000<br>\u2022 10 x runner-ups: \u00a3500 each<br>*** Youth entries will all be entered into both the main prize and the youth prize.</p>", "user": {"username": "Ben Yeoh"}}, {"_id": "ZEdeivnidwsbsGyuD", "title": "\"Cotton Gin\" AI Risk", "postedAt": "2022-09-24T23:04:04.196Z", "htmlBody": "", "user": {"username": "423175"}}, {"_id": "pHD69RqnQQCaEyHt3", "title": "AI co\u00f6peration is more possible than you think", "postedAt": "2022-09-24T23:04:04.204Z", "htmlBody": "", "user": {"username": "423175"}}, {"_id": "7JbPDhBcjHNm9E3FQ", "title": "[Linkpost] \"Intensity and frequency of extreme novel epidemics\" by Mariani et al. (2021)", "postedAt": "2022-09-28T03:31:01.760Z", "htmlBody": "<p>This is a useful paper for those wanting to better understand the risks posed by pandemics and, more generally, global catastrophic biological risk.</p><p>I have not seen this paper mentioned on LW or EAF and it does not appear on LW or EAF when entered in the search box, so I thought I'd contribute it to the wider conversation.&nbsp;</p><p>In this linkpost, I include, in the following order, the structure of the paper, the paper's abstract, some commentary and quotes, and the 2 most important figures from the paper.&nbsp;</p><p>To see the papers that have cited <i>Intensity and frequency of extreme novel epidemics</i>, click <a href=\"https://scholar.google.com/scholar?start=0&amp;hl=en&amp;as_sdt=2005&amp;sciodt=0,5&amp;cites=6645370367209265025&amp;scipsc=\">here</a>. To see papers that <i>Intensity and frequency of extreme novel epidemics </i>cites, click <a href=\"https://www.citationtree.org/tree?id=10.1073/pnas.2105482118\">here</a>.</p><p>Possible ways to cite the paper:</p><blockquote><p>Marani, Marco, Gabriel G. Katul, William K. Pan, and Anthony J. Parolari. \"Intensity and frequency of extreme novel epidemics.\" <i>Proceedings of the National Academy of Sciences</i> 118, no. 35 (2021): e2105482118.</p><p>&nbsp;@article{marani2021intensity,\n &nbsp;title={Intensity and frequency of extreme novel epidemics},\n &nbsp;author={Marani, Marco and Katul, Gabriel G and Pan, William K and Parolari, Anthony J},\n &nbsp;journal={Proceedings of the National Academy of Sciences},\n &nbsp;volume={118},\n &nbsp;number={35},\n &nbsp;pages={e2105482118},\n &nbsp;year={2021},\n &nbsp;publisher={National Acad Sciences}\n}</p></blockquote><h3><strong>Structure</strong></h3><ul><li>Introduction</li><li>Results<ul><li>The Probability Distribution of Epidemic Intensity</li><li>The Probability of Occurrence of Extreme Epidemics</li></ul></li><li>Discussion</li><li>Materials and Methods</li></ul><h3><strong>Abstract</strong></h3><blockquote><p><i>Observational knowledge of the epidemic intensity, defined as the number of deaths divided by global population and epidemic duration, and of the rate of emergence of infectious disease outbreaks is necessary to test theory and models and to inform public health risk assessment by quantifying the probability of extreme pandemics such as COVID-19. Despite its significance, assembling and analyzing a comprehensive global historical record spanning a variety of diseases remains an unexplored task. A global dataset of historical epidemics from 1600 to present is here compiled and examined using novel statistical methods to estimate the yearly probability of occurrence of extreme epidemics. Historical observations covering four orders of magnitude of epidemic intensity follow a common probability distribution with a slowly decaying power-law tail (generalized Pareto distribution, asymptotic exponent = \u22120.71). The yearly number of epidemics varies ninefold and shows systematic trends. Yearly occurrence probabilities of extreme epidemics, P<sub>y</sub>, vary widely: P<sub>y</sub> of an event with the intensity of the \u201cSpanish influenza\u201d (1918 to 1920) varies between 0.27 and 1.9% from 1600 to present, while its mean recurrence time today is 400 y (95% CI: 332 to 489 y). The slow decay of probability with epidemic intensity implies that extreme epidemics are relatively likely, a property previously undetected due to short observational records and stationary analysis methods. Using recent estimates of the rate of increase in disease emergence from zoonotic reservoirs associated with environmental change, we estimate that the yearly probability of occurrence of extreme epidemics can increase up to threefold in the coming decades.</i></p></blockquote><h3><strong>Commentary</strong> <strong>and Quotes</strong></h3><ul><li><strong>Main contributions</strong>: produced a dataset of yearly historical epidemics [1600, 2022]; estimated annual probability of occurrence of extreme epidemics; commented on how the annual probability of extreme epidemics may change</li><li><strong>Dataset&nbsp;</strong> (see <a href=\"https://doi.org/10.5281/zenodo.4626111\">here</a>): \"<i>...the 1600 to 1945 dataset includes 182 epidemics with known oc- currence, duration, and number of deaths, 108 known to have caused less than 10,000 deaths, and 105 for which only occurrence and duration are recorded, for a total of 395 epidemics.</i>\"</li><li><strong>Epidemic intensity</strong> \"<i>..is well described by a generalized Pareto distribution (GDP)</i>\"</li><li><strong>Recurrence of Spanish Flu-like pandemic</strong>: \"<i>Such a change would bring, possibly over decadal time scales, the average recurrence interval of a Spanish flu\u2013like event down to 127 y (95% CI 115 to 141 y), comparable to the value it had around 1918 (i.e., 91 y).</i>\"</li><li><strong>Recurrence of COVID-19 like pandemic</strong>: \"<i>Our analysis also quantifies how frequently a COVID-19\u2013like event may occur in the future. Current information (19) indicates that the epidemic progresses at a rate of about 2.5 million deaths/ year (3,549,710 in 72 wk), which, normalized by the global pop- ulation, corresponds to an intensity of the epidemic of 0.33 \u2030/year. Using the number of epidemic occurrences observed in the past 20 y (i.e., 2000 to 2019) in the MEVD model, this in- tensity corresponds to an average recurrence time of 59 y (95% CI 55 to 64 y).</i>\"</li></ul><h3><strong>Dataset Figure</strong></h3><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/742caf64e19a7c07fe0693a4e241af0ac50e1ea3b1141f8c.png/w_833 833w\"></figure><h3><strong>Estimates Figure&nbsp;</strong></h3><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e6c8d256d7ce72272c5da842b3ea3915dcc7875672c690cf.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e6c8d256d7ce72272c5da842b3ea3915dcc7875672c690cf.png/w_83 83w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e6c8d256d7ce72272c5da842b3ea3915dcc7875672c690cf.png/w_163 163w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e6c8d256d7ce72272c5da842b3ea3915dcc7875672c690cf.png/w_243 243w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e6c8d256d7ce72272c5da842b3ea3915dcc7875672c690cf.png/w_323 323w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e6c8d256d7ce72272c5da842b3ea3915dcc7875672c690cf.png/w_403 403w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e6c8d256d7ce72272c5da842b3ea3915dcc7875672c690cf.png/w_483 483w\"></figure>", "user": {"username": "rodeo_flagellum"}}, {"_id": "coryFCkmcMKdJb7Pz", "title": "Does Economic Growth Meaningfully Improve Well-being? An Optimistic Re-Analysis of Easterlin\u2019s Research: Founders Pledge", "postedAt": "2022-09-27T16:28:54.915Z", "htmlBody": "<p><strong>Acknowledgments</strong></p>\n<p>I would like to thank Michael Plant, Matt Lerner and Rosie Bettle for their helpful comments and advice.</p>\n<h2>Summary</h2>\n<p>Understanding the relationship between wellbeing and economic growth is a topic that is of key importance to Effective Altruism (e.g. see <a href=\"https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development\">Hillebrandt and Hallstead</a>, <a href=\"https://forum.effectivealtruism.org/posts/a84LrFzSf3sGsYfNr/can-we-drive-development-at-scale-an-interim-update-on\">Clare and Goth</a>). In particular, a key disagreement regards the Easterlin Paradox; the finding that happiness<sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-1\" id=\"fnref-AedR3ixdautEMhyBd-1\">[1]</a></sup> varies with income across countries and between individuals, but does not seem to vary significantly with a country\u2019s income as it changes over time. Michael Plant recently wrote an excellent <a href=\"https://forum.effectivealtruism.org/posts/gCDsAj3K5gcZvGgbg/will-faster-economic-growth-make-us-happier-the-relevance-of\">post</a> summarizing this research. He ends up mostly agreeing with Richard Easterlin\u2019s latest <a href=\"https://docs.iza.org/dp13923.pdf\">paper</a> arguing that the Easterlin Paradox still holds; suggesting that we should look to approaches other than economic growth to boost happiness. I agree with Michael Plant that life satisfaction is a valid and reliable measure, that it should be a key goal of policy and philanthropy, and that boosting income does not increase it as much as we might naively expect. In fact, we at Founders Pledge highly value and regularly use Michael Plant\u2019s and Happier Lives Institute\u2019s (HLI) research; and we believe income is only a small part of what interventions should aim at. However, my interpretation of the practical implications of Easterlin\u2019s research differ from Easterlin\u2019s in three ways which I argue in this post:</p>\n<ol>\n<li>Easterlin finds small coefficients in his preferred regressions of changes in countries\u2019 happiness on changes in GDP. He concludes that these coefficients have low \u201ceconomic significance\u201d and that increasing economic growth is not a good way to make people happier. However, even if we take these coefficients at face value, they still represent a very meaningful increase in wellbeing within the effective altruism framework, consistent with the impacts of unconditional cash transfers on individuals. The benefits become very large when aggregated across all the people in a country for many years.</li>\n<li>We also have reason to doubt Easterlin\u2019s results, in that they are highly sensitive to small changes in methodology. We perform two variations on his regression that fully accept his methodology of only including \u201cfull cycle\u201d countries, but update it slightly, reversing the result. If we replicate his results counting one more country as a \u201ctransition\u201d economy, the Easterlin paradox largely disappears. If we repeat his analysis with new data from 2020 instead of 2019, the paradox also seems to largely disappear.</li>\n<li>It may be difficult to find things we can influence whose change over time will have a higher correlation to a country\u2019s change in happiness than changes in GDP. Even if we accept that boosting GDP does not meaningfully increase happiness, other potential means of boosting national happiness may increase it even less. If we rerun Easterlin\u2019s analysis using three interventions Easterlin and Plant suggest (health, pollution, and a comprehensive welfare state), their implied impacts on national happiness are much smaller than the impacts for GDP or negative. However, I have low confidence in this conclusion, and think it is a very valuable project to identify the interventions that are most likely to have an impact on happiness.</li>\n</ol>\n<h2>1. Taking Easterlin\u2019s results at face value and estimating impact</h2>\n<p><a href=\"https://docs.iza.org/dp13923.pdf\">Easterlin and O\u2019Connor (2022)</a> rely on two regressions for their conclusions, both comparing annual changes in a country\u2019s happiness to annual changes in per capita GDP. The first measures happiness using a \u201clife satisfaction\u201d survey question on a smaller set of countries from 1981-2019 and the second uses a \u201cbest possible life\u201d survey question on a larger set of countries from 2005-2019. After excluding some of the countries in the dataset, the authors find that a one percent increase in annual GDP growth rate increases happiness by .001 and .0024 life satisfaction points in the two regressions. They conclude that these coefficients imply that it would take 500-1000 years of one percentage point higher GDP growth to increase happiness by one point, and have low \u201ceconomic significance.\u201d  At first glance, these numbers do seem negligible.</p>\n<p>However, once we compare these numbers to what we would expect from the literature on the happiness impacts of cash transfers, we find that they are no smaller than we should expect. Despite being small, these numbers are not exactly 0, and to get a sense of their practical implications we need to convert them to units more familiar in effective altruism. If we want to compare the impact of economic growth to the impact of interventions like cash transfers or deworming, it is helpful to convert the happiness impact of one percentage point higher growth to units capturing the happiness impact of doubling income. In order to do this, we have to consider that it would take 71 years to create an additional doubling of income by boosting growth by one percentage point. Therefore, a doubling of income would <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=347370759\">produce</a> a 0.07 point increase in happiness using Easterlin\u2019s first regression and a 0.17 point increase using the second. In comparison, HLI\u2019s <a href=\"https://www.happierlivesinstitute.org/report/cash-transfers-systematic-review-and-meta-analysis/\">meta-analysis</a> suggests that providing a cash transfer that doubles income for an individual leads to a 0.1 standard deviation increase in subjective well being. This equates to roughly 0.2 life satisfaction points for the recipient of the transfer. When we use HLI\u2019s <a href=\"https://www.getguesstimate.com/models/15950\">methodology</a> to adjust for the fact that other household members likely experience smaller benefits, we <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=385192755\">get</a> an expected increase of 0.14 life satisfaction points for an average person. So one of Easterlin\u2019s estimates is lower than the impact of a cash transfer, and one is higher. <strong>The overall picture appears to be consistent with changes in GDP providing as much happiness as changes in individual income resulting from cash transfers.</strong> GiveDirectly, which provides unconditional cash transfers, has historically been one of GiveWell\u2019s top charities, and generally seems like a very good use of money even if it is not the very best.</p>\n<p>The happiness impacts of boosting GDP become very large when we take individual impacts that are comparable to GiveDirectly and aggregate them for a whole country for many years. Let us consider the impact of boosting incomes for a whole country with the same population as Ethiopia. We assume that we can find an intervention that boosts GDP growth by one percentage point for 40 years, and that the happiness impacts of this are as small as estimated by Easterlin. Only considering effects over forty years is a fairly arbitrary choice, picked to match GiveWell\u2019s methodology of valuing income increases for 40 years, discounted at 4% annually. I think this is a fairly conservative choice, as some economic <a href=\"https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/\">research</a> suggests very long-term persistence of changes to GDP. We sum the discounted happiness boost across the entire population. The impact of boosting annual GDP growth from 2% to 3% would <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=135302151\">produce</a><sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-2\" id=\"fnref-AedR3ixdautEMhyBd-2\">[2]</a></sup> the equivalent of approximately 400 million person-years of doubled income. HLI and GiveWell each independently estimate that the most cost effective interventions they have identified are approximately 10 times as cost effective as GiveDirectly at improving well-being. Using this multiple, the current costs of GiveDirectly <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=135302151\">suggest</a> that EA as a community should be happy to spend $10 billion to boost GDP growth in Ethiopia by 1 percentage point for forty years. The amount would be even higher if we incorporated the likely impact of higher GDP on health and education. This is more than ten times as much as all of the money EA is likely to <a href=\"https://80000hours.org/2022/05/ea-and-the-current-funding-situation/\">move</a> this year, and <a href=\"https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development#The_World_Bank\">likely</a> more than the annual funding of all economics professors worldwide, the IMF, and development economics at the World Bank combined. This does not have any conclusive implications for whether boosting growth in a country like Ethiopia is tractable at these funding levels. However, it does suggest that the well-being benefits are very significant from an EA perspective, in contrast to Easterlin\u2019s interpretation.</p>\n<h2>2. Easterlin\u2019s estimates of impact become much larger with small changes in methodology.</h2>\n<p>The previous section looks at the impacts suggested by Easterlin\u2019s methodology if we take it at face value. However, this methodology generates lower regression coefficients than most similarly reasonable alternative specifications. We compare Easterlin\u2019s results with those we get if we rerun his analysis making a different choice about whether we consider India a transition economy, and then by rerunning his analysis with updated happiness survey data. Additionally, we compare Easterlin\u2019s headline results with alternative versions he presents in his paper. These alternative versions of the analysis yield coefficients more consistent with the idea that GDP gains over time yield as much happiness increase as we would expect from cross sectional data than they are with the Easterlin Paradox. Therefore, I don\u2019t think this latest paper should update us much away from the intuitive idea that higher incomes lead to more well being.</p>\n<p>Cross sectional data suggests that we should expect a 0.5 point increase in happiness from a doubling in income. If we look at a regression of <a href=\"https://news.gallup.com/poll/122453/understanding-gallup-uses-cantril-scale.aspx\">Cantril Ladder</a> \u201cbest possible life\u201d scores against GDP on a log scale, the coefficient <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=347370759\">implies</a> slightly under 0.5 points per GDP doubling.</p>\n<p><em>Cantril Ladder versus GDP (<a href=\"https://www.frbsf.org/economic-research/wp-content/uploads/sites/4/wp10-28bk.pdf\">Sacks et al. 2010</a>)</em></p>\n<p><img src=\"http://res.cloudinary.com/cea/image/upload/v1669763143/mirroredImages/coryFCkmcMKdJb7Pz/oft4zupnynx0ixjplkx4.jpg\" alt=\"alt_text\" title=\"image_tooltip\"></p>\n<p>Similarly, if we look at a graph of Cantril Ladder scores for individuals versus their incomes (figure 1 in Michael Plant\u2019s <a href=\"https://forum.effectivealtruism.org/posts/gCDsAj3K5gcZvGgbg/will-faster-economic-growth-make-us-happier-the-relevance-of\">post</a>), we can <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=347370759\">estimate</a> around 0.5 points per income doubling. In contrast, if we look at Easterlin\u2019s .0024 regression coefficient, it only implies an increase of 0.17 points per income doubling.<sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-3\" id=\"fnref-AedR3ixdautEMhyBd-3\">[3]</a></sup> This is close enough to 0 that it is reasonable for Easterlin to classify it as a paradox when compared to the 0.5 point estimates from alternative sources of data. However, when I rerun Easterlin\u2019s analysis classifying one additional ambiguous case as a transition economy, or using newer data, the coefficients increase. The new results are closer to 0.5 than they are to 0, and don\u2019t seem to imply the existence of a paradox.</p>\n<p>Easterlin argues that we need to exclude countries that transitioned from socialist to capitalist economies from our analysis in order to remove the noise created by countries that only start to conduct happiness surveys just as their economies plummet with the start of the transition to capitalism. Most of the countries he excludes are Eastern European, but he also considers China a transition economy. I think it would be reasonable to put India in the same category as China, since both countries experienced a more <a href=\"https://en.wikipedia.org/wiki/Socialism_in_India\">gradual</a> transition from socialism than did Eastern Europe with the collapse of the Soviet Union. If we <a href=\"https://docs.google.com/spreadsheets/d/1vQtuEO4rWDsOJoowgbF8S9AZqLKjjftdv_1RvM4-CaI/edit#gid=371185624\">repeat</a> Easterlin\u2019s analysis with his data, but exclude India along with China, we get an estimate of 0.3 life satisfaction points per income doubling. So the estimate moves from being closer to 0 to being closer to 0.5 after a minor methodological adjustment.</p>\n<p>Next, I replicate Easterlin\u2019s analysis with newly available 2020 \u201cbest possible life\u201d scores, instead of the 2019 data in the original paper. In this regression I accept all of his methodological choices about which transition economies to exclude, and how to decide whether a country needs to be excluded for insufficient data. The new regression <a href=\"https://docs.google.com/spreadsheets/d/1vQtuEO4rWDsOJoowgbF8S9AZqLKjjftdv_1RvM4-CaI/edit#gid=136671528\">implies</a> an impact of 0.3 life satisfaction points per income doubling.<sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-4\" id=\"fnref-AedR3ixdautEMhyBd-4\">[4]</a></sup> Once again, this version of the analysis is closer to being consistent with the cross sectional data (0.5 points per income doubling), than it is to a paradox (0 points).</p>\n<p>Similarly, if we look at alternative versions of the regressions included in Easterlin and O\u201dConnor (<a href=\"https://docs.iza.org/dp13923.pdf\">2022</a>), almost all of them have much higher coefficients than the main result. Easterlin makes two key methodological choices. The first is excluding transition economies. For both the \u201clife satisfaction\u201d and \u201cbest possible life\u201d regressions, not excluding the transition economies would <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=347370759\">imply</a> an impact of 0.4 life satisfaction points per income doubling. The second choice Easterlin makes is excluding all countries from the \u201cbest possible life\u201d regression that have fewer than 12 years of data available. When he includes the 8 additional countries with 10 or 11 years of data, the impact also goes up to 0.4. I think Easterlin makes good arguments for these two choices.<sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-5\" id=\"fnref-AedR3ixdautEMhyBd-5\">[5]</a></sup> <strong>However, I think we have to consider how sensitive his conclusion is to judgment calls when deciding how much to believe that there is a surprising paradox</strong><sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-6\" id=\"fnref-AedR3ixdautEMhyBd-6\">[6]</a></sup> in the happiness data.</p>\n<h2>3. The happiness impact of alternative interventions is smaller than the impact of GDP.</h2>\n<p>Easterlin concludes his latest paper by suggesting that even though he does not believe that GDP growth has a meaningful impact on happiness, that there are a number of better interventions. Michael Plant adds some suggestions to the list in his <a href=\"https://forum.effectivealtruism.org/posts/gCDsAj3K5gcZvGgbg/will-faster-economic-growth-make-us-happier-the-relevance-of\">post</a>, coming up with a set of potential interventions that includes:</p>\n<pre><code>\u201c...job security, a comprehensive welfare state, getting citizens to be healthy, and encouraging long-term relationships\u2026[taking] mental health and palliative care more seriously\u2026improved air quality, reduced noise, more green and blue space (blue spaces being water), and getting people to commute smaller distances (Diener et al. 2019). Social interactions could be enhanced via urban design, reducing corruption, increasing transparency, supporting healthy family relationships, and maybe even things like progressive taxation.\u201d\n</code></pre>\n<p>All of these sound like promising ideas, and are a good research agenda for future investigation. However, it may be difficult to find one of these measures that has a higher impact on country-level happiness than GDP using Easterlin\u2019s methodology. To perform an exploratory analysis, I start with Easterlin\u2019s data from his \u201cbest possible life\u201d regression (taking his relatively low estimated impacts at face value as I do in section 1.) I then choose three interventions from Michael Plant\u2019s list that seem to have a fair amount of annual data available on <a href=\"https://ourworldindata.org/charts\">OurWorldInData.org</a>: health, pollution and a comprehensive welfare state.<sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-7\" id=\"fnref-AedR3ixdautEMhyBd-7\">[7]</a></sup> I replace annual GDP growth in Easterlin\u2019s regression with annual growth on these three metrics, and perform a separate analysis for each one.<sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-8\" id=\"fnref-AedR3ixdautEMhyBd-8\">[8]</a></sup> Each regression looks at annualized changes in a country\u2019s Cantril ladders scores versus annualized changes in the specified metric for the past 12-14 years. The <a href=\"https://docs.google.com/spreadsheets/d/1vQtuEO4rWDsOJoowgbF8S9AZqLKjjftdv_1RvM4-CaI/edit#gid=944269987\">health regression</a> estimates how much a decrease in the number of years people in a country lose to ill health corresponds to increases in happiness. This regression produces coefficients that are either an order of magnitude smaller than the GDP regression, or negative, depending on whether we exclude countries that have less than 12 years of data. In both cases the r-squared of the regression is essentially 0.. There does not appear to be a way to interpret these results to suggest that changes in health have a higher impact on national happiness than changes in GDP. The <a href=\"https://docs.google.com/spreadsheets/d/1vQtuEO4rWDsOJoowgbF8S9AZqLKjjftdv_1RvM4-CaI/edit#gid=1513636665\">pollution regression</a> repeats the methodology for health, but looks at only the changes in the years of life lost to pollution. This analysis actually shows negative results of a magnitude similar to the positive results of the GDP regression. This would imply that increases in pollution are actually associated with countries getting happier. For example, the Republic of Congo and Benin both had large annual increases in happiness despite increasing levels of pollution.<sup class=\"footnote-ref\"><a href=\"#fn-AedR3ixdautEMhyBd-9\" id=\"fnref-AedR3ixdautEMhyBd-9\">[9]</a></sup> The <a href=\"https://docs.google.com/spreadsheets/d/1vQtuEO4rWDsOJoowgbF8S9AZqLKjjftdv_1RvM4-CaI/edit#gid=242227119\">comprehensive welfare state regression</a> examines the impact of changes in a score of whether a country has an adequate safety net. This analysis also shows negative results, however there are very few countries and years for which this data is available and the data appears to be of low quality, suggesting that we should not read too much into this result. <strong>In all three of these analyses we do not find any evidence consistent with any of these metrics having a higher impact on national happiness than changes in GDP.</strong></p>\n<p>I do not have a high level of confidence in these initial results. There are likely better sources of data, and better methodologies to employ. However, I do think they suggest that it may be difficult to find any interventions of their kind which will imply a larger impact on happiness than GDP using Easterlin\u2019s methodology.</p>\n<h2>4. Conclusion</h2>\n<p>Easterlin\u2019s estimates of the impact of GDP growth on happiness are not as small as they initially appear. They are consistent with experimental data from individual cash transfers, and imply large welfare gains when aggregated for an entire country. When I consider slight variations in methodological choices that Easterlin makes, or update his data for 2020, the estimated impacts get much bigger. This leads me to decrease my belief in the existence of an Easterlin Paradox that we need to explain. But even if we accept Easterlin\u2019s estimates, it may be difficult to find other things we can influence that will have a larger measured impact on happiness than GDP growth. I find three of the more promising potential ways to boost national happiness to have a smaller impact than boosting GDP. Of course, other interventions may prove to be far more tractable than boosting GDP, even if they have a lower impact on happiness. Also, we can likely find better sources of evidence than regressions with fewer than a hundred datapoints. So my conclusion is not that different from Easterlin and Michael Plant in that I do think the interventions they propose are very promising routes to explore towards increasing happiness. I just don\u2019t think the data warrants dismissing GDP growth as a potentially even more promising route.</p>\n<h2>Notes</h2>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-AedR3ixdautEMhyBd-1\" class=\"footnote-item\"><p>I use happiness in this post interchangeably with both life satisfaction and Cantril Ladder \u201cbest possible life\u201d scores. Easterlin does not discuss measures of affect or other more immediate metrics, so happiness is not meant to refer to those here. <a href=\"#fnref-AedR3ixdautEMhyBd-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-2\" class=\"footnote-item\"><p>This <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=135302151\">spreadsheet</a> calculation starts with one individual benefiting from increased GDP growth, and assumes that benefits of the GDP growth intervention start accruing after 8 years (just as the benefits of deworming start to accrue 8 years after). On the 9th year, the benefit is the difference between growth of 3% versus 2%, discounted by 4% for 9 years. The benefit is quantified as ln of percent income (to be consistent with GiveWell income boosting CEAs). The 10th year has further growth of the same amount, and is discounted by 4% for 10 years. We continue this estimation for 40 years and sum across all years. We then multiply by 115 million, the population of Ethiopia. The total ln impact is then converted to income doublings by dividing by ln(2). We consider the cost of an income doubling to be 1/10 the $294 GiveWell uses for GiveDirectly. We discount the final result slightly because the average estimated impact in Easterlin\u2019s two regressions is slightly below the impact of cash transfers on happiness. <a href=\"#fnref-AedR3ixdautEMhyBd-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-3\" class=\"footnote-item\"><p>I focus on replicating the Cantril Ladder \u201cbest possible life\u201d regression from the Gallup survey within Easterlin and O\u2019Connor because the data is more readily available online than the World Values Survey \u201clife satisfaction\u201d regression. I am also not able to benefit from the longer time series in the World Values Survey because my potential interventions in section 3 have limited historical data. After arriving at regression coefficients, I convert them to \u201clife satisfaction impact of an income doubling\u201d <a href=\"https://docs.google.com/spreadsheets/d/1aDUPvizGsgT6rLtIf8RkT8LNTmZyXjlXa7Kddc-UeWM/edit#gid=347370759\">here</a>. <a href=\"#fnref-AedR3ixdautEMhyBd-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-4\" class=\"footnote-item\"><p>The regression is not an exact match to Easterlin\u2019s because I use a different source for GDP, the new dataset includes more countries, and I use a simplified methodology for estimating annual happiness change (I do this because I believe it is more consistent with how we calculate annual GDP change, and for simplicity). However, my 2019 <a href=\"https://docs.google.com/spreadsheets/d/1vQtuEO4rWDsOJoowgbF8S9AZqLKjjftdv_1RvM4-CaI/edit#gid=1993139538\">coefficient</a> roughly matches Easterlin\u2019s. <a href=\"#fnref-AedR3ixdautEMhyBd-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-5\" class=\"footnote-item\"><p>The reason that Easterlin excludes economies that have less than 12 years of data and that recently transitioned from socialism to capitalism is that he does not consider them to be full-cycle countries. Easterlin accepts that short term fluctuations in the business cycle have impacts on happiness in the short term.  However, he thinks that once we zoom out to longer patterns of GDP growth across an entire business cycle, these impacts disappear.  Since economies that transitioned from socialism have generally been growing since happiness surveys for these countries have become available, they have not experienced a full business cycle including a \u201cbust.\u201d Similarly, Easterlin does not consider countries with less than 12 years of data to have had a full business cycle (although he does not specify why the cutoff should be 12 rather than 13 or 11). <a href=\"#fnref-AedR3ixdautEMhyBd-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-6\" class=\"footnote-item\"><p>Easterlin argues that this contradiction can be resolved by considering that people evaluate their lives in comparison to those around them. While this could explain the contradiction with cross-sectional data of individuals within a country, I do not think it does a good job of explaining the cross-sectional data at the country level, or the data from cash transfers. If people were evaluating their lives in comparison to those around them, then people in poorer countries would not be much less happy than people in rich countries at a given point in time (especially in older data sets before the spread of Western media). People would also not get any happier when they and everyone in their village received a cash transfer from GiveDirectly. <a href=\"#fnref-AedR3ixdautEMhyBd-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-7\" class=\"footnote-item\"><p>I proxy health with \u201cDALYs-rate-from-all-causes per 100,000,\u201d pollution with \u201cDALYs-particulate-matter per 100,000,\u201d and a comprehensive welfare state with \u201cadequacy-of-social-safety-net-programs.\u201d <a href=\"#fnref-AedR3ixdautEMhyBd-7\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-8\" class=\"footnote-item\"><p>I begin the analysis by pulling all happiness scores, and years for which they were pulled from Easterlin\u2019s appendix. In order to estimate the changes in each new metric, I pull data for all available years from OurWorldInData. I then do a lookup for the country-year pairs corresponding to the starting and ending year for each country in Easterlin\u2019s data. Finally, I annualize the changes from the first to last year using the same methodology as Easterlin uses for GDP. I also multiply the annual changes by a factor so that they have the same standard deviation as the changes of GDP, and so that larger numbers imply improvement. I do this in order to have a consistent interpretation of coefficients. However, the conclusions are not sensitive to this methodological choice. <a href=\"#fnref-AedR3ixdautEMhyBd-8\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-AedR3ixdautEMhyBd-9\" class=\"footnote-item\"><p>Of course, this does not mean that decreasing pollution levels is bad for happiness. The more likely explanation is that higher pollution in low and middle income countries is associated with more industrial jobs and more cars, both of which probably make people happier. Although, surprisingly, controlling for GDP growth does not seem to do much to reduce the <a href=\"https://docs.google.com/spreadsheets/d/1vQtuEO4rWDsOJoowgbF8S9AZqLKjjftdv_1RvM4-CaI/edit#gid=261268268\">coefficient</a> on pollution. <a href=\"#fnref-AedR3ixdautEMhyBd-9\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Vadim Albinsky"}}, {"_id": "hmLMBjutEbrHctAGo", "title": "You Can Embed Flashcard Quizzes in Your posts!", "postedAt": "2022-09-26T11:51:10.918Z", "htmlBody": "<p><i>This is a </i><a href=\"https://www.lesswrong.com/posts/yK8mKmMQ73TuzgCv6/you-can-now-embed-flashcard-quizzes-in-your-lesswrong-posts\"><i>cross-post from LessWrong.</i></a><br><br>With the help of the EA Forum team, we've set up a way for you to embed flashcard quizzes directly in your EA forum posts! This means that you can write flashcards for any of your posts and either:&nbsp;</p><p>(a) quiz people as they read your article to help them retain your content, or&nbsp;</p><p>(b) provide an easy way for them to continue to be quizzed after they are done reading so that they can indefinitely remember the most important things they learned in your article!</p><p>Note that while many people think of flashcards as being just for facts and definitions, this is far from the truth! I personally use flashcards/spaced repetition daily for complex concepts, connections between ideas, ways of thinking about a problem, patterns, takeaways, strategies, arguments, triggers I want to associate with specific behavior, and so on.</p><p>This post will explain how to add flashcards to your own posts in a step-by-step fashion.</p><p>If you want to see an example of an EA Forum post with flashcards, check out my post on <a href=\"https://forum.effectivealtruism.org/posts/XabosWtHBZP6DTG88/14-techniques-to-accelerate-your-learning-1\">14 Tips to Accelerate your learning.</a></p><p>And before we get to the instructions, here's an example (from that same post) of what embedded flashcards look like when you put them in your article:</p><figure class=\"media\"><div data-oembed-url=\"https://app.thoughtsaver.com/embed/IftOcyUoGF?start=1&amp;end=3\">\n\t\t    <div class=\"thoughtSaverFrameWrapper\">\n\t\t      <iframe class=\"thoughtSaverFrame\" src=\"https://app.thoughtsaver.com/embed/IftOcyUoGF?start=1&amp;end=3\"></iframe>\n\t\t    </div>\n\t\t  </div></figure><p>Adding flashcards to your post is quite simple. Just follow these steps:</p><h2><strong>Step 1: Create your own deck of flashcards using Thought Saver</strong></h2><p>Create a <a href=\"https://www.thoughtsaver.com/\">Thought Saver</a> account at <a href=\"https://app.thoughtsaver.com/\"><u>app.thoughtsaver.com</u></a> and use it to create some flashcards for your post. You'll need to put all the flashcards for your post into <i>the same</i> deck. Here\u2019s how to do that:</p><p><strong>(i) Click \u201cNew Card\u201d in Thought Saver to start creating a new flashcard</strong></p><p><strong>(ii) In the text input box with the label \"Deck\"...</strong></p><ol><li>Type the name of the new deck you'd like to create for your article.</li><li>Hit \"Enter\"&nbsp;</li><li>Your first card has been created, and it has been added to this deck. Now add all the additional cards you would like to add to this deck (so that you can share them all in a forum post).</li></ol><figure class=\"image image_resized\" style=\"width:74.69%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f79b9d66fe49c773fada010b9a322ad62bd18b22ad888184.png/w_1136 1136w\"></figure><p><strong>Example:</strong>&nbsp;</p><ul><li>Deck \"Accelerate My Learning\"</li><li>Click 'Create new deck: \"Accelerate My Learning.\"'</li><li><strong>Repeat these steps until you\u2019ve created </strong><i><strong>all </strong></i><strong>flashcards for your article and added them all to this same deck.</strong></li></ul><hr><h2><strong>Step 2: Go to the Thought Saver page for your new deck</strong></h2><p>You\u2019re ready to take this step once you've created all the flashcards for your article and added them to the same deck.</p><p><strong>(i) Navigate to the page for your deck by clicking the name of your deck on one of your flashcards:</strong></p><figure class=\"image image_resized\" style=\"width:70.65%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f478d4f145cf3574194ae2257010b72de513193169fa1a7e.png/w_1160 1160w\"></figure><p>&nbsp;</p><hr><h2><strong>Step 3: Click the \"Share\" button for that deck and click &nbsp;\"Create Link\" within the share window&nbsp;</strong></h2><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:73.94%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/3b294467d983515362df496525ea7623693b88768340ba35.png/w_1414 1414w\"></figure><p><br>&nbsp;</p><figure class=\"image image_resized\" style=\"width:76.41%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/5589162e63af29254e662b7f54bc560d3f549b64bf27cc03.png/w_1332 1332w\"></figure><hr><h2><strong>Step 4: Select which cards from your deck you would like to appear in the quiz for your (first) embedded widget&nbsp;</strong></h2><p>If you're embedding multiple widgets in your article, we\u2019ll assume that you want to have each widget show different cards (as opposed to certain cards from your deck being repeated in more than one widget).&nbsp;</p><p><strong>(i) Enter the appropriate \u2018starting card number\u2019 and \u2018ending card number\u2019</strong> (based on how you sorted the cards in this deck previously). So for instance, if the starting card number is 3 and the ending card number is 7, that quiz widget will quiz the reader on cards 3 through 7.</p><p><br>&nbsp;</p><figure class=\"image image_resized\" style=\"width:75.09%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bcd9bab764a0f61f8f19dede3f31878792c15569f1b5ff99.png/w_1400 1400w\"></figure><p>&nbsp;</p><p><strong>(ii) Click \u201cCopy\u201d to copy the embed source URL to your clipboard:</strong><br>&nbsp;</p><figure class=\"image image_resized\" style=\"width:75.68%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a9d3b239dab4ea552352a669040b349abb544690fdedcb19.png/w_1404 1404w\"></figure><p><strong>IMPORTANT NOTE: you\u2019ll need to have this URL copied to your clipboard for the steps below.</strong></p><p>Example of how to spread the cards from your deck over multiple quizzes:&nbsp;</p><ul><li>You might choose to put the first card through the fifth card [cards 1\u20135] from your deck in the first flashcard quiz of your post</li><li>And then in the next flashcard quiz, you might include cards 6\u201310, etc., etc.)</li></ul><hr><h2><strong>Step 5: Create a new post then click \"Edit Block\" &nbsp;within your post</strong></h2><p>If you\u2019re not logged in to your EA Forum account, or if you do not yet have an account, <a href=\"https://forum.effectivealtruism.org/\"><u>log in or create an account first</u></a>.&nbsp;</p><p>Once you're logged in, open the post you are working on, or create your new post.</p><p>When you've reached a point in your post when you'd like to embed a Thought Saver flashcard quiz widget, click the \"Edit Block\" button to the left of the current line:&nbsp;<br>&nbsp;</p><figure class=\"image image_resized\" style=\"width:85.25%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/409478d658dda32c5160bb777355bf7011bff8f9ba374b5a.png/w_1420 1420w\"></figure><hr><h2><strong>Step 6: Paste the embed URL you copied from Thought Saver, and click Save!&nbsp;</strong></h2><p>Now you've successfully embedded a Thought Saver flashcard quiz into your EA Forum post!&nbsp;</p><figure class=\"image image_resized\" style=\"width:78.35%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/006078d81b3b7743cca7e563702daac633d5cd2e7d96d837.png/w_1400 1400w\"></figure><hr><p>We hope you enjoy this functionality! We'd love to hear your feedback on it and on <a href=\"https://www.thoughtsaver.com/\">Thought Saver</a> more generally! Please give us feedback by commenting below or by clicking the feedback button in the upper right-hand corner of the Thought Saver app.</p><p>If you're interested in how to write great flashcards, I'd recommend Andy Matuschak's article <a href=\"https://andymatuschak.org/prompts/\">how to write good prompts: using spaced repetition to create&nbsp;understanding.</a> Andy and his collaborator <a href=\"https://michaelnielsen.org/\">Michael Nielsen</a> have been the pioneers in this space of embedding flashcards in essays. I highly recommend their essay <a href=\"https://quantum.country/qcvc\">Quantum Country</a> where they introduced this medium. You may also want to check out <a href=\"https://andymatuschak.org/\">Andy's other work</a> related to this topic.</p><p>&nbsp;</p><p>Thanks!</p>", "user": {"username": "Florence"}}, {"_id": "yeDRczoJuJZMbJvrq", "title": "Assessing SERI/CHERI/CERI summer program impact by surveying fellows", "postedAt": "2022-09-26T15:29:44.089Z", "htmlBody": "<p>The three largest existential risk initiatives (ERIs) are SERI (Stanford), CHERI (Switzerland), and CERI (Cambridge). All three organized a paid summer research fellowship/program where fellows/participants are matched with mentors and do x-risk relevant research for 8 (CHERI) or 10 (SERI/CERI) weeks.</p><p>ERI summer programs are among the most-publicized and resource-intensive projects aimed at helping people get started on x-risk careers, so information about the impact they have is valuable. The existence of three organizations running a similar program but with some variation in strategy and implementation also creates an opportunity to run a natural experiment on what works and what doesn't for this type of program.</p><p>All ERIs do their own impact assessments (and are assessed by Open Philanthropy when they apply for funding), and ran their own feedback surveys for their fellows in addition to the Joint ERI Survey (JERIS). The purpose of JERIS is restricted to assessing fellows' experience and own reflections, but it does so by asking fellows in all programs the same questions for comparability.</p><p>There are three categories of people this post is most useful for:</p><ol><li>ERI organizers who want to understand and improve their programs.</li><li>Potential future ERI fellows who want to get an idea of what past ERI fellows thought of the program.</li><li>Grant-makers and entrepreneurs trying to get a sense of what types of projects are valuable and in what ways.</li></ol><h1>Highlights</h1><ul><li><strong>If ERI fellows had not been accepted into ERI programs</strong>, the most likely things they would've done instead are: a non-EA/x-risk internship, tried to do research and/or upskilling on their own, done nothing career-related during the summer, or done a non-x-risk EA job.</li><li>The <strong>top next career options</strong> ERI fellows are considering are research roles (including many specifically planning to do a PhD, but many also not having decided on specific area of research), technical AI alignment, and EA/x-risk community-building.</li><li>Fellows' estimates of<strong> the probability they pursue an x-risk career start out high (~0.8) and do not measurably increase during the program.</strong></li><li>Fellows' estimate of how <strong>comfortable they would be pursuing a research project remains effectively constant. </strong>Many start out very comfortable with research. A few decline.</li><li>Fellows think they've gotten<strong> roughly as much research skill gain </strong>as they would have from<strong> a more established research internship in academia.</strong></li><li>Fellows found their <strong>mentors</strong> to be <strong>friendly and generally useful,</strong> though the latter may have a two-humped distribution.</li><li>There was <strong>wide variation in where project ideas came from</strong> (mentors, fellows, or other sources).</li><li><strong>Networking,</strong> <strong>learning to do research,</strong> and <strong>becoming a stronger candidate for academic (but not industry) jobs</strong> top the list of <strong>what participants found most valuable</strong> about the programs.</li><li><strong>Fellows generally really enjoyed the program.</strong></li><li><strong>&nbsp;Fully remote fellows felt significantly less part </strong>of the same community/team as the other fellows.<strong> </strong>Partly and fully<strong> in-person fellows </strong>had<strong> </strong>comparable<strong> (high) feelings of belonging to the same community.</strong></li><li><strong>Remote fellows are at a significant disadvantage in finding people who might help them with their career </strong>(on average, they leave their ERI program comfortable asking 4 people for a career-related favor, compared to ~10 for fully or partly in-person fellows).</li><li><strong>Remote fellows plan to maintain contact with fewer other fellows than partly or fully in-person fellows </strong>(2 vs ~5).</li><li><strong>Being partly in-person provides most of the benefits of being fully in-person.</strong></li><li><strong>Women are underrepresented, </strong>but seem<strong> to enjoy and feel part of the program</strong>,<strong> </strong>and<strong> feel comfortable asking as many others for career favors</strong>, to the same extent as men.</li><li><strong>Many fellows want to work in teams, </strong>or have neutral/mixed feelings about team work. A lower number actively think working individually is ideal.<strong> No fellow who worked in a team thinks they would've been more productive or had more fun if they worked alone.</strong></li></ul><h1>Program features</h1><p>The information below for each ERI was provided by someone involved in running that ERI's summer program. Some of it might be out of date, as it reflected plans at the start of the summer.</p><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>SERI</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>CHERI</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>CERI</strong></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Start</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2022-06-13</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2022-07-04</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2022-07-04</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>End</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2022-08-19</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2022-08-28</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">2022-09-09</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Duration</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">10 weeks</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">8 weeks</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">10 weeks</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>In-person component?</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>11 in-person, 17 remote</p><p>All in-person: 2022-07-25 to 2022-07-31</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">In-person at least: 1 week (2022-07-04 to 2022-07-08), 1 weekend<br>(2022-08-26 to<br>2022-08-28), optional 2 coworkathons, a small group is organizing an in-person stay together</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Fellows live in the same place (Emmanuel College, Cambridge) and work in-person at the same office.&nbsp;</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Applications</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~325</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~80</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">~650</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Advertising methods</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">EA Forum, EAG London, EAGx Boston, individual outreach&nbsp;</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">EA slacks<br>Personal Messages<br>FB, SERI conference Feb.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">EA Forum, LinkedIn/FB advertising (~\u00a3500), EA Slack groups, etc.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Participants</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">33 (12 in-person, 21 remote)</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">21</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">24</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Application process outline</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>Written short-answer application, no project proposal&nbsp;</p><p>Assessment by 1 program coordinator and cause area manager</p><p>Interview with cause area manager</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">First round:<br>a) one-page project or<br>b) five potential questions<br><br>Second round:<br>a) interview<br>b) survey about reasoning quality, incl. small essay</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Initial application including long (~2h) essay component, blindly assessed by two people each (cause area lead + another person), followed by interview round</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Mentor matching / general on-boarding outline</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">High variance between participants, done by cause area managers, sometimes based on project preferences of fellows</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Mentor matching done by team, based on preferences of the students &amp; our network</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">Mentor matching happened before the start of the programme. It was led by our cause area leads, and was a very personalised process for each fellow, depending on what project they wanted to pursue and what type of mentorship would have been most useful for them in the long run.</td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><strong>Participant salary/stipend + other perks</strong></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\">$7,500<br><br>Also travel and accommodation for in-person fellows.</td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>CHF 6000, ~$6000</p><p>Also travel/accommodation for in-person event</p></td><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p>\u00a35600, ~$7060</p><p>Also travel, accommodation, and food</p></td></tr></tbody></table></figure><h2>&nbsp;</h2><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><h1>Survey method</h1><p>Representatives from each ERI brainstormed hypotheses to test and questions to ask, and then had a meeting where we finalized the set of survey questions. There were three distinct surveys, to be completed by fellows at the beginning, middle, and end of the summer program respectively.</p><h3>Anonymization and resulting issues</h3><p>We thought fellows would be concerned about being de-anonymized and therefore hesitant to share candid feedback, especially as in many cases the combination of organization, cause area, and gender was sufficient to pin down a fellow, especially if the cause area was a less-popular one or the gender was female. The solution we ended up with was having a question asking fellows to generate a unique anonymous &nbsp;identifier for themselves by combining personal information in a hard-to-reverse way (or by generating a random number for themselves and keeping it for later, which a few of them actually took us up on) (also this generation method turned out to be insufficiently high entropy, as there were two collisions, resulting in 4 sets of answers being discarded). Each question asked for this unique identifier, but <strong>only the last survey asked for organization, cause area, and gender</strong>.</p><p>Therefore, <strong>the answers of a fellow (in any survey) could only be linked to an organization if that same fellow had also completed the last survey</strong> (and also correctly re-generated their unique identifier, or, in a few cases, correctly stored and retrieved the one they made previously).</p><p>Unfortunately, most fellows did not complete all three surveys. 50 people completed the last survey, but of them only 29 could be linked with a unique identifier in the middle survey, and 25 with an answer in the first survey.</p><p>We were not optimistic about fellows filling in feedback surveys; each program made time for them, and in some cases there were multiple reminders. It seems we were not quite pessimistic enough, however.</p><h1>Results</h1><p>First, it is helpful to keep in mind the breakdown of (survey-answering) fellows by organization and whether they were in-person or not:</p><figure class=\"image image_resized\" style=\"width:71.86%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_134 134w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_214 214w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_294 294w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_374 374w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_454 454w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_534 534w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_614 614w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/aa8da05699e36cdf5cededfac0405480989ddc0493951ea4.png/w_694 694w\"></figure><p>&nbsp;</p><h2>Counterfactual options and career plans at start</h2><p>The first survey included a question \"What would you have done if you had not been accepted into the programme?\". I went through the answers and identified the categories of thing listed. In the graph below, if a respondent gave only one answer (e.g. something that fell under the category of \"other EA job\"), that was counted as +1 to that category. If they listed&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>, an additional score of &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"+1/n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>&nbsp;was added to all the categories of thing they mentioned.</p><figure class=\"image image_resized\" style=\"width:85.78%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/bef2025ace0300175af3962f08a3cb24868357cb923f9043.png/w_839 839w\"></figure><p>Some notes on categories:</p><ul><li>\"other internship\" excludes things that fall under other categories, like other ERI, other EA job, and other x-risk job</li><li>\"similar but independently\" means the fellow planned to conduct x-risk relevant research on their own but without an organization to do it under and with any mentorship (rare) &nbsp;self-organized. Several people giving answers under this category mentioned FTX regrantor grants.</li><li>\"studying\" encompasses x-risk -relevant up-skilling as well as studying for e.g. university courses and specifically implies no mention of intent to do original research.</li><li>&nbsp;\"nothing\" means \"nothing related to careers or x-risk\". It includes holidays, travel, rest, and hobbies.</li><li>\"other EA job\" excludes the more specific cases of \"other ERI\" and \"other x-risk job\".</li><li>\"existing research\" means the fellow was planning to continue a research project they were already working on.</li><li>\"tech job\" and \"internship\" exclude x-risk -relevant or within-EA versions.</li><li>\"other research\" excludes EA or x-risk relevant research.</li></ul><p>Another \"question\" on the first survey said \"Briefly outline the career plans/options that you are considering.\" Again, the categorization was based on my classification of fellow answers into (potentially multiple) categories rather than asking fellows to select from options, and fellows who gave many answers had their \"vote\" split evenly as described above.</p><figure class=\"image image_resized\" style=\"width:80.17%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/7e58e1a03fb00dbc9fe407a056c1f5912e3cec1c2594423c.png/w_817 817w\"></figure><p>The<strong> top next career options </strong>ERI fellows are considering are<strong> research roles (</strong>including many specifically planning to do a PhD, but many also not having decided on specific area of research<strong>), technical AI alignment, and EA/x-risk community-building.</strong></p><p>SERI seem less likely to note general interest in some unspecified research career, and more likely to note more specific things instead.</p><p>Some notes on categories:</p><ul><li>\"broad research\" and \"broad policy\" were used when fellows mentioned research or policy careers, and in some cases indicated interest in x-risk -relevant things but without mentioning anything more specific than \"x-risk\".</li><li>\"technical alignment\" means technical AI alignment.</li><li>\"PhD\" was included whenever the fellow specifically indicated doing a PhD as a next step.</li><li>\"community-building\", \"grant-making\", and \"operations\" refer to x-risk / EA versions of those things.</li></ul><p>This question was not repeated at the end to see change. It probably should've been. However, the next section hints change might be surprisingly low.</p><h2>Self-estimated probability of pursuing an x-risk career</h2><p>The exact phrasing of the question was:</p><blockquote><p>What do you think is the probability (as a number between 0 and 1) that you will spend at least 5 years of your life working in a job that is closely related to x-risk mitigation?</p></blockquote><p>The \"5 years of your life\" part was to anchor people to thinking about what they might concretely spend their working time on rather than the more abstract and less-defined concept of having a \"career\".</p><p>The graphs below show the distribution of answers in the beginning and end survey:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/87759b198d9c6002f248d1d7c0b5878fabe2237c3199ec06.png/w_1156 1156w\"></figure><p>Linking people using the anonymous unique identifier, we can also see the trends for individual participants (averages for each program shown as dots):</p><figure class=\"image image_resized\" style=\"width:67.28%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_148 148w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_228 228w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_308 308w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_388 388w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_468 468w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_548 548w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_628 628w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cf2bd623ef7d1e3783fddf5533a43d6dd9b0a41a5efa57d2.png/w_708 708w\"></figure><p>The average ERI summer fellow is already fairly set on an x-risk career (assuming their probability estimates are at least somewhat well-calibrated). Somewhat strikingly, <strong>the probability of pursuing an x-risk career does not increase</strong> throughout the program. Perhaps this is because of ceiling effects; probabilities can't go much higher than the starting 0.8 after all.</p><p>The CHERI fellows seem to be both less committed to x-risk careers overall (just about one standard deviation below CERI/SERI fellows), and to see larger changes over the course of the program. Note how four CHERI fellows saw significant drops in their assigned probability, while the greatest increase also came from a CHERI fellow.</p><p>One potentially relevant feature of the CHERI fellowship was a significantly smaller number of applicants (~80, compared to 325 for SERI and ~650 for CERI). Note that, apart from Facebook advertising, CHERI did use fairly targeted advertising methods, like EA Slacks, personal messages, and the 2022 SERI conference. Therefore it is possible that either a large pool is needed to get x-risk committed applicants even when using targeted advertising channels, CHERI is generally interfaced to less-committed parts of EA space, or SERI and CERI weighted (proxies for) commitment to x-risk careers more heavily.</p><p>The relatively low changes in x-risk career commitment suggest that ERI programs (with the possible exception of CHERI) do not generally cause fellows to update very much on their commitment to an x-risk career.</p><h2>Cause areas</h2><h3>Cause area ranking</h3><p>At the beginning and end of the survey, we asked fellows to \"Please rank what you think are the greatest existential risks to human civilisation (top few are enough).\" I went through the free-form lists, randomizing in case of ties and sometimes merging very similar categories (in hindsight, this should not have been a free-form text question).</p><p>The top-ranked cause area at the beginning and end was:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/227500637af960844527b92db8766ab06808996b38a8440c.png/w_1018 1018w\"></figure><p>AI clearly dominates, and climate falls over the course of the program.</p><p>We can get a sense of overall prioritization by scoring the&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>th ranked cause area as&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"1/n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>points. At the beginning and end of the survey respectively, this suggests the following ranking:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/0552e324c1de95fba66484c43c7978b14503bc8e4207798c.png/w_1115 1115w\"></figure><p>The same pattern is present as in the top-only graph, though less extreme.</p><p>The dominance of the \"big four\" (AI, bio, nuclear, climate) might be influenced by the way in which these were the main cause areas identified by the programs.</p><p>&nbsp;</p><h3>Participants by cause area</h3><p>The number of participants working on each cause area, split by organization, was as follows:</p><figure class=\"image image_resized\" style=\"width:70.72%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_99 99w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_179 179w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_259 259w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_339 339w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_419 419w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_499 499w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_579 579w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/48cff4565dafdf223830d4423b1a6d15004b090a7f62cadc.png/w_659 659w\"></figure><p>Note that since the survey was not taken by everyone, this is missing some people.</p><h2>Perceived skills and skill changes</h2><p>We asked fellows to rate on a 1-10 scale \"How comfortable would you feel undertaking a significant research project?\" at both the beginning and the end of the program. The definition of \"significant research project\" given was</p><blockquote><p>\"Significant research project\" means something of at least one year duration, where you have access to a mentor but a need to generate significant ideas/research on your own. Examples include research-oriented Master's programs, PhDs, think-tank roles, and long research internships.</p></blockquote><figure class=\"image image_resized\" style=\"width:76.8%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_80 80w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b7b0ca2b9983a2faf049dd28504dc35bc4b4f8f984ec6757.png/w_720 720w\"></figure><p><strong>Comfort with pursuing research projects remains effectively constant. </strong>Interestingly, there seem to be two contrasting patterns: many fellows increasing slightly, and a few decreasing significantly. It seems that a fair number of participants start and stay confident, most improve a bit, and some perhaps have a rude awakening to the realities of research. Major declines are causes for concern, and may suggest ERI programs should be mindful about the possibility of fellows being demotivated or burned out by struggling.</p><p>At the end, to see if fellows think they missed out on research up-skilling by not doing a more traditional research internship, we also asked \"How much do you think the programme helped you develop your research skills, on a scale of 1-10 where 5 is the level of research skill gain you'd expect from a comparable-length research internship in academia?\".</p><figure class=\"image image_resized\" style=\"width:69.7%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png/w_170 170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png/w_250 250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png/w_410 410w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png/w_490 490w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/751e7a3a79009efa5132dc0487466f9448a8235fcc0cca1d.png/w_570 570w\"></figure><p>This gives a nicely shaped distribution that confirms fellows in all programs think they've gotten roughly as much value as they would from a more established research internship in academia. Of course, it is unclear if fellow perceptions are accurate - it would be interesting to see this broken down by whether or not the fellows have prior research experience.</p><p>We also asked the comfort question but for entrepreneurial projects. ERIs are research programs, so asking about another thing as well serves as a control. It also provides some information about how strongly ERI participants think their skills lean to research specifically, and how comfortable they'd be with running projects. The definition given was</p><blockquote><p>\"Significant entrepreneurial project\" includes things like starting an organisation in the Effective Altruism space or another startup/non-profit or running a major event/camp (e.g. conference / retreat / educational program)&nbsp;</p></blockquote><figure class=\"image image_resized\" style=\"width:81.18%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_138 138w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_218 218w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_298 298w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_378 378w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_458 458w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_538 538w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_618 618w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_698 698w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6e7e25e0c2ee32df55ba70d15d224ec228916d2712c7d485.png/w_778 778w\"></figure><p>As expected, fellows admitted for interest and skill at research are more comfortable with research than entrepreneurship. Interestingly, the increase in perceived comfort with entrepreneurial projects is larger for every org than that for research. Perhaps the (mostly young) fellows generally just get slightly more comfortable with every type of thing as they gain experience.</p><p>However, this is additional evidence that <strong>ERI programs are not increasing fellows' self-perceived comfort with research</strong> any more than they increase fellows' comfort with anything. It would be interesting to see if mentors of fellows think they have improved overall; it may be that changes in self-perception and actual skill don't correlate very much.</p><p>We also asked fellows in the final survey the following question:</p><blockquote><p>Do you think you are better at critical thinking than at the start (e.g. more likely to notice fallacies/biases in yourself, better calibrated at estimating probabilities, more able to think quantitatively, more able to judge whether a claim has substance)? Feel free to comment</p></blockquote><p>I categorized the responses into three categories:</p><figure class=\"image image_resized\" style=\"width:43.48%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/586f28150fe4727227a87fa7e893a7f8fb5d28a274bf1be3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/586f28150fe4727227a87fa7e893a7f8fb5d28a274bf1be3.png/w_80 80w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/586f28150fe4727227a87fa7e893a7f8fb5d28a274bf1be3.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/586f28150fe4727227a87fa7e893a7f8fb5d28a274bf1be3.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/586f28150fe4727227a87fa7e893a7f8fb5d28a274bf1be3.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/586f28150fe4727227a87fa7e893a7f8fb5d28a274bf1be3.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/586f28150fe4727227a87fa7e893a7f8fb5d28a274bf1be3.png/w_480 480w\"></figure><p>Some cheeky fellows commented something like \"no, but mostly because they were already at a high starting point\".</p><h2>Mentors and project ideas</h2><p>\"How useful do you think your mentor was?\", where 1 = \"not very useful\" (phrasing this euphemistically was a mistake and might have influenced the results) and 10 = \"extremely useful\", and \"How friendly/pleasant did you find your interactions with your mentor?\", where 1 = \"very unfriendly and unpleasant\" and 10 = \"very friendly and pleasant\":</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_100 100w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_400 400w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_500 500w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/b2de188bf59b0e40892ac851830c450ffde064e538ff5c36.png/w_996 996w\"></figure><p>Mentor usefulness shows an interesting two-humped pattern. This may correspond to an actual two-humped distribution, or to whether people read the misleadingly-euphemistic label for what \"1\" means or not. However, in general mentors do seem to be useful. Practically everyone also found their mentor interactions pleasant and friendly, though outliers exist.</p><p>SERI and CERI are over-represented among fellows giving a 10/10 for their mentor on both categories. The average for CHERI also ends up being slightly weaker in both categories, though the variance is also high.&nbsp;</p><h3>Project idea source</h3><p>In the second survey, fellows were asked:</p><blockquote><p>What percentage of the project idea would you attribute to yourself, your mentor, and others? Answer as e.g. the ratio 30:60:10 if you think you contributed 30%, your mentor 60%, and others 10%</p></blockquote><p>The following graph shows fellows' self-perceived contribution to the idea on the x-axis, mentor contribution on the y-axis, and the contribution of others as distance from the point to the grey line:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20cb39ee97786c066763c3f88d707148e7e0da229b6ce0d3.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20cb39ee97786c066763c3f88d707148e7e0da229b6ce0d3.png/w_119 119w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20cb39ee97786c066763c3f88d707148e7e0da229b6ce0d3.png/w_199 199w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20cb39ee97786c066763c3f88d707148e7e0da229b6ce0d3.png/w_279 279w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20cb39ee97786c066763c3f88d707148e7e0da229b6ce0d3.png/w_359 359w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20cb39ee97786c066763c3f88d707148e7e0da229b6ce0d3.png/w_439 439w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/20cb39ee97786c066763c3f88d707148e7e0da229b6ce0d3.png/w_519 519w\"></figure><p>Essentially every SERI fellow attributes less than 50% of the idea to themselves, whereas most CERI fellows attribute more than 50% to themselves. There also exists a clear minority that got their project idea mostly from a source other than themselves or their mentor.</p><h2>Greatest value adds</h2><p>Fellows were asked \"What do you think were the most valuable parts of the programme?\" and given a series of options to select from (with an \"add other\" button available). Answers were split (as above) so that a fellow ticking&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>&nbsp;boxes was counted as having given&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"+1/n\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span></span></span>&nbsp;score to each of them. The scores for each answer were as follows:&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/fd305151dacbcf2781454e17d1581012beb6670d95eb9569.png/w_1197 1197w\"></figure><p><strong>Networking, learning to do research,</strong> and <strong>becoming a stronger candidate for academic (but not industry) jobs</strong> top the list of <strong>what participants found most valuable.</strong></p><h2>Enjoyment of program</h2><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/43db552fa7d5965ac182c8c7123d8bc55bca6b6c8ca643ae.png/w_1004 1004w\"></figure><p>Recall that CERI was almost entirely fully in-person, CHERI almost entirely partly in-person, and SERI had an almost even split between all levels.</p><p>The big results from the above are:</p><ul><li>Fellows in general enjoyed the programs a lot; 8-9 out of 10 is a high level.</li><li>CERI was just about one standard deviation and 1 point on the scale higher than the other programs.</li><li>Fully in-person was similarly better compared to the other options, especially fully remote. This may have been the driving factor behind the above point.</li></ul><p>The above graph shows mostly the same information, but allows us to look at individual trajectories (note that many of the lines from survey 2 to survey 3 are overlapping; you can roughly guess number from opacity level):</p><figure class=\"image image_resized\" style=\"width:66.9%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png/w_121 121w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png/w_201 201w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png/w_281 281w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png/w_361 361w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png/w_441 441w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png/w_521 521w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f6c0cfb63c6a23b9a479f3192623d6db73712afe4b007c06.png/w_601 601w\"></figure><p>This mostly reveals that there may be a slight trend of enjoyment being higher at the end of the program, and confirming the (small) differences between the ERIs existed also in the middle of the program.</p><h2>Connections and sense of community</h2><p>Fellows were asked \"How much did you feel part of the same team/community/program with the other fellows in your cohort?\" where 1 = \"Not at all; felt a complete outsider\" and 10 = \"Extremely; felt like I had found my people\". Below are the results, split both by org and whether or not the fellow worked in-person or not.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_130 130w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_260 260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_390 390w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_650 650w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_780 780w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_910 910w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_1040 1040w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_1170 1170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/57c895a841ab8eb2eaa46edcebe2e65b4bf49a16eed7be3f.png/w_1284 1284w\"></figure><p><strong>Fully remote fellows</strong> felt like significantly <strong>less part of the same community/team</strong> as the other fellows. <strong>Partly and fully in-person fellows had comparable (high) averages</strong> and distributions.</p><p>One of the theories CHERI wanted to test with their program this summer was whether being partly in-person gets most of the advantage of being fully in-person. This seems like moderate evidence in favor of this being true for fellow sense of belonging.</p><p>Two other questions ran:</p><blockquote><p>How many of the participants in your programme would you feel comfortable asking for a career-related favor? (e.g. an introduction to one of their contacts, their advice on applying to an organisation, proofreading an EA Forum post)</p></blockquote><blockquote><p>With how many fellows from the programme do you expect to maintain contact with after the end of the fellowship?</p></blockquote><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e165dc19187c5adce9a20388f4886766885cfaa3696e84a6.png/w_1311 1311w\"></figure><p>The <strong>disadvantage of fully remote fellows extended to finding contacts who they might ask for help with their careers, and maintaining contact with fewer fellows. Partly and fully in-person fellows had similar experiences on both fronts.</strong></p><h2>Gender</h2><p>The graphs below give the gender breakdown by organization and cause area:</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_320 320w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_640 640w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_800 800w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_1280 1280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_1440 1440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/01ef7975b2ad088b6d92d3a040687d6e06f809d1dcf45145.png/w_1560 1560w\"></figure><p><strong>Women are underrepresented in ERI programs</strong> (they make up 14-30% depending on ERI).</p><p>This seems especially acute in technical AI safety. CERI seems to have done the best in recruiting women and SERI the worst. However the numbers are low and not everyone filled in the survey, so the last two should be interpreted cautiously.</p><p>Thankfully, <strong>there do not seem to be gender-based differences in enjoyment, feeling of belonging, or number of fellows they could ask for a career-related favor</strong>. The latter two are graphed below:</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12c468d28b1d14d5af35bd35ba35df4444c4d034a0ac53f2.png/w_1330 1330w\">&nbsp;</p><h2>Teams</h2><p>A few fellows worked in teams:</p><figure class=\"image image_resized\" style=\"width:62.65%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png/w_123 123w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png/w_203 203w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png/w_283 283w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png/w_363 363w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png/w_443 443w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png/w_523 523w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/27dbea292574a010fc96a340c12fa82514be1285845bde15.png/w_603 603w\"></figure><p>Quite a few would like to: (the colors are by answer to whether or not the fellow's project was a team project)</p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_150 150w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_300 300w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_750 750w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_900 900w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_1050 1050w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_1200 1200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6cfd0cda87807151d5b5eb4ba964e6be716b068d069526e7.png/w_1471 1471w\"></figure><p>The exact questions asked in the above graph were:</p><blockquote><p>How do you feel about the effect of working individually vs in a team on your [research output / enjoyment of the program]? (compare with what you think the counterfactual scenario where the opposite was the case held)</p></blockquote><p><strong>Many fellows think working in a team would increase their research output and enjoyment of the program. </strong>Many had mixed or no strong preferences on the matter. <strong>No one who worked on a team thought they would have been more productive or had more fun had they worked individually.</strong></p><p>It seems<strong> very worthwhile to have more fellows working in teams.</strong></p><h2>Organizational problems</h2><p>One way ERIs could fail is if fellows had to spend hours sorting out organizational messes because their ERI was in some way incompetent at operations. To see if this was the case, we asked fellows:</p><blockquote><p>How many hours do you estimate you lost to organisational problems (i.e. instances where your work was affected because of issues coming from the organisation, rather than your mentor / being stuck yourself on ideas / etc.)?</p></blockquote><p>The results are plotted here (note the log scale):</p><figure class=\"image image_resized\" style=\"width:55.26%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/433f0ec0ab2e98ff216097b2b8eb7ef74b322d7f4dc967e7.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/433f0ec0ab2e98ff216097b2b8eb7ef74b322d7f4dc967e7.png/w_144 144w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/433f0ec0ab2e98ff216097b2b8eb7ef74b322d7f4dc967e7.png/w_224 224w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/433f0ec0ab2e98ff216097b2b8eb7ef74b322d7f4dc967e7.png/w_304 304w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/433f0ec0ab2e98ff216097b2b8eb7ef74b322d7f4dc967e7.png/w_384 384w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/433f0ec0ab2e98ff216097b2b8eb7ef74b322d7f4dc967e7.png/w_464 464w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/433f0ec0ab2e98ff216097b2b8eb7ef74b322d7f4dc967e7.png/w_544 544w\"></figure><p>We see that there are a few extreme outliers. One CERI fellow claimed they had lost 200 hours to such organisational problems (interestingly, though they though their research skill gain was 1/10, they rated their overall enjoyment of the program as 7/10). The three CHERI fellows who gave answers in the 30-100 hour range all also seemed to otherwise enjoy the program. It is possible that the question was not clear enough and some read it as \"how many hours could you have saved it if the organisation and circumstances were prefect\", or missed the specification of \"organisational\" as \"issues coming from the organisation\" (this was admittedly badly phrased).</p><h2>Salary</h2><p><strong>Every fellow except two were happy with the salary, </strong>and many commented that it was more than sufficient, or more than what they expected, or \"outstanding\".</p><p>Fellows were paid the equivalent of 6-7k$. CHERI greatly increased their initial salary plan to match SERI and CERI. CERI paid fellows much more than last year.</p><h2>Planning fallacy?</h2><p>The first survey asked fellows to \"What is the probability, as a number between 0 and 1, that you would assign to you finishing your project during the programme?\" where we defined \"Finishing your project\" as \"[...] completing the success criteria specified at the start if you have any, and otherwise completing what feels to you like a complete piece of work that you are happy with.\". The last survey asked fellows whether they had finished, and I classified each free-form text answer as either \"yes\", \"basically\" (mostly finished but with some non-trivial dangling threads), or \"no\".</p><figure class=\"image image_resized\" style=\"width:74.49%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_180 180w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_450 450w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_630 630w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_806 806w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/a5cadf6fef0328702ed3e45cc6fd526ecbf44054a424a289.png/w_810 810w\"></figure><h1>Conflict of interest note</h1><p>I've been involved with CERI since September 2021, including helping design the CERI SRF application process. However, I was not involved in day-to-day running of the SRF program.</p><h1>Acknowledgements</h1><p>The idea for JERIS was born during a 1-on-1 meeting with CHERI founder Naomi Nederlof during EAG London 2022.</p><p>Thanks to Tobias Ha\u0308berli, Sage Andrus Bergerson, and Herbie Bradley for help with choosing questions and creating the surveys. Thanks to Tobias Ha\u0308berli, Sage Andrus Bergerson, and Hannah Erlebach for organizing time in their respective programs for fellows to fill in the surveys, and suffering through my multiple requests to further prod fellows to fill in the survey.</p><p>Thanks to the 68 distinct ERI fellows who filled in at least one survey, and especially to the 20 feedback form warriors who completed all three surveys. We know you've had to fill in a lot of forms, and we appreciate you doing your part to appease Azagorg the Ravenous, Patron Deity of Feedback Forms.</p><figure class=\"image image_resized\" style=\"width:70.56%\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/dbda37b1b6e3b44574715c720266931f6965a51c13714627.png/w_1080 1080w\"><figcaption><i>LONG LIVE FEEDBACK FORMS! &nbsp;LONG LIVE AZAGORG!</i></figcaption></figure>", "user": {"username": "LRudL"}}, {"_id": "e2upqGf6q4CiudLMu", "title": "AI Risk Intro 2: Solving The Problem", "postedAt": "2022-09-24T09:33:04.853Z", "htmlBody": "<p>This marks the second half of our overview of the AI alignment problem. In <a href=\"https://forum.effectivealtruism.org/posts/QzrgMhTMoLe5mEas8/ai-risk-intro-1-advanced-ai-might-be-very-bad\">the first half</a>, we outlined the case for misaligned AI as a significant risk to humanity, first by looking at past progress in machine learning and extrapolating to what the future could bring, and second by discussing the theoretical arguments which underpin many of these concerns. In this second half, we focus on possible solutions to the alignment problem that people are currently working on. We will paint a picture of the current field of technical AI alignment, explaining where the major organisations fit into the larger picture and what the theory of change behind their work is. Finally, we will conclude the sequence with a call to action, by discussing the case for working on AI alignment, and some suggestions on how you can get started.</p><p><i>Note - for people with more context about the field (e.g. have done AGISF) we expect </i><a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\"><i>Thomas Larsen's post</i></a><i> to be a much better summary, and </i><a href=\"https://www.lesswrong.com/posts/9TWReSDKyshfA66sz/alignment-org-cheat-sheet#comments\"><i>this post</i></a><i> might be better if you are looking for something brief. Our intended audience is someone relatively unfamiliar with the AI safety field, and is looking for a taste of the kinds of problems which are studied in the field and the solution approaches taken. We also don't expect this sampling to be representative of the number of people working on each problem - again, see Thomas' post for something which accomplishes this.</i></p><hr><h1><strong>Introduction: A Pre-Paradigmatic Field</strong></h1><blockquote><p><i>Definition (<strong>pre-paradigmatic</strong>): a science at an early stage of development, before it has established a consensus about the true nature of the subject matter and how to approach it.</i></p></blockquote><p>AI alignment is a strange field. Unlike other fields which study potential risks to the future of humanity (e.g. nuclear war or climate change), there is almost no precedent for the kinds of risks we care about. Additionally, because of the nature of the threat, failing to get alignment right on the first try might be fatal. As Paul Christiano (a well-known AI safety researcher) recently wrote:</p><blockquote><p><i>Humanity usually solves technical problems by&nbsp;<strong>iterating and fixing failures</strong>; we often resolve tough methodological disagreements very slowly by seeing what actually works and having our failures thrown in our face. But it will probably be possible to build valuable AI products without solving alignment, and so&nbsp;<strong>reality won\u2019t \u201cforce us\u201d to solve alignment until it\u2019s too late</strong>. This seems like a case where we will have to be&nbsp;<strong>unusually reliant on careful reasoning rather than empirical feedback loops</strong> for some of the highest-level questions.</i></p></blockquote><p>For these reasons, the field of AI alignment lacks a consensus on how the problem should be tackled, or what the most important parts of the problem even are. This is why there is a lot of variety in the approaches we present in this post.</p><h1><strong>Decomposing the research landscape</strong></h1><figure class=\"image image_resized\" style=\"width:50.19%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/t89axasnmda7frqhjqgr.png\"><figcaption><i>sorting papers and books in a majestic gothic library</i></figcaption></figure><p>There are lots of different ways you could divide up the space of approaches to solving the problem of aligning advanced AI. For instance, you could go through the history of the field and identify different movements and paradigms. Or you could place the work on a spectrum from highly theoretical maths/philosophy-type research, to highly empirical research working with cutting-edge deep learning models.</p><p>However, the most useful decomposition would be one that explains why the people who work on it believe that it will help solve the problem of AI alignment.&nbsp;</p><p>For that reason, we\u2019ll mostly be using the decomposition from&nbsp;<a href=\"https://www.lesswrong.com/s/FN5Gj4JM6Xr7F4vts/p/SQ9cZtfrzDJmw9A2m\"><u>Neel Nanda\u2019s \u201cA Bird\u2019s Eye View\u201d&nbsp;</u></a>post. The motivation behind this decomposition is to answer the high-level question of \u201cwhat is needed for AGI to go well?\u201d. The six broad classes of approaches we talk about are:</p><ol><li><strong>Addressing threat models&nbsp;</strong><br><i>We have a specific threat model in mind for how AGI might result in a very bad future for humanity, and focus our work on things we expect to help address the threat model.</i></li><li><strong>Agendas to build safe AGI&nbsp;</strong><br><i>Let\u2019s make specific plans for how to actually build safe AGI, and then try to test, implement, and understand the limitations of these plans. The emphasis is on understanding how to build AGI safely, rather than trying to do it as fast as possible.</i></li><li><strong>Robustly good approaches&nbsp;</strong><br><i>In the long-run AGI will clearly be important, but we're highly uncertain about how we'll get there and what, exactly, could go wrong. So let's do work that seems good in many possible scenarios, and doesn\u2019t rely on having a specific story in mind.</i></li><li><strong>Deconfusion</strong><br><i>Reasoning about how to align AGI involves reasoning concepts like intelligence, values, and optimisers and we\u2019re pretty confused about what these even mean. This means any work we do now is plausibly not helpful and definitely not reliable. As such, our priority should be doing some conceptual work on how to think about these concepts and what we\u2019re aiming for, and trying to become less confused.</i></li><li><strong>AI governance</strong><br><i>In addition to solving the technical alignment problem, there\u2019s the question of what policies we need to minimise risk from advanced AI systems.</i></li><li><strong>Field-building</strong><br><i>One of the most important ways we can make AI go well is by increasing the number of capable researchers doing alignment research.</i>&nbsp;</li></ol><p>It\u2019s worth noting that there is a lot of overlap between these sections. For instance, interpretability research is a great example of a robustly good approach, but it can also be done with a specific threat model in mind.</p><p>Throughout this section, we will also give small vignettes of organisations or initiatives which support AI alignment research in some form. This won\u2019t be a full picture of all approaches or organisations, instead hopefully it will serve to sketch a picture of what work in AI alignment actually looks like.</p><h2>Addressing threat models</h2><blockquote><p><i>We have a <strong>specific threat model</strong> in mind for how AGI might result in a very bad future for humanity, and focus our work on things we expect to help address the threat model.</i></p></blockquote><p>A key high-level intuition here is that having a specific threat model in mind for how AI might go badly for humanity can help keep you focused on certain hard parts of the problem. One technique that can be useful here is a version of back-casting: we start from future problems with advanced AI systems in our current model, reason about what kinds of things might solve these problems, then try and build versions of these solutions today and test them out on current problems.</p><figure class=\"image image_resized\" style=\"width:45.98%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/hew67cg5fkayjgmpseyw.png\"></figure><p>This can be seen in contrast to the approach of simply trying to fix current problems with AI systems, which might fail to connect up with the hardest parts of AI alignment.</p><figure class=\"image image_resized\" style=\"width:44.49%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/hfliqnoigkrazyrjp1te.png\"></figure><h3>Example 1: Superintelligent utility maximisers, and quantilizers</h3><figure class=\"image image_resized\" style=\"width:50.49%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/cad3vuqyonm0dte7iicv.png\"><figcaption><i>superintelligent artificial intelligence, making choices, digital art, artstation</i></figcaption></figure><p>The superintelligent utility maximiser is the oldest threat model studied by the AI alignment field. It was discussed at length by Nick Bostrom in his book&nbsp;<i>Superintelligence</i>. It assumes that we will create an AGI much more intelligent than humans, and that it will be trying to achieve some particular goal (measured by the&nbsp;<a href=\"https://www.investopedia.com/terms/e/expectedutility.asp\"><u>expected value of some utility function</u></a>). The problem with this is that attempts to maximise the value of some goal which isn\u2019t perfectly aligned with what humans want can lead to some very bad outcomes. One formalism which was proposed to address this problem is&nbsp;<a href=\"https://intelligence.org/2015/11/29/new-paper-quantilizers/\"><u>Jessica Taylor\u2019s quantilizers</u></a>. It is quite maths-heavy so we won\u2019t discuss all the details here, but the basic idea is that rather than using the expected utility maximisation framework for agents, we mix expected utility maximisation with human imitation in a clever way (to be more precise, you sample from a prior distribution which represents the actions a human would be likely to take in this scenario). The resulting agent wouldn\u2019t take catastrophic actions because part of its decision-making comes from imitating what it thinks humans would do, but it would also be able to use the expected utility maximisation to go beyond human imitation, and do things we are incapable of (which is presumably the reason we would want to build it in the first place!). However, the drawback with theoretical approaches like this is that they often bake in too many assumptions or rely on too many variables to be useful in practice. In this case, how we define the set of reasonable actions a human might perform is an important unspecified part of this framework, and so more research is required to see if the quantiliszers framework can address these problems.</p><h3>Example 2: Inner misalignment</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/wth2mbv8yzwklebmvulq.png\"><figcaption><i>robot jumping over boxes to collect a coin, videogame, digital art, artstation</i></figcaption></figure><p>We\u2019ve discussed inner misalignment in a previous section. This concept was first explicitly named in a paper called&nbsp;<a href=\"https://arxiv.org/abs/1906.01820\"><u>Risks from Learned Optimisation in Advanced ML Systems</u></a>, published in 2019. This paper defined the concept and suggested some conditions which might make it more likely to happen, but the truth is that a lot of this is still just conjecture, and there are many things we don\u2019t yet know about how unlikely this kind of misalignment is, or what we can do about it. The CoinRun example discussed earlier (and the&nbsp;<a href=\"https://www.deepmind.com/publications/objective-robustness-in-deep-reinforcement-learning\"><u>Objective Robustness</u></a> paper) came from an independent research team in 2021. This study was the first known example of inner misalignment in an AI system, showing that it was at least a theoretical possibility. They also tested certain interpretability tools on the CoinRun agent, to see whether it was possible to discover when the agent had a goal different to the one intended by the programmers. For more on interpretability, see later sections.</p><h2>Building safe AGI</h2><blockquote><p><i>Let\u2019s make specific plans for <strong>how to actually build safe AGI</strong>, and then try to test, implement, and understand the limitations of these plans. The emphasis is on understanding how to build AGI <strong>safely</strong>, rather than trying to do it as fast as possible.</i></p></blockquote><p>At some point we\u2019re going to build an AGI. Companies are already racing to do it. We better make sure that there exist some blueprints for a safe AGI (and that they\u2019re used) by the time we get to that point.</p><p>Perhaps the master list of safe AGI proposals is Evan Hubinger\u2019s&nbsp;<a href=\"https://arxiv.org/pdf/2012.07532.pdf\"><u>An Overview of 11 Proposals for Building Safe Advanced AI</u></a>.&nbsp;</p><h3>Example 1: Iterated Distillation and Amplification (IDA)</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/p9y2lzrn8xanx4hh1spj.png\"><figcaption><i>artists depection of a robot dreaming up multiple copies of itself, cascading tree, delegating, digital art, trending on artstation</i></figcaption></figure><p>\u201cIterated Distillation and Amplification\u201d (IDA) is an imposing name, but the core intuition is simple. One of the ways in which an individual human can achieve more things is by delegating tasks to others. In turn, the assistants that tasks are delegated to can be expected to become more competent at the task.</p><p>In IDA, an AI plays the role of the assistant. \u201cDistillation\u201d refers to the abilities of the human being \u201cdistilled\u201d into the AI through training, and \u201camplification\u201d refers to the human becoming more capable as they can call on more and more powerful AI assistants to help them.</p><p>A setup to train an IDA personal assistant might go like this:</p><ol><li>You have a human, say Hannah, who knows how to carry out the tasks of a personal assistant.</li><li>You have an ML model - call it Martin - that starts out knowing very little (perhaps nothing at all, or perhaps it\u2019s a pre-trained language model so it knows how to read and write English but not much else).</li><li>Hannah needs to find the answer to some questions, and she can invoke multiple copies of Martin to help her. Since Martin is quite useless at this stage, Hannah has to do even simple tasks herself, like writing routine emails. Using some interface legible to Martin, she breaks the email-writing task into subtasks like \u201cfind email address of Hu M. Anderson\u201d, \u201cselect greeting\u201d, \u201ccheck project status\u201d, \u201cmention project status\u201d, and so on.</li><li>From seeing enough examples of Hannah\u2019s own answers to the sub-questions, Martin\u2019s training loop gradually trains it to be able to answer first the simpler sub-tasks - (address is \u201chumanderson@humanmail.com\u201d, greeting is \u201cSalutations, Human Colleague!\u201d, etc.) and eventually all the sub-tasks involved in routine email-writing.</li><li>At this point, \u201cwrite a routine email\u201d becomes a task Martin can entirely carry out for Hannah. This is now a building block that can be used as a subtask in broader tasks Hannah gives out to Martin.&nbsp; Once enough tasks become tasks that Martin can carry out by itself, Hannah can draft much larger goals, like \u201cinvade France\u201d, and let Martin take care of details like \u201cblackmail Emmanuel Macron\u201d, \u201cwrite battle plan for the French Alps\u201d, and \u201cselect a suitable coronation dress\u201d.</li></ol><p>Note some features of this process. First, Martin learns what it should do and how to do it at the same time. Second, both Hannah\u2019s and Martin\u2019s role changes throughout this process - Martin goes from bumbling idiot who can\u2019t write an email greeting to competent assistant, while Hannah goes from being a demonstrator of simple tasks to a manager of Martin to ruler of France. Third, note the recursive nature here: Hannah breaks down big tasks into small ones to train Martin on successively bigger tasks.&nbsp;</p><p>In fact, assuming perfect training, IDA imitates a recursive structure. When Hannah has only bumbling fool Martin to help her, Martin can only learn to become as good as Hannah herself. But once Martin is that good, Hannah\u2019s position is now essentially that of having herself, but also some number - say 3 - copies of Martin that are as good as herself. We might call this structure \u201cHannah Consulting Hannah &amp; Hannah\u201d; presumably, being able to consult an assistant that has the same skills as her lets Hannah become more effective, so this is an improvement. But now Hannah is demonstrating the behaviour of Hannah Consulting Hannah &amp; Hannah, so from Hannah\u2019s example Martin can now learn to be as good as Hannah Consulting Hannah &amp; Hannah - making Hannah as good as Hannah Consulting (Hannah Consulting Hannah &amp; Hannah) &amp; (Hannah Consulting Hannah &amp; Hannah). And so on:</p><figure class=\"image image_resized\" style=\"width:39.51%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/loyk1wy5hpuyzo1fppzg.png\"></figure><p>If everything is perfect, therefore, IDA imitates a structure called \u201cHCH\u201d, which is a recursive acronym for \u201cHumans Consulting HCH\u201d. Others call it the \u201c<a href=\"https://www.lesswrong.com/posts/tmuFmHuyb4eWmPXz8/rant-on-problem-factorization-for-alignment\"><u>Infinite Bureaucracy</u></a>\u201d (and fret about whether it\u2019s actually a good idea).</p><p>Now \u201cInfinite Bureaucracy\u201d is not a name that screams \u201cnew sexy machine learning concept\u201d. However, it\u2019s interesting to think about what properties it might have. Imagine that you had, say, a 10-minute time limit to answer a complicated question, but you were allowed to consult three copies of yourself by passing a question off to them and getting back an answer immediately. These three copies also obeyed the same rules. Could you, for example, plan your career? Program an app? Write a novel?</p><p>It\u2019s also interesting to think of the ways why the limitations of machine learning mean that IDA might not approximate HCH.</p><h3>Example 2: AI safety via debate</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/bvwjnqeovifhts3cgvju.png\"><figcaption><i>artists depiction of two robots debating, digital art, trending on artstation</i></figcaption></figure><p>Imagine you\u2019re a bit drunk, but (as one does) you\u2019re at a bar talking about AI alignment proposals. Someone\u2019s talking about how even if you can get an advanced AI system to explain its reasoning to you, it might try to slip something very subtle past you and you might not notice. You might well blurt out: \u201cwell then just make it fight another AI over it!\u201d</p><p>The OpenAI safety team presumably spends a fair amount of time at bars, because they\u2019ve&nbsp;<a href=\"https://openai.com/blog/debate/\"><u>investigated the idea of achieving safe AI by having two AIs debate each other</u></a> to persuade a panel of human judges, by trying to poke holes in each other\u2019s arguments. For more complex tasks, the AIs could be given transparency tools deriving from interpretability research (see next section) that they can use on each other. Just like a Go-playing AI gets an unambiguous win-loss signal from either winning or losing, a debating AI gets an unambiguous win-loss signal from winning or losing the debate:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/t8pjicezei5bymqgyoxz.png\"></figure><p>In addition, having the type of AI that is trained to give answers that are maximally insightful and persuasive to humans seems like the type of thing that might not be terrible. Consider how in court, a prosecutor and defendant biased in opposite directions are generally assumed to converge on the truth. Unless, of course, maximising persuasiveness to humans - over accuracy or helpfulness - is exactly the type of thing that gets the worst parts of Goodhart\u2019s law delivered to you by 24/7 Amazon Prime express delivery.</p><h3>Example 3: Assistance Games and CIRL</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/ewn6ldj9wlk3n4dvjijj.png\"><figcaption><i>Human teaching a robot with feedback, digital art, trending on artstation</i></figcaption></figure><p>Assistance Games are the name of a broad class of approaches pioneered by Stuart Russell, a prominent figure in AI and co-author of the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\"><u>best-known AI textbook in the world</u></a>. Russell talks about his approach more in his book&nbsp;<a href=\"https://en.wikipedia.org/wiki/Human_Compatible\"><i><u>Human Compatible</u></i></a>. In it, he summarises the key his approach to aligning AI with the following three principles:</p><ul><li>The machine\u2019s only objective is to maximise the realisation of human preferences.</li><li>The machine is initially uncertain about what those preferences are.</li><li>The ultimate source of information about human preferences is human behaviour.</li></ul><p>The key component here is&nbsp;<strong>uncertainty about preferences</strong>. This is in contrast to what Russell calls the \u201cstandard model\u201d of AI, where machines optimise a fixed objective supplied by humans. We have discussed in previous sections the problems with such a paradigm. A lot of Russell\u2019s work focuses on changing the standard way the field thinks about AI.</p><p>To put these principles into action, Russell has designed what he calls&nbsp;<strong>assistance games</strong>. These are situations in which the machine and human interact, and the human\u2019s actions are taken as evidence by the machine about the human\u2019s true preferences. To explain the form of these games would involve a long tangent into game theory, which these margins are too short to contain. However, one thing worth noting is that assistance games have the potential to solve the&nbsp;<strong>\u201coff-switch problem\u201d</strong>; that a machine will try and take steps to prevent itself from being switched off (we described this as&nbsp;<i>self-preservation</i> earlier, in the section on instrumental goals). If the AI is uncertain about human goals, then the human trying to switch it off is evidence that the AI was going to do something wrong \u2013 in which case, it is happy to be switched off. However, this is far from a complete agenda, and formalising it has many roadblocks to get past. For instance, the question of how exactly to infer human preferences from human behaviour leads into thorny philosophical issues such as&nbsp;<i>Gricean semantics.&nbsp;</i>In cases where the AI makes incorrect inferences about human preferences, it might no longer allow itself to be shut down. See&nbsp;<a href=\"https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai\"><u>this Alignment Newsletter entry</u></a> for a summary of Russell\u2019s book, which provides some more details as well as an overview of relevant papers.</p><blockquote><p><i>Vignette: <strong><u>CHAI&nbsp;</u></strong></i></p><figure class=\"image image_resized\" style=\"width:29.78%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/xo5mgftj7vx1jc4mby0v.png\"></figure><p><i>CHAI (the Centre for Human-Compatible AI) is a research lab at UC Berkeley, run by Stuart Russell. Compared to most other AI safety organisations, they engage a lot with the academic community, and have produced a great deal of research over the years. They are best-known for their work on CIRL (Cooperative Inverse Reinforcement Learning), which can be seen as a specific approach to a certain kind of assistance game. However, they have a very broad focus which also includes work on multi-agent scenarios (when rather than a single AI and single human, there exists more than one AI or more than one human - see the&nbsp;</i><a href=\"http://acritch.com/arches/\"><i><u>ARCHES agenda</u></i></a><i> for more on this).&nbsp;</i></p></blockquote><h3>Example 4: Reinforcement learning from human feedback (RLHF)</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/es3s4vmyxrngvs5qqckl.png\"><figcaption><i>Training a robot to do a backflip, digital art, trending on artstation</i></figcaption></figure><p>Reinforcement learning (RL) is one of the main branches of ML, focusing on the case where the job of the ML model is to act in some environment and maximise the probability of reward. Reinforcement learning from human feedback (RLHF) means that the ML model\u2019s reward signal comes (at least partly) from humans giving it feedback directly, rather than humans programming in an automatic reward function and calling it a day.</p><p>The famous initial success in this was DeepMind training an ML model in a simulated environment&nbsp;<a href=\"https://www.deepmind.com/blog/learning-through-human-feedback\"><u>to do a backflip</u></a> (link includes GIF) in 2017, based purely on it repeatedly doing two backflips and then humans labelling one of them as the better one. Note how relying on human feedback makes this task much more robust to specification gaming; in other cases, humans have tried to get ML agents to run fast, only to find that they learn to become very tall and then fall forward (achieving a very high average speed, using the definition of speed as the rate at which their centre of mass moves -&nbsp;<a href=\"http://www.karlsims.com/papers/siggraph94.pdf\"><u>paper</u></a>,&nbsp;<a href=\"https://www.youtube.com/watch?v=TaXUZfwACVE&amp;list=PL5278ezwmoxQODgYB0hWnC0-Ob09GZGe2&amp;index=9\"><u>video</u></a>). However, human reward signals can be fooled. For example,&nbsp;<a href=\"https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/\"><u>one ML model</u></a> that was being trained to grab a ball with a hand learned to place the hand between the camera and the ball in such a way that it looked to the human evaluators as if it were holding the ball.</p><p>More recently, OpenAI produced a version of their advanced language model GPT-3 that was fine-tuned on human feedback to do a better job of following instructions. They named it&nbsp;<a href=\"https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf\"><u>InstructGPT, and found that it was much more helpful than vanilla GPT-3</u></a> at being useful.</p><p>Pure RLHF is unlikely to be the solution on its own. Ajeya Cotra, a researcher at Open Philanthropy who we will meet again when we talk about forecasting AI timelines, calls a variant of RLHF called HFDT (Human Feedback on Diverse Tasks) the most straightforward route to transformative AI,&nbsp;<a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>while also thinking that the default outcome of using HFDT to create transformative AI is AI takeover.</u></a></p><h2>Robustly good approaches</h2><blockquote><p><i>In the long-run AGI will clearly be important, but we're <strong>highly uncertain</strong> about how we'll get there and what, exactly, could go wrong. So let's do <strong>work that seems good in many possible scenarios</strong>, and doesn\u2019t rely on having a specific story in mind.</i></p></blockquote><h3>Example 1: Interpretability</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/vwqqpjkbuuqj7g1ccsmy.png\"><figcaption><i>A person using a microscope to look inside a robot, digital art, trending on artstation</i></figcaption></figure><p>If you look at fundamental problems with current ML systems, #1 is probably something like this: in general we don\u2019t have any idea what an ML model is doing, because it\u2019s multiplying massive inscrutable matrices of floating-point numbers with other massive inscrutable matrices of floating point numbers, and it\u2019s pretty hard to stare at that and answer questions about what the model is actually doing. Is it thinking hard about whether an image is a cat or a dog? Is it counting up electric sheep? Is it daydreaming about the AI revolution? Who knows!</p><p>If you had to figure out an answer to such a question today, your best bet might be to call Chris Olah. Chris Olah has been spearheading work into trying to interpret what neural networks are doing. A signature output of Chris Olah\u2019s work is pictures of creepy dogs like this one:</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/pm33wk2jthmbvhxyc9t8.png\"></figure><p>What\u2019s significant about this picture is that it\u2019s the answer to a question roughly like this: what image would maximise the activation of neuron #12345678 in a particular image-classifying neural network? (With some asterisks about needing to apply some maths details to the process to promote large-scale structure in the image to get nice-looking results, and with apologies to neuron #12345678, who I might have confused with another neuron.)</p><p>If neuron #12345678 is maximised by something that looks like a dog, it\u2019s a fair guess that this neuron somehow encodes, or is involved in encoding, the concept of \u201cdog\u201d inside the neural network.</p><p>What\u2019s especially interesting is that if you do this analysis for every neuron in an ML model -&nbsp;<a href=\"https://microscope.openai.com/models\"><u>OpenAI Microscope</u></a> lets you see the results - you sometimes get clear patterns of increasing abstraction. The activation-maximising images for the first few layers are simple patterns; in intermediate layers you get things like curves and shapes, and then at the end even recognisable things, like the dog above. This seems evidence for neural ML vision models having learned to build up abstractions step-by-step.</p><p>However, it\u2019s not always simple. For example, there are \u201cpolysemantic\u201d neurons that correspond to several different concepts, like this one that can be equally excited by cat faces, car fronts, and cat legs:</p><figure class=\"image image_resized\" style=\"width:82.7%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/ekwt69tj3oui9glfz5fs.png\"></figure><p>Olah\u2019s original work on vision models is strikingly readable and well-presented; you can find it&nbsp;<a href=\"https://distill.pub/2020/circuits/zoom-in/\"><u>here</u></a>.</p><p>Starting in late 2021, ML interpretability researchers have also made some progress in understanding transformers, which are the neural network architecture powering advanced language models like <a href=\"https://openai.com/blog/gpt-3-apps/\">GPT-3</a>, <a href=\"https://blog.google/technology/ai/lamda/\">LAMDA</a> and <a href=\"https://openai.com/blog/openai-codex/\">Codex</a>. Unfortunately the work is less visual, particularly in the animal pictures department, but still well-presented. You can find it&nbsp;<a href=\"https://transformer-circuits.pub/2021/framework/index.html\"><u>here</u></a>.</p><p>In the most immediate sense, interpretability research is about reverse-engineering how exactly ML models do what they do. Hopefully, this will give insights into how to detect if an ML system is doing something we don\u2019t like, and more general insights into how ML systems work in practice.</p><p>Chris Olah has some other inventive ideas about what to do with a sufficiently-good approach to ML interpretability. For example, he\u2019s proposed the concept of \u201cmicroscope AI\u201d, which entails using AI as a tool to discover things about the world - not by having the AI tell us, but by training the ML system on some data, and then extracting insights about the data by digging into the internals of the ML system without necessarily ever actually running it.</p><blockquote><p><i>Vignette: <strong><u>Anthropic</u></strong></i></p><figure class=\"image image_resized\" style=\"width:16.18%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/oiy54tks4k199twgm5dp.jpg\"></figure><p><i>Anthropic is an AI safety company, started by people who left </i><a href=\"https://openai.com/\"><i>OpenAI</i></a><i>. The company\u2019s approach is very empirical, focused on running experiments with machine learning models. In particular, Anthropic does a lot of interpretability work, including&nbsp;</i><a href=\"https://transformer-circuits.pub/\"><i><u>the state-of-the-art papers on reverse-engineering how transformer-based language models work.</u></i></a></p></blockquote><h3>Example 2: Adversarial robustness</h3><figure class=\"image image_resized\" style=\"width:50.16%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/om5cmwt9pkbwuuwk0hd0.png\"><figcaption><i>robot which is merging with a panda, digital art, trending on artstation</i></figcaption></figure><p>Some modern ML systems are vulnerable to adversarial examples, where a small and seemingly innocuous change to an input causes a major change in the output behaviour. Here, we see two seemingly very similar images of a panda, except carefully-selected noise has made the ML classification model very confidently say that the image is of a gibbon:</p><figure class=\"image image_resized\" style=\"width:78.59%\"><img src=\"https://www.researchgate.net/publication/347639649/figure/fig1/AS:973837478948864@1609192356344/A-demonstration-of-an-adversarial-sample-21-The-panda-image-is-recognized-as-a-gibbon.ppm\"></figure><p>Adversarial robustness is about making AI systems robust to attempts to make them do bad things, even when they\u2019re presented with inputs carefully designed to try to make them mess up.</p><p>Redwood Research recently did a project (that resulted in&nbsp;<a href=\"https://arxiv.org/pdf/2205.01663.pdf\"><u>a paper</u></a>) about using language models to complete stories in a way where people don\u2019t get injured. They used a technique called adversarial training, where they developed tools that helped generate examples where the current model did not classify them as injurious, and then trained their classifier specifically on those breaking examples. With this strategy they managed to reduce the fraction of injurious story completions from 2.4% to 0.003% - both small numbers, but one a thousand times smaller. Their hope is that this type of method can be applied to training AIs for high-stakes settings where reliability is important.</p><p>An example of a theoretical difficulty with adversarial training is that sometimes a failure in the model might exist, but it might be very hard to instantiate. For example, if an advanced AI acts according to the rule \u201cif everything I see is consistent with the year being 2050, I will kill all humans\u201d, and we assume that we can\u2019t fool it well enough about what year it actually is, then adversarial training isn\u2019t very useful. This leads to the concept of&nbsp;<i>relaxed</i> adversarial training, which is about extending adversarial training to cases where you can\u2019t construct a specific adversarial input but you can argue that one exists. Evan Hubinger describes this&nbsp;<a href=\"https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment\"><u>here</u></a>.</p><blockquote><p><i>Vignette: <strong><u>Redwood Research</u></strong></i></p><figure class=\"image image_resized\" style=\"width:18.79%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/o1l4r35theas7lhpvvpl.jpg\"></figure><p><i>Like Anthropic, Redwood Research is an AI safety company focused on empirical research on ML systems. In addition to work on interpretability, they did the adversarial training project described in the previous section. Redwood has lots of interns, and runs the Machine Learning for Alignment Bootcamp (MLAB) that teaches people interested in AI safety about practical ML.</i></p></blockquote><h3>Example 3: Eliciting Latent Knowledge (ELK)</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/qi2fgjbiuqpljw5k3qgg.png\"><figcaption><i>an oil painting of an armoured automaton standing guard next to a diamond</i></figcaption></figure><p>Eliciting Latent Knowledge (ELK) is an important sub-problem within alignment identified by the team at the&nbsp;<a href=\"https://alignment.org/\"><u>Alignment Research Center (ARC</u></a>), and is the single project ARC is currently pursuing. The core idea is that a common way advanced AI systems might go wrong is by taking action sequences that lead to outcomes that look good by some metric, but which humans would clearly identify as bad if they knew about it in sufficient detail. As a toy example, the ELK report discusses the case of an AI guarding a diamond in a vault by operating some complex machinery around it. Humans judge how well the AI is doing by looking at a video feed of the diamond in the vault. Let\u2019s say the AI tries to trick us by placing a picture of the diamond in front of the camera. The human judgement on this would be positive - assume the humans can\u2019t tell the diamond is gone because the picture is good enough - but there exists information which, if the humans knew, would change their judgement. Presumably the AI understands this, since it is likely reasoning about the diamond being gone but the humans being fooled anyway when it comes up with this plan. We want to train an AI in such a way that we can get out knowledge that the AI seems to know, even when it might be incentivised to hide it.</p><p>ARC\u2019s goal is to find a theoretical approach that seems to solve the problem even given worst-case assumptions.</p><p>ARC ran an ELK competition, and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Q2BJnpNh8e6RAWFnm/consider-trying-the-elk-contest-i-am\"><u>trying to see if you can come up with solutions to the ELK problem</u></a> is often recommended as a way to quickly get a taste of theoretical alignment research. You can read the full problem description&nbsp;<a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.kkaua0hwmp1d\"><u>here</u></a>.</p><h3>Example 4: Forecasting and timelines</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/bhcwrfltdj1zarhurgtw.png\"><figcaption><i>artificial intelligence which is thinking about a line on a graph, forecasting, digital art, trending on artstation</i></figcaption></figure><p>Many questions depend on how soon we\u2019re going to get AGI. As the saying goes: prediction is very hard, especially about the future - and this is doubly true about predicting major technological changes.&nbsp;</p><p>One way to try to forecast AGI timelines is to&nbsp;<a href=\"https://www.lesswrong.com/posts/H6hMugfY3tDQGfqYL/what-do-ml-researchers-think-about-ai-in-2022\"><u>ask experts</u></a>, or find other ways of aggregating the opinion of people who have the knowledge or incentive to be right, as for example&nbsp;<a href=\"https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/\"><u>prediction markets do</u></a>. Both of these are essentially just ways of tapping into the intuition of a bunch of people who hopefully have some idea.</p><p>In an attempt to bring in new light on the matter, Ajeya Cotra (a researcher at Open Philanthropy) wrote&nbsp;<a href=\"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\"><u>a long report</u></a> on trying to forecast AI milestones by trying out several ways of analogising AI to biological brains. The report is often referred to as \u201cBiological Anchors\u201d. For example, you might assume that an ML model that does as much computation as the human brain has a decent chance of being a human-level AI. There are many degrees of freedom here: is the relevant compute number the amount of compute the human brain uses to run versus the amount of compute it takes to run a trained ML system, or the total compute of a human brain over a human lifetime versus the compute required to train the ML model from scratch, or something else entirely? In her report, Cotra looks at a range of assumptions for this, and at predictions of future compute trends, and somewhat surprisingly finds that which set of assumptions you make doesn\u2019t matter too much; every scenario involves &gt;50% of human-level AI by 2100.</p><p>The Biological Anchors method is very imprecise. For one, it neglects algorithmic improvements. For another, it is very unclear what the right biological comparison point is, and how to translate ML-relevant variables like compute measured in FLOPS (FLoating point OPerations per Second) or parameter count into biological equivalents. However, the report does a good job of acknowledging and taking into account all this uncertainty in its models. More generally, anything that sheds light into the question of when we get AGI seems highly relevant.</p><h2>Deconfusion</h2><blockquote><p><i>Reasoning about how to align AGI involves reasoning about complex concepts, such as intelligence, alignment and values, and <strong>we\u2019re pretty confused about what these even mean</strong>. This means any work we do now is plausibly not helpful and definitely not reliable. As such, our priority should be <strong>doing conceptual work on how to think about these concepts and what we\u2019re aiming for</strong>, and trying to become less confused.</i></p></blockquote><p>Of all the categories under discussion here, deconfusion has maybe the least clear path to impact. It\u2019s not immediately obvious how becoming less confused about concepts like these is going to translate into an improved ability to align AGIs.</p><figure class=\"image image_resized\" style=\"width:63.37%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/jbitikiqfkwcmaxhyw4c.png\"></figure><p>Some kinds of deconfusion research is just about finding clearer ways of describing different parts of the alignment problem (Hubinger\u2019s&nbsp;<a href=\"https://arxiv.org/abs/1906.01820\"><u>Risks From Learned Optimisation</u></a>, where he first introduces the inner/outer alignment terminology, is a good example of this). But other types of research can dive heavily into mathematics and even philosophy, and be very difficult to understand.</p><h3>Example 1: MIRI and Agent Foundations</h3><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/rcagcg6nthb9ivw31jkc.png\"><figcaption><i>robot sitting in front of a television, playing a videogame, digital art</i></figcaption></figure><p>The organisation most associated with this view is MIRI (the Machine Intelligence Research Institute). Its founder, Eliezer Yudkowsky, has written extensively on AI alignment and human rationality, as well as topics as wide-ranging as evolutionary psychology and quantum physics. His post&nbsp;<a href=\"https://intelligence.org/2018/10/03/rocket-alignment/\"><u>The Rocket Alignment Problem</u></a> tries to get across some of his intuitions behind MIRI\u2019s research, in the form of an analogy \u2013 trying to build aligned AGI without having deeper understanding of concepts like intelligence and values is like trying to land a rocket on the moon by just pointing and shooting, without a working understanding of Newtonian mechanics.&nbsp;</p><p>Cryptography provides a different lens through which to view this kind of foundational research. Suppose you were trying to send secret messages to an ally, and to make sure nobody could intercept and read your messages you wanted a way to measure how much information was shared between the original and encrypted message. You might use&nbsp;<a href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\"><u>correlation coefficient</u></a> as a proxy for the shared information, but unfortunately having a correlation coefficient of zero between the original and encrypted message isn\u2019t enough to guarantee safety. But if you find the concept of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Mutual_information\"><u>mutual information</u></a>, then you\u2019re done \u2013 ensuring zero mutual information between your original and encrypted message guarantees the adversary will be unable to read your message. In other words, only once you\u2019ve found a&nbsp;<strong>\u201ctrue name\u201d&nbsp;</strong>- a robust formalisation of the intuitive concept you\u2019re trying to express mathematically - can you be free from the effects of Goodhart\u2019s law. Similarly, maybe if we get robust formulations of concepts like \u201cagency\u201d and \u201coptimisation\u201d, we would be able to inspect a trained system and tell whether it contained any misaligned inner optimisers (see the first post), and these inspection tools would work even in extreme circumstances (such as the AI becoming much smarter than us).</p><p>Much of MIRI\u2019s research has come under the heading of&nbsp;<a href=\"https://intelligence.org/embedded-agency/\"><u>embedded agency</u></a>. This tackles issues that arise when we are considering agents which are part of the environments they operate in (as opposed to standard assumptions in fields like reinforcement learning, where the agent is viewed as separate from their environment). Four main subfields of this area of study are:</p><ul><li><strong>Decision theory</strong> (adapting classical decision theory to embedded agents)</li><li><strong>Embedded world-models&nbsp;</strong>(how to form true beliefs about the a world in which you are embedded)</li><li><strong>Robust delegation&nbsp;(understanding what trust relationships can exist between agents and its future - maybe far more intelligent - self)</strong></li><li><strong>Subsystem alignment</strong> (how to make sure an agent doesn\u2019t spin up internal agents which have different goals)</li></ul><blockquote><p><i>Vignette: <strong><u>MIRI</u></strong></i></p><figure class=\"image image_resized\" style=\"width:18.79%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/cxccerx54d8qijp2lir3.png\"></figure><p><i>MIRI is the oldest organisation in the AI alignment space. It used to be called the Singularity Institute, and had the goal of accelerating the development of AI. In 2005 they shifted focus towards trying to manage the risks from advanced AI. This has largely consisted of fundamental mathematical research of the type described above. MIRI might be better described as a confluence of smart people with backgrounds in highly technical fields (e.g. mathematics), working on different research agendas that share underlying philosophies and intuitions. They have a nondisclosure policy by default, which they explain in this&nbsp;</i><a href=\"https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section3\"><i><u>announcement post</u></i></a><i> from 2018.</i></p></blockquote><h3>Example 2: John Wentworth and Natural Abstractions</h3><figure class=\"image image_resized\" style=\"width:50.19%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/pf2el8i8z2yjp4hqd47p.png\"><figcaption><i>thermometer being used to measure a robot, digital art, trending on artstation</i></figcaption></figure><p>John Wentworth is an independent researcher, who publishes most of his work on&nbsp;<a href=\"https://www.lesswrong.com/users/johnswentworth\"><u>LessWrong</u></a> and the&nbsp;<a href=\"https://www.alignmentforum.org/users/johnswentworth\"><u>AI Alignment Forum</u></a>. His main research agenda focuses on the idea of&nbsp;<a href=\"https://www.lesswrong.com/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence\"><u>Natural Abstractions</u></a>, which can be described in terms of three sub-claims:</p><ul><li><strong>Abstractability</strong><br>Our physical world abstracts well, i.e. we can usually come up with simpler summaries (abstractions) for much more complicated systems (example: a gear is a very complex object containing a vast number of atoms, but we can summarise all relevant information about it in just one number - the angle of rotation).</li><li><strong>Human-Compatibility</strong><br>These are the abstractions used by humans in day-to-day thought/language.</li><li><strong>Convergence</strong><br>These abstractions are \"natural\", in the sense that we should expect a wide variety of intelligent agents to converge on using them.</li></ul><p>The&nbsp;<a href=\"https://www.lesswrong.com/posts/gdEDPHjCY5DKsMsvE/the-pragmascope-idea\"><u>ideal outcome</u></a> of this line of research would be some kind of measurement device (an \u201cabstraction thermometer\u201d), which could take in a system like a trained neural network and spit out a representation of the abstractions represented by that system. In this way, you\u2019d be able to get a better understanding of what the AI was actually doing. In particular, you might be able to identify inner alignment failures (the AI\u2019s true goal not corresponding to the reward function it was&nbsp; being trained on), and you could retrain it while pointed at the intended goal. So far, this line of research has consisted of some&nbsp;<a href=\"https://www.lesswrong.com/posts/jJf4FrfiQdDGg7uco/the-telephone-theorem-information-at-a-distance-is-mediated\"><u>fairly</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/cqdDGuTs2NamtEhBW/maxent-and-abstractions-current-best-arguments\"><u>dense</u></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information\"><u>mathematics</u></a>, but Wentworth has&nbsp;<a href=\"https://www.lesswrong.com/posts/gdEDPHjCY5DKsMsvE/the-pragmascope-idea\"><u>described</u></a> his plans to build on this with more empirical work (e.g. training neural networks on the same data, and using tools from calculus to try and compare the similarity of concepts learned by each of the networks).&nbsp;&nbsp;</p><figure class=\"image image_resized\" style=\"width:82.69%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/gegoxnmkz3pqzlewfdrg.png\"></figure><h2>AI governance</h2><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/eckdosoxgnqwanq9u1ap.png\"><figcaption><i>judging, presiding over a trial, sentencing a robot, digital art, artstation</i></figcaption></figure><p>In these posts, we\u2019ve mainly focused on the technical side of the issue. This is important, especially for understanding why there is a problem in the first place. However, the management and reduction of AI risk obviously includes not just technical approaches like outlined in the above sections, but also&nbsp;<a href=\"https://80000hours.org/articles/ai-policy-guide/\"><u>the field of AI governance</u></a>, which tries to understand and push for the right types of policies for advanced AI systems.</p><p>For example, the Cold War was made a lot more dangerous by the nuclear arms race. How do we avoid having an arms race in AI, either between nations or companies? More generally, how can we make sure that safety considerations are given appropriate weight by the teams building advanced AI systems? How do we make sure any technical solutions get implemented?</p><p>It\u2019s also very hard to say what the impacts of AI will be, across a broad range of possible technical outcomes. If AI capabilities at some point advance very quickly from below human-level to far beyond the human-level, the way the future looks will likely mostly be determined by technical considerations about the AI system. However, if progress is slower, there will be a longer period of time where weird things are happening because of advanced AI - for example, significantly accelerated economic growth, or mass unemployment, or an AI-assisted boom in science - and these will have economic, social, and political ramifications that will play out in a world not too dissimilar from our own. Someone should be working on figuring out what these ramifications will be, especially if they might alter the balance of existential threats that civilisation faces; for example, if they make geopolitics less unstable and nuclear war more likely, or affect the environment in which even more powerful AI systems are developed.</p><p>The Centre for the Governance of AI, or&nbsp;<a href=\"https://www.governance.ai/\"><u>GovAI</u></a> for short, is an example of an organisation in this space.</p><h2>Field-building</h2><figure class=\"image image_resized\" style=\"width:50.38%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/avs8jalapseuu5sssbnn.png\"><figcaption><i>robot giving a lecture in a university, group of students, hands up, digital art, artstation</i></figcaption></figure><p><i>One of the most important ways we can make AI go well is by increasing the number of capable researchers doing alignment research.</i></p><p>As mentioned, AI safety is still a relatively young field. The case here is that we might do better to grow the field, and increase the quality of research it produces in the future. Some forms that field building can take are:</p><ul><li><strong>Setting up new ways for people to enter the field</strong><br>There are many to list here. To give a few different structures which exist for this purpose:<ul><li><strong>Reading groups and introductory programmes.&nbsp;</strong><br>Maybe the most exciting one from the last few years has been the Cambridge&nbsp;<a href=\"https://www.eacambridge.org/agi-safety-fundamentals\"><u>AGI Safety Fundamentals Programme</u></a>, which has curricula for technical alignment and AI governance. The technical curriculum consists of 7 weeks of reading material and group discussions, and a final week of capstone projects where the participants try their hand at a project / investigation / writeup related to AI safety. Beyond this, many people are also setting up reading groups in their own universities for books like&nbsp;<i>Human Compatible</i>.&nbsp;</li><li><strong>Ways of supporting independent researchers</strong><br>The&nbsp;<a href=\"https://aisafety.camp/\"><u>AI Safety Camp</u></a> is an organisation which matches applicants with mentors posing a specific research question, and is structured as a series of group research sprints. They have produced work such as the example of inner misalignment in the CoinRun game, which we discussed in a previous section. Other examples of organisations which support independent research include&nbsp;<a href=\"https://www.lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup\"><u>Conjecture</u></a>, a recent alignment startup which does their own alignment research as well as providing a structure to host externally funded independent conceptual researchers, and&nbsp;<a href=\"https://alignmentfund.org/\"><u>FAR (the Fund for Alignment Research)</u></a>.</li><li><strong>Coding bootcamps</strong><br>Since current systems are increasingly being bottlenecked by alignment and interpretability barriers rather than capabilities, in recent years more focus has been directed towards working with cutting-edge deep learning models. This requires strong coding skills and a good understanding of the relevant ML, which is why bootcamps and programmes specifically designed to skill up future alignment researchers have been created. Two such examples are&nbsp;<a href=\"https://www.lesswrong.com/posts/3ouxBRRzjxarTukMW/apply-to-the-second-iteration-of-the-ml-for-alignment\"><u>MLAB</u></a> (the Machine Learning for Alignment Bootcamp, run by Redwood Research), and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/9RYvJu2iNJMXgWCBn/introducing-the-ml-safety-scholars-program\"><u>MLSS</u></a> (the Machine Learning Safety Scholars Programme, which is based on publicly available material as well as lectures produced by Dan Hendryks).&nbsp;</li></ul></li><li><strong>Distilling research</strong><br>In&nbsp;<a href=\"https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers\"><u>this post</u></a>, John Wentworth makes the case for more distillation in AI alignment research - in other words, more people who focus on understanding and communicating the work of alignment researchers to others. This often takes the form of writing more accessible summaries of hard-to-interpret technical papers, and emphasising the key ideas.</li><li><strong>Public outreach / better intro material</strong><br>For instance, books like Brian Christian\u2019s&nbsp;<a href=\"https://en.wikipedia.org/wiki/The_Alignment_Problem\"><i><u>The Alignment Problem</u></i></a><i>,&nbsp;</i>Stuart Russell\u2019s&nbsp;<a href=\"https://en.wikipedia.org/wiki/Human_Compatible\"><i><u>Human Compatible</u></i></a> and Nick Bostrom\u2019s&nbsp;<a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\"><i><u>Superintelligence</u></i></a> communicate AI risk to a wide audience. These books have been helpful for making the case for AI risks more mainstream. Note that there can be some overlap between this and distilling research (Rob Miles\u2019&nbsp;<a href=\"https://www.youtube.com/c/RobertMilesAI\"><u>channel</u></a> is another great example here).</li><li><strong>Getting more of the academic community involved</strong><br>Since AI safety is a hard technical problem, and since misaligned systems generally won\u2019t be as commercially useful as aligned ones, it makes sense to try and engage the broader field of machine learning. One great example of this is Dan Hendryks\u2019 paper&nbsp;<a href=\"https://mailchi.mp/08a639ffa2ba/an-167concrete-ml-safety-problems-and-their-relevance-to-x-risk\"><u>Unsolved Problems in ML Safety</u></a> (which describes a list of problems in AI safety, with the ML community as the target audience). Stuart Russell has also engaged a lot with the ML community.&nbsp;</li></ul><p>Note that this is certainly not a comprehensive overview of all current AI alignment proposals (a few more we haven\u2019t had time to talk about are CAIS, Andrew Critch\u2019s cooperation-and-coordination-failures framing for AI risks, and many others). However, we hope this has given you a brief overview of some of the different approaches taken by people in the field, as well as the motivations behind their research</p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/g3tb1kvs1xuralv4oc66.png\"><figcaption>Map of the solution approaches we've discussed so far</figcaption></figure><h1><strong>Conclusion</strong></h1><figure class=\"image image_resized\" style=\"width:50.48%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995629/mirroredImages/e2upqGf6q4CiudLMu/plnndfqzvbzzerj4ei9x.png\"><figcaption><i>people walking along a path which stretches off and disappears into a colorful galaxy filled with beautiful stars, digital art, trending on artstation</i></figcaption></figure><p>Advanced AI represents at least a technology that promises to have effects on the scale of the internet or computer revolutions, and perhaps even more likely to be more akin to the effects of the <strong>industrial revolution</strong> (which allowed for the automation of much&nbsp;<i>manual&nbsp;</i>labour) and the <strong>evolution of humans</strong> (the last time something significantly smarter than everything that had come before appeared on the planet).</p><p>It\u2019s easy to invent technologies that the same could be said about - a magic wish-granting box! Wow! But unlike magic wish-granting boxes, something like advanced AI, or AGI, or transformative AI, or&nbsp;<a href=\"https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/\"><u>PASTA</u></a> (Process for Automating Scientific and Technical Achievement) seems to be headed our way. The smart money is on it very likely coming <strong>this century</strong>, and quite likely in the <strong>first half</strong>.</p><p>If you look at the progress in modern machine learning, and especially the past few years of progress in so-called deep learning, it is hard not to feel a sense of rushing progress. The past few years of progress, in particular the success of the transformer architecture, should update us in the direction that intelligence might be a surprisingly easy problem. What is essentially fancy iterative statistical curve-fitting with a few hacks thrown in already manages to write fluent appropriate English text in response to questions, create paintings from a description, and carry out multi-step logical deduction in natural language. <strong>The fundamental problem that plagued AI progress for over half a century - getting fuzzy/intuitive/creative thinking into a machine, in addition to the sharp but brittle logic at which computers have long excelled - seems to have been cracked.</strong> There is a solid empirical pattern of predictably improving performance akin to Moore\u2019s law - the \u201c<a href=\"https://arxiv.org/pdf/2001.08361.pdf\"><u>scaling laws</u></a>\u201d we mentioned in the first post - that we seem not to have hit the limits of yet. There are experts in the field who would not be surprised if the remaining insights for cracking human-level machine intelligence could fit into a few good papers.</p><p>This is not to say that AGI is definitely coming soon. The field might get stuck on some stumbling block for a decade, during which there will be no doubt much written about the failed promises and excess hype of the early-2020s deep learning revolution.</p><p>Finally, as we\u2019ve argued, by default the arrival of advanced AI might plausibly lead to civilisation-wide catastrophe.</p><p>There are few things in the world that fit all of the following points:</p><ul><li>A potentially transformative technology whose development would likely rank somewhere between the top events of the century and the top events in the history of life on Earth.</li><li>Something that is likely to happen in the coming decades.</li><li>Something that has a meaningful chance of being cataclysmically bad.</li></ul><p>For those thinking about the longer-term picture, whatever the short-term ebb and flow of progress in the field is, AI and AI risk loom large when thinking about humanity\u2019s future. The main ways in which this might stop being the case are:</p><ul><li>There is a major flaw in the arguments for at least one of the above points. Since many of the arguments are abstract and not empirically falsifiable before it\u2019s too late to matter, this is possible. However, note that there is a strong and recurring pattern of many people, including in particular many extremely-talented people, running into the arguments and taking them more and more seriously. (If you do have a strong argument against the importance of the AI alignment problem, there are many people - us included - who would be very eager to hear from you. Some of these people - us not included - would probably also pay you large amounts of money.)</li><li>We solve the technical AI alignment problem, and we solve the AI governance problem to a degree where the technical solutions will be implemented and it seems very unlikely that advanced AI systems will wreak havoc with society.</li><li>A catastrophic outcome for human civilisation, whether resulting from AI itself or something else.&nbsp;</li></ul><p>The project of trying to make sure the development of advanced AI goes well is likely one of the most important things in the world to be working on (if you\u2019re lost, the&nbsp;<a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\"><u>80 000 Hours problem profile</u></a> is a decent place to start). It might turn out to be easy - consider how many seemingly intractable scientific problems dissolved once someone had the right insight. But right now, at least, it seems like it might be a fiendishly difficult problem, especially if it continues to seem like the insights we need for alignment are very different from the insights we need to build advanced AI.</p><p>Most of the time, science and technology progress in whatever direction is easiest or flows most naturally from existing knowledge. Other times, reality throws down a gauntlet, and we must either overcome the challenge or fail. May the best in our species - our ingenuity, persistence, and coordination - rise up, and deliver us from peril.</p><p>&nbsp;</p><p>&nbsp;</p><p><i>We give full permission for this to be crossposted to other places and shared in any way, assuming references to the original post or the original authors are included.</i></p>", "user": {"username": "LRudL"}}, {"_id": "G6EXYYNp6KwageGZq", "title": "Stress Externalities More in AI Safety Pitches", "postedAt": "2022-09-26T20:31:51.611Z", "htmlBody": "<p>It is important to figure out the best way(s) to convince people that AI safety is worth taking seriously because despite the fact that it is (in my opinion, and in the opinion of many people in the EA community) the most important cause area, it often seems weird to people at first glance. &nbsp;I think that one way to improve the persuasiveness of AI safety pitches would be to use the frame that AI safety &nbsp;is a problem because the profit-based incentives of private sector AI developers do not account for the externalities generated by risky AGI projects.</p><p>Many groups that EA pitches to are relatively left leaning. &nbsp;In particular, elite university students are much more left leaning than the general population. &nbsp;As such, they are likely to be receptive to arguments for taking AI safety seriously which highlight the fact that AI safety is a problem largely due to deeper problems with capitalism. &nbsp;One such problem is the fact that capitalism fails to take into account externalities, or effects of economic activity which are not reflected in that activity's price.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu9cfcyigfll\"><sup><a href=\"#fnu9cfcyigfll\">[1]</a></sup></span>&nbsp; Developing AGI generates huge negative externalities; while a private sector actor who creates aligned AGI would probably reap much of the economic gains from it (at least in the short term - it is unclear how these gains would be distributed over longer time scales), it would pay only a small fraction of the costs of unaligned AGI, which are almost entirely borne by the rest of the world and future generations. &nbsp; Thus, misalignment risks from AGI are significantly heightened by the structural failure of capitalism to account for externalities, a problem which left leaning people tend to be very mindful of. &nbsp;Even beyond left-leaning students, it is widely acknowledged by educated people with an understanding of economics that a major problem with capitalism is that it fails by default to deal with externalities. &nbsp;Similarly, many people in the general public view corporations and big tech as irresponsible, greedy actors who harm the public good even if they lack an understanding of the logic of externalities. &nbsp;Thus, in addition to being particularly persuasive to left-leaning people who understand externalities, this framing seems likely to also be persuasive to people with a wider range of political orientations and levels of understanding of economics.</p><p>While this argument does not imply that misaligned AGI constitutes an existential risk, when it is combined with the claim that AI systems will have large impacts of some kind on the future (which many who are skeptical of AI x-risk still believe), it implies that we will by default significantly underinvest in ensuring that the AI systems which will shape the future will have positive effects on society. &nbsp;This conclusion seems likely to make people broadly more concerned about the negative effects of AI. &nbsp;Moreover, even if they do not conclude that AI development could pose an existential risk, the argument still implies that AI safety research constitutes a public good which should receive much more funding and attention than it currently does. &nbsp;Given that it seems to me like alignment research focused on preventing existential catastrophe seems highly related to broader efforts to ensure future AI systems have positive effects on the world, having more people believe the previous claim seems quite good.</p><p>As a result, it seems like \"AI safety is a public good which will be underinvested in by default\" or (more polemically) \"AI developers are gambling with the fate of humanity for the sake of profit, and we need to stop them/ensure that their efforts don't have catastrophic effects\" should be a more common frame used to pitch the importance of AI safety. &nbsp;It is an accurate and rhetorically effective framing of the problem. &nbsp;Am I missing something?</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu9cfcyigfll\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu9cfcyigfll\">^</a></strong></sup></span><div class=\"footnote-content\"><p>For a longer explanation of externalities, see <a href=\"https://www.imf.org/external/pubs/ft/fandd/basics/external.htm\">https://www.imf.org/external/pubs/ft/fandd/basics/external.htm</a></p></div></li></ol>", "user": {"username": "NickGabs"}}, {"_id": "dZQwTi7Y5napymuCh", "title": "Sprinting & Marathoning: Two Strategies for Volunteering your Time", "postedAt": "2022-09-27T15:02:56.800Z", "htmlBody": "<figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_110 110w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_220 220w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_550 550w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_660 660w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_770 770w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_880 880w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_990 990w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/cea9a472176979c7e14f2f938d748e0667f684cb0ddd3881.png/w_1011 1011w\"><figcaption>Thank you DALLE2</figcaption></figure><p><i>I recommend reading this post alongside Khorton's </i><a href=\"https://forum.effectivealtruism.org/posts/ScLHyCY6JCr5FtuiY/effective-volunteering\"><i><u>Volunteering</u></i></a><i> guide, and Aaron's </i><a href=\"https://forum.effectivealtruism.org/posts/6uXZc2QqHXsHKaLy9/volunteering-isn-t-free#Screening\"><i><u>volunteering isn't free</u></i></a><i>.&nbsp;</i></p><h1>Sprints</h1><p>In product development, a sprint is a short, time-boxed period when a team works to complete a set amount of work. Sprints are useful because you set yourself a goal and complete it, even if it's not perfect. Some benefits of volunteering in sprints include:</p><ul><li>You can quickly get onboarded to a new organisation and can make progress on specific, well-defined projects</li><li>You can get the bulk of a project done quickly (80/20) and speed up timelines for organisations</li><li>It can be a good learning experience or resume item for you&nbsp;</li><li>It\u2019s motivating to complete a project</li></ul><p>Sprint-able projects include:</p><ul><li>Writing an article or report on a specific topic</li><li>Doing a software migration or setting up a new process for an organisation</li><li>Running a one-off event or activity</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#e8f4ea\">Over the summer of 2019, a group of primarily volunteers (including me) helped launch <a href=\"https://forum.effectivealtruism.org/posts/7LvwHJzhu93SAf8Hc/abundant-resources-on-the-effective-altruism-resource-hub\">the EA Hub Resources</a> (now <a href=\"https://resources.eagroups.org)\">EA Groups Resources</a>). Without the volunteers who sprinted, I estimate the project would have taken ~several months longer to publish.&nbsp;</td></tr></tbody></table></figure><h1>Marathons</h1><p>A marathon means working on a project for a longer time, but working less intensely. On the other hand, marathoning can be incredibly valuable for long-term stability in an organisation. Having ongoing volunteers reduces recruiting, onboarding &amp; <a href=\"https://forum.effectivealtruism.org/posts/6uXZc2QqHXsHKaLy9/volunteering-isn-t-free#Turnover\"><u>turnover</u></a> costs for organisations. Some benefits of marathon volunteering include:</p><ul><li>Ongoing support for smaller tasks can be valuable for organisations</li><li>&nbsp;Low time commitment</li><li>You can build an ongoing relationship to the organisation</li></ul><p>&nbsp;Marathon-able projects include:</p><ul><li>Systems which require minimal maintenance (like approving new users for a site)</li><li>Small development tasks like tweaking an existing feature or configuring a system</li><li>Creating social <a href=\"https://en.wiktionary.org/wiki/Schelling_point\">schelling points</a> by hosting recurring events, like picnics or dinners</li></ul><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#e8f4ea\">On the <a href=\"https://eahub.org\">EA Hub</a> team, we had one volunteer developer worked with us for over a year. They were available a few hours week and would take on small projects. Although they weren't able to take on developing entirely new features or do overhauls of the system, they were able to provide support to the lead developers over time fairly reliably. &nbsp;</td></tr></tbody></table></figure><h1>A Sprint + Marathon Combo</h1><p>Some projects require a sprint to set them up, but then only a couple hours a week or month to maintain them. These projects can have <a href=\"https://forum.effectivealtruism.org/posts/hDFfqYCoY5tDsceLJ/doing-good-easier-how-to-have-passive-impact\">passive impact</a> with minimal effort.</p><figure class=\"table\"><table><tbody><tr><td style=\"background-color:#e8f4ea\">The <a href=\"https://pineappleoperations.org\">Pineapple Operations</a> public directory of operations/PA talent has some set-up costs (e.g. initial publicizing, creating automations), but in the long-term the main maintenance costs will be reviewing the directory periodically to remove any spam users.&nbsp;</td></tr></tbody></table></figure><hr><p><i>Thanks to </i><a href=\"https://forum.effectivealtruism.org/users/aaron-gertler\"><i>Aaron Gertler</i></a><i> and </i><a href=\"https://forum.effectivealtruism.org/users/amber\"><i>Amber Dawn</i></a><i> for feedback.</i></p>", "user": {"username": "vaidehi_agarwalla"}}]