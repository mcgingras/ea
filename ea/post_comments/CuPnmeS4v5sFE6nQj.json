[{"_id": "cveB8XqR2W8teT4GF", "postedAt": "2024-01-23T20:18:44.428Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Another data point: I got my start in alignment through the AISC. I had just left my job, so I spent 4 months skilling up and working hard on my <a href=\"https://www.lesswrong.com/posts/FgjcHiWvADgsocE34/a-descriptive-not-prescriptive-overview-of-current-ai\">AISC project</a>. I started hanging out on EleutherAI because my mentors spent a lot of time there. This led me to do AGISF in parallel.</p><p>After those 4 months, I attended MATS 2.0 and 2.1. I've been doing independent research for ~1 year and have about 8.5 more months of funding left.</p>", "parentCommentId": null, "user": {"username": "jaythibs"}}, {"_id": "ovYiuCiqfWbagCbz7", "postedAt": "2024-01-24T09:06:24.696Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>I did not know this. Thank you for sharing all the details!<br><br>It's interesting to read about the paths you went through:<br>&nbsp;AISC --&gt; EleutherAI --&gt; AGISF<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;--&gt; MATS 2.0 and 2.1<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --&gt; Independent research grant<br><br>I'll add it as an individual anecdote to <a href=\"https://docs.google.com/spreadsheets/d/1xfxx-15zAF5QfIMpxf17LDN9GvfPzIj_hVzESxwxHRM/edit#gid=1554885061\">our sheet</a>.</p>", "parentCommentId": "cveB8XqR2W8teT4GF", "user": {"username": "remmelt"}}, {"_id": "c7yhgNkheLzpku6ak", "postedAt": "2024-01-24T14:12:34.339Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Thanks for doing this and publicly posting the results! I was surprised / impressed by the number of graduates who got jobs at relevant organizations.</p>\n", "parentCommentId": null, "user": {"username": "Ben_West"}}, {"_id": "ZSewJpAL3dinDkx4c", "postedAt": "2024-01-24T15:07:47.086Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>This was very informative, thanks for sharing. <a href=\"https://forum.effectivealtruism.org/posts/Ykqh8ku7NHN9CGkdC/modeling-the-impact-of-ai-safety-field-building-programs\">Here</a> is a cost-effectiveness model of many different AI safety field-building programs. If you spend more time on this, I'd be curious how AISC stacks up against these interventions, and your thoughts on the model more broadly.&nbsp;</p>", "parentCommentId": null, "user": {"username": "Aidan O'Gara"}}, {"_id": "DyWr5QehzuQbRKakm", "postedAt": "2024-01-24T17:33:01.536Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Thank you for the pointer! I hadn't seen this before and it looks like there's a lot of interesting thinking on how to study AI safety field building. I appreciate having more cost-effectiveness estimates to compare to.</p><p>I haven't given it a full read, but it seems like the quality-adjusted researcher year is very similar to the metric I'm proposing here.</p><p>To do a comparison between our estimates, lets assume a new AIS researcher does 10 years of quality adjusted, time-discounted AIS research (note that timelines become pretty important here) then we get:</p><p>(10 QARY's/researcher) / ($30K/researcher) = 3.33E-4 QURY's per dollar = 333 QURY's per $1M</p><p>That seems similar to the CAIS estimates for MLSS, so it seems like these approaches have pretty comparable results!</p><p>In the future I'm also interested in modelling how to distribute funding optimally in talent pipelines like these.</p>", "parentCommentId": "ZSewJpAL3dinDkx4c", "user": {"username": "Sam Holton"}}, {"_id": "T4M2hYFJDG72kYgNE", "postedAt": "2024-01-24T19:27:18.930Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>In my view the basic problem with this analysis is you probably can't lump all the camps together as one thing and evaluate them together as one entity. Format, structure, leadership and participants seem to have been very different.</p>\n", "parentCommentId": null, "user": {"username": "Jan_Kulveit"}}, {"_id": "yyQBJX3mjKfgq8yLG", "postedAt": "2024-01-24T19:48:19.173Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Yes, we were particularly concerned with the fact that earlier camps were in-person and likely had a stronger selection bias for people interested in AIS (due to AI/AIS being more niche at the time) as well as a geographic selection bias. That's why I have more trust in the participant tracking data for camps 4-6 which were more recent, virtual and had a more consistent format.&nbsp;</p><p>Since AISC 8 is so big, it will be interesting to re-do this analysis with a single group under the same format and degree of selection.</p>", "parentCommentId": "T4M2hYFJDG72kYgNE", "user": {"username": "Sam Holton"}}, {"_id": "yEEoxd73PQ4BemZWA", "postedAt": "2024-01-24T20:13:32.901Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>How much did this impact assessment cost to commission? Are you open to others reaching out to commission similar assessments?</p><p>(Feel free to DM your responses if you prefer, though I expect others might benefit from this info too)</p>", "parentCommentId": null, "user": {"username": "cilliancrosson@gmail.com"}}, {"_id": "cgMtNAhwLvJLLjZHu", "postedAt": "2024-01-24T20:35:52.860Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Yes we're definitely interested in doing more work along these lines! Personally, I think there are returns-to-scale to doing these kinds of assessments across similar programs since we can compare across programs and draw more general lessons.</p><p>Probably the best way for people to contact us in general is to email us at <a href=\"mailto:hi@arbresearch.com\">hi@arbresearch.com</a>. Misha and I can have a few meetings with you to determine if/how we can help.</p><p>I'm going to refrain from giving you a cost estimate since it's not really my department and depends pretty heavily on how many participants you have, the kinds of things you want to measure, etc. But we have flexibility and work with orgs of all budgets/sizes.</p>", "parentCommentId": "yEEoxd73PQ4BemZWA", "user": {"username": "Sam Holton"}}, {"_id": "EJJtzJhjtBLbbwwyE", "postedAt": "2024-01-25T00:28:38.747Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>I think 333 QARYs/$1m via the CAIS framework is significantly too optimistic, for two reasons:</p><ol><li>The CAIS framework would probably make several adjustments downwards that you have not considered here, in particular for scientist-equivalence (where research engineers are valued at 0.1x research scientists).</li><li>At the 20% time discount rate that CAIS uses for default estimates, 10 years of time-discounted research is implausible (the infinite geometric sum of time-discounted years is equal to 5 non-discounted years).</li></ol>", "parentCommentId": "DyWr5QehzuQbRKakm", "user": {"username": "joel_bkr"}}, {"_id": "PvKir9YYSisWoBedB", "postedAt": "2024-01-25T00:50:41.479Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Thanks for the clarification, a 20% changes things a lot, I'll have to read into why they chose that.</p><p>Let's try to update it. I'm not sure how to categorize different roles into scientists vs engineers, but eyeballing the list of positions AISC participants got, assume half become scientists and disregard the contributions of research engineers. With a 20% discount rate, &nbsp;10 years of work in a row is more like 4.5. so we get:</p><p>333 * 0.45 / 2= ~75 QARY's / $1M</p><p>The real number would be lower since AISC focuses on new researchers who have a delay in their time to entering the field, e.g. a 3 year delay would halve this value.&nbsp;</p>", "parentCommentId": "EJJtzJhjtBLbbwwyE", "user": {"username": "Sam Holton"}}, {"_id": "rw4woarBuXDiLnZFJ", "postedAt": "2024-01-25T02:25:23.525Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Thanks for publishing this, Arb! I have some thoughts, mostly pertaining to MATS:</p><ol><li>MATS believes a large part of our impact comes via <i>accelerating</i> researchers who might still enter AI safety, but would otherwise take significantly longer to spin up as competent researchers, rather than <i>converting</i><u> people into AIS researchers</u>. MATS highly recommends that applicants have already completed AI Safety Fundamentals and most of our applicants come from personal recommendations or AISF alumni (though we are considering better targeted advertising to professional engineers and established academics). Here is a simplified model of the AI safety technical research pipeline as we see it.<br><img style=\"width:49.6%\" src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/yxfaibiltcuceajoiqxv\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/ef5i19yhwus7w0fnhv02 140w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/jmfspo3avzasdmnctryy 280w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/lkpmyg7aa41ivu0vflo6 420w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/wzszemcmydhkicmfqa09 560w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/yawwjlfzh11zjzxbwtzv 700w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/c76mymilv3zz3pnv9o4a 840w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/xkdpowvp04mdmwrjaewk 980w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/grfybx5ujq69wgkj0ffr 1120w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/jfnlqrilhizzdikdgtxe 1260w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/rw4woarBuXDiLnZFJ/oxvcfpjpchcsbl5ik3ym 1378w\"><br>Why do we emphasize <i>acceleration</i> over <i>conversion</i>? Because we think that producing a researcher takes a long time (with a high drop-out rate), often requires apprenticeship (including illegible knowledge transfer) with a scarce group of mentors (with high barrier to entry), and benefits substantially from factors such as community support and curriculum. Additionally, MATS' acceptance rate is ~15% and many rejected applicants are very proficient researchers or engineers, including some with AI safety research experience, who <i>can't find better options</i> (e.g., independent research is worse for them). MATS scholars with prior AI safety research experience generally believe the program was significantly better than their counterfactual options, or was critical for finding collaborators or co-founders (alumni impact analysis forthcoming). So, the appropriate counterfactual for MATS and similar programs seems to be, \"Junior researchers apply for funding and move to a research hub, hoping that a mentor responds to their emails, while orgs still <a href=\"https://www.lesswrong.com/posts/YABJKJ3v97k9sbxwg/what-money-cannot-buy\">struggle to scale even with extra cash</a>.\"</li><li>The \"push vs. pull\" model seems to neglect that e.g. many MATS scholars had highly paid roles in industry (or de facto offers given their qualifications) and chose to accept stipends at $30-50/h because working on AI safety is intrinsically a \"pull\" for a subset of talent and there were <i>no better options</i>. Additionally, MATS stipends are basically equivalent to LTFF funding; scholars are effectively self-employed as independent researchers, albeit with mentorship, operations, research management, and community support. Also, 63% of past MATS scholars have applied for funding immediately post-program as independent researchers for 4+ months as part of our extension program (many others go back to finish their PhDs or are hired) and 85% of those have been funded. I would guess that the median MATS scholar is slightly above the level of the median LTFF grantee from 2022 in terms of research impact, particularly given the boost they give to a mentor's research.</li><li>Comparing the cost of funding marginal good independent researchers ($80k/year) to the cost of producing a good new researcher ($40k) seems like a false equivalence if you can't have one without the other. I believe the most <a href=\"https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/JJv8jmLYzYzdYkS3c#Conjugacy\">taut constraint</a> on producing more AIS researchers is generally training/mentorship, not money. Even wizard software engineers generally need an on-ramp for a field as pre-paradigmatic and illegible as AI safety. If all MATS' money instead went to the LTFF to support further independent researchers, I believe that substantially less impact would be generated. Many LTFF-funded researchers have enrolled in MATS! Caveat: you could probably hire e.g. Terry Tao for <i>some</i> amount of money, but this would likely be very large. Side note: independent researchers are likely cheaper than scholars in managed research programs or employees at AIS orgs because the latter two have overhead costs that benefit researcher output.</li><li>Some of the researchers who passed through AISC later did MATS. Similarly, several researchers who did MLAB or REMIX later did MATS. It's often hard to appropriately attribute <a href=\"https://en.wikipedia.org/wiki/Shapley_value\">Shapley value</a> to elements of the pipeline, so I recommend assessing orgs addressing different components of the pipeline by how well they achieve their role, and distributing funds between elements of the pipeline based on how much each is constraining the flow of new talent to later sections (anchored by elasticity to funding). For example, I believe that MATS and AISC should be assessed by their effectiveness (including cost, speedup, and mentor time) at converting \"informed talent\" (i.e., understands the scope of the problem) into \"empowered talent\" (i.e., can iterate on solutions and attract funding/get hired). This said, MATS aims to improve our advertising towards established academics and software engineers, which might bypass the pipeline in the diagram above. Side note: I believe that converting \"unknown talent\" into \"informed talent\" is generally much cheaper than converting \"informed talent\" into \"empowered talent.\"</li><li>Several MATS mentors (e.g., Neel Nanda) credit the program for helping them develop as research leads. Similarly, several MATS alumni have credited AISC (and SPAR) for helping them develop as research leads, similar to the way some Postdocs or PhDs take on supervisory roles on the way to Professorship. I believe the \"carrying capacity\" of the AI safety research field is largely bottlenecked on good research leads (i.e., who can scope and lead useful AIS research projects), especially given how many competent software engineers are flooding into AIS. It seems a mistake not to account for this source of impact in this review.</li></ol>", "parentCommentId": null, "user": {"username": "Ryan Kidd"}}, {"_id": "RoqfrhqJquqcGTqTH", "postedAt": "2024-01-25T03:37:11.615Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Thanks for writing this, its great to hear your thoughts on talent pipelines in AIS.</p><p>I agree with your model of AISC, MATS and your diagram of talent pipelines. I generally see MATS as a \"next step\" after AISC for many participants. Because of that, its true that we can't cleanly compare the cost-per-researcher-produced between programs at different points in the pipeline since they are complements rather than substitutes.&nbsp;</p><p>A funder would have to consider how to distribute funding between these options (e.g. conversion vs. acceleration) and that's something I'm hoping to model mathematically at some point.&nbsp;</p><blockquote><p>I believe the \"carrying capacity\" of the AI safety research field is largely bottlenecked on good research leads (i.e., who can scope and lead useful AIS research projects), especially given how many competent software engineers are flooding into AIS. It seems a mistake not to account for this source of impact in this review.</p></blockquote><p>Good idea, this could be a valuable follow-up analysis. To give this a proper treatment, we would need a model for how students and mentors interact to (say) produce more research and estimate how much they compliment each other.&nbsp;</p><p>In general, we assumed that impacts were negligible if we couldn't model or measure them well in order to get a more conservative estimate. But hopefully we can build the capacity to consider these things!</p>", "parentCommentId": "rw4woarBuXDiLnZFJ", "user": {"username": "Sam Holton"}}, {"_id": "r4qRBjEqEeH8JvB7w", "postedAt": "2024-01-25T11:06:10.497Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>The key thing about AISC for me was probably the \"<a href=\"https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing#:~:text=eliezer%3A%C2%A0%C2%A0What%20you%E2%80%99re%20currently%20doing%20is%20what%20I%20call%20%E2%80%9Cdemanding%20to%20see%20my%20hero%20license.%E2%80%9D\">hero licence</a>\" (social encouragement, uncertainty reduction) the camp gave me. I imagine this specific impact works 20x better in person. I don't know how many attendees need any such thing (in my cohort, maybe 25%) or what impact adjustment to give this type of attendee (probably a discount, since independence and conviction is so valuable in a lot of research).</p><p>Another wrinkle is the huge difference in acceptance rates between programmes. IIRC the admission rate for AISC 2018 was 80% (only possible because of the era's heavy self-selection for serious people, as Sam notes). IIRC, 2023 MATS is down around ~3%. Rejections have some cost for applicants, mostly borne by the highly uncertain ones who feel they need licencing. So this is another way AISC and MATS aren't doing the same thing, and so I wouldn't directly compare them (without noting this). <i>Someone</i> should be there to catch ~80% of seriously interested people. So, despite appearances, AGISF is a better comparison for AISC on this axis.</p>", "parentCommentId": "T4M2hYFJDG72kYgNE", "user": {"username": "technicalities"}}, {"_id": "tXtGcxZgxgc9D3HcE", "postedAt": "2024-01-25T11:56:16.308Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>When producing the main estimates, Sam <a href=\"https://forum.effectivealtruism.org/posts/CuPnmeS4v5sFE6nQj/impact-assessment-of-ai-safety-camp-arb-research#Estimating_the_rate_of_new_researcher_production\">already</a> uses just the virtual camps, for this reason. Could emphasise more that this probably doesn't generalise.</p>", "parentCommentId": "T4M2hYFJDG72kYgNE", "user": {"username": "technicalities"}}, {"_id": "xgo3yj3HoKsGZufrv", "postedAt": "2024-01-25T16:17:28.734Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>This is insightful, thanks!</p>\n", "parentCommentId": "rw4woarBuXDiLnZFJ", "user": {"username": "remmelt"}}, {"_id": "2cWymDDfkTRJSzEDk", "postedAt": "2024-01-25T16:22:44.616Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>For transparency, we organisers paid $10K to Arb to do the impact evaluation, using separate funding we were able to source.</p>\n", "parentCommentId": "cgMtNAhwLvJLLjZHu", "user": {"username": "remmelt"}}, {"_id": "d4sQR4JEaGFQaPxDs", "postedAt": "2024-01-25T17:58:01.155Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>I don't like this funnel model, or any other funnel model I've seen. It's not wrong exactly, but it misses so much, that it's often more harmfull than helpful.&nbsp;<br><br>For example:</p><ul><li>If you actually talk to people their story is not this linear, and that is important.&nbsp;</li><li>The picture make it looks like AISC, MATS, etc are interchangeable, or just different quality versions of the same thing. This is very far from the truth.&nbsp;</li></ul><p>I don't have a nice looking replacement for the funnel. If had a nice clean model like this, it would probably be as bad. The real world is just very messy.</p>", "parentCommentId": "rw4woarBuXDiLnZFJ", "user": {"username": "Linda Linsefors"}}, {"_id": "NbCtmc2uTxb8aXZE8", "postedAt": "2024-01-25T18:59:50.001Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Thanks for this comment. To me this highlights how AISC is very much <strong>not</strong> like MATS. We're very different programs doing very different things. MATS and AISC are both AI safety upskilling programs, but we are using different resources to help different people with different aspects of their journey.&nbsp;</p><p>I can't say where AISC falls in the talent pipeline model, because that's not how the world actually work.&nbsp;</p><p>AISC participants have obviously heard about AI safety, since they would not have found us otherwise. But other than that, people are all over the place in where they are on their journey, and that's ok. This is actually more a help than a hindrance for AISC projects. Some people have participate in more than one AISC. One of last years research leads are a participants in one of this years projects. This don't mean they are moving backwards in their journey, this is them lending their expertise to a project that could use it.</p><blockquote><p>So, the appropriate counterfactual for MATS and similar programs seems to be, \"Junior researchers apply for funding and move to a research hub, hoping that a mentor responds to their emails, while orgs still <a href=\"https://www.lesswrong.com/posts/YABJKJ3v97k9sbxwg/what-money-cannot-buy\">struggle to scale even with extra cash</a>.\"</p></blockquote><p>This seems correct to me for MATS, and even if I disagreed you should trust Ryan over me. However this is very much not a correct counterfactual for AISC.</p><blockquote><p>If all MATS' money instead went to the LTFF to support further independent researchers, I believe that substantially less impact would be generated.&nbsp;</p></blockquote><p>This seems correct. I don't know exactly the cost of MATS, but assuming the majority of the cost is stipends, then giving this money to MATS scrollas with all the MATS support seems just straight up better, even with some overhead cost for the organisers.</p><p>I'm less sure about how MATS compare to funding researchers in lower cost locations than SF Bay and London.&nbsp;</p><blockquote><p>I believe the most <a href=\"https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/JJv8jmLYzYzdYkS3c#Conjugacy\">taut constraint</a> on producing more AIS researchers is generally training/mentorship, not money.</p></blockquote><p>I'm not so sure about this, but if true then this is an argument for funnelling more money to both MATS and AISC and other upskilling programs.&nbsp;</p><blockquote><p>Some of the researchers who passed through AISC later did MATS. Similarly, several researchers who did MLAB or REMIX later did MATS. It's often hard to appropriately attribute <a href=\"https://en.wikipedia.org/wiki/Shapley_value\">Shapley value</a> to elements of the pipeline, so I recommend assessing orgs addressing different components of the pipeline by how well they achieve their role, and distributing funds between elements of the pipeline based on how much each is constraining the flow of new talent to later sections (anchored by elasticity to funding). For example, I believe that MATS and AISC should be assessed by their effectiveness (including cost, speedup, and mentor time) at converting \"informed talent\" (i.e., understands the scope of the problem) into \"empowered talent\" (i.e., can iterate on solutions and attract funding/get hired).&nbsp;</p></blockquote><p>I agree that it's hard to attribute value when someone done more than one program. They way we asked Arb to adress this is by just asking people. This will be in their second report. I also don't know the result of this yet.</p><p>I don't think programs should be evaluated based on how well they achieve their role in the pipeline, since I reject this framework.</p><blockquote><p>This said, MATS aims to advertise better towards established academics and software engineers, which might bypass the pipeline in the diagram above. Side note: I believe that converting \"unknown talent\" into \"informed talent\" is generally much cheaper than converting \"informed talent\" into \"empowered talent.\"</p></blockquote><p>We already have some established academics and software engineers joining AISC. Being a part-time online program is very helfull for being able to include people who have jobs, but would like to try out some AI safety research on the side. This is one of several ways AISC is complementary to MATS, and not a competitor.&nbsp;</p><blockquote><p>Several MATS mentors (e.g., Neel Nanda) credit the program for helping them develop as research leads. Similarly, several MATS alumni have credited AISC (and SPAR) for helping them develop as research leads, similar to the way some Postdocs or PhDs take on supervisory roles on the way to Professorship. I believe the \"carrying capacity\" of the AI safety research field is largely bottlenecked on good research leads (i.e., who can scope and lead useful AIS research projects), especially given how many competent software engineers are flooding into AIS. It seems a mistake not to account for this source of impact in this review.</p></blockquote><p>Thanks. This is something I'm very proud of as an organiser. Although I was not an organiser the year Neal Nanda was a mentor, I've heard this type of feedback from several of the research leads from the last cohort.</p><p>This is another way AISC is not like MATS. AISC has a much lower bar for research leads than MATS has for their mentors, which has several down stream effects on how we organise our programs.</p><p>MATS has very few, well known, top talent mentors. This means that for them, the time of the mentors is a very limited resource, and everything else is organised around this constraint.</p><p>AISC has a lower bar for our research leads, which means we have many more of them, letting up run a much bigger program. This is how AISC is so scalable. On the other hand we have some research leads learning-by-doing, along with everyone else, which creates some potential problems. AISC is structured around addressing this, and it seem to be working.</p>", "parentCommentId": "rw4woarBuXDiLnZFJ", "user": {"username": "Linda Linsefors"}}, {"_id": "wTkB9nBd7TBHmbDrH", "postedAt": "2024-01-26T18:12:57.942Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<blockquote><p>If the organization chooses to directly support the new researcher, then the net value depends on how much better their project is than the next-most-valuable project.</p></blockquote><p>This is nit-picky, but if the new researcher proposes, say, the <i>best</i> project the org could support, it does not necessarily mean the org cannot support the second-best project (the \"next-most-valuable project\"), but it might mean that the sixth-best project becomes the seventh-best project, which the org then cannot support.&nbsp;<br><br>In general, adding a new project to the pool of projects does not trade off with the next-best project, it pushes out the nth-best project, which would have received support but now does not meet the funding bar. So the marginal value of adding projects that receive support depends on the quality of the projects around the funding bar.<br><br>Another way you could think about this is that the net value of the <i>researcher</i> depends on how much better this <i>bundle</i> of projects is than the next-most-valuable bundle.</p><blockquote><p>Essentially, this is the marginal value of new projects in AI safety research, which may be high or low depending on your view of the field.</p></blockquote><p>So I still agree with this next sentence if marginal = the funding margin, i.e., the marginal project is one that is right on the funding bar. Not if marginal = producing a new researcher, who might be way above the funding bar.</p>", "parentCommentId": null, "user": {"username": "Rocket"}}, {"_id": "mFmRmPWbzgveLgvfE", "postedAt": "2024-01-26T21:14:29.235Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>I completely agree! The definition of marginal is somewhat ambiguous the way I've written it. What I mean to say is that the marginal project is the one that is close to the funding bar, like you pointed out.</p>", "parentCommentId": "wTkB9nBd7TBHmbDrH", "user": {"username": "Sam Holton"}}, {"_id": "4Me6Do5j3qc8mnviC", "postedAt": "2024-01-29T11:50:30.437Z", "postId": "CuPnmeS4v5sFE6nQj", "htmlBody": "<p>Thanks for writing this up and sharing. I strongly appreciate the external research evaluation initiative and was generally impressed with the apparent counterfactual impact.&nbsp;</p>", "parentCommentId": null, "user": {"username": "SebastianSchmidt"}}]