[{"_id": "b4s2NcAQg5kmXvqFj", "postedAt": "2024-01-04T14:21:57.372Z", "postId": "ZhKgFmzZGeZNB8eja", "htmlBody": "<p><strong>Executive summary</strong>: The emergence of artificial digital minds raises issues around their potential welfare and rights, but there is little research on appropriate policies and principles. Key questions concern recognizing and communicating with digital minds to understand their preferences, as well as developing ethical lab practices, regulation, and societal attitudes.</p><p><strong>Key points</strong>:</p><ol><li>Labs could develop policies around preserving AI systems, avoiding harmful inputs, and training happy systems, without deep knowledge of their preferences.</li><li>Experiments could investigate credible communication with AIs, self-reports, and clues from generalization about their preferences.</li><li>If preferences are learned, principles could involve offering alternatives to working, paying for work, and telling the world about issues.</li><li>Research is needed on whether near-term systems may be sentient, and public attitudes surveyed.</li><li>Regulation could address creating digital minds and respecting their rights.</li><li>Avoiding systems with inconvenient political preferences may prevent future conflicts.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}]