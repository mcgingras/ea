[{"_id": "PtrKQGA8qyjCt8Lo6", "postedAt": "2022-10-10T10:58:20.642Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>(I've only skimmed the post)</p><p>This seems right theoretically, but I'm &nbsp;worried that people will read this and think this consideration ~conclusively implies fewer people should go into AI alignment, when my current best guess is the opposite is true. I agree sometimes people make the argmax vs. softmax mistake and there are status issues, but I still think not enough people proportionally go into AI for various reasons (underestimating risk level, it being hard/intimidating, not liking rationalist/Bay vibes, etc.).</p>", "parentCommentId": null, "user": {"username": "elifland"}}, {"_id": "3GydGbdzjpuugqpAz", "postedAt": "2022-10-10T12:55:12.002Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>The above makes EA's huge investment in research seem like a better bet: \"do more research\" is a sort of exploration. Arguably we don't do enough <a href=\"https://forum.effectivealtruism.org/posts/by8u954PjM2ctcve7/experimental-longtermism-theory-needs-data\">active exploration</a> (learning by doing), but we don't want less research.</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "FKCisYAKbT47vzghW", "postedAt": "2022-10-10T13:07:44.525Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>In practice the thing that the EA community is doing is much closer to <a href=\"https://intelligence.org/files/QuantilizersSaferAlternative.pdf\">quantilization</a> (<a href=\"https://www.youtube.com/watch?v=gdKMG6kTl6Y\">video explanation</a>) than maximization anyway, and that's okay. The goal could be an ever-increasing <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"q\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;\">q</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span>.</p>\n", "parentCommentId": null, "user": {"username": "niplav"}}, {"_id": "62kwYvM9aa4kc4Dmd", "postedAt": "2022-10-10T13:17:48.757Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Mostly true, but a <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">string</a> of posts about the risks attests to there being some unbounded optimisers. (Or at least that we are at risk of having some.)</p>", "parentCommentId": "FKCisYAKbT47vzghW", "user": {"username": "technicalities"}}, {"_id": "HQfSd5nfkysXWwNgc", "postedAt": "2022-10-10T13:31:43.075Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Agree that this could be misused, just as the sensible 80k framework is misused, or as anything can be.</p><p>Some skin in the game then: me and Jan both spend most of our time on AI.</p>", "parentCommentId": "PtrKQGA8qyjCt8Lo6", "user": {"username": "technicalities"}}, {"_id": "TBvu5Kzhbq2Pck4Aw", "postedAt": "2022-10-10T13:33:05.507Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>I'm a bit confused if by 'fewer people' / 'not enough people proportionally' you mean 'EAs'. In my view, while too few people (as 'humans') work on AI alignment, too large fraction of EAs 'goes into AI'.</p>", "parentCommentId": "PtrKQGA8qyjCt8Lo6", "user": {"username": "Jan_Kulveit"}}, {"_id": "9dYNfxJKw9Xqi3dpt", "postedAt": "2022-10-10T13:39:06.378Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>I mean EAs. I\u2019m most confident about \u201ctalent-weighted EAs\u201d. But probably also EAs in general.</p>\n", "parentCommentId": "TBvu5Kzhbq2Pck4Aw", "user": {"username": "elifland"}}, {"_id": "5pDZeToQAqadcbnWf", "postedAt": "2022-10-10T13:39:33.176Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Thanks for clarifying. Might be worth making clear in the post (if it isn\u2019t already, I may have missed something).</p>\n", "parentCommentId": "HQfSd5nfkysXWwNgc", "user": {"username": "elifland"}}, {"_id": "Dt3dmT9HvhwDC7XyL", "postedAt": "2022-10-10T13:43:34.255Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>In particular, I think many of the epistemically best EAs go into stuff like grant making, philosophy, general longtermist research, etc. which leaves a gap of really epistemically good people focusing full-time on AI. And I think the current epistemic situation in the AI alignment field (both technical and governance) is pretty bad in part due to this.</p>\n", "parentCommentId": "9dYNfxJKw9Xqi3dpt", "user": {"username": "elifland"}}, {"_id": "JiXavAvdmHJMd6Wxy", "postedAt": "2022-10-10T14:12:21.591Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>See also the muti-armed bandit &lt;<a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\">https://en.wikipedia.org/wiki/Multi-armed_bandit</a>&gt; problem.</p>", "parentCommentId": null, "user": {"username": "NunoSempere"}}, {"_id": "5kmEGNLCCvdTu2Mmh", "postedAt": "2022-10-10T14:52:22.754Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Are there any principled probability assignments we could use? E.g., the probability that this would be my top choice after N further hours of investigation into it and alternatives (including realistically collecting data or performing experiments), maybe allowing N to be unrealistic?</p>\n<p>From my understanding, softmax is formally sensitive to parametrizations, so the specific outputs seem pretty unprincipled unless you actually have feedback and are doing some kind of optimization like minimizing some kind of softmax loss.</p>\n", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "cAW7Frcv4krBJSgxC", "postedAt": "2022-10-10T15:24:42.055Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>A couple of comments. &nbsp;</p><p>(1), I found this post quite hard to understand - it was quite jargon-heavy.&nbsp;</p><p>(2) I'd have appreciated it if you'd located this in what you take to be the relevant literature. I'm not sure if you're making an argument about (A) why you might want to diversify resources across various causes, even if certain in some moral view (for instance because there are diminishing marginal returns, so you fund option X up to some point and then switch to Y) &nbsp;or (B) why you might want to diversify because you are morally uncertain. &nbsp;</p><p>(3), because of (2), I'm not sure what your objection to 'argmax' is. You say 'naive argmax' doesn't work. But isn't that a reason to do 'non-naive argmax' rather than do something else? Cf. debates where people object to consequentialism by claiming it implies you ought to kill people and harvest their organs, and the consequentialist says that's naive and not actually what consequentialism would recommend.</p><p>Fwiw, the standard approaches to moral uncertainty ('my favourite theory' and 'maximise expected choiceworthiness') provide no justification <i>in themselves</i> for splitting your resources. In contrast, the 'worldview diversfication' approach does do this. You say that worldview diversification is <i>ad hoc</i>, but I think it can be justified by a non-standard approach to moral uncertainty, one I call 'internal bargaining' and have written about <a href=\"https://forum.effectivealtruism.org/posts/kxEAkcEvyiwmjirjN/wheeling-and-dealing-an-internal-bargaining-approach-to\">here</a>.&nbsp;</p>", "parentCommentId": null, "user": {"username": "MichaelPlant"}}, {"_id": "TbwfaFfKwB2RgG5Av", "postedAt": "2022-10-10T15:47:08.312Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>On the other hand, I can see even using the credences like I proposed to be way too upside-focused, i.e. focused on picking the best interventions, but little concern for avoiding the worst (badly net negative in EV) interventions. Consider an intervention that has a 55% chance of being the best and vastly net positive in expectation after further investigation, but a 45% chance of being the worst and vastly net negative in expectation (of similar magnitude), and your current overall belief is that it's vastly net positive and highest in EV. It's plausible some high-leverage interventions are sensitive in this way, because they involve tradeoffs for existential risks (tradeoffs between different x-risks, but also within x-risks, like differential progress), or, in the near-term, because of wild animal effects dominating and having uncertain sign. Would we really want to put the largest share of resources, let alone most resources, into such an intervention?</p><p>Alternatively, we may have multiple choices, among which three, A, B and C are such that, for some c&gt;0, after further investigation, we expect that:</p><ol><li>A is 40% likely to be the best, with EV = c, and 35% likely to be the worst, with EV=-c, and and the rest of the time EV=0.</li><li>B is 35% likely to be the best, with EV=c and 30% likely to be the worst, with EV=-c, and the rest of the time EV=0.</li><li>C is 5% likely to be the best, with EV=c, and otherwise has EV=0 and has probability 0 of being the worst.</li></ol><p>How should we weight our resources between these three (ignoring other options)? Currently, they all have the same overall EV (=5%*c). What if we increase the probability that A is best slightly, without changing anything else? Or, what if we increase the probability that C is best slightly, without changing anything else?</p>", "parentCommentId": "5kmEGNLCCvdTu2Mmh", "user": {"username": "MichaelStJules"}}, {"_id": "Kf9otGdLdy5icXq8X", "postedAt": "2022-10-10T16:24:15.695Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>A comment and then a question. One problem I've encountered in trying to explain ideas like this to a non-technical audience is that actually the standard &nbsp;rationales for 'why softmax' are either a) technical or b) not convincing or even condescending about its value as a decision-making approach. Indeed, the 'Agents as probabilistic programs' page you linked to introduces softmax as \"<i>People do not always choose the normatively rational actions. The softmax agent provides a simple, analytically tractable model of sub-optimal choice.</i>\" The 'Softmax demystified' page offers relatively technical reasons (smoothing is good, flickering bad) and an unsupported claim (it is good to pick lower utility options some of the time). Implicitly this makes presentations of ideas like this have the flavor of \"trust us, you should use this because it works in practice, even it has origins in what we think is irrational or that we can't justify\". And, to be clear, I say that as someone who's on your side, trying to think of how to share these ideas with others. I think there is probably a link between what I've described above and Michael Plant's point (3).<br><br>So, I'm wonder if 'we can do better' in justifying softmax (and similar approaches). What is the most convincing argument you've seen?&nbsp;<br><br>I feel like the holy grail would be an empirical demonstration that an RL agent develops softmax like properties across a range of realistic environments. And/or a theoretical argument for why this should happen.</p>", "parentCommentId": null, "user": {"username": "jh"}}, {"_id": "r4kRqh3omfwkaAtqB", "postedAt": "2022-10-10T17:01:32.024Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Do we have reason to believe softmax is a better approximation to \"Enlightened argmax\" than just directly trying to approximate Enlightened argmax or its outputs?</p>", "parentCommentId": null, "user": {"username": "MichaelStJules"}}, {"_id": "RQNdxdHqfLDy4xmvL", "postedAt": "2022-10-11T14:56:13.060Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Oh full disclosure I guess: I am a well-known shill for <a href=\"https://www.gleech.org/\">argmin</a>.</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "Dhmpkd3nRibbCFjMo", "postedAt": "2022-10-12T17:26:23.093Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>One justification might be that in an online setting where you have to learn which options are best from past observations, the naive \"follow the leader\" approach -- exactly maximizing your &nbsp;action based on whatever seems best so far -- is easily exploited by an adversary.&nbsp;</p><p>This problem resolves itself if you make actions more likely if they've performed well, but regularize a little to smooth things out. The most common regularizer is entropy, and then as described on the \"Softmax demystified\" page, you basically end up recovering softmax (this is the well-known \"multiplicative weight updates\" algorithm).</p>", "parentCommentId": "Kf9otGdLdy5icXq8X", "user": {"username": "anonymous6"}}, {"_id": "u6CGTBAdDbLQamg7r", "postedAt": "2022-10-12T22:24:13.419Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Animals do this intuitively:</p><blockquote><p>Pigeons were presented with two buttons in a Skinner box, each of which led to varying rates of food reward. The pigeons tended to peck the button that yielded the greater food reward more often than the other button, and the <strong>ratio of their rates to the two buttons matched the ratio of their rates of reward on the two buttons</strong>.&nbsp;</p></blockquote><p><a href=\"https://en.wikipedia.org/wiki/Matching_law\">Matching Law</a></p>", "parentCommentId": null, "user": {"username": "ChrisLakin"}}, {"_id": "tRxuyv6GNb83A2vNc", "postedAt": "2022-10-13T00:01:36.266Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Curious what people think of the argument that, given that people in the EA community have different rankings of the top causes, a close-to-optimal community outcome could be reached if individuals argmax using their own ranking?</p>\n<p>(At least assuming that the number of people who rank a certain cause as the top one is proportional to how likely it is to be the top one.)</p>\n", "parentCommentId": null, "user": {"username": "Buhl"}}, {"_id": "kjBJJCoYg7DeCbMR4", "postedAt": "2022-10-14T08:51:39.465Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>(1) The post attempts to skirt between being completely non-technical, and being very technical. It's unclear if successfully.<br><br>(2) The technical &nbsp;claim is mostly that <i>argmax(actions) &nbsp;</i>is a dumb decision procedure in the real world for boundedly rational agents, if the actions are not very meta.<br><br>Softmax is one of the more principled alternative choices (see eg <a href=\"https://michielstock.github.io/posts/2021/2021-03-20-softmax/\">here</a>)<br><br>(3) That argmax(actions) is not the optimal thing to do for boundedly rational agents is perhaps best illuminated by <a href=\"https://arxiv.org/abs/1512.06789\">information-theoretic bounded rationality</a>.&nbsp;<br><br>In my view the technical research useful for developing good theory of moral uncertainty for bounded agents in the real world is currently mostly located in other fields of research (ML, decision theory, AI safety, social choice theory, mechanism design, etc), so I would not expect lack of something in the moral uncertainty literature to be evidence of anything.<br>E.g., the internal bargaining you link is mostly simply OCB and HG applying bargaining theory to bargaining between moral theories.&nbsp;<br><br>We say worldview diversification is less <i>ad hoc</i> than the other things: worldview diversification is mostly Thompson sampling.&nbsp;<br><br>(4) You can often \"rescue\" some functional form if you really want. Love argmax()? Well, do argmax(ways how to choose actions) or something. &nbsp;Really attached to the label of utilitarianism, but in practice want to do something closer to virtues? Well, do utilitarianism but just on the of actions of the type \"select your next self\" or similar.</p>", "parentCommentId": "cAW7Frcv4krBJSgxC", "user": {"username": "Jan_Kulveit"}}, {"_id": "omiHNrokKti8RnNHD", "postedAt": "2022-10-15T22:50:33.217Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Booo &lt;3</p>", "parentCommentId": "RQNdxdHqfLDy4xmvL", "user": {"username": "NunoSempere"}}, {"_id": "rXrtrRnPg5gnjqXzP", "postedAt": "2022-10-16T10:18:05.442Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Seems like people <a href=\"https://twitter.com/YonatanCale/status/1579878011134148609\">agree</a> with you!</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_480 480w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_600 600w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_720 720w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_960 960w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/8c494a9ec827f9939088968da083697c62d9f81b47c95af1.png/w_1174 1174w\"></figure>", "parentCommentId": null, "user": {"username": "hibukki"}}, {"_id": "SFaMJjvzv7jjHxLQ9", "postedAt": "2022-10-18T18:25:09.761Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Interestingly, I have the opposite intuition, that entire subareas of EA/longtermism are kinda plodding along and not doing much because our best people keep going into AI alignment. Some of those areas are plausibly even critical for making the AI story go well.</p><p>Still, it's not clear to me whether the allocation is inaccurate, just because alignment is so important.</p><p>Technical biosecurity and <i>maybe</i> forecasting might be exceptions though.</p>", "parentCommentId": "Dt3dmT9HvhwDC7XyL", "user": {"username": "Linch"}}, {"_id": "rQgQHYz5qQZEGFthi", "postedAt": "2022-10-20T00:44:22.722Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Yes, and is there a proof of this that someone has put together? Or at least a more formal justification?</p>\n", "parentCommentId": "Dhmpkd3nRibbCFjMo", "user": {"username": "jh"}}, {"_id": "z3TFFzz5CecBgPkKg", "postedAt": "2022-10-20T11:12:26.923Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Also see Brian Christian briefly suggesting a cause allocation rule a bit like this towards the end of <a href=\"https://80000hours.org/podcast/episodes/brian-christian-algorithms-to-live-by/\">80k's interview with him</a>.</p><p>We were discussing solutions to the explore-exploit problem, and one is that you allocate resources in proportion to your credence the option is best.</p>", "parentCommentId": null, "user": {"username": "Benjamin_Todd"}}, {"_id": "KMKr93kZN8wmptuvH", "postedAt": "2022-10-20T11:50:36.635Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Upvoted, though I was struck by this part of the appendix:</p><blockquote><p>Appendix: Other reasons to diverge from argmax&nbsp;</p><p>In order of how much we endorse them:</p><ul><li><a href=\"https://forum.effectivealtruism.org/topics/value-of-information\"><u>Value of information</u></a> is usually incredibly high&nbsp;</li><li><a href=\"https://clearerthinkingpodcast.com/episode/121/#transcript\"><u>You don\u2019t know the whole option set</u></a></li><li><a href=\"https://www.rug.nl/filosofie/organization/news-and-events/events/2018-toel/hedging-our-bets\"><u>Moral uncertainty</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/GzJHagyuEbWdxDFjJ/concave-and-convex-altruism#Diversifying\"><u>Concave altruism</u></a> (i.e. Jensen\u2019s inequality!)</li><li><a href=\"https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it\"><u>The optimiser\u2019s curse</u></a></li><li><a href=\"https://www.openphilanthropy.org/research/worldview-diversification/\"><u>Worldview diversification</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/xAoZotkzcY5mvmXFY/longtermism-risk-and-extinction?commentId=e79ih6uaJdnds6GEM#comments\"><u>Principled risk aversion</u></a>,&nbsp;<a href=\"https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\"><u>as at GiveWell</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal#The_Leverage_Hypothesis\"><u>Strategic skulduggery</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/9pktesiW2WPEFNvCQ/uncorrelated-bets-an-easy-to-understand-and-very-important#comments\"><u>Decrease variance of your portfolio for more impact compounding</u></a>(?)&nbsp;</li></ul></blockquote><p>&nbsp;</p><p>While I totally agree with the the conclusion of the post (the community should have a portfolio of causes, and not invest everything in the top cause), I feel very unsure that a lot of these reasons are good ones for spreading out from the most promising cause.&nbsp;</p><p>Or if they do imply spreading out, they don't obviously justify the standard EA alternatives to AI Risk.</p><p>I noticed I felt like I was disagreeing with your reasons for not doing argmax throughout the post, and this list helped to explain why.</p><p>1. Starting with VOI, that assumes that you can get significant information about how good a cause is by having people work on it. In practice, a ton of uncertainty is about scale and neglectedness, and having people work on the cause doesn't tell you much about that. Global priorities research usually seems more useful.</p><p>VOI would also imply working on causes that <i>might</i> be top, but that we're very uncertain about. So, for example, that probably wouldn't imply that that longtermist-interested people should work on global health or factory farming, but rather spread out over lots of weirder small causes, like those listed here: <a href=\"https://80000hours.org/problem-profiles/#less-developed-areas\">https://80000hours.org/problem-profiles/#less-developed-areas</a></p><p>2. \"You don't know the whole option set\" sounds like a similar issue to VOI. It would imply trying to go and explore totally new areas, rather than working on familiar EA priorities.</p><p>3. Many approaches to moral uncertainty suggest that you factor in uncertainty in your choice of values, but then you just choose the best option with respect to those values. It doesn't obviously suggest supporting multiple causes.</p><p>4. Concave altruism. Personally I think there are increasing returns on the level of orgs, but I don't think there are significant increasing returns at the level of cause areas. (And that post is more about exploring the implications of concave altruism rather than making the case it actually applies to EA cause selection.)</p><p>5. Optimizer's curse. This seems like a reason to think your best guess isn't as good as you think, rather than to support multiple causes.</p><p>6. Worldview diversification. This isn't really an independent reason to spread out \u2013 it's just the name of Open Phil's approach to spreading out (which they believe for other reasons).</p><p>7. Risk aversion. I don't think we should be risk averse about utility, so agree with your low ranking of it.</p><p>8. &nbsp;Strategic skullduggery. This actually seems like one of the clearest reasons to spread out..</p><p>9. Decreased variance. I agree with you this is probably not a big factor.</p><p>&nbsp;</p><p>You didn't add diminishing returns to your list, though I think you'd rank it near the top. I'd also agree it's a factor, though I also think it's often oversold. E.g. if there are short-term bottlenecks in AI that create diminishing returns, it's likely the best response is to invest in career capital and wait for the bottlenecks to disappear, rather than to switch into a totally different cause. You also need big increases in resources to get enough diminishing returns to change the cause ranking e.g. if you think AI safety is 10x as effective as pandemics at the margin, you might need to see the AI safety community roughly 10x in size relative to biosecurity before they'd equalise.</p><p>&nbsp;</p><p>I tried to summarise what I think the good reasons for spreading out are <a href=\"https://80000hours.org/articles/your-choice-of-problem-is-crucial/#are-you-saying-everyone-should-work-on-the-top-issue\">here</a>.</p><p>For a longtermist, I think those considerations would suggest a picture like:</p><ul><li>50% into the top 1-3 issues</li><li>20% into the next couple of issues</li><li>20% into exploring a wide range of issues that <i>might</i> be top</li><li>10% into other popular issues</li></ul><p>If I had to list a single biggest driver, it would be personal fit / idiosyncratic opportunities, which can easily produce orders of magnitude differences in what different people should focus on.</p><p>The question of how to factor in neartermism (or other alternatives to AI-focused longtermism) seems harder. It could easily imply still betting everything on AI, though putting some % of resources into neartermism in proportion to your credence in it also seems sensible.&nbsp;</p><p>Some more here about how worldview diversification can imply a wide range of allocations depending on how you apply it: https://twitter.com/ben_j_todd/status/1528409711170699264</p>", "parentCommentId": null, "user": {"username": "Benjamin_Todd"}}, {"_id": "eAnmrXpFuzPncQeZr", "postedAt": "2022-10-20T11:54:08.950Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Also maybe of interest, I think the current EA portfolio is actually allocated pretty well in line with what this heuristic would imply:&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/nws5pai9AB6dCQqxq/how-are-resources-in-ea-allocated-across-issues#What_might_we_learn_from_this_\">https://forum.effectivealtruism.org/posts/nws5pai9AB6dCQqxq/how-are-resources-in-ea-allocated-across-issues#What_might_we_learn_from_this_</a></p><p>I think the bigger issue might be that it's currently demoralising not to work on AI or meta. So I appreciate this post as an exploration of ways to make it more intuitive that everyone shouldn't work on AI.</p>", "parentCommentId": null, "user": {"username": "Benjamin_Todd"}}, {"_id": "FfczEXK5DS2pmhcfn", "postedAt": "2022-10-20T13:02:50.429Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Here's one set of lecture notes (don't endorse that they're necessarily the best, just first I found quickly) <a href=\"https://lucatrevisan.github.io/40391/lecture12.pdf\">https://lucatrevisan.github.io/40391/lecture12.pdf</a></p><p>Keywords to search for other sources would be \"multiplicative weight updates\", \"follow the leader\", \"follow the regularized leader\".</p><p>Note that this is for what's sometimes called the \"experts\" setting, where you get full feedback on the counterfactual actions you didn't take. But the same approach basically works with some slight modification for the \"bandit\" setting, where you only get to see the result of what you actually did.</p>", "parentCommentId": "rQgQHYz5qQZEGFthi", "user": {"username": "anonymous6"}}, {"_id": "DfaJamZtqfjTBjtMX", "postedAt": "2022-10-22T14:09:30.559Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Looking back two weeks later, this post really needs</p><ul><li>to discuss of the cost of prioritisation (we use softmax because we are boundedly rational) and the Price of Anarchy;</li><li>to have separate sections for individual prioritisation and collective prioritisation;</li><li>to at least mention bandits and the Gittins index, which is optimal where softmax is highly principled suboptimal cope.</li></ul>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "u4a5GBZBDBGyKDyin", "postedAt": "2022-10-22T15:52:09.865Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>FWIW, I didn't get the impression there's a very principled justification for softmax in this post, if that's what you intended by \"highly principled\". That it might work better than naive argmax in practice on some counts isn't really enough, and there wasn't really much comparison to enlightened argmax, which is optimal in theory.</p>\n<p>I'd probably require being provably (approximately) optimal for a principled justification. Quickly checking bandits and the Gittins index on Wikipedia, bandits are general problems and the Gittins index is just the value of the aggregate reward. I guess you could say \"maximize Gittins index\" (use the Gittins index policy), but that's, imo, just a formal characterization of what enlightened argmax should be under certain problem assumptions, and doesn't provide much useful guidance on its own. Like what procedure should we follow to maximize the Gittins index? Is it just calculate really hard?</p>\n<p>Also, according to the Wikipedia page, the Gittins index policy is optimal if the projects are independent, but not necessarily if they aren't, and the problem is NP-hard in general if they can be dependent.</p>\n", "parentCommentId": "DfaJamZtqfjTBjtMX", "user": {"username": "MichaelStJules"}}, {"_id": "7aDf6bpmHwnzfCM22", "postedAt": "2022-10-24T16:34:29.997Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Not in this post, we just link to <a href=\"https://michielstock.github.io/posts/2021/2021-03-20-softmax/\">this one</a>. By \"principled\" I just mean \"not arbitrary, has a nice short derivation starting with something fundamental (like the entropy)\".</p><p>Yeah, the Gittins stuff would be pitched at a similar level of handwaving.</p>", "parentCommentId": "u4a5GBZBDBGyKDyin", "user": {"username": "technicalities"}}, {"_id": "DQRQbkDpL698gc5w5", "postedAt": "2022-10-24T16:44:54.522Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Excellent comment, thanks!&nbsp;</p><p>Yes, wasn't trying to endorse all of those (and should have put numbers on their dodginess).</p><p>1. Interesting. I disagree for now but would love to see what persuaded you of this. Fully agree that softmax implies long shots.</p><p>2. Yes, new causes and also new interventions within causes.</p><p>3. &nbsp;Yes, I really should have expanded this, but was lazy / didn't want to disturb the pleasant brevity. It's only \"moral\" uncertainty about how much risk aversion you should have that changes anything. (\u00e0 la <a href=\"https://forum.effectivealtruism.org/posts/xAoZotkzcY5mvmXFY/longtermism-risk-and-extinction\">this</a>.)</p><p>4. Agree.</p><p>5. Agree.</p><p>6. I'm using (possibly misusing) WD to mean something more specific like \"given cause A, what is best to do?; what about under cause B? what about under discount x?...\"&nbsp;</p><p>7. Now I'm confused about whether 3=7.</p><p>8. Yeah it's effective in the short run, but I would guess that the loss of integrity hurts us in the long run.</p><p>Will edit in your suggestions, thanks again.</p>", "parentCommentId": "KMKr93kZN8wmptuvH", "user": {"username": "technicalities"}}, {"_id": "j7zGNM6DonWgK5sCL", "postedAt": "2022-10-24T16:49:39.691Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>3. Tarsney <a href=\"https://www.rug.nl/filosofie/organization/news-and-events/events/2018-toel/hedging-our-bets\">suggests</a> one other plausible reason moral uncertainty is relevant: nonunique solutions leaving some choices undetermined. &nbsp;But I'm not clear on this.</p>", "parentCommentId": "KMKr93kZN8wmptuvH", "user": {"username": "technicalities"}}, {"_id": "5k4etzuigNdsrF9gz", "postedAt": "2022-10-29T10:56:51.270Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p><a href=\"https://philpapers.org/rec/ORDBAA\">Ord's undergrad thesis</a> is a tight argument in favour of enlightened argmax: search over <i>decision procedures and motivations </i>and pick the best of those instead of acts or rules.</p>", "parentCommentId": null, "user": {"username": "technicalities"}}, {"_id": "kMqBZgFdCxgGKT4yg", "postedAt": "2022-10-30T11:43:26.782Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Interesting thesis! Though, it's his doctoral thesis, not from one of his bachelor's degrees, right?</p>\n", "parentCommentId": "5k4etzuigNdsrF9gz", "user": {"username": "jh"}}, {"_id": "wDAbpfhyazirHGSn6", "postedAt": "2022-10-31T10:48:45.323Z", "postId": "8Ban7AnoqwdzQphsK", "htmlBody": "<p>Yep ta, even says so on page 1.&nbsp;</p>", "parentCommentId": "kMqBZgFdCxgGKT4yg", "user": {"username": "technicalities"}}]