[{"_id": "Fs8YFCemyLabseoCb", "postedAt": "2023-03-27T21:37:58.027Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Hi Toby, thanks for posting this here. The value of a vote is something I've wondered about for a long time; I was aware of the models in Banzhaf 1965 and Brennan 2011 and found them inadequate for the reasons you mention. I really like your lower bound argument for the chance of a decisive vote. But I still have a question about something you mention in the introduction,</p><blockquote><p><i>the chance of your vote being decisive can't be much lower than 1 in the number of voters. So voting will be worth it around the point where the value your preferred candidate would bring to the average citizen exceeds the cost of you voting.</i></p></blockquote><p>Why is it the point where the cost of you voting is less than the value your preferred candidate would bring? Shouldn't you be comparing the cost of voting to the difference in the expectations of the distribution of the value your preferred candidate would bring and the distribution of value the other candidate would bring?</p>", "parentCommentId": null, "user": {"username": "robirahman"}}, {"_id": "3A2y4q7gjsZTDvjfc", "postedAt": "2023-03-28T07:51:05.847Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Ah, that's what I meant by the value your candidate would bring. There isn't any kind of neutral outcome to compare them against, so I thought it clear that it meant in comparison to the other candidate. Evidently not so clear!</p>", "parentCommentId": "Fs8YFCemyLabseoCb", "user": {"username": "Toby_Ord"}}, {"_id": "bcDvoDW9qbPDoxXKS", "postedAt": "2023-03-28T18:28:52.667Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>There is also the evidentialist consideration (see e.g. section 5 of <a href=\"https://www.jstor.org/stable/pdf/2254984.pdf?refreqid=excelsior%3Ac2285c949f2e62f308702c77bb57be15\">Leslie [1991]</a>): to cast your vote plausibly gives you evidence that others decided to do so as well. The correlation between you and any given person is of course very low, but if the population is sufficiently large then the effect could be substantial.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnglh6yl6vah\"><sup><a href=\"#fnnglh6yl6vah\">[1]</a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnglh6yl6vah\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnglh6yl6vah\">^</a></strong></sup></span><div class=\"footnote-content\"><p>However, this is not necessarily an argument <i>in favour</i> of voting since you might be more correlated with people who would vote for the other party.</p></div></li></ol>", "parentCommentId": null, "user": {"username": "Sylvester Kollin"}}, {"_id": "aCpA4ov5gq2EMGKTc", "postedAt": "2023-03-28T18:33:32.146Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Relatedly: deciding to vote can also be important due to one's decisions being correlated with the decisions of other potential voters. A more general version of this consideration is discussed in <a href=\"https://longtermrisk.org/multiverse-wide-cooperation-via-correlated-decision-making/\">Multiverse-wide Cooperation via Correlated Decision Making</a> by Caspar Oesterheld.</p>\n", "parentCommentId": null, "user": {"username": "ofer"}}, {"_id": "qo5BM4iXS6T9bzBjD", "postedAt": "2023-03-28T19:08:30.644Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>But I guess that changing a decision based on such an argument wouldn't be correlated to practically anyone else, no?</p>", "parentCommentId": "aCpA4ov5gq2EMGKTc", "user": {"username": "edoarad"}}, {"_id": "SHYpyMxp54kpcyFYM", "postedAt": "2023-03-29T01:32:32.747Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>I agree the <i>share</i> of individuals who would be convinced to vote based on such an argument seems pretty small. In particular, the share of people hearing these arguments seems pretty small, although maybe if you include far future beings, the share (or influence-weighted share) could be large.</p><p>It could matter for people who are concerned with difference-making and think the probability of making a difference is too low under standard causal decision theory and assign reasonably high probability to an infinite universe. See <a href=\"https://forum.effectivealtruism.org/posts/CJtGbGZBvBvEr8jBP/can-an-evidentialist-be-risk-averse-hayden-wilkinson\">Can an evidentialist be risk-averse? by Hayden Wilkinson</a>. Maybe on other views, too, but not risk neutral expected value-maximizing total utilitarianism.</p>", "parentCommentId": "qo5BM4iXS6T9bzBjD", "user": {"username": "MichaelStJules"}}, {"_id": "JsggvBwTXaZfosCWb", "postedAt": "2023-03-29T11:12:02.607Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>I really like this argument. I think there's another way of framing it that occurred to me when reading it, that I also found insightful (though it may already be obvious):</p><ul><li>Suppose the value of your candidate winning is X, and their probability of winning if you don't do anything is p.</li><li>If you could buy all the votes, you would pay X(1-p) to do so (value of your candidate winning minus a correction because they could have won anyway). This works out at X(1-p)/N per vote on average.</li><li>If p&gt;1/2, then buying votes probably has diminishing returns (certainly this is implied by the unimodal assumption).</li><li>Therefore, if p&gt;1/2, the amount you would pay for a single vote must be bounded below by X(1-p)/N.</li><li>If p&lt;1/2, I think you can just suppose that you are in a zero-sum game with the opposition party(ies), and take their perspective instead to get the same bound reflected about p=1/2.</li></ul><p>The lower bound this gives seems less strict (1/2&nbsp;<i> </i>X/N in the case that p=1/2, instead of X/N), but it helps me understand intuitively <i>why</i> the answer has to come out this way, and why the value of contributing to voting is directly analogous to the value of contributing to, say, Parfit's water tank for the injured soldiers, even though there are no probabilities involved there.</p><p>If as a group you do something with value O(1), then the value of individual contributions should usually be O(1/N), since value (even in expectation) is additive.</p>", "parentCommentId": null, "user": {"username": "tobycrisford"}}, {"_id": "tqcion3qEQ3Em7DjX", "postedAt": "2023-03-29T13:23:59.815Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>A caution re interpreting of my argument in two-level elections:</p><p>One might read the above piece as an argument that voting is generally worthwhile. But note that the two-level structure of many elections (at least in countries without PR) does dampen the value of voting for many voters. e.g. if you are in the 10%+ of the US population who live in California, then not only are you very unlikely to cast a decisive vote to win the state's electoral college votes (since the probability that the underdog wins is very low), but it is very likely that in the situation where California comes down to a single vote, the rest of the country has skewed overwhelmingly to the Republicans, making the Californian electoral college votes irrelevant. Similar situations hold for safe seats in the lower house in the US, UK or Australia.&nbsp;</p><p>It might still be that in some sense two-level elections function on average like a single level election, but even if so, that could be because there are some people in marginal seats/states with disproportionate chances of changing the outcome, while many or most people have very little.</p><p>So while my adjusted formula above does apply in two-level elections, the intuitive interpretation that it supports a moral case for voting for the superior candidate may not apply.</p>", "parentCommentId": null, "user": {"username": "Toby_Ord"}}, {"_id": "rdSCfETSi33n7duwF", "postedAt": "2023-03-29T14:10:04.452Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Small comment... From memory, Jason Brennan doesn't do the calculations himself, but instead uses the calculation of Geoffrey Brennan and Loren Lomasky to arrive at his extremely low probability of the chance of deciding an election. So, it would be more accurate to say that this is the Geoffrey Brennan and Loren Lomasky estimate, as used by Jason Brennan in one prominent book about the ethics of voting.&nbsp;</p><p>(Brennan does mention the much lower Gelman estimate of the chance of being the decisive voter in a footnote, but I think dismisses it as implausible.)</p><p>Jason Brennan has a 2022 <a href=\"https://onlinelibrary.wiley.com/doi/epdf/10.1111/jopp.12273\">article</a> on voting with Chris Freiman which I can't access. perhaps he says something different there.&nbsp;</p>", "parentCommentId": null, "user": null}, {"_id": "xe5LNCoyJjkfgzm8Y", "postedAt": "2023-03-29T21:37:51.503Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>I'm not sure. Very few people would use the term \"correlation\" here; but perhaps quite a few people sometimes reason along the lines of: \"Should I (not) do X? What happens if many people (not) do it?\"</p>\n", "parentCommentId": "qo5BM4iXS6T9bzBjD", "user": {"username": "ofer"}}, {"_id": "pRZ8RaxLCfu2oooqZ", "postedAt": "2023-03-31T13:00:13.150Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Something like that. Geoffrey Brennan and Lomasky indeed present the binomial formula and suggest using it in their earlier work, but I haven't found a case of them using it in any particular way (which could get results like Jason Brennan's or results like Banzhaf's), so didn't want to pin this on them. So I cited Jason Brennan whose uses it to produce these crazily low probabilities in his book. It is possible that Jason Brennan didn't do the calculations himself and that someone else did (either Geoffrey Brennan and Lomasky or others), but I don't know and haven't found an earlier source for the crazy numbers.</p>", "parentCommentId": "rdSCfETSi33n7duwF", "user": {"username": "Toby_Ord"}}, {"_id": "ipHd4rYrLc2BBh2bf", "postedAt": "2023-04-02T19:06:20.857Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Jason Brennan discusses the background on it a bit here - <a href=\"https://bleedingheartlibertarians.com/2012/10/on-the-probability-of-being-decisive/\">https://bleedingheartlibertarians.com/2012/10/on-the-probability-of-being-decisive/</a></p><p>Gelman responds in the comments</p>", "parentCommentId": "pRZ8RaxLCfu2oooqZ", "user": null}, {"_id": "ZnKeJETjDztGjJB7A", "postedAt": "2023-04-03T07:22:41.482Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Nice post, Toby!</p><p>I think one should skip voting if it requires more than around 10 min, and the counterfactual is either working on <a href=\"https://80000hours.org/problem-profiles/#most-pressing-world-problems\">80,000 Hours' most pressing problems</a>, or donating to interventions which aim to solve them:</p><ul><li>If we conservatively assume the total benefit of an election is doubling real GDP, and that half of the population votes, the benefit per voter is 4 times the real GDP per capita. This is about 120 k$ in the United States.</li><li>Donating to GiveDirectly (GD) is about 100 times as effective as increasing the GDP of the United States, so the benefit per voter amounts to donating 1.2 k$ to GD.</li><li>Donating to GiveWell's top charities is about 10 times as effective as to GD, so the benefit per voter amounts to donating 120 $ to e.g. Against Malaria Foundation.</li><li>From&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/cjH2puDzAFrtrrThQ/despite-billions-of-extra-funding-small-donors-can-still\"><u>here</u></a>, Benjamin Todd \u201cwould donate to the <a href=\"https://funds.effectivealtruism.org/funds/far-future\">Long Term Future Fund</a> [LTFF] over the global health fund, and would expect it to be perhaps 10-100x [whose geometric mean is about 30] more cost-effective (and donating to global health is already very good)\u201d.</li><li>If donating to the LTFF is 30 times as effective as donating to GiveWell's top charities, the benefit per voter amounts to donating 4 $ to the LTFF. Note this supposes the total benefits correspond to doubling real GDP.</li><li>For a modest salary of 20 $/h, 4 $ is earned in 12 min. Not much time if electronic voting is not possible.</li><li>So I tend to think spending more than 10 min on voting is not worth it if the counterfactual is either working on <a href=\"https://80000hours.org/problem-profiles/#most-pressing-world-problems\">80,000 Hours' most pressing problems</a>, or donating to interventions which aim to solve them (like those supported by the LTFF).</li></ul><p>I suppose one could argue that participating in civil society via voting is a good norm, but I tend to think doing whatever is most impactful is a better one. One can also skip voting to donate to organisations working on voting reform, like the <a href=\"https://electionscience.org/commentary-analysis/voting-methods-an-open-target-for-effective-altruism/\">The Center for Election Science</a>.</p><p>Am I missing something? I recognise some of the inputs I used are quite uncertain, but I think I mostly used conservative numbers.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "fJAEe4DsmmtzLbq7C", "postedAt": "2023-04-03T08:44:43.072Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Thanks Vasco,</p><p>Interesting analysis. Here are a few points in response:</p><ul><li>It is best to take my piece as an input into a calculation of whether voting is morally justified on account of changing the outcome \u2014 it is an input in the form of helping work out the probability the outcome gets changed. More analysis would be needed to make the overall moral case \u2014 especially in the many voting systems that have multiple levels, where it may be much more important to vote in marginal seats and much less in safe seats, so taking the average may be inappropriate.</li><li>You make a good point that the value depends on who it is and their counterfactuals. Most people looking at this are trying to work out the average value to defend against claims that voting is not typically morally justified, rather than trying to work out the case for particular groups such as EAs \u2014 though that is a relevant group for this forum.</li><li>In such empirical arguments, I'd be cautious about claims that $1 dollar to the LTFF (or similar) is literally worth the same as $30,000 distributed across US citizens. Once the ratios get this extreme, you do need to worry more about issues like 0.1% of the $30,000 flowing through to extremely high value things and then outweighing the small targeted donation.</li><li>While you were trying to be very conservative by allocating a very large financial benefit to the better of the two parties, it is also relevant that who is in power at the time of the development of transformative AI capabilities could be directly relevant to existential risk, so even your generous accounting may be too small. (This factor will only apply in a small number of elections, but US presidential elections are likely one of them.)</li><li>I have a general presumption in favour of EAs acting as most people think morally responsible people should. In part because there is a good chance that the common-sense approach is tracking something important that our calculations may have lost sight of, in part because I don't think we should be trying to optimise all aspects of our behaviour, and in part because it is a legible sign of moral earnestness (i.e. it is reasonable for people to trust you less if you don't do the things those people see as basic moral responsibilities).</li></ul>", "parentCommentId": "ZnKeJETjDztGjJB7A", "user": {"username": "Toby_Ord"}}, {"_id": "dviAtqffQa8K7RxSW", "postedAt": "2023-04-03T14:33:44.947Z", "postId": "YLLtdNBJNbhdkopG7", "htmlBody": "<p>Thanks for the reply, great points!</p><blockquote><p>In such empirical arguments, I'd be cautious about claims that $1 dollar to the LTFF (or similar) is literally worth the same as $30,000 distributed across US citizens. Once the ratios get this extreme, you do need to worry more about issues like 0.1% of the $30,000 flowing through to extremely high value things and then outweighing the small targeted donation.</p></blockquote><p>I think this relates to <a href=\"https://forum.effectivealtruism.org/posts/jYT6c8ByLfDpYtwE9/why-charities-usually-don-t-differ-astronomically-in\">this</a> (great!) post from Brian Tomasik. I was assuming the doubling of real GDP corresponded to all the benefits. I can see 0.1 % of the 30 k$ going to something of extremely high value, but it can arguably lead to extremely high disvalue too. In addition, I would say it is unclear whether increasing real GDP is good, because it does not forcefully lead to differential progress (e.g. it can increase carbon emissions, consumption of animal products, and shorten AI timelines). Some longtermist interventions seem more robustly good, not those around AI, but ones like patient philanthropy, or increasing pandemic preparedness, or civilisation resilience.</p><blockquote><p>While you were trying to be very conservative by allocating a very large financial benefit to the better of the two parties, it is also relevant that who is in power at the time of the development of transformative AI capabilities could be directly relevant to existential risk, so even your generous accounting may be too small. (This factor will only apply in a small number of elections, but US presidential elections are likely one of them.)</p></blockquote><p>Repeating my analysis for existential risk:</p><ul><li>Based on the existential risk between 2021 and 2120 of 1/6 you guessed in The Precipice (which I really liked!), the annual existential risk is 0.182 % (= 1 - (1 - 1/6)^(1/100)).</li><li>If one assumes the benefit of one vote corresponds to eliminating 2 times the annual existential risk per capita (because maybe only half of the population votes), it would be 4.55*10^-13 (= 2*(0.182 %)/(8*10^9)). I may be underestimating the annual existential risk per capita due to high-income countries having greater influence, but underestimating due to existential risk arguably being lower early.</li><li>Assuming the LTFF has a cost-effectiveness of 3.16 <a href=\"https://en.wikipedia.org/wiki/Basis_point\">bp</a>/<a href=\"https://www.nist.gov/pml/owm/metric-si-prefixes\">G</a>$, which is the geometric mean of the lower and upper bound proposed by Linchuan Zhang <a href=\"https://forum.effectivealtruism.org/posts/cKPkimztzKoCkZ75r/how-many-ea-2021-usds-would-you-trade-off-against-a-0-01\">here</a>, the benefit of one vote would amount to donating about 1.44 $ (= 4.55/3.16) to the LTFF.</li><li>For a salary of 20 $/h, 1.44 $ is earned in 4 min. This is similar to what I got before, and continues to suggest one should not spend much time voting if the counterfactual is working on <a href=\"https://80000hours.org/problem-profiles/#most-pressing-world-problems\">80,000 Hours' most pressing problems</a>, or earning to support intervention aiming to solve them.</li><li>However, the analysis is so uncertain now that one can arrive to a different conclusion with reasonable inputs. So my overall take is that, neglecting indirect effects (like on how much people trust me), I do not know whether voting is worth it or not given that counterfactual.</li></ul><blockquote><p>I have a general presumption in favour of EAs acting as most people think morally responsible people should. In part because there is a good chance that the common-sense approach is tracking something important that our calculations may have lost sight of, in part because I don't think we should be trying to optimise all aspects of our behaviour, and in part because it is a legible sign of moral earnestness (i.e. it is reasonable for people to trust you less if you don't do the things those people see as basic moral responsibilities).</p></blockquote><p>Makes sense. I think I have been voting mostly based on this, although I am not sure about whether it makes sense for me to do so.</p>", "parentCommentId": "fJAEe4DsmmtzLbq7C", "user": {"username": "vascoamaralgrilo"}}]