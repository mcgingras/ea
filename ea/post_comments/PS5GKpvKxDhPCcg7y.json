[{"_id": "mXZYDHEWtLX7y7aZj", "postedAt": "2023-05-24T20:53:53.949Z", "postId": "PS5GKpvKxDhPCcg7y", "htmlBody": "<blockquote>\n<p>If we assume that no AGI system develops its own goal(s), which I assign a probability of 0.2, then it is also necessary to consider whether any AGI system\u2019s programmed goal(s) still leads to an EC. I assign this a probability of 0.04 because the human(s) who trained the AGI might not have thought out in enough detail what the consequences of programming the AGI with a specific goal or set of goals would be. The paperclip maximizer scenario is a classic example of this. Another scenario is if a nefarious human (or multiple nefarious humans) purposely creates and releases an AGI system with a destructive goal (or goals) that no human can control (including the person or people who released the AGI system) after it is discharged into the world.</p>\n</blockquote>\n<p>I only see arguments for the 0.04 case, but not for the 0.96 case. Do you have any concrete goals in mind that would not result in an EC?</p>\n<p>If I understand correctly, you claim to be 0.96 confident that not only outer alignment will be solved, but also that all AGIs will use some kind of outer alignment solution, and no agent builds an AGI with inadequate alignment. What makes you so confident?</p>\n", "parentCommentId": null, "user": {"username": "harfe"}}, {"_id": "TaJw6zZAuShycLtjg", "postedAt": "2023-05-24T21:46:40.071Z", "postId": "PS5GKpvKxDhPCcg7y", "htmlBody": "<p>Thank you for your comment and insight. The main reason why my forecast for this scenario is not higher is because I think there is a sizable risk of an existential catastrophe unrelated to AGI occurring before the scenario you mentioned were to resolve positively.</p><p>I am very open to adjusting my forecast, however. Are there any resources you would recommend that make an argument for why we should forecast a higher probability for this scenario relative to other AGI x-risk scenarios? And what are your thoughts on the likelihood of another existential catastrophe occurring to humanity before an AGI-related one?</p><p>Also please excuse any delay in my response because I will be away from my computer for the next several hours, but I will try to respond within the next 24 hours to any points you make.</p>", "parentCommentId": "mXZYDHEWtLX7y7aZj", "user": {"username": "Jared Leibowich"}}, {"_id": "Bfy64ziLkHZvAA9vx", "postedAt": "2023-05-25T14:23:03.660Z", "postId": "PS5GKpvKxDhPCcg7y", "htmlBody": "<p>I appreciate the framework you've put together here and the diagram is helpful. In your model, what do you think is the highest EV path humanity could take to lower the risk of EC? What would it look like (e.g. how would it start, how long would it take)?&nbsp;</p>", "parentCommentId": null, "user": {"username": "JonathanMann"}}, {"_id": "Pnwq6RXikzLGPtS3T", "postedAt": "2023-05-25T21:15:21.452Z", "postId": "PS5GKpvKxDhPCcg7y", "htmlBody": "<p>Glad the diagram is helpful for you! As far as the highest EV path, here are some of my thoughts:</p>\n<p>Most ideal plan: The easiest route to lowering almost every path in my diagram is by simply ensuring that AI doesn\u2019t get to a certain point of advancement. This is something I\u2019m very open to. While there are economic and geopolitical incentives to create increasingly advanced AI, I don\u2019t think this is an inevitable path that humans have to take. For example, we as a species have somewhat come to an agreement that nuclear weapons should basically never be used (even though some countries have them) and that it\u2019s unideal to do nuclear weapons research that figures out ways to make cheaper and more powerful nuclear weapons (although this is still being done to a certain extent).</p>\n<p>If there was a treaty in place that all countries (and companies) had to abide by as far as capacity limits, I think this would be a good thing because huge economic gains could still be had even without super-advanced AI. I am hopeful that this is actually possible. I think many people were genuinely freaked out when they saw what GPT-4 was capable of, and this is not even that close to AGI. So I am confident that there will be pushback from society as a whole to creating increasingly advanced AI.</p>\n<p>I don\u2019t think there is an inevitable path that technology has to take. For example, I don\u2019t think the internet was destined to operate the way it currently does. We might have to accept that AI is one of those things that we place limits on as far as research, just as we do so with nuclear weapons, bioweapons, and chemical weapons.</p>\n<p>Second plan (if first plan doesn\u2019t work): If humanity decides not to place limits on how advanced AI is allowed to get, my next recommendation is to minimize the chance that AGI systems are able to succeed in their EC attempts. I think this is doable as far as getting some kind of international treaty (the same way we have nuclear weapons treaties) with an organization that\u2019s a part of the UN focused on ensuring that there are agreed upon barriers put in place to cut off AGI from accessing weapons of mass destruction.</p>\n<p>Also, there should perhaps be some kind of watermarking standards implemented to ensure that communication between nations can be trusted, so that there are no wars between nations as a result of AGI tricking them with fake information that could lead to a conflict. That said, watermarking is hard, and people (and probably AI) eventually always find a way to get around a watermark.</p>\n<p>I think #2 is much more unideal than #1 because if AGI were to get intelligent enough, I think it would be significantly harder to prevent AGI systems from succeeding with their goals.</p>\n<p>I think both #1 and #2 could be relatively cheap (and easy) to implement if the political will is there.</p>\n<p>Going back to your question though, as far as how it would start and how long it would take:</p>\n<ul>\n<li>\n<p>If there was an international effort, humanity could start #1 and/or #2 tomorrow.</p>\n</li>\n<li>\n<p>I don\u2019t see any reason why these could not be successfully implemented within the next year or two.</p>\n</li>\n</ul>\n<p>While my recommendations might come across as na\u00efve to some, I am more optimistic than I was several months ago because I have been impressed with how quickly many people got freaked out by what AI is already capable of. This gives me reason to think that if progress were to continue with AI capabilities, there will be an increasing amount of pushback in society, especially as AI starts affecting people\u2019s personal and professional lives in more jarring ways.</p>\n", "parentCommentId": "Bfy64ziLkHZvAA9vx", "user": {"username": "Jared Leibowich"}}]