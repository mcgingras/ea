[{"_id": "ALMX8BbR5HfhwbPwp", "title": "To those who contributed to Carrick Flynn in Oregon  CD-6 - Please help now", "postedAt": "2022-10-03T00:21:24.838Z", "htmlBody": "<p>I wrote briefly during and after the primary election about people on this forum who contributed fairly heavily to elect Carrick Flynn, a known entity in your community. &nbsp;Many of you contributed the maximum of $2900 to his campaign. &nbsp; He lost significantly to Andrea Salinas who brings powerful and progressive credentials to this race. &nbsp;Unfortunately, &nbsp;the contributions you made plus the many millions from the PAC funded by Bankman-Fried required Salinas supporters to dig deeply in their pockets to respond to a tsunami of ads, including &nbsp;untrue attacks.&nbsp;</p><p>Now Salinas is in a very tight general election race against Mike Erickson, a conservative Republican who has praised Trump. &nbsp;Erickson, a multimillionaire, is financing part of his election from his millions. &nbsp;Salinas, who has been in public service most of her life has no such option.</p><p>Building a financial base for a close race is made more difficult after a financially hard primary. &nbsp;In addition, Oregon has three swing Congressional seats in addition to a three way Governor's race and multiple close legislative races. &nbsp;There has rarely been as many demands for dollars and volunteers on the left as there are this election. &nbsp;The fate of much national legislation, including funding for science, pandemic research and mitigation, and climate change issues may hinge on whether or not Salinas wins and secures a Democratic majority. &nbsp;As one of the few new congressional districts in the nation, CD-6 is central to the future of much that EA adherents say that they value.</p><p>My ask: &nbsp;To the many hundreds of you who so readily supported Flynn in the primary, &nbsp;please consider supporting Andrea Salinas now. &nbsp;The choice of readers of this forum to fund &nbsp;Flynn in the primary plus the massive influx of money from PACs guided by EA principles made it more likely that she will lose this general election. &nbsp;If the Congress loses the Dem majority, if the chairs of committees that will make decisions about pandemic research and preparedness are all Republican, if the focus is on short term business success instead of long term sustainability of our world, we stand to lose so much. You can learn about this candidate and contribute here: <a href=\"https://www.andreasalinasfororegon.com\">https://www.andreasalinasfororegon.com</a></p>", "user": {"username": "Carol Greenough"}}, {"_id": "FtHhC7CfN4r5xD3Lm", "title": "Easy fixing Voting", "postedAt": "2022-10-02T17:03:20.229Z", "htmlBody": "<p>[I forgot to post this post during the elections, but still relevant]</p><h1>Context : Two round uninominal ballot</h1><p>At this moment in France, it's the election period. And between two existential anguishes about AGIs, I sometimes think about the presidential campaigns.</p><p>We use a two round uninominal ballot: in each round, each citizen votes for a single candidate.</p><h1>Problem : Bad properties</h1><p>The uninominal voting system really does suck because primaries are not mandatory, so some candidates who could ally themselves do not ally themselves. So you have to vote strategically, and that has a lot of bad properties.</p><p>A classic example that highlights the bad properties of the uninominal ballot is the case where the majority of the population is left-wing, but 10 left-wing candidates are running while only one right-wing candidate is running. It is then the right wing that wins the election because of the spoiler effect, i.e. the dispersal of the votes of the left wing candidates.</p><p>Another problem is that strategic voting prevents conviction voting. It is only interesting to vote for those who have a chance to win.</p><h1>Easy fix : the vote by approval.</h1><p>There are a lot of possible voting systems (ranking, randomized condorcet, bordas point ballot, majority judgment ballot...). Which one to choose? Condorcet voting is relatively popular on this platform, but the main problem is its difficulty to implement and its complexity to vulgarize. The voting systems with the best properties are also unfortunately difficult to use.</p><p>The French YouTuber Mr Phi proposes a more pragmatic goal: By changing the minimum of rules possible, how to improve the elections as much as possible? What would be a credible and feasible alternative in the near future?</p><p><a href=\"https://www.youtube.com/watch?v=zmCl5i_sEiM\"><u>Pourquoi notre syst\u00e8me de vote est nul (et le moyen le plus simple de l'am\u00e9liorer)</u></a>[Why our voting system is bad]</p><p>The answer presented is very simple: instead of voting for a single person, you can endorse one or several candidates. In practice, it would be enough to remove the constraint of a single name in the envelope, giving the possibility to vote for several candidates at the same time in the envelope.</p><p>This solution is straightforward because we would keep all the rest of the election organization: we count the names, and who has the most wins. We could even take the opportunity to eliminate the second round.&nbsp;</p><h1>Link with effective altruism?</h1><p>In theory, approval voting would make it possible to vote for several parties at the same time, and thus to promote more ideas at each election. This seems particularly important for animalist or transhumanist parties, whose votes are systematically cannibalized by the larger ecological parties. Some left-wing parties would have obtained funding much more easily, thanks to the guaranteed reimbursement starting from the 5% threshold.</p><p>It would also help to fight against hyper-polarization: candidates would no longer have to criticize other candidates, but could concentrate on being the best candidates, thus creating a more positive atmosphere. This would be a partial solution to the \"<a href=\"https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer\"><u>politics is the mind killer</u></a>\".</p><h1>Politics</h1><p>An association keeps a&nbsp;<a href=\"https://electionscience.org/voting-methods/approval-voting-progress/\"><u>list</u></a> of applications of usage of approval voting, but this list is rather short. In the short term in France, Emmanuel Macron and the majority have no interest in focusing attention on approval voting. But what is surprising is that the smaller parties don't talk about it that often either.</p><p>A petition currently exists in France to pass the Condorcet ballot, but this petition has only 200 signatories when 500k are needed for the petition to be considered.</p><p><a href=\"https://petitions.assemblee-nationale.fr/initiatives/i-90\"><u>R\u00e9forme de l'\u00e9lection pr\u00e9sidentielle par un scrutin math\u00e9matiquement juste - Plateforme des p\u00e9titions de l'Assembl\u00e9e nationale</u></a>[Reforming the presidential election with a mathematically fair ballot - National Assembly Petitions Platform]</p><p>The title of the petition is \"Reforming the presidential election with a mathematically fair voting system\". The fact that the Condorcet voting system is so complicated to explain does not help. So I have created a new petition in favor of the approval voting system.</p><p>I encourage you to do the same in your respective countries.</p>", "user": null}, {"_id": "doKK7okPRD9BdwxQw", "title": "EA Serbia is now launching!", "postedAt": "2022-10-02T16:48:02.025Z", "htmlBody": "<p>Hello everyone, short and sweet news from the Balkans!</p><p>&nbsp;</p><p>My partner (<a href=\"https://forum.effectivealtruism.org/users/tatiana-skuratova-1\">Tatiana K. Nesic Skuratova</a>) and I are starting an EA chapter in Serbia, and we kick it off with a casual meeting (upcoming weekend, October 9th) as well as a \"What We Owe the Future\" book discussion later this month. This post serves as a contact point for anyone in or near Serbia or Belgrade who wants to join or connect, as well as anyone who has tips on starting a new club :)</p><p>&nbsp;</p><p>We are happy to say that the support from Catherine Low and Amarins Veringa was on-point; while many other community builders have helped with advice and resources along the way! We are looking forward to seeing those of you that make their way to Belgrade, Serbia, or to cooperate virtually wherever you are!</p>", "user": {"username": "Du\u0161an D. Ne\u0161i\u0107 (Dushan)"}}, {"_id": "3wcNkri9CjRC4t5Cj", "title": "Why does AGI occur almost nowhere, not even just as a remark for economic/political models?", "postedAt": "2022-10-02T14:43:40.009Z", "htmlBody": "<p>Depending of different attitudes towards questions like take-off speed, people argue that with the development of AGI we will face situations of world GDP doubling days/weeks/a few years (with the number of years shriking with each further doubling). Many peoples's timelines here seem to be quite broad, including quite commonly expectations like \"AGI within the next 2-3 decades very likely\".</p><p>How the global world order politically as well as economically will change over the next decades is a quite extensively discussed topic in public as well as academia, with many goals and forecasts made until years like 2050 or 2070 (\"climate neutral 2050\", \"china's economy in 30 years\"). Barely is AGI mentioned in economics classes, political research papers and the like, despite its apparent impact of making any politics redundant and throwing over any economic forecasts. If AGI was even significantly less mighty than we think and there was even just a 20% chance of it occuring in the next 3 decades, that should be the number one single factor debated in every single argument on any economic/political topic with medium-length scope. &nbsp;Why, do you think, is it the case, that AGI is comparatively so rarely a topic there?</p><p>My motivated reasoning would immediately come up with explanations along the lines of&nbsp;</p><ol><li>people in these disciplines are just not so much aware of AI developments</li><li>any forecasts/plans made assuming short timelines and fast takeoff speeds are useless anyways, so it makes sense to just assume longer timelines</li><li>Maybe I am just not noticing the omnipresence of AGI debate in economic/political long-term discourse</li></ol><p>@1 seems unreasonable, because as soon as the first AI-economics people would come up with these arguments, if they were reasonable, they would become mainstream</p><p>@2 if that assumption was consciously made, I'd expect to hear this more often as side note</p><p>@3 hard to argue against, given it assumes I don't see the discourse. But I regularity engage with media/content from the UN on their SDGs, have taken some Economics/IR/Politics electives, try to be a somewhat informed citicien and have friends studying these things, and I barely see AI suddenly speeding up things in any forecasts or discussions</p><p>Why might this be the case?<br>To me it seems like either mainstream academia, global institutions and public discourse heavily miss something or we tech/ea/ai people are overly biased in the actual relevance of our own field (I'm CS student)?</p>", "user": {"username": "Franziska Fischer"}}, {"_id": "o3Cg4bxa2znzhHm76", "title": "Any further work on AI Safety Success Stories?", "postedAt": "2022-10-02T11:59:12.608Z", "htmlBody": "", "user": {"username": "Krieger"}}, {"_id": "FLTJtDmCxfpoZDA5K", "title": "Questions on databases of AI Risk estimates", "postedAt": "2022-10-02T09:12:26.287Z", "htmlBody": "<p>I was hoping to write something for the Future Fund contest and - being entirely a one-trick pony - was going to look at uncertainty analysis in AI Catastrophe predictions.&nbsp;</p><p>I've done a review of the forums and my conclusion is that predictions around AI Catastrophe are very heavily focussed on when AI will be invented and the overall top-level probability that AI will be a catastrophe if it is invented. Other than that predictions about AI Risk are quite sparse. For example, few people seem to have offered a numerical prediction about whether they think the AI Alignment Problem is solvable in principle, few people have offered a numerical prediction about the length of time we could contain a misaligned AI and so on. The only end-to-end model of AI Risk with numerical prediction I have found is Carlsmith (2021): <a href=\"https://arxiv.org/abs/2206.13353.\">https://arxiv.org/abs/2206.13353.</a>&nbsp;</p><ul><li>Is my review of the state of the literature roughly accurate? That is, my impression that people mostly predict the time AI is invented and the risk that AI leads to catastrophe, but do not predict other important related questions (at least not numerically)?</li><li>Am I right that Carlsmith (2021) is the only end-to-end model of AI Risk with numerical predictions at each stage (by end-to-end I mean there are steps in between 'AI invented' and 'AI catastrophe' which are individually predicted)? Any other examples would be really helpful so I can scope out the community consensus on the microdynamics of AI risk.</li><li>If I'm right about the above, I think an essay looking at the microdynamics of AI Risk predictions could be novel and informative (for example the probability that we solve the Alignment Problem before AI is invented seems pretty important but I don't think anyone has looked at validating the Metaculus prediction on this topic). Is this already a known quantity? Are there any particular pitfalls I should watch out for?</li></ul><p>In my review I came across the 'Database of Existential Risk Estimates' - link here: <a href=\"https://forum.effectivealtruism.org/posts/JQQAQrunyGGhzE23a/database-of-existential-risk-estimates.\">https://forum.effectivealtruism.org/posts/JQQAQrunyGGhzE23a/database-of-existential-risk-estimates.</a> This seems to contain many estimates of exactly what I am looking for - predictions of specific events which will occur on the path to an AI catastrophe, rather than the overall risk of catastrophe itself.</p><ul><li>Are there any other databases of this sort, especially those which focus on topics other than when AI will be invented or the top-level probability it will be a catastrophe?</li><li>Is the database regarded as generally credible on the forums? I have found a handful of predictions which I don't think are included (especially on Metaculus), but the database has many more which I would never have found without it. If there is no known systematic bias in the database I'd really like to use it!</li><li>Is there anything else I should know about the database?</li></ul><p>Thanks so much!</p>", "user": {"username": "Froolow"}}, {"_id": "jdAEsBfpFei4pGmAM", "title": "Request for feedback on sample blurbs for the EA fantasy novel I wrote", "postedAt": "2022-10-02T09:02:11.707Z", "htmlBody": "<p>Thanks everyone for the help and comments, I\u2019ve taken everything I read seriously, and changed a lot of how the main character thinks about EA topics due to it \u2014 I am now looking for more readings that are specifically aimed at finding stuff that might accidentally offend people before I send it out into the world. So if anyone knows a non-ea friend who might enjoy reading something like this, and who is willing to leave comments about what bothered them, I would find that really useful.</p><p>Otherwise, my current plan is that in a few weeks I\u2019ll start publishing it through Royal Road, Spacebattles forum, my own website and Amazon (though I\u2019m undecided between the 2.99 price point or the .99c price point) for publication.&nbsp;</p><p>I am still actively looking for other places to publish it, and if anyone can help with promoting this book, that would be useful.</p><p>Also if you haven\u2019t yet read it, but do want to comment and tell me what definitely needs to be changed, here is the&nbsp;<a href=\"https://docs.google.com/document/d/1ZppL3mlO6M98TLQk2IAdL3nM__Pmjxirt59WXGzbIMM/edit?usp=sharing\"><u>shared google doc.</u></a></p><p>&nbsp;</p><p><strong>Blurbs</strong></p><p>Also, here are three possible versions of the blurb that I\u2019ll be using when I publish it. What do you all think about them? I think it is important to include in the blurb that I received some support while writing this novel.&nbsp;</p><p>===</p><p>After Isaac died and woke up as a powerful cultivator in a fantasy world, he wanted to do as much good as he could possibly manage.</p><p>But it turned out to be hard to figure out what actually was the&nbsp;<i>single&nbsp;</i>most useful thing to do.</p><p>And the guy whose body he now occupied, wasn\u2019t actually dead, and he had his own goals that weren\u2019t the same as Isaac\u2019s\u2026&nbsp;</p><p>Note: While it is supposed to be a fun story, a major goal of this novel is to describe the ideas of Effective Altruism, and I received a grant from an EA associated organization to help write it.</p><p>===<br>&nbsp;</p><p>I\u2019d died in a car crash, and woke up as a powerful cultivator in a new world.</p><p>Honestly, this was pretty awesome, even though there was a war coming to the island that I\u2019d woken up on, which I wanted to have nothing to do with. I wanted to help as many people as I could, as much as I could, and dying in a war that I had nothing to do with wouldn\u2019t help anyone.</p><p>I really didn\u2019t want to die again.</p><p>But it turned out that the cultivator whose body I\u2019d found myself in wasn\u2019t dead, and he was absolutely determined to fight and probably die.</p><p>Note: This novel was partly written to promote the ideas of effective altruism in a fun and fictional context, and I received support from an Effective Altruist organization to help me write it.</p><p>===</p><p>Dead after being hit by a van.</p><p>Then not dead.</p><p>Isaac had wanted to help other people and do as much good as he could when he was alive in our world. Now that he had woken up in the body of a powerful cultivator in a fantasy world, he still wanted to do as much good as he could.</p><p>Unfortunately figuring out what is the best thing to do, especially when everything potential has unintended consequences is hard.</p><p>And then it turned out that the guy whose body Isaac had woken up in still was alive, still had control of the body half of the time, and he mainly wanted to die heroically, and bravely in a doomed battle to defend his homeland.</p><p>Isaac really didn\u2019t want to die again.</p><p>&nbsp;</p><p>Note: I\u2019m trying to promote the idea that most of us should both donate more money to improve the world, and try to make sure we are donating to the most effective or useful causes with this book. I received a grant from an Effective Altruist organization to make it possible for me to write this book.</p><p>&nbsp;</p><p><strong>Possible Titles</strong></p><p>&nbsp;</p><p>If anyone wants to help me brain storm possible titles (and even more significantly subtitles) for the novel, here is my current plan.</p><p>&nbsp;</p><p>The Split Summon: A Cultivation novel about trying to do the most good</p><p>Thanks again, everyone, and here another copy of the <a href=\"https://docs.google.com/document/d/1ZppL3mlO6M98TLQk2IAdL3nM__Pmjxirt59WXGzbIMM/edit?usp=sharing\">link </a>if you want to read it!</p>", "user": {"username": "timunderwood"}}, {"_id": "ogdDRgfmBRj6paPbv", "title": "Global life satisfaction distribution", "postedAt": "2022-10-01T15:39:15.613Z", "htmlBody": "<h1>Summary</h1><ul><li>I estimated the global life satisfaction distribution in <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1hiXtsUzKU7Nr8R0-6Oogx028wgQTozJBPKnIVb2gS9M/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1664641705864208&amp;usg=AOvVaw1TUY7DYAv0lgg93VIgEBOK\"><u>this</u></a>&nbsp;Sheet and <a href=\"https://www.google.com/url?q=https://colab.research.google.com/drive/1EVO4GhHxhQbUF6qnlz2IabYucXfg_pWg?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1664641705864893&amp;usg=AOvVaw3RaC6siLtCTeywFvqNRGl7\"><u>this</u></a>&nbsp;Colab based on the mean and standard deviation of the life satisfaction distribution of 142 countries.</li><li>For the neutral life satisfaction of 0.949 (on a scale from 0 to 10) I supposed, the distribution I obtained implies the fraction of people with negative lives is 6.37 %.</li></ul><h1>Methods</h1><p>I determined the 2020 global life satisfaction distribution as assessed by the Cantril Ladder<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefep214805o9i\"><sup><a href=\"#fnep214805o9i\">[1]</a></sup></span>&nbsp;generating samples from the life satisfaction distribution of 142 countries totalling 95.0 %<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkd9tur1pxoq\"><sup><a href=\"#fnkd9tur1pxoq\">[2]</a></sup></span>&nbsp;of the global population. To do this:</p><ul><li>I generated 1 sample for each 1 k people, and defined the population size based on data from <a href=\"https://www.google.com/url?q=https://ourworldindata.org/grapher/population-past-future?time%3D1700..latest&amp;sa=D&amp;source=editors&amp;ust=1664641705865857&amp;usg=AOvVaw3Ss2cqyk2_BgNDYrwA06Pu\"><u>OWID</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref19ivxkqkrzo\"><sup><a href=\"#fn19ivxkqkrzo\">[3]</a></sup></span>.</li><li>I assumed <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Truncated_normal_distribution&amp;sa=D&amp;source=editors&amp;ust=1664641705866268&amp;usg=AOvVaw3J4w80-UmVZ5AtlBHdjsRZ\"><u>truncated normal distributions</u></a>&nbsp;with minimum and maximum equal to 0 and 10 for all countries, as these are the minimum and maximum possible values in the Cantril Ladder.</li><li>For each country, I supposed the mean and standard deviation of the non-truncated distribution to be equal to the mean and standard deviation of the life satisfaction<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3e9j7uhtflr\"><sup><a href=\"#fn3e9j7uhtflr\">[4]</a></sup></span>.<ul><li>I set the mean to the mean life satisfaction between 2019 and 2021 given in the <a href=\"https://www.google.com/url?q=https://worldhappiness.report/ed/2022/&amp;sa=D&amp;source=editors&amp;ust=1664641705866835&amp;usg=AOvVaw3qXaUmRabp8uRHaGyRa7V1\"><u>2022 World Happiness Report</u></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjv9nun7868d\"><sup><a href=\"#fnjv9nun7868d\">[5]</a></sup></span>&nbsp;(WHR).</li><li>I calculated the standard deviation from the 95 % <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Confidence_interval&amp;sa=D&amp;source=editors&amp;ust=1664641705867216&amp;usg=AOvVaw2jPpueAZ6DCkEyz_5pSyyQ\"><u>confidence interval</u></a>&nbsp;(CI) and sample size of the life satisfaction<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdvy7l27vntc\"><sup><a href=\"#fndvy7l27vntc\">[6]</a></sup></span>.</li></ul></li><li>Of the 146 countries analysed in the WHR, 1 was excluded due to missing population data, and 3 due to lack of sample size data.</li></ul><p>In addition, I estimated the fraction of people with negative lives<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzk169wjri5e\"><sup><a href=\"#fnzk169wjri5e\">[7]</a></sup></span>&nbsp;assuming a value for neutral life satisfaction. In reality, the neutral point varies from country to country (even from person to person), and is very uncertain. According to the section \u201cHow Many People Have Positive Wellbeing?\u201d of Chapter 9 of <a href=\"https://www.google.com/url?q=https://whatweowethefuture.com/uk/&amp;sa=D&amp;source=editors&amp;ust=1664641705867871&amp;usg=AOvVaw0_0XpBIhO_k1dCYzGX4ruU\"><u>What We Owe the Future</u></a>&nbsp;(WWOF) from William MacAskill:</p><blockquote><p>The relative nature of the scale means that it is difficult to interpret where the neutral point should be, and unfortunately, there have been only two small studies directly addressing this question. Respondents from Ghana and Kenya put the neutral point at 0.6, while one British study places it between 1 and 2.</p></blockquote><p>Moreover, although I aggregated life satisfaction scores of different countries into the same global distribution, they need not be comparable. From the same section of WWOF:</p><blockquote><p>Life satisfaction surveys mainly provide insights into relative levels of wellbeing across different people, countries, and demographics. They do not provide much guidance on people\u2019s absolute level of wellbeing.</p></blockquote><p>With these caveats, I supposed the neutral life satisfaction to be 0.949. I estimated this from the geometric mean between the neutral point for Ghana and Kenya of 0.6, and the mean of the lower and upper bound of the neutral point for the UK of 1.5 (= (1 + 2)/2).</p><h1>Results</h1><p>The calculations are in <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1hiXtsUzKU7Nr8R0-6Oogx028wgQTozJBPKnIVb2gS9M/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1664641705869130&amp;usg=AOvVaw0n6PRsXstZ1w8Itf2MC3iv\"><u>this</u></a>&nbsp;Sheet and <a href=\"https://www.google.com/url?q=https://colab.research.google.com/drive/1EVO4GhHxhQbUF6qnlz2IabYucXfg_pWg?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1664641705869552&amp;usg=AOvVaw1oA-umr4uya9JWmBSOMtAo\"><u>this</u></a>&nbsp;Colab. The mean and standard deviation of the truncated and non-truncated distribution, minimum, deciles, maximum, <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Cumulative_distribution_function&amp;sa=D&amp;source=editors&amp;ust=1664641705869941&amp;usg=AOvVaw1eZtUvEpmLPPSB3XAPAlzr\"><u>cumulative distribution function</u></a>, and histogram<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkvggfmyxunb\"><sup><a href=\"#fnkvggfmyxunb\">[8]</a></sup></span>&nbsp;of the 2020 global life satisfaction are presented below. For the supposed neutral life satisfaction of 0.949, the fraction of people with negative lives is 6.37 %.</p><figure class=\"table\"><table><thead><tr><th style=\"text-align:center\" colspan=\"4\"><strong>2020 global life satisfaction</strong></th></tr><tr><th style=\"text-align:center\" colspan=\"2\"><strong>Truncated</strong></th><th style=\"text-align:center\" colspan=\"2\"><strong>Non-truncated</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefynyuwwxaq88\"><sup><a href=\"#fnynyuwwxaq88\">[9]</a></sup></span></th></tr><tr><th style=\"text-align:center\"><strong>Mean</strong></th><th style=\"text-align:center\"><strong>Standard deviation</strong></th><th style=\"text-align:center\"><strong>Mean</strong></th><th style=\"text-align:center\"><strong>Standard deviation</strong></th></tr></thead><tbody><tr><td style=\"text-align:center\">5.14</td><td style=\"text-align:center\">2.62</td><td style=\"text-align:center\">5.17</td><td style=\"text-align:center\">4.21</td></tr></tbody></table></figure><figure class=\"table\"><table><thead><tr><th style=\"text-align:center\"><strong>Quantile</strong></th><th style=\"text-align:center\"><strong>2020 global life satisfaction</strong></th></tr></thead><tbody><tr><td style=\"text-align:center\">0 (minimum)</td><td style=\"text-align:center\">0</td></tr><tr><td style=\"text-align:center\">0.1</td><td style=\"text-align:center\">1.44</td></tr><tr><td style=\"text-align:center\">0.2</td><td style=\"text-align:center\">2.59</td></tr><tr><td style=\"text-align:center\">0.3</td><td style=\"text-align:center\">3.54</td></tr><tr><td style=\"text-align:center\">0.4</td><td style=\"text-align:center\">4.41</td></tr><tr><td style=\"text-align:center\">0.5</td><td style=\"text-align:center\">5.22</td></tr><tr><td style=\"text-align:center\">0.6</td><td style=\"text-align:center\">6.01</td></tr><tr><td style=\"text-align:center\">0.7</td><td style=\"text-align:center\">6.82</td></tr><tr><td style=\"text-align:center\">0.8</td><td style=\"text-align:center\">7.70</td></tr><tr><td style=\"text-align:center\">0.9</td><td style=\"text-align:center\">8.70</td></tr><tr><td style=\"text-align:center\">1 (maximum)</td><td style=\"text-align:center\">10.0</td></tr></tbody></table></figure><figure class=\"image image_resized\" style=\"width:601.7px\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995921/mirroredImages/ogdDRgfmBRj6paPbv/dkfinbwrjcm3ce3it5eg.png\"><figcaption>Cumulative distribution function of the 2020 global life satisfaction.</figcaption></figure><figure class=\"image image_resized\" style=\"width:601.7px\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995921/mirroredImages/ogdDRgfmBRj6paPbv/ayrbqfg9ykypw9p7oscq.png\"><figcaption>Histogram of the 2020 global life satisfaction.</figcaption></figure><h1>Discussion</h1><p>The fraction of people with negative lives of 6.37 % I estimated is of the same order of magnitude of Will\u2019s best guess of 10 %<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrj4ywf35oer\"><sup><a href=\"#fnrj4ywf35oer\">[10]</a></sup></span>, and also suggests most humans have positive lives. In addition, the estimate for the mean global life satisfaction of 5.14 being higher than that for the neutral life satisfaction of 0.949 hints the mean human life is positive.</p><p>For a more <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/credal-resilience&amp;sa=D&amp;source=editors&amp;ust=1664641705885644&amp;usg=AOvVaw2W2puVFqU8VhGm3MwO5vHJ\"><u>resilient</u></a>&nbsp;conclusion, it would be important to consider at a global level other lines of evidence addressed in WWOF besides life satisfaction surveys:</p><ul><li>\u201cSurveys that simply ask people if they are happy\u201d.</li><li>\u201cAsking people at random times how they feel in that moment\u201d.</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnep214805o9i\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefep214805o9i\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From <a href=\"https://www.google.com/url?q=https://ourworldindata.org/happiness-and-life-satisfaction&amp;sa=D&amp;source=editors&amp;ust=1664641705886425&amp;usg=AOvVaw1M0FONb3Z6SDett05Urzwp\"><u>OWID</u></a>, the question respecting the Cantril Ladder is as follows. \u201cPlease imagine a ladder, with steps numbered from 0 at the bottom to 10 at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkd9tur1pxoq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkd9tur1pxoq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;See B153 of tab \u201cLife satisfaction parameters\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn19ivxkqkrzo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref19ivxkqkrzo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Except for Kosovo, whose data were taken from <a href=\"https://www.google.com/url?q=https://data.worldbank.org/indicator/SP.POP.TOTL&amp;sa=D&amp;source=editors&amp;ust=1664641705887572&amp;usg=AOvVaw3AS-cPssUQECDxadV8S8fL\"><u>The World Bank</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3e9j7uhtflr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3e9j7uhtflr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Note the mean and standard deviation of the non-truncated distributions are different from the mean and standard deviation of the truncated distributions (see <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Truncated_normal_distribution&amp;sa=D&amp;source=editors&amp;ust=1664641705888222&amp;usg=AOvVaw27DJdk2gJlVcqKZVKz-Ado\"><u>here</u></a>). I supposed them to be equal because I failed to determine the mean and standard deviation of the non-truncated distributions with <a href=\"https://www.google.com/url?q=https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html%23scipy.optimize.fsolve&amp;sa=D&amp;source=editors&amp;ust=1664641705888558&amp;usg=AOvVaw2S09QL3zJd7eT5IZ7JOtPV\"><u>scipy.optimise.fsolve</u></a>&nbsp;for 76.1 % (= 108/142) of the analysed countries. Columns H and I of tab \u201cLife satisfaction parameters\u201d of <a href=\"https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1hiXtsUzKU7Nr8R0-6Oogx028wgQTozJBPKnIVb2gS9M/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1664641705888846&amp;usg=AOvVaw1hl5ijX0wUt239RZw-BbBB\"><u>this</u></a>&nbsp;Sheet contain the values I managed to determine.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjv9nun7868d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjv9nun7868d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The respective data can be downloaded by clicking on \u201cData for Figure 2.1\u201d.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndvy7l27vntc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdvy7l27vntc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cStandard deviation\u201d = \u201clength of the 95 % CI\u201d * \u201cmean sample size between 2019 and 2021\u201d^0.5 / 0.95 / 2. Data for the 95 % CI can be downloaded by clicking on \u201cData for Figure 2.1\u201d, and for the sample size is in Table 2 of the <a href=\"https://www.google.com/url?q=https://happiness-report.s3.amazonaws.com/2022/Appendix_1_StatiscalAppendix_Ch2.pdf&amp;sa=D&amp;source=editors&amp;ust=1664641705887205&amp;usg=AOvVaw3biR1IuXjAKu-7jv2zsXCA\"><u>Statistical Appendix</u></a>&nbsp;of the WHR.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzk169wjri5e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzk169wjri5e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;A person would rather not be born than living a negative life, neglecting effects on other beings.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkvggfmyxunb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkvggfmyxunb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;The bin length of the histogram is 0.01.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnynyuwwxaq88\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefynyuwwxaq88\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;I determined the mean and standard deviation of the non-truncated distribution with <a href=\"https://www.google.com/url?q=https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html%23scipy.optimize.fsolve&amp;sa=D&amp;source=editors&amp;ust=1664641705889269&amp;usg=AOvVaw1FD3pQWHjqwJtacOdhepW4\"><u>scipy.optimise.fsolve</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrj4ywf35oer\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrj4ywf35oer\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;From the aforementioned section of WWOF:</p><blockquote><p>I [William MacAskill] would tentatively suggest that something like 10 percent of the global population have lives with below-neutral wellbeing.</p></blockquote></div></li></ol>", "user": {"username": "vascoamaralgrilo"}}, {"_id": "WqrEp6SZk5uj8Zjwo", "title": "On Artificial General Intelligence: Asking the Right Questions", "postedAt": "2022-10-02T05:00:14.659Z", "htmlBody": "<p>Recently, the <a href=\"https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/\"><u>Future Fund posted a challenge</u></a>, asking for essays on three propositions on the timing and impact of Artificial General Intelligence (AGI).&nbsp; While the challenge may generate some interesting discussion about the technology of artificial intelligence and scenario building for its impact on the world, it is fundamentally focused on the wrong questions.</p><p>&nbsp;</p><p>Two of the propositions focus on when AGI will arrive.&nbsp; This makes it seem like AGI is a natural event, like an asteroid strike or earthquake.&nbsp; But AGI is something we will create, if we create it.&nbsp; And while <a href=\"https://www.academia.edu/44663332/Scientific_Freedom_and_Social_Responsibility\"><u>the 20</u><sup><u>th</u></sup><u> century was dominated</u></a>by a view of science and technology that only asked whether something was technically possible, the 21<sup>st</sup> century is increasingly recognizing that the crucial question is less can we do this, <a href=\"https://www.aaas.org/programs/scientific-responsibility-human-rights-law/aaas-statement-scientific-freedom\"><u>than </u><i><u>should&nbsp;</u></i><u>we</u></a>.&nbsp; Human beings will have to choose to pursue and develop AGI.&nbsp; One question is, why should we?</p><p>&nbsp;</p><p>AGI might be helpful for thinking about complex human problems, but it is doubtful that it would be better than task specific AI.&nbsp; Task specific AI has already proven successful at useful, difficult jobs (such as cancer screening for tissue samples and hypothesizing protein folding structures).&nbsp; Part of what has enabled such successful applications is the task specificity.&nbsp; That allows for clear success/fail training and ongoing evaluation measures.&nbsp;&nbsp;</p><p>&nbsp;</p><p>The more general applications of AI (such as automated driving) have not yet proven so successful.&nbsp; General navigation of the world involves far more complex judgments, including ethical judgments, that AI is not good at.&nbsp;&nbsp;</p><p>&nbsp;</p><p>Now, AI might get better at these more real world contexts, or it might not.&nbsp; We might learn that the complicated judgments needed to drive a car can only be assisted by AI (lane maintenance assistance, parking assistance, etc.), and not taken over from human drivers generally.&nbsp; But it is important to note that even if AI could fully drive a car in all the contexts and challening situations in which humans can drive a car, that would still be far removed from AGI.&nbsp;&nbsp;</p><p>&nbsp;</p><p>Successful AI car driving could be evaluated by the increased safety of driving (the great hope) and the social acceptability of AI (successful crossing of the valley of the uncanny).&nbsp; But such success is still task specific.&nbsp; How would know we had successful AGI if/when we created it?&nbsp; It would nothing like human intelligence, which is shaped not only by information processing, but by embodiment and the emotions central to human existence.&nbsp; Empathy, love, grief, fear, anger, and our other emotions don\u2019t just shape what we know but shape how we value and how we act with what we know.&nbsp; Both physical and emotional pain set the crucial stakes for our learning.&nbsp; AI cannot be fully imbued with these attributes\u2014even if we could make failure in some sense physically painful for the AI, emotional pain, the far more potent teacher, is out of reach for AI.&nbsp; We are not even clear on <a href=\"https://www.amazon.com/Body-Keeps-Score-Healing-Trauma/dp/0143127748/\"><u>how it works in humans</u></a> to model it properly.&nbsp;</p><p>&nbsp;</p><p>So AGI cannot be like human intelligence.&nbsp; Its generality cannot be tied to the potency of human stakes, including the fact of <a href=\"https://www.amazon.com/Being-Mortal-Medicine-What-Matters/dp/1250076226/\"><u>human mortality</u></a> and human vulnerability.&nbsp; We set the stakes for AI with task specific applications, and so can train such limited application AI.&nbsp; Such stakes for AGI are irreducibly murky or out of reach.&nbsp;&nbsp;</p><p>&nbsp;</p><p>This raises the question of why then pursue AGI?&nbsp; Recall we don\u2019t have to; no scientist has to.&nbsp; Task specific AI is useful.&nbsp; What would an AGI utterly different from our own intelligence do for us?&nbsp;&nbsp;</p><p>&nbsp;</p><p>Such reflection clarifies what we should be asking at this point.&nbsp; Why <i>should</i> we pursue AGI?&nbsp; Just because we <i>can&nbsp;</i>is not an adequate answer.&nbsp;&nbsp;</p><p>&nbsp;</p><p>And if we do pursue it, how might such pursuit be done responsibly?&nbsp; What would count as success for an AGI (recognizing it will not be like us)?&nbsp; And should we ever allow it to have access to controlling real things in the real world (like cars, packages, airplanes, etc.)?&nbsp; An AGI that controls nothing in the world but serves as an unusual interlocutor for humans might be interesting, but it is not even clear it would be helpful, as it would not be aligned with what is important to us.&nbsp; Why have this kind of thing around?</p><p>&nbsp;</p><p>The development of AGI is up to us as intelligent human actors.&nbsp; We make these choices, and we should make them asking the right questions.&nbsp;&nbsp;</p><p>&nbsp;</p><p>Future Fund should be grappling with these questions, and questions of how to assist scientists making choices about what to pursue and how to pursue it.&nbsp; Societal responsibility in science is now an endemic feature of scientific research.&nbsp; How to grapple well with this responsibility is something worth incentivizing, not how to change minds about probabilities of future events.</p>", "user": {"username": "Heather Douglas"}}, {"_id": "Youhe5KQK6SwFS6Bq", "title": "Against longtermism: a care-centric approach?", "postedAt": "2022-10-02T05:00:26.310Z", "htmlBody": "<p>Hi all, new to the forum, longtime observer in the wings. I've been trying to translate my anti-longtermist intuitions into meaningful arguments and would appreciate some feedback.</p><p>One promising line of thought, I think, involves considering the ratio of suffering to the amount of care available to deal with it.</p><p>As phrased in the Tube ad for <i>What We Owe the Future</i>, longtermism weights the damaging consequence of our actions equally regardless of their location in time. A cut foot is a cut foot whether it happens today or a hundred years in the future. However, I don't think the reverse is true. Preventing a cut foot today may be much more valuable than preventing one a hundred years from now, because if we don't do it now, no-one will. Whereas our future selves and our descendants have a hundred years to work on the future problem.&nbsp;</p><p>In other words, we should weight suffering today much more highly than suffering in the future, because <strong>only we can do something about it</strong>.</p><p>Moreover, we might plausibly guess that the proportion of humans living in extreme suffering will continue to decrease over time. And, as EA continues to gather momentum, more people will allocate spare income, wealth, and time to EA-linked causes, or to altruism and philanthropy more generally. These twin factors may mean that (setting x-risks temporarily aside), the ratio of \"care\" available to each hypothetical unit of suffering may grow enormously over time.</p><p>If that is true, then we should greatly prioritise eliminating suffering today and in the near future, and leave most of the caring for future generations to future generations. &nbsp;</p><p>I realise the argument depends on a host of unarticulated &amp; tested premises, not least setting aside x-risks, which I think needs to be considered separately. But also keen to hear whether it's all been said, or dealt with, before? Does it ring true to you?</p>", "user": {"username": "Aron P"}}, {"_id": "xWrrfGzDmxFhaajxf", "title": "Monthly Overload of EA - October 2022", "postedAt": "2022-10-01T12:32:12.484Z", "htmlBody": "<p><i>Link post for </i><a href=\"https://moea.substack.com/p/2022-october-ea-updates\"><i>2022 October EA Updates</i></a><i>.</i></p><h1><strong>Top Posts</strong></h1><ul><li>Holden Karnofsky - <a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\"><u>EA is about maximization, and maximization is perilous</u></a></li><li>Eli Lifland - <a href=\"https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future\"><u>My take on What We Owe the Future</u></a></li><li>A post announcing the <a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\"><u>Future Fund's AI Worldview Prize</u></a>, with prizes ranging from $15k to $1.5M for work that informs the Future Fund's fundamental assumptions about the future of AI</li><li>A highly upvoted post on the forum <a href=\"https://forum.effectivealtruism.org/posts/hbejbRBpd6quqnTAB/red-teaming-cea-s-community-building-work-2\"><u>red teaming CEA\u2019s community building work</u></a></li><li>Sawyer - <a href=\"https://forum.effectivealtruism.org/posts/dvcpKuajunxdaZ6se/ea-is-too-reliant-on-personal-connections\"><u>EA is too reliant on personal connections</u></a></li><li>Announcing <a href=\"https://forum.effectivealtruism.org/posts/tWawcXaNnLAihA2Fv/announcing-ea-pulse-large-monthly-us-surveys-on-ea\"><u>EA Pulse</u></a>, large monthly US surveys on EA</li><li>Vadim Albinsky - <a href=\"https://forum.effectivealtruism.org/posts/coryFCkmcMKdJb7Pz/does-economic-growth-meaningfully-improve-well-being-an\"><u>Does Economic Growth Meaningfully Improve Well-being? An Optimistic Re-Analysis of Easterlin\u2019s Research</u></a></li><li>Eleos Arete Citrini - <a href=\"https://forum.effectivealtruism.org/posts/haoHTwwtFp9M2NewH/switzerland-fails-to-ban-factory-farming-lessons-for-the\"><u>Switzerland fails to ban factory farming - lessons for the pursuit of EA-inspired policies?</u></a></li></ul><hr><h1><strong>Upcoming Events</strong></h1><h1>Global</h1><ul><li>September to December - <a href=\"https://forum.effectivealtruism.org/posts/ZxHnaPmxmBrZmerBj/announcing-the-prague-fall-season\"><u>Prague Fall Season</u></a></li><li>21st-23rd October - <a href=\"https://www.eaglobal.org/events/eagxvirtual-2022/\"><u>EAGx Virtual</u></a> - Apply by October 19th</li><li>November to January - <a href=\"https://forum.effectivealtruism.org/posts/kBgMD7GwJboxGFvsp/mexico-ea-fellowship\"><u>Mexico EA Fellowship</u></a></li><li>4th-6th November - <a href=\"https://www.eaglobal.org/events/eagxrotterdam-2022/\"><u>EAGxRotterdam</u></a> - Admissions close 23rd October</li><li>24th-26th November - <a href=\"https://forum.effectivealtruism.org/events/FgxQdY8epubagx4ev/eastern-europe-community-building-retreat\"><u>Eastern Europe Community Building Retreat</u></a></li><li>2nd-4th December - <a href=\"https://www.eaglobal.org/events/eagxberkeley2022/\"><u>EAGxBerkeley</u></a></li><li>7th-8th January - <a href=\"https://forum.effectivealtruism.org/posts/KkPo9cpLThWyd7yju/save-the-date-eagx-latam\"><u>EAGxLatinAmerica</u></a></li></ul><h1>Virtual</h1><ul><li>6th October - <a href=\"https://docs.google.com/forms/d/1LoThrrNZ82yGX2ZdJSEXlPoUSrTdk0iVtKQj7RdgVOE\"><u>What\u2019s the most effective way to help as many animals as possible?</u></a>, Talk by Charity Entrepreneurship</li><li>9th October - <a href=\"https://us02web.zoom.us/meeting/register/tZApceCoqz8qGt1NOj4vBZ0qiLhvnsgnBoGA\"><u>Giving What We Can Meetup</u></a></li><li>October/November - <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeLV0WMnHcRi-iU5z0EiJV10hoIIt0yCEhnbL_ze7Q6g1qjBg/viewform\"><u>High Impact Medicine Fellowship</u></a>, Apply by 9th October</li><li>EA Virtual Programmes<ul><li><a href=\"https://www.effectivealtruism.org/virtual-programs/the-precipice-reading-group\"><u>The Precipice Reading Group</u></a></li><li><a href=\"https://www.effectivealtruism.org/virtual-programs/introductory-program\"><u>Intro to EA</u></a></li><li><a href=\"https://www.effectivealtruism.org/virtual-programs/in-depth-program\"><u>In-Depth EA Program</u></a></li></ul></li><li>29th October - <a href=\"https://www.eaforchristians.org/intro-to-effective-altruism-for-christians-1\"><u>Intro Course to EA for Christians</u></a></li></ul><hr><h1>Meta</h1><ul><li>David Manheim - <a href=\"https://forum.effectivealtruism.org/posts/f9NpDx65zY6Qk9ofe/doing-good-best-isn-t-the-ea-ideal\"><u>\"Doing Good Best\" isn't the EA ideal</u></a></li><li>Luke Freeman - <a href=\"https://forum.effectivealtruism.org/posts/5Xio8uECTH3GqYARc/we-re-still-extremely-funding-constrained-but-don-t-let-fear\"><u>We\u2019re still (extremely) funding constrained (but don\u2019t let fear of getting funding stop you trying).</u></a></li><li>Nathan Young - <a href=\"https://forum.effectivealtruism.org/posts/nDawZHxDR3j53zdbf/summaries-are-underrated\"><u>Summaries are underrated</u></a></li><li>Giving What We Can - <a href=\"https://forum.effectivealtruism.org/posts/nhbeKbwMgFKfrzLNb/marketing-messages-trial-for-gwwc-giving-guide-campaign\"><u>Marketing Messages Trial for GWWC Giving Guide Campaign</u></a></li><li>Sam Freedman - <a href=\"https://samf.substack.com/p/the-politics-of-effective-altruism\"><u>The Politics of Effective Altruism</u></a></li><li>Jehan - <a href=\"https://forum.effectivealtruism.org/posts/j4RnXAQgyMCSLzBkW/chesterton-fences-and-ea-s-x-risks\"><u>Chesterton Fences and EA\u2019s X-risks</u></a></li><li>Ronja - <a href=\"https://forum.effectivealtruism.org/posts/mjvWYNKwFRLQMMsZ7/ea-for-people-with-non-technical-skillsets\"><u>EA for people with non-technical skillsets</u></a></li><li>Rudolf - <a href=\"https://forum.effectivealtruism.org/posts/g8aBf2oLwDvgd4ovf/much-ea-value-comes-from-being-a-schelling-point\"><u>Much EA value comes from being a Schelling point</u></a></li><li>Akash - <a href=\"https://forum.effectivealtruism.org/posts/A2YwuXe3Eo5kMZhZo/13-background-claims-about-ea\"><u>13 background claims about EA</u></a></li><li>Sabrina C - <a href=\"https://forum.effectivealtruism.org/posts/YtPcJx6yMHqQGYbWK/young-eas-should-choose-projects-more-carefully\"><u>Young EAs should choose projects more carefully</u></a></li><li>Richard Ren - <a href=\"https://forum.effectivealtruism.org/posts/ErrJc34wChZW8oHYN/what-is-neglectedness-actually\"><u>What is neglectedness, actually?</u></a></li><li>Jordan Arel - <a href=\"https://forum.effectivealtruism.org/posts/MaN23i6XsSkkgoLY9/why-wasting-ea-money-is-bad\"><u>Why Wasting EA Money is Bad</u></a></li><li>Vaidehi Agarwalla - <a href=\"https://forum.effectivealtruism.org/posts/dZQwTi7Y5napymuCh/sprinting-and-marathoning-two-strategies-for-volunteering\"><u>Sprinting &amp; Marathoning: Two Strategies for Volunteering your Time</u></a></li><li>Thomas Kwa - <a href=\"https://forum.effectivealtruism.org/posts/kGrLQZzG3rLcXvhjg/ea-forum-content-might-be-declining-in-quality-here-are-some\"><u>EA forum content might be declining in quality. Here are some possible mechanisms</u></a></li><li>Jeremiah Johnson asking <a href=\"https://forum.effectivealtruism.org/posts/Geh9DFZQnj9s5HjoW/the-hundred-billion-dollar-opportunity-that-eas-mostly\"><u>why EA isn't spending more effort on influencing individual donations</u></a></li><li>Chana Messinger<ul><li><a href=\"https://forum.effectivealtruism.org/posts/CmmtKKD84BYhmmxrD/chanamessinger-s-shortform?commentId=zdTRScgAiGANo8kxa\"><u>Scattered Takes and Unsolicited Advice</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/B2sxxATzHdaGNc5YK/advice-i-give-to-people-who-don-t-currently-have-an-ea-job\"><u>Advice I Give to People Who Don't Currently Have an EA Job and are thinking of transitioning</u></a></li></ul></li><li>Evie Cottrell - <a href=\"https://forum.effectivealtruism.org/posts/acyfmFTN3cNgwnYw6/agency-needs-nuance\"><u>\"Agency\" needs nuance</u></a></li><li>High Impact Professionals<ul><li><a href=\"https://forum.effectivealtruism.org/posts/bsqtndpfKbcbQRFEs/fundraising-campaigns-at-your-organization-a-reliable-path\"><u>Fundraising Campaigns at Your Organization: A Reliable Path to Counterfactual Impact</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/sE7Y43JRQARAKzZ6k/key-factors-for-success-in-organizing-a-fundraising-campaign\"><u>Key Factors for Success in Organizing a Fundraising Campaign at Your Company</u></a></li><li>If you're interested in seeing what a campaign could look like at your organisation, you can contact federico@highimpactprofessionals.org</li></ul></li></ul><hr><h1>New Projects</h1><ul><li><a href=\"https://www.reaps.info/about\"><u>Research in Effective Altruism and Political Science</u></a> aims to create a community of researchers interested in governance, politics and EA</li><li>Asterix are sending out their first quarterly EA inspired magazine soon, <a href=\"https://asteriskmag.com/\"><u>sign up here</u></a></li><li>Rachel Norman - <a href=\"https://forum.effectivealtruism.org/posts/AFgvA9imsT6bww8E3/announcing-the-rethink-priorities-special-projects-program\"><u>Announcing the Rethink Priorities Special Projects Program</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/zQ5apJGAJb6otXdvh/high-impact-psychology-hipsy-piloting-a-global-network\"><u>High-Impact Psychology</u></a> are piloting a global network for psychologists</li><li><a href=\"https://effectivedevelopers.org/\"><u>Effective Developers</u></a> aims to help software developers and people running software projects in the EA community</li><li><a href=\"https://www.tarbellfellowship.org/\"><u>The Tarbell Fellowship</u></a> - A one year programme for early-career journalists intent on improving the world - 9th October</li><li><a href=\"https://pineappleoperations.org/\"><u>Pineapple Operations</u></a> has expanded to include anyone interested in operations work in EA</li><li>C\u00e9line Galletti and Camille Fayet have launched <a href=\"https://bettermatters.uk/\"><u>Better Matters</u></a>, a student magazine based on effective altruism</li><li>There are new sub forums on the EA forum for <a href=\"https://forum.effectivealtruism.org/topics/bioethics/subforum\"><u>bioethics</u></a> and <a href=\"https://forum.effectivealtruism.org/topics/software-engineering/subforum\"><u>software engineering</u></a></li><li>Carson Ezell, Madeleine Chang and Olaf Willner - <a href=\"https://forum.effectivealtruism.org/posts/x5czbdCW6SqnLJhbD/announcing-the-space-futures-initiative\"><u>Announcing the Space Futures Initiative</u></a></li><li>Luca Parodi introducing <a href=\"https://forum.effectivealtruism.org/posts/D5RdHgzeczrDKKywH/introducing-school-of-thinking-1\"><u>School of Thinking</u></a>, an English and Italian media startup</li><li>There is an <a href=\"https://www.effectivejobsboard.org/\"><u>effective jobs board</u></a> that you can vote on, set up by Nathan Young</li><li>CEA Ops is now <a href=\"https://forum.effectivealtruism.org/posts/9gHTYC5qbSH9E37vx/cea-ops-is-now-ev-ops\"><u>Effective Ventures</u></a> - a federation of organisations and projects working to have a large positive impact in the world</li><li>Sofia Fogel - Announcing the <a href=\"https://forum.effectivealtruism.org/posts/t2z2jxSaM7X4TCeTh/announcing-the-nyu-mind-ethics-and-policy-program\"><u>NYU Mind, Ethics, and Policy Program</u></a></li><li><a href=\"https://forum.effectivealtruism.org/posts/W6gGKCm6yEXRW5nJu/quantified-intuitions-an-epistemics-training-website\"><u>Quantified Intuitions</u></a>: An epistemics training website including a new EA-themed calibration app</li></ul><hr><h1>Movement building</h1><ul><li>A post by me on the <a href=\"https://forum.effectivealtruism.org/posts/2CwsQai9hHsa2CDJ8/the-mistakes-of-focusing-on-student-outreach-in-ea\"><u>mistakes of focusing too much on student community building</u></a></li><li>Tony Senanayake with <a href=\"https://twitter.com/tony__sena/status/1562356753336918022\"><u>10 reflections from an EA India retreat</u></a></li><li>80,000 Hours podcast with <a href=\"https://80000hours.org/after-hours-podcast/episodes/kuhan-jeyapragasan-effective-altruism-university-groups/\"><u>Kuhan Jeyapragasan on effective altruism university groups</u></a></li><li>Cold Button Issues - <a href=\"https://forum.effectivealtruism.org/posts/HrFBf2DuSo6KPBYG5/smart-movements-start-academic-disciplines\"><u>Smart Movements Start Academic Disciplines</u></a></li><li>Amy Labenz - <a href=\"https://forum.effectivealtruism.org/posts/fHqP9NmTvALTTQSv9/cea-s-events-team-is-hiring\"><u>CEA's Events Team is Hiring!</u></a></li><li>Chana Messinger - <a href=\"https://forum.effectivealtruism.org/posts/CmmtKKD84BYhmmxrD/chanamessinger-s-shortform?commentId=MBdtNjktCQbstDrgc\"><u>Transparency for undermining the weird feelings around systematizing community building</u></a></li></ul><hr><h1>EA Ideas Explained</h1><ul><li>Luke Muehlhauser - <a href=\"https://forum.effectivealtruism.org/posts/i9RJjun327SnT3vW8/reasoning-transparency\"><u>Reasoning Transparency</u></a></li><li>Alana Horowitz Friedman<ul><li><a href=\"https://www.givingwhatwecan.org/blog/what-is-counterfactual-thinking-and-why-should-you-care-about-it\"><u>What is counterfactual thinking and why should you care about it?</u></a></li><li><a href=\"https://www.givingwhatwecan.org/blog/should-charity-begin-at-home\"><u>Should charity begin at home?</u></a></li></ul></li></ul><hr><h1>Critiques</h1><ul><li>Peter Wildeford - <a href=\"https://forum.effectivealtruism.org/posts/X47rn28Xy5TRfGgSj/21-criticisms-of-ea-i-m-thinking-about\"><u>21 criticisms of EA I'm thinking about</u></a></li><li>Ruth Grace - <a href=\"https://forum.effectivealtruism.org/posts/7ajePuRKiCo7fA92B/evaluating-large-scale-movement-building-a-better-way-to\"><u>Evaluating large-scale movement building: A better way to critique Open Philanthropy's criminal justice reform</u></a></li><li>Rohit - <a href=\"https://www.strangeloopcanon.com/p/ea-is-a-fight-against-knightian-uncertainty\"><u>Effective Altruism is a constant fight against Knightian uncertainty</u></a></li><li>Yonatan Cale - <a href=\"https://forum.effectivealtruism.org/posts/YCMgg6x6zWJmran5L/criticism-of-the-80k-job-board-listing-strategy\"><u>Criticism of the 80k job board listing strategy</u></a></li><li>Thomas Aitken - <a href=\"https://forum.effectivealtruism.org/posts/cR4pCrATD5SSN35Sm/the-role-of-economism-in-the-belief-formation-systems-of\"><u>The Role of \"Economism\" in the Belief-Formation Systems of Effective Altruism</u></a></li><li>Connor Tabarrok - <a href=\"https://forum.effectivealtruism.org/posts/A5tRZC2mduJfpMhud/ea-worries-and-criticism\"><u>EA Worries and Criticism</u></a></li><li>Akash - <a href=\"https://forum.effectivealtruism.org/posts/qgQaWub8iR2EERq7i/criticism-of-ea-criticisms-is-the-real-disagreement-about\"><u>Criticism of EA Criticisms: Is the real disagreement about cause prio?</u></a></li><li>Francis - <a href=\"https://forum.effectivealtruism.org/posts/4LsrNczpF6mfrHP4M/prediction-markets-are-somewhat-overrated-within-ea\"><u>Prediction Markets are Somewhat Overrated Within EA</u></a></li></ul><hr><h1>EA Global</h1><ul><li>Scott Alexander with a suggestion to <a href=\"https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global\"><u>make EA global open access</u></a></li><li>Dion with her <a href=\"https://forum.effectivealtruism.org/posts/qybqnKYBGSDPYbcBd/my-closing-talk-at-eagxsingapore\"><u>closing speech at EAGx Singapore</u></a></li><li>Constance Li - <a href=\"https://forum.effectivealtruism.org/posts/hur3ejqvA2QmYEfZa/case-study-of-ea-global-rejection-criticisms-solutions\"><u>Case Study of EA Global Rejection + Criticisms/Solutions</u></a></li><li>Tony Senanayake with <a href=\"https://twitter.com/tony__sena/status/1567725101784924160\"><u>10 reflections from EAGx Singapore</u></a></li></ul><hr><h1>Careers</h1><ul><li>Madhav Malhotra - <a href=\"https://forum.effectivealtruism.org/posts/ACJyrSRyMjz2uiTLd/internship-lessons-at-the-happier-lives-institute\"><u>Internship Lessons at the Happier Lives Institute</u></a></li><li>Hear This Idea podcast with <a href=\"https://hearthisidea.com/episodes/aird\"><u>Michael Aird</u></a> on how to do impact-driven research</li><li>80,000 Hours - <a href=\"https://80000hours.org/career-reviews/should-you-go-to-law-school/\"><u>Should you go to law school in the US to have a high-impact career?</u></a></li><li>Gabriel Mukobi - <a href=\"https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering\"><u>Levelling Up in AI Safety Research Engineering</u></a></li><li>Dan Pandori - <a href=\"https://forum.effectivealtruism.org/posts/NTuTSEEn2Ka54nwFC/offering-faang-style-mock-interviews\"><u>Offering FAANG-style mock interviews</u></a></li></ul><hr><h1>Prizes</h1><ul><li>GiveWell have announced the <a href=\"https://forum.effectivealtruism.org/posts/6wwK6Qduxt7mMmj8k/announcing-the-change-our-mind-contest-for-critiques-of-our\"><u>Change Our Mind Contest</u></a> for critiques of their cost-effectiveness analyses</li><li><a href=\"https://forum.effectivealtruism.org/posts/bvK44CdpG7mGpQHbw/the-usd100-000-truman-prize-rewarding-anonymous-ea-work\"><u>The Truman Prize: Rewarding Anonymous EA Work</u></a> ($100,000 in prizes)</li><li><a href=\"https://forum.effectivealtruism.org/posts/CetKqoq3qKo5ggZ4f/usd13-000-of-prizes-for-changing-our-minds-about-who-to-fund\"><u>$13,000 of prizes for changing our minds about who to fund</u></a> (Clearer Thinking Regrants Forecasting Tournament)</li><li><a href=\"https://forum.effectivealtruism.org/posts/noDYmqoDxYk5TXoNm/usd5k-challenge-to-quantify-the-impact-of-80-000-hours-top\"><u>$5,000 challenge to quantify the impact of 80,000 hours' top career paths</u></a></li></ul><hr><h1>Grants</h1><ul><li>Open Philanthropy have made <a href=\"https://www.openphilanthropy.org/grants/\"><u>24 grants recently</u></a>, with a value of $25 million<ul><li>$8,000,000 - Farm Animal Welfare<ul><li>$3,600,000 - Compassion in World Farming - Broiler Chicken Welfare and Efforts to End Cages and Crates</li><li>$2,500,000 - Sinergia Animal - Farm Animal Welfare in Southeast Asia and Latin America</li><li>$1,500,000 - World Animal Protection - Farm Animal Welfare in Asia</li></ul></li><li>$6,800,000 - Potential Risks from Advanced AI<ul><li>$4,800,000 - Center for a New American Security - Work on AI Governance</li><li>$2,000,000 - Epoch</li></ul></li><li>$4,600,000 - Global Health &amp; Development<ul><li>$3,400,000 - Bridges to Prosperity - Trailbridge Building RCT in Rwanda</li><li>$1,000,000 - Center for Global Development</li></ul></li><li>$4,000,000 - Transformative Basic Science<ul><li>Oregon Health and Science University - Research on Oocyte Production</li></ul></li><li>$550,000 - Effective Altruism Community Growth<ul><li>Apollo Fellowship - Effective Altruism Debate Camp</li><li>University of Edinburgh - Effective Altruism Psychology Course</li><li>Universidad de Sevilla - Course on Effective Altruism</li><li>Peter McIntyre - Effective Altruism Community Building Projects</li></ul></li><li>$400,000 - Biosecurity &amp; Pandemic Preparedness</li><li>$400,000 - Longtermism<ul><li>Berkeley Existential Risk Initiative - SERI Summer Fellowships</li></ul></li></ul></li><li>The Future Fund has made <a href=\"https://ftxfuturefund.org/all-grants/\"><u>14 grants in August</u></a>, with a value of $6.4 million<ul><li>$2,900,000 - Longview Philanthropy</li><li>$1,000,000 - Fund for Alignment Research</li><li>$800,000 - VIVID</li><li>$350,000 - University of California, Santa Cruz, Cihang Xie</li><li>$300,000 - ALLFED</li><li>$290,000 - University of Pennsylvania, Professor Geoff Goodwin</li><li>$200,000 - HR Luna Park</li></ul></li></ul><hr><h1>Global Development</h1><ul><li>The <a href=\"https://www.devex.com/news/global-fund-falls-short-of-18b-target-as-uk-italy-delay-pledges-104046\"><u>Global Fund to Fight AIDS, Tuberculosis and Malaria</u></a> didn't hit its $18 billion replenishment target, although the final tally of $14.25 billion exceeded the amount raised in the last pledging cycle</li><li>Open Philanthropy have <a href=\"https://forum.effectivealtruism.org/posts/XBHx9zhAtkiBJnZNu/cause-exploration-prizes-announcing-our-prizes-1\"><u>announced the winners of their cause exploration prizes</u></a><ul><li>Top prize - Organophosphate pesticides and other neurotoxicants by Ben Stewart.</li><li>Second prizes<ul><li>Violence against women and girls by Akhil Bansal</li><li>Sickle Cell Disease</li><li>Shareholder activism by S Behmer</li></ul></li></ul></li><li>Bastian Herre - <a href=\"https://ourworldindata.org/less-democratic\"><u>The world has recently become less democratic</u></a></li><li>Charles Kenny with a post suggesting that people interested in EA <a href=\"https://cgdev.org/blog/effective-altruists-should-be-working-here\"><u>should work at the Centre for Global Development</u></a></li><li>GiveWell - <a href=\"https://forum.effectivealtruism.org/posts/aZhQhsHX4PyxCCpkz/the-maximum-impact-fund-is-now-the-top-charities-fund-1\"><u>The Maximum Impact Fund is now the Top Charities Fund</u></a></li><li>Leo - <a href=\"https://forum.effectivealtruism.org/posts/DBFZmsu7DJ8BSoZMp/assessing-cost-effectiveness-malnutrition-famine-and-cause\"><u>Assessing Cost Effectiveness: malnutrition, famine, and cause prioritization</u></a></li><li>Rosie Bettle - <a href=\"https://founderspledge.com/stories/measuring-health\"><u>How Founders Pledge use (and sometimes don't use) DALYs</u></a></li><li>The Gates Foundation with an <a href=\"https://www.gatesfoundation.org/goalkeepers/report/2022-report/\"><u>update on the Sustainable Development Goals</u></a></li><li>India develops its <a href=\"https://www.reuters.com/business/healthcare-pharmaceuticals/india-develops-its-first-cervical-cancer-vaccine-2022-09-01/\"><u>first cervical cancer vaccine</u></a></li><li>Yonatan Cale - <a href=\"https://forum.effectivealtruism.org/posts/hprGdAiW73EsPfWzu/zzapp-malaria-more-effective-than-bed-nets-wanted-cto-coo\"><u>Zzapp Malaria: More effective than bed nets? (Wanted: CTO, COO &amp; Funding)</u></a></li><li>Joel Tan - <a href=\"https://forum.effectivealtruism.org/posts/ymEqipmiM3SLyQvaC/value-of-life-vsl-estimates-vs-community-perspective\"><u>Value of Statistical Life Estimates vs Community Perspective Evaluations</u></a></li></ul><hr><h1>Animal Welfare</h1><ul><li>Kenny Torrella looking at the <a href=\"https://www.vox.com/future-perfect/2022/9/12/23339898/global-meat-production-forecast-factory-farming-animal-welfare-human-progress\"><u>history and future of animal product consumption</u></a></li><li>An <a href=\"https://forum.effectivealtruism.org/posts/pfSiMpkmskRB4WxYW/an-evaluation-of-animal-charity-evaluators\"><u>Evaluation of Animal Charity Evaluators</u></a></li><li>80,000 Hours podcast with <a href=\"https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/\"><u>Andr\u00e9s Jim\u00e9nez Zorrilla on the Shrimp Welfare Project</u></a></li><li>Sofia Balderson with the September edition of <a href=\"https://impactfulanimal.substack.com/p/september-edition-of-impactful-animal\"><u>Impactful Animal Advocacy Newsletter</u></a></li><li>Max Carpendale - <a href=\"https://forum.effectivealtruism.org/posts/yq5NyHqEdewn6JBpn/pathways-to-victory-how-can-we-end-animal-agriculture\"><u>Pathways to victory: How can we end animal agriculture?</u></a></li><li>Animal Ask and Ren Springlea - <a href=\"https://forum.effectivealtruism.org/posts/xTNrsFLvaMBu3rFZZ/cctv-cameras-in-slaughterhouses-modest-benefits-for-animal\"><u>CCTV cameras in slaughterhouses: Modest benefits for animal welfare</u></a></li><li>GWWC podcast with <a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkLnBvZGJlYW4uY29tL2dpdmluZ3doYXR3ZWNhbi9mZWVkLnhtbA/episode/Z2l2aW5nd2hhdHdlY2FuLnBvZGJlYW4uY29tLzQxMGJkMWYyLTU2MjItMzNkYi1iYmVjLTUxNTcwYWMyNzQ1Mg?sa=X&amp;ved=0CAUQkfYCahcKEwjQn7Hi3Ln6AhUAAAAAHQAAAAAQAQ\"><u>Neil Dullaghan on using research &amp; strategic thinking to help animals effectively</u></a></li><li>Animal Advocacy Careers - <a href=\"https://forum.effectivealtruism.org/posts/LK4tamRuvtKgJvHmu/the-impact-we-achieved-to-date-animal-advocacy-careers\"><u>The impact we achieved to date</u></a></li></ul><hr><h1>Alternative Proteins</h1><ul><li>Fai - <a href=\"https://forum.effectivealtruism.org/posts/2KbLLyxj5gYJniHp2/could-it-be-a-bad-lock-in-to-replace-factory-farming-with\"><u>Could it be a (bad) lock-in to replace factory farming with alternative protein?</u></a></li><li>Seren L. Kell - <a href=\"https://www.nature.com/articles/s41587-022-01454-4\"><u>How researchers can join the race to develop new ways of making meat</u></a></li><li>The UK's Ivy Farm has opened the <a href=\"https://agfundernews.com/uks-ivy-farm-opens-the-largest-cultivated-meat-pilot-plant-in-europe\"><u>largest cultivated meat pilot plant in Europe</u></a></li></ul><hr><h1>Biosecurity</h1><ul><li>The World Bank has set up a new <a href=\"https://www.worldbank.org/en/news/press-release/2022/09/09/new-fund-for-pandemic-prevention-preparedness-and-response-formally-established\"><u>financial intermediary fund for pandemic prevention, preparedness and response</u></a> to strengthen capabilities in low- and middle-income countries with over $1.4 billion in financial commitments already announced</li><li>Byron Cohen - <a href=\"https://forum.effectivealtruism.org/posts/hGEdeqtmbZ8EMdvpc/biosecurity-challenges-posed-by-dual-use-research-of-concern\"><u>Biosecurity challenges posed by Dual-Use Research of Concern</u></a></li><li>Hear This Idea podcast with Tessa Alexanian and Janvi Ahuja on <a href=\"https://hearthisidea.com/episodes/alexanian-ahuja\"><u>synthetic biology and GCBRs</u></a></li><li>Andy Graham and Tsmilton - <a href=\"https://forum.effectivealtruism.org/posts/D6oKe6kjHeBeiCtWn/developing-next-gen-ppe-to-reduce-biorisks\"><u>Developing Next Gen PPE to Reduce Biorisks</u></a></li><li><a href=\"https://www.openphilanthropy.org/open-philanthropy-biosecurity-scholarships/\"><u>Open Philanthropy Biosecurity Scholarships</u></a> - Apply by 11th October</li></ul><hr><h1>Existential &amp; Catastrophic Risks</h1><ul><li>Kelsey Piper - <a href=\"https://www.vox.com/future-perfect/23362175/un-human-development-report-ord-existential-security\"><u>How to stop rolling the dice on the destruction of human civilization</u></a></li><li>Ethan Siegel - <a href=\"https://bigthink.com/starts-with-a-bang/redirecting-a-killer-asteroid/\"><u>The biggest challenges of redirecting a killer asteroid</u></a></li><li>Rani Martin - <a href=\"https://forum.effectivealtruism.org/posts/ggiCDnYcSKLxwFbBv/the-pugwash-conferences-and-the-anti-ballistic-missile\"><u>The Pugwash Conferences and the Anti-Ballistic Missile Treaty as a case study of Track II diplomacy</u></a></li><li>James R looking at what <a href=\"https://forum.effectivealtruism.org/posts/Xbt3QL9ycdaDezZFP/what-could-a-fellowship-scheme-aimed-at-tackling-the-biggest\"><u>potential fellowship schemes/secondments</u></a> could look like for tackling existential threats</li><li>NASA\u2019s <a href=\"https://www.nasa.gov/press-release/nasa-s-dart-mission-hits-asteroid-in-first-ever-planetary-defense-test\"><u>DART mission hits asteroid</u></a> in first planetary defense test</li><li>WHO has released a <a href=\"https://www.who.int/publications/i/item/9789240056107\"><u>global guidance framework</u></a> for the responsible use of the life sciences: mitigating biorisks and governing dual-use research</li><li>The Neoliberal podcast with <a href=\"https://www.stitcher.com/show/the-neolib-podcast/episode/saving-the-world-from-catastrophic-weapons-ft-christine-parthemore-206763811\"><u>Christine Parthemore</u></a> discussing the risks from nuclear and biological weapons</li><li>Lingnan University has set up the <a href=\"https://www.ln.edu.hk/lingnan-touch/121/lu-establishes-asia-s-first-catastrophic-risk-centre\"><u>Hong Kong Catastrophic Risk Centre</u></a></li><li>The Future of Life Institute has <a href=\"https://futureoflife.org/nuclear-war-grant-rfp/\"><u>a request for proposals</u></a> targeting research on the humanitarian impacts of nuclear weapons use and nuclear war, initial deadline 15th November. They are planning on granting at least $3,000,000</li></ul><hr><h1>Improving Institutions &amp; Metascience</h1><ul><li>Sophia Brown - <a href=\"https://forum.effectivealtruism.org/posts/eJfHbPp8r9AQzwfnQ/improving-improving-institutional-decision-making-a-brief\"><u>Improving \"Improving Institutional Decision-Making\": A brief history of IIDM</u></a></li><li>Miranda Dixon-Luinenburg - <a href=\"https://www.vox.com/future-perfect/2022/9/18/23356630/open-science-academic-research-paywall-biden\"><u>Tearing down the academic research paywall could come with a price</u></a></li><li>The Foresight Institute podcast with <a href=\"https://www.stitcher.com/show/the-foresight-institute-podcast/episode/tom-kalil-career-counseling-with-chief-innovation-officer-at-schmidt-futures-206785677\"><u>Tom Kalil</u></a>, Chief Innovation Officer at Schmidt Futures</li><li>The Center for Open Science looking at the new policy update from the White House Office of Science Technology &amp; Policy <a href=\"https://www.cos.io/blog/white-house-ostp-guidance-advances-open-access-and-data-sharing\"><u>ensuring free, immediate, and equitable access to federally funded research</u></a></li></ul><hr><h1>Progress Studies</h1><ul><li>Big Think has an <a href=\"https://bigthink.com/special-issues/the-progress-issue/\"><u>issue focused on progress</u></a>, including contributions from Tyler Cowen, Hannah Ritchie, Jason Crawford, Saloni Dattani and Rutger Bregman</li><li>Jason Crawford - <a href=\"https://rootsofprogress.org/towards-a-philosophy-of-safety\"><u>Towards a philosophy of safety</u></a></li></ul><hr><h1>Environment</h1><ul><li>Hannah Ritchie - <a href=\"https://worksinprogress.substack.com/p/notes-on-progress-an-environmentalist\"><u>Why being an effective environmentalist can often feel like being a bad one</u></a></li><li>Turchin - <a href=\"https://forum.effectivealtruism.org/posts/bdSpaB9xj67FPiewN/a-pin-and-a-balloon-anthropic-fragility-increases-chances-of\"><u>A Pin and a Balloon: Anthropic Fragility Increases Chances of Runaway Global Warming</u></a></li><li>A new report analysing the environmental benefits of <a href=\"https://thebreakthrough.org/issues/food-agriculture-environment/agriculture-research-and-development-growing-food-for-the-next-generation?\"><u>agricultural research and development</u></a></li><li>Vasco Grilo - <a href=\"https://forum.effectivealtruism.org/posts/NbWeRmEsBEknNHqZP/longterm-cost-effectiveness-of-founders-pledge-s-climate\"><u>Longterm cost-effectiveness of Founders Pledge's Climate Change Fund</u></a></li><li>Cullen O'Keefe - <a href=\"https://forum.effectivealtruism.org/posts/fEcP9zzrXBKaiwKLG/contra-appiah-defending-defending-climate-villains\"><u>Contra Appiah Defending Defending \"Climate Villains\"</u></a></li></ul><hr><h1>Longtermism</h1><ul><li>Future Matters newsletter on <a href=\"https://forum.effectivealtruism.org/posts/ZzwMBRq5KAo6wfP4K/future-matters-5-supervolcanoes-ai-takeover-and-what-we-owe\"><u>Supervolcanoes, AI takeover and What We Owe the Future</u></a></li><li>Cold Button Issues - <a href=\"https://forum.effectivealtruism.org/posts/KiafCpixvtg5BWfyZ/the-base-rate-of-longtermism-is-bad\"><u>The Base Rate of Longtermism Is Bad</u></a></li><li>Julian Hazell - <a href=\"https://hazell.substack.com/p/a-more-convincing-case-for-longtermism\"><u>A more convincing case for longtermism</u></a></li><li>Thomaaas - <a href=\"https://forum.effectivealtruism.org/posts/zLZMsthcqfmv5J6Ev/the-discount-rate-is-not-zero\"><u>The discount rate is not zero</u></a></li><li>Brian Kateman - <a href=\"https://www.forbes.com/sites/briankateman/2022/09/06/optimistic-longtermism-is-terrible-for-animals/?\"><u>Optimistic \u201cLongtermism\u201d Is Terrible For Animals</u></a></li><li>Dan Luu - <a href=\"https://forum.effectivealtruism.org/posts/ujzBegXSieyR6dJ5f/dan-luu-futurist-prediction-methods-and-accuracy\"><u>Futurist prediction methods and accuracy</u></a></li><li>Benjamin Todd on <a href=\"https://twitter.com/ben_j_todd/status/1570387077966987268\"><u>four types of Longtermism</u></a></li><li>Garrison Lovely - <a href=\"https://jacobin.com/2022/09/socialism-longtermism-effective-altruism-climate-ai\"><u>The Socialist Case for Longtermism</u></a></li><li>Michael Dickens - <a href=\"https://forum.effectivealtruism.org/posts/dvz2FWu2fTBG9E2oe/should-patient-philanthropists-invest-differently\"><u>Should Patient Philanthropists Invest Differently?</u></a></li><li>Dwarkesh Patel - <a href=\"https://forum.effectivealtruism.org/posts/v4Z6phNcDsdXtzj2K/evaluation-of-longtermist-institutional-reform\"><u>Evaluation of Longtermist Institutional Reform</u></a></li><li>William Macaskill writing in the Big Think - <a href=\"https://bigthink.com/the-future/spaceguard-what-we-owe-the-future/\"><u>What NASA\u2019s Spaceguard can teach us about our uncertain future</u></a></li><li>Sigal Samuel <a href=\"https://www.vox.com/future-perfect/23298870/effective-altruism-longtermism-will-macaskill-future\"><u>writing for Vox about longtermism</u></a></li><li>Richard Ren - <a href=\"https://forum.effectivealtruism.org/posts/S9JeqH4qYvoLZqq9c/an-entire-category-of-risks-is-undervalued-by-ea-summary-of\"><u>Systemic Cascading Risks: Relevance in Longtermism &amp; Value Lock-In</u></a></li></ul><hr><h1>AI Policy</h1><ul><li>Mauricio - <a href=\"https://forum.effectivealtruism.org/posts/BJtekdKrAufyKhBGw/ai-governance-needs-technical-work\"><u>AI Governance Needs Technical Work</u></a></li><li>Matthijs Maas - <a href=\"https://forum.effectivealtruism.org/posts/np3KfjMadGsRc5qCm/artificial-intelligence-governance-under-change-phd\"><u>Artificial Intelligence Governance under Change</u></a></li><li>Haydn Belfield - <a href=\"https://forum.effectivealtruism.org/posts/uSH6DqjzggAYQGjxm/the-rival-ai-deployment-problem-a-pre-deployment-agreement\"><u>The Rival AI Deployment Problem: a Pre-deployment Agreement as the least-bad response</u></a></li><li>Micha\u00ebl Trazzi <a href=\"https://forum.effectivealtruism.org/posts/PauhAAw7Y5bHMawkT/shahar-avin-on-how-to-strategically-regulate-advanced-ai\"><u>interviewing Shahar Avin</u></a> on how to strategically regulate advanced AI systems</li></ul><hr><h1>Technical AI</h1><ul><li>Thomas Larsen and Eli Lifland - <a href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is\"><u>What Everyone in Technical Alignment is Doing and Why</u></a></li><li>Janus with a post on what it would look like <a href=\"https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\"><u>if self-supervised learning created AGI or its foundation</u></a></li><li>Akash - <a href=\"https://forum.effectivealtruism.org/posts/jZnZuNhF4C7zrALsu/7-traps-that-we-think-new-alignment-researchers-often-fall\"><u>7 traps that (we think) new alignment researchers often fall into</u></a></li></ul><hr><h1>AI &amp; Forecasting</h1><ul><li>David Roodman with a <a href=\"https://forum.effectivealtruism.org/posts/tAsyRARbkMym5D4jK/roodman-s-thoughts-on-biological-anchors\"><u>review of Ajeya Cotra's Forecasting TAI with biological anchors</u></a></li><li>Eli Lifland - <a href=\"https://forum.effectivealtruism.org/posts/EG9xDM8YRz4JN4wMN/samotsvety-s-ai-risk-forecasts\"><u>Samotsvety's AI risk forecasts</u></a></li><li>Micha\u00ebl Trazzi interviewing <a href=\"https://forum.effectivealtruism.org/posts/ukaauHKCc9sDNeBhK/alex-lawsen-on-forecasting-ai-progress\"><u>Alex Lawsen on forecasting AI progress</u></a></li></ul><hr><h1>Other AI</h1><ul><li>Nuno Sempere - <a href=\"https://forum.effectivealtruism.org/posts/EPhDMkovGquHtFq3h/an-experiment-eliciting-relative-estimates-for-open\"><u>An experiment eliciting relative estimates for Open Philanthropy\u2019s 2018 AI safety grants</u></a></li><li>Eleni A - <a href=\"https://forum.effectivealtruism.org/posts/PWKWEFJMpHzFC6Qvu/alignment-is-hard-communicating-that-might-be-harder\"><u>Alignment is hard. Communicating that, might be harder</u></a></li><li>Fergus Q - <a href=\"https://forum.effectivealtruism.org/posts/hLbWWuDr3EbeQqrmg/reasons-for-my-negative-feelings-towards-the-ai-risk\"><u>Reasons for my negative feelings towards the AI risk discussion</u></a></li><li>Henry J - <a href=\"https://forum.effectivealtruism.org/posts/Q7gqF9ZCah2BEwZ9b/a-california-effect-for-artificial-intelligence\"><u>A California Effect for Artificial Intelligence</u></a></li><li>Mathieu Putz and Rudolf - <a href=\"https://forum.effectivealtruism.org/posts/iBeWbfQLA9EKfsdhu/why-we-re-not-founding-a-human-data-for-alignment-org\"><u>Why we're not founding a human-data-for-alignment org</u></a></li><li>Kat Woods &amp; Amber Dawn - <a href=\"https://forum.effectivealtruism.org/posts/RkpdA8763yGtEovj9/two-reasons-we-might-be-closer-to-solving-alignment-than-it\"><u>Two reasons we might be closer to solving alignment than it seems</u></a></li><li>Micha\u00ebl Trazzi <a href=\"https://forum.effectivealtruism.org/posts/2xrTTgvosGSsM85RZ/katja-grace-on-slowing-down-ai-ai-expert-surveys-and\"><u>interviewing Katja Grace</u></a> on Slowing Down AI, AI Expert Surveys And Estimating AI Risk</li></ul><hr><h1>Philosophy</h1><ul><li>80,000 Hours podcast with <a href=\"https://80000hours.org/podcast/episodes/andreas-mogensen-deontology-and-effective-altruism/\"><u>Andreas Mogensen on whether effective altruism is just for consequentialists</u></a></li><li>Calvin Baker - <a href=\"https://forum.effectivealtruism.org/posts/Gk7NhzFy2hHFdFTYr/a-dilemma-for-maximize-expected-choiceworthiness-mec\"><u>A dilemma for Maximize Expected Choiceworthiness</u></a></li><li>Larks - <a href=\"https://forum.effectivealtruism.org/posts/o5Q8dXfnHTozW9jkY/the-long-reflection-as-the-great-stagnation\"><u>The Long Reflection as the Great Stagnation</u></a></li><li>Richard Y Chappell - <a href=\"https://forum.effectivealtruism.org/posts/AgQx44vtw5bi4LmBh/puzzles-for-everyone\"><u>Puzzles for Everyone</u></a></li><li>William D'Alessandro - <a href=\"https://forum.effectivealtruism.org/posts/DKe5eQhJoLNMWgaQv/deontology-the-paralysis-argument-and-altruistic-longtermism\"><u>Deontology, the Paralysis Argument and altruistic longtermism</u></a></li></ul><hr><h1>Will MacAskill Interviews</h1><ul><li><a href=\"https://forum.effectivealtruism.org/posts/HYoo94dsMyWD4Jy3F/william-macaskill-the-daily-show\"><u>The Daily Show with Trevor Noah</u></a></li><li><a href=\"https://www.theatlantic.com/health/archive/2022/09/oxford-philosophy-professor-william-macaskill-effective-altruism-interview/671597/\"><u>The Atlantic</u></a></li><li><a href=\"https://twitter.com/willmacaskill/status/1566854841372094464\"><u>Max Roser interviewing Will MacAskill</u></a> for Intelligence Squared</li></ul><hr><h1>Stories</h1><ul><li>Eleni A - <a href=\"https://forum.effectivealtruism.org/s/3GcDihvoYpkTGstwi/p/mHduzSmbfTuB5Kxyr\"><u>Talking EA to my philosophy friends</u></a></li><li>Carmen Csilla Medina on her experience <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:6974397119600209920/\"><u>teaching at Condor Camp</u></a>, an introductory retreat to EA for Brazilian students</li><li>GWWC Podcast with Michael Noetel discussing the <a href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkLnBvZGJlYW4uY29tL2dpdmluZ3doYXR3ZWNhbi9mZWVkLnhtbA/episode/Z2l2aW5nd2hhdHdlY2FuLnBvZGJlYW4uY29tLzdlMGIzMzZkLTM0MzUtMzAwZS04ZTU1LTc2MmQ3MzRkODI0MQ?sa=X&amp;ved=0CAUQkfYCahcKEwjQn7Hi3Ln6AhUAAAAAHQAAAAAQAQ\"><u>mistakes he made</u></a> when trying to improve the world</li><li>Sam Brown - <a href=\"https://forum.effectivealtruism.org/posts/6xX96ZqFtH5n7mchW/my-emotional-reaction-to-the-current-funding-situation\"><u>My emotional reaction to the current funding situation</u></a></li><li>Giving What We Can member profiles</li><li><a href=\"https://www.givingwhatwecan.org/blog/member-profile-fernando-martin-gullans\"><u>Fernando Martin-Gullans</u></a> - Professional ballet dancer</li><li><a href=\"https://www.givingwhatwecan.org/blog/member-profile-rupal-ismin\"><u>Rupal Ismin</u></a> - Director of a research commercialisation centre at the University of Sydney</li><li><a href=\"https://www.givingwhatwecan.org/blog/member-profile-peter-jensen\"><u>Peter Jensen</u></a> - Retired U.S. Army officer</li></ul><hr><h1>Other Links</h1><ul><li>A post <a href=\"https://forum.effectivealtruism.org/posts/j2ccaxmHcjiwGDs9T/ea-vs-fire-reconciling-these-two-movements\"><u>reconciling the EA and Financial Independence movements</u></a></li><li>Econ Talk podcast with <a href=\"https://www.econtalk.org/erik-hoel-on-effective-altruism-utilitarianism-and-the-repugnant-conclusion/\"><u>Erik Hoel on EA, utilitarianism and the repugnant conclusion</u></a></li><li>Charlotte - <a href=\"https://forum.effectivealtruism.org/posts/2inLysWXiWqBwo4yv/exploring-ea-donation-parliaments\"><u>Exploring (EA) Donation Parliaments</u></a></li><li>Interview with author <a href=\"https://forum.effectivealtruism.org/posts/dcBfdecnm5gzntshZ/author-rutger-bregman-about-effective-altruism-and\"><u>Rutger Bregman on effective altruism and philanthropy</u></a></li><li>Vipul Naik - <a href=\"https://forum.effectivealtruism.org/posts/F4DxPrfmnEtEwhPJu/levels-of-donation\"><u>Levels of donation</u></a></li><li>Julia Wise looking at <a href=\"https://juliawise.net/moral-aesthetics/\"><u>moral aesthetics</u></a> of EA compared to Quakers</li><li>An obituary for <a href=\"https://forum.effectivealtruism.org/posts/ivep4R7LoSLhWwHGX/peter-eckersley-1979-2022\"><u>Peter Eckersley</u></a></li><li>Elizabeth - <a href=\"https://forum.effectivealtruism.org/posts/AuewiZayG6xBeQfKK/impact-shares-for-speculative-projects\"><u>Impact Shares For Speculative Projects</u></a></li><li>Sam Stowers - <a href=\"https://forum.effectivealtruism.org/posts/jAqm6gXt8QZfj8h5c/altruist-dreams-a-collaborative-art-piece-from-eag-sf\"><u>Altruist Dreams</u></a> - a collaborative art piece from EAG SF</li><li>Habiba Islam - <a href=\"https://twitter.com/FreshMangoLassi/status/1566854492032729089\"><u>A peek into the EA Forum if Hamilton characters wrote the posts</u></a></li></ul><hr><h1>Good News</h1><ul><li>Measles cases <a href=\"http://apps.who.int/iris/bitstream/handle/10665/361744/WER9733-eng-fre.pdf\"><u>declined by 92% between 2002 and 2020 in southeast Asia</u></a> and the death rate decreased by 97% - saving an estimated 9.3 million lives during this period</li><li>Maternal mortality rates in Zimbabwe declined from <a href=\"https://gh.bmj.com/content/bmjgh/7/8/e009465.full.pdf\"><u>657 deaths per 100,000 births in 2007, to 217 per 100,000 births in 2019</u></a></li><li>Togo has become <a href=\"https://www.who.int/news/item/22-08-2022-who-director-general-congratulates-togo-on-becoming-first-country-to-eliminate-four-neglected-tropical-diseases\"><u>the first country to eliminate four neglected tropical diseases</u></a></li><li><a href=\"https://about.bnef.com/blog/wind-and-solar-top-10-of-global-power-generation-for-first-time\"><u>Wind and solar top 10% of global power generation</u></a> for the first time</li><li>Italy <a href=\"https://plantbasednews.org/culture/ethics/italy-bans-slaughter-male-chicks\"><u>bans slaughter of male chicks</u></a>, sparing up to 40 million birds a year</li><li>Deaths due to malaria in Tanzania have <a href=\"https://www.globalcitizen.org/en/content/impact-of-climate-change-on-malaria/\"><u>decreased by 71% from 2015 to 2021</u></a></li><li>The murder rate in El Salvador has <a href=\"https://elsalvadorinfo.net/homicide-rate-in-el-salvador/\"><u>dropped from 18.2 per day in 2015 to 3.1 per day in 2021</u></a></li><li>Equatorial Guinea <a href=\"https://www.theguardian.com/world/2022/sep/19/equatorial-guinea-abolishes-death-penalty-state-television-reports\"><u>abolishes death penalty</u></a></li></ul>", "user": {"username": "DavidNash"}}, {"_id": "EGSNTNdm49RhXxmBt", "title": "Strange Love - Developing Empathy With Intention (or: How I Learned To Stop Calculating And Love The Cause)", "postedAt": "2022-10-01T14:03:50.740Z", "htmlBody": "<p><i>Tl;dr: if you are intellectually convinced that a cause is important, but you don\u2019t emotionally empathize with the people or animals affected, you can develop this emotional empathy intentionally.</i></p><p>When Andr\u00e9s Jim\u00e9nez Zorrilla first heard that Charity Entrepreneurship wanted to incubate a charity devoted to shrimp welfare, he was pretty sceptical. He\u2019d cared about animal rights for a long time, but this still seemed like a deeply weird idea. However, over the course of the CE incubation program, he learnt some things which shifted his perspective. He discovered that 350 billion shrimp per year are farmed for food. Even if there was only a tiny chance that shrimp are sentient, astronomical numbers like that meant that he should take the issue seriously.</p><p>Andr\u00e9s gradually became intellectually convinced that shrimp welfare could be a very important cause, but he was not yet emotionally invested. He decided that it was important to emotionally connect to the cause. When covid lockdowns lifted, he went to visit a shrimp farm and actually witness the beings he would be&nbsp;helping, an experience that he found extremely powerful.</p><p>He now runs the <a href=\"https://www.shrimpwelfareproject.org/\">Shrimp Welfare Project</a>. He says that he feels deep empathy for shrimp, as well as being intellectually convinced that it is important to improve their welfare. He even has a shrimp tattoo on his arm!</p><p><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_270 270w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_540 540w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_810 810w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_1080 1080w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_1350 1350w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_1620 1620w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_1890 1890w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_2160 2160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_2430 2430w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/f3f66467c01483ae4943e67116f6761773cdbc5ce097bc81.jpg/w_2683 2683w\">EAs sometimes say that they feel intellectually compelled by the arguments that they should care about the suffering of non-human animals, future people, digital beings, or some other group of sentients who are outside of our typical moral circle, but they struggle to have emotional empathy for the suffering of this group.</p><p>However, in my experience, this is not a fixed fact about a person -<strong>&nbsp;you can work to develop empathy for beings who are very different to you</strong>, or distant from you in time or space. In this post, we offer some advice on how to do this.</p><h1>Why should I develop more empathy?</h1><p>It\u2019s not <i>necessarily</i>&nbsp;a problem to have a mismatch between your beliefs and your emotions.</p><p>In <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/s/QMrYGgBvg64JhcQrS/p/ehZK259et52Xnvw5F&amp;sa=D&amp;source=editors&amp;ust=1664625971710114&amp;usg=AOvVaw2v5FtZgs_QprFhFz8miGf8\"><u>Radical Empathy</u></a>, Holden Karnofsky suggests that we should care <i>intellectually</i>&nbsp;about many beings and causes that we don\u2019t care about <i>emotionally</i>. The opposite is also true - in his book <a href=\"https://www.google.com/url?q=https://www.vox.com/conversations/2017/1/19/14266230/empathy-morality-ethics-psychology-compassion-paul-bloom&amp;sa=D&amp;source=editors&amp;ust=1664625971710593&amp;usg=AOvVaw0c5RxGYqV4XT9gJwUJxyy8\"><u>Against Empathy</u></a>, Paul Bloom argues that our intuitive emotional caring might lead to bad choices. EAs are generally somewhat suspicious of \u2018warm fuzzies\u2019, and it\u2019s a central aspect of EA that we should be guided more by our brains than our guts when it comes to altruism.</p><p>However, there are some reasons why people might want their empathy to match their beliefs:</p><ul><li><strong>Congruence</strong>: it\u2019s easier to decide what to do if our emotional intuitions and our considered beliefs line up. When they conflict, we are more unsure about how to act, or how to think about unfamiliar scenarios.</li><li><strong>Motivation</strong>: you might feel unmotivated to work on the causes you care about if you don\u2019t feel enough empathy for beings who benefit</li><li><strong>Understanding other EAs:</strong>&nbsp;even if you don\u2019t want to work on (for example) ending insect suffering yourself, developing some empathy for insects might help you better understand people who <i>do</i></li></ul><p>This post is about aligning our intuitive <i>emotional </i>empathy with our cognitive, intellectual empathy.</p><h1>How can I develop more empathy?</h1><p>Here are some suggestions for how to develop empathy. In general, <strong>detailed knowledge creates empathy</strong>&nbsp;(particularly for people who are already empathetic by nature). If you want to empathize with a certain type of being, try to find out more about their lives and their troubles, or imagine in detail what it\u2019s like to be them.</p><h2>Learn more about the beings you want to empathize with \ud83e\udd90</h2><p>When I (Edo) heard about Andr\u00e9s\u2019 story, I decided, as a personal challenge, to try to develop empathy for farmed shrimp. Despite initially finding the concept of shrimp welfare almost absurd, I found this surprisingly easy! For example, it turns out that shrimp are <a href=\"https://www.google.com/url?q=https://www.youtube.com/watch?v%3DZZMKQEGpJ9U&amp;sa=D&amp;source=editors&amp;ust=1664625971713200&amp;usg=AOvVaw3ZSglVoCrau2ZURHoLLoOY\"><u>quite cute</u></a>. And shrimp farming involves <a href=\"https://www.google.com/url?q=https://animalsaustralia.org/latest-news/prawn-farming/&amp;sa=D&amp;source=editors&amp;ust=1664625971713517&amp;usg=AOvVaw1g0NSyxXJprBeuLDnCsbVL\"><u>painfully removing their eyes</u></a>&nbsp;\ud83d\ude22. By just finding out more about shrimp, and the ways in which farmed shrimp suffer, I immediately developed more empathy for them.</p><h3>GiveDirectly \ud83d\udcb5</h3><p>Needless to say, most people find it a lot easier to empathize with other humans than with non-human animals like shrimp. However, even with humans, it can build our emotional empathy if we learn more about the day-to-day experience of people whose lives are very different from ours. GiveDirectly has always included stories about their recipients on their website, and they are now experimenting with a new system where <a href=\"https://www.google.com/url?q=https://www.givedirectly.org/beta/&amp;sa=D&amp;source=editors&amp;ust=1664625971714389&amp;usg=AOvVaw1ZoU9dl_XXyIoO2meKdI-E\"><u>donors are told the specific person or people that their donation will fund.</u></a>&nbsp;This helps donors understand the lives of people in poverty more concretely, and see how their donations can help them.</p><h3>Building compassion for insects \ud83d\udc1b</h3><p>If you want to care more about insect suffering, you could try to learn more about the experience of insects. We are so used to seeing insects as pests or irritants that it doesn\u2019t occur to us that insects might have their own complex internal experiences.</p><p><a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/wZB6ie8iNHF3jfwWT/why-should-i-care-about-insects&amp;sa=D&amp;source=editors&amp;ust=1664625971715469&amp;usg=AOvVaw3Wi5-IBiPGt7ROdKZMk154\"><u>In this post</u></a>, Jamie Gittins discusses some of the science of insect behaviour, highlighting facts that suggest that insects are sentient; Brian Tomasik discusses similar things <a href=\"https://www.google.com/url?q=https://reducing-suffering.org/the-importance-of-insect-suffering/&amp;sa=D&amp;source=editors&amp;ust=1664625971715710&amp;usg=AOvVaw3iNUOm68OmrGGHVN1Hf3EH\"><u>here</u></a>.</p><h2>Videos, photos and first-hand experiences \ud83d\udcf9</h2><p>We&nbsp;often find it easier to emotionally empathize with people when we directly witness their experiences: in photos, videos, or even first-hand. Animal rights organizations often share videos of cute <a href=\"https://www.google.com/url?q=https://www.peta.org/living/animal-companions/perfect-vidoes-cute-pigs/&amp;sa=D&amp;source=editors&amp;ust=1664625971716313&amp;usg=AOvVaw2Bkv6aAUKf15853N9Pjnpc\"><u>piglets</u></a>&nbsp;or fluffy chicks to make people feel warm and compassionate towards those creatures.</p><p>Some people find that witnessing suffering itself helps them become more empathetic. <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/5EqJozsDdHcF7dpPL/introduction-to-effective-altruism-ajeya-cotra&amp;sa=D&amp;source=editors&amp;ust=1664625971716864&amp;usg=AOvVaw2w3yaSwzRlMZGs-HogD03D\"><u>Ajeya Cotra </u></a>says that&nbsp;she started caring about suffering when she visited India as a child and witnessed the \u2018hunger, pain, anger, desperation\u2019 of many people living in poverty there. &nbsp;In <a href=\"https://www.google.com/url?q=https://www.youtube.com/watch?v%3DRyA_eF7W02s&amp;sa=D&amp;source=editors&amp;ust=1664625971717379&amp;usg=AOvVaw0DL9zT6TrgNd2F7LmYEQLj\"><u>this video</u></a>&nbsp;about the moral priority of preventing extreme suffering, Brian Tomasik says, \u2018I think it\u2019s important to see suffering in its raw form to appreciate how bad it can be.\u2019</p><h2>Meditation \ud83e\uddd8\u200d\u2640\ufe0f</h2><p>Max G\u00f6rlitz has developed an <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/posts/mWYbcL45T8qy58YAL/meditations-on-caring&amp;sa=D&amp;source=editors&amp;ust=1664625971718207&amp;usg=AOvVaw2UlQ59-RpcEwbDosbHOKG2\"><u>EA-flavoured metta (loving-kindness) meditation</u></a>, designed to increase our compassion for suffering beings and our motivation to end suffering.</p><h2>Visualization \ud83d\udc7d</h2><p>Sometimes, we can\u2019t develop empathy by watching videos or reading research, because the beings we want to care about don\u2019t exist yet - for example, the people, animals and digital sentiences who might exist in the far future. For this, we can use our imagination.</p><p>Reading <a href=\"https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/effective-altruism-art-and-fiction&amp;sa=D&amp;source=editors&amp;ust=1664625971718946&amp;usg=AOvVaw33rf44U-XwWkCIoYCTr1T8\"><u>science fiction</u></a>&nbsp;about future societies can make future humans seem more concrete to us. The humans (or post-humans) of the future may be very different to us, but they will likely have feelings, dreams, experiences, and hopes just like we do. You could try writing your own EA fiction (even if you don\u2019t intend to publish it) - write down some of your predictions for what will happen to sentient beings in the next hundred, thousands, million, billion\u2026 years, and imagine what that will <i>be like</i>.</p><h1>Conclusion</h1><p>A classic post argues that EAs should <a href=\"https://www.google.com/url?q=https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately&amp;sa=D&amp;source=editors&amp;ust=1664625971719691&amp;usg=AOvVaw2enGN3pwQKxJ2_0pSSFBTa\"><u>purchase fuzzies and utilons separately</u></a>, and not assume that the most important work will also <i>feel</i>&nbsp;the most urgent or the most emotionally satisfying. However, if we consciously try to align our empathetic feelings with our considered beliefs, maybe we can purchase both at once.</p><p><i><strong>Acknowledgments</strong>: This post was written collaboratively by Edo Arad and Amber Dawn Ace. The ideas are Edo\u2019s; Edo contracted Amber to help with the writing. If you are interested in working with Amber, you can email her at </i><a href=\"mailto:ambace@gmail.com\"><i>ambace@gmail.com</i></a><i>.</i><br><i>Edo would like to thank Karolina Sarek and many people from the EA Israel community and introductory courses for engaging and inspiring conversations about the topic.</i><br><i>We are especially thankful to Andr\u00e9s for agreeing to be interviewed with such kindness and humble openness. &nbsp;</i></p>", "user": {"username": "edoarad"}}, {"_id": "3Jm6tK3cfMyaan5Dn", "title": "EA is Not Religious Enough (EA should emulate peak Quakerism)", "postedAt": "2022-10-01T10:55:54.638Z", "htmlBody": "<p>Here is a post from my blog which I'd really appreciate community thoughts on - and what it might look like if we were to seriously examine how to repeat some of the policy victories of past movements of this kind. Essentially, I believe that using examples from the past can provide us with blueprints for effective actions. This is especially true the more that EA wants to grow, and the more that longtermism implies both policy actions and longevity of the movement. Text is below (I've deleted the footnotes for slightly more readability on the forums):</p><hr><p>Pretty much every criticism of Effective Altruism has some claim that EA \u201cis a lot like a religion\u201d. This is a strange and useless criticism. Religion is just an example of what it looks like when people believe things. If you watch political people (or even sometimes sports fans) together in large groups, multiple aspects of their behaviour look decidedly religious - they address one another with special terms, they have chants, sing songs, have sacred texts or even sacred unimpeachable characters, they adorn themselves in signals of their beliefs, they pontificate about perfect worlds in which their policy (or victory) is fully realised and everyone lives happily ever after, and they have special beliefs built upon assumptions they protect emotionally regardless (frequently) of evidence - it doesn\u2019t mean <i>these are religions</i>. However, I think EA <i>does have a religion</i> problem, namely, that it isn\u2019t <i>religious enough</i>.</p><p>Tyler Cowen argued at EAG DC that EAs should \u201cbe more Mormon\u201d. His point was, essentially, that the historical lessons from Mormon communities showed clear upsides and thrust people in the general direction of \u201cdoing good\u201d. In other words, the vibe of Mormonism was probably an overall good vibe and we shouldn\u2019t spend forever analysing a priori what is best to do - we can learn from history and emulate successful groups. (I should note, he didn\u2019t use the word vibe, but that is a good description of the approach to take to large historical trends we want to emulate). I agree with his criticism that EAs should weigh, understand and emulate suitable historical precedents and learn more clearly from them, but I strongly disagree that Mormonism, or another established church, is the right fit. Instead: EAs should emulate Quakerism. There are very good reasons for EAs to consider this history as the closest set of lessons they could learn from.</p><h2>Doing Longterm Good as a Minority View</h2><p>Quakerism, since its inception, was a reforming and productive presence. Quakerism, at its heart, is essentially a radical form of protestantism. Quakers believe that no individual has a privileged access to the word of God and thus everyone is to be listened to, heard and able to speak. It is, in many ways, more radical than evangelicals today - it takes further the concept of individual interpretation of the bible and instead moves this to individual interpretation writ-large. God speaks through everyone, and it is only through reflection, consideration and understanding of multiple views, that the Quaker community believes it can better understand what it is to do.</p><p>This belief has had outsized positive effects since its inception. George Fox proposed these ideas in the English civil wars, a time period wherein radical protestants were voicing increasingly violent and concerning things. Inspired by George Fox, the Quakers arise, continue that radical protestant logic a step <i>further</i>, emphasising how this actually leads to a belief in toleration of views and non-violence and set about trying to convince essentially ISIS-level protestant groups that \u201chey, maybe you should be non-violent and actually listen to others and be productive?\u201d\u2026 and it <i>works</i>? Just take a moment to appreciate the success of that. They took vast numbers of people ready to do old testament style wars and genocides and instead directed them towards non-violence, toleration and understanding. And did so just through argument, logic and reasoning from their baseline assumptions. It is incredibly impressive.</p><p>But this is not why EAs should consider better understanding Quakerism and its history. For precedent of how to achieve longterm good at a policy and social level, with a minority membership, you cannot find much better examples than those within the Quakers. Quakers were not just reformists, but were ahead of the moral curve on a bunch of issues, and improved these without violence, and often whilst constructing useful businesses that spurred the progress of the industrial revolution. Let us look at a very brief rundown of only some of their achievements:</p><h2>A Super Quick Rundown of Quaker Achievements</h2><p>Quakers were ahead of the moral curve, and acted consistently in a way we would now consider moral - even when within the context of their own time periods. Take, for exmaple, slavery.</p><p>The first statement of any religious group against slavery within the Thirteen Colonies was by Quakers in 1688. Not only this, but their beliefs were not limited to <i>statements</i>, they affected massive policy change across the British Empire. Quakers lead the Society for Effecting the Abolition of the Slave Trade in 1787, being the chief drivers behind the policy of ending of the British slave trade in 1807. Not being satisfied with simply ending the practice of slavery (a practice undertaken by every civilisation in history), they continued to drive changes, succeeding in assisting the banning of slavery across the british empire in 1838.</p><p>Again, this is not the case of a single individual pushing an agenda, or a benevolent leader: this is multiple Quaker communities deciding on morally good actions and working together to achieve them. This is the result of the Quaker approach to discussion, and policy change. They were non-violent, considered, calm but <i>principled</i>. They had beliefs that were well constructed, well founded and considered - and beliefs they held <i>strongly to</i>, but never violently.</p><p>One of my personal favourite examples of this is the Quaker marriage ceremony. As Quakers refuse to recognise any privileged access to the bible, they do not have priests. In the mid-17th Century, the English state wanted everyone to be married according to the Book of Common prayer, administered by a priest. Quakers did not adhere to this. Instead, Quakers did their own ceremonies, without vicars, and outside of the Church of England, but they asked everyone in attendance to sign as witnesses. In the 18th century, the English state moved to reform the law so that all marriages were to be recorded in a church, by a priest. The effect of this was to regulate and record marriages, and to encourage membership of the Church - ie to expressly stop minority religious groups growing. Quakers couldn\u2019t compromise on that final point - they could, however, compromise on recording and regulation. So when the state asked \u201cWhere is the proof you got married?\u201d, Quakers could show their certificates and reply \u201cThere were 100 witnesses present, are you really saying it didn\u2019t happen because we didn\u2019t get a priest to do it?\u201d. This reasoning <i>worked</i>, and the 1753 Marriage Act has a specific exemption for usual marriage registration for Quaker ceremonies - an exemption in English law which continues <i>to this day</i>.</p><p>Again, this is a religious minority group, managing to obtain an exemption from a law that existed (at least in part) to regulate religious minority groups, but their marriages solved a significant problem for the state and thus they could compromise in such an unobjectionable way that this enabled them to successfully lobby for a specific exemption for their beliefs. This is real positive policy change, and policy change achieved without compromising central tenents of their beliefs.</p><p>Not only this, but Quakers were disproportionally successful in major businesses that spurred the progress of the industrial revolution. They were not objectivists, or obstructivists. They <i>built</i>, they <i>created</i> and they <i>improved</i> the world around them through contributing meaningfully to economic growth. They were not <a href=\"https://precedents.substack.com/p/degrowth-mindset\"><u>degrowthers</u></a>. <a href=\"https://en.wikipedia.org/wiki/List_of_Quaker_businesses,_organizations_and_charities\"><u>Here is a brief list </u></a>of some of the major companies headed by Quakers, many of which were central to the industrial revolution and some of which are so successful they still exist a couple of hundred years later. (Here is also a link to work on <a href=\"https://theceme.org/wp-content/uploads/2015/07/Quaker-Capitalism.pdf\"><u>Quaker Capitalism</u></a>, attempting to describe why they were so industrially successful).</p><p>This is not to say that all Quakers were unimpeachable. That is not the point. It is instead that, in aggregate, they were influential in the longterm, that this influence was, in many instances, more moral than their counterparts, and that they were successful in positive ways across social, industrial and policy lines.</p><p>Futher, am I arguing that the world would be a better place if Quakers had ruled the West? No. I am unsure that a state run by Quakerism could survive - much like Constantine in his conversion to Christianity, the theory of just wars was necessary to defend a state, and Quakerism would have to come to terms with such compromises - in other words, would have to become less Quaker in order to sustain a state. I am instead arguing that Quakerism as a minority religious group, within a wider political ecosystem, did real longterm good through policy entrepreneurialism and corporate and social innovation. This is the history that EA should explore, and spend time studying. Real lessons are to be learned from this.</p><h2>Some Broad EAs Analogies to Quakerism</h2><p>I know many EAs will dislike considering learning from religion. But there are a few points of similarity that particularly make lessons from the Quakers more immediately applicable than we otherwise might assume.</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d4d531-fd94-47ca-a488-cf0daf7a53b5_500x300.gif\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d4d531-fd94-47ca-a488-cf0daf7a53b5_500x300.gif\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d4d531-fd94-47ca-a488-cf0daf7a53b5_500x300.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d4d531-fd94-47ca-a488-cf0daf7a53b5_500x300.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d4d531-fd94-47ca-a488-cf0daf7a53b5_500x300.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7d4d531-fd94-47ca-a488-cf0daf7a53b5_500x300.gif 1456w\"></a></p><ol><li>EAs are not centrally political. Insofar as they have political views, they are more likely to be informed by demographics than logical conclusions from EA ideas (a point made by Cowen at EAG). What unites EAs as a whole is a focus on working out the best good that they can do. <i>This is not a popular view</i>, and invites suspicion and competition from other policy groups. In this sense, EA is closer to a religion than it is to political parties or pressure groups. It is less useful to learn from Democrats, or Republicans, or the UK Conservative Party or so on. It is far more useful to draw the analogy to religion. Especially to a religion which was not the head of a state, was not focused on forcing others to adopt their views, and instead on just <i>doing what they considered God wanted them to do</i> - in other words, in their own lights, what was <i>good</i>. But, unlike other groups that claim this, Quakers have <i>real tangible achievements</i> which we would consider unanimously good.</li><li>Further, EA is built fundamentally on well intentioned, good faith debate, with a desire to do the most good. Quakers had an analogous approach to debate - emphasising that no one had a privileged access to the word of God and, thereby, that all points of view should be listened to, considered and prayed over. This lead to many Quaker communities adopting a rule that no position should be taken without complete agreement within the community. Though this wasn\u2019t strictly debate - it was about listening to one another, and considering deeply each opinion. God was attempting to tell you something, whenever someone spoke. Radical openness to a variety of views is not the norm in human social groups - EAs are exceptional in this, as were Quakers.</li><li>Connectedly, both Quakers and EAs are comfortable, and even encourage, criticisms. Quakers are not a church which punishes heresies. Indeed modern Quakers even encourage atheists to attend because criticisms are as equally as informative of God\u2019s view as support is - no one has privileged access to God. EAs increasingly seek criticisms because <i>we may be wrong</i> on something, and EAs want to hear views (even sometimes those not made in good faith), to ensure they we are truly doing good, and doing so effectively. <i>This, again, is not a normal part of a movement</i>. There are very few examples of groups - especially successful groups - that this behaviour can be found in (beyond, perhaps, very successful adaptive companies and national intelligence agencies on occasion). Quakers provide an analogous blueprint of this working for a few centuries and provide EAs <i>something to refer to</i> and to <i>learn from</i>.</li><li>Finally, and briefly, Quakers also provide a model for success as a minority. Perhaps EA will become the main moral belief of the majority. I doubt this - at least for quite some time. It is far more likely that it becomes a belief of smart, connected and generally moral people. These are people likely to start businesses (and indeed many already do), or work in governmental positions of power, or create technologies, innovations or otherwise <i>build</i> things. If a criticism of Quakerism was that it was a religion of powerful aristocrats and business owners, I think this speaks to the audience drawn from its chief appeal: a form of concerted reasoning about <i>what is the best thing to do</i> - a reasoning process that invites people with the time, energy and ability (both financial and otherwise) to spend time considering this and then acting upon it. Quakers, again, provide a blueprint for a similar community - with different baseline assumptions - that EAs can learn from.</li></ol><h2>What do we do with this?</h2><p>The answer is to dedicate time and energy to understanding the success of Quakerism. How did they achieve so many incredible policy and businesses successes? How did they best ensure they were on the right-side of the moral curve? We may also ask why now their beliefs are far less distinctive - perhaps the loss of religion, or that their beliefs tracked with mainstream progressive causes that are now wholly subsumed by other groups? But I think the better question is to look to their successes in industry, policy and society more broadly. If we can understand this, and emulate the better parts - perhaps even adopt some of the <i>vibe</i> that made them so successful, we can expect future EAs to look back on three centuries of good results. That would be a legacy to be proud of, and we have blueprints we can follow. I propose we pay attention, consider and learn from them. History is a fantastic blueprint, do not ignore the data.</p><h2>A final note - unintended good</h2><p>When I got married this summer, we did so in a Quaker inspired ceremony. I have a wedding certificate signed by all attendees. I did so as I have never liked government control, do not like church power and love individuality, freedom and the liberty of expression each of us possess. What I didn\u2019t appreciate was that I now also have a record of so many of my loved ones, all in one place at one time, including the signature of my now recently deceased grandfather who watched it online and signed from his bed when we returned. It is an historical moment and a wonderful thing to have, another beautiful innovation by the Quaker community - an unintended additional good. His signature is there, for posterity. It is yet another up-side good that I didn\u2019t even appreciate or consider. I want EA to do good through choice and through accidental good externalities. What an incredible legacy of good inventions. They just couldn\u2019t help it.</p>", "user": {"username": "Lawrence Newport"}}, {"_id": "nqFQzrmg5Tzcxfzq7", "title": "Female effective altruists (ideally with research/action in development economics) to suggest for a colloquium talk?", "postedAt": "2022-10-01T09:51:31.039Z", "htmlBody": "<p>A colleague asked me for suggestions of female speakers in the EA sphere for their high-profile colloquium in Washington DC (not sure if I can disclose the institution, &amp; not sure if it's in person or virtual). This may be a terrific opportunity for both this person &amp; the EA movement. Who would you recommend?</p>", "user": {"username": "acristia"}}, {"_id": "htYajLE9PKbAh9heS", "title": "Effecting value change by secularising religion and founding EA-aligned creative media company ", "postedAt": "2022-10-01T10:06:29.300Z", "htmlBody": "<p>I have this idea that in order to reach a better future, a massive and profound change of our values throughout civilisation is necessary. In order to stop humans from exploiting each other and the planet, we need to establish a value system that holds the integrity of nature in highest esteem, while considering the human as what it is: an animal - with all shortcomings this entails (evolutionary deficiencies such as \u201cfight-or-flight\u201d). This value system should value rationality as our most powerful tool for both truth-finding and decision-making, love as the feeling that connects us with each other and our cohabiting species in a positive, mutually beneficial way and beauty (in nature, art, maths, character and anything else...) as the thing that gives our lives meaning. Beside these central values, another thing I find extremely valuable and which in my perception quite sadly \u2013 it actually seems a little unfair to me \u2013 is mostly owned by religious people, is faith.&nbsp;</p><p>My understanding of faith</p><p>What I understand faith to be at the fundamental level is an unconditional, unwavering trust in something (usually a god or metaphysical concept). From my experience it seems that this basic trust is something very positive that can be helpful in many life situations and is overall beneficial towards mental health \u2013 it affords a religious person a safe space a non-religious person simply does not have. The problem is that belief in something without evidence, i.e. good epistemic reason, is irrational and hence any rational person will find it hard to adopt religious assumptions about the world. And luckily so: belief in religious concepts such as creationism or rebirth is not merely irrational but has played out extremely harmful for humanity. It would certainly be better for people in general to only believe things with a degree of confidence aligned with the degree of certainty afforded for it by evidence \u2013 this IS rationality. Still, the fact that only religious people should benefit from the positive effects of faith seems sad to me.&nbsp;</p><p>Proposed solution</p><p>The solution ought to be quite simple, however, I believe. If we free belief from its object (god, metaphysical concept etc.), turning \u201cbelieving in XYZ (without rational epistemic reason to do so)\u201d into simply \u201cbelieving\u201d, we arrive at what I would call the highest, purest form of faith \u2013 that is <i>truly </i>unconditional trust in the world without making it depend on the truth of any fixed set of assumptions, such as the existence of god or any other religious concepts. We can set this trust as an axiom of our worldview in the sense that we choose to simply have it, simply \u201cbelieve\u201d, i.e. trust in the world and thus we may benefit from the positive effects of faith without being forced to hold irrational beliefs in order to unlock them. This kind of \u201cobject-free\u201d faith should, I think, be another central value for our civilisation to adopt in the future. Freeing faith from an object of belief should make it accessible even to materialist people who don\u2019t find any kind of spiritualism appealing. These people in particular seem, from my point of view, to often lack a sense of meaning in life and don\u2019t have an inner safe space to come back to. By simply experiencing profound beauty, such as can be found in nature or music just to name some examples, this basic trust should be possibly developed by anybody, I think, without necessitating any kind of esotericism.&nbsp;</p><p>Speculative epistemic reasons for assuming faith</p><p>Beside the pragmatic reason of emotional safety it affords, there are also epistemic reasons to assume faith. The one that seems the most convincing to me - though maybe not to others? - is the point of maths being beautiful. Logic and maths are essentially unavoidable consequences of rational thinking and the truths they describe MUST be true in any thinkable world. They thus describe a real kind of metaphysics, a structure that is absolutely necessary and which all things in the universe can\u2019t help but obey \u2013 and at least a tiny fraction of it is accessible to human understanding. This metaphysical ANS (absolutely necessary structure) that is described by logic and maths is in many places where it is known to us beautiful, which has been a central motivator historically for many mathematicians and physicists to pursue its uncovering. The fact that something actually metaphysical that underlies the fabric of all existence is, at least in part, beautiful and therefore possible to be thought of in a positive way suggests to me that we can think of existence as fundamentally positive. Of course, you may argue our perception of beauty is merely an evolutionary artifact, a meaningless by-product of our pattern recognition abilities and hence beauty is not an attribute of the ANS itself, but merely of our cognitive representation of the part of it we can understand. But even then, this doesn\u2019t change the fact that it\u2019s thus a posteriori proven that it is possible for a cognitive representation of at least part of the ANS to be perceived as beautiful.&nbsp; However mundane the reasons for our ability to perceive things as beautiful may be, within our cognition it is still true that representations of some things are beautiful. Since the only thing ultimately limiting possibility is the ANS by prescribing what is logically consistent (i.e. fundamentally possible) and what is inconsistent (i.e. absolutely impossible), the fact that beauty is possible as an attribute of perception means it is, in a sense, already contained in the ANS, just as anything else that is or could be. Now one may argue that among the things that are and could be there is a lot of bullshit which is, then, in the same sense already contained in the ANS. However, if we think of beauty fundamentally as the concept of something being represented positively in our mind and therefore as something we value, it does not follow that we have to \u201cunvalue\u201d its opposite, i.e. \u201cugliness\u201d or negative representation, in a symmetric way. That is to say, only because we value one thing we do not need to think of the existence of its opposite as devaluating that thing/neutralising its value. We may instead consider beauty, i.e. positive representation, as meaningful whereas we consider ugliness as meaningless and therefore as something that does not matter. Thus, the beauty of a beautiful perception can never be negated, not even by millions of ugly things, because we choose to simply not care about ugly things but instead only about beautiful things, which seems like a sensible pragmatic solution. Then if beauty is contained as a possibility in the ANS this can afford us faith in the world even when ugly things are equally contained or even outnumbering the beautiful aspects by vast amounts.&nbsp;</p><p>Secularising religion</p><p>In ridding religion of its irrational (and other negative) aspects while preserving its positive ones, I think we would be utilising existing resources in an efficient way \u2013 that is to say instead of abolishing religion altogether because of its apparent conflict with science and rationality and its inherent indulgence towards abuse of power, we could just <i>update </i>it to better serve humanity\u2019s purposes. This is what I would call secularising religion. The main positive things I see in religion, besides my already mentioned understanding of faith, are: community, culture and wisdom (by which I mean ideas like \u201cforgive us our trespasses, as we forgive those who trespass against us\u201d or \u201cif you meet the Buddha on the road, kill him\u201d and other good ideas). And I think each of these points is extremely important and valuable to humanity. The way to plausibly achieve this \u201cconservative riddance\u201d is to my mind to stop taking things which are said or written in religious contexts literally and instead conceive of them as symbolic or allegorical stories that <i>may </i>be interpreted to express something meaningful but don\u2019t describe any specific actual facts. In fact, it seems to me that this is already a trend among religious people \u2013 for example in non-Catholic Northwest European Christianity, where the bible is nowadays read with a much higher level of critical reflection and abstraction than used to be the case in earlier times. If you think of \u201cgod\u201d as an abstract concept, somethings that is simply too high to be understood (yet?) by humans or as some entity that existed before the big bang and was somehow able to fine-tune the natural constants of our universe&nbsp;so as to enable life \u2013 rather than as a wizard-in-the-clouds like in medieval times \u2013 I think your faith is already quite close to this abstract form I describe \u2013 namely detached from stipulations about the actual circumstances of the physical world such as human origin or the \u201cproper\u201d role of women. Maybe the \u201cobject-free\u201d faith I\u2019m describing is in fact where religion is ultimately headed for anyway given sufficient time for reflection and abstraction to take place and thus, secularising religion would be just a matter of accelerating an already ongoing process rather than inventing something completely new.</p><p>Next steps</p><p>How to <i>realistically </i>achieve this, I have no clue. I\u2019m hoping that presenting a good enough argument to the right people might do the trick, but then institutionalised power structures like those of large religions are not easily overcome and I doubt the people in power have much interest in secularising their religion even if they were to agree with the arguments in favour of it, since for them religion is primarily about power. If you have any ideas regarding this, please tell me. On a different note, I want to use story-telling media as a means of propagating the values briefly described above (rationality, love and beauty \u2013 among others not yet mentioned \u2013 with or without the additional aspect of \u201cobject-free\u201d faith). Therefore, I\u2019m planning to found a media company pursuing the goal of creating media content such as video games, anime, (graphic) novels, audiobooks, movies, etc. etc. to inspire people towards these values and in order to critically reflect on many pressing social issues of our time, such as gender roles and patriarchy, sexuality and relationship norms, racism and speciesism, the value of culture and cultural diversity, personal growth and perfectionism, nature and sustainability and philosophical questions like the meaning of life, morality and the nature of aesthetics. Somewhat similar to the studio Ghibli, I want to make these works&nbsp;both morally and aesthetically compelling and <i>not </i>mere pieces of entertainment to earn money by wasting people\u2019s precious time, like most fiction IMO does. If anybody here has a good idea how to achieve any of those goals, or you want to collaborate UwU, please reach out! Also of course, if you find these ideas interesting or alternatively think what I\u2019m saying is complete, utter bullshit, please feel free to comment and/or talk to me! I\u2019d be happy to update or overwrite my ideas with better ones before wasting time trying to propagate false ideas any further.&nbsp;</p><p>Thanks for reading and caring</p>", "user": {"username": "Robert  Nenninger"}}, {"_id": "YgbpxJmEdFhFGpqci", "title": "Winners of the EA Criticism and Red Teaming Contest", "postedAt": "2022-10-01T01:50:09.257Z", "htmlBody": "<p>We\u2019re excited to announce the winners of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming\"><u>EA Criticism and Red Teaming Contest</u></a>.&nbsp;<strong>We had 341 submissions and are awarding $120,000 in prizes to our top 31 entries.&nbsp;</strong></p><p>We<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmw7b1cl0v68\"><sup><a href=\"#fnmw7b1cl0v68\">[1]</a></sup></span>&nbsp;set out with the primary&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Rationale\"><u>goals</u></a> of identifying errors in existing work in effective altruism, stress-testing important ideas, raising the average quality of criticism (in part to create examples for future work), and supporting a culture of openness and critical thinking. We\u2019re pleased about the progress submissions to this contest made, though there\u2019s certainly still lots of work to be done. We think the winners of the contest are both valuable in their own right as criticisms, and as helpful examples of different types of critique.</p><p>We had a large judging panel. Not all panelists read every piece (even among the winners), and some pieces have won prizes despite being read by relatively few people or having some controversy over their value. Particularly when looking at challenges to the basic frameworks of effective altruism, there can be cases where there is significant uncertainty about whether a contribution is ultimately helpful. But if it is, it\u2019s often&nbsp;<i>very</i> important, so we didn\u2019t want to exclude cases like this from winning prizes when they had some strong advocates.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref61yl0xxk5mi\"><sup><a href=\"#fn61yl0xxk5mi\">[2]</a></sup></span>&nbsp;You can read about our process and overall thoughts on the contest <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest#Notes_on_the_judging_process\">at the end of this post</a>. Prize distribution logistics are also discussed <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest#How_winners_will_get_their_prizes\">at the end of this post</a>.</p><h1>An overview of the winners</h1><ol><li><strong>Top prizes</strong> <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest#Top_prizes\">[see more]</a><ol><li><i>A critical review of GiveWell's 2022 cost-effectiveness model</i> and&nbsp;<i>Methods for improving uncertainty analysis in EA cost-effectiveness models</i> by Alex Bates (Froolow) ($25,000 total)</li><li><i>Biological Anchors external review</i> by Jennifer Lin ($20,000)</li><li><i>Population Ethics without Axiology: A Framework</i> by Lukas Gloor ($20,000)</li></ol></li><li><strong>Second prizes</strong> (runners up) \u2014 $5,000 each <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest#Second_prizes__runners_up____5_000_each_\">[see more]</a><ol><li><i>\u200b\u200bAre you really in a race? The Cautionary Tales of Szil\u00e1rd and Ellsberg</i> by Haydn Belfield</li><li><i>Against Anthropic Shadow</i> by Toby Crisford</li><li><i>An Evaluation of Animal Charity Evaluators</i> by eaanonymous1234</li><li><i>Red Teaming CEA\u2019s Community Building Work</i> by AnonymousEAForumAccount</li><li><i>A Critical Review of Open Philanthropy\u2019s Bet On Criminal Justice Reform</i> by Nu\u00f1o Sempere</li><li><i>Effective altruism in the garden of ends</i> by Tyler Alterman</li><li><i>Notes on effective altruism</i> by Michael Nielsen</li></ol></li><li><strong>Honorable mentions</strong> \u2014 $1,000 for each of the 20 in this category <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest#Honorable_mentions___1_000_for_each_of_these_20_submissions_\">[see more]</a></li></ol><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995477/mirroredImages/YgbpxJmEdFhFGpqci/pxzxmamozx7gmp7xplbp.png\"></p><h1>Top prizes</h1><h2><a href=\"https://forum.effectivealtruism.org/posts/6dtwkwBrHBGtc3xes/a-critical-review-of-givewell-s-2022-cost-effectiveness\"><i><u>A critical review of GiveWell's 2022 cost-effectiveness model</u></i></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/CuuCGzuzwD6cdu9mo/methods-for-improving-uncertainty-analysis-in-ea-cost\"><i><u>Methods for improving uncertainty analysis in EA cost-effectiveness models</u></i></a><i> </i>by Alex Bates (Froolow<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuz0nb2b6wr\"><sup><a href=\"#fnuz0nb2b6wr\">[3]</a></sup></span>) ($25,000 prize in total)</h2><p>We\u2019re awarding a total of $25,000 for these two submissions by the same author covering similar ground.&nbsp;<i>A critical review of GiveWell\u2019s 2022 cost-effectiveness model</i> is a deep dive into the strengths and weaknesses of GiveWell\u2019s analysis, and how it might be improved.&nbsp;<i>Methods for improving uncertainty analysis in EA cost-effectiveness models</i> extracts some more generalizable lessons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefl2cuqrdjdej\"><sup><a href=\"#fnl2cuqrdjdej\">[4]</a></sup></span></p><p><strong>Summary of&nbsp;</strong><i><strong>A critical review of GiveWell\u2019s 2022 cost-effectiveness model</strong></i><strong>:</strong> The submission replicates GiveWell\u2019s cost-effectiveness models, critiques their design and structure, notes some minor errors, and suggests some broader takeaways for GiveWell and effective altruism. The author emphasizes GiveWell\u2019s lack of uncertainty analysis as a weakness, notes issues with the models\u2019 architectures (external data sources appear as inputs on many different levels of the model, elements from a given level in the model \u201cgrab\u201d from others on that level, etc.), and discusses ways in which communication of the models is confusing. Overall, though, the author seems&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/6dtwkwBrHBGtc3xes/a-critical-review-of-givewell-s-2022-cost-effectiveness#Section_6___Conclusions\"><u>impressed</u></a> with GiveWell\u2019s work.</p><p>You can also see the author\u2019s own picture-based summary of their findings:&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995477/mirroredImages/YgbpxJmEdFhFGpqci/uosll1llhojeso2wniro.png\"></p><p><strong>Summary of&nbsp;</strong><i><strong>Methods for improving uncertainty analysis in EA cost-effectiveness models</strong></i><strong>:</strong> This post argues that the EA community seriously undervalues uncertainty analysis in economic modelling (that a common attitude in EA is to simply plug in \u201cbest we can do\u201d numbers and move on, whereas state-of-the-art models in Health Economics use specific tools for uncertainty analysis). The submission explains specific methods that should be applied in different cases, proposes that developing better tools for uncertainty modeling could be useful,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm20zu85x9so\"><sup><a href=\"#fnm20zu85x9so\">[5]</a></sup></span>&nbsp;notes that a deep-dive into GiveWell\u2019s model suggests that resolving \u201cmoral\u201d disagreements is more impactful than resolving empirical disagreements, and generally shares a lot of expertise about different tools for uncertainty analysis. You can see the author\u2019s summary of these tools here:&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995477/mirroredImages/YgbpxJmEdFhFGpqci/asmoaihusfxctqvotror.png\"></p><p><strong>What we liked in these posts:</strong> The author was rigorous in trying to understand what is actually important (for GiveWell and for effective altruism more broadly).<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref9lo2iej4qne\"><sup><a href=\"#fn9lo2iej4qne\">[6]</a></sup></span>&nbsp;They rebuilt GiveWell\u2019s model, allowing them to understand critical inputs and features of the model and to suggest many concrete improvements.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5rf9hpcl6p6\"><sup><a href=\"#fn5rf9hpcl6p6\">[7]</a></sup></span>&nbsp;These suggestions seem to focus on key weaknesses, not minor sidetracks that are unlikely to be decision-relevant. The author also made these posts an easy and enlightening experience for readers. The inclusion of clear and information-packed diagrams, and the addition of a second post explaining how people can approach such analyses in general, were both exemplary. We also appreciated that they drew out broader conclusions for effective altruism.</p><p>When we asked an external expert reviewer to assess these posts, they wrote:&nbsp;</p><blockquote><p><i>It\u2019s hard to overstate what a gift the author of this post has provided to GiveWell \u2013 replicating a complex model is tedious, time-consuming, and often thankless work.</i></p></blockquote><p>(In case you\u2019re considering contributing similar work, note that GiveWell&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/6wwK6Qduxt7mMmj8k/announcing-the-change-our-mind-contest-for-critiques-of-our\"><u>recently announced the \u201cChange Our Mind Contest\u201d</u></a> to encourage (more) critiques of their cost-effectiveness analyses.)</p><p><strong>What we didn\u2019t like</strong>: It\u2019s not clear that GiveWell\u2019s charity recommendations should change based on the submission\u2019s findings (although the broader lessons seem important). Given this, some panelists worried that classifying the lack of uncertainty analysis as an \u201cextremely severe\u201d error is overstating things. And within EA, GiveWell is a strong example of the type of work that can most benefit from this type of analysis; it is unclear whether the lessons are meaningfully generalizable areas in which highly sophisticated measurement and quantification are less central.</p><h2><a href=\"https://docs.google.com/document/d/1_GqOrCo29qKly1z48-mR86IV7TUDfzaEXxD3lGFQ8Wk/edit\"><i><u>Biological Anchors external review</u></i></a><i>&nbsp;</i>by Jennifer Lin ($20,000)</h2><p><strong>Summary</strong>: This is a summary and critical review of Ajeya Cotra\u2019s&nbsp;<a href=\"https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP\"><u>biological anchors report</u></a> on AI timelines.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxgaax8e5lws\"><sup><a href=\"#fnxgaax8e5lws\">[8]</a></sup></span>&nbsp;It provides an easy-to-understand overview of the main methodology of Cotra\u2019s report. It then examines and challenges central assumptions of the modelling in Cotra\u2019s report. First, the review looks at reasons why we might not expect 2022 architectures to scale to AGI. Second, it raises the point that we don\u2019t know how to specify a space of algorithmic architectures that contains something that could scale to AGI and can be efficiently searched through (inability to specify this could undermine the ability to take the evolutionary anchors from the report as a bound on timelines).</p><p><strong>What we liked</strong>: AI timelines are important in a great deal of strategic thinking in EA, especially in the longtermist context, and Cotra\u2019s report has become a standard reference, so reviewing it is engaging with central and sophisticated thinking in EA. Lin\u2019s submission is clearly written despite being on a difficult technical topic, and we appreciated its straightforwardness about its own uncertainties and confusions. The critiques that it raises seem to be of central importance rather than just nitpicking or missing the point \u2014 if the biological anchors forecasts are misleading or uninformative, this may well be why. And the review provides suggestions for further research that could improve our understanding of these key uncertainties.&nbsp;</p><p><strong>What we didn\u2019t like</strong>:<strong>&nbsp;</strong>A lot of this review was focused on understanding which of Cotra\u2019s arguments could be made most legible and strong. While this outlined important gaps in the arguments in Cotra\u2019s report (explaining why readers are not&nbsp;<i>compelled</i> to adopt certain views on timelines), the submission didn\u2019t really discuss how we could reasonably form certain important views on AI timelines (like the likelihood that small extensions of current architectures could scale to AGI, or how probable it is that the first few evolutionary approaches might contain something that works). While we admire the project of putting arguments on as firm foundations as possible (and attempting to undermine foundations others have proposed), we think this review could have done more to position itself to be helpful to people needing to make decisions in light of the current lack of fully-supported arguments.</p><h2><a href=\"https://forum.effectivealtruism.org/posts/dQvDxDMyueLyydHw4/population-ethics-without-axiology-a-framework\"><i><u>Population Ethics without Axiology: A Framework</u></i></a><i> </i>by Lukas Gloor ($20,000)</h2><p><strong>Summary</strong>: Population ethics is relevant for decisions that could affect large groups of people, some of whom might or might not exist depending on our actions. It is usually discussed with&nbsp;<i>axiologies</i> \u2013 accounts of the objective good. Gloor introduces an alternative framework that considers population ethics from the perspective of individual decision-makers. He introduces the notion of&nbsp;<i>minimal morality</i> (\u201cdon\u2019t be a jerk\u201d) that all moral agents should follow, and proposes that what to do beyond that is up to the individual in important ways.</p><p><strong>What we liked</strong>: This is an ambitious reconceptualization of a key field of ethics. It makes space to capture the powerful intuitions people often have that standard ethical discourse doesn\u2019t allow room for. It proposes a conceptualization of person-affecting views which don\u2019t have the same problems that these views are commonly understood to have (in axiological frameworks). And it provides space for scope-sensitive effective altruism to have moral force without that creating an overwhelming moral pressure to optimize.</p><p><strong>What we didn\u2019t like</strong>: This is very abstract and at times speculative. Although there are a few practical suggestions for how EA (especially longtermist EA) might present its ideas, these suggestions seem predicated on the ideas of the post being at least plausibly correct. It would have been nice to see a discussion of the likelihood that this framing would actually help people relate to EA ideas in healthier and more valuable ways.</p><h1>Second prizes (runners up \u2014 $5,000 each)</h1><p><i>Note: We didn\u2019t deliberately select winners in different categories, but we wanted to structure this announcement post a bit (rather than just listing all the runners up in one large section). So for this post, we\u2019re organizing them by their broad subject.&nbsp;</i></p><h2>Critiques of specific concepts and assumptions</h2><h3><i>\u200b\u200b</i><a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\"><i><u>Are you really in a race? The Cautionary Tales of Szil\u00e1rd and Ellsberg</u></i></a><i> </i>by Haydn Belfield</h3><p>This post tells a compelling story of how even smart people with the best intentions can end up in a harmful arms-race dynamic, and extends this lesson to effective altruism and AI safety: \u201cI am concerned that at some point in the next few decades, well-meaning and smart people who work on AGI research and development, alignment and governance will become convinced they are in an existential race with an unsafe and misuse-prone opponent.\u201d (You can see a brief summary that goes into more detail&nbsp;<a href=\"https://us8.campaign-archive.com/?u=52b028e7f799cca137ef74763&amp;id=46de2b714d&amp;e=%5BUNIQID%5D#:~:text=Cautionary%20tales%20from%20the%20history%20of%20nuclear%20weapons\">here</a>.)&nbsp;</p><p>We think many people might do well to internalize this point more strongly, and think the post helps by providing a clear example. We also loved the concrete description of a dynamic that could plausibly lead to an existential catastrophe and the fact that this description was grounded in a relevant historical analogy. On the other hand, we thought the post\u2019s suggestions for the effective altruism community were vague, and it\u2019s unclear to what extent its implicit recommendations are already understood.&nbsp;</p><h3><a href=\"https://forum.effectivealtruism.org/posts/A47EWTS6oBKLqxBpw/against-anthropic-shadow\"><i><u>Against Anthropic Shadow</u></i></a><i> </i>by Toby Crisford</h3><p>This post digs into some toy examples to challenge the concept of&nbsp;<i>anthropic shadow</i>. This post argues that anthropics is not an important factor to adjust for in assessing the world we find ourselves in; it claims that people have fallen into the (oh-so-easy) trap of confusing themselves with arguments about the idea.&nbsp;</p><p>We liked how the post went step-by-step through the thinking to help the reader build intuitions about what\u2019s going on, and we\u2019d love to be able to throw away an unnecessary concept (which might plausibly be the right response here). On the other hand, the idea of anthropic shadow has always been a niche one, and we suspect it has a limited impact on actual prioritization decisions.</p><h2>Criticisms of (work by) specific organizations</h2><p><i>A note: This category is for critiques people have made of (</i><a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Criteria\"><i><u>important</u></i></a><i>) organizations working in the EA space. We think outside scrutiny can be valuable. It can help alert organizations to their blindspots, and it can also help the broader community to understand the strengths and weaknesses of those organizations. At the same time, we\u2019re aware that it\u2019s a hard game to play since there\u2019s often a lot of context outsiders aren\u2019t aware of, so it can be unusually easy for them to miss the point. (In several of these cases one of the panelists had inside context which altered their perception of the criticism. However, they recognized a potential conflict of interest and recused themselves. For example, multiple panelists are affiliated with CEA.)</i></p><h3><a href=\"https://forum.effectivealtruism.org/posts/pfSiMpkmskRB4WxYW/an-evaluation-of-animal-charity-evaluators\"><i><u>An Evaluation of Animal Charity Evaluators</u></i></a> by eaanonymous1234</h3><p>This post argues that Animal Charity Evaluators (ACE) communicates poorly about which charities it recommends, insufficiently evaluates charities across different types of interventions (thus losing information about big differences in impact), underrates the effectiveness of animal welfare reforms, and tries to fill too many roles at the same time. It also suggests some concrete improvements, like changes to the language on ACE\u2019s website and a recommendation for ACE to pivot towards having more emphasis on producing original research (leaning into the \u201cevaluator\u201d side of its role) relative to functioning as a fund.&nbsp;</p><p>We were impressed by the level of detail in the post, its focus on big problems in ACE\u2019s work and explanations of which issues it felt were more important, and the amount of constructive work done by the author (e.g. diving into research into animal welfare reforms and outlining how this compares to other interventions ACE recommends). On the other hand, we\u2019re not sure how much of the post is novel. We\u2019re also aware of minor errors in the post, like suggesting that an organization that disbanded in 2022 is able to take on some of the roles that ACE is currently holding.&nbsp;</p><h3><a href=\"https://forum.effectivealtruism.org/posts/hbejbRBpd6quqnTAB/red-teaming-cea-s-community-building-work-2\"><i><u>Red Teaming CEA\u2019s Community Building Work</u></i></a> by AnonymousEAForumAccount</h3><p>This post lists a number of issues found in different projects run by the Centre for Effective Altruism (CEA), like understaffing, a shortage of project evaluations, and poor public communications.&nbsp;</p><p>We were impressed by the thoroughness of the post, the fact that it was critically reviewing an influential organization, and the fair and constructive spirit of the post. We think it\u2019s helpful to give the community the perspective of issues in a historical context, so they can better assess present and historical work. On the other hand, the recommendations err in the direction of generic, and since many of the issues it lists are from a past period of turbulence at CEA (which the post acknowledges), it\u2019s unclear how actionable these elements of the critique still are.&nbsp;</p><h3><a href=\"https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal\"><i><u>A Critical Review of Open Philanthropy\u2019s Bet On Criminal Justice Reform</u></i></a> by Nu\u00f1o Sempere</h3><p>This post analyzes $200M in grants spent by Open Philanthropy on criminal justice reform, and estimates that these grants were worse than their other grants in global health and development. It then draws conclusions about Open Philanthropy\u2019s policies and procedures and suggests some improvements.&nbsp;</p><p>We appreciated that the post built a rough (but useful) model for estimating the effectiveness of interventions for criminal justice reform, that it listed different reasons why Open Philanthropy might have made the grants in the first place (like value of information), that the errors it focuses on are major (cost-effectiveness of big grants), and that it targeted an extremely important entity in effective altruism \u2014 it\u2019s a good example of hitting up. On the other hand, we think it\u2019s possible that second-order effects might be especially important in US criminal justice reform (disagreeing with the author, who claims that they\u2019re comparable to those of malaria prevention), and were disappointed that these were not considered in the analysis. At least one panelist felt that the arguments and conclusions in the section on why Open Philanthropy donated to criminal justice were overly speculative and insinuated more than was warranted.&nbsp;</p><h2>Broad criticism of effective altruism as a phenomenon</h2><h3><a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\"><i><u>Effective altruism in the garden of ends</u></i></a> by Tyler Alterman</h3><p>This post tells the story of the author\u2019s journey through EA, and in particular into (and later out of) a totalizing consequentialist attitude. Even hardcore consequentialists will want to do lots of things that we regard as everyday goods because they are instrumentally useful. But the post warns us that approaching these things with an attitude of&nbsp;<i>because they\u2019re useful</i> can lead to them being less useful, as we engage with them less wholeheartedly. (\u201cBy analogy, imagine (a) reading your favorite book for its own sake vs (b) reading the same book only to get an A+ on a test. You can feel what\u2019s different about these experiences. What is it?\u201d)</p><p>We liked the rich texture of the author\u2019s account, and the conceptual underpinning offered for avoiding totalizing attitudes. The post is a clear account of how a highly involved person in EA became disillusioned. It also gives the historical example of JS Mill\u2019s breakdown, which should be widely known since it prefigures a great deal of \u201cEA disillusionment\u201d.&nbsp;On the other hand, we weren\u2019t very compelled by the alternative vision the author offers of how EA should work and felt that many of the recommendations were vague.</p><h3><a href=\"https://forum.effectivealtruism.org/posts/JBAPssaYMMRfNqYt7/michael-nielsen-s-notes-on-effective-altruism\"><i><u>Notes on effective altruism</u></i></a> by Michael Nielsen</h3><p>This post cautions against new moral systems, describes a phenomenon of \u201cEA judo\u201d (whereby people in effective altruism respond to valid outside criticism by saying the critic has simply made the case for EA even stronger, by making EA more effective), and suggests ways for people in effective altruism to avoid \u201cmisery traps\u201d \u2014 high levels of stress due to worries that one is living wrongly.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref845dcpm7xmk\"><sup><a href=\"#fn845dcpm7xmk\">[9]</a></sup></span></p><p>We felt that this was a very strong holistic critique of effective altruism that approached EA with curiosity, knowledge, and a focus on significant problems (a number of which seemed not to have been discussed before). Several panelists have referred to \u201cEA judo\u201d multiple times since reading the post, and feel that it is putting words to an important phenomenon (that is relevant for discussions of criticism). We thought that the lack of suggestion of an alternative to EA (a lack the author acknowledges) made the post weaker. Several panelists also believe that some of the author\u2019s points in the \u201cSumming up\u201d section were not well justified.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995477/mirroredImages/YgbpxJmEdFhFGpqci/ixw9j0eygibsviebkgly.png\"></p><h1>Honorable mentions ($1,000 for each of these 20 submissions)</h1><p><i>Disclaimer: Some submissions below only got two scores from the panel. This means that many of the panelists have not vetted each submission, and this should not be viewed as a strong endorsement of the claims made (even more than for the winners above). These are organized by the type of criticism they represent (although we didn\u2019t deliberately select winners in different categories \u2014 these all got rankings that made them honorable mentions).&nbsp;</i></p><h2>Philosophical work</h2><ul><li><a href=\"https://www.cambridge.org/core/journals/economics-and-philosophy/article/abs/better-vaguely-right-than-precisely-wrong-in-effective-altruism-the-problem-of-marginalism/3FE3A694EEAF686194AB22A941691205\"><strong><u>Better vaguely right than precisely wrong in effective altruism: the problem of marginalism</u></strong></a><strong> by Nicolas C\u00f4t\u00e9 and Bastian Steuwer</strong><ul><li>We liked: the construction of a clear model that identifies interventions for which a classic approach of estimating value per additional unit of resources fails (because they have discontinuous benefits).&nbsp;</li><li>We didn\u2019t like: that this submission seems to conflate effective altruism with global health and development (or perhaps with GiveWell), and we thought that a number of points made were not very original (or were misguided, by criticizing something that wasn\u2019t quite true).&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/kxEAkcEvyiwmjirjN/wheeling-and-dealing-an-internal-bargaining-approach-to\"><strong><u>Wheeling and dealing: An internal bargaining approach to moral uncertainty</u></strong></a><strong> by Michael Plant</strong><ul><li>We liked: how it explored the intuitions for how moral bargaining might work while avoiding some of the traps people worry about.</li><li>We didn\u2019t like: that the post took a long time to get to its points and didn\u2019t make it clear how its proposal differed from the parliamentary approaches that have&nbsp;<a href=\"https://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\"><u>been discussed a number of times before</u></a>.</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/xAoZotkzcY5mvmXFY/longtermism-risk-and-extinction\"><strong><u>Longtermism, risk, and extinction</u></strong></a><strong> by Richard Pettigrew</strong><ul><li>We liked: that the post introduced and clearly summarized important arguments against expected utility theory (in particular, by incorporating risk-aversion), which is very relevant to work inspired by longtermism.&nbsp;</li><li>We didn\u2019t like: that some of the premises seemed unconvincing and that the takeaways weren't very actionable (although future work might build on this paper).&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/N6hcw8CxK7D3FCD5v/existential-risk-pessimism-and-the-time-of-perils-4\"><strong><u>Existential risk pessimism and the time of perils</u></strong></a><strong> by David Thorstad</strong><ul><li>We liked: that it developed a concrete model for the value of the future, and drew implications for positions one would have to adopt to reach certain conclusions.</li><li>We didn\u2019t like: the lack of engagement with object-level reasons to find the assumptions reasonable or not. In fact, we thought the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/N6hcw8CxK7D3FCD5v/existential-risk-pessimism-and-the-time-of-perils-4?commentId=MCZTKWjuec4GTuzBx\"><u>top comment on the post</u></a> did a fantastic job of making these counterpoints, and we decided to&nbsp;<strong>award it an extra $1,000 prize</strong> (although the comment was not formally submitted to the contest, and the funds for this come from the Forum team\u2019s budget).</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/rvvwCcixmEep4RSjg/prioritizing-x-risks-may-require-caring-about-future-people\"><strong><u>Prioritizing x-risks may require caring about future people</u></strong></a><strong> by Eli Lifland</strong><ul><li>We liked: that the post addressed a narrative that has been growing in influence \u2014 that the case for mitigating existential risks should be introduced without discussing the idea that future people have moral value \u2014 and pointed out serious mistakes (or misleading oversimplifications) in this idea.&nbsp;</li><li>We didn\u2019t like: that the numbers cited in the post were extremely rough \u2014 we felt like the post made its points too strongly given how fragile the numbers it used were.</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/vZ4kB8gpvkfHLfz8d/critique-of-macaskill-s-is-it-good-to-make-happy-people\"><strong><u>Critique of MacAskill\u2019s \u201cIs It Good to Make Happy People?\u201d</u></strong></a><strong> by Magnus Vinding</strong><ul><li>We liked: that the post critiques an influential work and theory in effective altruism, and uses specific evidence against the theory.&nbsp;</li><li>We didn\u2019t like: as a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/vZ4kB8gpvkfHLfz8d/critique-of-macaskill-s-is-it-good-to-make-happy-people?commentId=mxm3d9FyWp4EGAxXJ\"><u>comment</u></a> points out, that some of the evidence is incredibly sensitive to different framings of a study. Some of the arguments presented were also not very new.&nbsp;</li></ul></li><li><a href=\"https://www.spencergreenberg.com/2022/08/tensions-between-moral-anti-realism-and-effective-altruism/\"><strong><u>Tensions between moral anti-realism and effective altruism</u></strong></a><strong> by Spencer Greenberg</strong><ul><li>We liked: that the post addresses a particular combination of beliefs that a group of people holds (and is explicit about addressing the post to this group of people), and that it draws out a potential contradiction while exploring other possible explanations.&nbsp;</li><li>We didn\u2019t like: that the conclusion is a little vague.&nbsp;</li></ul></li></ul><h2>Climate change and energy issues</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/nBN6NENeudd2uJBCQ/the-most-important-climate-change-uncertainty\"><strong><u>The most important climate change uncertainty</u></strong></a><strong> by cwa</strong><ul><li>We liked: that it pushes back against the tendency in EA to focus mostly on extreme-warming scenarios (&gt;6\u00b0C), outlining reasons for why most of the uncertainty comes from poor models of the effects of much more probable medium-warming scenarios. We also liked that the post notes potential points of disagreement and how they might affect readers\u2019 conclusions.&nbsp;</li><li>We didn\u2019t like: that we\u2019re more unsure than usual about whether the conclusions are accurate. For instance, a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nBN6NENeudd2uJBCQ/the-most-important-climate-change-uncertainty?commentId=JjndjDa2Y3eukFvNG\"><u>comment</u></a> from John Halstead points to literature from climate economics that suggests that harms from medium-warming scenarios can be constrained (although this is disputed in further comments). We also appreciate&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/nBN6NENeudd2uJBCQ/the-most-important-climate-change-uncertainty?commentId=kt9LN98mxrorPDR4L\"><u>this comment</u></a>.&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/wXzc75txE5hbHqYug/the-great-energy-descent-short-version-an-important-thing-ea\"><strong><u>The great energy descent (short version) - An important thing EA might have missed</u></strong></a><strong> by Corentin Biteau</strong><ul><li>We liked: that this makes the case for a really-big-if-true feature of the world that could impact a lot of EA prioritization.</li><li>We didn\u2019t like: that it seemed weak in its engagement with arguments about how the world might adapt to avoid the worst outcomes described (we recommend the top comments for more discussion of these issues).</li></ul></li></ul><h2>Critiques of work by specific organizations</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/ycLhq4Bmep8ssr4wR/quantifying-uncertainty-in-givewell-s-givedirectly-cost\"><strong><u>Quantifying Uncertainty in GiveWell's GiveDirectly Cost-Effectiveness Analysis</u></strong></a><strong> by Sam Nolan (Hazelfire</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbeuolx9h0ro\"><sup><a href=\"#fnbeuolx9h0ro\">[10]</a></sup></span><strong>)</strong><ul><li>We liked: listing specific reasons for performing uncertainty analyses, developing one for GiveWell\u2019s cost-effectiveness estimates for GiveDirectly, the use of this work to support future similar projects by demonstrating Squiggle\u2019s capabilities, and the inclusion of some concrete takeaways (like the fact that GiveDirectly seems to perform better if a different utility measure is used).&nbsp;</li><li>We didn\u2019t like: that we\u2019re not sure how decision-relevant the specific criticisms were (although the broader takeaways seemed useful), and, as&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ycLhq4Bmep8ssr4wR/quantifying-uncertainty-in-givewell-s-givedirectly-cost?commentId=YRgbGbdfebJrnx73Z\"><u>a comment points out</u></a>, that the analysis used some parameters that were out of distribution (numbers from the UK instead of numbers from LMICs, which we might expect to be very different).&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause\"><strong><u>A philosophical review of Open Philanthropy\u2019s Cause Prioritisation Framework</u></strong></a><strong> by Michael Plant</strong><ul><li>We liked: that it explicitly called out possible implicit assumptions and spelled out their implications; that it made concrete recommendations for possible alternate approaches.</li><li>We didn\u2019t like: that it said \u201cI don\u2019t understand what OP means by worldview so I\u2019ll assume it means a set of philosophical assumptions\u201d when this is at odds with how OP describes it (the interpretation is explicitly disavowed in the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bdiDW83SFAsoA4EeB/a-philosophical-review-of-open-philanthropy-s-cause?commentId=b7NFHrfaankovTJCF\"><u>top comment</u></a>).</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/deworming-and-decay-replicating-givewell-s-cost\"><strong><u>Deworming and decay: replicating GiveWell\u2019s cost-effectiveness analysis</u></strong></a><strong> by Joel McGuire, Samuel Dupret, and Michael Plant</strong><ul><li>We liked: that this submission points out real issues; Alex Cohen&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/deworming-and-decay-replicating-givewell-s-cost?commentId=Qt26uR9ZT6ru8xDqi\"><u>responded</u></a> on behalf of GiveWell, agreeing that incorporating decay more into their model would reduce the cost-effectiveness of deworming (they plan on conducting more research into this), and noting that making this change earlier would have redirected $2-8M in grants. (The comment notes that the submission will likely change some future funding recommendations and improve GiveWell\u2019s decision-making, which seems like a strong positive signal.) We also liked the post\u2019s discussion on reasoning transparency.&nbsp;</li><li>We didn\u2019t like: As the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MKiqGvijAXfcBHCYJ/deworming-and-decay-replicating-givewell-s-cost?commentId=Qt26uR9ZT6ru8xDqi\"><u>comment</u></a> linked above notes, it\u2019s unclear that the authors\u2019 approach to incorporating decay is accurate (it might overestimate the effect of decay due to issues with measurement) \u2014 it seems like this could lead to systematic underestimation of the effectiveness of programs with long-term benefits (due to increased uncertainty).&nbsp;</li></ul></li><li><strong>[A private submission] by Nu\u00f1o Sempere</strong><ul><li>We have discussed internally and with Nu\u00f1o the fact that this submission is private, and are sufficiently compelled by the claim that keeping it private will lead to a better outcome than a public submission. The author has shared it directly with the organization in question.</li><li>We have also encouraged Nu\u00f1o to share how his critique has been addressed within 12 months, or to make his original piece public if its core claims have not been addressed by then.</li></ul></li></ul><h2>Critiques of real-world dynamics in effective altruism</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read\"><strong><u>Critiques of EA that I want to read</u></strong></a><strong> by Abraham Rowe</strong><ul><li>We liked: that this post lists many issues that could be quite serious and deserve more consideration, and that it inspired more discussion.&nbsp;</li><li>We didn\u2019t like: that the critiques are quite vague (which is natural, given that this is a list), that some were unoriginal, and that the critiques\u2019 relative importance isn\u2019t discussed in the post (i.e. the list is basically flat \u2014 it\u2019s not clear what should be prioritized).&nbsp;</li></ul></li><li><a href=\"https://docs.google.com/document/d/1dKaPvviGNIM3xGbP8NBRd1MytXBVvGi7daP6cW4z1RQ/edit?usp=sharing\"><strong><u>EA Criticism: Vegan Nutrition</u></strong></a><strong> by Elizabeth Van Nostrand</strong><ul><li>We liked: that the submission identifies an extremely specific issue that seems potentially incredibly important (and like a bad sign about the community\u2019s epistemics), and suggests a number of changes and projects that could help address the issue.&nbsp;</li><li>We didn\u2019t like: that it didn\u2019t have an estimate of the potential harm from the lack of guidance for vegan nutrition in EA, and we\u2019re not sure about some of the factual claims.&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/YKEPXLQhYjm3nP7Td/ways-money-can-make-things-worse\"><strong><u>Ways money can make things worse</u></strong></a><strong> by Jan Kulveit</strong><ul><li>We liked: that this is a pretty comprehensive list of issues that funders should potentially pay attention to.&nbsp;</li><li>We didn\u2019t like: that the post didn\u2019t explain how some of the phenomena it lists could become real issues (it generally relied on \u201cstylized examples\u201d), or how likely it is that they\u2019re happening.</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/X8YMxbWNsF5FNaCFz/senior-ea-ops-roles-if-you-want-to-undo-the-bottleneck-hire\"><strong><u>Senior EA 'ops' roles: if you want to undo the bottleneck, hire differently</u></strong></a><strong> by AnonymousThrowAway</strong><ul><li>We liked: that the post had extremely specific and actionable takeaways in an area that\u2019s important for effective altruism (hiring).</li><li>We didn\u2019t like: that the takeaways from this post didn\u2019t seem as novel or critical as some of the other winners.&nbsp;</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/qjMPATBLM5p4ABcEB/criticism-of-ea-criticism-contest\"><strong><u>Criticism of EA Criticism Contest</u></strong></a><strong> by Zvi</strong><ul><li>We liked: that the submission criticizes a concrete thing (this competition) as a way to get at broader, often unspoken assumptions in the EA community. We particularly liked the list of 21 implicit assumptions as a jumping-off point for discussion.</li><li>We didn\u2019t like: that the post is long, a bit convoluted, and doesn't make concrete recommendations that many in the EA community are likely to find actionable. And many on our panel disagreed with the object-level claims about criticism.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefodahb5x4zg\"><sup><a href=\"#fnodahb5x4zg\">[11]</a></sup></span></li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/MjTB4MvtedbLjgyja/leaning-into-ea-disillusionment\"><strong><u>Leaning into EA Disillusionment</u></strong></a><strong> by Helen</strong><ul><li>We liked: that this post describes (and names) a real and under-discussed phenomenon, and suggested actions members of the community could take to improve (e.g. maintaining non-EA connections, viewing EA-the-community as only one of the possible paths to impact, notice disagreements and areas where you\u2019re uncomfortable with something).&nbsp;</li><li>We didn\u2019t like: large parts of this post were pretty vague, and some of the ideas were not very new.</li></ul></li><li><a href=\"https://forum.effectivealtruism.org/posts/bo6Jsvmq9oiykbDrM/aesthetics-as-epistemic-humility\"><strong><u>Aesthetics as Epistemic Humility</u></strong></a><strong> by \u00c9tienne Fortier-Dubois</strong><ul><li>We liked: that it identified a potentially important blindspot for EA (aesthetics), noted that it\u2019s a symmetric weapon, and suggested a number of specific reasons that aesthetics could be important (and flagged pitfalls for those in EA who might start paying attention to aesthetics).</li><li>We didn\u2019t like: that a number of claims seemed unjustified or too strong and some key arguments seemed incorrect.</li></ul></li></ul><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995477/mirroredImages/YgbpxJmEdFhFGpqci/jsgbgejma13xj9fxe11m.png\"></p><h1>Notes on the judging process</h1><p>We got more submissions than we were expecting \u2014 341 submissions (105 of which were submissions via the form). A few of them were entirely private (we\u2019re awarding one private prize, listed above). Some submissions (about 45) were disqualified, usually for being written earlier than our March cutoff. Approximately 60 submissions became finalists.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg72opbmrdjm\"><sup><a href=\"#fng72opbmrdjm\">[12]</a></sup></span>&nbsp;Panelists cast nearly 800 votes across the 341 submissions (all submissions got at least 2 votes, and top and second prizes got at least 4). When we were out of our depth, we tried to reach out to experts in relevant fields. We also tried to discuss disagreements on the panel as much as possible, but a number of disagreements about which submissions we should reward were unresolved, so the fact that we\u2019re awarding a prize does not mean that everyone on the panel endorses the submission.</p><p>After the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming\"><u>original announcement</u></a>, one panelist (Nicole Ross) had to step down from the panel due to other commitments. Because of this and the volume of submissions we got, we invited two other people to join the panel:&nbsp;<a href=\"https://forum.effectivealtruism.org/users/aaron-gertler\"><u>Aaron Gertler</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/bruce\"><u>Bruce Tsai</u></a>. I\u2019m extremely grateful to everyone who spent time and energy making this happen, and I want to give a special shout-out to&nbsp;<a href=\"https://forum.effectivealtruism.org/users/technicalities\"><u>Gavin Leech</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/bruce\"><u>Bruce Tsai</u></a>, who collectively gave more than a quarter of the total panelist scores.&nbsp;</p><h1>How winners will get their prizes</h1><p>We\u2019ll be emailing or messaging all winners (and referrers, when relevant) about how they should claim their prizes. If you haven\u2019t been contacted by October 8 and you think you should have gotten a prize, please email forum@centreforeffectivealtruism.org.&nbsp;</p><p>We don\u2019t plan to reach out to people who did not get prizes.&nbsp;</p><h1>Closing thoughts</h1><p>It was fascinating to see the wide range of criticisms that people submitted to this contest. There were many submissions that we felt could have been finalists, and multiple panelists remarked that they learned something important from a post that didn\u2019t end up getting a prize. And although we tried to be critical in our reviews, we were impressed by the quality of the winning submissions. It\u2019s been touching to see people who clearly care deeply about effective altruism put so much into suggesting how it could be even better.&nbsp;</p><p>Although we didn\u2019t ask for submissions in particular categories, we did find that there was some natural clustering into similar types of work.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefuuixg8ihms\"><sup><a href=\"#fnuuixg8ihms\">[13]</a></sup></span>&nbsp;We are slightly more confident in our comparisons within clusters than between clusters.</p><p>We also want to note that we deeply appreciate a lot of the projects (ranging from research to the work that organizations in effective altruism do) that got criticized by the winning submissions. We hope that readers don\u2019t come away with the sense that everything that got criticized by a winning entry in this contest is&nbsp;<i>bad</i> \u2014 we think everything criticized here has some flaws, but to a first approximation&nbsp;<i>everything</i> has flaws, and when things are valuable enough it\u2019s worth taking the time to identify and learn from those flaws (in practice that sometimes means fixing the issues, and sometimes throwing things away and starting again). Moreover, there are some laudable qualities that a certain project can have that make it more amenable to (especially useful) criticism, like&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/i9RJjun327SnT3vW8/reasoning-transparency\"><u>reasoning transparency</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\"><u>epistemic legibility</u></a>. (We think that GiveWell is a good example of this.)</p><p>Of course, the existence of criticism (and the selection of some especially high-quality criticism) doesn\u2019t solve all our problems. Most importantly, criticisms are only useful if they lead to actual changes and improvements.&nbsp;</p><p>So we would be excited to see changes in response to these criticisms. (These could be changes suggested by submissions to the contest, changes that are&nbsp;<i>prompted</i> by submissions\u2019 identification of certain issues \u2014 even if the people running the relevant projects disagree with the recommendations made by submissions, or other kinds of changes entirely.) Changes could include:</p><ul><li>Corrections of concrete errors listed in the winning submissions (e.g. faults in models, confusing language, incorrect research conclusions, etc.)</li><li>Shifts in mindset of people in EA</li><li>Shifts in prioritization decisions (by organizations and by individuals)</li><li>Development of more uncertainty analyses for important research in EA</li><li>Better feedback loops<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref400yuzgv3ab\"><sup><a href=\"#fn400yuzgv3ab\">[14]</a></sup></span></li></ul><p>Finally, one of our goals was to accelerate meaningful discussion in these areas, and (of course) awarding the critiques funding does not mean we think they are entirely correct or the final word on the subject. So&nbsp;<strong>we strongly encourage further discussion on the topics brought up by the criticisms we\u2019re highlighting here.</strong></p><p>Thank you all so much!&nbsp;</p><figure class=\"image\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995477/mirroredImages/YgbpxJmEdFhFGpqci/vfqxid6nbxxm0sazxmnf.png\"><figcaption><a href=\"https://www.digitalcommonwealth.org/search/commonwealth:ww72cb78j\">Source of these images.</a></figcaption></figure><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmw7b1cl0v68\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmw7b1cl0v68\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This announcement was written by Lizka, with significant help from Owen Cotton-Barratt and more help from Bruce Tsai, and Fin Moorhouse. Other panelists got the chance to review it and some shared feedback, but most didn't have time to read it carefully. In general, views stated here do not represent views of everyone on the panel.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn61yl0xxk5mi\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref61yl0xxk5mi\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In particular, we wanted to avoid vetoes, or rewarding submissions that satisfied everyone, which would bias us towards uncontroversial submissions.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuz0nb2b6wr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuz0nb2b6wr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These were published under the name \u201cFroolow,\u201d and we learned the author\u2019s real name after scoring was over.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnl2cuqrdjdej\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefl2cuqrdjdej\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We think that each post was individually very valuable, and having both adds to this \u2014 but is less than twice as valuable, as the posts cover similar ground. We have therefore decided to award a prize to this pair equal to the sum of a first ($20,000) and second ($5,000) prize.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm20zu85x9so\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm20zu85x9so\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Like the new programming language for probabilistic estimation,&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/squiggle\"><u>Squiggle</u></a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn9lo2iej4qne\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref9lo2iej4qne\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We were also impressed by the fact that the author \u2014 a self-described relative outsider to EA \u2014 first&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/EuEBEnQyeEoQbwjC8/what-is-the-state-of-the-art-in-ea-cost-effectiveness\"><u>posted a question</u></a> on the Forum to make sure that they\u2019d avoid straw-manning the \u201cstate of the art\u201d in EA cost-effectiveness analysis and check that GiveWell\u2019s models are the best thing to critique. We think this is a brilliant example to follow.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5rf9hpcl6p6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5rf9hpcl6p6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These suggestions included adding an uncertainty analysis, developing a system for prioritizing key inputs in their models (which would make it clearer which data is most important to check more carefully) [<a href=\"https://forum.effectivealtruism.org/posts/6dtwkwBrHBGtc3xes/a-critical-review-of-givewell-s-2022-cost-effectiveness#3_2_2_Robustness_of_data_sources_cited\"><u>1</u></a>], and re-organizing the presentation of their data, e.g. by fixing inconsistent markup in important sheets [<a href=\"https://forum.effectivealtruism.org/posts/6dtwkwBrHBGtc3xes/a-critical-review-of-givewell-s-2022-cost-effectiveness#4_3_1_Unclear_markup\">2</a>].</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxgaax8e5lws\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxgaax8e5lws\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Here\u2019s a summary of the original report:&nbsp;<a href=\"https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/\"><u>Forecasting transformative AI: the \"biological anchors\" method in a nutshell</u></a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn845dcpm7xmk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref845dcpm7xmk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Readers who are interested in this submission might also benefit from listening to this podcast episode:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/2dHk3zBmmnNTefjWB/a-podcast-episode-exploring-critiques-of-effective-altruism\"><u>A podcast episode exploring critiques of effective altruism (with Michael Nielsen and Ajeya Cotra)</u></a>&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbeuolx9h0ro\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbeuolx9h0ro\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This was published under the name \u201cHazelfire,\u201d and we learned the author\u2019s real name after scoring was over.&nbsp;</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnodahb5x4zg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefodahb5x4zg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Many members of the panel don't agree with a number of the claims the post makes (though opinions were divided), including the list of 21 assumptions. For an alternative take on criticisms, see&nbsp;<a href=\"https://astralcodexten.substack.com/p/criticism-of-criticism-of-criticism\"><u>Criticism Of Criticism Of Criticism</u></a>. While we recused panelists with a conflict of interest for other posts, we were unable to do that in this case (since all panelists had a conflict of interest, by definition) but at least one panelist did recuse themselves.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng72opbmrdjm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg72opbmrdjm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We don\u2019t plan on publishing a full list of finalists, as we haven\u2019t vetted these submissions enough for us to feel comfortable highlighting them so prominently. However, I (Lizka) encourage panelists to share any submissions they particularly liked in the comments of this post.</p><p>You can also <a href=\"https://forum.effectivealtruism.org/topics/criticism-and-red-teaming-contest\">see a lot of the submissions here</a>.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnuuixg8ihms\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefuuixg8ihms\">^</a></strong></sup></span><div class=\"footnote-content\"><p>One panelist speculates that there was a negative correlation between how much pieces criticized the foundational assumptions of EA, and how much they made crisp or actionable recommendations. They thought this may be because there is a lot of work required for deriving clear actions as well as for laying new foundations, so it\u2019s rare to see a piece do both.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn400yuzgv3ab\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref400yuzgv3ab\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Including for this contest; we\u2019d love to hear general feedback, and are also interested in hearing about any cases where a submission (or our reviews) changed your mind or actions. You might also want to tell the author(s) of the submission if this happens.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxjymve4r1j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxjymve4r1j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is a link to a public Google Document.&nbsp;</p></div></li></ol>", "user": {"username": "Lizka"}}, {"_id": "7zg6pkrL6nQDLHPmk", "title": "EA may look like a cult (and it's not just optics)", "postedAt": "2022-10-01T13:07:35.643Z", "htmlBody": "<h2>Is EA a cult?</h2><p>Cultishness is&nbsp;<a href=\"https://www.lesswrong.com/posts/yEjaj7PWacno5EvWa/every-cause-wants-to-be-a-cult\"><u>a spectrum</u></a>. As I\u2019ll demonstrate, EA has some characteristics normally associated with cults, which can give both outsiders and those already engaged with EA a very bad impression. I'm not arguing though that the situation is so bad that EA is a cult in the extreme sense of the word. But I think that people who don\u2019t know much about EA and get the impression that it\u2019s a cult are not crazy to think that.</p><h2>A narrative of EA as a cult</h2><p>What if I told you about a group of people calling themselves Effective Altruists (EAs) who wish to spread their movement to every corner of the world. Members of this movement spend a lot of money and energy on converting more people into their ideology (they call it community building). They tend to target young people, especially university students (and increasingly high school students). They give away free books written by their gurus and run fellowship programs. And they will gladly pay all the expenses for newcomers to travel and attend their events so that they can expose them to their ideas.</p><p>EAs are encouraged to consult the doctrine and other movement members about most major life decisions, including what to do for a living (career consultation), how to spend their money (donation advice), and what to do with their spare time (volunteering). After joining the movement, EAs are encouraged to give away 10% or more of their income to support the movement and its projects. It\u2019s not uncommon for EAs to socialize and live mostly with other EAs (some will get subsidized EA housing). Some EAs want even their romantic partners to be members of the community.&nbsp;</p><p>While they tend to dismiss what\u2019s considered common sense by normal mainstream society, EAs will easily embrace very weird-sounding ideas once endorsed by the movement and its leaders.</p><p>Many EAs believe that the world as we know it may soon come to an end and that humanity is under existential threat. They believe that most normal people are totally blind to the danger. EAs, on the other hand, have a special role in preventing the apocalypse, and only through incredible efforts can the world be saved. Many of these EAs describe their aspirations in utopian terms, declaring that an ideal world free of aging and death is waiting for us if we take the right actions. To save the world and navigate the future of humanity, EAs often talk about the need to influence governments and public opinion to match their beliefs.</p><h2>It\u2019s not just optics</h2><p>While I\u2019ve focused on how EA might be perceived from the outside, I think that many of the cult-like features of EA pose a real issue, so it\u2019s not just a PR problem. I don\u2019t yet have a good mental model of all the ways in which it plays out, but I believe there are other negative consequences. The impression of a cult could also explain why some of the recent media attention on EA hasn\u2019t been very positive (I\u2019m not saying it\u2019s the only reason).</p><h2>How to make EA less cultish</h2><p>Many of the features of EA that make it look and sound like a cult (e.g. pursuit of growth, willingness to accept unconventional ideas) are quite essential to the project of doing the most good, so I\u2019m not suggesting to automatically get rid of everything that could possibly be perceived as cultish (on the other hand, I think that how EA is perceived&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/82ig8odF9ooccfJfa/how-ea-is-perceived-is-crucial-to-its-future-trajectory\"><u>is quite important</u></a>, so&nbsp; we shouldn\u2019t ignore these considerations either). Having said that, I believe there are cultural norms we could embrace that would push us away from cultishness without significantly compromising other goals.</p><p>Some helpful anti-clutishness norms that are already established in EA to some extent and I\u2019d like to see further cultivated include:</p><ul><li>Critical thinking</li><li>Diversity of opinions</li><li><a href=\"https://astralcodexten.substack.com/p/criticism-of-criticism-of-criticism\"><u>Openness to criticism</u></a></li></ul><p>Other norms that I\u2019d like to see include:</p><ul><li>Advertising EA too aggressively can be counterproductive (even though we want EA to grow).</li><li>Having one\u2019s whole life revolved around EA should be considered unhealthy.</li><li>Being known as the person who only talks about EA might be a red flag.</li><li>Going against social norms is not a virtue; it\u2019s a price we sometimes have to pay for doing the right thing.</li><li><a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\"><u>Moderation</u></a> is a virtue.</li><li>Mixing work interactions and intimate relationships (such as sex or residence) shouldn't be taken lightly.</li><li>Conflicts of interests should be taken seriously.</li><li>It\u2019s important to seriously consider what non-EA people and organizations have to say, even when they don\u2019t think and communicate in EA\u2019s preferred style (and it might be tempting to dismiss as grounded in bad epistemics).</li></ul><p>Others have also written about different aspects of the problem and potential solutions (see for example&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/i6Hday6mudh5j4mZ4/ea-can-sound-less-weird-if-we-want-it-to\"><u>1</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/SjK9mzSkWQttykKu6/big-tent-effective-altruism-is-very-important-particularly\"><u>2</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building#Part_1___Reasons_I_and_others_did_not_become_an_EA\"><u>3</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/MjTB4MvtedbLjgyja/leaning-into-ea-disillusionment\"><u>4</u></a>).</p><h2>Summary</h2><p>I don\u2019t think that EA is a cult in the worst sense of the word, but it seems to have many cult-like features that easily give a bad impression (mainly to outsiders but not only). There are cultural norms and attitudes that, if cultivated, could make EA less cultish.&nbsp;</p><h2>Acknowledgements</h2><p>I want to thank&nbsp;<a href=\"https://forum.effectivealtruism.org/users/edoarad\"><u>Edo Arad</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/users/gidonkadosh\"><u>Gidon Kadosh</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/users/sella\"><u>Sella Nevo</u></a> for their feedback. Insofar as this post still sucks, it\u2019s entirely my fault.</p>", "user": {"username": "nadavb"}}, {"_id": "etMkznb7hTsk5FE8E", "title": "Small improvements for university group organizers", "postedAt": "2022-09-30T20:09:54.127Z", "htmlBody": "<p>My opinions, please comment with feedback and critiques. By \"improvements,\" I mean \"make the world a better place,\" which sometimes requires &nbsp;more time investment. Not a comprehensive list, not sorted by importance.</p><ol><li><strong>React to Slack messages.</strong><ol><li>Takes 2 or 3 seconds, shows you read the message (or appreciated it, or did the task, etc), builds a sense of community.</li></ol></li><li><strong>Provide copy+pasteable marketing </strong>when sharing opportunities with other organizers.<ol><li>Saves loads of time if you expect people to advertise for you.</li></ol></li><li><strong>Email departmental advisors</strong> to share relevant EA opportunities.<ol><li>And other people with large mailing lists (e.g. honors programs, career centers). Easy way to reach a lot of people. You can advertise opportunities that your group doesn't organize, like <a href=\"https://effectivethesis.org/\">Effective Thesis</a>.</li><li>This is an excellent way to advertise fellowships (but be cautious about mass outreach if you're a new group).</li><li>Follow-up once or twice if they don't respond, since they might not have seen your email.</li></ol></li><li><strong>Share your knowledge and resources</strong> with organizers for other groups.<ol><li>Via Google docs, Slack messages, and forum posts especially, as these reach many people without much time investment. 1:1 meetings allow you to provide much more context and details (but these only reach 1 person at a time).</li><li>To connect with new people, see \"<a href=\"https://resources.eagroups.org/support-and-funding-for-groups/connecting-with-cea-and-other-organisers\">Connecting With CEA and Other Organisers</a>\" from the EA Groups Resource Center.</li><li>Encourage feedback (Google Docs comments make this easy).</li><li>I expect that plenty of organizers don't know about time-saving resources that already exist, like the <a href=\"https://www.canva.com/brand/join?token=VaqFCyMwnlpS0t9-sQeUMw&amp;referrer=team-invite\">EA Groups Team Canva</a>.</li><li>There should be comprehensive lists with <strong>all</strong> useful resources on a given topic. Currently there are non-comprehensive lists containing only some of the useful resources.</li></ol></li><li><strong>Recurring meeting(s) with organizers</strong> for your group<strong>.</strong><ol><li>Reduces time spent on random messages like \"hey, can you send out an email for our event tonight?\" Creates an incentive to help the group, because people can report what they did and feel appreciated. But be judicious in scheduling meetings to avoid using tons of collective organizer time.</li></ol></li><li><strong>Use bold in communications </strong>for readability.</li><li><strong>Add a #humor channel </strong>to your group's Slack.</li><li><strong>What else?</strong></li></ol>", "user": {"username": "jakubkraus07@gmail.com"}}, {"_id": "c5SeLNpnHNNif6Doz", "title": "Announcing the AI Safety Nudge Competition to Help Beat Procrastination", "postedAt": "2022-10-01T01:49:22.976Z", "htmlBody": "<p><strong>Have you achieved your goal? Fill in&nbsp;</strong><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeXq1S5Qwdozhkk9YUbIxez34OW29Hgi_unYZBuFDl1TUKgBQ/viewform?usp=sharing\"><strong><u>this form</u></strong></a><strong> to be added to the prize draw!</strong></p><p>Have you been postponing doing alignment work? Is your learning progressing slower than you'd like? We've had these experiences too, so we've created the AI Safety Nudge competition to help.</p><p><strong>You have until the end of October to register your goal and complete it. Then you have until the 7th of November to register completion. We will only accept registrations up to this date.</strong></p><p><a href=\"https://forms.gle/6vYZH43AW8sh2DHC7\">Pre-register your goal here before you start work.</a></p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp/w_120 120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp/w_200 200w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp/w_360 360w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp/w_440 440w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp/w_520 520w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/12d96647b5257ebc651fd7b87c897fbfc739235e50767744.webp/w_600 600w\"></figure><p>&nbsp;</p><p>The AI Safety Nudge Competition aims to encourage people to do things related to AI Safety today instead of procrastinating it into the future by allowing them to enter into a draw if they complete the goal that they set for themselves.</p><p>Participants will define a specific goal for themselves, examples include:</p><p><strong>\u2022&nbsp;</strong>Finish reading Superintelligence</p><p><strong>\u2022&nbsp;</strong>Finish writing up a relevant blog post</p><p><strong>\u2022&nbsp;</strong>Organize a local dinner for people interested in AI Safety</p><p>&nbsp;</p><p>If you complete the goal you set out for yourself you will be entered into the draw:</p><p><strong>\u2022&nbsp;</strong>ten prizes of $100</p><p><strong>\u2022&nbsp;</strong>two prizes of $100 specifically for Australia and New Zealand</p><p>&nbsp;</p><p><strong>Unfortunately, we cannot include participants from the EU due to local laws.</strong></p><p>&nbsp;</p><p>We are relying on an honor system for the completion of projects. We would prefer if you registered your goal before you started working on it so that we can best learn from this experiment.</p><p>&nbsp;</p><p><strong>We encourage you to pick something for which you need motivation rather than something where it won't make a difference.</strong></p><p>&nbsp;</p><p>To prevent&nbsp;<strong>downside risks</strong>, certain projects would be excluded including:</p><p><strong>\u2022&nbsp;</strong>Outreach to famous people, politicians, the media, children, high-net-worth individuals, and top AI researchers</p><p><strong>\u2022&nbsp;</strong>Projects that could be controversial, come with significant down-side risks, or could produce negative PR</p><p>\u2022 AI Safety research with high capabilities externalities</p><p>&nbsp;</p><p>For the above kinds of projects, we believe that they should only be pursued after careful consideration rather than under a deadline.&nbsp;Please message chris@aisafetysupport.org if you are uncertain.</p><p>&nbsp;</p><h3><strong>Dates</strong></h3><ul><li>Applications start the 1st of October</li><li>Participants will have to finish the goal they set out for themselves by the 31st of October, but will have until the 7th Novermber to register a successful completion.</li><li>Winners will be announced by the 14th of November&nbsp;</li></ul><p>&nbsp;</p><h3><strong>Next Steps</strong></h3><h3><a href=\"https://forms.gle/6vYZH43AW8sh2DHC7\">Pre-register your goal here before you start work.</a></h3><p>Please feel free to share this post with other people you know who need additional motivation to complete an AI Safety related project.</p><p>&nbsp;</p><h3><strong>Free book</strong></h3><p>If you'd like a free book related to AI Safety or Effective Altruism such The Alignment Problem or Super-Intelligence, you can request one here:&nbsp;<a href=\"https://airtable.com/shrZ4FYiyBokPnIZm\"><u>https://airtable.com/shrZ4FYiyBokPnIZm</u></a></p><p>&nbsp;</p><p><strong>Credits:&nbsp;</strong>This competition is being produced by Chris Leong (AI Safety Australia and New Zealand) and being implemented by Marc Carauleanu. This project wouldn't be possible without the support of the Long-Term Future Fund or AI Safety Support.</p><hr><h2><strong>F. A. Q.</strong></h2><h3>How will the winners be selected?</h3><p>After you completed your goal and submitted the appropriate form, you will be added to a list with all of the participants that did the same, and 10 winners will be selected at random. The same procedure will be repeated but this time there will be a random draw to select 2 winners from the list of participants from Australia and New Zealand. As noted projects with downside risk will be excluded from winning.</p><p>&nbsp;</p><h3>Is there a limit on the size of the project?</h3><p>No, you can start/finish a project related to AI Safety of any size as long as it is doable in a month. If the entire project is difficult to finish in a month, you can state that you want to finish only a part of it during this month. Similarly, the project can be as small as reading one article that you\u2019ve been procrastinating on.</p><p>&nbsp;</p><h3>Why do I need to pre-register my project?</h3><p>We prefer if you register your project before you start on it as we believe this increases the chance this competition causes people to complete their projects and because it also allows us to collect data on the effectiveness of this competition.</p>", "user": {"username": "marc"}}, {"_id": "aoAkWaMLAHSprNBko", "title": "\"A Creepy Feeling\": Nixon's Decision to Disavow Biological Weapons", "postedAt": "2022-09-30T15:17:40.384Z", "htmlBody": "<p><i>I wrote this paper for a class in December 2021. I had been meaning to put it up on the forum for a while, but only just now got around to it. I found this research useful for my own understanding of the kinds of events that can contribute to dramatic policy changes. At the end I mention the obvious relevance to autonomous weapons regulation, but I think that lessons learned apply to all kinds of regulation of dangerous technologies.</i></p><p>In September of 1950, a ship sailed by the Golden Gate Bridge. It carried a stockpile of&nbsp;<i>Serratia marcescens&nbsp;</i>bacteria, which it released in a huge plume over the city of San Francisco. Those onboard hoped to expose as many people as possible to the bacteria. Their mission was a success, and most of the city\u2019s residents were exposed.&nbsp;</p><p>The ship was not operated by a hostile foreign government or terrorist operatives, but by the United States Navy. Though&nbsp;<i>Serratia marcescens</i> is a \u201csimulant\u201d bacterium not known to cause harm, the test showed the potential for attacks with more deadly forms of bacteria.&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw6lo9dvpx78\"><sup><a href=\"#fnw6lo9dvpx78\">[1]</a></sup></span>&nbsp;Despite the \u201cbenign\u201d nature of the bacterium, Stanford University doctors reported several bizarre cases of urinary tract infections at the time, leading eventually to one death.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdqvwt4fgy6\"><sup><a href=\"#fndqvwt4fgy6\">[2]</a></sup></span>&nbsp;The test was far from the only biological weapons test conducted in secrecy by the U.S. government from World War II until as late as 1968.</p><p>On November 25th, 1969, President Richard Nixon gave a speech to the American public following a briefing to Congress. He announced the United States would renounce the use of biological weapons,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh37mdbhp4w5\"><sup><a href=\"#fnh37mdbhp4w5\">[3]</a></sup></span>&nbsp;destroy its stockpiles, and research only what was necessary to defend against possible attacks from enemies. He also voiced support for a United Kingdom initiative to ban biological weapons internationally, which would eventually become the Biological Weapons Convention.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhfuuusplmvl\"><sup><a href=\"#fnhfuuusplmvl\">[4]</a></sup></span></p><p>How did the United States go from biological weapons testing on its own population to leading the world in opposition to biological weapons? Historians have offered many possible explanations.</p><p>One common argument is that public pressure forced Nixon\u2019s hand. In a 2002 paper, Jonathan B. Tucker, a chemical and biological weapons (CBW) expert and then a researcher at the Monterey Institute of International Studies, specifically emphasizes television reports in early 1969 and their contribution to increased awareness of the weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmfng1d5gws\"><sup><a href=\"#fnmfng1d5gws\">[5]</a></sup></span>&nbsp;Brian Balmer and Alex Spelling of University College London conducted a 2016 analysis of contemporaneous newspaper articles about biological weapons, finding that they routinely portrayed biological weapons as dangerous even if they presented mixed messages about their effectiveness.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref344te2kmgt6\"><sup><a href=\"#fn344te2kmgt6\">[6]</a></sup></span>&nbsp; Robert W. McElroy, a prelate of the Catholic church, included a discussion of CBW in a 1992 book, stressing in particular the idea that the public viewed such weapons as morally repugnant.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefp5bbblnkoua\"><sup><a href=\"#fnp5bbblnkoua\">[7]</a></sup></span>&nbsp;I will argue that public pressure was a significant factor behind Nixon\u2019s decision, but that it was not sufficient to convince Nixon that renouncing biological weapons would be safe.</p><p>A second argument is that international pressure created an environment where the American position was untenable. James Revill, a research fellow at the University of Sussex, wrote in a 2018 article that international arms control was a major factor, while also suggesting that the renunciation may have been an attempt to deflect attention from the Vietnam War as well as a response to the advocacy of international organizations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref81koa3uvjhg\"><sup><a href=\"#fn81koa3uvjhg\">[8]</a></sup></span>&nbsp;McElroy also discussed international opposition, again from a moral perspective, and Tucker acknowledged it as well. I will argue that while international developments were important in shaping Nixon\u2019s decision, they were not his primary motivation.</p><p>A third argument was that internal government scientists were behind much of the change. In a 1974 book, Joel Primack and Frank von Hippel, then at Stanford University, specifically focus on the work of Matthew Meselson, a Harvard professor and biologist who was on the President\u2019s Scientific Advisory Committee (PSAC) and had previously organized a petition to President Lyndon Johnson and testified before Congress against biological weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc0qwx77xk4k\"><sup><a href=\"#fnc0qwx77xk4k\">[9]</a></sup></span>&nbsp;Tucker also emphasizes Meselson and other scientists on the PSAC panel. I will argue that their advocacy was necessary in Nixon\u2019s final decision, though it was not sufficient.</p><p>I will argue that there were two essential steps that contributed to Nixon\u2019s decision. First, a series of public revelations and government mishaps brought more awareness to the problem of biological and chemical weapons, which had previously been highly concealed. A growing number of newspaper articles shed light on the program, while Congress became far more interested in using its oversight authority. Thus, novel public pressure convinced Nixon to reconsider both chemical and biological weapons. Second, a core group of scientists close to the White House persuaded him that the abandonment of biological weapons in particular would not be a military liability. They did so mainly through arguments that such weapons would be useless and even dangerous for a nation already in possession of nuclear weapons.</p><h2>\"Dirty Business\": The American Bioweapons Program</h2><p>The 1925 Geneva Protocol was ratified by all major world powers\u2014with the exception of the United States and Japan\u2014in the aftermath of brutal chemical warfare during the First World War.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffjapf6a83f5\"><sup><a href=\"#fnfjapf6a83f5\">[10]</a></sup></span>&nbsp;It banned the use, though not the development or proliferation, of chemical and biological weapons. The United States signed the Protocol, but it was never ratified by the Senate, meaning that it would not be legally binding.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm2yo28z1n5\"><sup><a href=\"#fnm2yo28z1n5\">[11]</a></sup></span>&nbsp;Still, in 1943, at the height of the Second World War, President Franklin D. Roosevelt declared \u201ccategorically\u201d that \u201cwe shall under no circumstances resort to the use of [chemical weapons] unless they are first used by our enemies,\u201d and his pronouncement was understood to extend to biological weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1wzexbff1tfh\"><sup><a href=\"#fn1wzexbff1tfh\">[12]</a></sup></span>&nbsp;The Secretary of War at the time wrote to Roosevelt, saying that biological warfare was \u201cdirty business\u201d but that the country \u201cmust be prepared.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefr1sdg072vcj\"><sup><a href=\"#fnr1sdg072vcj\">[13]</a></sup></span>&nbsp;And the country did prepare.</p><p>While public pronouncements by presidents regarding biological warfare were almost nonexistent for the next twenty years, the U.S. biological weapons program flourished in near perfect secrecy. At Fort Detrick in Maryland, volunteers, mainly Seventh Day Adventists who believed they were contributing to the cause of peace, participated in experiments involving dangerous germs overseen by the Army\u2019s Chemical Corps.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref61eeibdqcnj\"><sup><a href=\"#fn61eeibdqcnj\">[14]</a></sup></span>&nbsp;But the Army\u2019s experiments were not limited to volunteers: it secretly conducted simulant tests not just in San Francisco but in a Greyhound bus terminal,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7swegiel4uy\"><sup><a href=\"#fn7swegiel4uy\">[15]</a></sup></span>&nbsp;an airport,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefupigubvziwh\"><sup><a href=\"#fnupigubvziwh\">[16]</a></sup></span>&nbsp;parks,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyetygg9gs7r\"><sup><a href=\"#fnyetygg9gs7r\">[17]</a></sup></span>&nbsp;and even the Pentagon.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy09l2arqv9\"><sup><a href=\"#fny09l2arqv9\">[18]</a></sup></span>&nbsp;By 1968, the budget for chemical and biological weapons had reached $413 million<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrkxsofngdd\"><sup><a href=\"#fnrkxsofngdd\">[19]</a></sup></span>&nbsp;($3.2 billion in 2021 dollars<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefn9ob5c2vqkd\"><sup><a href=\"#fnn9ob5c2vqkd\">[20]</a></sup></span>), and the army had stockpiled agents that cause anthrax, tularemia, brucellosis, Q-fever, and Venezualan equine encephalitis, as well as anti-crop fungi.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyulhusi2m1h\"><sup><a href=\"#fnyulhusi2m1h\">[21]</a></sup></span></p><h2>The Silence of the Lambs: CBW Bursts Into The Open</h2><p>In September 1966, Harvard Professor Matthew Meselson, a frequent critic of biological and chemical weapons, organized a petition to President Lyndon Johnson signed by thousands of scientists, arguing simply that \u201cthe barriers to the use of these weapons must not be allowed to break down\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkwffhoqgdah\"><sup><a href=\"#fnkwffhoqgdah\">[22]</a></sup></span>&nbsp;A version of the petition contains a handwritten note from Meselson to a journalist: \u201cnow is the time for us to urge you to write a piece.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefap7fuh7uinw\"><sup><a href=\"#fnap7fuh7uinw\">[23]</a></sup></span>&nbsp;Meselson later spoke of intentionally trying to get his message to the press through this kind of personal outreach to journalists.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe04yj1xwh4e\"><sup><a href=\"#fne04yj1xwh4e\">[24]</a></sup></span>&nbsp;The petition was, in fact, picked up in the media,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4obbgab03yu\"><sup><a href=\"#fn4obbgab03yu\">[25]</a></sup></span>&nbsp;but Johnson did not issue a public response and there was no change in government policy.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo0yqpzieqpe\"><sup><a href=\"#fno0yqpzieqpe\">[26]</a></sup></span>&nbsp;Something more than the advocacy of scientists would be needed to push the country away from biological war.</p><p>The most significant revelation did not come until 1968. At a testing ground in Utah, the Army accidentally released a huge plume of a chemical nerve agent into the desert sky. Over the next two weeks, 3,000 sheep in the nearby area died.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefzsb1qzip18c\"><sup><a href=\"#fnzsb1qzip18c\">[27]</a></sup></span>&nbsp;At first, the Army was extremely loath to admit that there had been any testing at all, but after a Utah senator stepped in, it revised its argument to state that the sheep had died for an unrelated reason.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm8v2cjenlgk\"><sup><a href=\"#fnm8v2cjenlgk\">[28]</a></sup></span>&nbsp;Eventually, the Army agreed to pay hundreds of thousands of dollars to the sheep farmer, though they would not admit their responsibility.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvoq8b7nnrmk\"><sup><a href=\"#fnvoq8b7nnrmk\">[29]</a></sup></span></p><p>The sheep controversy finally brought dedicated attention from the press. CBS and National Educational Television covered chemical weapons in early 1969.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefoedsedo7bzd\"><sup><a href=\"#fnoedsedo7bzd\">[30]</a></sup></span>&nbsp;The biggest broadcast came when NBC\u2019s magazine&nbsp;<i>First Tuesday</i> aired a documentary about the chemical and biological weapons program on February 4th, 1969. It not only described the Utah debacle, but also the process of disposing of waste agents by sinking them in the ocean.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbypz25xyes9\"><sup><a href=\"#fnbypz25xyes9\">[31]</a></sup></span>&nbsp;The documentary generated significant public attention. Perhaps the most important viewer was Congressman Richard McCarthy. According to McCarthy\u2019s later writings, he and his wife watched the documentary together, and his wife asked him, \u201cWhat do you know about this?\u201d Despite being a member of Congress, he replied, \u201cNothing\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwx93wd1uvo\"><sup><a href=\"#fnwx93wd1uvo\">[32]</a></sup></span>&nbsp;He would not be ignorant for long.</p><h2>Not Something You Buy In A Can: Congress Steps In</h2><p>On April 30th, 1969, the Senate Foreign Relations Committee, with Congressman McCarthy present, called Matthew Meselson to testify about chemical and biological weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefbhlbh7ra5jb\"><sup><a href=\"#fnbhlbh7ra5jb\">[33]</a></sup></span>&nbsp;Meselson had previously served in the U.S. government investigating such weapons,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref01h6lyse9m4s\"><sup><a href=\"#fn01h6lyse9m4s\">[34]</a></sup></span>&nbsp;and he methodically detailed the ways in which the military used them to his still very uninformed audience. When Meselson was describing the use of aerosol clouds as a dispersal mechanism, Senator Clifford Case of New Jersey asked, \u201cIs that a word or is that a description of something that you buy in a can?\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffib9u03scef\"><sup><a href=\"#fnfib9u03scef\">[35]</a></sup></span>&nbsp;Chairman J.W. Fullbright asked Meselson whether the United States was a party to the Geneva Convention, and what the budget of the CBW program was.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqa8oa8tv69m\"><sup><a href=\"#fnqa8oa8tv69m\">[36]</a></sup></span>&nbsp;It was critical that the Senate was finally hearing about the weapons program, particularly in light of their ignorance, especially because they were hearing from Matthew Meselson, a noted anti-CBW activist. The session was wide-ranging, and included Meselson\u2019s thoughts on biological weapons: that they were a \u201ctotally unpredictable weapon\u201d that would not be useful for a country that had more predictable weapons of mass destruction, namely nuclear weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreftmf8jk0nvcc\"><sup><a href=\"#fntmf8jk0nvcc\">[37]</a></sup></span>&nbsp;Senator Gale McGee said that the topics discussed in the hearing gave him a \u201ccreepy feeling.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1x6wmznr91e\"><sup><a href=\"#fn1x6wmznr91e\">[38]</a></sup></span>&nbsp;At the end, Fullbright thanked Meselson for his \u201csurprising\u201d statements.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7dsc4qcwfwy\"><sup><a href=\"#fn7dsc4qcwfwy\">[39]</a></sup></span></p><p>Meselson\u2019s testimony did not stay in the confines of the Senate. After removal of classified information, it was released to the public and reported on by several newspapers. The Associated Press article, reprinted in multiple newspapers, described Meselson as having described \u201ca chamber of horrors\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref857r6na0jh8\"><sup><a href=\"#fn857r6na0jh8\">[40]</a></sup></span>&nbsp;Another headline read \u201cSpeaking of the Unspeakable.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref67jzax3ojmw\"><sup><a href=\"#fn67jzax3ojmw\">[41]</a></sup></span>&nbsp;The press, Congress, and Meselson were amplifying their messages.</p><p>Congress, eventually with the assistance of the Secretary of Defense, resolved to broaden their oversight role over the CBW program. In May, investigation uncovered more details about the disposal of CBW, specifically Project CHASE, which stood for \u201ccut holes and sink \u2018em\u201d in reference to the boats used to dispose of weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6o19mxjo96v\"><sup><a href=\"#fn6o19mxjo96v\">[42]</a></sup></span>&nbsp;Congress was now alert enough to be listening when the&nbsp;<i>Wall Street Journal</i> broke a story on July 18 under the appealing headline \u201cA Coup For Red Propaganda?\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefla04iyucp1\"><sup><a href=\"#fnla04iyucp1\">[43]</a></sup></span>&nbsp;It revealed the July 8th accidental explosion of a 500-pound American chemical bomb in Okinawa, which the Japanese government did not even know had any chemical weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2zewk6e91aw\"><sup><a href=\"#fn2zewk6e91aw\">[44]</a></sup></span>&nbsp;On August 8th, Senator Thomas J. McIntyre proposed an amendment to the annual defense appropriations bill requiring semiannual reports on the program to Congress and restricting means of storing and disposing of weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefotcw4zcnzhg\"><sup><a href=\"#fnotcw4zcnzhg\">[45]</a></sup></span>&nbsp;The amendment was supported by Secretary of Defense Melvin Laird and was eventually passed.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmy1lq8hsf4\"><sup><a href=\"#fnmy1lq8hsf4\">[46]</a></sup></span>&nbsp;Congress had determined that its answer to the American people would no longer be that it did not know anything. The press continued to investigate: On October 31st,&nbsp;<i>The New York Times</i> published a report alleging the Army had thousands of poison bullets containing the botilinum toxin that it claimed could only serve the purpose of \u201cassassination.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefmnph2l1btq\"><sup><a href=\"#fnmnph2l1btq\">[47]</a></sup></span>&nbsp; The jig was up for the executive branch: Congress and the press had woken up.</p><h2>An \"example of the right leadership\": The NSC Review and Nixon's Decision</h2><p>On April 30th, 1969, the very same day that Meselson was testifying before Congress, Laird had written to Henry Kissinger, Nixon\u2019s national security advisor. \u201cI am increasingly concerned about the structure of our chemical and biological warfare programs,\u201d Laird wrote, adding that \u201cit would seem reasonable to have the subject brought before the National Security Council at an early date.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefg2auh8pe2n8\"><sup><a href=\"#fng2auh8pe2n8\">[48]</a></sup></span>&nbsp;On May 9th, Laird received a response, where Kissinger promised to consider the issue before the NSC, which he chaired.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflnxsxq5gbt\"><sup><a href=\"#fnlnxsxq5gbt\">[49]</a></sup></span>&nbsp;Kissinger\u2019s decision began a flurry of reports from many different agencies of the federal government. In a press release supporting the McIntyre amendment on August 9th, 1969, while the report was being prepared, Laird made a point to separately mention \u201cchemical warfare\u201d and \u201cbiological research,\u201d even if he did not specify what exactly the relevant policy difference was between the two.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefw66mecowzd\"><sup><a href=\"#fnw66mecowzd\">[50]</a></sup></span>&nbsp;The distinction was not new to the political realm; for instance, Meselson had emphasized the unpredictability of biological weapons in particular in his testimony.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefrp4m02l93j\"><sup><a href=\"#fnrp4m02l93j\">[51]</a></sup></span>&nbsp;But it was motivated especially by events on the other side of the world: on July 10th, the UK had proposed a draft convention to the Eighteen Nation Disarmament Committee at the United Nations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefeb8jlbqorue\"><sup><a href=\"#fneb8jlbqorue\">[52]</a></sup></span>&nbsp;Their proposal was different from previous proposals in specifically focusing on biological weapons and making no provisions for chemical weapons, believing that it would be more politically feasible. Laird, likely with similar motivations, had adopted the habit of separating the two, and the internal reports reflected that separation.</p><p>The Joint Chiefs of Staff (JCS) of the Department of Defense argued for the expansion of the chemical weapons program and the maintenance of the biological weapons program. They suggested that the Soviet Union was working on both weapons, and the United States should be able to retaliate in kind, without risking having to take nuclear action.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffemsici8msu\"><sup><a href=\"#fnfemsici8msu\">[53]</a></sup></span>&nbsp;PSAC produced a report quite different from the JCS: It emphasized that biological weapons were unpredictable, difficult to store, and had long incubation periods, properties which made the technology less than useful for a country with nuclear weapons. Meselson, a member of the committee and personal friend of Kissinger\u2019s, clearly had made a mark on the report.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5o4n37n0xm8\"><sup><a href=\"#fn5o4n37n0xm8\">[54]</a></sup></span>&nbsp;Multiple studies of the report have suggested that it was highly influential on Defense Secretary Laird\u2019s surprise decision to withdraw the JCS report as he threw his weight behind the McIntyre amendment in Congress.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz7zatehveu\"><sup><a href=\"#fnz7zatehveu\">[55]</a></sup></span>&nbsp;The Office of Systems Analysis produced a report that largely followed the PSAC report, calling for only the minimum research necessary to develop defenses against biological weapons like antidotes and protective equipment.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefntncga2juun\"><sup><a href=\"#fnntncga2juun\">[56]</a></sup></span></p><p>The final National Security Council meeting was held on November 18th, 1969. General Earle Wheeler argued for maintenance of the biological weapons program.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0ro1s0jphhv\"><sup><a href=\"#fn0ro1s0jphhv\">[57]</a></sup></span>&nbsp;Science Advisor Lee DuBridge perfectly summarized Nixon\u2019s predicament, stating \u201cThere is great public interest in this subject...the value of a BW retaliatory capability is not clear.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefy9yyy7kngjs\"><sup><a href=\"#fny9yyy7kngjs\">[58]</a></sup></span>&nbsp;The discussion of public pressure, combined with Meselson\u2019s argument that the weapons were not useful, were being articulated directly to the president. Laird was adamant about separating biological weapons and chemical weapons, worrying that \u201cpeople who are against biological warfare also go against chemical warfare\u201d even though he thought this was unjustified, and favored ending biological but not chemical weapons.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnwdtk5w4d98\"><sup><a href=\"#fnnwdtk5w4d98\">[59]</a></sup></span>&nbsp;Nearly all Nixon\u2019s advisors were united in opposition to biological weapons and in favor of chemical weapons, with Wheeler being the only exception. Wheeler did not attempt to argue his point any further, essentially conceding to the unanimity of the others.</p><p>Nixon ultimately agreed with the consensus, saying that it was important to distinguish biological and chemical weapons, and notably adding that \u201cthe public relations aspect is very important.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefunsm8jfus9g\"><sup><a href=\"#fnunsm8jfus9g\">[60]</a></sup></span>&nbsp;He wanted a public statement drafted that specifically pointed to this as an \u201cexample of the right leadership,\u201d while also noting that the decision had \u201cthe national security in mind.\u201d<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4dcjv0vlj5q\"><sup><a href=\"#fn4dcjv0vlj5q\">[61]</a></sup></span></p><p>Nixon wanted a public relations victory, and he saw the possibility of one in the growing unpopularity of chemical and biological weapons. In the belief held by most of his advisors that biological weapons were not particularly useful for the military, Nixon saw an encouraging lack of roadblocks to achieving the biological part of that victory. Had his advisors stuck their necks out for biological weapons as they had for chemical weapons, it is unlikely that Nixon would have made the decision he did; in the end, he simply followed their recommendations. After years of petitions, hearings, mishaps, and investigations, the decision was simple. A week later, Nixon announced it before the nation.</p><h2>The Legacy of Nixon's Decision</h2><p>Nixon\u2019s renunciation of biological weapons was the first renunciation of any entire class of weapons by the United States. Without public and Congressional pressure, Nixon may never have felt the need to scrutinize CBW at all. And without the work of a dedicated group of scientists persuading Congress, the people, and the government that biological weapons would not be useful, Nixon might not have been convinced to ignore the military\u2019s opinion. Both factors were essential to the outcome, and both heavily leveraged the other in making their arguments. It was only fitting that later that day, Meselson was cheered by the&nbsp;<i>New York Times</i> with the subheadings \u201cinfluence on president\u201d and \u201ccalls weapons useless\u201d.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreffaihciypgi4\"><sup><a href=\"#fnfaihciypgi4\">[62]</a></sup></span></p><p>Nixon\u2019s decision was followed not long later with a renunciation of toxin weapons, and it paved the way for the 1972 Biological Weapons Convention, which banned all development and stockpiling of biological weapons for its hundreds of signatory nations.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref60faxxhynm\"><sup><a href=\"#fn60faxxhynm\">[63]</a></sup></span>&nbsp;At the same time, weaknesses remained. The Soviet Union continued its biological weapons program even after it signed the Convention.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefsakhg2bsr\"><sup><a href=\"#fnsakhg2bsr\">[64]</a></sup></span>&nbsp;Today, there are few strong enforcement mechanisms, and in 2019 the Convention had a budget of only $1.4 million,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref702g9l8uhk8\"><sup><a href=\"#fn702g9l8uhk8\">[65]</a></sup></span>&nbsp; less than the budget of the average McDonald\u2019s franchise.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdvlhsh2m238\"><sup><a href=\"#fndvlhsh2m238\">[66]</a></sup></span>&nbsp; Encouragingly, however, National Security Advisor Jake Sullivan recently released a statement urging the world to \u201cstrengthen and revitalize\u201d the Biological Weapons Convention.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6jneehuby9u\"><sup><a href=\"#fn6jneehuby9u\">[67]</a></sup></span></p><p>&nbsp;In 1993, George H.W. Bush signed the Chemical Weapons Convention, finally relegating the other pillar of the US CBW program to the history books, at least for now.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5o18n9a1fuf\"><sup><a href=\"#fn5o18n9a1fuf\">[68]</a></sup></span>&nbsp;Yet there has still not been a similar convention for the class of nuclear weapons, perhaps because it is far harder to make the argument that such weapons have no retaliatory purpose.</p><p>In recent years, a new weapon of mass destruction has surfaced: lethal autonomous weapons. The first documented case of a fully autonomous drone hunting down a target came in Libya in January 2021,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefne95z78vrcs\"><sup><a href=\"#fnne95z78vrcs\">[69]</a></sup></span>&nbsp;while an autonomous drone swarm was used by Israel in Gaza in July 2021.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref99rhytpvlse\"><sup><a href=\"#fn99rhytpvlse\">[70]</a></sup></span>&nbsp; Like chemical and biological weapons, autonomous weapons are cheap, can fall into the wrong hands, and might be capable of destroying entire cities. Prominent scientists, including UC Berkeley Professor Stuart Russell,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefyz9zlu6klx\"><sup><a href=\"#fnyz9zlu6klx\">[71]</a></sup></span>&nbsp;are sounding the alarm, arguing for a ban on such weapons in the mold of the biological and chemical weapons conventions. In addition to petitions, they have created hypothetical future \u201cdocumentaries\u201d with the intent of rousing public and political support for such a ban.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefm9q48ijszh\"><sup><a href=\"#fnm9q48ijszh\">[72]</a></sup></span>&nbsp;Still, it remains to be seen whether these efforts will convince Congress and the public that autonomous weapons should go the way of biological weapons, and that their prohibition should be rigorously enforced. We can only hope that they will.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw6lo9dvpx78\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw6lo9dvpx78\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Helen Thompson, \u201cIn 1950, the U.S. Released a Bioweapon in San Francisco,\u201d Smithsonian Magazine, July 6, 2015, https://www.smithsonianmag.com/smart-news/1950-us-released-bioweapon-san-francisco-180955819/.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndqvwt4fgy6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdqvwt4fgy6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Richard P. Wheat, Anne Zuckerman, and Lowell A. Rantz, \u201cInfection Due To Chromobacteria: Report of Eleven Cases,\u201d A.M.A. Archives of Internal Medicine 88, no. 4 (October 1, 1951): 461\u201366, https://doi.org/10.1001/archinte.1951.03810100045004.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh37mdbhp4w5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh37mdbhp4w5\">^</a></strong></sup></span><div class=\"footnote-content\"><p><i>A note on terminology.</i> A&nbsp;<i>biological weapon</i> consists of living organisms, such as bacteria, fungi, or viruses. A&nbsp;<i>chemical weapon</i> consists of non-living chemical agents, such as nerve or chlorine gas. A&nbsp;<i>toxin weapon</i> is a kind of chemical weapon made from chemicals synthesized by living organisms such as the&nbsp;<i>botilinum&nbsp;</i>toxin produced by a bacterium. The distinction between chemical and toxin weapons is not critical for this paper.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhfuuusplmvl\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhfuuusplmvl\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Richard Nixon, \u201cRemarks Announcing Decisions on Chemical and Biological Defense Policies and Programs.\u201d (Washington, DC, November 25, 1969), https://www.presidency.ucsb.edu/documents/remarks-announcing-decisions-chemical-and-biological-defense-policies-and-programs.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmfng1d5gws\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmfng1d5gws\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Jonathan B. Tucker, \u201cA Farewell to Germs: The U.S. Renunciation of Biological and Toxin Warfare, 1969\u201370,\u201d International Security 27, no. 1 (July 2002): 107\u201348, https://doi.org/10.1162/016228802320231244.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn344te2kmgt6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref344te2kmgt6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Brian Balmer, Alex Spelling, and Caitr\u00edona McLeish, \u201cPreventing \u2018A Virological Hiroshima\u2019: Cold War Press Coverage of Biological Weapons Disarmament,\u201d Journal of War &amp; Culture Studies 9, no. 1 (January 2, 2016): 74\u201390, https://doi.org/10.1080/17526272.2015.1101877.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp5bbblnkoua\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp5bbblnkoua\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Robert W. McElroy, \u201cAmerica\u2019s Renunciation of Chemical and Biological Warfare,\u201d in Morality and American Foreign Policy: The Role of Ethics in International Affairs (Princeton University Press, 1992), 88\u2013114, https://www.jstor.org/stable/j.ctt7zvnjj.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn81koa3uvjhg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref81koa3uvjhg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;James Revill, \u201c\u2018Muddling through\u2019 in the Biological &amp; Toxin Weapons Convention,\u201d International Politics 55, no. 3\u20134 (2018): 386\u2013401.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc0qwx77xk4k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc0qwx77xk4k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Joel Primack and Frank von Hippel, \u201cMatthew Meselson and the United States Policy on Chemical and Biological Warfare,\u201d in Advice and Dissent: Scientists in the Political Arena (New York: Basic Books, 1974), 143\u201364.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfjapf6a83f5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffjapf6a83f5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cGeneva Protocol,\u201d U.S. Department of State, accessed December 11, 2021, https://2009-2017.state.gov/t/isn/4784.htm.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm2yo28z1n5\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm2yo28z1n5\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cThe Living Weapon,\u201d American Experience (PBS, 1997), http://www.shoppbs.pbs.org/wgbh/amex/weapon/program/index.html.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1wzexbff1tfh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1wzexbff1tfh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tucker, \u201cA Farewell to Germs,\u201d 110.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnr1sdg072vcj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefr1sdg072vcj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cThe Living Weapon.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn61eeibdqcnj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref61eeibdqcnj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cThe Living Weapon.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7swegiel4uy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7swegiel4uy\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;George C. Wilson, \u201cArmy Conducted 239 Secret, Open-Air Germ Warfare Tests,\u201d Washington Post, March 9, 1977, https://www.washingtonpost.com/archive/politics/1977/03/09/army-conducted-239-secret-open-air-germ-warfare-tests/b17e5ee7-3006-4152-acf3-0ad163e17a22/.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnupigubvziwh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefupigubvziwh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Wilson.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyetygg9gs7r\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyetygg9gs7r\">^</a></strong></sup></span><div class=\"footnote-content\"><p>David I. Goldman, \u201cThe Generals and the Germs: The Army Leadership\u2019s Response to Nixon\u2019s Review of Chemical and Biological Warfare Policies in 1969,\u201d The Journal of Military History 73, no. 2 (2009): 536, https://doi.org/10.1353/jmh.0.0242.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny09l2arqv9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy09l2arqv9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cThe Living Weapon.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrkxsofngdd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrkxsofngdd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Goldman, \u201cThe Generals and the Germs,\u201d 536.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnn9ob5c2vqkd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefn9ob5c2vqkd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cUS Inflation Calculator,\u201d November 10, 2021, https://www.usinflationcalculator.com/.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyulhusi2m1h\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyulhusi2m1h\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Jonathan Tucker and Erin Mahan, \u201cPresident Nixon\u2019s Decision to Renounce the U.S. Offensive Biological Weapons Program\u201d (Washington, DC: National Defense University, Center for the Study of Weapons of Mass Destruction, October 1, 2009), 1.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkwffhoqgdah\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkwffhoqgdah\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;John Edsall and Matthew Meselson to Richard Nixon, \u201cPetition to President Lyndon Johnson Expressing Concern over Existing Restraints on Employment of Chemical and Biological Weapons,\u201d September 19, 1966, https://profiles.nlm.nih.gov/spotlight/bb/catalog/nlm:nlmuid-101584906X15953-doc.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnap7fuh7uinw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefap7fuh7uinw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Edsall and Meselson to Nixon.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne04yj1xwh4e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe04yj1xwh4e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Matthew Meselson, Future of Life Institute Podcast (Part 1): From DNA to Banning Biological Weapons With Matthew Meselson and Max Tegmark, interview by Ariel Conn and Max Tegmark, February 28, 2019, https://futureoflife.org/2019/02/28/fli-podcast-part-1-from-dna-to-banning-biological-weapons-with-matthew-meselson-and-max-tegmark/.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4obbgab03yu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4obbgab03yu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201c22 Scientists Bid Johnson Bar Chemical Weapons in Vietnam,\u201d The New York Times, September 20, 1966; \u201cChemical Arms Criticized,\u201d Dallas Morning News, September 20, 1966.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno0yqpzieqpe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo0yqpzieqpe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Primack and von Hippel, \u201cMatthew Meselson and the United States Policy on Chemical and Biological Warfare,\u201d 149.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnzsb1qzip18c\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefzsb1qzip18c\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tucker, \u201cA Farewell to Germs,\u201d 113.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm8v2cjenlgk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm8v2cjenlgk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Goldman, \u201cThe Generals and the Germs,\u201d 540.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvoq8b7nnrmk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvoq8b7nnrmk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Tucker, \u201cA Farewell to Germs,\u201d 114.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnoedsedo7bzd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefoedsedo7bzd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Jack Gould, \u201cTV: \u2018First Tuesday\u2019 Explores Chemical Warfare,\u201d The New York Times, February 5, 1969, sec. Archives, https://www.nytimes.com/1969/02/05/archives/tv-first-tuesday-explores-chemical-warfare.html.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbypz25xyes9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbypz25xyes9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Tucker, \u201cA Farewell to Germs,\u201d 113.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwx93wd1uvo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwx93wd1uvo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Richard D. (Richard Dean) McCarthy, The Ultimate Folly: War by Pestilence, Asphyxiation, and Defoliation (New York: Knopf, 1969), 146.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnbhlbh7ra5jb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefbhlbh7ra5jb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cChemical And Biological Warfare\u201d (Washington, DC, April 30, 1969).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn01h6lyse9m4s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref01h6lyse9m4s\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Meselson, Future of Life Institute Podcast (Part 1): From DNA to Banning Biological Weapons With Matthew Meselson and Max Tegmark.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfib9u03scef\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffib9u03scef\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cChemical And Biological Warfare,\u201d 8.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqa8oa8tv69m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqa8oa8tv69m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cChemical And Biological Warfare,\u201d 14.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fntmf8jk0nvcc\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreftmf8jk0nvcc\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cChemical And Biological Warfare,\u201d 39.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1x6wmznr91e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1x6wmznr91e\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cChemical And Biological Warfare,\u201d 16.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7dsc4qcwfwy\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7dsc4qcwfwy\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cChemical And Biological Warfare,\u201d 50.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn857r6na0jh8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref857r6na0jh8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cU.S. Stocking Chamber of Horrors, Adviser Says of Chemical Warfare,\u201d The Augusta, Ga. Chronicle, June 23, 1969.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn67jzax3ojmw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref67jzax3ojmw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;James J. Kilpatrick, \u201cSpeaking of the Unspeakable,\u201d The Plain Dealer, August 1, 1969.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6o19mxjo96v\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6o19mxjo96v\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Goldman, \u201cThe Generals and the Germs,\u201d 541.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnla04iyucp1\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefla04iyucp1\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Robert Keatley, \u201cNerve Gas Accident: Okinawa Mishap Bares Overseas Deployment Of Chemical Weapons Leak at U.S. Base Fells 25,\u201d Wall Street Journal, July 18, 1969, sec. 1</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2zewk6e91aw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2zewk6e91aw\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Goldman, \u201cThe Generals and the Germs,\u201d 546.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnotcw4zcnzhg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefotcw4zcnzhg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cSenate Congressional Record\u201d (Washington, DC, August 11, 1969).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmy1lq8hsf4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmy1lq8hsf4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Melvin R. Laird, \u201cMemorandum for Correspondents,\u201d August 9, 1969, https://primarysources-brillonline-com.yale.idm.oclc.org/reader/open?rotate=1&amp;starEnabled=1&amp;shareLink=http://primarysources.brillonline.com//browse/weapons-of-mass-destruction/dod-press-release-memorandum-for-correspondents-august-9-1969-unclassified-dod-foia;wmdowmdo07292&amp;workUri=weapons-of-mass-destruction/dod-press-release-memorandum-for-correspondents-august-9-1969-unclassified-dod-foia;wmdowmdo07292&amp;download=1&amp;startPage=1&amp;maxCopy=-1\u00d7tamp=2021-11-13T17:34:43&amp;cite=1&amp;watermark=BrillOnline_Primary_Sources_WEAPONS_OF_MASS_DES_DOD_PRESS_RELE_1850_13_11_2021&amp;token=EVbZbC8+aDIA9a2/WLYJbYGsdAs=&amp;callbacks=0&amp;pbox=1&amp;userId=1850&amp;maxPrint=-1&amp;accId=1850</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnmnph2l1btq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefmnph2l1btq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Robert M. Smith, \u201c20,000 Poison Bullets Made and Stockpiled by Army,\u201d The New York Times, October 31, 1969, sec. Archives, https://www.nytimes.com/1969/10/31/archives/20000-poison-bullets-made-and-stockpiled-by-army.html.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fng2auh8pe2n8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefg2auh8pe2n8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Melvin R. Laird to Henry A. Kissinger, \u201cMemorandum for: Assistant to the President for National Security Affairs,\u201d April 30, 1969, https://primarysources-brillonline-com.yale.idm.oclc.org/reader/open?rotate=1&amp;starEnabled=1&amp;shareLink=http://primarysources.brillonline.com//browse/weapons-of-mass-destruction/dod-memorandum-laird-to-kissinger-april-30-1969-confidential-rnl;wmdowmdo07261&amp;workUri=weapons-of-mass-destruction/dod-memorandum-laird-to-kissinger-april-30-1969-confidential-rnl;wmdowmdo07261&amp;download=1&amp;startPage=1&amp;maxCopy=-1\u00d7tamp=2021-11-13T17:19:46&amp;cite=1&amp;watermark=BrillOnline_Primary_Sources_WEAPONS_OF_MASS_DES_DOD_MEMORANDUM_1850_13_11_2021&amp;token=hVLW+1gHzomVi4y0AjtiMhgKb8M=&amp;callbacks=0&amp;pbox=1&amp;userId=1850&amp;maxPrint=-1&amp;accId=1850.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlnxsxq5gbt\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflnxsxq5gbt\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Henry A. Kissinger to Melvin R. Laird, \u201cMemorandum for the Secretary of Defense,\u201d May 9, 1969, https://primarysources-brillonline-com.yale.idm.oclc.org/reader/open?rotate=1&amp;starEnabled=1&amp;shareLink=http://primarysources.brillonline.com//browse/weapons-of-mass-destruction/nsc-memorandum-cbw-study-may-9-1969-confidential-rnl;wmdowmdo07263&amp;workUri=weapons-of-mass-destruction/nsc-memorandum-cbw-study-may-9-1969-confidential-rnl;wmdowmdo07263&amp;download=1&amp;startPage=1&amp;maxCopy=-1\u00d7tamp=2021-11-13T17:34:43&amp;cite=1&amp;watermark=BrillOnline_Primary_Sources_WEAPONS_OF_MASS_DES_NSC_MEMORANDUM_1850_13_11_2021&amp;token=wd/CHCJfTBdmvWOohHWiMmkmNzU=&amp;callbacks=0&amp;pbox=1&amp;userId=1850&amp;maxPrint=-1&amp;accId=1850.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnw66mecowzd\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefw66mecowzd\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Laird, \u201cMemorandum for Correspondents.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnrp4m02l93j\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefrp4m02l93j\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cChemical And Biological Warfare.\u201d</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fneb8jlbqorue\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefeb8jlbqorue\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cBritish Proposal to the Eighteen Nation Disarmament Committee\u201d (United States Arms Control and Disarmament Agency, July 10, 1969).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfemsici8msu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffemsici8msu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cMinutes of NSC Meeting on Chemical Warfare and Biological Warfare,\u201d November 18, 1969, https://primarysources-brillonline-com.yale.idm.oclc.org/reader/open?rotate=1&amp;starEnabled=1&amp;shareLink=http://primarysources.brillonline.com//browse/weapons-of-mass-destruction/nsc-report-minutes-of-nsc-meeting-on-chemical-warfare-and-biological-warfare-november-18-1969-top-secret-dos-foia;wmdowmdo07319&amp;workUri=weapons-of-mass-destruction/nsc-report-minutes-of-nsc-meeting-on-chemical-warfare-and-biological-warfare-november-18-1969-top-secret-dos-foia;wmdowmdo07319&amp;download=1&amp;startPage=1&amp;maxCopy=-1\u00d7tamp=2021-11-13T17:35:01&amp;cite=1&amp;watermark=BrillOnline_Primary_Sources_WEAPONS_OF_MASS_DES_NSC_REPORT_MIN_1850_13_11_2021&amp;token=ensr0z/33CfiRrr/v0kdK4tZ6wY=&amp;callbacks=0&amp;pbox=1&amp;userId=1850&amp;maxPrint=-1&amp;accId=1850.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5o4n37n0xm8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5o4n37n0xm8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Meselson, Future of Life Institute Podcast (Part 1): From DNA to Banning Biological Weapons With Matthew Meselson and Max Tegmark.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz7zatehveu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz7zatehveu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Goldman, \u201cThe Generals and the Germs,\u201d 553.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnntncga2juun\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefntncga2juun\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Goldman, 552\u201353.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0ro1s0jphhv\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0ro1s0jphhv\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cMinutes of NSC Meeting on Chemical Warfare and Biological Warfare,\u201d 1\u20133.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fny9yyy7kngjs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefy9yyy7kngjs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cMinutes of NSC Meeting on Chemical Warfare and Biological Warfare,\u201d 4.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnwdtk5w4d98\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnwdtk5w4d98\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cMinutes of NSC Meeting on Chemical Warfare and Biological Warfare,\u201d 5.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnunsm8jfus9g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefunsm8jfus9g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cMinutes of NSC Meeting on Chemical Warfare and Biological Warfare,\u201d 7.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4dcjv0vlj5q\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4dcjv0vlj5q\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cMinutes of NSC Meeting on Chemical Warfare and Biological Warfare,\u201d 7.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnfaihciypgi4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreffaihciypgi4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;David E. Rosenbaum, \u201cActivist Germ War Foe Matthew Stanley Meselson,\u201d The New York Times, November 26, 1969.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn60faxxhynm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref60faxxhynm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cBiological Weapons Convention \u2013 UNODA,\u201d accessed December 2, 2021, https://www.un.org/disarmament/biological-weapons/.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnsakhg2bsr\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefsakhg2bsr\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Russel e Dybvik, \u201cRussia Commits to End Biological Weapons Program,\u201d September 14, 1992, https://nuke.fas.org/control/bwc/news/920914-242819.htm.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn702g9l8uhk8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref702g9l8uhk8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cReport on the Overall Financial Situation of the Biological Weapons Convention\u201d (Geneva: Biological Weapons Convention, December 3, 2019), https://undocs.org/pdf?symbol=en/BWC/MSP/2019/5#:~:text=With%20regard%20to%20the%202019,is%20therefore%2092.3%20per%20cent.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndvlhsh2m238\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdvlhsh2m238\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cThe QSR 50,\u201d QSR magazine, accessed December 2, 2021, https://www.qsrmagazine.com/content/qsr-50-0.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6jneehuby9u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6jneehuby9u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;\u201cStatement by National Security Advisor Jake Sullivan on the U.S. Approach to Strengthening the Biological Weapons Convention,\u201d The White House, November 19, 2021, https://www.whitehouse.gov/briefing-room/statements-releases/2021/11/19/statement-by-national-security-advisor-jake-sullivan-on-the-u-s-approach-to-strengthening-the-biological-weapons-convention/.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5o18n9a1fuf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5o18n9a1fuf\">^</a></strong></sup></span><div class=\"footnote-content\"><p>George H.W. Bush and Mikhail Gorbachev Agree to End Production of Chemical Weapons,\u201d HISTORY, accessed December 2, 2021, https://www.history.com/this-day-in-history/superpowers-to-destroy-chemical-weapons.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnne95z78vrcs\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefne95z78vrcs\">^</a></strong></sup></span><div class=\"footnote-content\"><p>\u201cFinal Report of the Panel of Experts on Libya Establish Pursuant to Security Council Resolution 1973 (2011)\u201d (Panel of Experts on Libya, March 8, 2021), 17, https://documents-dds-ny.un.org/doc/UNDOC/GEN/N21/037/72/PDF/N2103772.pdf?OpenElement.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn99rhytpvlse\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref99rhytpvlse\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;David Hambling, \u201cIsrael Used World\u2019s First AI-Guided Combat Drone Swarm in Gaza Attacks,\u201d New Scientist, accessed December 2, 2021, https://www.newscientist.com/article/2282656-israel-used-worlds-first-ai-guided-combat-drone-swarm-in-gaza-attacks/.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnyz9zlu6klx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefyz9zlu6klx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Stuart Russell et al., \u201cLethal Autonomous Weapons Exist; They Must Be Banned,\u201d IEEE Spectrum, June 16, 2021, https://spectrum.ieee.org/lethal-autonomous-weapons-exist-they-must-be-banned.; Note, the author works in Stuart Russell\u2019s lab.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnm9q48ijszh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefm9q48ijszh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;Future of Life Institute, Slaughterbots - if human: kill(); Future of Life Institute, Slaughterbots.</p></div></li></ol>", "user": {"username": "ThomasWoodside"}}, {"_id": "qL2JeTf5osAuSRvvo", "title": "Scattered Takes and Unsolicited Advice (new ones added to the top)", "postedAt": "2022-09-30T15:10:25.273Z", "htmlBody": "<ul><li>There are lots of contexts, even ones that aren't work contexts, where you might be being evaluated or assessed informally. This is kind of just true in the world in general, but if you end up making EA a big part of your social and professional networks, then the consequences of that are bigger.</li><li>If you care about being able to do EA work longterm, it's worth pretty significant costs to avoid resenting EA. Take that into account when you think about what decisions you're making and with what kind of sacrifice.</li><li>\"Say more?\" and \"If your thoughts are in a pile, what's on top?\" are pretty powerful conversational moves, in my experience</li><li>You can really <a href=\"https://forum.effectivealtruism.org/posts/dMNFCv7YpSXjsg8e6/how-to-raise-others-aspirations-in-17-easy-steps\">inspire people to do a bunch in some cheap ways</a> (though thinking about it, I think I can think of...2-3 people I've directly affected this way?)</li><li>A lot of our feelings and reactions come reactively / contextually / on the margins - people feel a certain way e.g. when they are immersed in EA spaces and sometimes have critiques, and when they are in non-EA spaces, they miss the good things about EA spaces. This seems normal and healthy and a good way to get multiple frames on something, but also good to keep in mind.</li><li>People who you think of as touchstones of thinking a particular thing may change their minds or not be as bought in as you'd expect</li><li>The world has so much detail</li><li>One of the most valuable things more senior EAs can do for junior EAs is contextualize: EA has had these conversations before, the thing you experienced was a 20th/50th/90th percentile experience, other communities do/don't go through similar things etc.</li><li>Maybe one of the best things we can all do for each other is push on <a href=\"https://chanamessinger.com/blog/building-agency-list\">expanding option sets</a>, and <a href=\"https://forum.effectivealtruism.org/posts/uqcKKTRWcED6y3WFW/questions-that-lead-to-impactful-conversations\">ask questions</a> that get us to think more about what we think and what we should do.&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/CmmtKKD84BYhmmxrD/chanamessinger-s-shortform?commentId=vTDfApB7jWHkyxiBZ\">About going to hubs to network</a></li><li>When you're new to EA, it's very exciting: Don't let your feet go faster than your brain - know what you're doing and why. It's not good for you or the world if in two years you look around and don't believe any of it and don't know how you got there and feel tricked or disoriented.</li><li>You're not alone in feeling overwhelmed or like an imposter</li><li>If you're young in EA: Don't go into community building just because the object level feels scarier and you don't have the skills yet</li><li>Networking is great, but it's not the only form of <a href=\"https://forum.effectivealtruism.org/posts/acyfmFTN3cNgwnYw6/agency-needs-nuance\">agency</a> / initiative taking</li><li>Lots of ick feelings about persuasion and outreach get better if you're honest and <a href=\"https://forum.effectivealtruism.org/posts/CmmtKKD84BYhmmxrD/chanamessinger-s-shortform?commentId=MBdtNjktCQbstDrgc\">transparent</a></li><li>Lots of ick feelings about all kinds of things are tracking a lot of different things at once: people's vibes, a sense of honesty or dishonesty, motivated reasoning, underlying empirical disagreements - it's good to track those things separately</li><li>Ask for a reasonable salary for your work, it's not as virtuous as you think to work for nothing<ul><li>Sets bad norms for other people who can't afford to do that</li><li>Makes it more like volunteering so you might not take the work as seriously</li></ul></li><li>Don't be self-hating about EA; figure out what you believe and don't feel bad about believing it and its implications and acting in the world in accordance with it</li><li>There are sides of spectra like <a href=\"https://chanamessinger.com/blog/feelings-about-money\">pro-spending money</a> or <a href=\"https://forum.effectivealtruism.org/posts/3k4H3cyiHooTyLY6p/why-i-find-longtermism-hard-and-what-keeps-me-motivated\">longtermism or meta work</a> that aren't just \"logic over feelings\", they have feelings too.</li><li>Earnestness is shockingly effective - if you say what you think and why you think it (including \"I read the title of a youtube video\"), if you say when don't know what to do and what you're confused about, if you say what you're confident in and why, if you say how you feel and why, I find things (at least in this social space) go pretty damn well, way better than I would have expected.</li></ul>", "user": {"username": "ChanaMessinger"}}, {"_id": "uMNhoAoxDoYDnBoqd", "title": "Transparency for improving weird feelings around community building", "postedAt": "2022-09-30T15:09:32.701Z", "htmlBody": "<p>There's a lot of potential ick as things in EA formalize and professionalize, especially in community building. People might reasonably feel uncomfortable realizing that the intro talk they heard is entirely scripted, or that interactions with them have been logged in a spreadsheet or that the events they've been to are taking them through the ideas on a path from least to most weird (all things I've heard of happening, with a range of how confident I am in them actually happening as described here). I think there's a lot to say here about how to productively engage with this feeling (and things community builders should do to mitigate it), but I also think there's a quick trick that will markedly improve things (though do not fix all problems): <strong>transparency</strong>.</p><p>(This is an outside take from someone who doesn't do community building on college campuses or elsewhere, I think that work is hard and filled with paradoxes, and it's also possible that this is already done by default, but in the spirit of stating the obvious)</p><p>I've been updating over and over again over the last few years that earnestness is just very powerful, and I think there are ways (though maybe they require some social / communication skills that aren't universal) to say things like (conditional on them being true):</p><p><i>NB: I don't think these are the best versions of these scripts, this was a first pass to point at the thing I mean</i></p><ul><li>\"This EA group is one of many around the country and the world. There is a standard intro talk that contains framings we think are exceptionally useful and helps us make sure we don't miss any of the important ideas or caveats, so we are giving it here today. I am excited to convey these core concepts, and then for the group of people who come in subsequent weeks to figure out which &nbsp;aspects of these they're most interested in pursuing and customizing the group to our needs.\"</li><li>\"Hey, I'm excited to talk to you about EA stuff. The organizers of this group are hoping to chat with people who seem interested and not be repetitive or annoying to you, would it be ok with you if I took some notes on our conversation that other organizers can see?\"</li><li>\"EA ideas span a huge gamut from really straightforward to high-context / less conventional. These early dinners start with the less weird ones because we think the core ideas are really valuable to the world whether or not people buy some of the other potential implications. Later on, with more context, we'll explore a wider range.\"</li><li>\"I get that the perception that people only get funding or help if they seem interested in EA is uncomfortable / seems bad. From my perspective, I'm engaged in a particular project with my EA time / volunteer time / career / donations / life, and I'm excited to find people who are enthused by that same project and want to work together on it. If people find this is not the project for them, that's a great thing to have learned, and I'm excited for them to find people to work with on the things they care about most\"</li></ul><p>Not everything needs to be explicit, but this at least tracks whether you're passing the <a href=\"https://www.yourdictionary.com/red-face-test\"><strong>red face test</strong></a><strong>.</strong></p><p>I think that being transparent in this way requires:</p><ul><li>Some communication skills to convey things like the above with nuance and grace</li><li>Being able to track when explicitness is bad or unhelpful</li><li>Some social skills in tracking what the other person cares about and is looking for in conversations</li><li><strong>Non-self-hatingness</strong>: Thinking that you are doing something valuable, that matters to you, that you don't have to apologize for caring about, along with its implications</li><li>A willingness to be honest and earnest about the above.</li></ul><p><br>&nbsp;</p>", "user": {"username": "ChanaMessinger"}}, {"_id": "ZRZHJ3qSitXQ6NGez", "title": "About going to a hub", "postedAt": "2022-09-30T15:08:00.145Z", "htmlBody": "<p>A response to: <a href=\"https://forum.effectivealtruism.org/posts/M5GoKkWtBKEGMCFHn/what-s-the-theory-of-change-of-come-to-the-bay-over-the\">https://forum.effectivealtruism.org/posts/M5GoKkWtBKEGMCFHn/what-s-the-theory-of-change-of-come-to-the-bay-over-the</a>&nbsp;<br><br>For people who consider taking or end up taking this advice, some things I'd say if we were having a 1:1 coffee about it:</p><ul><li>Being away from home is by its nature intense, this community and the philosophy is intense, and some social dynamics here are unusual, I want you to go in with some sense of the landscape so you can make informed decisions about how to engage.</li><li>The culture here is full of energy and ambition and truth telling. That's really awesome, but it can be a tricky adjustment. In some spaces, you'll hear a lot of frank discussion of talent and fit (e.g. people might dissuade you from starting a project not because the project is a bad idea but because they don't think you're a good fit for it). Grounding in your own self worth (and your own inside views) will probably be really important.</li><li>People both are and seem really smart. It's easy to just believe them when they say things. Remember to flag for yourself things you've just heard versus things you've discussed at length &nbsp;vs things you've really thought about yourself. Try to ask questions about the <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">gears</a> of people's <a href=\"https://www.lesswrong.com/s/zucjLBpQ9S9eWPWGu\">models</a>, ask for credences and <a href=\"https://forum.effectivealtruism.org/posts/53JxkvQ7RKAJ4nHc4/some-thoughts-on-deference-and-inside-view-models\">cruxes</a>. &nbsp;Remember that people disagree, including about very big questions. Notice the difference between people's offhand hot takes and their areas of expertise. We want you to be someone who can disagree with high status people, who can think for themselves, who is in touch with reality.</li><li>I'd recommend staying grounded with friends/connections/family outside the EA space. Making friends over the summer is great, and some of them may be deep connections you can rely on, but as with all new friends and people, you don't have as much evidence about how those connections will develop over time or with any shifts in your relationships or situations. It's easy to get really attached and connected to people in the new space, and that might be great, but I'd keep track of your level of emotional dependency on them.</li><li>We use the word \"community\" but I wouldn't go in assuming that if you come on your own you'll find a waiting, welcoming pre -made social scene, or that people will have the capacity to proactively take you under their wing, look out for you and your well being, especially if there are lots of people in a similar boat. I don't want you to feel like you've been promised anything in particular here. That might be up to you to make for yourself.</li><li>One thing that's intense is the way that the personal and professional networks overlap, so keep that in mind as you think about how you might keep your head on straight and what support you might need if your job situation changes, you have a bad roommate experience, you date and break up with someone (maybe get a friend's take on the EV of casual hookups or dating during this intense time, given that the emotional effects might last a while and play out in your professional life - you know yourself best and how that might play out for you).</li><li>This might be a good place to flag that just because people are EAs doesn't mean they're automatically nice or trustworthy, pay attention to your own sense of how to interact with strangers.</li><li>I'd recommend reading <a href=\"https://forum.effectivealtruism.org/posts/fovDwkBQgTqRMoHZM/power-dynamics-between-people-in-ea\">this post on power dynamics in EA</a>.</li><li>Read<a href=\"https://www.lewissociety.org/innerring/\"> CS Lewis 's The Inner Ring</a></li><li>Feeling lonely or ungrounded or uncertain is normal. There is <a href=\"https://forum.effectivealtruism.org/topics/self-care\">lots of discussion on the forum</a> about people feeling this way and what they've done about it. There is an <a href=\"https://www.facebook.com/groups/ea.peer.support/about/\">EA peer support Facebook group</a> where you can post anonymously if you want. If you're in more need than that, you can contact <a href=\"https://forum.effectivealtruism.org/users/julia_wise\">Julia Wise</a> or <a href=\"https://forum.effectivealtruism.org/users/cafelow\">Catherine Low</a> on the community health team.</li><li>As per my other comment, some of this networking is constrained by capacity. Similarly, I wouldn't go in assuming you'll find a mentor or office space or all the networking you want. By all means ask, but also also give affordance for people to say no, respect their time and professional spaces and norms. Given the capacity constraints, I wouldn't be surprised if weird status or competitive dynamics formed, even within people in a similar cohort. That can be hard.</li><li>Status stuff in general is likely to come up; there's just a ton of the ingredients for feeling like you need to be in the room with the shiniest people and impress them. That seems really hard; be gentle with yourself if it comes up. On the other hand, that would be great to avoid, which I think happens via emotional grounding, cultivating the ability to figure out what you believe even if high status people disagree and keeping your eye on the ball.</li><li>This comment and this post and even many other things you can read are not all the possible information, this is a community with illegibility like any other, people all theoretically interacting with the same space might have really different experiences. See what ways of navigating it work for you, if you're unsure, treat it as an experiment.</li><li>Keep your eye on the ball. Remember that the goal is to make incredible things happen and help save the world. <strong>Keep in touch with your actual goals, maybe by making a plan in advance of what a great time in the Bay would like, what would count as a success and what wouldn't. Maybe ask friends to check in with you about how that's going.</strong></li><li>My guess is that having or finding projects and working hard on them or on developing skills will be a better bet for happiness and impact than a more \"just hang around and network\" approach (unless you approach that as a project - trying to create and develop models of community building, testing hypotheses empirically, etc). If you find that you're not skilling up as much as you'd like, or not getting out of the Bay what you'd hoped, figure out where your impact lies and do that. If you find that the Bay has social dynamics and norms that are making you unhappy and it's limiting your ability to work, take care of yourself and safeguard the impact you'll have over the course of your life.</li></ul><p>We all want (I claim) EA to be a high trust, truth-seeking, impact-oriented professional community and social space. Help it be those things. Blurt truth (but be mostly nice), have integrity, try to avoid status and social games, make shit happen.</p>", "user": {"username": "ChanaMessinger"}}, {"_id": "EKfixour65Az8W48M", "title": "7 learnings from 3 years running a corporate EA Group", "postedAt": "2022-09-30T16:03:20.806Z", "htmlBody": "<p><i>Thanks to my co-organizers, Will Hastings, John Yan, Denisse Sevastian and Farhan Azam for their input and feedback on this post</i><br>&nbsp;</p><p>3 years ago, I founded the Effective Altruism @ Meta corporate interest group. We\u2019ve had middling success in drumming up interest for EA at the company, and thought it may be useful to share some of our learnings for others doing the same thing. I wouldn\u2019t assume that these learnings necessarily generalize to other corporate settings, but I\u2019m eager to hear from others on what does and doesn't resonate.&nbsp;<br>&nbsp;</p><h2>1. Finding committed organizers is difficult in a corporate setting</h2><p>In a higher intensity environment like Meta, people don\u2019t have much time to commit to activities outside of their core responsibilities. We cycled through 8-10 organizers before we finally found a more committed group of 3-4. Most other organizers flaked and eventually stopped attending events altogether.&nbsp;</p><p>&nbsp;</p><p>Beyond being busy with their day jobs, there are a few other reasons I think people consistently flaked: first, unlike a city or university community, there is no additional personal incentive to volunteer your time. Individuals are not in need of financial compensation, and holding volunteer positions does not meaningfully further their careers. Secondly, given organizers are not colocated and have varied time zones, it can be hard to find consistent times for organizers to interact.<br>&nbsp;</p><p>Ultimately, the organizers that stuck around seemed to be motivated by two things. First, most were independently excited about effective altruism and inherently motivated by the altruistic goals of the group. Secondly, they were more integrated with the social elements of our group: hosting events and attending meetings was their primary way to connect with other EAs.&nbsp;<br>&nbsp;</p><h2>2. Community &amp; discussion oriented activities are probably high ROI</h2><p>Fairly early on in our work at Meta we started a bi-weekly discussion group. We would host a poll for discussion topics a few days before, and had an open meeting to discuss the top-voted item for 45 minutes. Actual attendance for these events were low - rarely more than 10 attendees per session - but through regular attendees we eventually found consistent organizers and several individuals that took more serious EA actions. Many of these individuals took donation pledges, several helped us organize events and raise money, attended EAGs, and some considered or are taking career pivots. It\u2019s obviously hard to assess counterfactual impact, but given that for most of these individuals this was their primary interaction with other EAs and EA ideas, it seems likely that their further EA actions were attributable to us.&nbsp;<br>&nbsp;</p><p>The actual investment in setting up this regular social community where we discussed EA ideas was negligible. We crowd-sourced discussion topics or pulled from personal reading or existing databases, and advertised the meeting once every two weeks. By that measure, this was possibly the highest ROI activity we took.&nbsp;<br>&nbsp;</p><h2>3. Speaker events are probably low ROI</h2><p>Most of our tactical efforts were online events targeted at exposing Meta employees to EA ideas or effective giving principles. Our very first event had Peter Singer come speak about EA principles, and afterwards we had speakers from ACE, GiveDirectly, Google Brain, Giving What We Can, 1 For the World, and a slew of other EA-aligned organizations. Events were meaningful amounts of work to set up, as they involved internal approvals, event logistics, and internal advertising to ensure attendance.&nbsp;<br>&nbsp;</p><p>Despite the medium-high lift to run an event, we became increasingly skeptical that the events were high impact. Actual event attendance numbers were low: smaller events would sometimes result in less than 5 attendees, including EA@Meta organizers, and even our biggest name speakers like Peter Singer would cap out at ~40 live attendees. All events were recorded, and we\u2019d get a meaningful increase of viewers after the fact, but metrics suggested that most of these later viewers would not watch for long.</p><p>&nbsp;</p><p>Most importantly, many of the new attendees to these events would not engage with further content. Even after our largest events, we would not see a meaningful uptick in attendance to our bi-weekly discussion groups or engagement on our internal posts. For events that attempted to garner donations, we would only see small, marginal donations even when the event was designed to track and explicitly encourage giving.&nbsp;<br>&nbsp;</p><p>We had one series of events that plausibly had a larger impact. In December 2021, we hosted a series of events around Give Month, including 8 different talks from a variety of different EA-aligned speakers. We secured 25K in charity matching dollars from an internal organization, and received matched donations from employees resulting in &gt;50K total donated to effective charities. We relentlessly advertised and cross-posted our events in other internal groups, which led to an increase of our internal Effective Altruism group size (think Facebook group membership) by ~100%. Although these seem like positive signals, 50K total is a fairly marginal increase at the scale of big tech salaries, and the increase in new group members did not lead to a noticeable uptick in engagement or attendance for future events. This also all came at the cost of a fairly large time and effort investment from our organizers and external speakers.&nbsp;</p><p>&nbsp;</p><p>Overall, if our events were having a large impact relative to the investment, whether through increasing awareness of EA ideas or large ongoing increases in donations, we did not receive meaningful signals that this was the case.&nbsp;<br>&nbsp;</p><h2>4. Many EA software engineers do not feel like there are good direct work opportunities</h2><p>Amongst our members, all but the most committed software engineering EAs did not feel that there were good opportunities for them to contribute to direct work. This seems to be due to a large mix of factors. Hear were some of the reasons folks mentioned:<br>&nbsp;</p><ul><li>Financial considerations, especially for those that own a home or have a family, as moving to EA orgs normally involved a hefty pay cut</li><li>Not being enthusiastic or having the right skills to work on AI, and feeling most software engineering EA jobs were in this space</li><li>Not being persuaded that the opportunity cost of dramatically reducing donations compared to their potential value over replacement in an EA job</li><li>Not feeling \u201cEA\u201d enough to work in an EA org (despite donating large percentages of their salary to EA charities)</li><li>Not being able to get (or feeling sufficiently skilled) to get a job in EA</li><li>Simply not being aware these opportunities existed</li></ul><p>&nbsp;</p><h2>5. Employees tended to be more skeptical about longtermism compared to the average EA</h2><p><i>Extra note about epistemic status</i>: sample size here is very small, largely based on discussion group attendees (i.e. 20-30 unique individuals). Additionally, there are obvious biases resulting from viewpoints of prominent group leaders, chosen discussion topics, etc. With that said, this disparity in viewpoints between my outside-Meta EA community and my internal-Meta EA community was sufficiently notable that I thought worth sharing.&nbsp;</p><p>&nbsp;</p><p>In my normal EA circles interacting with folks in a large city group (NYC), I anecdotally find that most individuals (~90%) are sympathetic to at least a weaker version of longtermism. Amongst our regular attendees at EA@Meta events, however, I estimate that our discussion groups were closer to 50/50, and many of those more sympathetic to longtermism were often the same individuals that had independent connections to the EA community outside of Meta.&nbsp;<br>&nbsp;</p><p>Based on what I heard in discussion groups, the core skepticisms around longtermism were related to its epistemic foundations. In contrast to classic GiveWell interventions, group members seemed wary of investing in more speculative existential risk scenarios where empirical data was limited, and even more skeptical of interventions that did not have rigorous empirical methodology backing them up or effective feedback loops.&nbsp;</p><p>&nbsp;</p><p>A few theories for why these concerns may be more prevalent amongst our group attendees:&nbsp;</p><ul><li>Meta employees work in a space where empirical testing is highly prioritized; almost every major feature or change launched goes through a round of A/B testing and statistical analysis</li><li>In general, employees that have mostly spent time in corporate spaces are more likely to have skepticism around qualitative, academic arguments that haven\u2019t been demonstrated to reflect truth \u201cin the real world\u201d</li><li>Most employees were donating regularly, and wanted to feel that their contributions to EA were meaningful, whereas there was a sense that the longtermist shifts in EA considered their contributions unimportant (at least comparatively), which rubbed them the wrong way</li><li>Similarly, feeling the longtermist shifts in the community had meant there was less space for them to contribute if there wasn\u2019t an obvious path to change careers for them (see above)</li><li>Simply having less exposure to mainstream EA ideas by virtue of not spending time in EA circles outside of Meta</li></ul><p>&nbsp;</p><h2>6. Influencing key decision making is hard, even from within the organization</h2><p>One of our theories of change within EA@Meta was that we could influence corporate decision making. For example, we theorized that we may be able to nudge internal AI teams to think more about safety, or push Facebook\u2019s fundraising tools to consider efficacy.&nbsp;</p><p>&nbsp;</p><p>Our first project in this space is trying to convince our internal Social Impact teams to consider charitable impact within the Facebook fundraising tool. When you create a fundraiser within Facebook (as many folks do for their birthday), it recommends a set of charities to pick. There is a ranking model underlying which charities are recommended, and we were trying to figure out if we could incorporate charitable efficacy either through the UX or directly through the ranking model.&nbsp;<br>&nbsp;</p><p>We ran into a few problems trying to make such a change. First of all, none of our immediate volunteers actually worked on the product - there was a team that worked full time on the fundraising tool, and we would need their support and (more importantly) their consent to ship any changes. When we interacted more with this team, we quickly learnt that their topline metrics - what their leaders were concerned about, and what individuals were assessed on - were metrics like charitable dollars moved and number of total donors. This was framed under the context of \u201cbuilding a community of donors\u201d. Incorporating efficacy too strongly within the product could plausibly harm these metrics.&nbsp;<br>&nbsp;</p><p>Although we eventually found sympathetic individual contributors within the Social Impact team, including some who were even familiar with EA ideas, I remain skeptical that we can ship any code that would have a large negative impact on their topline metrics. We have some ideas for how we could tweak the experience to incorporate efficacy more without major costs to the team\u2019s core metrics, but these changes seem much more on the margin than what we had originally envisioned.&nbsp;<br>&nbsp;</p><p>Ultimately, I worry that other attempts at influencing large organizations will encounter similar roadblocks. Senior leaders in organizations are compelled to focus on a specific, EA-unaligned vision, and they in turn pressure their teams to optimize for the corresponding metrics. Even if there is manpower and sympathy at more junior levels to update systems, these top-down pressures will make it very difficult.&nbsp;<br>&nbsp;</p><h2>7. There are promising areas within corporate organizing that deserve more exploration</h2><p>There are a few areas I would be keen to explore more if I hadn\u2019t left Meta, and I hope that the remaining organizers continue to try. These are all low-confidence ideas, but I wanted to list them quickly out here in case they may spark ideas in others:<br>&nbsp;</p><ul><li><strong>Build relationships with senior executives:</strong>&nbsp;We could potentially dodge some of the difficulties around internal decision making if we had senior allies. At Meta, we know there are some&nbsp;<i>very</i> senior individuals who are sympathetic to EA. Our previous attempts to build these connections had been unsuccessful as most potential executive sponsors told us they were simply too busy, even when our requests were small (\u201csponsor\u201d an event, help us secure some charitable dollars, etc.). Potentially, a more concerted effort here could bear fruit.&nbsp;</li><li><strong>Focus on asynchronous engagement:</strong>&nbsp;One comparative advantage that workplace groups have versus university or city groups is they have access to workplace communication channels. Re-posting a newsletter, external opportunities or other interesting EA content is low effort, but fairly high return in that it could reach high potential but un-engaged future EAs. Our internal Meta@EA group had ~400 members, and posts would often be seen by &gt;100 individuals. We may have under-invested here and didn\u2019t experiment enough with content despite potentially high ROI.</li><li><strong>Focus on social activities:</strong>&nbsp;Given varying locations (our organizers were in a mix of SF, Seattle, NYC, Zurich, London, etc) we always felt it was difficult to organize social activities, especially given high burnout on online activities post quarantine. However, I think a lot of our successes can be attributed to our existing \u201csocial\u201d activities like our bi-weekly discussion group, so trying to identify what social events individuals will actually want to attend seems promising. Just trying to funnel individuals into local EA city events could be one strategy here.&nbsp;</li><li><strong>Building an external community for corporate organizers:</strong>&nbsp;We definitely felt pretty isolated as corporate organizers. Although we had interacted with some folks from other tech companies, these meetings were often sporadic and unstructured.&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/YNP9MZj5qqr46zsj7/introducing-ea-tech-network\"><u>There are already folks working on this</u></a>, and I could see these sorts of activities bolstering organizer retention in addition to sharing useful learnings.&nbsp;</li><li><strong>Support for corporate organizers:</strong>&nbsp;I know both HIP and CEA are thinking about how to support corporate organizers, although it\u2019s unclear to me what is most useful here. Conventional support mechanisms (money, formal positions) are not interesting to corporate organizers and many of the existing resources for university or city groups work just fine for corporate groups. Some potential areas that may be helpful: facilitating the external community (see above), helping think about measurement (measuring impact is super tough, I assume others have useful expertise here), building programs for professionals that give them some credentialism (e.g.,build an AI Safety program that folks might see as furthering their career) or simply legitimizing the groups somehow (may help organizer retention).&nbsp;<br>&nbsp;</li></ul>", "user": {"username": "Conor McGurk"}}, {"_id": "qmiQFGKpAySosDwAa", "title": "List of donation opportunities (focus: non-US longtermist policy work)", "postedAt": "2022-09-30T14:32:34.115Z", "htmlBody": "<h1>Introduction&nbsp;</h1><p>In the past I have written on the EA Forum about where I am donating (<a href=\"https://forum.effectivealtruism.org/posts/6k6k5r9DtxQKMtT5i/where-are-you-donating-this-year-and-why-in-2019-open-thread\"><u>2019</u></a>,<a href=\"https://forum.effectivealtruism.org/posts/jkMgbiXcR4MDNFs3z/where-are-you-donating-in-2021-and-why?commentId=JWCEGcisffgngqTTw\">&nbsp;<u>2021</u></a>). This year I have a dilemma. I have too many places I am excited to investigate and potentially donate too.</p><p><strong>I have a document where I have been listing opportunities I am excited by and thought why not share my list with others in the run up to the giving season.</strong> In my opinion there are a lot of EA projects lacking funding. I believe donors (especially medium-size non-US donors) who could evaluate and fund some of these would have an outsized impact, more impact than just giving to various&nbsp;<a href=\"https://funds.effectivealtruism.org/\"><u>EA Funds</u></a> (in fact this post is a follow up of a&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Xfon9oxyMFv47kFnc/some-concerns-about-policy-work-funding-and-the-long-term\"><u>red team post on the LTFF</u></a>). &nbsp;Also I am also interested in feedback and criticism of my donation list as I plan to donate to at least some of the places listed below (to be decided).</p><p><strong>The primary focus is on longtermist/ EA policy work</strong>. By policy work I am considering organisations that directly influence current policy so it is better for the long run future (not just indirectly doing policy-adjacent academic research or supporting individuals policy careers, etc). I list a few ideas on other cause areas at the end. Note that not every org/person listed considers themselves to be EA affiliated.</p><p>This list has about 25-30 funding ideas, ranging $15,000 to $4m, of which I estimate perhaps 66% are worth funding if investigated.</p><p>I want to caveat this with some warnings:</p><ul><li><u>Expect inaccuracies:</u> Not every org has had a chance to input. Also&nbsp;<strong>some details might be out of date</strong> \u2013 this list has been growing for a while now and some orgs may now have funding (or have run out of funding).</li><li><u>Expect conflicts of interest</u>: I have worked with and/or helped and/or am friends with the staff at and/or am an adviser/affiliate at a lot of the organisations I list. The only organisations listed that have paid me for work are the APPG for Future Generations and Charity Entrepreneurship. As a rule of thumb, assume I have some bias towards funding this kind of work.</li></ul><p>&nbsp;</p><h1>Background reasoning \u2013 why policy?</h1><h2><strong>Why fund policy work?</strong></h2><p>Influencing policy is an effective way to drive change in the world. It is the key focus of advocacy groups and campaigns around the globe and seen as one of the most high impact ways to affect society for the better.</p><p>This applies to existential risks. 80000 Hours research suggests there are two ways to protect the future from anthropogenic risks: technical work and governance/policy work (e.g. see&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/#ai-governance-research-and-implementation\"><u>here</u></a> on AI). There are many things to advocate for that would protect the future. See&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/5GnFLDfnzmK3Gv4gB/release-of-existential-risk-policy-database\"><u>here</u></a> for a list of 250 longtermist policy ideas and see (mostly UK focused) collections of policy ideas on&nbsp;<a href=\"https://docs.google.com/document/d/1kyc1p8mGx5c0d_pFbShNdJp5pKXQkpX58u7pDhfc248/edit\"><u>long-term institutions</u></a> and&nbsp;<a href=\"https://docs.google.com/document/d/1O-khCxjEPS6MzKtTvaKi3tHtufimVIk1ZMCBBj8ahcU/edit\"><u>biosecurity</u></a> and&nbsp;<a href=\"https://docs.google.com/document/d/1bI1ZCPmAOAlodaFH_eHeuBFfj4-vQPwSMaBwGwC1tIg/edit\"><u>ensuring AI regulation goes well</u></a> and&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors?commentId=sMB4nsao2nNtFd2yn\"><u>malevolent actors</u></a>.</p><p>Furthermore EA policy change work which is targeted and impact focused and carefully measured can be extremely effective. Analysis of 100s of historical policy change campaigns (look across various reports&nbsp;<a href=\"https://www.charityentrepreneurship.com/health-reports\"><u>here</u></a>) suggests a new EA charity spending around $1-1.5m, has a 10-50% chance driving a major policy change. And existing EA charities seem even more effective than that. The&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xX2JKae8GyzdY98Zn/appg-for-future-generations-impact-report-2020-2021\"><u>APPG for Future Generations</u></a> seemed to consistently drive a policy change for every ~$50k.&nbsp;<a href=\"https://leadelimination.org/projects/\"><u>LEEP</u></a> seems to have driven their first policy change for under ~$50k and seems on track to keep driving changes at that level.</p><h2><strong>Why might&nbsp;policy work be underfunded?</strong></h2><p>In short there is a lack of funders with the motivation and capability to fund this work.</p><ul><li>Some funders are avoiding funding policy work. The Long Term Future Fund (LTFF) does not fund any new policy work and is sceptical of policy work (see<a href=\"https://forum.effectivealtruism.org/posts/Xfon9oxyMFv47kFnc/some-concerns-about-policy-work-funding-and-the-long-term\">&nbsp;<u>here</u></a>). As far as I can tell OpenPhil\u2019s longtermist teams appear to have never funded non-US policy work (based on payout reports<a href=\"https://www.openphilanthropy.org/grants/\">&nbsp;<u>here</u></a>) and may be sceptical of such work (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/M2SBwctwC6vBqAmZW/a-personal-take-on-longtermist-ai-governance\"><u>here</u></a>).</li><li>Other funders are hard to access. FTX Future Fund, Effective Giving and Longview philanthropy do not currently accept funding requests. It is also very unclear their focus on funding policy work (e.g. there are some signs FTX are being cautious preferring regranters with relevant expertise to look into it). Also some policy projects might not want to be mostly funded by crypto money which makes up a significant chunk of some of these funds.</li><li>The Survival and Flourishing Fund (see <a href=\"https://survivalandflourishing.fund/\">SFF</a> and <a href=\"http://survivalandflourishing.org/\">SFP</a>) do accept applications and fund policy work. They might fund most of these things eventually but that is a single funding source and can be fairly slow to access. There are risks of opportunities being missed.</li><li>I think very few of groups are actively looking to fund existing think tanks (or projects) outside of the US.</li></ul><p>My best guess is that the reasons for this lack of funding are related to the challenges with vetting policy projects, especially the risk of such projects, especially for new projects, especially for non-US projects and US based funders that lack in-house policy expertise. (See some of my public conversations between me and funders to understand their cruxes&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Xfon9oxyMFv47kFnc/some-concerns-about-policy-work-funding-and-the-long-term\"><u>here</u></a>, including in the comments).</p><p>Currently I do see projects that look impactful (not evaluated in detail) closing down or failing to get started due to lack of funding.</p><p>I think this is part of a wider challenge that the longtermist community has about learning how to fund and support more direct type work by EA actors (as opposed to meta and research type work). See:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/TruJuwtdfszFJgzwB/longtermist-ea-needs-more-phase-2-work\"><u>Longtermist EA needs more Phase 2 work</u></a>.</p><p>So I believe there is an opportunity for small-medium sized funders to fund these projects.&nbsp;</p><p>&nbsp;</p><h1>Donation opportunities 4 U</h1><p><strong><u>Policy organisations seeking funding:</u></strong></p><p>These are groups already working on (or trying to get set up to work on) topics that would be of interest to the EA community. Not every org/person listed considers themselves to be EA affiliated:</p><ul><li><strong>APPG for Future Generations and related projects (Caroline Baylon)&nbsp;(UK). </strong>Could scale up work on the UK All-Party Parliamentary Group (APPG), get similar groups started around the world, do policy research, work with the UN, mentoring (with SERI), etc. Could use: $130,000 per year.&nbsp;<a href=\"https://www.appgfuturegenerations.com/\"><u>Website</u></a>.&nbsp;</li><li><strong>CSER (Centre for the Study of Existential Risk). </strong>UK based academic institute that has a policy staff member and does a fair amount of UK and international policy work. Runs a Science-Policy interface group of 30 members from the academic and policy world.&nbsp;<a href=\"https://www.cser.ac.uk/\"><u>Website</u></a>. Policy lead:&nbsp;<a href=\"https://www.cser.ac.uk/team/clarissa-rios-rojas/\"><u>Clarissa</u></a>.</li><li><strong>Instituut voor Publieke Economie (Netherlands). </strong>Institute for Public Economics. A think tank in the Hague focused on macroeconomic policy.&nbsp;<a href=\"https://www.instituut-pe.nl/\"><u>Website</u></a>.</li><li><strong>Norwegian policy think tank&nbsp;(Norway). </strong>There is interest in setting up a general EA / longtermist think tank. Could use $800k per year.</li><li><strong>Pandemic Prevention Network&nbsp;(UK). </strong>Engaging with UK MPs on stopping the next pandemic. Could use: $125k for next year increasing thereafter.&nbsp;<a href=\"https://www.pandemicpreventionnetwork.org/who-we-are\"><u>Website</u></a>.</li><li><strong>Simon institute for Longterm Governance&nbsp;(Switzerland). </strong>Future Generations and risk focused policy work in Geneva. Sam has independently evaluated the Simon Institute for a private donor, see <a href=\"https://docs.google.com/document/d/1TuR9n28qkTRLQuJjpMJuXz-Kw6C1_52pNglT0FKYEao/edit#heading=h.kr6zxlbn99zi\">here</a>. I think they still have a bit more room for more funding.&nbsp;<a href=\"https://www.simoninstitute.ch/\"><u>Website</u></a>.</li><li><strong>School of International Futures \u2013 Future Check project&nbsp;(UK). </strong>Plan is to vet all UK legislation before it goes through Parliament on the effect it will have on the future. Maybe other funding opportunities too.&nbsp;<a href=\"https://soif.org.uk/blog/future-check/\"><u>Website</u></a>.</li><li><strong>Social Change Lab. </strong>A research project looking at how social movements can be used to tackle the world's most pressing problems, why some movements succeed in achieving their aims and why some fail.&nbsp;<a href=\"https://www.socialchangelab.org/\"><u>Website</u></a>.</li><li><strong>Society for Decision Making Under Deep Uncertainty. </strong>Do useful and relevant work to promote a focus on how to decide under uncertainty in academics and a bit in policy spaces. Could benefit from $50k.&nbsp;<a href=\"https://www.deepuncertainty.org/\"><u>Website</u></a>.</li><li><strong>The Future Society&nbsp;(EU-US). </strong>Think tank that works on AI policy.&nbsp;<a href=\"https://thefuturesociety.org/\"><u>Website</u></a>. Contact point:&nbsp;<a href=\"mailto:nicolas.moes@thefuturesociety.org\"><u>Nicolas</u></a>.</li></ul><p>&nbsp;</p><p><strong><u>Policy organisations I would consider funding, that are not currently seeking funding:</u></strong></p><p>I think active grantmaking can be powerful. These projects that could be in the list above but are not currently looking for funding. Yet they are perhaps small enough they could maybe benefit from a proactive funder saying \u201c<i>this is exciting if you wanted funding we could provide $X to support or grow this.</i>\u201d</p><ul><li><strong>Spanish speaking world longtermist think tank.&nbsp;</strong>See EA Forum posts&nbsp;<a href=\"https://forum.effectivealtruism.org/s/jRkh2y4LPFFkgAAju\"><u>here</u></a><strong>.&nbsp;</strong>Contact point:&nbsp;<a href=\"mailto:juan@allfed.info\"><u>Juan</u></a></li><li><strong>Sweden Parliamentary group for Future Generations</strong></li></ul><p>I would also consider:</p><ul><li><strong>Funding people who have applied to the LTFF.&nbsp; </strong>Could reach out and see what policy projects they have rejected.</li></ul><p>&nbsp;</p><p><strong><u>Policy projects within larger organisations I would consider funding.</u></strong></p><p>For the case for such work see this post on&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/wggTqDSpJEmoGvjhD/we-should-consider-funding-well-known-think-tanks-to-do-ea\"><u>funding well-known think tanks to do EA policy research</u></a>. These are larger more established organisations that could be funded to work on topics that the EA community is interested in. I am not sure that funding think tanks drive lots of impact, but I do think there is a case for trying more of it outside the US (currently EA funders have funded many US based think tanks but very few non-US based think tanks, see comments&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/wggTqDSpJEmoGvjhD/we-should-consider-funding-well-known-think-tanks-to-do-ea#comments\"><u>here</u></a>). I think these each of these organisations could absorb $50-$500k and I could connect anyone keen to fund them:</p><ul><li><strong>Center for Global Development (UK-US). </strong>Global development focused think tank. They have been funded by EA funders before.</li><li><strong>Chatham House&nbsp;</strong>(UK). Very well respected top think tank working on International Affairs. Could be funded to work on global risks, international security, biosecurity, climate change, AI, etc.&nbsp;</li><li><strong>Emerging Markets Investors Alliance</strong> <strong>(Global / emerging markets)</strong>. Lobbies for change among governments and corporates; leverages the influence of investors. .</li><li><strong>Fabian Society&nbsp;(UK). </strong>Left wing think tank.</li><li><strong>Institute for Government&nbsp;(UK). </strong>Very well respected think tank. Could be well-placed to do work on government processes relating to future generations.</li><li><strong>New Diplomacy Project (UK)</strong>. Progressive foreign policy think tank.&nbsp;<a href=\"https://www.newdiplomacy.uk/\"><u>Website</u></a>.</li><li><strong>RAND&nbsp;(US)</strong>. Think tank soon being led by Jason Matheny, has done some work for Open Phil in the past.</li><li><strong>Social market Foundation</strong> <strong>(UK)</strong>. Centrist think tank that could be funded to work on various EA-adjacent UK-domestic policy topics such as: biosecurity, institutional decision making, macroeconomics, growth, housing, subjective wellbeing, pronatalism, etc. Contact point: <a href=\"mailto:aveek@smf.co.uk\">Aveek</a>.</li></ul><p>&nbsp;</p><p><strong><u>Other projects on my list of things to fund (all cause areas)</u></strong></p><ul><li><strong>Charity Entrepreneurship charities.&nbsp;(Mixed)</strong>. Charity Entrepreneurship starts a host of new charities every year. Next bacth soon to be announced, keep your eyes on&nbsp;<a href=\"https://www.charityentrepreneurship.com/our-charities\"><u>https://www.charityentrepreneurship.com/our-charities</u></a>&nbsp;<ul><li><strong>Vida Plena</strong> and&nbsp;<strong>Kaya Guides</strong>. (Mental health). Two newly incubated mental health charities I am excited about.</li></ul></li><li><strong>Food Fortification Initiative. (Global health).</strong> I think this project is doing really impactful policy work to encourage adoption of mandatory food fortification. Should be more well known, although maybe it is sufficiently funded. See&nbsp;<a href=\"https://www.givewell.org/charities/food-fortification-initiative-March-2017-version\"><u>GiveWell analysis</u></a> and&nbsp;<a href=\"https://www.ffinetwork.org/\"><u>website</u></a>.</li><li><strong>Good growth. (Animal welfare). </strong>Animal work in China. Seems super important and neglected. Could maybe benefit from $100k.&nbsp;<a href=\"https://www.goodgrowth.io/\"><u>Website</u></a></li><li><strong>Independent evaluation of EA meta orgs. </strong>I would be interested in funding EA meta charities to have an independent evaluation. I think I know some folk who could do this decently. Estimate \u00a35k-\u00a310k per evaluated charity. Now just need a target organisation...</li><li><strong>Other work in China.</strong> I would be interested to hear from donors in China with regards to funding projects in China, please get in touch.</li><li><strong>Professor Chris Chambers.&nbsp;</strong>Working on registered reports. Maybe still seeking $420,000. See:&nbsp;<a href=\"https://lets-fund.org/better-science/\"><u>Lets Fund Review</u></a>.</li><li><strong>Scandinavian X-risk institute in Stockholm.&nbsp;</strong>(Longtermist). I have minimal details on this. Maybe it is like an FHI outside the UK.</li><li><strong>Training for good, (EA meta). </strong>Impact-focused training organisation, focused on journalism and policy careers. Was looking for $500k. See <a href=\"https://docs.google.com/document/d/1xAPxbIuu3kV2DbN2AxyjxEW_QiILSVbJHltFU24c5VA/edit#\">proposal</a> and <a href=\"https://www.trainingforgood.com/\">website</a>.</li></ul><p>If you're interested in learning more about or getting in touch with any of these, please ask me and I can connect you.&nbsp;</p><p>&nbsp;</p><h1>Thoughts on giving to these projects</h1><h2><strong>How much to give?</strong></h2><p>I think medium sized funders<strong>&nbsp;</strong>(giving $25k+) could have a transformative impact on some of these projects and have a positive effect on the direction these projects go in (or even if they keep running at all). Smaller donors could give too but might alternatively consider entering&nbsp;<a href=\"https://funds.effectivealtruism.org/donor-lottery\"><u>a donor lottery</u></a> to have a chance at larger donation.</p><p>I think some of the amounts being asked for are at the top end of what is needed and may be inflated for pitching to large funders. Additionally for some of the newer projects encouraging slow and steady growth would be sensible to minimise the chance of growing in the wrong direction. Maybe offering 50-80% of a requested budget would be ideal for a funding constrained donor.</p><p>On the other hand small projects not currently looking for funding could still maybe benefit from a proactive funder offering support.</p><h2><strong>How to evaluate?</strong></h2><p>I have not evaluated these projects and do not give a blanket endorsement.</p><p>Some things to think through before evaluating these projects are:</p><ul><li><strong>Local context.&nbsp;</strong>Talk to people you know who work in policy in that country. I think the main opportunity here comes from non-US funders who understand their local regions, collaborating with policy folk they know locally, and working out if projects in their region should be funded.</li><li><strong>The risks.&nbsp;</strong>Policy has some reputation risks that funders should think through. The biggest risk will be from big public facing campaigns (like&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Qi9nnrmjwNbBqWbNT/the-best-usd5-800-i-ve-ever-donated-to-pandemic-prevention\"><u>this one</u></a>). That said, identifying risks does not mean refusing all funding (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Xfon9oxyMFv47kFnc/some-concerns-about-policy-work-funding-and-the-long-term#A_guess_as_to_what_might_be_going_on_\"><u>here</u></a>) and funders should consider using donations to help mitigate risks (e.g. encouraging hiring of a comms staff or policy design staff, etc). See some general thoughts on policy and risks&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Q7qzxhwEWeKC3uzK3/managing-risk-in-the-ea-policy-space\"><u>here</u></a>.</li><li><strong>Evidence of impact. </strong>I think in most cases a well run policy project should be able to demonstrate actually having changed government policy within its first 3 years of running, or less. Not just evidence of interest, but actual government commitments or funding or appointments. We should hold policy work to high standards.</li></ul><p>As mentioned one donor asked me to do an independent evaluation of the Simon Institute (SI). I did this in two rounds (read SI's stuff, wrote up a view, got comments from SI, reconsidered view). If this is a useful example for people to see it can be seen <a href=\"https://docs.google.com/document/d/1TuR9n28qkTRLQuJjpMJuXz-Kw6C1_52pNglT0FKYEao/edit\">here</a>. If you want me help with evaluating policy projects please do get in touch.</p><p>&nbsp;</p><h1>Good luck giving!!</h1><p>Best of luck maximising the impact of your donations this year!!</p>", "user": {"username": "weeatquince"}}, {"_id": "GMfJpubiw3enNZzQX", "title": "High Impact Professionals Will Help Fill Open Roles at Your EA Org", "postedAt": "2022-09-30T13:17:33.035Z", "htmlBody": "<p><a href=\"https://www.highimpactprofessionals.org/\"><strong><u>High Impact Professionals</u></strong></a><strong> is trialing a new matchmaking service where we connect qualified EA talent with the high-impact organizations that need them.</strong> We will recruit for your organization and try to find candidates that you wouldn\u2019t have come across otherwise. The end result for you is more positions filled with better talent. We have a large network of talented EAs, both professionals and non, that we are ready to align with your organization\u2019s hiring needs.</p><p><strong>If you would like support in filling a fully remote position at your EA organization, please fill out&nbsp;</strong><a href=\"https://bit.ly/3USn4l2\"><strong><u>this form</u></strong></a><strong>&nbsp;</strong>and we\u2019ll get back to you with next steps.</p><p>&nbsp;</p><p>During this initial trial phase, we plan to help a few organizations due to capacity constraints, so please understand if we don\u2019t reply to all requests. From there, we\u2019ll scale up or down based on our assessment of the service\u2019s viability.</p><p>Feel free to message&nbsp;<a href=\"mailto:devon@highimpactprofessionals.org\"><u>devon@highimpactprofessionals.org</u></a> or write in the comments if you have any questions.</p><p><br>&nbsp;</p>", "user": {"username": "High Impact Professionals"}}, {"_id": "2gG7eeDD5uqud4Rfm", "title": "Cost-effectiveness of iron fortification in India is lower than GiveWell's estimates", "postedAt": "2022-09-30T12:50:18.032Z", "htmlBody": "<h2>Summary</h2><p>GiveWell considers iron fortification programs in India as cost-effective and has made incubation grants to Fortify Health that runs wheat fortification initiatives in 2018, <a href=\"https://www.givewell.org/research/incubation-grants/fortify-health/august-2019-grant#reallocate\">2019</a> and 2021. The effectiveness of iron fortification was assessed to primarily arise from three categories of benefits - 1. averting anemia morbidity, 2. cognitive benefits for children, and 3. cognitive benefits for adults. Two key variables underlying the magnitude of these benefits are \u2014 the<strong> prevalence of anemia</strong> across age and sex groups in India and the<strong> proportion of anemia cases that can be attributed to iron-deficiency</strong>.&nbsp;</p><p>Recent work in the public health of anemia and data from the Comprehensive National Nutrition Survey <strong>suggests a significant update in the above two variables that dramatically decreases the cost-effectiveness</strong> of iron fortification programs in India.&nbsp;</p><p>First, WHO guidelines for hemoglobin cutoffs to diagnose anemia have come under scrutiny for not being representative of non White populations. Recent evidence from work seeking to establish<strong> Indian population representative hemoglobin cutoffs</strong> suggests that the <strong>actual prevalence of anemia in different groups in India may be 1/2 - 1/3 </strong>of the estimates used by GiveWell's model. This alone r<strong>educes the direct health benefits </strong>of the $4.5M 2019 grant to Fortify Health by 58%<strong>, </strong>or<strong> 1.2x the effectiveness of GiveDirectly.</strong></p><p>Second, work on the The Biomarkers Reflecting Inflammation and Nutritional Determinants of Anemia (BRINDA) project suggests that in <strong>countries with a high degree of infection, iron-deficiency may contribute to a smaller proportion of anemia</strong> cases. Data from the CNNS provided evidence that this is the case with Indian children/adolescents and it seems intuitive that it also applies to adults.</p><p>&nbsp;Finally, GiveWell assessed that there was no significant risk of harm associated with iron fortification. Emerging evidence suggests a potential for risks that are dramatically higher than assumed, particularly that<strong> increases in iron can lead to a rise in non-communicable diseases like type-2 diabetes</strong>. Such risks are not covered in GiveWell's report and deserve careful consideration given their magnitude of harm.</p><h2>Major Sources of Uncertainty</h2><ul><li>The role of iron in health and disease, the aetiology of anemia, and fortification as an intervention are vast and nuanced topics with hundreds of papers and meta-reviews that I could not possibly review. I am also not a medical or public health scientist and although I consulted doctors while researching/writing and received positive feedback from a public health scientist on a version of this work, it is possible I'm making some basic mistakes in my interpretation of the literature.</li><li>One of the major empirical leaps I make is that the proportion of anemic adults using new hemoglobin cutoffs can be roughly inferred from the ratio of prevalences b/w 15-19yr olds and adult age groups from the old cutoffs and data of 15-19yr olds from the new cutoffs. That is, I'm <strong>assuming the distribution of anemia across age-groups is similar using either old or new cutoffs</strong>. This seems like a fair assumption to make for a ballpark estimate but maybe the distributions are weird such that this was unfounded. &nbsp;This is especially important as GW's model places a very high weight on adult benefits.</li><li>The section on risks from excess iron relies on reading only a few recent papers and the Indian specific evidence comes from a pre-print. &nbsp;</li></ul><h2>Background</h2><p>Anemia is a condition with a multifactorial etiology characterized by a decrease in red blood cells and reliably marked by a decrease in hemoglobin levels. The loss of red blood cells leads to a deprivation of oxygen with numerous physiological, cognitive and economic implications. Anemia during childhood and adolescence can lead to severe developmental issues, some of which may be irreversible. (<a href=\"https://www.who.int/health-topics/anaemia#tab=tab_1\">WHO</a>)&nbsp;</p><p>Anemia is often associated with a deficiency of iron.&nbsp;<a href=\"https://academic.oup.com/jn/article/151/8/2422/6287924\">However</a>, deficiencies in other micronutrients such as the vitamins folate, B12 and A, infections by malarial parasites, worms, and others, inflammation from sub-acute infections, microbiota disturbances as well as hemoglobinopathies (structural defects in RBC\u2019s) are other important factors associated with anemia, many of which have a complex relationship with iron levels.&nbsp;</p><p>GiveWell's&nbsp;<a href=\"https://www.givewell.org/international/technical/programs/iron-fortification#Effect_on_iron_deficiency_and_anemia\">report</a> on iron fortification concludes that \u201cThere is strong evidence that iron fortification reduces cases of iron deficiency and anemia\u201d and iron fortification is a program they are \u2018actively supporting'. GiveWell has made incubation grants towards Fortify Health, with&nbsp;the latest being $8M in 2021 to expand wheat fortification in India. The report notes that the primary driver of the cost-effectiveness (3/4th of the value) comes the from the value of information the grant would provide towards reallocation of future additional funding to FH over less-effective programs.</p><h2>Current cost-effectiveness modeL</h2><p>In GiveWell\u2019s cost-effectiveness model for iron fortification, the benefits are broken down into three categories -</p><ol><li>Benefits from averting anemia-related morbidity (39% of the value)&nbsp;</li><li>Cognitive benefits for children (8% of the value)&nbsp;</li><li>Cognitive benefits for adults (53% of the value)</li></ol><p>In their <a href=\"https://docs.google.com/spreadsheets/d/1QZNP8J_TSCjLN1iIuOdupR1l_v4r-789jXm2kBFfyNg/edit#gid=1417746729\">model</a> for the 2019 grant to FH for wheat iron fortification in India, these benefits were calculated using data from the Global Burden of Disease (GBD) database. The model then factors in the specific demographic of FH's target population, specifically looking at only the states of Maharashtra and West Bengal, and only the top three wealth quintiles. All benefits end up being downstream from estimates of the anemia prevalence in India. Specifically -</p><ul><li>For 1, the model bases anemia-related morbidity on the 2017 estimate of 859 YLDs/100,000, based on 2017 anemia prevalence of 28.4% across ages and sex.</li><li>For 2, the model is based on the estimate that 21.1% of children 0-14 have dietary iron deficiency.</li><li>For 3, the model is based on the estimate that 24.5% of working-age adults have dietary iron deficiency.</li></ul><p>GBD anemia data for India was based on <a href=\"https://(https://ghdx.healthdata.org/gbd-2019/data-input-sources?components=5&amp;impairments=192&amp;locations=163)\">42</a> sources, with the most recent being the Demographic Health Survey (DHS) conducted in 2016. This survey, as well as the most recent one from 2021, uses hemoglobin level (g/dL in blood) cutoffs based on current WHO standards.&nbsp;</p><h2>Updates to cost-effectiveness</h2><h3>Anemia prevalence in India may be significantly lower than reported</h3><p>Crucially, the WHO hemoglobin cutoffs are under scrutiny for being non-representative of non White populations.&nbsp; The current cutoffs are largely intact from 1968, where they were proposed based on five studies from North America and European populations. (<a href=\" https://doi.org/10.1016/S2352-3026(18)30004-8\">paper</a>) In a <a href=\"https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14096\">review</a> of 60 global studies between 1975 and 2018, the authors evaluated haemoglobin variation across the lifecycle and found that haemoglobin cutoffs in children and adolescents from African and Asian regions were substantially lower (1\u20132 g/dL) in some datasets. This prompted the WHO to hold a technical <a href=\"https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1111/nyas.14090 \">meeting</a> in 2019 where emphasis was given to the need to change diagnostic criteria, with one proposal being \"anemia could be defined by hemoglobin levels below the reference range, that is, below a statistical centile (e.g., the 2.5th centile of hemoglobin in a healthy population).\"</p><p>Following up on this work, a 2021 Lancet <a href=\"https://www.sciencedirect.com/science/article/pii/S2214109X21000772\">paper</a> by Sachdev and colleagues examined data from the 2019 Comprehensive National Nutrition Survey [CNNS], a large-scale, nationally representative survey of children and adolescents aged 0\u201319 years in India and \u201cconstructed age-specific and sex-specific haemoglobin percentiles from values reported for a defined healthy population in the CNNS.\u201d They found that \u201cstudy cutoffs for haemoglobin were lower at all ages, usually by 1\u20132 g/dL\u201d that resulted in a large drop in anemia prevalences as compared to WHO findings.&nbsp;</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667994657/mirroredImages/2gG7eeDD5uqud4Rfm/okhtqfjtpbzbkbwjopkt.png\"><br>&nbsp;</p><p>Overall, the new representative cutoffs lead to a 20 point decrease in anemia prevalence, or 1/3rd the prevalence estimated using WHO cutoffs, for 0-19 yr olds in India. The same was true when excluding 15-19yr olds.</p><p>It's important to note that the anemia prevalences using the standard WHO cutoffs differs significantly between the CNNS and the DHS (used by GBD and GW) for the same age-groups. The likely reason is that the CNNS performed a more rigorous assessment using multiple biomarkers and a whole blood count, while the DHS only used capillary blood from a finger prick (known to provide a less robust measure of hemoglobin).&nbsp;</p><p>I performed a very crude calculation to estimate anemia prevalences using the age-group proportions in the DHS data and the 15-19 year old data from both surveys as a bridge, giving the following estimates -&nbsp;</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_141 141w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_221 221w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_301 301w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_381 381w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_461 461w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_541 541w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_621 621w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_701 701w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/e47e1c305c124fc5a4d940fe02164d0dae87f98de8671c78.png/w_781 781w\"></figure><p>Roughly, the decrease in anemia prevalence for both males and females across age groups is 1/3rd the original prevalence when WHO cutoffs were used.</p><p>To sum up, the basic assumptions of the model would change in the following ways (I'm not accounting for state-wise differences though I expect the effect size to not matter in this ballpark) -&nbsp;</p><ul><li>The <strong>prevalence of anemia in 0-14yr olds is 10.78%, half the previous estimate&nbsp;</strong></li><li>The <strong>prevalence of anemia in adults is 8%, a third of the previous estimate&nbsp;</strong></li><li>The <strong>YLDs from anemia is halved</strong> (conservative simplification)</li></ul><p>Keeping all the other parameters the same, this<strong> </strong>reduces the direct health benefits of FH's iron fortification by 58%, leaving the program's direct benefits to be only 1.2x as effective as GiveDirectly.</p><h3>Proportion of anemia cases due to iron-deficiency may be lower than estimated</h3><p>Not only are actual anemia prevalences likely lower than estimates used in GW's model, the proportion of anemia cases in India attributable to a deficiency of iron may also be less than GBD data suggests. Recent <a href=\"https://academic.oup.com/ajcn/article/106/suppl_1/402S/4668592\">work</a>, like The Biomarkers Reflecting Inflammation and Nutritional Determinants of Anemia (BRINDA) project, points towards a smaller contribution of iron-deficiency (ID) to anemia in countries with high infection rates. &nbsp;The authors note that -</p><blockquote><p>our results suggest that, in settings with low and moderate burdens of infectious disease, nutritional factors such as ID are important contributors to anemia. In contrast, in settings with a high burden of infectious disease, the contributors to anemia are more diverse, and both nutrition and infectious disease control are important factors to address.</p></blockquote><p>The authors suggest that anemia mitigation interventions should be tailored towards specific countries and follow detailed risk assessment studies, as well as factoring in inflammation-adjusted iron status indicators.</p><p>The 2018 CNNS data once again comes in handy as it includes relevant information needed to identify the prevalence of ID-associated anemia. A 2021 Journal of Nutrition paper by Sachdev et al performs an analysis of the CNNS data for 0-19 yr olds and finds that -</p><blockquote><p>This is the first study from India providing estimates of ID prevalence in a representative sample of children and adolescents at the national and state levels using multiple inflammation-adjusted ID indicators. In preschool children and adolescent girls, ID based on SF adjusted for inflammation by the modified BRINDA method was a public health problem of \u201cmoderate\u201d proportions (\u223c30%\u201332%), whereas in 5- to 9-y-old children (15%) and adolescent boys (11%) it was a public health problem categorized as \u201cmild\u201d&nbsp;</p></blockquote><p>The authors claim that these ID-prevalences are lower than other smaller non-nationally representative samples (~<a href=\"https://academic.oup.com/jn/article/142/11/1997/4743383\">63%</a> for 10yr olds, ~<a href=\"https://academic.oup.com/jn/article/148/9/1462/5054991\">50%</a> for 12-16 yr olds).</p><p>I'd stress that the above findings pack a lot of nuance. For one, a surprising finding was that ID inferred from serum-ferritin levels was higher in richer and urban states vs poorer and rural states - which on the surface level implies a more optimistic modelling outcome for GW's FH grant (which assumed fortification would reach richer quintiles who would have lower ID rates and thus have a lower impact). However, the authors note one possible reason for this could be a \"functional iron deficiency\" where</p><blockquote><p>possible inefficient utilization of stored iron for hemoglobin synthesis in children from lower wealth quintiles suggests that iron supplementation alone may be of limited value, and the focus should be on improved diet diversity, which facilitates iron absorption while providing all other hematopoietic nutrients.</p></blockquote><p>This consideration leads the authors to conclude that</p><blockquote><p>Our findings have important implications for iron supplementation and fortification programs. An important conclusion is that given the impaired iron utilization for hemoglobin synthesis in low-SES children, intended benefits on anemia will not accrue if iron intake is increased without also addressing the multiple environmental constraints related to poverty.</p></blockquote><p>I'm unsure how to assess the impact of this section on GiveWell's cost-effectiveness model as I wasn't able to pin down the estimate for iron-deficiency's contribution to anemia in the model. Since the BRINDA method and other work were published from 2018 onwards, and the GBD's data sources are much older, about a decade prior on average, it seems likely that GBD overestimates ID's contribution to anemia. &nbsp;Like before, the CNNS data is restricted to age groups b/w 0-19 yrs, but intuitively should extend to adult age groups as well.</p><h3>Risks from iron overload may be higher than expected</h3><p>In their report on iron fortification, GiveWell notes the potential adverse effect of iron fortification and concludes that not enough evidence exists to suggest that risk is high. In particular, the risks examined were an increased risk of malaria, an increased risk of non-malarial infections, iron overload, and gastrointestinal side effects. On the subject of iron overload specifically, they note -</p><blockquote><p>We have not come across any evidence on the prevalence and effects of iron overload in our reviews of the literature and are uncertain about how common this is. Our guess is that cases of <strong>severe iron overload are rare in iron fortification programs because the effective dose is far lower</strong> than in most supplementation programs. However, we would<strong> consider investigating iron overload in more depth if we recommended additional funding to iron fortification programs.</strong></p></blockquote><p>In their $8 million 2021&nbsp;<a href=\"https://www.givewell.org/research/incubation-grants/Fortify-Health-expansion-December-2021#Cost-effectiveness\">report</a> to Fortify Health, they note that they have not \u201cincorporated any additional evidence on fortification published since that [iron fortification] report.\u201d Adverse effects are also not included in their cost-effectiveness model for the 2019 grant.</p><p>While I did not extensively examine the literature, a few papers stood out that provided evidence for additional iron, in the range of the effective dose from fortification, had potential to lead to harmful outcomes. Worryingly, these outcomes were not mentioned, let alone assessed, in GiveWell's report.</p><p>Specifically, two risks that appear to be associated with excess iron are non-alcoholic fatty liver disease and type-2 diabetes \u2014</p><p>Additional iron may increase non-alcoholic fatty liver disease in obesity through the gut microbiome (<a href=\"https://link.springer.com/article/10.1186/s40168-021-01052-7#Sec9\">paper</a>)</p><blockquote><p>Serum ferritin levels, as a markers of liver iron stores, were positively associated with liver fat accumulation in parallel with lower gut microbial gene richness, composition and functionality.</p></blockquote><p>Iron metabolism and type 2 diabetes mellitus: A meta-analysis and systematic review (<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/jdi.13216\">paper</a>)</p><blockquote><p>A total of 12 case\u2013control and cohort studies were analyzed. Of the 12 studies, 11 described the correlation between serum ferritin levels and type&nbsp;2 diabetes. The median and high serum ferritin concentrations were significantly associated with the risks of type&nbsp;2 diabetes (odds ratio [OR] 1.20, 95% confidence interval [CI] 1.08\u20131.33 and OR 1.43, 95% CI 1.29\u20131.59, respectively).</p></blockquote><p>Again, data from the CNNS analysed by Sachdev and co provides information for the Indian context. They investigated whether serum ferritin levels (a functional form of iron) was associated with high fasting blood sugar levels (FBS - marker of diabetes), high blood pressure (HTN - marker of hypertension) and high total cholesterol levels (TC - marker of dyslipidaemia). They found an \u201codds ratio (OR) of high FBS, HTN and TC were 1.05 (95% CI 1.01-1.08), 1.02 (95% CI: 1.001-1.03) and 1.04 (95% CI: 1.01-1.06) respectively for every 10\u03bcg/L increase in SF. The odds for high TC increased with co-existing prediabetes. The scenario analysis showed that providing<strong> 10 mg of iron/day by fortification could increase the prevalence of high FBS by 2%-14%</strong> across states of India. Similar increments in HTN and TC can also be expected.\u201d</p><p>The increase in risk of diabetes is especially relevant in India where a large proportion of children (50%)&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/33893450/\">appear</a> to have at least one biomarker of \u2018metabolic obesity\u2019 that predisposes them to diabetes.</p><p><br>I didn't try to quantify the magnitude of risk and want to stress that my research and the findings are preliminary/limited. However, the directionality and potential effect size warrants a closer look at the evidence of harm from iron fortification, especially in light of the smaller size of the benefits.</p><h3>Meta-level, Optics, and Ethics considerations</h3><p>Three parting remarks &nbsp;-</p><ul><li>In GW's report on iron fortification, they note that \"In our assessment of the effect of iron fortification on iron deficiency and anemia, we rely heavily on <a href=\"https://academic.oup.com/ajcn/article/96/2/309/4576822\"><strong>Gera, Sachdev, and Boy 2012</strong></a>, a meta-analysis of randomized and quasi-randomized trials of the effect of iron fortification on measures related to iron deficiency and anemia\". &nbsp;The corresponding author of the meta-analysis and the analyses of the CNNS data, including the iron-diabetes linkage, is <a href=\"https://scholar.google.com/citations?user=8i7HiEcAAAAJ&amp;hl=en&amp;oi=ao\">HPS Sachdev</a>. Mentioning this because it caused me to update positively towards the credibility/quality of the evidence I present here.</li><li>Multiple pieces critical of iron fortification have been published recently in leading Indian newspapers (<a href=\"https://www.thehindu.com/sci-tech/science/inescapable-risks-of-mandatory-iron-fortification/article37986787.ece\">The Hindu</a>, <a href=\"https://indianexpress.com/article/opinion/columns/rice-fortification-programme-iron-anaemia-7470938/\">Indian Express</a>, <a href=\"https://timesofindia.indiatimes.com/city/raipur/report-says-iron-fortified-rice-unscientific-and-risky/articleshow/92349468.cms\">Times of India</a>). International philanthropy/charities already tend to be viewed with a degree of hostility in India and it's possible EA gets dragged under the bus because of its support for iron fortification.</li><li>I'm worried about the ethics of fortification with respect to the way it takes away agency from the dietary choices of people. This seems okay if we can be extremely confident that the choices being pushed have minimal risks of harm (a priori seems true for chlorination of water and vitamin A supplementation) but iron doesn't seem to as clear-cut. It also seems important that GiveWell consults with medical and public health researchers from respective countries where interventions are being assessed to catch country-specific considerations.</li></ul><p>&nbsp;</p><h3>References</h3><p>GiveWell Iron Fortification report -<a href=\"https://www.givewell.org/international/technical/programs/iron-fortification#Effect_on_iron_deficiency_and_anemia\"> https://www.givewell.org/international/technical/programs/iron-fortification#Effect_on_iron_deficiency_and_anemia</a>&nbsp;</p><p>Fortify Health 2021 Grant report -<a href=\"https://www.givewell.org/research/incubation-grants/Fortify-Health-expansion-December-2021#Cost-effectiveness\"> https://www.givewell.org/research/incubation-grants/Fortify-Health-expansion-December-2021#Cost-effectiveness</a>&nbsp;</p><p>GiveWell 2019 model</p><p><a href=\"https://docs.google.com/spreadsheets/d/1QZNP8J_TSCjLN1iIuOdupR1l_v4r-789jXm2kBFfyNg/edit#gid=1417746729\"><u>https://docs.google.com/spreadsheets/d/1QZNP8J_TSCjLN1iIuOdupR1l_v4r-789jXm2kBFfyNg/edit#gid=1417746729</u></a>&nbsp;</p><p>WHO Anemia -<a href=\"https://www.who.int/health-topics/anaemia#tab=tab_1\"> https://www.who.int/health-topics/anaemia#tab=tab_1</a>&nbsp;</p><p>DHS 2021 data -&nbsp;</p><p><a href=\"https://dhsprogram.com/pubs/pdf/FR375/FR375.pdf\"><u>https://dhsprogram.com/pubs/pdf/FR375/FR375.pdf</u></a>&nbsp;</p><p>1968 WHO hemoglobin cutoffs -&nbsp;<a href=\"https://doi.org/10.1016/S2352-3026(18)30004-8\"><u>https://doi.org/10.1016/S2352-3026(18)30004-8</u></a></p><p>Cutoffs varying across population and lifecycle -&nbsp;<a href=\"https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1111/nyas.14096\"><u>https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1111/nyas.14096</u></a>&nbsp;</p><p>Report on 2019 WHO technical meeting -&nbsp;<a href=\"https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1111/nyas.14090\"><u>https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1111/nyas.14090</u></a>&nbsp;</p><p>Haemoglobin thresholds to define anaemia in a national sample of healthy children and adolescents aged 1\u201319 years in India: a population-based study, The Lancet, 2021 -<a href=\"https://www.sciencedirect.com/science/article/pii/S2214109X21000772\"> https://www.sciencedirect.com/science/article/pii/S2214109X21000772</a></p><p>Prevalence of Iron Deficiency and its Sociodemographic Patterning in Indian Children and Adolescents: Findings from the Comprehensive National Nutrition Survey 2016\u201318, The Journal of Nutrition, 2021 -<a href=\"https://academic.oup.com/jn/article/151/8/2422/6287924\"> https://academic.oup.com/jn/article/151/8/2422/6287924</a></p><p>BRINDA project -&nbsp;</p><p><a href=\"https://academic.oup.com/ajcn/article/106/suppl_1/402S/4668592\"><u>https://academic.oup.com/ajcn/article/106/suppl_1/402S/4668592</u></a>&nbsp;</p><p>Is iron status associated with markers of noncommunicable disease in Indian children?-<a href=\"https://assets.researchsquare.com/files/rs-1136688/v1_covered.pdf?c=1638568122\"> https://assets.researchsquare.com/files/rs-1136688/v1_covered.pdf?c=1638568122</a>&nbsp;</p><p>Metabolic obesity in Indian children -&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/33893450/\">https://pubmed.ncbi.nlm.nih.gov/33893450/</a>&nbsp;</p><p>The Hindu Press piece - 'Inescapable risks of mandatory iron fortification' by Kurpad and Sachdev -<a href=\"https://www.thehindu.com/sci-tech/science/inescapable-risks-of-mandatory-iron-fortification/article37986787.ece\"> </a><a href=\"https://www.thehindu.com/sci-tech/science/ines\">https://www.thehindu.com/sci-tech/science/ines</a></p><p>Times of India - Report says iron-fortified rice unscientific and risky <a href=\"https://timesofindia.indiatimes.com/city/raipur/report-says-iron-fortified-rice-unscientific-and-risky/articleshow/92349468.cms\">https://timesofindia.indiatimes.com/city/raipur/report-says-iron-fortified-rice-unscientific-and-risky/articleshow/92349468.cms</a></p><p>Indian Express - Don\u2019t chase the mirage of iron-fortified rice - <a href=\"https://indianexpress.com/article/opinion/columns/rice-fortification-programme-iron-anaemia-7470938/\">https://indianexpress.com/article/opinion/columns/rice-fortification-programme-iron-anaemia-7470938/</a>&nbsp;</p>", "user": {"username": "Akash Kulgod"}}, {"_id": "icdd4FCKuwqyAuYBm", "title": "Eli's review of \"Is power-seeking AI an existential risk?\"", "postedAt": "2022-09-30T12:21:19.509Z", "htmlBody": "", "user": {"username": "elifland"}}, {"_id": "EArsSv7QuKyy8GyBg", "title": "The threat of synthetic bioterror demands even further action and leadership", "postedAt": "2022-09-30T08:58:58.644Z", "htmlBody": "<p><strong>Tl;dr: </strong>Technical progress in DNA synthesis has outpaced regulatory safeguards against negligent and malevolent misuse of DNA synthesis technology. The need for intervention from the community has been on our collective radar for sometime and <u>still</u> coordination on the hottest issues looks underwhelming.&nbsp;</p><p><strong>Epistemic status</strong>: Low - Master's degree in epidemiology and broad expertise in public sector/health policy but &lt;5 hours reading on this issue, synthetic biology, bioterrorism etc</p><p>The conversation on biosecurity in the EA community appears to somewhat&nbsp;<a href=\"https://forum.effectivealtruism.org/topics/biosecurity\"><u>infrequent</u></a><u> </u>and when it does to me it appears immature - short of leadership and expertise proportionate to the imminence, scale, neglectedness and tractability of the threat and solution, and the standard of analysis we expect in analyses and ideas proposed by EAs in the global health and development space.&nbsp;</p><p><strong>Background</strong></p><p>The problem is that DNA synthesis procedures enable persons to render pathogens undetectable by established methods, vaccines ineffective and increase virulence or lethality [<a href=\"https://www1.health.gov.au/internet/main/publishing.nsf/Content/ssba-fs-14\"><u>source</u></a>]. In a pandemic simulation by the Johns Hopkins Center for Health Security in 2018, a terror group modeled on Aum Shinrikyo engineered a virus that killed&nbsp;<u>150 million</u> people [<a href=\"https://www.newyorker.com/science/elements/the-terrifying-lessons-of-a-pandemic-simulation\"><u>source</u></a>]. At the 2021 Munich Security Conference, the Nuclear Threat Initiative wargamed an exercise where a terrorist group engineered a strain of monkeypox in a country with weak oversight that left&nbsp;<u>270 million dead</u> and more than three billion sickened [<a href=\"https://www.nti.org/wp-content/uploads/2021/10/2021-NTI-Annual-Report.pdf\"><u>source</u></a>].</p><p><strong>Urgency</strong></p><p>Engineered viruses are already cheap and could be even more readily available to bioterrorists or negligent entities in the immediate future:</p><ul><li>DNA synthesis technology lets you \u201cprint\u201d the sequence and create your own copy of the virus once you have the digital sequence for a biological agent. [<a href=\"https://www.vox.com/22937531/virus-lab-safety-pandemic-prevention\"><u>source</u></a>]. The cost of DNA&nbsp;<u>sequencing</u> dropped from $2.7 Billion to $300 in under 20 Years [<a href=\"https://onezero.medium.com/the-price-of-dna-sequencing-dropped-from-2-7-billion-to-300-in-less-than-20-years-f5e07c2f18b4#:~:text=The%20Price%20of%20DNA%20Sequencing,in%20Less%20Than%2020%20Years\"><u>source</u></a>]. The cost of gene&nbsp;<u>synthesis</u> per base pair has dropped from approximately $10 a base pair to approximately $0.10 a base pair over the past 10 years [<a href=\"https://synbio-tech.com/gene-synthesis-cost/#:~:text=Similar%20to%20the%20drastic%20decline,over%20the%20past%2010%20years.\"><u>source</u></a>]. The smallest viral genomes are 1000-2000 base pairs [<a href=\"https://www.frontiersin.org/articles/10.3389/fevo.2015.00143/full\"><u>source</u></a>].</li><li>By one estimate [<a href=\"https://www.synbiobeta.com/read/the-future-of-dna-synthesis\"><u>source</u></a>] well short of&nbsp;<a href=\"https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/\"><u>AI Impact\u2019s</u></a> survey standards, tabletop DNA synthesis that could widely proliferate biological weapon capabilities are less than 20 years away.&nbsp;&nbsp;</li></ul><p><i>Capital</i></p><p>U.S. federal government funding for civilian biosecurity in 2012 was around $560 million&nbsp;<a href=\"https://www.openphilanthropy.org/research/biosecurity/\"><u>[source]</u></a>. The only philanthropic funders of biosecurity identified by Open Philanthropy in their pre-COVID-19, 2014 shallow investigation was the Skoll Global Threats Fund, the Gates Foundation and Rockefeller Foundation which together funded biosecurity work by the Nuclear Threat Initiative. Since the advent of COVID-19 the landscape has changed but appears inadequate. To illustrate, the Nuclear Threat Initiative spent just under $6 million on the biological program in 2021 [<a href=\"https://www.nti.org/wp-content/uploads/2021/10/2021-NTI-Annual-Report.pdf\"><u>source</u></a>].</p><p><strong>Does anyone have a more up-to-date characterisation of the synthetic biological terrorism funding landscape?&nbsp;</strong></p><p><i>Policy</i></p><p>It was reported in September 2021 that the United Nation\u2019s Biological Weapons Convention has 3 staff members [<a href=\"https://genevasolutions.news/science-tech/covid-heats-up-debate-over-biological-weapons-convention\"><u>source</u></a>] whereas the Chemical Weapons Convention has hundreds (<strong>and has been how effective</strong>, to consider tractability). Screening DNA synthesis orders is&nbsp;<u>not legally required</u> by&nbsp;<u>any</u> national government [<a href=\"https://www.nti.org/about/programs-projects/project/preventing-the-misuse-of-dna-synthesis-technology/\"><u>source</u></a>] and The Johns Hopkins Center for Health Security found in 2021 that&nbsp;<u>94%</u> of countries have&nbsp;<u>no</u> national oversight, laws, regulation, agency or assessment of dual&nbsp;<a href=\"https://www.ghsindex.org/wp-content/uploads/2021/12/2021_GHSindexFullReport_Final.pdf\"><u>[source]</u></a>. <strong>Should we agitate within our home countries to strengthen biosecurity regulatory regimes?</strong></p><p>An estimated&nbsp;<u>80%</u> of global DNA synthesis providers belong to the International Gene Synthesis Consortium which&nbsp;<u>voluntarily</u> screen DNA synthesis orders and the remainder are believed to go unscreened [<a href=\"https://www.nti.org/about/programs-projects/project/preventing-the-misuse-of-dna-synthesis-technology/\"><u>source</u></a>].&nbsp;</p><p>The Nuclear Threat Initiative are coordinating international dialogue around biosecurity and DNA synthesis [<a href=\"https://www.nti.org/about/programs-projects/project/global-biosecurity-dialogue/\"><u>source</u></a>]. However, I see nothing published that could indicate a timeline for concrete regulatory strengthening, in their patch or otherwise. <strong>Is there any known work on this? </strong>I know they are developing a software package for their \"common mechanism\u2019\u2019 [<a href=\"https://www.nti.org/analysis/articles/common-mechanism-prevent-illicit-gene-synthesis/\">source</a>] for every DNA provider to be able to screen DNA sequences, making it easier and cheaper for them to screen DNA sequences and customers efficiently and at lower cost. However, it's unclear whether anyone is seriously working towards a global regulatory mechanism to compulsory that could control against opting out of screening, or illicit practice in under-regulated jurisdictions.</p>", "user": {"username": "dEAsign"}}, {"_id": "oc327uKuFeaPAChxZ", "title": "Job listing (open): Two positions at Malengo", "postedAt": "2022-09-30T08:43:35.544Z", "htmlBody": "<p>Dear all,</p><p>I hope you're well! Our new NGO Malengo, whose mission is to facilitate&nbsp;international educational migration, has had a good year: we more than doubled our program, and 18 young Ugandans are on their way to Germany to start their Bachelor's degrees. In the coming years we want to grow to many hundreds of students each year.&nbsp;</p><p>We're now looking to grow our team: We're hiring a Country Director for Germany, and a Senior Program Manager \u2014 Africa. Both jobs have great conditions; e.g. we offer unlimited time off, and very competitive salaries. Here are the detailed descriptions:&nbsp;<br><br>Country Director Germany:&nbsp;<a href=\"https://smrtr.io/bJKsh\">https://smrtr.io/bJKsh</a><br>Senior Program Manager \u2014 Africa:&nbsp;<a href=\"https://smrtr.io/bJrLK\">https://smrtr.io/bJrLK</a><br><br>More information about Malengo is on our website:&nbsp;<a href=\"https://malengo.org/\">https://malengo.org/</a><br><br>Please help us spread the word by sharing this widely! And please apply if you want to join us!</p><p>Many thanks and best wishes,</p><p>Johannes&nbsp;</p><p>&nbsp;</p><p>PS: Here is this year's cohort at a reception organized for them by the German Embassy in Kampala:</p><p><img src=\"https://mail.google.com/mail/u/0?ui=2&amp;ik=4e9e5f138c&amp;attid=0.1&amp;permmsgid=msg-a:r-9106793004409356285&amp;th=1838d5e08bad6ae4&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ9OSIhDKZOztqV9UHPksbIa5ys6LDPP8h-clvIwB4dFYhkz_3EG1VpelcajdLJhkz5BZ3qIdWt-rW-038R_9y0HosCNkGgKtu9wne80f7LsY9RC8tnfVXsm9CI&amp;disp=emb&amp;realattid=ii_l8o6g6mn0\"></p>", "user": {"username": "Johannes Haushofer"}}, {"_id": "ab8CuXZCRvXniLubZ", "title": "Have GWWC talk to your workplace/group this Giving Season!", "postedAt": "2022-09-30T05:11:13.464Z", "htmlBody": "<p>Giving What We Can is excited to talk to your workplace or community group about effective giving this Giving Season.</p><p>If you bring the people, we\u2019ll bring the content!</p><p>We\u2019ve found that these kinds of engagements have been successful for fundraising and engaging people in the effective giving and effective altruism communities. In fact, our Head of Marketing, Grace first heard about GWWC at a workplace talk!</p><p>You can register your interest in us running one of the following events for your group during November and December:</p><ul><li><strong>A talk on effective giving:&nbsp;</strong></li></ul><p>A standard talk we usually give to groups and workplaces (options for 15min, 30min, 60min, new content is being developed right now).</p><ul><li><strong>A workshop on high-impact philanthropy:&nbsp;</strong></li></ul><p>A new workshop format which aims to get people to reason about their approach to philanthropy and come up with their first steps.</p><ul><li><a href=\"https://www.givingwhatwecan.org/events/guides/giving-games\"><strong><u>A giving game</u></strong></a><strong>:</strong></li></ul><p>We sponsor some funds, give your group some information on different effective charities and the group votes on where to donate!</p><h1><strong>[&nbsp;</strong><a href=\"https://forms.gle/Xnywrca47a5b3Jmr7\"><strong><u>Register your interest!</u></strong></a><strong> ]</strong></h1><p>Our friends over at One for the World and High Impact Professionals are also running talks and fundraising events this season.</p><p>If you want some further evidence about why this could be impactful, or want to see what else is happening in this space, check out the following forum posts:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/bsqtndpfKbcbQRFEs/fundraising-campaigns-at-your-organization-a-reliable-path\"><u>Fundraising Campaigns at Your Organization: A Reliable Path to Counterfactual Impact</u></a> - High Impact Professionals</li><li><a href=\"https://forum.effectivealtruism.org/posts/sE7Y43JRQARAKzZ6k/key-factors-for-success-in-organizing-a-fundraising-campaign\"><u>Key Factors for Success in Organizing a Fundraising Campaign at Your Company</u></a> - High Impact Professionals</li><li><a href=\"https://forum.effectivealtruism.org/posts/erpcHiAPE3sAwej2Y/a-playbook-for-running-corporate-fundraising-campaigns\"><u>A Playbook for Running Corporate Fundraising Campaigns</u></a> - High Impact Professionals</li><li><a href=\"https://forum.effectivealtruism.org/posts/w285WuhX2AhyBWde5/would-you-like-to-host-an-effective-giving-talk-in-your\"><u>Would you like to host an effective giving talk in your workplace this Giving Season?</u></a> - One for The World</li></ul><p><br>&nbsp;</p>", "user": {"username": "Giving What We Can"}}, {"_id": "8n7bQRPS8mbscLybm", "title": "Which organizations are looking for funding from small donors?", "postedAt": "2022-09-30T05:21:59.019Z", "htmlBody": "<p>Ben Todd suggested <a href=\"https://forum.effectivealtruism.org/posts/cjH2puDzAFrtrrThQ/despite-billions-of-extra-funding-small-donors-can-still\">last year </a>that small donors might help organizations diversify their funding:</p><blockquote><p>It\u2019s also not healthy for an organisation to depend 100% on a single foundation for its funding. This means that until we have 3+ large foundations covering each organisation, small donors play a role in diversifying the funding base of large organisations. (Though note that you\u2019re only providing this benefit if your grantmaking process is independent from the large donors.)</p></blockquote><p>So... which organizations are looking for funding from small donors?</p><p>Open Philanthropy used to publish its suggestions, but their latest is <a href=\"https://www.openphilanthropy.org/research/suggestions-for-individual-donors-from-open-philanthropy-staff-2020/\">from 2020</a>. I imagine the funding landscape has changed since then.</p><p>(Also related: <a href=\"  https://forum.effectivealtruism.org/posts/rRvWNwdfveSS2uNtT/how-should-large-donors-coordinate-with-small-donors\">How should large donors coordinate with small donors?</a>)</p>", "user": {"username": "smountjoy"}}, {"_id": "ZPNNnEu2HGNSNmifo", "title": "We all teach: here's how to do it better", "postedAt": "2022-09-30T02:06:25.594Z", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/posts/bbtvDJtb6YwwWtJm7/epistemic-status-an-explainer-and-some-thoughts\"><i>Epistemic status</i></a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefe6toyvcwnf6\"><sup><a href=\"#fne6toyvcwnf6\">[1]</a></sup></span><i>: There\u2019s consistent meta-analytic evidence for the interventions and models presented here. Still, that research is only one piece of your&nbsp;</i><a href=\"https://www.frontiersin.org/articles/10.3389/feduc.2020.583157/full\"><i>evidence-based decision-making.</i></a><i> I\u2019ve been directive for brevity\u2019s sake. Use your judgement, adapt to the context, and let me know where you disagree in the comments.</i></p><h1><strong>Summary: we need to learn to teach better</strong></h1><p>Education is the #2&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/strategy\">focus area</a> for the Centre for Effective Altruism. Many of us educate, from field building to fundraisers, from coffees to conferences. As a result, I think we can do better by <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#If_educate_is_the_goal__use_evidence_based_education_strategies\">applying the meta-analyses on what works from educational psychology</a>. We can use models and strategies that have been shown to work for hundreds of thousands of students, rather than feeling like community building is a whole new paradigm.</p><ul><li>I think the EA community can make better decisions, and track their progress more effectively, with an <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#We_need_better_models_of_community_building_so_fewer_people_bounce_off\">evidence-based theory of change for community building</a>.</li><li>One <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#Self_determination_theory_is_a_very_well_supported_model_of_how_to_develop_motivation__agency__and_confidence\">useful and robustly supported model, self-determination theory</a>,<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefxlx4w02jlmm\"><sup><a href=\"#fnxlx4w02jlmm\">[2]</a></sup></span>&nbsp;suggests we can be more effective if we help community members:<ul><li><a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#We_say__educate__don_t_persuade___but_what_s_the_difference__Support_people_s_psychological_needs\">Feel more competent</a> by <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#If_educate_is_the_goal__use_evidence_based_education_strategies\">helping them developing valuable skills using evidence-based teaching strategies</a> (rather than focusing so much on knowledge, especially via passive learning)</li><li><a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#We_say__educate__don_t_persuade___but_what_s_the_difference__Support_people_s_psychological_needs\">Feel more autonomy</a> by <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#Educators_that_support_autonomy_tend_to_\">making learning more meaningful and aligning motivation with their personal values and goals (rather than using quite so much guilt and fear)</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#We_say__educate__don_t_persuade___but_what_s_the_difference__Support_people_s_psychological_needs\">Feel more connected</a> by <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#To_build_a_sense_of_belonging_and_relatedness__educators_can_\">empathising and accepting their experiences, promoting collaboration, and providing relatable role models.</a></li><li>Some of these ideas seem obvious, but the <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#Guilt_and_pressure_are_bad_long_term_motivators__Use_existing_values_instead_\">community is still affected by burnout, imposter syndrome, and perceived rejection</a>. By <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#Guilt_and_pressure_are_bad_long_term_motivators__Use_existing_values_instead_\">better supporting psychological needs</a>, we can mitigate the risks of people bouncing off or burning out.</li><li>If we want to know how well our community building is working, I think measuring these needs will give us an upstream predictor of <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#A__highly_engaged_EA__is_someone_who_has_internalised_the_values_and_knowledge_of_the_EA_community__and_is_autonomously_motivated_to_act_on_them\">highly-engaged EAs</a></li></ul></li><li>Of these psychological needs, I think community-building efforts in EA struggle mostly in promoting feelings of \u2018competence\u2019, and <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#If_educate_is_the_goal__use_evidence_based_education_strategies\">neglect the long list of evidence-based strategies for building skills</a>.<ul><li>Most efforts are currently reading, listening to talks, and engaging in discussions. There are far more effective methods of helping people learn valuable skills and aptitudes.</li><li>Primarily, we <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#If_educate_is_the_goal__use_evidence_based_education_strategies\">should give people more time doing hands-on practice with the most important skills in an environment where they get constructive feedback. </a>We should also <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#Learn_how_to_use_multimedia_so_it_doesn_t_overload_your_audience\">learn to better use multimedia</a>.</li></ul></li></ul><p>I appreciate and admire all those who do community building. If I identify any groups that could improve, it\u2019s because I think they have the skills and track record to do an incredible amount of good. I want them to thrive, and I hope they see any feedback here as constructive.</p><h1><strong>How to teach effectively, in five steps</strong></h1><p>For those of you who don\u2019t care about the causal model, or the evidence, the following summary will explain what good teaching looks like. I think we could frame more community building events using these steps. Doing so will build capacity in the community, and build members\u2019 motivation and engagement. This obviously isn\u2019t everything we should be doing to build a community\u2014social events, 1 on 1s, conferences, and reading groups still have their place. But, if we\u2019re trying to build a capable and motivated group, we should try the following more often.</p><ol><li>Generally,&nbsp;<strong>work backward from some new skills you\u2019re going to help people learn</strong>. These are your learning objectives. Knowledge is obviously good, but is most valuable and motivating when connected to important skills.<ol><li>For example, don\u2019t say \u201clearn about AI\u201d but instead \u201cmake well-calibrated forecasts about the future of society\u201d or \u201cmake career plans that do as much good as possible\u201d</li><li>Make these learning objectives explicit to the learners, so they know what you\u2019re trying to achieve together</li></ol></li><li>For longer programs, <strong>consider assessing people\u2019s competence at the end</strong>, in a way that helps both you and the learner know how you\u2019re going. This is your \u2018summative assessment.\u2019<ol><li>For example, don\u2019t just run a discussion group with no assessment, and avoid \u2018knowledge tests\u2019 like exams. Instead, have them do the skill (like make and justify some forecasts, make a career plan) and provide some feedback on how they\u2019re going.</li><li>If you\u2019re teaching an important skill, then practising it in this way should be motivating. If they\u2019re not doing it, the skill might not be important enough, you might not have explained how it\u2019s important, you might have set the bar too high, or might have forgotten to craft formative activities.</li></ol></li><li><strong>Craft \u2018formative\u2019 activities for them to practise the skill with you.</strong> This lets you give them feedback. You don\u2019t learn to play tennis by watching tennis or talking about tennis. It\u2019s hard to learn to volley for the first time by playing a real match. Show them how to volley, then have them hit lots volleys, then make it progressively closer to a match.<ol><li>Match the formative assessments to the summative assessments (and therefore, the learning outcomes).<ol><li>Don\u2019t just use discussions if you want people to finish your program by posting some research on the EA forum.</li></ol></li><li>Piece these activities together so they add up to your \u2018summative assessment.\u2019 Don\u2019t require skills on the summative assessment that you haven\u2019t practised together.<ol><li>If you want people to post research on the forum, break that skill down into parts. Practice each part together so you can provide feedback on their skills.</li></ol></li><li>Add variety to formative activities so they don\u2019t get boring.</li><li>Make them easier than you think they need to be. You have probably forgotten how hard it was to learn this skill. Make them progressively harder.</li><li>Some formative activities are better than others. For example, at some point consider using concept maps, having peers work together, having them grade each other\u2019s work, or simulate a professional role.</li></ol></li><li><strong>Now think about what knowledge or demonstrations might help people first practise the skill.</strong> You can learn to better hit tennis balls by seeing someone show good technique (slowly), or by understanding why top-spin is good. Present new content in small bites that engage both the learner's eyes and ears.<ol><li>Our brains are built for multimedia that uses sight and sounds. So, in general, video is better than text + pictures, which is better than just text or sound.</li><li>Make these simpler than you think they need to be. Use short sentences and simple language. You like skills, you probably forget how hard it was to understand this knowledge the first time. Start with a concrete or relatable example (like&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">\u2018humans run the world, not chimps\u2019</a>) before moving to something abstract (like&nbsp;<a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">\u2018AI could be catastrophic\u2019</a>).</li><li>Make it really obvious where learners should focus. Don\u2019t give them more than one thing to focus on at once. Don\u2019t distract them with (too many) memes, jokes, and asides unless they directly contribute to the learning points.</li><li>Empathise with misconceptions, but then correct them (like \"many people think the top charities are about twice as impactful as the average one, but it turns out the number is closer to 100 or 1000 times\"). By focusing your plan on correcting misconceptions, you're more likely to update beliefs rather than entrenching existing ones.</li><li>Intersperse this knowledge with frequent opportunities to think, remember, and apply what you\u2019re saying. Add little multiple choice questions, opportunities for discussion, or ask them to apply the idea immediately to a new example.</li></ol></li><li><strong>Provide warm, encouraging, understanding role-models</strong> (including yourself).<ol><li>Take care with things that might induce guilt or fear. Show empathy for how people might be feeling (like \u201cmany people, including me, feel a pang of guilt with this thought experiment\u201d). Check your values align (ask \u201cwhat\u2019s important to you?\u201d). Provide them with options to better reach their goals (for example, \u201cThings are less scary when we feel like we\u2019re doing something to fix the problem. Here are some things I found helpful.\u201d)</li><li>Encourage people for the progress they\u2019ve made rather than making them feel inadequate for not doing enough (\u201cdonating 1% to the best charities is amazing. You\u2019re doing more good than most people.\u201d)</li><li>Have high expectations but take (at least some) ownership for supporting people to get there (\u201cI know you can get a paper into NeurIPS. My job is to get you there.\u201d; \u201cI\u2019m not sure I explained that very well, can you try to say it back to me?\u201d)</li><li>Paraphrase the learner\u2019s position back to them before correcting any misconceptions (like \u201cyour saying that this is too sci-fi to be real. Is that right?\u201d)</li><li>Be careful how you spend your&nbsp;<a href=\"https://www.lesswrong.com/posts/wkuDgmpxwbu2M2k3w/you-have-a-set-amount-of-weirdness-points-spend-them-wisely\">weirdness points</a>: focus on the more relatable parts of your identity so you have enough social capital to push on the more important intuitions (like if your goal is longtermism for the general public, probably don\u2019t also bring up donating 10% and veganism at the same time; instead focused on shared appreciation for promoting education, democracy, and protecting the climate).</li><li>Tell stories of people, including yourself, who have embodied the values you\u2019re aspiring to cultivate (like&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/q7WwTuZQWMqDEEoWM/virtues-for-real-world-utilitarians\">altruism, compassion, truth-seeking, collective growth, determination</a>)</li></ol></li></ol><h1><strong>We need better models of community building so fewer people bounce off</strong></h1><p>Community building in EA is a precarious tightrope walk. We want to be truth-seeking but socially accepting. We want to challenge ideas without being adversarial, to push around ideas but not push around people. We want to be able to be avant garde while having many ideas adopted by mainstream institutions. We want to welcome weird thinking without looking so weird that promising people bounce off.</p><p>If you\u2019re like me, you\u2019ve seen many promising people bounce off. I\u2019ve seen committed, vegan medical students who are earning to give bounce off their EA groups. People made Jeremy<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefwfcbbuooyg\"><sup><a href=\"#fnwfcbbuooyg\">[3]</a></sup></span>&nbsp;feel he wasn\u2019t doing enough to help others.</p><p>I feel the tension. I want Jeremy to see if he fits direct work too. We want to help people who feel comfortable challenging their career plans. But, we also want value aligned people like Jeremy to feel like they belong. If we don\u2019t, then we won\u2019t be able to help them challenge those plans.</p><p>That EA group was doing all the right&nbsp;<i>things</i>. They were tabling, had a fellowship, were running intro events and doing 1:1s. But, I don\u2019t think they knew what it meant to do those things well. Some groups measure outcomes: \u2018people go from table to email list, and from email list to event, from event to fellowship\u2019. But, if we only focus on outcomes then it\u2019s hard to understand how to get better at the&nbsp;<i>process</i>. Why is Nikola better at tabling than Trevor? A better theory of change would help people know what they should be trying to improve. How do we create highly engaged EAs?</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/x0uplsfzpfyxvtbxxmui.jpg\"></p><p>If we don\u2019t know&nbsp;<i>why</i> some things work, how can we measure if it\u2019s working, and how do we choose what to prioritise, or what to change?</p><h2><strong>A \u2018highly engaged EA\u2019 is someone who has internalised the values and knowledge of the EA community, and is autonomously motivated to act on them</strong></h2><p>If we want people to make clear, impartial, and prosocial decisions\u2014like dedicating their time, their money, or their careers to the causes EAs care about\u2014it can require sacrifices and emotional strength. To do that, I think we need:</p><ol><li>People who have internalised the values of EA (including the value to challenge ideas)</li><li>People whose&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/LrpdPrfdEezK7xTYC/altruism-as-a-central-purpose\">life goals align</a> with the intrinsic values of EA (like wanting to have an impactful career, to build the community, do the most good through their donations)</li><li>People who are motivated to act on those values for their own reasons, not out of guilt or pressure or to seek status, and</li><li>People who feel the agency and confidence to take action on the basis of those values, rather than feeling paralysed by impending doom or intractable problems<ol><li>Ultimately, this likely leads to behaviours like choosing a career aligned with EA ideas, donating money to effective charities, and attending community events, but a \u2018highly engaged EA\u2019 doesn\u2019t have to do all of these.</li></ol></li></ol><p>There are well-supported psychological theories that explain how people internalise values, how they set goals, and how they develop motivation, confidence and agency. In this post I\u2019ll explain self-determination theory (or SDT) applied to these questions, and the data that supports SDT in application.</p><h2><strong>Self-determination theory is a very well supported model of how to develop motivation, agency, and confidence</strong></h2><p>SDT explains many motivational puzzles, and explains many controversies in EA. What happens if we guilt people with the drowning child? Should we encourage or discourage people from identifying as \u2018effective altruists\u2019?</p><p>Importantly, the evidence for SDT is compelling. Here\u2019s an abbreviated list of meta-analytic findings on SDT in practice. For reference, in psychology, a&nbsp;<a href=\"https://journals.sagepub.com/doi/10.1177/2515245919847202\">\u2018large\u2019 effect size</a> is a correlation of 0.3 (<i>r</i> = .3; about ~10% of the variance explained [<i>R\u00b2</i> = .10]; an intervention that increases an outcome by about 0.8 standard deviations [<i>d</i> = 0.8]). These are important for putting these findings in perspective. Any jargon used here I will explain in the following sections.</p><ul><li>The \u2018autonomous\u2019 or \u2018self-determined\u2019 types of motivation described in SDT robustly predict outcomes we care about. A&nbsp;<a href=\"https://journals.sagepub.com/doi/abs/10.1177/1745691620966789\">meta-analysis</a> of 223,209 students found self-determined forms of motivation have very large effects on engagement (<i>r</i> = .57), effort (<i>r</i> = .51), and enjoyment (<i>r</i> = .56). Effects on objectively measured academic performance are small but reliable (<i>r</i> = .1).</li><li>We can build those \u2018autonomous\u2019 types of motivation by supporting psychological needs. Another&nbsp;<a href=\"https://journals.sagepub.com/doi/10.3102/00346543211042426\">meta-analysis</a> of 79,000 students found that the level of \u2018psychological need support\u2019 explains about 30\u201340% of the variance in autonomous motivation. This is 3 to 4 times the size of a \u2018large\u2019 effect in psychology.</li><li>Educators can learn to support these psychological needs. An 10-year old&nbsp;<a href=\"https://link.springer.com/article/10.1007/s10648-010-9142-7\">meta-analysis</a> of 20 studies showed teachers can learn to become more supportive of psychological needs (<i>d</i> = 0.63) in 1\u20133 hours. In a&nbsp;<a href=\"https://journals.sagepub.com/doi/abs/10.3102/0034654315617832\">meta-analysis</a> of all interventions to improve motivation, those informed by SDT had the second biggest effect sizes (<i>d</i> = .7 from 11 studies; #1st was at <i>d</i> = .74 with only 4 studies on \u2018transformative experiences\u2019)</li><li>The same patterns show up in&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11031-018-9698-y\">leadership</a>,&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/26168470/\">health</a>, and&nbsp;<a href=\"https://psycnet.apa.org/record/2019-61785-001\">physical activity</a>. SDT interventions&nbsp;<a href=\"https://www.tandfonline.com/doi/full/10.1080/17437199.2020.1718529\">change how supported people feel</a>. Feeling supported makes people more&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/26168470/\">motivated, engaged, and happy</a>.<ul><li>For example, this graph from a&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11031-018-9698-y\">meta-analysis</a> of 32,780 employees shows SDT at work. Good leaders make their followers feel competent, connected, and in control. Those followers are then more motivated, happier, and perform better.<br><a href=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11031-018-9698-y/MediaObjects/11031_2018_9698_Fig3_HTML.gif?as=webp\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/fnc51bxum7av9idiankf.png\"></a></li></ul></li></ul><p>Understanding SDT helps us know what we\u2019re trying to do in community building. We\u2019re trying to support people\u2019s psychological needs as they learn how to improve the world. If we target those needs, and measure how well we're doing it, we can help people productively and happily do as much good as they can.</p><h2><strong>SDT describes how motivation varies not just in quantity, but in quality</strong></h2><p>When I was 18, I was on a date and a Hare Krishna gave me a book. Distracted by my teenage infatuation, I accepted the book and kept walking. In a classic \u2018persuasion\u2019 move (like from&nbsp;<a href=\"https://www.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/006124189X\">Cialdini's Influence</a>), they asked for a donation. I obviously paid it to avoid looking stingy in front of my date. I then tossed the book, saw the Hare Krishna pick it up, and repeat the ruse on someone else. This got the donation, but if we did this for the Against Malaria Foundation, why does it make us cringe? Isn\u2019t $10 dollars from a distracted man still life-saving bed-nets for people in need?</p><h3>Controlled types of motivation are weak, short-term, and fragile</h3><p>Many&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11266-022-00499-y\">donation-solicitation methods involve guilt and pressure to get a short-term outcome (see our meta-review here)</a>. We can pressure people into motivation (external regulation) or have them internalise beliefs that make them feel guilty (introjected motivation). Both of these&nbsp;<i>do</i> create&nbsp;<i>short term</i> behaviour change.</p><p><i>But,</i> these controlled forms of motivation are not what I think we\u2014as a community\u2014would hope for. They are why charity solicitation develops a bad name. They\u2019re \u2018controlled\u2019 because we feel motivated by a force that doesn\u2019t align with our values. It generally leads to short-term motivation, or even covert resistance.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/tvpkxoncyiyyemdlz5kh.png\"></p><p>That\u2019s not to say we should&nbsp;<i>never</i> do things that lead to guilt. Extrinsic forms of motivation can provide a strong impetus when it\u2019s&nbsp;<i>also</i> value aligned. People use&nbsp;<a href=\"https://www.beeminder.com/\">Beeminder</a> to provide extrinsic motivation that\u2019s aligned with what\u2019s important to them (e.g., to exercise). That\u2019s very different from the time my mum paid me $10 every time I lost a kilo of weight. Her incentive turned into an excellent money-spinner: I would lose a few kilos, buy a new video game, put the weight back on again, and the cycle would repeat. The problem here was the extrinsic motivations didn\u2019t match what was important to me.</p><h3>Autonomous motivations are the sustainable, value aligned motivation of highly engaged EAs</h3><p>So if extrinsic motivation is weak, what are the alternatives? Why do we play video games, music, go bouldering, do yoga, or donate to charity when there are often few extrinsic rewards?</p><p>It\u2019s because these are&nbsp;<i>autonomously motivating.</i> I<i>ntrinsic</i> motivation is a special type of autonomous motivation: it\u2019s where we\u2019re driven purely by the joy of the activity itself. This is more relevant for sport, music, art, or gaming, where people often find the activity&nbsp;<i>fun</i>. But, donating to charity isn\u2019t&nbsp;<i>fun.</i> Eating healthily and intense cardio aren\u2019t&nbsp;<i>fun</i>. These are driven by different types of motivation. We often do these things because they\u2019re aligned with our personal goals and values (known as&nbsp;<i>identified motivation</i>), or because they\u2019re part of who we want to be (known as&nbsp;<i>integrated motivation</i>).</p><p>These better explain why people, for example, might sustainably avoid eating animal products. It\u2019s not&nbsp;<i>fun</i> to have a less varied diet, and most people I know don\u2019t avoid animal products out of guilt or pressure. People do it because they care about animals and the planet, and want to minimise their impact on both (which is&nbsp;<i>identified motivation</i>). Or, they&nbsp;<i>identify</i> with being vegetarian or vegan; that is, it\u2019s part of their identity (which is&nbsp;<i>integrated motivation</i>). All of these \u2018autonomous\u2019 sources of motivation\u2014identified, integrated, and intrinsic\u2014are sustainable and distinct from extrinsic motivation because they mean the behaviour aligns with your values.</p><p>To promote highly engaged EAs, we want people driven to act by these autonomous sources of motivation. We want them to look for the best way to impartially do the most good (and take action on it) because that intellectual project aligns with their personal goals and values.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1jikv29qg6k\"><sup><a href=\"#fn1jikv29qg6k\">[4]</a></sup></span>&nbsp;This framing helps answer some of the controversies in EA already.</p><ol><li>Do we want people to experience guilt in response to the drowning child metaphor? Maybe, but only if the person already sees how EA aligns with their values. If they don\u2019t see that alignment, you might get short-term compliance or a hefty dose of resistance.</li><li>Do we want people to identify as \u2018an effective altruist\u2019? Yes, as long as that identity also includes the values of&nbsp;<a href=\"https://www.centreforeffectivealtruism.org/team-values\">truth seeking and a scientific mindset</a>. As outlined in&nbsp;<a href=\"https://www.amazon.com/Scout-Mindset-Perils-Defensive-Thinking/dp/0735217556\">Scout Mindset</a>,&nbsp;<a href=\"https://www.youtube.com/watch?v=NxqTOm3TzsY\">it\u2019s true that some identities do cloud our abilities to see the truth</a>, but most agree an&nbsp;<a href=\"https://psyarxiv.com/w52zm\">EA identity explicitly involves truth-seeking as a value</a>. If we discourage people from holding EA identities, we may be turning them away from a powerful source of sustainable motivation. Not using EA as an identity is like not using nuclear power. Yeah, sometimes it causes a meltdown, but most of the time it\u2019s safe, low carbon, base-load energy.</li></ol><p>Right, so autonomous motivation good, controlled motivations (generally) bad. But how do we, as a community, promote autonomous motivation?</p><h1><strong>We say \u2018educate, don\u2019t persuade\u2019, but what\u2019s the difference? Support people\u2019s psychological needs</strong></h1><p>I like the idea of \u2018educate, don\u2019t persuade.\u2019 It rings true to me. But what does it mean? Education and persuasion both involve one person communicating ideas or beliefs to another person. When is communicating about, say, factory farming \u2018education\u2019 and when is it \u2018persuasion\u2019?</p><p><a href=\"https://www.apa.org/ed/schools/science-persuasion/brochure.pdf\">Persuasion has a bad reputation</a> because it often fails to support people\u2019s psychological needs, like their freedom of choice. \u2018Educating\u2019 feels more supportive of these needs.</p><p>The three most commonly accepted basic psychological needs are:</p><ul><li>a need to feel like you can align your behaviour to what you value (the need for \u2018autonomy\u2019),</li><li>the need to feel like you have what you need to achieve your goals (the need for \u2018competence\u2019),</li><li>and the need to belong and feel connected to others (the need for \u2018relatedness\u2019).</li></ul><p>When we satisfy these needs, we motivate people over the long-term. When we don\u2019t we might get short-term compliance, but nothing sustainable.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/ytyskx6nwxudztfud92j.jpg\"></p><p>If you\u2019re having dinner with a friend and they ask you where you donate, you probably have a good feel for their values. If you talk about why you donate to the Long Term Future Fund, but given what you know about them, how they might prefer the&nbsp;<a href=\"https://funds.effectivealtruism.org/funds/global-development\">Global Health and Development fund</a> or&nbsp;<a href=\"https://www.givewell.org/maximum-impact-fund\">Top Charities Fund</a>, that feels more supportive. You\u2019re connecting with them, acknowledging what they value, and giving them a path to effectively achieve those goals.</p><p>If you instead say \u201canything you do in the present is probably meaningless over the long term, so you should only really donate to the Long Term Future Fund or you\u2019re neglecting to protect your great-grandchildren\u2026\u201d that feels more like bad \u2018persuasion\u2019. You\u2019re ignoring what they value, making them feel hopeless toward achieving their goals, and guilting them into doing what you value.</p><p>I don\u2019t think people in the community do the latter very often. However, consistently supporting psychological needs is a very difficult challenge with a mixed audience. That is, it\u2019s hard to appeal to people\u2019s values when their values might differ. It\u2019s hard to give all people the information they need to achieve their goals when their goals diverge. A place where you belong and feel connected might be one where I feel alienated.</p><p>We\u2019re more likely to be open to the views of others when we connect with them\u2014when we feel like we belong in their group. Both the source of the message and the message itself matters to whether or not we believe it. For example,&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S027249441830611X\">pro-environmental messages were only persuasive to conservatives when coming from a conservative source (even if the message was tailored to their values)</a>. I focused on autonomy first because I think value alignment is a better method of signalling in-group relationships than external cues (e.g., appearance). I don\u2019t think EAs should wear MAGA hats to reach republicans in the US. I think they\u2019re&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/WABgFWK3x5jK4CqcC/should-eas-be-more-welcoming-to-thoughtful-and-aligned\">better aligning on the benefits of market solutions</a>, or the desire to make government spending decisions cost-effective. We can make EA ideas helpful to many groups.</p><p>Still, EA won\u2019t be a supportive place for everyone. Most&nbsp;<a href=\"https://selfdeterminationtheory.org/wp-content/uploads/2016/12/2015_Martela_Ryan_J_Personality.pdf\">people want to do good</a>, and as&nbsp;<a href=\"https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/#transcript\">Hillary Graves</a> says:</p><blockquote><p><i>\u201cAll you really need is some component of your moral philosophy which acknowledges that making the world a better place is a worthwhile thing to do. And that\u2019s really all you need to get the effective altruist project off the ground.\u201d</i></p></blockquote><p>We could focus on this shared common ground, and helping people better live by their values. Instead, I\u2019d hazard we bounce people because many focus on the counterintuitive or demanding conclusions from EA (e.g., AI doom). Yes, we make normative arguments (e.g., \u2018it\u2019s better to do more good than less\u2019). But, we don\u2019t have to make normative arguments in a controlling way.</p><p>By being \u2018need supportive\u2019, we can help people feel understood, competent, and empowered. If they feel this way, they are more likely to listen to your ideas. It makes them more likely to take on new beliefs and values (e.g., a more impartial sense of altruism). Then, as they become more aligned, it\u2019s easier to support their psychological needs. It creates a virtuous cycle.</p><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/animations/50aca17ec7c19f17a8dbc178e1703ced7b0be0bd269d6ceb.gif\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/animations/50aca17ec7c19f17a8dbc178e1703ced7b0be0bd269d6ceb.gif/w_80 80w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/animations/50aca17ec7c19f17a8dbc178e1703ced7b0be0bd269d6ceb.gif/w_160 160w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/animations/50aca17ec7c19f17a8dbc178e1703ced7b0be0bd269d6ceb.gif/w_240 240w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/animations/50aca17ec7c19f17a8dbc178e1703ced7b0be0bd269d6ceb.gif/w_320 320w\"></figure><p>So, this post aims to outline some gaps between what research says on building long-term motivation, and what sometimes happens in EA events I\u2019ve been to.</p><h2><strong>Guilt and pressure are bad long-term motivators.</strong><br><strong>Use existing values instead.</strong></h2><p>Like many EAs, I found&nbsp;<a href=\"https://www.thelifeyoucansave.org.au/child-in-the-pond/\">Peter Singer\u2019s Drowning Child thought experiment</a> persuasive. It was certainly part of my initial drive toward donating to global poverty, and those donations were central to becoming an EA.</p><p>It also took me to a pretty dark place, where I felt I couldn\u2019t justify my life decisions or minor purchases, once converted into lives I could have saved. Some EA forum posts even suggested we use this as a persuasive appeal (see the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/RmTqKTkjiiPgRTWdL/please-use-art-to-convey-ea\">\u2018currency\u2019 described in this post</a>; I found the currency too unpalatable to repeat here). It took a few good years of therapy, supportive colleagues, and some quality writing from the EA community<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref917ch2stmkh\"><sup><a href=\"#fn917ch2stmkh\">[5]</a></sup></span>&nbsp;to get me out of that space into something more sustainable.</p><p>I also noticed the downing child metaphor got the hackles up in other people. Yes, I needed to be more tactful with some of them, but with others, they just found the conclusions too demanding to even entertain. I don\u2019t think my experiences here are unique. I think they align with the research around motivation.</p><p>The problems with guilt or fear appeals are two-fold:</p><ol><li>They rely upon the recipient feeling you have a shared set of values. In cases where that isn\u2019t explicit, or when change is hard, guilt- or fear-based appeals can lead to resistance, both to the messenger and the idea. For example:<ol><li>The&nbsp;<a href=\"https://www.bi.team/wp-content/uploads/2020/03/BIT_Report_A-Menu-for-Change_Webversion_2020.pdf.pdf\">Behavioural Insights Team\u2019s report on changing diets summarised a range of studies showing guilt-based appeals can backfire</a>, and pride and positivity appeals are more likely to work. For example,&nbsp;<a href=\"https://doi.org/10.1037/pspa0000149\">for most people, moralising around meat can backfire</a>: people often eat more meat in the month afterwards.</li><li>Interventions that target controlled motivations tend to not work for reducing criminal reoffending (see&nbsp;<a href=\"https://econtent.hogrefe.com/doi/10.1027/1016-9040/a000323\">this meta-review for effects across a range of interventions</a>;&nbsp;<a href=\"https://www.college.police.uk/research/crime-reduction-toolkit/scared-straight?InterventionID=2\">a classic example of this failure known to the EA community is the harm caused by Scared Straight</a> where shaming juvenile offenders led to higher rates of recidivism)</li><li>Our recent meta-analysis of 167 studies showed that\u2014across cross-sectional, longitudinal, and experimental designs\u2014<a href=\"https://psycnet.apa.org/record/2022-22515-003\">controlled motivation did not increase prosocial behaviour, but increased antisocial behaviour. Autonomy-supportive environments were what increased prosocial behaviour</a> (<a href=\"https://psyarxiv.com/e3dfw/\">preprint here</a>).</li></ol></li><li>Other meta-analyses have found that guilt and fear don\u2019t harm engagement or performance (see in&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11031-018-9698-y\">work</a> and&nbsp;<a href=\"https://journals.sagepub.com/doi/full/10.1177/1745691620966789\">education</a>). But, even if people work hard, I worry those forms of motivation may lead to mental health problems and burn-out in the community. For example:<ol><li>Obsessive passion is where someone is strongly committed to an activity but for controlled reasons (e.g., driven by guilt).&nbsp;<a href=\"https://link.springer.com/article/10.1007/s11031-015-9503-0\">A meta-analysis of 1,308 effect sizes showed obsessive passion was associated with higher burnout, negative affect, anxiety, rumination, and activity/life conflict</a>.</li><li><a href=\"https://journals.sagepub.com/doi/full/10.1177/1745691612447309\">A meta-analysis of a range of health behaviours found guilt-based motives do lead to positive health behaviours, but lead to negative mental health outcomes</a>. Similarly,&nbsp;<a href=\"https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0021466\">experiences of guilt and shame are strongly correlated with depressive symptoms (meta-analysis of 108 studies)</a>.</li></ol></li></ol><p>Whether we\u2019re trying to educate about the importance of the long-term future, the plight of factory farmed animals, or the opportunities to improve global health, there are better alternatives to guilt and fear.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6sh9dpjydh6\"><sup><a href=\"#fn6sh9dpjydh6\">[6]</a></sup></span>&nbsp;Many of the&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/WaHJD3Jz29n7QCJiz/what-are-the-key-claims-of-ea-1\">core claims of EA are intuitive to most people</a>, for example:</p><ul><li>all else being equal, we should do more good rather than less;</li><li>science and reason are good methods for identifying what works;</li><li>we\u2019ll make better decisions if we see the world as it is, rather than as we want it to be;</li><li>future generations matter;</li><li>animals can suffer;</li><li>luck determines where and when you were born;</li><li>it\u2019s good to help others;</li><li>it\u2019s better to help them with what they need rather than what is nice for us to give.</li></ul><p>EA provides some ideas that might be novel to many people, but aligning on the intuitive claims first can establish value-alignment first. Some of these might sound like&nbsp;<a href=\"https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights\">applause lights</a>, but they serve to align us with the audience. As pointed out by others,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/i6Hday6mudh5j4mZ4/ea-can-sound-less-weird-if-we-want-it-to\">EA can sound less weird, if we want it to</a>. We should&nbsp;<a href=\"https://www.lesswrong.com/posts/wkuDgmpxwbu2M2k3w/you-have-a-set-amount-of-weirdness-points-spend-them-wisely\">spend our weirdness points more wisely</a>. We can&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/mnGkj5aerHbfTdf7o/the-explanatory-obstacle-of-ea\">frame EA in ways that explain it accurately, clearly, and convincingly (e.g., Gidon Kadosh\u2019s piece)</a>.</p><h2><strong>Using values and avoiding guilt are just&nbsp;</strong><i><strong>part</strong></i><strong> of creating a \u2018need supportive environment\u2019</strong></h2><p>We had over&nbsp;<a href=\"https://psyarxiv.com/4vrym/\">30 world experts in self-determination theory agree on what makes a motivating learning environment.</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh52w3z5lbp\"><sup><a href=\"#fnh52w3z5lbp\">[7]</a></sup></span>&nbsp;They agreed on a range of strategies that would have moderate-to-strong improvements to motivation. Expert opinion is relatively&nbsp;<a href=\"https://en.wikipedia.org/wiki/Hierarchy_of_evidence\">weak evidence</a> but the goal here is to make more concrete what a \u2018need supportive\u2019 environment looks like. I\u2019ll focus below on the less obvious ones.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/xktuw6a2obqpzfqwfkf6.png\"></p><p>For more detail, I made a short video series talking about how to satisfy these psychological needs. I\u2019ll explain the key ideas in the rest of this post, but if you want more, you might find&nbsp;<a href=\"http://bit.ly/PEECtraining\">this playlist of 5 short videos (5 minutes each)</a> useful, or this one-page summary:</p><p><a href=\"http://bit.ly/PEECsummary\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/g4hsdmknotflnvymuguu.jpg\"></a></p><h3><strong>Educators that support autonomy tend to\u2026</strong></h3><ul><li>Provide rationales that connect to the learners\u2019 values<ul><li>For example, a clean-meat speaker starts with environmental and health benefits, because she knows&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S2666833521000976\">people are more likely to respond to these values than they are for animal welfare benefits (see our other meta-review)</a>,</li></ul></li><li>Explain the real problem in the world you\u2019re hoping to help the person solve, how they\u2019re going to learn to solve it, and how they\u2019ll know they\u2019ve learned it<ul><li>The technical term here is&nbsp;<a href=\"https://en.wikipedia.org/wiki/Constructive_alignment\">\u2018constructive alignment\u2019</a>.&nbsp;<br><img src=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_140 140w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_280 280w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_420 420w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_560 560w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_700 700w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_840 840w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_980 980w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_1120 1120w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_1260 1260w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6331140fa99523bc2318045f89b6c91ea34bc5af3b0e2830.png/w_1382 1382w\"><br>Basically, work backwards from real world problem to the learning objective(s) to the assessment(s) to the learning activity, for example:<ul><li>\u201cAs you\u2019ll learn in this program, AI is likely to become a powerful force in the world, and we don\u2019t yet know how to build it safely.\u201d (real world problem)</li><li>\u201cIn this program I want to help you identify whether you think you might fit a career in AI safety.\u201d (specific learning objective)</li><li>\u201cAt the end of the program, you\u2019ll have an opportunity to do a 4-week project to test your fit for this kind of work. I will help you craft something that fits your interests.\u201d (assessment task)</li><li>\u201cToday, to help prepare you for that project, we are going to\u2026\u201d (explain rationale for the learning activity)</li></ul></li><li>In EA education and outreach, these four components often do not align well. For example:<ul><li>We have fellowships that are almost exclusively discussions but projects (\u2018assessments\u2019) that are almost all writing. If I need to learn to write for the project, the learning activities should help me practice my writing with opportunities for feedback.</li><li>In an AI and Economics course I started, the content pivoted to the history of Claude Shannon and Information Theory. For me to stay motivated, I need to understand why learning about Information Theory (the learning activity) helps me solve economic problems around AI (the real world problem).</li></ul></li></ul></li><li>Use the language of choices, empathy, and values instead of \u2018shoulds\u2019 and \u2018musts\u2019<ul><li>It\u2019s okay to make normative and scientific claims, but the language can be more or less autonomy supportive. There\u2019s a big difference between a doctor showing empathic concern about a client\u2019s drug use and\u2026<img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/jd7ymmhmyv5a7oum5qi8.jpg\"></li></ul></li><li>People really don\u2019t like feeling judged and pressured. So, instead of\u2026<ul><li>\u201cDon\u2019t you care about animals?\u201d</li><li>\u201cYou should really think about another career.\u201d</li><li>\u201cIf you don\u2019t donate,&nbsp;<a href=\"https://www.thelifeyoucansave.org.au/the-book/\">you\u2019re doing something wrong.</a>\u201d</li></ul></li><li>Try\u2026<ul><li>Talking about what others do: \u201cMany people I know have made small changes that make a big difference for animals.\u201d</li><li>Using if-then rationales: \u201cIf you want to have an impact with your career, 80k has some great resources.\u201d</li><li>Preemptively empathising: \u201cSome people find Singer\u2019s argument pretty demanding. What did you think?\u201d</li></ul></li></ul><h3><strong>To build a sense of belonging and relatedness, educators can\u2026</strong></h3><ul><li><a href=\"https://www.centreforeffectivealtruism.org/ceas-guiding-principles\">Follow CEA\u2019s guiding principles (e.g., commitment to others, openness, integrity, and collaborative spirit)</a></li><li>Explicitly showing empathy to people in a range of ways, like:<ul><li>In their current feelings or beliefs, even if you differ: \u201cSo you think there\u2019s basically no chance of humans ever going extinct. Yeah I know that can be hard to imagine.\u201d (<a href=\"https://rationalwiki.org/wiki/Rapoport%27s_Rules\">see also Rapoport\u2019s Rules</a>) or</li><li>In their misconceptions: \u201cI used to think the top charity might be 2x the average charity, as well\u2026\u201d</li></ul></li><li>Being expressive and enthusiastic ourselves<ul><li>Role-modelling the kind of behaviour we want to see in the group is an important component of transformational leadership (i.e., \u2018idealised influence\u2019), which is one of the most well-established models of leadership for producing the outcomes we care about (<a href=\"https://journals.sagepub.com/doi/full/10.1177/0149206316665461\">see this meta-analysis</a>)</li></ul></li><li>Help people cooperate, share, and interact<ul><li>Creating shared tasks where people work together to produce something valuable (e.g., rather than only using discussion prompts, I had my&nbsp;<a href=\"https://www.eacambridge.org/agi-safety-fundamentals\">AGI Safety Fundamentals</a> cohort&nbsp;<a href=\"https://miro.com/app/board/uXjVOO2CuD0=/?moveToWidget=3458764518474433217&amp;cot=14\">work together on creating a concept map of what they\u2019d learned so far and how it all fit together</a>)</li></ul></li><li>Create environments where people are likely to form friendships, not&nbsp;<i>just</i> learn (<a href=\"https://forum.effectivealtruism.org/posts/woXZTysgpe2GPwhoM/community-builders-should-focus-more-on-supporting\">e.g., see recent forum post here</a>), by providing opportunities for either unstructured (e.g., food afterward) or structured social interaction (e.g., some activities purely for fun, or the [sometimes dreaded] ice-breaker)</li></ul><h3><strong>And to make people feel competent, we can\u2026</strong></h3><p>Use the hoards of meta-analyses on what improves learning. There are dozens of these that are relevant to community builders. I\u2019ll turn to those now.</p><h1><strong>If educate is the goal, use evidence-based education strategies</strong></h1><p>There are many repositories of evidence-based teaching strategies, most of them targeting school children (e.g.,&nbsp;<a href=\"https://visible-learning.org/\">Visible Learning</a>,&nbsp;<a href=\"https://educationendowmentfoundation.org.uk/education-evidence/teaching-learning-toolkit\">the Educational Endowment Foundation,</a> and&nbsp;<a href=\"https://evidenceforlearning.org.au/the-toolkits/the-teaching-and-learning-toolkit/\">Evidence for Learning</a>). Adult learning can look and feel different from school classrooms. As a result, we\u2019ve created&nbsp;<a href=\"https://www.inspiretoolkit.com.au/evidence-toolkit\">a toolkit summarising the meta-analytic evidence for what works in adult learning (usually universities) here</a>.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefdx2389nn8lk\"><sup><a href=\"#fndx2389nn8lk\">[8]</a></sup></span>&nbsp;See these links if you want to know <a href=\"https://www.youtube.com/watch?v=xwWQ7JabeIE\">why meta-analyses are important for teaching</a>, how <a href=\"https://www.inspiretoolkit.com.au/about-inspire\">we tried to make them more accessible</a>, and <a href=\"https://www.inspiretoolkit.com.au/training\">how to use them in your teaching</a>.<a href=\"https://inspiretoolkit.com.au\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/l7blxikf6vuvnmlnanqb.png\"></a></p><p>These strategies are not all common sense: many educators use strategies that have been shown to&nbsp;<i>reduce</i> learning. For example, I still hear people espousing so-called \u2018facts\u2019 about learning styles (e.g., \u2018I\u2019m tailoring this to visual learners\u2019).&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/learning-styles\">Learning styles don\u2019t work</a>\u2014instead use&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/modality-and-multimedia\">speech for words and images alongside them, wherever possible</a>. Make <a href=\"https://www.inspiretoolkit.com.au/articles/spoken-language\">the language as simple as you can,</a> because you\u2019re probably&nbsp;<a href=\"https://www.youtube.com/playlist?list=PLvSeo4-B-agV3yExA_jNderX6g5_BsP17\">plagued with a curse of knowledge</a>. That means\u2014as is often recommended on the forum\u2014that we should avoid using jargon (see&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/uGt5HfRTYi9xwF6i8/3-suggestions-about-jargon-in-ea\">Michael Aird</a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/Ja5akrpNsTqwSQ97W/the-case-for-reducing-ea-jargon-and-how-to-do-it\">Akash Wasil</a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/F5YXWJt2rXo6ESzDQ/when-you-shouldn-t-use-ea-jargon-and-how-to-avoid-it\">Rob Wiblin</a>) unless the goal of the session is to learn that jargon (e.g., \u2018instrumental convergence\u2019 in the AI Safety Fundamentals course) or we know everyone has covered that jargon (e.g., later weeks of that course).</p><p>More controversially, meta-analyses show that&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/seductive-details\">too many jokes and memes distract an audience</a>, so they\u2019re more likely to remember the joke than what you\u2019re trying to teach. This is controversial because, you know, funny stuff is funny. For example, I love&nbsp;<a href=\"https://www.youtube.com/user/lastweektonight\">Last Week Tonight</a>, or&nbsp;<a href=\"https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg\">Robert Miles\u2019 AI safety videos</a>. Their jokes make things&nbsp; more engaging\u2014but I do often notice myself attending to a meme and forgetting what he was saying. I obviously don\u2019t think we should eschew humour completely\u2014I still add terrible jokes to&nbsp;<a href=\"http://bit.ly/ScissorsPaperRockTraining\">my videos</a>\u2014but we need to be careful to make the jokes relevant to the learning objectives, so the learning still comes through.</p><p>Other\u2014possibly obvious but often neglected\u2014evidence based strategies include:</p><ul><li>Make things interactive<ul><li>We usually only realise what we know and what we missed when we try to explain our thinking, or use our knowledge to solve problems.&nbsp;<a href=\"https://psycnet.apa.org/record/2014-45290-002\">Learning is roughly proportional to the amount of interactivity you create</a>:<ul><li>Passive listening and reading leads to the lowest learning</li><li>Highlighting and taking notes is better (active listening)</li><li>Answering questions and solving problems is even better (constructive learning)</li><li>Collaborating with others means you can fill their knowledge gaps, and they can fill yours (interactive learning)</li></ul></li></ul></li><li>Provide gradually increasing, self-referenced challenges, instead of one challenge for everyone<ul><li><a href=\"https://www.inspiretoolkit.com.au/articles/scaffolding\">\u2018Scaffolding\u2019 is an effective way of doing this, where learners solve easier problems before being challenged by more difficult ones</a>. This is generally obvious: start with&nbsp;<a href=\"https://theprecipice.com/\">The Precipice</a> instead of&nbsp;<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">Greaves and MacAskill\u2019s working paper on Axiological and Deontic Strong Longtermism</a>. But, for developing skills, the community is not always great at scaffolding. I enjoyed the&nbsp;<a href=\"https://www.eacambridge.org/agi-safety-fundamentals\">AGISF</a> but the independent project \u2018snuck up\u2019 on many of my colleagues. Some got overwhelmed and stopped. If we want people to eventually be able to do an independent project around AI Safety, then the learning design might want to give people fewer discussions, and more group activities that look like \u2018mini projects\u2019</li></ul></li><li>Provide encouraging and constructive feedback on how they\u2019re going<ul><li>Discussions can be good if they encourage people to voice their understanding of a topic, and then get feedback on how well they\u2019ve understood it. The problem is that\u2014often\u2014many people in the group don\u2019t get any feedback on their understanding. If you speak last, you might not be able to really check your knowledge if everyone has spoken before you.<ul><li>Instead, ensure each person gets&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/feedback\">fast and reliable feedback</a>. One of the best reasons for doing group, hands-on activities is that it\u2019s easier to give feedback on what people are learning. Give each person an opportunity to show what they know&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/feedback\">so you can supportively give fast, accurate feedback</a> on how well they know it.</li></ul></li></ul></li><li>Related to constructive alignment (i.e., working backward from the skill to the assessment to the learning activity, <a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#Educators_that_support_autonomy_tend_to_\">see&nbsp;above</a>), one of the most robust findings in evidence-based teaching is the&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/simulation\">benefit of so-called \u2018simulation\u2019</a>: basically, have people practise the thing you want them to eventually do. If you want people to be able to facilitate a discussion about EA, then don\u2019t have them read a forum post\u2014they\u2019ll learn the best if you have them actually facilitate a real session, facilitate a pretend session, or have them watch a real session and make decisions about how they would respond. With all of these, though, remember the key mechanism is fast and faithful feedback.</li><li><a href=\"https://www.inspiretoolkit.com.au/articles/concept-maps\">Have learners build their own concept maps</a> (either individually or in a group) helps them consolidate their knowledge and flag areas of disagreement.</li><li><a href=\"https://www.inspiretoolkit.com.au/articles/problem-based-learning\">Design case studies with problems for people to work through</a> (i.e., problem based learning)</li><li>Most of these strategies can be used in group, collaborative settings, which not only&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/ZPNNnEu2HGNSNmifo/we-re-all-teachers-learn-to-do-it-better#To_build_a_sense_of_belonging_and_relatedness__educators_can_\">builds connections</a>, but&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/group-collaboration\">group collaboration is one of the best things we know to improve learning</a></li><li>Don\u2019t make projects or \u2018assessments\u2019 collaborative. The opportunity to let others do the work (so called \u2018social loafing\u2019) is too tempting for many to resist. As a result,&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/group-assessments\">group assessments lead to less learning than individual ones</a>, and there\u2019s no good data that they lead to increased \u2018soft skills\u2019 like \u2018communication\u2019. My experience is that those soft skills need to be taught explicitly and deliberately practised.</li><li>If it makes sense in your setting, then&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/quizzes\">use quizzes to help provide people with feedback on their learning</a>. In a perfect world, some <a href=\"https://www.inspiretoolkit.com.au/articles/gamification\">gamification of the course can make it more engaging</a>, but is usually costly and hard to implement. Still, there are&nbsp;<a href=\"https://www.inspiretoolkit.com.au/articles/technology\">lots of educational technologies that improve learning</a>, especially if they make it easier to promote the learning mechanisms described above (e.g.,&nbsp;<a href=\"https://h5p.org/\">better, faster feedback via H5P</a>;&nbsp;<a href=\"https://miro.com/\">better group collaboration via Miro</a>; better simulation of the real-world task like <a href=\"https://swirlstats.com/\">learning statistical analyses in R using Swirl</a>)</li></ul><h1><strong>Learn how to use multimedia so it doesn\u2019t overload your audience</strong></h1><p>Even if you\u2019re not making curricula or designing virtual programs, you\u2019re probably trying to communicate your ideas publicly. You might be writing on the forum, in which case&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/dAbs7w4J4iNm89DjP/why-fun-writing-can-save-lives-the-case-for-it-being-high\">learning to write in a clear and compelling way is going to increase your impact (HT Kat Woods)</a>. But writing is an inefficient mechanism for communication. Our brains are designed to&nbsp;<i>hear</i> words<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreflzdcxxovi9m\"><sup><a href=\"#fnlzdcxxovi9m\">[9]</a></sup></span>&nbsp;and&nbsp;<i>see</i> pictures (which is why&nbsp;<a href=\"https://journals.sagepub.com/doi/abs/10.3102/0034654321990713\">videos work better than many other forms of teaching and communication</a>). There are a handful of&nbsp;<a href=\"https://journals.sagepub.com/doi/abs/10.3102/00346543211052329\">strategies shown to improve learning from multimedia</a> from hundreds of experiments (see our&nbsp;<a href=\"https://journals.sagepub.com/doi/abs/10.3102/00346543211052329\">meta-review from last year</a>). I\u2019m going to practise what I preach here and direct you to one of three multimedia methods for learning these principles. The best is&nbsp;<a href=\"http://bit.ly/ScissorsPaperRockTraining\">this video playlist (five videos, roughly five minutes each)</a> that walks through how to make multimedia that works. If you\u2019re strapped for time, this&nbsp;<a href=\"https://bit.ly/reduceoverload\">twitter thread covers all the evidence-based strategies</a>. If you don\u2019t want to leave this post, this one-page summary covers most of the key ideas.</p><p><a href=\"http://bit.ly/SPRsummary\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995500/mirroredImages/ZPNNnEu2HGNSNmifo/h9xyxacbgfaeahc8xhxm.jpg\"></a></p><p>If possible, try to use a narrative arc that makes the <a href=\"https://denpeakacademy.com/2017/06/20/peak-book-summary-made-to-stick-by-chip-heath-and-dan-heath/\">problem concrete, hits an emotion, has something unexpected, then a concrete resolution.</a> My adaption of Chip and Dan Health's excellent book (<a href=\"https://denpeakacademy.com/2017/06/20/peak-book-summary-made-to-stick-by-chip-heath-and-dan-heath/\">'Made to stick'</a>; my <a href=\"https://www.youtube.com/watch?v=O-yb71xl-DA\">video summary here</a>) involves the following five steps:</p><ol><li>Problem with emotion: who's the hero? What problem do they face? How did they feel?</li><li>Stuck point with misconception: where did they get stuck? What unhelpful belief kept them stuck?</li><li>Empathy then refutation of stuck point or misconception, made by someone credible: Show you understand why people have the refutation. Then, explain why it's wrong.</li><li>Principle for next steps, using an analogy: what should the hero believe instead? Ideally, provide an analogy to the solution.</li><li>Solution to the original problem, with a new and positive emotion: how did the hero win using the new belief? How did they feel afterwards?</li></ol><p>Zan Saeri used some of my old video scripts to turn this into a <a href=\"https://beta.openai.com/playground/p/GqjDYpPOxU3p6Q9UBUtm7mpV?model=text-davinci-002\">GPT-3 preset</a>. Have fun.</p><h1><strong>In summary, we\u2019re all teachers.</strong><br><strong>Learn to do it well.</strong></h1><p>So much of community and field building involves what great teachers do: we share important ideas, make a motivating environment to explore the ideas, and create activities that help us engage with them. It\u2019s not the only thing, of course, but by trying some of the ideas in this post, you can make this part of your community building more effective and evidence based. In doing so, we can better grow the community of people who are committed to EA.</p><h3><strong>If you want to study how to do this better, reach out.</strong></h3><p><a href=\"https://noetel.com.au/projects/\">I'm an academic</a> in the School of Psychology at the University of Queensland. I think building the EA, rationalist, x-risk and longtermist communities are very important pathways for doing a huge amount of good. If you're interested in this and want to do research or run projects in this space, don't be afraid to reach out via noetel at gmail dot com</p><h3><strong>Thanks to the following people who provided constructive feedback on this post.</strong></h3><p>Alexander Saeri, Peter Slattery, Emily Grundy, Yi-Yang Chua, Jamie Bernardi, and Sebastian Schmidt were all autonomy supportive teachers that helped me improve this post. Mistakes are my own.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fne6toyvcwnf6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefe6toyvcwnf6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I\u2019m acutely aware of the irony that a forum post including prohibitions against jargon contains jargon as the first two words.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnxlx4w02jlmm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefxlx4w02jlmm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Per Ivar Friborg has also published a great summary of SDT for EA&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BoQZhKs26BdyQHpte/how-to-incubate-self-driven-individuals-for-leaders-and\">here</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnwfcbbuooyg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefwfcbbuooyg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Gender and name randomised for anonymity</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1jikv29qg6k\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1jikv29qg6k\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This aligns well with&nbsp;<a href=\"https://lukemuehlhauser.com/effective-altruism-as-i-see-it/\">Luke Muehlhouser\u2019s conceptualisation of Effective Altruism as a project that some people are excited about, but others aren\u2019t</a>, and I think that difference is the degree of alignment between the project and their own values</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn917ch2stmkh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref917ch2stmkh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There were too many forum and blogs posts to name each one that helped me become more sustainably motivated, but if I had to choose one, it\u2019d be&nbsp;<a href=\"https://juliawise.net/\">Julia Wise</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6sh9dpjydh6\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6sh9dpjydh6\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I\u2019m obviously not the first person to suggest we should&nbsp;<a href=\"https://replacingguilt.com/\">replace guilt (e.g., Nate Soares)</a></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh52w3z5lbp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh52w3z5lbp\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://psyarxiv.com/4vrym/\">The experts also agreed on mistakes some exhausted, defeated teachers might make, like yelling at students</a>, but I didn\u2019t include them here because I can\u2019t imagine an EA community builder making the same mistakes.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fndx2389nn8lk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefdx2389nn8lk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>We haven\u2019t finished porting INSPIRE from our <a href=\"https://inspireprogram.notion.site/Evidence-Toolkits-9f16be2254a54f95b6698beee7e436a2\">beta version</a> to the <a href=\"https://www.inspiretoolkit.com.au/\">proper website</a>, so I actually prefer the user experience on&nbsp;<a href=\"https://inspireprogram.notion.site/Evidence-Toolkits-9f16be2254a54f95b6698beee7e436a2\">this version on Notion</a>. Still, I\u2019ve linked to the new website so this post is more likely to be evergreen.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnlzdcxxovi9m\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreflzdcxxovi9m\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Another hat tip to <a href=\"https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library\">Kat and Nonlinear for the Nonlinear Library</a>. Not only are podcast versions of the forum more practical for many people, we\u2019re likely to learn just as well, if not better (assuming the posts don\u2019t have many images that we\u2019d miss). The&nbsp;<a href=\"https://sscpodcast.libsyn.com/\">Astral Codex Podcast</a> does an amazing job of this where Solenoid Entity explains the graphs in exquisite detail, and has them show up on your podcast player (\ud83e\uddd1\u200d\ud83c\udf73\ud83d\ude19\ud83e\udd0c)</p></div></li></ol>", "user": {"username": "mnoetel"}}, {"_id": "zDcGWSqwYC6qAR5A4", "title": "Giving What We Can September Newsletter", "postedAt": "2022-09-29T23:13:57.037Z", "htmlBody": "<p>Welcome to our September Newsletter!</p><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/j-b_xs6FklI\"><div><iframe src=\"https://www.youtube.com/embed/j-b_xs6FklI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><strong>Thank you for being a part of our growing community.</strong></p><p>Every month, our community grows and it warms our hearts. Since the 1st of September, we\u2019ve had 186 people take a new pledge with us and over 4000 people make a donation!</p><p>When someone takes a pledge with us, they are asked to write down what motivated them to pledge, and we wanted to share a few of our favourites from September:</p><ul><li>\u201c... because I want to help other people and improve the quality of life on earth.\u201d</li><li>\u201cI consider it every able person\u2019s duty to do what they can to leave the world a better place for all.\u201d</li><li>\u201cI want to dedicate my life to promoting the happiness and well-being of other beings.\u201d</li><li>\u201cMy main motivation in life is to minimize suffering. It is the value I hold the highest.\u201d</li><li>\u201cI want to do more good, and I know I can.\u201d</li></ul><p>Knowing that there are thousands of people who are taking lifelong action to help others, especially those they will never meet, is a wonderful thing.<br>&nbsp;</p><p><strong>Get us to talk at your workplace or community group!</strong></p><p>Many people start thinking about how to give back towards the end of the year. If you\u2019d like us to host a talk or workshop at your workplace or with your community group about effective giving over the coming months,<a href=\"https://forms.gle/9gEotHfggNxHCqCEA\"> please fill in this form</a>! We are excited to share the ideas of effective giving with new people and have found talks and workshops to be impactful.</p><p>&nbsp;</p><p>There\u2019s lots of interesting news, content from Giving What We Can and events to attend in the rest of our newsletter down below!</p><p>&nbsp;</p><p>Until next time, keep doing good!</p><p>-Grace Adams &amp; the rest of the Giving What We Can team</p><p>&nbsp;</p><p><img src=\"https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format\" srcset=\"https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=640&amp;q=75&amp;fit=clip&amp;auto=format 640w, https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=750&amp;q=75&amp;fit=clip&amp;auto=format 750w, https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=828&amp;q=75&amp;fit=clip&amp;auto=format 828w, https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1080w, https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1200w, https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=1920&amp;q=75&amp;fit=clip&amp;auto=format 1920w, https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2048w, https://cdn.sanity.io/images/4rsg7ofo/production/05f8198ebb1994abc57b69328d661417f220d6b5-694x747.jpg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 3840w\"></p><p>Many of our Giving What We Can Pledge members have received their Pledge Pins and have been wearing them out and about. Here\u2019s one of our early members Jos\u00e9 Oliveira wearing his pin!</p><h2>Attend An Event</h2><p><strong>Meetups</strong><br>&nbsp;</p><p>This month, join us for a discussion on our co-founder Will MacAskill\u2019s new book, <i>What We Owe The Future</i>. Bring your questions and thoughts on the book and chat with other community members.</p><p>(Americas/Oceania)</p><ul><li><a href=\"https://savvytime.com/converter/utc-to-australia-sydney-united-kingdom-london-ny-new-york-city-india-mumbai-singapore-singapore-germany-munich-ca-san-francisco-new-zealand-auckland-israel-tel-aviv/oct-08-2022/10-00pm\">Oct. 8: 22:00 UTC</a> (New York: 6:00 pm; San Francisco: 3:00 pm, Sydney: Sun, Oct. 9, 9:00 am, Auckland: Sun, Oct. 9, 11:00 am)</li><li><a href=\"https://www.facebook.com/events/1066468280961785/\">RSVP on Facebook</a></li><li><a href=\"https://us02web.zoom.us/meeting/register/tZAocOGsqjsoHtw3MuBPqVYaT6FfrL2ZxK_n\">Register</a></li></ul><p>(Europe/Asia)</p><ul><li><a href=\"https://savvytime.com/converter/utc-to-australia-sydney-united-kingdom-london-ny-new-york-city-india-mumbai-singapore-singapore-germany-munich-ca-san-francisco-new-zealand-auckland-israel-tel-aviv/oct-09-2022/8-30am\">Oct. 9: 8:30 UTC</a> (London: 9:30 am, Munich: 10:30 am, Mumbai: 2:00 pm, Singapore: 4:30 pm)</li><li><a href=\"https://www.facebook.com/events/424002822700063/\">RSVP on Facebook</a></li><li><a href=\"https://us02web.zoom.us/meeting/register/tZApceCoqz8qGt1NOj4vBZ0qiLhvnsgnBoGA\">Register</a></li></ul><p>Open Forum</p><p>Our open forum is an event where you can come along with questions about effective giving and/or to meet others interested in effective giving. This event alternates between Europe/Asia and Americas/Oceania each month.<br>&nbsp;</p><p>Next Open Forum (Europe/Asia)</p><ul><li><a href=\"https://savvytime.com/converter/utc-to-australia-sydney-united-kingdom-london-ny-new-york-city-india-mumbai-singapore-singapore-germany-munich-ca-san-francisco-new-zealand-auckland-israel-tel-aviv/oct-17-2022/11-00am\">Oct. 17: 11:00 UTC</a> (London: 12:00 pm, Munich: 1:00 pm, Mumbai: 4:30 pm, Singapore: 7:00 pm)</li><li><a href=\"https://www.facebook.com/events/1460044384446479/\">RSVP on Facebook</a></li><li><a href=\"https://us02web.zoom.us/meeting/register/tZ0ofuygqjguE9XA-p5PkVilf7SlyuEeJ6S9\">Register</a><br>&nbsp;</li></ul><h2>New content from Giving What We Can</h2><h3>Blog</h3><ul><li><a href=\"https://www.givingwhatwecan.org/blog/should-charity-begin-at-home\">Should charity begin at home?</a> - Alana Horowitz Friedman, contributing writer</li><li>Member profile: <a href=\"https://www.givingwhatwecan.org/blog/member-profile-fernando-martin-gullans\">Fernando Martin-Gullans</a></li><li><a href=\"https://www.givingwhatwecan.org/blog/announcement-renaming-of-legal-entity-to-effective-ventures-foundation\">Announcement: Renaming of legal entity to Effective Ventures Foundation</a> - Giving What We Can team</li><li><a href=\"https://www.givingwhatwecan.org/blog/rutger-bregman-on-effective-giving\">Rutger Bregman on Effective Giving: Highlights from an Interview with Effektiv Spenden</a> - Grace Adams, Head of Marketing</li><li><a href=\"https://www.givingwhatwecan.org/blog/what-is-counterfactual-thinking-and-why-should-you-care-about-it\">What is counterfactual thinking and why should you care about it?</a> - Alana Horowitz Friedman, contributing writer</li></ul><h3>YouTube</h3><ul><li><a href=\"https://youtu.be/hYm443i3lb8\">You can prevent animal suffering. Here\u2019s how.</a> - <a href=\"https://www.youtube.com/playlist?list=PLT88QiptgOaJBj0E-XrJFsjnQcYRhpkeY\">Giving Effectively</a> series</li><li><a href=\"https://youtu.be/tb4KlQHC3tU\">Zachary Brown</a> shares his giving story - <a href=\"https://www.youtube.com/playlist?list=PLT88QiptgOaLYdVhB7OnmjZbJnp1Gm3YI\">People Who Give Effectively</a> series</li><li><a href=\"https://www.youtube.com/watch?v=efVh51hbRHY&amp;feature=youtu.be\">Don\u2019t make these 10 mistakes when trying to improve the world</a>: Dr. Michael Noetel, High Performance Psychologist - <a href=\"https://www.youtube.com/watch?v=WyprXhvGVYk&amp;list=PLT88QiptgOaLGaq5J_1w7dZcwWL6F9P4N\">Effective Altruism</a> series</li><li><a href=\"https://www.youtube.com/watch?v=TS28DFakkik&amp;feature=youtu.be\">Using research &amp; strategic thinking to help animals effectively</a>: Interview with Neil Dullaghan, Senior Researcher at Rethink Priorities - <a href=\"https://www.youtube.com/watch?v=pK5b2fGKrU8&amp;list=PLT88QiptgOaK2tU5Cxaw9O8iXU30qSOih\">Podcast</a> series</li></ul><h3>Podcast</h3><ul><li>Find audio-only versions of new YouTube content on the <a href=\"https://givingwhatwecan.podbean.com/\">Giving What We Can podcast</a>!<ul><li>Lesson: <a href=\"https://givingwhatwecan.podbean.com/e/lesson-you-can-prevent-animal-suffering-here-s-how/\">You can prevent animal suffering. Here\u2019s how</a>.</li><li>Member story: <a href=\"https://givingwhatwecan.podbean.com/e/member-story-zachary-brown/\">Zachary Brown</a></li><li>Dr. Michael Noetel: <a href=\"https://givingwhatwecan.podbean.com/e/lesson-don-t-make-these-10-mistakes-when-trying-to-improve-the-world-from-dr-michael-noetel/?token=f733d51a2254e76d9bcb3a949724414d\">Don\u2019t make these 10 mistakes when trying to improve the world</a></li><li>Neil Dullaghan: <a href=\"https://givingwhatwecan.podbean.com/e/10-neil-dullaghan-using-research-strategic-thinking-to-help-animals-effectively/\">Using research &amp; strategic thinking to help animals effectively</a></li></ul></li></ul><h2>News &amp; Updates</h2><h3>Effective altruism community</h3><ul><li><a href=\"https://www.vox.com/future-perfect/2022/8/24/23318033/effective-altruism-longtermism-givewell-will-macaskill\">Caring about the future doesn\u2019t mean ignoring the present</a>: Kelsey Piper argues that perceived \u201ctrade-offs' ' between longtermism and present-day global health interventions are blown out of proportion.</li><li>Oxford\u2019s Andreas Mogensen, an effective altruist who rejects consequentialism,&nbsp; makes a deontological case for effective giving (and the GWWC pledge!) in this <a href=\"https://80000hours.org/podcast/episodes/andreas-mogensen-deontology-and-effective-altruism/\">80,000 Hours podcast episode</a>.</li><li><a href=\"https://eaforjews.org/\">EA for Jews</a> is launching an <a href=\"https://eaforjews.org/take-action/fellowship/\">8-week virtual fellowship program</a> starting the week of October 9th \u201cthat explores the core ideas of effective altruism and their relation to Jewish tradition, texts, culture, and history.\u201d <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScvjnc9qS4vmMEHqWasmMlYq9a59BB2Zd3vB3MdgVL8scnaAg/viewform\">Apply</a> by September 29th.</li><li>Easy EMDR, a company offering \u201call-in-one solutions\u201d for mental health practitioners specialising in trauma, has published a blog post \u2014 <a href=\"https://www.easy-emdr.com/effective-altruism.html\">Fighting Global Trauma with Effective Altruism</a> \u2014 about its commitment to effective giving.</li><li><a href=\"https://www.highimpactprofessionals.org/\">High Impact Professionals</a> (HIP) has recently shown how running a corporate fundraising campaign at your workplace can be a <a href=\"https://forum.effectivealtruism.org/posts/bsqtndpfKbcbQRFEs/fundraising-campaigns-at-your-organization-a-reliable-path\">highly effective way to multiply your impact</a>. HIP has also published a <a href=\"https://forum.effectivealtruism.org/posts/sE7Y43JRQARAKzZ6k/key-factors-for-success-in-organizing-a-fundraising-campaign\">list of key factors</a> to organising a successful campaign.</li><li>This EA <a href=\"https://forum.effectivealtruism.org/posts/nhbeKbwMgFKfrzLNb/marketing-messages-trial-for-gwwc-giving-guide-campaign\">forum post</a> summarises results from and reasoning behind GWWC and EA Market Testing Team\u2019s exploratory marketing messages trial, which aimed to compare the effectiveness of different messaging approaches during GWWC\u2019s Giving Guide campaign.</li><li><a href=\"https://www.youtube.com/watch?v=H9fIC0I6Rvk\">What we\u2019re learning about spreading EA ideas</a>: GWWC Head of Marketing Grace Adams speaks at EAGxOxford.</li><li>Our very own Luke Freeman (GWWC Executive Director) has written <a href=\"https://forum.effectivealtruism.org/posts/5Xio8uECTH3GqYARc/we-re-still-extremely-funding-constrained-but-don-t-let-fear\">a poem (and post-script)</a> about funding constraints within effective altruism.</li></ul><h3>Evaluators, grantmakers and incubators</h3><ul><li>GiveWell\u2019s (formerly-named) \u201cMaximum Impact Fund\u201d will now be called the \u201cTop Charities Fund\u201d; GiveWell explains the name change in this <a href=\"https://blog.givewell.org/2022/09/08/renaming-top-charities-fund/\">blog post</a>.</li><li>GiveWell is launching a <a href=\"https://www.givewell.org/research/change-our-mind-contest\">\u201cChange Our Mind\u201d contest</a> to solicit critiques of its cost-effectiveness analyses that could improve its allocations. Entries are due by October 31st.</li><li>Animal Charity Evaluators <a href=\"https://animalcharityevaluators.org/blog/announcing-our-new-executive-director/\">welcomes Stien van der Ploeg as its new Executive Director</a>; she will assume the role in early October.</li><li>Open Philanthropy has <a href=\"https://forum.effectivealtruism.org/posts/XBHx9zhAtkiBJnZNu/cause-exploration-prizes-announcing-our-prizes-1\">announced</a> the winning submissions to its <a href=\"https://www.causeexplorationprizes.com/\">Cause Exploration Prizes</a> process, which solicited suggestions for new cause areas to support and awarded prizes to particularly compelling entries.</li><li>Founders Pledge has published an article about its approach to comparing funding opportunities in global health and wellbeing: <a href=\"https://founderspledge.com/stories/measuring-health\">Measuring Health: How We Use (And Sometimes Don\u2019t Use) DALY Estimates</a></li><li>Open Philanthropy is <a href=\"https://www.openphilanthropy.org/careers/\">hiring for several roles</a>, including an <a href=\"https://jobs.ashbyhq.com/openphilanthropy/dfdb0ea9-23c6-462e-9d5b-d141ce2ef33a\">Executive Assistant to support its Biosecurity and Pandemic Preparedness team</a>, a <a href=\"https://jobs.ashbyhq.com/openphilanthropy/b15460ec-5554-4c01-9855-0651864b0965\">Programs Operation Assistant to support its Global Health and Wellbeing team</a>, and a <a href=\"https://jobs.ashbyhq.com/openphilanthropy/243da982-26bb-4412-8d48-71d6da45ea92\">Grants Associate focused on longtermist grantmaking</a>.</li><li>GiveWell is <a href=\"https://www.givewell.org/about/jobs\">hiring for several positions</a>, including an <a href=\"https://www.givewell.org/about/jobs/operations-assistant\">Operations Assistant</a> to support other operations staff and help ensure the smooth functioning of the office. It is still seeking candidates for its open <a href=\"https://www.givewell.org/about/jobs/senior-researcher\">Senior Researcher</a>, <a href=\"https://www.givewell.org/about/jobs/senior-research-associate\">Senior Research Associate</a>, and <a href=\"https://www.givewell.org/about/jobs/content-editor\">Content Editor</a> positions.</li></ul><h3>Cause areas</h3><p>Animal welfare</p><ul><li>Good Food Institute has published a <a href=\"https://gfi.org/wp-content/uploads/2022/08/GFI_2022-Mid-Year-Impact-Report_Stories-from-the-road-to-a-brighter-food-future.pdf\">mid-year impact report</a>.</li><li>Faunalytics has published <a href=\"https://faunalytics.org/going-veg-barriers-and-strategies/\">the third report</a> (focusing on barriers and supports for new vegans/vegetarians) in its longitudinal study.</li><li>Wild Animal Initiative has<a href=\"https://www.wildanimalinitiative.org/blog/grantee-altricial-birds\"> issued a grant to a project</a> that will use thermal imaging to study the relationship of body temperature to stress signals and energy reserves in young birds.</li><li>The Humane League reports on significant corporate cage-free progress:<ul><li>After a worldwide Open Wing Alliance campaign, Toridoll Holdings Corporation released a \u201cfirst of its kind\u201d<a href=\"https://thehumaneleague.org/article/victory-toridoll-releases-a-global-cage-free-commitment\"> global cage-free commitment</a>, which the Humane League reports will \u201cdirectly improve the lives of 40,000 egg-laying hens by the end of 2023 in Japan alone.\u201d</li><li>In response to consumer pressure, <a href=\"https://thehumaneleague.org/article/bjs-cage-free-progress\">BJ\u2019s Wholesale Club</a> has renewed its commitment to go cage-free.</li></ul></li></ul><p>Global health and development</p><ul><li>GiveDirectly has <a href=\"https://www.givedirectly.org/aug29-announcement/\">appointed</a> Former UK International Development Secretary Rory Stewart as President/CEO and is planning an ambitious program scale-up.</li><li><a href=\"https://www.vox.com/future-perfect/2022/8/31/23329242/givedirectly-cash-transfers-rory-stewart\">The rise and rise of GiveDirectly</a>: Dylan Matthews discusses the growth of GiveDirectly, how its cash transfer model has helped change the charitable giving landscape, and what\u2019s coming next under Stewart\u2019s leadership.</li><li>Malaria Consortium <a href=\"https://www.malariaconsortium.org/news-centre/r21-malaria-vaccine-gives-up-to-80-percent-protection.htm\">comments</a> on the R21 malaria vaccine\u2019s impressive <a href=\"https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(22)00442-X/fulltext\">trial results</a> (up to 80% protection).</li><li>Against Malaria Foundation <a href=\"https://www.againstmalaria.com/News.aspx\">is hiring for a few roles</a>, including Junior and Senior <a href=\"https://www.againstmalaria.com/NewsItem.aspx?newsitem=AMF-is-hiring-Junior-and-Senior-Operations-Managers\">Operations Managers</a> and a <a href=\"https://www.againstmalaria.com/NewsItem.aspx?newsitem=AMF-is-hiring-Donations-Administrator\">Donations Administrator</a>.</li></ul><p>Long-term future</p><ul><li><a href=\"https://jacobin.com/2022/09/socialism-longtermism-effective-altruism-climate-ai\">The Socialist Case for Longtermism</a>: In an article for Jacobin magazine, Garrison Lovely explains why he sees longtermism as \u201cperfectly compatible with a socialist worldview.\u201d</li><li>Clean Air Task Force has <a href=\"https://www.catf.us/2022/09/new-report-finds-heightened-cancer-risk-14-million-people-toxic-air-pollution-emitted-us-oil-gas-sector/\">released</a> a newly updated report showing that 14 million people are at increased risk of cancer due to toxic oil and gas emissions in the U.S. CATF also discusses <a href=\"https://www.catf.us/2022/09/fossil-fuel-emissions-driving-climate-change-increasing-cancer-risk-heres-what-epa-can-do-now/\">concrete mitigation measures</a> it believes the EPA should enact.</li><li>Clean Air Task Force <a href=\"https://www.catf.us/2022/09/new-global-steel-standard-puts-emissions-reductions-forefront-major-step-forward-decarbonizing-industry/\">reports</a> on a new global steel standard which it believes will help decarbonise the industry.</li><li>Terra Praxis has <a href=\"https://uploads-ssl.webflow.com/610b21fd36b690654ace3fe8/6321e590f8aee17e04ed332a_TerraPraxis%20Microsoft%20Repowering%20Coal%20Press%20Release.pdf\">announced</a> a collaboration with Microsoft that will aim to \u201cdecarbonise coal facilities with nuclear power,\u201d \u201crepurposing over 2,400 coal-fired power plants worldwide to run on carbon-free energy.\u201d</li><li>The Johns Hopkins Center for Health Security is <a href=\"https://www.centerforhealthsecurity.org/news/center-news/2022-09-13-ApplicationsNowOpen.html\">accepting applications</a> for its Health Security track PhD program (2023-2024 academic year). Full funding is available.</li></ul><figure class=\"media\"><div data-oembed-url=\"https://youtu.be/efVh51hbRHY\"><div><iframe src=\"https://www.youtube.com/embed/efVh51hbRHY\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><a href=\"https://www.givingwhatwecan.org/donate/organizations\"><strong>Streamline your giving</strong></a><strong> by setting up a recurring donation to a highly-effective charity today!</strong></p><h2>Useful Links</h2><ul><li>Review <a href=\"https://app.effectivealtruism.org/funds\">our giving recommendations</a>.</li><li>Report your donations with <a href=\"https://app.effectivealtruism.org/pledge\">your pledge dashboard</a>.</li><li><a href=\"https://www.givingwhatwecan.org/get-involved/share-our-ideas\">Share our ideas</a> to help grow our community and multiply your impact.</li><li>Join other members in the <a href=\"https://www.facebook.com/groups/givingwhatwecancommunity/\">Giving What We Can Community</a> Facebook group.</li><li>Find more ways to <a href=\"https://www.givingwhatwecan.org/get-involved\">get involved</a> with Giving What We Can and effective altruism.</li><li>Discuss effective giving and effective altruism on the <a href=\"https://forum.effectivealtruism.org/\">EA Forum</a>.</li></ul><p>You can follow us on <a href=\"https://twitter.com/givingwhatwecan\">Twitter</a>, <a href=\"https://www.facebook.com/givingwhatwecan\">Facebook</a>, <a href=\"https://www.linkedin.com/company/2760845/\">LinkedIn</a>, <a href=\"https://www.instagram.com/giving_what_we_can/\">Instagram</a>, <a href=\"https://www.youtube.com/channel/UC_gBQtUE3BBl-Mh_IBQkg9Q\">YouTube</a>, or <a href=\"https://www.tiktok.com/@givingwhatwecan\">TikTok</a> and subscribe to the <a href=\"https://www.effectivealtruism.org/ea-newsletter-archives/\">EA Newsletter</a> for more news and articles.<br><br>Do you have questions about the pledge, Giving What We Can, or effective altruism in general? Check out our <a href=\"https://www.givingwhatwecan.org/about-us/frequently-asked-questions\">FAQ page</a>, or <a href=\"https://www.givingwhatwecan.org/about-us/contact-us\">contact us </a>directly.</p>", "user": {"username": "Giving What We Can"}}, {"_id": "sGdaTFqpuPe6i68uM", "title": "Distribution Shifts and The Importance of AI Safety", "postedAt": "2022-09-29T22:38:12.656Z", "htmlBody": "", "user": {"username": "Leon_Lang"}}, {"_id": "bpPgYzKtFTNEhkCgy", "title": "Where I currently disagree with Ryan Greenblatt\u2019s version of the ELK approach", "postedAt": "2022-09-29T21:19:54.092Z", "htmlBody": "<p>Context: This post is my attempt to make sense of Ryan Greenblatt's research agenda, as of April 2022. I understand Ryan to be heavily inspired by Paul Christiano, and Paul left some comments on early versions of these notes.</p><p>Two separate things I was hoping to do, that I would have liked to factor into two separate writings, were (1) translating the parts of the agenda that I understand into a format that is comprehensible to me, and (2) distilling out conditional statements we might all agree on (some of us by rejecting the assumptions, others by accepting the conclusions). However, I never got around to that, and this has languished in my drafts folder too long, so I'm lowering my standards and putting it out there.</p><p>The process that generated this document is that Ryan and I bickered for a while, then I wrote up what I understood and shared it with Ryan, and we repeated this process a few times. I've omitted various intermediate drafts, on the grounds that sharing a bunch of intermediate positions that nobody endorses is confusing (moreso than seeing more of the process is enlightening), and on the grounds that if I try to do something better then what happens instead is that the post languishes in the drafts folder for half a year.</p><p>(Thanks to Ryan, Paul, and a variety of others for the conversations.)</p><p>&nbsp;</p><h2>Nate's model towards the end of the conversation</h2><p>Ryan\u2019s plan, as Nate currently understands it:</p><ul><li>Assume AGI is going to be paradigmatic, in the sense of being found by something roughly like gradient descent tuning the parameters in some fixed architecture. (This is not intended to be an argument for paradigmaticity; attempting to align things in the current paradigm is a good general approach regardless (or so Nate understands Ryan to claim).)</li><li>Assume further that Earth's first AGIs will be trained according to a process of our choosing. (In particular, it needs to be the case that AGI developers can train for more-or-less any objective they want, without thereby sacrificing competitiveness. Note that this might require significant feats of reward-shaping.)</li><li>Assume further that most capability gains will be driven by something roughly like gradient descent. (Ryan has some hope that this plan would generalize to various other drivers of capability gains, but Nate hasn't understood any of the vague gestures towards those ideas, and as of April 2022, Ryan's plans were mostly stated in terms of SGD, so I'll set that aside for now.)</li><li>With those three assumptions in hand, part one of the plan is to pretend like we have the One True Objective Function, and that we've contracted the services of the Devil Himself, which we utilize to train the AGI to pursue the given objective. The special power of the devil is that he can generate the inputs that cause the nascent AI to do as poorly as possible according to the one true objective function, and we can use this to find the activations of the worst misbehaviors, and expose them to gradient updates. This is intended to result in a system that Nate might call \"inner aligned\" with the objective we use in training.</li><li>Part two of the plan is to solve the problem of eliciting a paradigmatic AI system's latent knowledge, and then use it to substitute for the One True Objective Function and the Devil Himself. The basic idea here is that you don't need to actually have a perfect objective function or a perfect adversary for part one to work; it should in general suffice to nip any misbehavior in the bud the moment that the AI itself starts to have any knowledge of that misbehavior.</li><li>The scope of this plan is to produce a system that would work well in-distribution, and that would at least not behave catastrophically if deployed out-of-distribution. Having it behave well (and competitively) out-of-distribution is outside the scope of this particular plan (though there's some hope that it Just Works, because, e.g., the nip-all-misbehavior-in-the-bud stuff may just generalize cleanly if you're doing your job right).</li><li>It is worth noting explicitly that the task of the adversary is quite tricky, here, on account of how there are types of distributional shift that could cause the AGI to start misbehaving (e.g., because it can tell that it's now actually in deployment where it won't get shut down if it misbehaves), and which are infeasible in practice for an adversary to spoof by generating adversarial inputs alone (because it's hard to persuasively convince an AI in training that it is perhaps in deployment). Clever alternatives are needed here, like spoofing the system's mental states instead of generating the sorts of observations that only deployment can generate, but hopefully something can be made to work.</li></ul><p>&nbsp;</p><p>Nate's response:</p><ul><li>You're sidestepping a large chunk of the problem by avoiding the need to behave well when deployed far out of distribution.</li><li>I don't think it's sociopolitically feasible to pick a training method that trains the AGI from inception (which, e.g., would break down if the first AGI comes from an experimental new \"lifelong learning with varied objectives\" regime, but I am expecting reality to be at least that inconvenient).</li><li>I don't think it's practically feasible to get an AGI by training purely on the deployment objective.</li><li>I suspect that SGD won't be the primary driver of capabilities gains, near the end.&nbsp;<a href=\"https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment#Ryan_Greenblatt___Eliciting_Latent_Knowledge\"><u>Compare humans</u></a>, who reached a certain capability level and then began to rapidly improve via knowledge, science, and culture, rather than just improving via our genes. More generally, I think there's a solid chance that, at some point, capabilities will start generalizing far OOD, and that the sort of alignment guarantees you can get from these sorts of methods&nbsp;<a href=\"https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization\"><u>will fail to generalize with them</u></a>.</li><li>I doubt you can produce adversaries that are up to the task of preventing your AGI from distinguishing training from deployment.</li><li>I doubt you can elicit the latent knowledge of a nascent AGI (in a way that scales with the capabilities of the AGI) well enough to substitute for the one true objective function and the devil himself and thus produce inner alignment.</li><li>If you could, I'd begin to suspect that the latent-knowledge-eliciter is itself containing lots of dangerous machinery that more-or-less faces its own version of the alignment problem.</li></ul><p>&nbsp;</p><h2>An attempt at conditional agreement</h2><p>I suggested the following:</p><p>&nbsp;</p><p>If it is the case that:</p><ul><li>Gradient descent on a robust objective cannot quickly and easily change the goals of early paradigmatic AGIs to move them sufficiently toward the intended goals,</li><li>OR early deployments need to be high-stakes and out-of-distribution for humanity to survive, AND<ul><li>adversarial training is insufficient to prevent early AGIs from distinguishing deployment from training,</li><li>OR the critical outputs can be readily distinguished from all other outputs, e.g., by their universe-on-a-platter nature,</li></ul></li><li>OR early paradigmatic AGIs can get significant capability gains out-of-distribution from methods other than more gradient descent,</li></ul><p>... THEN the Paulian family of plans don't provide much hope.</p><p>&nbsp;</p><p>My understanding is that Ryan was tentatively on board with this conditional statement, but Paul was not.</p><p>&nbsp;</p><h2>Postscript</h2><p>Reiterating a point above: observe how this whole scheme has basically assumed that capabilities won't start to generalize relevantly out of distribution. My model says that they eventually will, and that this is precisely when things start to get scary, and that one of the big hard bits of alignment is that&nbsp;<i>once that starts happening</i>,&nbsp;<a href=\"https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization\"><u>the capabilities generalize further than the alignment</u></a>. A problem that has been simply assumed away in this agenda, as far as I can tell, before we even dive into the details of this framework.</p><p>To be clear, I'm not saying that this decomposition of the problem fails to capture difficult alignment problems. The \"prevent the AGI from figuring out it's in deployment\" problem is quite difficult! As is the \"get an ELK head that can withstand superintelligent adversaries\" problem. I think these are the wrong problems to be attacking, in part on account of their difficulty. (Where, to be clear, I expect that toy versions of these problems are soluble, just not solutions rated for the type of opposition it sounds like the rest of this plan requires.)</p>", "user": {"username": "So8res"}}, {"_id": "qPEmQtgnbNgmLTmi4", "title": "Biosecurity Dual Use Screening - Project Proposal (seeking vetting & project lead)", "postedAt": "2022-09-29T19:01:18.007Z", "htmlBody": "<h1>Pitch:</h1><ul><li>Publishing dual use papers is bad.</li><li>It\u2019s also easy, people can upload it to arxiv/bioRxiv/medRxiv, even by accident (not knowing they are publishing something potentially harmful).</li><li>It might be useful for these platforms to reject such publications, and the platforms seem <a href=\"https://forum.effectivealtruism.org/posts/qPEmQtgnbNgmLTmi4/biosecurity-dual-use-filtering-project-proposal-seeking#Why_I_think_the_platforms_are_interested_in_doing_this\">interested</a>.</li><li>I\u2019m guessing they\u2019re lacking resources: People that will vet papers, maybe a software system, maybe money.</li><li>Let\u2019s talk to the platforms, ask what they need, and give it to them.</li></ul><h1>Why I think the platforms are interested in doing this</h1><p>A founder from bioRxiv and medRxiv,&nbsp;<a href=\"https://www.linkedin.com/in/richardsever\"><u>Richard Sever</u></a>, says about this screening:</p><ol><li>\"This is desirable and in fact already happens to an extent\"</li><li>\"arXiv and bioRxiv/medRxiv already communicate regularly\"</li></ol><p>I can provide the reference for this.</p><h1>Request for vetting</h1><p>My experience in biosecurity is about 3 hours.</p><p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995971/mirroredImages/qPEmQtgnbNgmLTmi4/gumjyovpt4elsqlye7jd.png\"></p><p>Please, people doing biosecurity, reply with your opinion, even if it is very short like \u201csounds good\u201d or \u201csounds bad\u201d.</p><h1>Looking for project lead</h1><p>Do you know someone who could run this? Comment on the post (or DM me, and I\u2019ll pass it on somewhere).</p><h1>Before starting this project, please review the ways it could go wrong</h1><p>As a naive example, just to be concrete: Someone gets mad that their (dangerous) article wasn\u2019t accepted, so they publish it on Twitter and it goes viral.</p><p>But more generally, before starting this project, please talk to the people who reply \u201csounds bad\u201d.</p><p>It was just&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/aeZJqNht9izeLCpkm/9-26-is-petrov-day\"><u>Petrov Day</u></a>, \u201dWherever you are, whatever you're doing, take a minute to not destroy the world\u201d.</p><h1>Thanks</h1>", "user": {"username": "hibukki"}}, {"_id": "nTybQwrnyRMenasCc", "title": "Carnegie Council MisUnderstands Longtermism", "postedAt": "2022-09-30T02:57:37.313Z", "htmlBody": "<p>I came across this article from the Carnegie Council's Artificial Intelligence and Equality Initiative and I can't help but feel like they misunderstand longtermism and EA. The article mentions the popularity of William Macaskill's new book \"What We Owe the Future\" and the case for considering future generations and civilization. I would recommend you read the article before you read my take below, but Carnegie Council made the common fallacious arguments against longtermism.</p><ol><li>They make it seem like in order to address longtermism, you have to completely ignore the present. I have never heard an EA argue for disregarding contemporary issues.</li><li>They convey that longtermism requires you to \"put all your eggs in one basket,\" the basket being longtermism and not today's problems.</li><li>Regulating AI will result in a slowdown in production. Yes, this is true but risking an uncontrollable accelerating risky technology like AI can result in the end of humanity and mass suffering. Therefore, the trade-off should be worth it much like regulating carbon emissions is (better word for worth it).</li></ol>", "user": {"username": "Jeffrey Arana"}}, {"_id": "zQ5apJGAJb6otXdvh", "title": "High-Impact Psychology (HIPsy): Piloting a Global Network", "postedAt": "2022-09-29T13:08:26.255Z", "htmlBody": "<p><strong>Engaged with psychology or mental health? This is for you.</strong></p><p>Impartial compassion. Rationality. Wellbeing. For a movement built on these values, EA likely underutilizes&nbsp;psychology professionals. Together with our supporters from the Global Priorities Institute, the Center for Effective Altruism, and the Happier Lives Institute, HIPsy aims to help people engaged with psychology or mental health maximize their impact.&nbsp;</p><p><strong>Vision</strong></p><p>We will follow in the footsteps of the EA Consulting Network, High-Impact Medicine, and High-Impact Athletes. Accordingly, the goal of HIPsy shall be to increase the likelihood of high-impact decisions, make collaboration and information processes more effective, and reduce the risk of value drift for people engaged in psychology or mental health.&nbsp;</p><p>Relevant resources shall be available, easy to access, and use:&nbsp;</p><ul><li>up-to-date high-quality information,&nbsp;</li><li>career and work advice,&nbsp;</li><li>networking, and collaboration opportunities.</li></ul><p>Psychological know-how shall be effectively acquired, shared, and used for EA. Psychology expertise is particularly needed in the fields of:</p><ul><li>mental health and well-being, both within EA and globally,&nbsp;</li><li>community building, and outreach,&nbsp;</li><li>management, HR, and operations,</li><li>priorities research, applied research, effectiveness research,</li><li>x-risk-reduction and AI safety, e.g. awareness-building, and persuasion.&nbsp;</li></ul><p>Summary: The goal for the next few months is to find out which of the many potential actions to prioritize, and how to address them most effectively. We will check what materials, events, and services are in-demand, and pilot some of them.</p><p>&nbsp;</p><figure class=\"image image_resized\" style=\"width:60.33%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669628592/mirroredImages/zQ5apJGAJb6otXdvh/ulync5xyjrlpjzstenkc.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/055d45ec7f1a920c116b91fdf48a1158294ac4ae0668f901.png/w_90 90w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/055d45ec7f1a920c116b91fdf48a1158294ac4ae0668f901.png/w_170 170w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/055d45ec7f1a920c116b91fdf48a1158294ac4ae0668f901.png/w_250 250w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/055d45ec7f1a920c116b91fdf48a1158294ac4ae0668f901.png/w_330 330w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/055d45ec7f1a920c116b91fdf48a1158294ac4ae0668f901.png/w_410 410w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/055d45ec7f1a920c116b91fdf48a1158294ac4ae0668f901.png/w_490 490w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/055d45ec7f1a920c116b91fdf48a1158294ac4ae0668f901.png/w_570 570w\"></figure><p><strong>Join us and pre-sign-up</strong>&nbsp;</p><p>Here you can let us know which resources would be most valuable to you. We aim to provide the most wanted and most promising ones\u2014very likely for free. Feel free to forward this to anyone who might be interested in benefitting from this:</p><ul><li><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe79DiNOj-3rn9-e6UJF9Iagmk8g2bkXDZQH8XYJ1Y6RbQqtw/viewform?usp=sf_link\">5 min - for you if you have a background in psychology</a></li><li><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSek_fDDmAunEQR-g4AfTOKOT44h4FsLJcJV-8e0pJvYKLbnrw/viewform?usp=sf_link\">5 min - for you if you are interested in, working in, or adjacent to the mental health sector</a></li><li><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdFbUZUyK4jrwRiii7QwEXogRFzm50PENxIyLwtByKQn5nuPw/viewform?usp=sf_link\">3 min - for everyone interested in increasing their impact using evidence-based psychology</a></li></ul><p>You want to help more? &nbsp;</p><p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScAa9_Sql-kPkFaZYhKwlaNqRUil9GvS0cG9hH1G-zq2o3vRQ/viewform?usp=sf_link\"><u>Let us know here</u></a>. If you'd like to collaborate, fund us, or if you have got any of the following skills, we want to hear from you: online content creation, running mentorship programs, hosting events, web-dev, design, community-building, operations, running surveys, research, and cost-effectiveness analyses. Let us know if you have any other ideas.</p><p><strong>Opportunities</strong></p><p>Engaging with &gt; 50 members of the EA community and their materials revealed three major opportunities:<br><br>1. EA has skill bottlenecks that psychological professionals can help with</p><ul><li>Industry:&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building\"><u>management, entrepreneurship</u></a>, operations, HR&nbsp;</li><li>Science: at universities and EA mental health/wellbeing orgs (lack of seniors)</li><li>Therapy: EA-informed psychotherapists and coaches for members of the EA community</li></ul><p><br>2. It\u2019s hard for psychology professionals to enter EA even if they are likely good matches</p><ul><li>Existing materials and advice are difficult to find, scattered, contradictory, out of date, and selective (e.g.&nbsp;<a href=\"https://docs.google.com/document/d/1EYMpTpJ1cXGJ4ayLpuCJZmlw6mZKnX-RfhYqbHuqgnU/edit\"><u>reading list</u></a>,&nbsp;<a href=\"https://effectivethesis.org/theses/\"><u>Effective Thesis</u></a>) which likely discourages prospects</li><li>Effectiveness and altruism: People largely enter psychology because they want to help others, and most universities have very high entrance requirements and thus select intelligent and ambitious individuals&nbsp;</li><li>Mental health as a cause area faces large funding constraints that make it hard to enter, contribute and stick for people engaged in this field.&nbsp;</li></ul><p><br>3. There is unfulfilled potential for synergistic action and systematic exchange<br>The impact of psychology-related EA orgs could be boosted by&nbsp;</p><ul><li>informing members of the community of techniques, knowledge, and best practices they can use to be more impactful, e.g. status quo of psychology research</li><li>fostering collaboration and joint efforts between organizations that face similar challenges, e.g. mental health orgs that need to learn how to develop scalable and sustainable psychological interventions</li><li>cooperating with leading psychology experts and EA-aligned individuals outside of EA that can help EA stay up to date with current industry, market, and research standards.</li></ul><p><i>Conclusion</i>. It can be easier to access up-to-date high-quality information, advice, and networking opportunities. We imagine doing so would foster effective action, i.e. knowledge transfer, exchange, and collaboration, enabling high-speed and high-impact work in psychology-related areas.&nbsp;</p><p>&nbsp;</p><p><strong>Plan&nbsp;</strong></p><p>Expected outcomes by mid-November (pilot phase):</p><ul><li>We\u2019ve assessed how promising potential actions are to close the gaps and have prioritized based on expected effort, impact, and our comparative advantage&nbsp;vis-\u00e0-vis existing initiatives.</li><li>We\u2019ve gathered sufficient data from surveys and pilot activities on whether to move on with HIPsy.&nbsp;</li><li>If yes, we have made a concrete plan on how and with whom to move forward cost-effectively.</li></ul><p><br>These are the potential areas of activity:</p><ul><li><i>Networking and events</i><br>Build engaging, accessible peer networks to foster productive exchange and relationships between psychology professionals and psychology-related orgs within EA. For example:<ul><li>EAG contributions, retreats, and one-on-one meetings</li><li>online meet-ups, workshops, and discussion events</li><li>mentoring, feedback, and career advice arrangements</li><li>online groups, e.g. contributions to existing ones such as Slack: \"<a href=\"https://join.slack.com/t/eamentalhealth/shared_invite/zt-y9nx933l-jwDgoJoN6jYBIsStDHCuTA\"><u>EA Mental Health</u></a>\", \"EA Psychology Research\"; Facebook: \"<a href=\"https://www.facebook.com/groups/1275690949170991/\"><u>Psychology for EA</u></a>\", \"<a href=\"https://www.facebook.com/groups/EAmentalhealthandhappiness/\"><u>EA, Mental Health, and Happiness</u></a>\u201d</li></ul></li><li><i>Information materials&nbsp;</i><br>Create, update, and distribute comprehensive high-quality resources, and create a system for organizing, requesting, and sharing resources of various kinds.&nbsp;<br>For example:<ul><li><i>Overview</i>:&nbsp;<br>a website where the target groups can find all resources organized and linked, as well as structured, e.g. high-impact opportunities in psychology-related fields as well as everything listed below</li><li><i>Tailored materials:&nbsp;</i><br>on how to get funded or hired, or how to found an org in psychology-related cause areas</li><li><i>Summaries:</i>&nbsp;<br>of the most important studies, data, interventions, problems, bottlenecks, opportunities, and best practices, e.g.:<ul><li>lists of high-impact career and volunteering opportunities,&nbsp;</li><li>directory of psychologists in EA,&nbsp;</li><li>list with mental health orgs and projects relevant to EA,&nbsp;</li><li>list of ideas related to psychology for AIS</li><li>manual on how to measure impact,&nbsp;</li><li>manual on how to do in-group outreach to psychology students or psychotherapists in training</li></ul></li></ul></li></ul><p>We will check what materials, events, and services missing, requested, and potentially impactful.&nbsp;</p><p>&nbsp;</p><p>Our prioritization will be informed by::</p><ul><li><i>surveys within the EA community</i>&nbsp;<br>Number of people who request certain materials and sign up for different events and services</li><li><i>engagement in pilot activities</i>&nbsp;<br>Number and ratings of \u201cusers\u201d, e.g. for sessions at EAGs, the website, and the new mental health cause area report</li><li><i>qualitative assessment</i>&nbsp;<br>Expected impact and available resources for execution of certain activities, judged with the help of cognitive walkthroughs and expert interviews</li></ul><p><br>Future potential measures of impact for the prioritized activities:</p><ul><li><i>Main goals</i><ul><li><i>No. of people reached who expressed interest or are highly engaged&nbsp;</i></li><li>No. of people/orgs that made higher-impact decisions based on activities/resources</li></ul></li><li><i>Instrumental goals</i><ul><li>Events: no. of participants, evaluation as done at the end of EAGs</li><li>Material: no. of shares/uses/contributions to creation/updates, ratings of helpfulness</li><li>Groups: no. of people joining/writing/reacting, surveys for leaders/members about significant actions taken (initiatives run, switches to EA-aligned jobs, engagement changes)</li></ul></li></ul><figure class=\"image image_resized\" style=\"width:54.35%\"><img src=\"http://res.cloudinary.com/cea/image/upload/v1669628592/mirroredImages/zQ5apJGAJb6otXdvh/ba4b319cif3vmi80twsx.png\" srcset=\"https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2fad405f5d0c2d9ec47cc73058de87d5c7d07310e0d335e5.png/w_133 133w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2fad405f5d0c2d9ec47cc73058de87d5c7d07310e0d335e5.png/w_213 213w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2fad405f5d0c2d9ec47cc73058de87d5c7d07310e0d335e5.png/w_293 293w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2fad405f5d0c2d9ec47cc73058de87d5c7d07310e0d335e5.png/w_373 373w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2fad405f5d0c2d9ec47cc73058de87d5c7d07310e0d335e5.png/w_453 453w, https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/2fad405f5d0c2d9ec47cc73058de87d5c7d07310e0d335e5.png/w_533 533w\"></figure><p><strong>Team</strong></p><p>Lead:</p><p><a href=\"https://www.linkedin.com/in/inga-gro%C3%9Fmann/\"><u>Dr. Inga Grossmann</u></a> was appointed as the youngest professor to date at the HMKW in Berlin, a university of applied science. Having worked in academia and business for 10 years, she has broad and up-to-date knowledge of research, the industry, and the practice related to all sorts of psychological interventions. She managed a variety of research and development projects, small teams in companies and larger groups of students, and built networks of experts for companies. Currently, Inga assists and consults existing EA mental health and meta projects, e.g.&nbsp;Overcome, the&nbsp;Mental Health Navigator,&nbsp;The Berlin Hub, the Wellbeing Program.&nbsp;</p><p><br>Supporters:</p><ul><li>Lucius Caviola (Havard/GPI): is in the lead for EA Psychology Research, with the<a href=\"https://www.eapsychology.org/\">&nbsp;<u>EA Psychology Lab</u></a></li><li>Dvir Caspi / Jon Massmann (Psychology for EA): collaborating to create the HIPsy Website</li><li>Lesley Schimanski: helps with different parts of the project</li><li>Akhil Bansal (CE), Erik Jentzen (Hi-Med): collaborating on a new mental health cause area report and on events for health care professionals</li><li>Devon Fritz (HIP): will assist with marketing and provide strategic support</li><li>Manuel Allgaier (EA Germany), Anneke Pogarell (CEA), Nadia Montazeri (EA Switzerland), Jona Glade (CEA/EACN): advising on community building</li><li>Michael Plant (Happier Lives Institute), Peter Brietbart (Mindease)<u>,</u> Julia Wise (CEA), Emily Jennings (Mental Health Navigator), Sreevidhya Sk (Overcome), John Salter (Overcome): advising on and helping with mental health as a cause area</li></ul><p>&nbsp;</p><p><strong>You can help HIPsy to achieve its impact!</strong></p><ul><li>We'd appreciate it a lot if you pre-signed up and joined us by filling out the forms linked in the \"Vision\" section. You can also forward them to people who might be interested in benefiting from this project.</li><li><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScAa9_Sql-kPkFaZYhKwlaNqRUil9GvS0cG9hH1G-zq2o3vRQ/viewform?usp=sf_link\"><u>Let us know here</u></a> if here you want to collaborate, fund us, or if you want to contribute with any of the following skills: online content creation, running mentorship programs, hosting events, web-dev, community-building, running surveys, research, and cost-effectiveness analyses.&nbsp;</li></ul><p>&nbsp;</p><p>Who helped with this post?</p><p>Thank you to Devon Fritz, Jona Glade, John Salter, Dvir Caspi, Dawn Denis Drescher and Elias Mannherz.</p>", "user": {"username": "Inga"}}, {"_id": "erpcHiAPE3sAwej2Y", "title": "A Playbook for Running Corporate Fundraising Campaigns", "postedAt": "2022-09-29T12:42:10.105Z", "htmlBody": "<p><i>This post is Part 3 in a series on organizing fundraising campaigns in the workplace:</i></p><ul><li><i>Part 1:&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/bsqtndpfKbcbQRFEs/fundraising-campaigns-at-your-organization-a-reliable-path\"><i><u>Fundraising Campaigns at Your Organization: A Reliable Path to Counterfactual Impact</u></i></a></li><li><i>Part 2:&nbsp;</i><a href=\"https://forum.effectivealtruism.org/posts/sE7Y43JRQARAKzZ6k/key-factors-for-success-in-organizing-a-fundraising-campaign\"><i><u>Key Factors for Success in Organizing a Fundraising Campaign at Your Company</u></i></a></li></ul><p>&nbsp;</p><p><a href=\"https://bit.ly/3U8WqDT\"><u>High Impact Professionals</u></a> (\u201cHIP\u201d) is pleased to announce the release of our new&nbsp;<a href=\"https://bit.ly/3dUKUvN\"><u>Guide to Running Corporate Fundraising Campaigns</u></a> (the \u201cGuide\u201d).&nbsp;</p><p>In&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/bsqtndpfKbcbQRFEs/fundraising-campaigns-at-your-organization-a-reliable-path\"><u>Part 1 of this series</u></a>, we described how&nbsp;we\u2019ve found that organizing a fundraising campaign can be&nbsp;<strong>as impactful per hour as direct work</strong>.&nbsp;The Guide provides a step-by-step playbook on how to plan and run a successful fundraising campaign at your company.</p><p>Last year, HIP supported EAs in organizing fundraising campaigns across 8 different companies, which&nbsp;<strong>counterfactually raised about 240,000 USD&nbsp;</strong>for effective charities.&nbsp;</p><p>This year,&nbsp;<strong>we want to help 30 fundraising campaigns</strong> and&nbsp;<strong>we need you</strong> to join us.&nbsp;</p><p>We\u2019ve designed the Guide and the 1-on-1 support we provide to campaign organizers like you as a way to take the guesswork out of the process. So, please&nbsp;<a href=\"https://calendly.com/federico-hip/hip-fundraising-campaign\"><u>book a time</u></a> or&nbsp;<a href=\"mailto:fundraising@highimpactprofessionals.org?subject=Fundraising Campaign Support\"><u>email us</u></a>&nbsp;to explore what a&nbsp;fundraising campaign could look like at your organization.</p><h1>What\u2019s in the Guide?</h1><p>The Guide provides an in-depth look at the how-to, tips, and pitfalls to avoid around the four essential steps of the fundraising campaign process:</p><ol><li>Getting your company on board</li><li>Preparing the campaign</li><li>Executing the campaign</li><li>Wrapping up</li></ol><p>The Guide also complements the key factors that we\u2019ve identified as particularly valuable to running a highly effective campaign, which we highlighted in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/sE7Y43JRQARAKzZ6k/key-factors-for-success-in-organizing-a-fundraising-campaign\"><u>Part 2 of this series</u></a>. As a reminder, those factors include:</p><ul><li>Timing \u2013 Giving season</li><li>Timing \u2013 Liquidity events</li><li>Setting a personal example</li><li>Being open about your donations</li><li>And more\u2026</li></ul><p>To discover more about the ins and outs of running a successful campaign,&nbsp;<a href=\"https://bit.ly/3dUKUvN\"><u>access the Guide here</u></a>, and&nbsp;<a href=\"https://calendly.com/federico-hip/hip-fundraising-campaign\"><u>book a time</u></a> or&nbsp;<a href=\"mailto:fundraising@highimpactprofessionals.org?subject=Fundraising Campaign Support\"><u>email us</u></a> to discuss how we could work together on an effective giving campaign at your organization.</p>", "user": {"username": "High Impact Professionals"}}, {"_id": "W8beqyNmGCWDxvoxA", "title": "London EAs in tech: October meetup", "postedAt": "2022-09-29T14:09:40.210Z", "htmlBody": "", "user": {"username": "Sam Watts"}}, {"_id": "QzRKdZeT3pfZimipz", "title": "Heuristics for making theoretical progress (in philosophy) from Alan Hajek (ANU)", "postedAt": "2022-09-29T11:24:34.806Z", "htmlBody": "<p><i>&nbsp;Note: I share this because this paper seems like a useful collection of somewhat general heuristics for making theoretical progress, especially in philosophy by ANU philosophy professor Alan Hajek . I expect this to be <strong>most useful for people who are very early on in their research careers</strong>, but perhaps it's also a good reminder for more experienced researchers. To me, it felt like I knew many of the heuristics but still often fail to apply them. I encourage anyone to read the parts of the paper where Hajek discusses the application of these heuristics and gives examples of how to use them. (I would nevertheless skip large parts of the paper such as the first two sections). <strong>I also encourage everyone to share when and to what extent they find these heuristics useful and perhaps suggest additional ones that they have found helpful.&nbsp;</strong></i><br><br><i>Some excerpts (in order of how interesting they seem to me, not how they appear in the paper; highlighting added by me)</i></p><h2><br><br><br>'<strong>Future projects: dissertations and books waiting to be written&nbsp;</strong></h2><p>When you are looking for a big project to work on, take some big philosophical idea or program, and apply it to a new case. The scheme is to apply philosophical system X to specific problem Y, for suitable X and Y. This is the closest I can come to producing a heuristic for producing ground-breaking philosophy. Here the thought is that rather than merely responding to someone else\u2019s agenda, you can do some agenda-setting of your own. And if ground-breaking philosophy when it succeeds doesn\u2019t count as creative, I don\u2019t know what does. However, even if the results are not quite so dramatic, still the heuristic encourages one to look beyond entrenched ways of thinking about an issue. System X is illuminated if a hitherto unrecognized application of it is revealed; progress may be made on recalcitrant problem Y if it is approached from a fresh perspective.&nbsp;<br><br>[...] Closest to my heart, Bayesian confirmation theory has illuminated the confirmation of scientific theories. (See Howson and Urbach 2006.) I believe it has yet to be applied to the confirmation of historical theories.'<br>&nbsp;</p><h2><br>'<strong>Begetting new arguments out of old</strong></h2><p><br>&nbsp;Arguments are often easily transformed from one domain to another. Arguments involving space can often be rewritten to create parallel arguments involving time; arguments involving time can often be rewritten to create parallel arguments involving modality; and we can reverse these directions. [...]<br>&nbsp;7.1.1 Parfit (1984) has an argument for the irrationality of discounting the future that turns on the absurdity of a similar spatial discounting.'</p><p><br>&nbsp;</p><h2><strong>'Death by diagonalization: reflexivity/self-reference</strong></h2><p>You can't bite your own teeth, unless something has gone badly wrong for you, dentally speaking. You can't see your own eyes\u2014not directly, anyway\u2014unless something has gone very badly wrong for you, optically speaking. The next heuristic bids us to take a philosophical thesis, and to make it refer to itself, to plug into a function itself as its own argument, and more generally, to appeal to self-referential cases. This technique is another handy way of cutting down the search space when you are looking for counterexamples. Let us take our cue from Cantor's \u2018diagonalization\u2019 proof of the uncountability of the reals, or G\u00f6del's proof of the incompleteness of arithmetic, or the halting problem, or Russell's paradox, or the liar paradox. They remind us of the august history of the technique of selfreference; its application can yield profound results.</p><p>[...] According to the betting interpretation of subjective probability, your degree of belief in a proposition is the price (in cents) at which you are indifferent between buying and selling a bet that pays $1 if the proposition is true, and nothing otherwise. But I have degrees of belief about my own betting behavior\u2014e.g. I am confident that I will not enter into any bets today. This degree of belief cannot be understood in terms of a betting price of mine.'<br>&nbsp;</p><h2><br>'<strong>Check extreme cases</strong></h2><p>Start with a hard problem: someone proposes a philosophical position or analysis and you are looking for trouble for it, because you suspect that there is something wrong with it. (The \u2018someone\u2019 might be you, in which case your job is to find trouble for your own position before someone else generously does it for you.) Try this simpler problem: look for trouble among extreme cases\u2014the first, or the last, or the biggest, or the smallest, or the best, or the worst, or the smelliest, or ... It is a snappy way to reduce the search space. Even if there are no counterexamples lurking at the extreme cases, still they may be informative or suggestive. They may give you insights that you would have missed by focusing on more run-of-the-mill, typical cases.&nbsp;<br><br>[...] Some philosophers regard 'every event has a cause' to be a necessary truth. At first, one may wonder how to argue against this claim\u2014where should one start? The heuristic guides the search for a counterexample: start with extreme events. For instance, start with the start. The first event is an extreme event: the big bang. There was no prior event to cause it; it surely did not cause itself; and it surely was not retro-caused by some later event\u2014so we have our counterexample.'</p>", "user": {"username": "Aaron__Maiwald"}}, {"_id": "ygdpXBoLzzsLXhhDF", "title": "I'm interviewing prolific AI safety researcher Richard Ngo (now at OpenAI and previously DeepMind). What should I ask him?", "postedAt": "2022-09-29T00:00:15.615Z", "htmlBody": "<p>Next week I'm interviewing <a href=\"https://www.linkedin.com/in/richard-ngo-9056b473/\">Richard Ngo</a>, current AI (Safety) Governance Researcher at OpenAI and previous Research Engineer at DeepMind.</p>\n<p>Before that he was doing a PhD in the Philosophy of Machine Learning at Cambridge, on the topic of \"to what extent is the development of artificial intelligence analogous to the biological and cultural evolution of human intelligence?\"</p>\n<p>He is focused on making the development and deployment of AGI more likely to go well and less likely to go badly.</p>\n<p>Richard is also a highly prolific contributor to online discussion of AI safety in a range of places, for instance:</p>\n<ul>\n<li><a href=\"https://thinkingcomplete.blogspot.com/2022/07/moral-strategies-at-different.html\">Moral strategies at different capability levels</a> on his blog Thinking Complete</li>\n<li><a href=\"https://forum.effectivealtruism.org/posts/QYbP47ZErrgFYXBLX/the-alignment-problem-from-a-deep-learning-perspective\">The alignment problem from a deep learning perspective</a> on the EA Forum</li>\n<li><a href=\"https://www.alignmentforum.org/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects\">Some conceptual alignment research projects</a> on the AI Alignment Forum</li>\n<li><a href=\"https://www.lesswrong.com/posts/7im8at9PmhbT4JHsW/ngo-and-yudkowsky-on-alignment-difficulty\">Richard Ngo and Eliezer Yudkowsky policely debating AI Safety</a> on Less Wrong</li>\n<li><a href=\"https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ\">The AGI Safety from First Principle education series</a></li>\n<li>And on <a href=\"https://twitter.com/RichardMCNgo/status/1559991216636186624\">his Twitter</a></li>\n</ul>\n<p>What should I ask him?</p>\n", "user": {"username": "Robert_Wiblin"}}, {"_id": "tokGikSg3fSJun4Lw", "title": "EA & LW Forums Weekly Summary (19 - 25 Sep 22')", "postedAt": "2022-09-28T20:13:00.964Z", "htmlBody": "<p><br><i>Supported by Rethink Priorities</i></p><p>This is part of a weekly series - you can see the full collection <a href=\"https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN\">here.</a> The first post includes some details on purpose and methodology.</p><p>If you'd like to receive these summaries via email, you can subscribe <a href=\"https://easummaries.substack.com/?r=1p817z&amp;s=w&amp;utm_campaign=pub&amp;utm_medium=web\">here.</a></p><p><strong>Podcast version</strong>: prefer your summaries in podcast form? A big thanks to Coleman Snell who is now producing these! The first episode is now up, and this week's will be up soon. Subscribe on your favorite podcast app by searching for 'Effective Altruism Forum Podcast'.</p><p>&nbsp;</p><h1>Top Readings / Curated</h1><p>Designed for those without the time to read all the summaries. Everything here is also within the relevant sections later on so feel free to skip if you\u2019re planning to read it all.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\"><u>Announcing the Future Fund's AI Worldview Prize</u></a></p><p><i>by Nick_Beckstead, leopold, William_MacAskill, ketanrama, ab</i></p><p>The Future Fund believes they may be significantly under or over estimating catastrophic risk from AI, or that they are focusing on the wrong aspects of it. Since this affects the distribution of hundreds of millions of dollars, they are offering prizes of up to $1.5M for arguments that significantly shift their credences on when and how AGI will be developed and its chance of catastrophic effect. A secondary goal is to test the efficacy of prizes at motivating important new insights. Smaller prizes are also available. Entries are due Dec 23rd.<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/AFgvA9imsT6bww8E3/announcing-the-rethink-priorities-special-projects-program\"><u>Announcing the Rethink Priorities Special Projects Program</u></a></p><p><i>by Rachel Norman</i></p><p>Rethink Priorities is launching a program to help start promising EA initiatives. Some will be internally incubated and later spun-off into independent organizations, others run externally will be fiscally sponsored and given operations support. The team is currently incubating / sponsoring eight projects ranging from insect welfare to community building in Brazil to AI alignment research, and is looking for expressions of interest for those interested in fiscal sponsorship or project incubation.</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis\"><u>Nearcast-based \"deployment problem\" analysis</u></a></p><p><i>by HoldenKarnofsky</i></p><p>The AI deployment problem is the question of how and when to (attempt to) build and deploy powerful AI, when unsure about safety and how close other actors are to deploying. This post assumes a major AI company (\u2018Magma\u2019) thinks it is 6months - 2years to TAI using human feedback on diverse tasks (HFDT), and what both it and an organization dedicated to tracking and censoring dangerous AI (\u2018IAIA\u2019) would ideally do in this situation.&nbsp;<br><br>He splits this into three phases, with suggestions in a longer summary under the LessWrong -&gt; AI section below. This forecast implies actions we should take today, such as create an IAIA equivalent, selectively share info, and for labs to do outreach and advocacy (non-comprehensive list of Holden\u2019s suggestions).</p><p>&nbsp;</p><p>&nbsp;</p><h1>EA Forum</h1><h2>Philosophy and Methodologies</h2><p><a href=\"https://forum.effectivealtruism.org/posts/bvtAXefTDQgHxc9BR/just-look-at-the-thing-how-the-science-of-consciousness\"><u>Just Look At The Thing! \u2013 How The Science of Consciousness Informs Ethics</u></a></p><p><i>by algekalipso</i></p><p>Ethical theories often contain background assumptions about consciousness, personal identity, and valence. Instead of arguing these or theorizing around them, we can test them in reality. For instance, mixed valence states (pain&nbsp;<i>and</i> pleasure, together) are relevant to negative utilitarianism. From real life, we can observe that if a pleasant experience (eg. music) occurs during a painful experience (eg. stomach ache) it can mute the pain. This helps us refine the negative utilitarianism view, and opens up new questions like if this is still the case for extensive pains, or if they are always net negative - which we can also test empirically.</p><p>Other examples are given, including high-dose DMT as experiences that aren\u2019t successfully captured by many philosophical frameworks. The author argues this approach becomes more important as we open up futures with more complex valences and states of consciousness available.<br>&nbsp;</p><p>&nbsp;</p><h2>Object Level Interventions &amp; New Projects</h2><p><a href=\"https://forum.effectivealtruism.org/posts/W6gGKCm6yEXRW5nJu/quantified-intuitions-an-epistemics-training-website\"><u>Quantified Intuitions: An epistemics training website including a new EA-themed calibration app</u></a></p><p><i>by Sage</i></p><p><a href=\"http://quantifiedintuitions.org/\"><u>Quantified Intuitions</u></a> helps users practice assigning credences to outcomes with a quick feedback loop. Currently it includes a calibration game, and pastcasting (forecasting on resolved questions you don\u2019t know about already).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/t2z2jxSaM7X4TCeTh/announcing-the-nyu-mind-ethics-and-policy-program\"><u>Announcing the NYU Mind, Ethics, and Policy Program</u></a></p><p><i>by Sofia_Fogel</i></p><p>The&nbsp;<a href=\"https://sites.google.com/nyu.edu/mindethicspolicy/\"><u>NYU Mind, Ethics, and Policy Program</u></a> (MEP) will conduct and support foundational research about the nature and intrinsic value of nonhuman minds (biological and artificial). Several projects are currently underway, including a free public talk on whether large language models are sentient (signup&nbsp;<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdKX7ETjulX7mLhH0D9rxHFHVM29ug-mDWdfgVbhzJBQICFCA/viewform?usp=sf_link\"><u>here</u></a>), and an award and workshop seeking published papers on animal and AI consciousness (details<a href=\"https://sites.google.com/nyu.edu/mindethicspolicy/opportunities\"><u> here</u></a>).<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/YNcjDoH6DaHzfhWGb/the-next-ea-global-should-have-safe-air\"><u>The Next EA Global Should Have Safe Air</u></a></p><p><i>By joshcmorrison</i></p><p>Indoor air quality is a promising biosafety intervention, but experimental evidence on which methods to use is sparse. EA being an early adopter of interventions like air filters or UV lights, and recording outcomes, would help assess what works - and help the community reduce infections at the same time. EAG is a natural candidate for this due to its size. Other possibilities include EA office / coworking spaces or group houses.<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/AFgvA9imsT6bww8E3/announcing-the-rethink-priorities-special-projects-program\"><u>Announcing the Rethink Priorities Special Projects Program</u></a></p><p><i>by Rachel Norman</i></p><p>Rethink Priorities is launching a program to help start promising EA initiatives. Some will be internally incubated and later spun-off into independent organizations, others run externally will be fiscally sponsored and given operations support. The team is currently incubating / sponsoring eight projects ranging from insect welfare to community building in Brazil to AI alignment research, and is looking for expressions of interest for those interested in fiscal sponsorship or project incubation.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/tWawcXaNnLAihA2Fv/announcing-ea-pulse-large-monthly-us-surveys-on-ea\"><u>Announcing EA Pulse, large monthly US surveys on EA</u></a></p><p><i>by David_Moss, Jamie Elsey</i></p><p>Rethink priorities is running a monthly survey of the US aimed at understanding public perceptions of Effective Altruism and related cause areas, funded by FTX Future Fund.<br><br>This includes over time tracking of general attitudes, as well as ad-hoc questions such as testing support for particular policies or EA messaging. Requests for both sections are welcome - ideally by October 20th.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/Geh9DFZQnj9s5HjoW/the-hundred-billion-dollar-opportunity-that-eas-mostly\"><u>The Hundred Billion Dollar Opportunity that EAs mostly ignore</u></a></p><p><i>by JeremiahJohnson</i></p><p>Individual charitable donations are in the hundreds of billions eg. last year individuals, bequests, foundations and corporations gave an estimated $484.85 billion to charities. The largest proportions are religious donations, and educational donations to existing 3-4 year colleges. Only 6% is donated internationally. More public outreach, approachable arguments and asks, specialized charity evaluators and public attacks on practices like Ivy League university endowments could help direct this money more effectively.<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/zNS53uu2tLGEJKnk9/ea-s-brain-over-body-bias-and-the-embodied-value-problem-in#Context__A_bottom_up_approach_to_the_diversity_of_human_values_worth_aligning_with\"><u>EA\u2019s brain-over-body bias, and the embodied value problem in AI alignment</u></a></p><p><i>by Geoffrey Miller</i></p><p>AI alignment might benefit from thinking about our bodies\u2019 values, not just our brains\u2019. Behavior in humans is often an interplay eg. If we consider hunger and eating, this includes some cognition, but also interplay with the stomach, gut, hunger-related hormones etc. which in some sense have \u2018goals\u2019 like to keep blood glucose within certain bounds.</p><p>If we align AI with only our cognitive values, this has error modes. Eg. many like donuts - but our bodies usually prefer leafy greens. If an AI is trained to care for human bodies the way the body \u2018wants\u2019, and keep its systems running smoothly, this makes us safer from these failure modes. It also allows us to include preferences of infants, fetuses, those in comas, and others unable to communicate cognitively. We can learn these body values and goals via pushing forward evolutionary biology.<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/PauhAAw7Y5bHMawkT/shahar-avin-on-how-to-strategically-regulate-advanced-ai\"><u>Shahar Avin on How to Strategically Regulate Advanced AI Systems</u></a></p><p><i>by Micha\u00ebl Trazzi</i></p><p>Excerpts and links to discussion with Shahar Avin, a senior researcher at the Center for the Study of Existential Risk. Key points from excerpts include that:</p><ul><li>A lot of cutting-edge AI research is probably private</li><li>Security failures are unavoidable with big enough systems</li><li>Companies should be paying the red tape cost of proving their system is safe, secure &amp; aligned. Big companies are used to paying to meet regulations.</li><li>We should regulate now, but make it \u2018future ready\u2019 and updateable.<br><br>&nbsp;</li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/RkpdA8763yGtEovj9/two-reasons-we-might-be-closer-to-solving-alignment-than-it\"><u>Two reasons we might be closer to solving alignment than it seems</u></a></p><p><i>by Kat Woods, Amber Dawn</i></p><p>There\u2019s a lot of pessimism in the AI safety community. However, keep in mind:</p><ol><li>All of the arguments saying that it\u2019s hard to be confident that transformative AI isn\u2019t just around the corner also apply to safety research progress.&nbsp;</li><li>It\u2019s still early days and we\u2019ve had about as much progress as you\u2019d predict given that up until recently we\u2019ve only had double-digit numbers of people working on the problem.&nbsp;</li></ol><p><br>&nbsp;</p><h2>Opportunities</h2><p><a href=\"https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize\"><u>Announcing the Future Fund's AI Worldview Prize</u></a></p><p><i>by Nick_Beckstead, leopold, William_MacAskill, ketanrama, ab</i></p><p>The Future Fund believes they may be significantly under or over estimating catastrophic risk from AI, or that they are focusing on the wrong aspects of it. Since this affects the distribution of hundreds of millions of dollars, they are offering prizes of up to $1.5M for arguments that significantly shift their credences on when and how AGI will be developed and its chance of catastrophic effect. A secondary goal is to test the efficacy of prizes at motivating important new insights. Smaller prizes are also available. Entries are due Dec 23rd.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/fHqP9NmTvALTTQSv9/cea-s-events-team-is-hiring\"><u>CEA's Events Team is Hiring!</u></a></p><p><i>by Amy Labenz</i></p><p>Hiring for an EA Global Events Associate, Retreats Associate, and Community Events Associate. The team has grown rapidly, and on track to facilitate 4x the connections in the EA community this year as in 2021. Apply by October 11th.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/CetKqoq3qKo5ggZ4f/usd13-000-of-prizes-for-changing-our-minds-about-who-to-fund\"><u>$13,000 of prizes for changing our minds about who to fund (Clearer Thinking Regrants Forecasting Tournament)</u></a></p><p><i>by spencerg</i></p><p>13K of prizes up for grabs. Win some by either changing Clearer Thinking\u2019s mind about which of 28 finalists to fund (and by how much), or being a top 20 forecaster for what projects they end up funding.</p><p>&nbsp;<br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/yKX8btfwih2FpecaT/open-position-s-risk-community-manager-at-clr\"><u>[Open position] S-Risk Community Manager at CLR</u></a></p><p><i>by stefan.torges</i></p><p>This role will be across areas like event &amp; project management, 1:1 outreach &amp; advising calls, setting up &amp; improving IT infrastructure, writing, giving talks, and attending in-person networking events. Previous community building experience is helpful but not required. Deadline Oct 16th.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/noDYmqoDxYk5TXoNm/usd5k-challenge-to-quantify-the-impact-of-80-000-hours-top\"><u>$5k challenge to quantify the impact of 80,000 hours' top career paths</u></a></p><p><i>by NunoSempere</i></p><p>$5k prize pool for quantitative estimates of the value of some or all of 80,000 hours' top 10 career paths. Flexibility on units (eg. QALYs, % x-risk reduction) and methodology. Deadline Nov 1st.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/bvK44CdpG7mGpQHbw/the-usd100-000-truman-prize-rewarding-anonymous-ea-work\"><u>The $100,000 Truman Prize: Rewarding Anonymous EA Work</u></a></p><p><i>by Drew Spartz</i></p><p>The Truman Prize, now live on EA prize platform Superlinear, recognizes Effective Altruists with $5,000-$10,000 prizes for declining credit in order to increase their impact, in ways that can't be publicized directly. Submissions are&nbsp;<a href=\"https://www.super-linear.org/prize?recordId=reccomnC7E6SPaK8P/#list2\"><u>now open</u></a>.</p><p><br>&nbsp;</p><h2>Community &amp; Media</h2><p><a href=\"https://forum.effectivealtruism.org/posts/SdjFiM5dj2zxvLwJF/let-s-advertise-infrastructure-projects\"><u>Let's advertise infrastructure projects</u></a></p><p><i>by Arepo</i></p><p>There are many projects providing free or cheap support to EAs globally, but aren\u2019t well known. This post and comment section aims to collect them. Multiple examples are linked in each of the following sections: coworking and socializing spaces, professional services, coaching, financial support.<br>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/F4DxPrfmnEtEwhPJu/levels-of-donation\"><u>Levels of donation</u></a></p><p><i>by vipulnaik</i></p><p>Personas of donors by donation amount, eg. a retail donor (&lt;1K) is less likely to care about transaction costs or to investigate charities. Each level is separated by a factor of 10. The author also discusses how a person might move up levels (donate substantially more) eg. via increasing income or pooling donations with others.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/mjvWYNKwFRLQMMsZ7/ea-for-people-with-non-technical-skillsets\"><u>EA for people with non-technical skillsets</u></a></p><p><i>by Ronja</i></p><p>EA is big, and we need a wide range of skills. This isn\u2019t always obvious due to the visibility of AI safety and biosecurity discussions. Examples include: operations management, design, communications, policy, historical or behavioral research. There\u2019s also EAs using their existing niche eg. Kikiope Oluwarore, a veterinarian who co-founded healthier hens, or Liv Boeree, a poker player who uses her network to encourage other poker players to donate.<br><br>The author suggests increasing visibility of EAs doing this wide range of work eg. via a \u2018humans of EA\u2019 monthly feature.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/dcBfdecnm5gzntshZ/author-rutger-bregman-about-effective-altruism-and\"><u>Author Rutger Bregman about effective altruism and philanthropy</u></a></p><p><i>by Sebastian Schwiecker</i></p><p>Linkpost for an interview of Rutger Bregman on his personal view of philanthropy. He\u2019s a historian and author good at reaching people outside the EA community eg. being mentioned more than any other individual in&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/38GG44y2hYLovADeK/effektiv-spenden-review-of-the-year-2021\"><u>Effektiv Spenden</u></a>\u2019s post donation survey.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/2CwsQai9hHsa2CDJ8/the-mistakes-of-focusing-on-student-outreach-in-ea\"><u>The Mistakes of Focusing on Student Outreach in EA</u></a></p><p><i>by DavidNash</i></p><p>EA started in universities, and community building efforts have heavily focused there. Even if students are on average more receptive and flexible, continuing this trend can be a mistake because it risks an image of EA as a \u2018youth movement\u2019, losing out on important skills and networks from experienced professionals, and creates a lack of mentorship.</p><p>The author suggests redirecting resources at the margin by encouraging general community building, skill building, or object-level work more strongly with students (over becoming university group organizers).</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/YCMgg6x6zWJmran5L/criticism-of-the-80k-job-board-listing-strategy\"><u>Criticism of the 80k job board listing strategy</u></a></p><p><i>by Yonatan Cale</i></p><p>The 80K job board isn\u2019t just highly impactful jobs - it also lists jobs good for career capital. In an informal twitter poll, 55% of EAs weren\u2019t aware of this and believed it important. The author suggests only including high impact jobs, allowing community discussion of impact level, and better communicating current state.<br><br>Kush_kan from 80K comments they plan to visually distinguish top impact roles, update their job board tagline and FAQ, link orgs EA forum pages, and add a new feedback form to help address this. They also note most roles are there for a mix of impact and career capital reasons.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/hfnuwh6miJ3yn2Jpq/what-do-ai-safety-pitches-not-get-about-your-field\"><u>What Do AI Safety Pitches Not Get About Your Field?</u></a></p><p><i>by Aris Richardson</i></p><p>Arguments that misunderstand a field can reduce credibility and put experts from those fields off. The question author wants to collect examples to minimize this with AI Safety. Responses include:</p><ul><li>Psychology (no clear definition of \u2018intelligence\u2019 that encompasses eg. cultural intelligence)</li><li>Economics (forecasting double-digit GDP growth based on AI feels dubious)</li><li>Anthropology (the \u2018AI is to us what we are to chimps\u2019 misunderstands how humans acquired power / knowledge ie. cumulative cultural changes over time)</li><li>Politics (nonstarter political ideas)</li><li>Philosophy (muddled use of concept of \u2018agency\u2019)<br><br>&nbsp;</li></ul><p><a href=\"https://forum.effectivealtruism.org/posts/b4wJcm2LaWCjsKswF/summarizing-the-comments-on-william-macaskill-s-nyt-opinion\"><u>Summarizing the comments on William MacAskill's NYT opinion piece on longtermism</u></a></p><p><i>by West</i></p><p>Numerical summary of 300 comments on MacAskill\u2019s NYT piece on longtermism. 60 were skeptical, 42 were positive. The most common skepticism themes were \u2018Our broken culture prevents us from focusing on the long-term\u2019 (20) and \u2018We're completely doomed, there's no point\u2019 (16). Few commenters engaged on biorisk or AI, associating long-term future concern with the environmental concern. Many also assumed long-term referred to 2-7 generations.<br><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/vcwrEmCLTC5xqkWnX/optimizing-seed-pollen-ratio-to-spread-ideas\"><u>Optimizing seed:pollen ratio to spread ideas</u></a></p><p><i>by Holly_Elmore</i></p><p>EA community building often talks about the \u2018funnel\u2019, and more recently focus has been on creating core EAs in that funnel. Another model is we have outreach that\u2019s like creating seeds (putting lots of resources into few promising proto-EAs) and like creating pollen (low resource per person but spread widely). Like plants, we need to get the ratio right. The author suggests we\u2019re currently too weighted towards seeds and should be doing more pollen-like efforts - spreading ideas to a wide audience. Sometimes it will stick and someone will become a core EA, despite not having much dedicated support.</p><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/2jWLvBH8abzscDbPs/announcing-effective-dropouts\"><u>Announcing \u201cEffective Dropouts\u201d</u></a></p><p><i>by Yonatan Cale, Gavin, Vardev, Jonny Spicer</i></p><p>Author\u2019s tl;dr: \u201cEffective Dropouts\u201d is meant to be a casual fun excuse for reminding people that dropping out of a degree can be a good decision, as a counter to all the existing pressure for degrees being the default/only/obvious way. The rest of the post is mostly a joke.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/tnzLTnBQLEDv9zygo/establishing-oxford-s-ai-safety-student-group-lessons-learnt\"><u>Establishing Oxford\u2019s AI Safety Student Group: Lessons Learnt and Our Model</u></a></p><p><i>by CharlieGriffin, juliakarbing</i></p><p>The group aimed to increase the number and quality of technical people pursuing a career in AI safety research. Key lessons included:</p><ul><li>Using an AI audience instead of an EA one greatly increased the technical audience. AI safety was still an easy sell without EA / longtermist philosophy.</li><li>Socials after talks were of high value.</li><li>Expert supervisors providing research questions and limited (~1hr per week) support to groups of students was an effective development opportunity.<br>&nbsp;</li></ul><p><br><a href=\"https://forum.effectivealtruism.org/posts/L3WPuztkSMohBTWqZ/etgp-2022-materials-feedback-and-lessons-for-2023\"><u>ETGP 2022: materials, feedback, and lessons for 2023</u></a></p><p><i>by trammell</i></p><p>Lecture slides and exercises from the course \u2018Topics in Economic Theory and Global Prioritization\u2019, designed primarily for economics graduate students considering careers in global priorities research. The program included lunches, social opportunities, and shared accommodations, and will run again in 2023.<br><br>All participants were very satisfied or satisfied with the course, and over 18 / 34 wrote it may have or definitely changed their plans. Social aspects were the favorite portion.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/hur3ejqvA2QmYEfZa/my-personal-story-of-rejection-from-ea-global\"><u>My Personal Story of Rejection from EA Global</u></a></p><p><i>by Constance Li</i></p><p>The author has been involved with EA for over a decade, making significant life path changes such as ETG (medicine), launching a 60K+ per month business and donating profits, and running successful cage-free egg campaigns. They\u2019ve felt alternately welcomed (EAGx Virtual, local groups, and a supportive CEA member) and disillusioned (multiple EAG rejections with lack of context and unsatisfactory replies) by the EA community.</p><p>The author suggests improvements in the EAG admissions process:</p><ul><li>Consider how rejection can feel like a judgment of applicants\u2019 worth. Send better responses, which could include:<ul><li>Links to sensitive explanations of common reasons for rejection.</li><li>A semi-automated system for feedback eg. flagging applications by primary reason for rejection and sending templated emails based on that.</li><li>Training CEA staff on authentic relating / non-violent communication methods.</li></ul></li><li>Analyze the cost of rejection, experiment with interventions to reduce it, publish results.</li><li>Address the power of a small group (CEA) in admissions via transparency, blinding admissions, and potentially renaming EAG (eg. to CEA Global).</li></ul><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/MaN23i6XsSkkgoLY9/why-wasting-ea-money-is-bad\"><u>Why Wasting EA Money is Bad</u></a></p><p><i>by Jordan Arel</i></p><p>EA spends on luxuries to save time, increase productivity or community build (eg. flights, uber eats, fancy retreats). This is usually justified with the argument that some EA work can be incredibly impactful, to where 30m of time might be worth more than a year of someone\u2019s life (which costs ~$200 via donations). However, the author argues frugality is important to EA\u2019s internal and external reputation, to a healthy community, and can be more motivating than luxury.</p><p>&nbsp;</p><p><br><a href=\"https://forum.effectivealtruism.org/posts/ivjgAZRM7No4fnDgo/ceri-research-symposium-presentations-incl-youtube-links\"><u>CERI Research Symposium Presentations (incl. Youtube links)</u></a></p><p><i>by Will Aldred</i></p><p>Links to talks by 22 fellows from CERI (Cambridge Existential Risk Initiative)\u2019s research symposiums over the past 2 years, split by subject areas. These were given by summer research fellows, and cover AI risk (technical &amp; governance), biorisk, nuclear risk, extreme climate change, and meta x-risk.</p><p><br>&nbsp;</p><p><a href=\"https://forum.effectivealtruism.org/posts/8QfQcFyj6aGNM78kz/learning-from-matching-co-founders-for-an-ai-alignment\"><u>Learning from Matching Co-Founders for an AI-Alignment Startup</u></a></p><p><i>by Patrick Gruban, LRudL, Maris Sala, simeon_c, Marc Carauleanu</i></p><p>Beth Barnes shared an idea and proposal for human data for AI alignment with multiple people at EAG London. Six interested people then self-organized cofounder selection.</p><p>Their steps were to ask CE for advice, answer&nbsp;<a href=\"https://proof-assets.s3.amazonaws.com/firstround/50%20Questions%20for%20Co-Founders.pdf\"><u>\u201850 questions to explore with cofounders\u2019</u></a>, work weekly in pairs on test tasks for five weeks, and meet for an in-person workshop to finalize preferences and choose a co-founding pair.</p><p>Participants rated highest the pair working, and suggested focusing it on customer interviews to better define the intervention (in this case, the final cofounder pair dropped the project after doing this stage post cofounder matching). The reveal of preferences on who to co-found with was also highly rated and successfully selected one pair. Other stages could have been cut or done informally to speed up the process.</p><p><br>&nbsp;</p><h1>LW Forum</h1><h2>AI Related</h2><p><a href=\"https://www.lesswrong.com/posts/jMRuwXdC6NPFw8HLq/quintin-s-alignment-papers-roundup-week-2\"><u>Quintin's alignment papers roundup - week 2</u></a></p><p><i>by Quintin Pope</i></p><p>Weekly themed round-ups of papers, published each Monday. They include links, abstracts and Quintin\u2019s thoughts. This week\u2019s theme is the structure/redundancy of trained models, as well as linear interpolations through parameter space.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/eymFwwc6jG9gPx5Zz/summaries-alignment-fundamentals-curriculum\"><u>Summaries: Alignment Fundamentals Curriculum</u></a></p><p><i>by Leon Lang</i></p><p>Linkpost for the author\u2019s summaries of most core readings, and many further readings, from the alignment fundamentals curriculum composed by Richard Ngo.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/9TWReSDKyshfA66sz/alignment-org-cheat-sheet\"><u>Alignment Org Cheat Sheet</u></a></p><p><i>by Akash, Thomas Larsen</i></p><p>Describes the work of AI Safety researchers, research organizations, and training programs in one sentence each. Not intended to be comprehensive, but includes 15 researchers / research orgs, and 8 training / mentoring programs.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/mSDwPeqAzYk79vLiA/understanding-infra-bayesianism-a-beginner-friendly-video\"><u>Understanding Infra-Bayesianism: A Beginner-Friendly Video Series</u></a></p><p><i>by Jack Parker, Connall Garrod</i></p><p>New video series that covers what infra-Bayesianism is at a high level and how it's supposed to help with alignment, assuming no prior knowledge. It also targets those who want to gain mastery of the technical details behind IB so that they can apply it to their own alignment research, and is good preparation for more technical sources like the&nbsp;<a href=\"https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa\"><u>original IB sequences</u></a>.<br><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/vZzg8NS7wBtqcwhoJ/nearcast-based-deployment-problem-analysis\"><u>Nearcast-based \"deployment problem\" analysis</u></a></p><p><i>by HoldenKarnofsky</i></p><p>The AI deployment problem is the question of how and when to (attempt to) build and deploy powerful AI, when unsure about safety and how close other actors are to deploying. This post assumes a major AI company (\u2018Magma\u2019) thinks it is 6months - 2years to TAI using human feedback on diverse tasks (HFDT), and what both it and an organization dedicated to tracking and censoring dangerous AI (\u2018IAIA\u2019) would ideally do in this situation.&nbsp;<br><br>He splits this into three phases, with suggestions as below. This forecast implies actions we should take today, such as create an IAIA equivalent, selectively share info, and for labs to do outreach and advocacy (non-comprehensive list of Holden\u2019s suggestions).</p><p><strong>Phase 1: Before Magma develops aligned TAI</strong></p><p>Magma should:</p><ul><li>Focus on developing aligned TAI asap, before other actors</li><li>Reduce risk from other actors - prioritize internal security, reduce \u2018race\u2019 pressure by making deals with other AI companies, and educate other players on misaligned AI risk.</li></ul><p>IAIA should:</p><ul><li>Monitor all major AI development projects, looking for signs of dangerous AI, ensuring sufficient safety measures, information security and selective info sharing, and taking action where they find issues.</li><li>Serve as a hub to share public goods such as education on AI alignment or coordinating deals between different orgs.</li></ul><p>Selective info sharing is important for both parties. \u2018Dual-use\u2019 information (helpful for avoiding misaligned AI, and for making powerful AI) should be shared more readily with cautious parties.</p><p>&nbsp;</p><p><strong>Phase 2: Magma has developed aligned TAI, but other actors may develop misaligned TAI</strong></p><p>Magma &amp; IAIA should:</p><ul><li>Focus on deploying AI systems that can reduce the risk that other actors cause a catastrophe (eg. that can align more powerful systems, patch cybersecurity holes, cover more uses to reduce space for other AIs, detect and obstruct misaligned AIs, enforce safety procedures, or offer general guidance)</li><li>Reduce misuse risk of the aligned TAI eg. deploy with appropriate oversight, ensure users have good intentions, and bake in some resistance to misuse.</li></ul><p>If despite this, dangerous AI is catching up, Magma &amp; IAIA should consider drastic action:</p><ul><li>Recommend governments suppress AI development by any means necessary.</li><li>Develop AI capabilities to persuade or threaten actors to achieve the above - even if that holds some misalignment risk in itself.</li></ul><p><strong>Phase 3: No actors are likely to be able to develop misaligned TAI</strong></p><p>Magma &amp; IAIA should:</p><ul><li>Focus on avoiding lock-in of bad worlds where one player has seized global power.</li><li>Broker peaceful compromises or coalitions.</li><li>Design AIs that may help humans with moral progress.</li></ul><p>&nbsp;</p><p>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/jP9cKxqwqk2qQ6HiM/towards-deconfusing-wireheading-and-reward-maximization\"><u>Towards deconfusing wireheading and reward maximization</u></a></p><p><i>by leogao</i></p><p>A response to&nbsp;<a href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\"><u>\u201cReward is not the optimization target\u201d</u></a>. The author argues that it\u2019s not possible for reinforcement learning policies to care about \u201creward\u201d in an embedded setting, but wireheading in RL agents is still possible because wireheading doesn\u2019t mean \u201cthe policy has reward as its objective\u201d.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/boBZkTqPdboX5u7g9/public-facing-censorship-is-safety-theater-causing\"><u>Public-facing Censorship Is Safety Theater, Causing Reputational Damage</u></a></p><p><i>by Yitz</i></p><p>Censorship in AI (for instance, preventing the user from viewing model results that contain swear words or real faces) is an issue because:<br>a) like other forms of social censorship, it can have negative social effects. Eg. see controversy over social media censorship.</p><p>b) It confuses the term \u2018AI Safety\u2019, such that people associate it with censorship under a false banner of \u2018safety\u2019, and have a negative view of the field overall.</p><p>The author suggests reducing censorship in public-facing models, using differentiated terminology for different types of AI safety, and more media interaction to direct opinion. A top comment notes \u2018safety\u2019 in the online context is already strongly associated to prevention of things like pornography and racism, and AI security may be a better term to use.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/CTh74TaWgvRiXnkS6/toy-models-of-superposition\"><u>Toy Models of Superposition</u></a></p><p><i>by evhub</i></p><p>Linkpost for a new Anthropic paper which explores superposition - where one neuron in a net is used to capture several unrelated concepts, a problem for interpretability.</p><p><br>&nbsp;</p><h2>Research &amp; Productivity Advice</h2><p><a href=\"https://www.lesswrong.com/posts/6LzKRP88mhL9NKNrS/how-my-team-at-lightcone-sometimes-gets-stuff-done\"><u>How my team at Lightcone sometimes gets stuff done</u></a></p><p><i>by jacobjacob</i></p><p>Lightcone\u2019s founder on practices that seem to have helped with high output. These primarily revolve around removing blockers, and includes:</p><ul><li>Have a single decision maker to resolve ties</li><li>Have clear top priorities, and set aside time where everyone is focused on just them (and time specifically for other stuff eg. a day for less important meetings)</li><li>Work together (team in same room, work in pairs or trios sometimes, no-one remote, everyone full time, if you\u2019re blocked ask for help immediately, and if you\u2019re tagged respond immediately)</li><li>Keep context high (lots of 1-1s, never DM in slack - use a public channel named for your 1-1, make time for chit-chat)<br><br>&nbsp;</li></ul><p><a href=\"https://www.lesswrong.com/posts/ma7FSEtumkve8czGF/losing-the-root-for-the-tree\"><u>Losing the root for the tree</u></a></p><p><i>by Adam Zerner</i></p><p>Story-like examples of getting lost in the details, and why it\u2019s worthwhile to step back and ask \u2018what was this aiming at? Is it the most effective way to get it?\u2019 Hierarchical tree representations with lines weighted on impact can be a clear way to visualize this.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring\"><u>You Are Not Measuring What You Think You Are Measuring</u></a></p><p><i>by johnswentworth</i></p><p>When you measure a metric, you usually don\u2019t learn what you think you do. For instance, a company measures click-through-rate on two sign-up flows to determine which info is more important to put upfront, but really the CTR changed due to latency differences in the two options. Solution: measure lots of things, and you might figure out what\u2019s really going on. Some tools are better at this (eg. a microscope gives heaps of info, an AB test just one data point). In addition to your own experimental design, keep this in mind when reading other\u2019s research - don\u2019t just take in the abstract and the p-values, look at the data and cross-reference other papers to build a picture of what\u2019s going on.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/AcbtSuGYB4jWGYk6x/some-notes-on-solving-hard-problems\"><u>Some notes on solving hard problems</u></a></p><p><i>by Joe Rocca</i></p><p>Tips for solving hard problems:</p><p><strong>Start simple</strong></p><ul><li>Start with a minimal problem and solution, and build from there, incrementally adding cases and generalizing. Don\u2019t jump wildly from path to path.</li><li>Start with things you\u2019re (almost) sure about, and ask \u2018what do they imply?\u2019</li><li>Abstract first, then make it concrete (eg. a real world thought experiment).</li></ul><p><strong>Question assumptions</strong></p><ul><li>See it from multiple perspectives / mental models.</li><li>Take a break. It might help you realize a mistaken assumption.</li><li>If something feels promising but you hit a dead-end, keep pushing! Question fundamental assumptions to make it work.</li></ul><p><strong>Make it easier to hold in your head</strong></p><ul><li>Draw it - diagrams are great for reasoning.</li><li>Name your concepts.</li><li>Surround yourself with context - lists of key questions, considerations, milestones - to help pull yourself out of the weeds.</li></ul><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/EgfmaEg4vanE5EbCP/twitter-polls-evidence-is-evidence\"><u>Twitter Polls: Evidence is Evidence</u></a></p><p><i>by Zvi</i></p><p>Breakdown of an argument on twitter about twitter polls. One user argues they aren\u2019t representative and contain response bias, and therefore they offer actively \u2018bad\u2019 (ie. misleading) evidence and shouldn\u2019t be used. The post author argues that any evidence is good, as long as you update on it properly considering the context (such as the sample, any likely bias), and therefore we should do more twitter polls since they\u2019re cheap and neat. There are methods to help with bias, such as comparing results between polls by the same user.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/6BPqAbx9woMfpyJF5/a-game-of-mattering\"><u>A game of mattering</u></a></p><p><i>by KatjaGrace</i></p><p>A productivity hack for when the amount of tasks to-do feels overwhelming. The author organizes the tasks into time boxes during the day, and tries to \u2018collect\u2019 as many as possible by completing them before the end of their box. If they complete one early, they can go back to try and retrieve an earlier one. This helps by focusing them on one task at a time, and making every minute feel meaningful.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/foEr8gtkpzmjkvcDp/methodological-therapy-an-agenda-for-tackling-research\"><u>Methodological Therapy: An Agenda For Tackling Research Bottlenecks</u></a></p><p><i>by adamShimi, Lucas Teixeira, remember</i></p><p>Interviews with AI alignment researchers found they reported the following as difficult and costly: running experiments, formalizing intuitions, unifying disparate insights into a coherent frame and proving theorems. Conjecture\u2019s epistemology team questions whether these four things are the right approaches, or just \u2018cached patterns\u2019 on how to do research that can be adjusted to better suit the context. They plan to consult with more alignment researchers, question, refine and replace cached patterns that aren\u2019t serving their purpose, and therefore improve alignment research. They call this \u2018methodological therapy\u2019, and have developed some framing questions to kick off this work (eg. \u2018what are the researchers trying to accomplish?\u2019)</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/sYzH9h6zkxSAmKFEq/scraping-training-data-for-your-mind#Introspection\"><u>Scraping training data for your mind</u></a></p><p><i>by Henrik Karlsson</i></p><p>\u201cTo get good at something\u2014parenting, writing code, doing research\u2014you need to internalize examples of prime achievements in that field.\u201d Look for awards, review articles of scientific fields, who is cited etc. to generate a list of those at the top of your domain. Then study those people\u2019s processes - your brain will learn from the training data.</p><p>Or in short: Identify your aim. Locate the prime achievements in this field, and observe them in a messy living context. Reflect on how your understanding of the world has changed. Is this a fitting method for you? If not, course correct.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/vXtEegPirxgxWNdhG/fake-qualities-of-mind\"><u>Fake qualities of mind</u></a></p><p><i>by Kaj_Sotala</i></p><p>If you\u2019re not feeling motivated, you might push yourself through something instead. But if that becomes the norm, you can forget the \u2018pushing yourself\u2019 was a substitution for true motivation to begin with. This pattern can happen in many areas eg. \u2018doing\u2019 empathy vs. \u2018having\u2019 empathy. A common trigger is that stress blocks the useful / happy mind state, so you substitute with a forced one, stay stressed and can\u2019t get it back. The first step to fixing this is to notice it.</p><p><br>&nbsp;</p><h2>Other</h2><p><a href=\"https://www.lesswrong.com/posts/CQsEwAyJP6NYvKZw6/gene-drives-why-the-wait\"><u>Gene drives: why the wait?</u></a></p><p><i>by Metacelsus</i></p><p>Gene drives work by transmitting an allele to 100% of offspring, soon covering the whole population. In 2018, Crispr allowed the Cristani lab to create a gene drive in a lab environment that suppressed all reproduction in the only malaria-transmitting mosquito species. However, this has still not been released in the wild, for two reasons:</p><ol><li>Possibility of the mosquitoes generating resistant alleles - so the population is not eliminated, and future gene drives are harder. (being addressed via larger tests to ensure this doesn\u2019t occur)</li><li>If done without good local and government buy-in, could cause a backlash that restricts the development of other gene drives. (being addressed via building community consensus)</li></ol><p>In the meantime, 1.6K die every day from Malaria. Is the wait worth it?</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/KTEciTeFwL2tTujZk/lw-petrov-day-2022-monday-9-26\"><u>LW Petrov Day 2022 (Monday, 9/26)</u></a></p><p><i>by Ruby</i></p><p>Petrov Day commemorates when Petrov avoided a nuclear war by reporting a false alarm, when sensors seemed to show a nuclear attack. LW will celebrate by having a button which can bring down the site\u2019s frontpage for all, available anonymously to all existing users with non-negative karma.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/FuZ7MoR3dJEJuoRbN/announcing-usd5-000-bounty-for-ending-malaria\"><u>Announcing $5,000 bounty for ending malaria</u></a></p><p><i>by lc</i></p><p>The author will give 5K to anyone who reduces malaria by 95%+ without causing negative effects that outweigh that, because it seems like this should have a reward.</p><p><br>&nbsp;</p><p><a href=\"https://www.lesswrong.com/posts/sksP9Lkv9wqaAhXsA/orexin-and-the-quest-for-more-waking-hours\"><u>Orexin and the quest for more waking hours</u></a></p><p><i>by ChristianKl</i></p><p>People with the DEC2-P384R mutation produce more prepro-orexin and have a reduced need for sleep. Plausible reasons it wasn\u2019t evolutionarily selected for include a higher food need and metabolism, less anxiety / more risk-taking. Due to orexin being a natural human protein, it can\u2019t be patented so hasn\u2019t been studied in detail. Trials are underway for an orexin antagonist (binds to its receptors) for use in treating Narcolepsy. The author suggests we fund direct orexin supplementation studies, originally for Narcolepsy to get FDA approval, and then for reducing general sleep needs.</p><p>&nbsp;</p><h2>Didn\u2019t Summarize</h2><p><a href=\"https://www.lesswrong.com/posts/WNpvK67MjREgvB8u8/do-bamboos-set-themselves-on-fire\"><u>Do bamboos set themselves on fire?</u></a>&nbsp;<i>by Malmesbury</i></p><p><a href=\"https://www.lesswrong.com/posts/CKgPFHoWFkviYz7CB/the-redaction-machine\"><u>The Redaction Machine</u></a>&nbsp;<i>by Ben</i> (original fiction story about a world where you can wind someone back to a prior state)</p><p><a href=\"https://www.lesswrong.com/posts/eS7LbJizE5ucirj7a/dath-ilan-s-views-on-stopgap-corrigibility\"><u>Dath Ilan's Views on Stopgap Corrigibility</u></a>&nbsp;<i>by David Udell</i></p><p><a href=\"https://www.lesswrong.com/posts/eDicGjD9yte6FLSie/interpreting-neural-networks-through-the-polytope-lens\"><u>Interpreting Neural Networks through the Polytope Lens</u></a>&nbsp;<i>by Sid Black, Lee Sharkey, Connor Leahy, beren, CRG, merizian, EricWinsor, Dan Braun</i></p><p><a href=\"https://www.lesswrong.com/posts/Sq75dZniBctPyTrWX/funding-is-all-you-need-getting-into-grad-school-by-hacking\"><u>Funding is All You Need: Getting into Grad School by Hacking the NSF GRFP Fellowship</u></a></p><p><i>by hapanin</i><br>&nbsp;</p><p>&nbsp;</p><h1>This Week on Twitter</h1><h2>AI</h2><p>OpenAI trained an AI that \u201capproaches human-level robustness and accuracy on English speech recognition\u201d.&nbsp;<a href=\"https://twitter.com/OpenAI/status/1572629923017400326?s=20&amp;t=ZHMxtbyFZPvdodqrGpiDAg\"><u>(tweet)</u></a>&nbsp; Other tweets note it works well even with fast speech and back-tracking.&nbsp;<a href=\"https://twitter.com/karpathy/status/1573123790795837440?s=20&amp;t=ZHMxtbyFZPvdodqrGpiDAg\"><u>(tweet)</u></a><br><br>Deepmind released a new chatbot, Sparrow, trained on human feedback to follow rules like not impersonating humans, and to search the internet for helpful info.&nbsp;<a href=\"https://twitter.com/DeepMind/status/1572950758575808512?s=20&amp;t=ZHMxtbyFZPvdodqrGpiDAg\"><u>(tweet)</u></a><br><br>CSET shares that China is building \u201ccyber ranges\u201d to allow cybersecurity teams to test new tools and practice attack and defense.&nbsp;<a href=\"https://cset.georgetown.edu/publication/downrange-a-survey-of-chinas-cyber-ranges/\"><u>(article)</u></a></p><p>&nbsp;</p><h2>National Security</h2><p>Vladimir Putin has threatened use of nuclear weapons, saying Russia had \u2018lots of weapons to reply\u2019 to threats, and that he was \u2018not bluffing\u2019.&nbsp;<a href=\"https://www.theguardian.com/world/2022/sep/21/putin-announces-partial-mobilisation-in-russia-in-escalation-of-ukraine-war\"><u>(article)</u></a> The US and allies have threatened catastrophic consequences if so.&nbsp;<a href=\"https://www.ft.com/content/e7212f93-6635-40eb-a356-c8e1bb14cea3?shareType=nongift\"><u>(article)</u></a>&nbsp;<br><br>Current metaculus forecasts put the chances of Russia launching a nuclear weapon before 2023 at 3%, after a brief period directly after the announcement where they rose to 6%.&nbsp;<a href=\"https://www.metaculus.com/questions/12591/nuclear-detonation-in-ukraine-by-2023/\"><u>(metaculus forecast)</u></a> The metaculus forecast for chances of&nbsp;<a href=\"https://www.metaculus.com/questions/3150/no-non-test-nuclear-detonations-before-2035/\"><u>non-test nuclear detonation before 2035</u></a> (anywhere globally) have risen to 27%.</p><p>Russia has initiated partial conscription, mobilizing ~300K of its reserve forces. Thousands flee Russia in response.&nbsp;<a href=\"https://www.metaculus.com/questions/12532/russia-general-mobilization-before-2023/\"><u>(metaculus forecast)</u></a>&nbsp;<a href=\"https://www.csis.org/analysis/what-does-russias-partial-mobilization-mean\"><u>(article)</u></a></p><p>&nbsp;</p><h2>Science</h2><p>NASA successfully crashed a spacecraft into an asteroid, to test the ability to deflect a problematic one.&nbsp;<a href=\"https://www.nytimes.com/live/2022/09/26/science/nasa-dart-asteroid-mission\"><u>(tweet)</u></a></p><p>Scientists engineered mosquitoes that slow the growth of malaria-causing parasites in their guts, preventing transmission to humans - though reduces mosquito lifespan, so is likely not viable for large-scale.&nbsp;<a href=\"https://twitter.com/SteveStuWill/status/1573439605655097344?s=20&amp;t=Tfgmox89sRTfr_AajrKpdQ\"><u>(tweet)</u></a></p><p>The White House announced $2B funding to launch a National Biotechnology and Biomanufacturing Initiative which aims to foster innovation, strengthen supply chains, mitigate biological risks and improve health outcomes.&nbsp;<a href=\"https://twitter.com/LongResilience/status/1573334228003168256?s=20&amp;t=Y9bJv1CpNrza5clGaqNzmg\"><u>(tweet)</u></a></p>", "user": {"username": "GreyArea"}}, {"_id": "NzC7DcBkmaw4zy36i", "title": "Funds to fight climate change", "postedAt": "2022-10-02T05:32:53.564Z", "htmlBody": "<p>Hi everyone I am new here!!</p><p>I was wondering if this is the kind of place I can find people interested in funding our tech.</p><p>We finished the research and development state of our patented tech, now we need resources in order to finish our first commercial product.</p><p>Our disruptive tech can increase energy efficiency of any energy transformation system may it be in heavy transportation, heavy machinery, industrial machinery and in fuel based power generators or renewable power generators. This means our tech decreases fuel usage by HALF in those sectors and improves renewable energy sources.</p><p>We really need help, I have tried to reach out big companies, celebrities, foundations that are \"fighting\" climate change but no one will even answer.</p><p>I am reaching you guys so we can get the help we need in order to do our part as men of science and fight our own extinction. &nbsp;</p>", "user": {"username": "Alan Coumans"}}, {"_id": "A4kpWmxCEHoSa9dxs", "title": "Why don't all EA-suggested organizations disclose salary in job descriptions?", "postedAt": "2022-09-28T22:42:52.738Z", "htmlBody": "<h1>Summary</h1><p>I believe every organization that is part of the wider EA community<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefq7k84tuk00b\"><sup><a href=\"#fnq7k84tuk00b\">[1]</a></sup></span>&nbsp;, should disclose salary in the job description. Especially those listed on popular EA places, such as <a href=\"https://80000hours.org/job-board/\">job boards</a>, <a href=\"https://forum.effectivealtruism.org\">related forums</a> or <a href=\"https://www.facebook.com/groups/1062957250383195\">social media groups</a>.</p><p>Reason: Applying to jobs and processing those applications takes a lot of time. This time is essentially wasted if a certain (low) salary would prevent an applicant from being able to accept the position.</p><p>Organizations who value using (applicant's) time efficiently should hence disclose salary up front.</p><h1>Request</h1><p>I didn't immediately find another post on the EA forum answering this question. <a href=\"https://forum.effectivealtruism.org/posts/gicYG5ymk4pPzrKAd/doing-good-is-a-privilege-this-needs-to-change-if-we-want-to#Make_sure_that_your_org_is_paying_a_living_wage\">There is this post </a>outlining the same issue, but it doesn't mention the arguments.&nbsp;<br>I would be grateful if somebody could point me to a list of reasons why that should not be the case. If there are great arguments to hide salaries, which outweigh the benefits of disclosing it, I'd be happy to change my view.&nbsp;</p><p>If the general consensus is that all organizations should disclose salary, then why isn't this the case everywhere? For example, vacancy locations recommended by EA organizations could have salary as a listing criteria. Or at least strongly encourage it, as well as making the amount or non-disclosure visible right away.</p><h2>More information</h2><p>Applying to jobs takes a lot of time, if you include time for these examples:</p><ul><li>searching for open positions (e.g. browsing job board or Facebook groups)</li><li>filtering jobs for requirements you meet</li><li>learning more about the respective organizations</li><li>reading job descriptions</li><li>tailoring your CV to the job</li><li>writing a customized Cover Letter</li></ul><p>If you actually get positive feedback, you will additionally spend time on:</p><ul><li>possibly talking to people working at these organizations</li><li>one or multiple interviews</li><li>potential trial tasks (plus perhaps test preparation)</li><li>possibly required travel</li></ul><p>For most of your applications you might never hear back, or likely get rejections. <a href=\"https://forum.effectivealtruism.org/posts/jmbP9rwXncfa32seH/after-one-year-of-applying-for-ea-jobs-it-is-really-really\">There is already a great post</a> with several time examples and which outlines the mental challenge coming with this (lack of) feedback.</p><p>It also mentions that several EA organizations actually provide compensation for trial tasks, which is amazing! But of course you'll only get that if you reach a higher round. Regardless of you advancing, you will not be compensated for the initial application process steps.</p><p>You might argue that if you just start applying to jobs, having the practice is beneficial, and hence even rejections are worth your time. However, after having sent out a dozen applicants you're unlikely to benefit sufficiently from extra practice to justify spending the time.</p><p>On top of that, not getting any feedback with your rejections, doesn't even allow you to learn and improve your future applications. While I believe it would be incredibly helpful if organizations provide such feedback - where feasible - this is a whole different argument, since it requires significant resources.&nbsp;</p><p>Compared to that, providing a salary range in the job description doesn't require as much time. This includes factoring in more initial effort, such as calculating different ranges and potential additional paragraphs about location etc. It could potentially even save resources from the organization, which would have been spent on reading CVs from applicants, who wouldn't be able to accept an offer either way.</p><h2>Further Notes</h2><p>Please note that I don't see an issue with the actual salary amount. It's understandable that many NGOs will have a lower budget than for-profit companies. And you likely also get the benefit of doing good directly with your job at an NGO.&nbsp;</p><p>Giving a perfectly accurate number is not required either. I've seen many jobs offer a certain range, so applicants at least have a ballpark. Other descriptions even have multiple ranges, based on applicants\u2019 locations. Even if your personal location isn't part of the list, the other pointers still allow you to make an informed estimate.</p><p>The only problem I want to point out are job descriptions which don't disclose &nbsp;salaries at all. If organizations know they won't be able to pay a rate competitive to for-profit equivalents, why do they hide it? They wouldn't want to attract candidates that purely care about high salary anyways.&nbsp;</p><p>For candidates who actually want to work at a specific organization, but for whom a certain low salary might be an issue, not disclosing it doesn't feel right to me, since it can waste applicants' time.&nbsp;On the flip side, if the organization is able to provide a competitive or even high salary, not disclosing it seems even stranger to me, as they &nbsp;might be missing out on good applications.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnq7k84tuk00b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefq7k84tuk00b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>With these organizations one might mainly understand <a href=\"https://www.notion.so/Wiki-List-of-EA-related-organisations-ae330bb9deac4186b7528b84939d5448\">EA-related organizations</a>. But I would also include for-profit organizations that are recommended by EA-aligned organizations. This is currently done for example by listing them on their job boards. In the latter case it would be more up to the EA organization maintaining these listings to check for salary disclosure.</p></div></li></ol>", "user": {"username": "Cassidy"}}, {"_id": "bktDcapniZsRu8tix", "title": "Effectiveness of Malala fund versus Global Health and Development Fund", "postedAt": "2022-10-01T02:35:45.033Z", "htmlBody": "<p>Here is a good reference book on all the evidence that girls' education is the world's best investment: <a href=\"https://gdc.unicef.org/media/3166/download\">https://gdc.unicef.org/media/3166/download</a> &nbsp;</p><p>On the one hand, I believe that empowering women is incredibly effective for reducing war, keeping the population managable and protecting democracy. Moreover, the education will help thos girls grow up to become better mothers. I think that producing more engineers will be crucial for the future progress of technology and there are currently many women in developing countires that have no opportunity at all to become an engineer.</p><p>On the other hand, the global health and development fund has determined based on quantifiable results, that interventions in health are the most effective. This sounds terrible to say, but if we help people in poverty survive without solving their poverty won't that increase the number of people living in poverty?</p><p>To be clear, I'm aware that most experts no longer believe global overpopulation will be an issue, and I think more population growth could be good for the progress of technology.&nbsp;</p><p>I think the answer depends on your values. Personally, I do not assign intrinsic value to education. In a nutshell, my goal is to get to the star trek vision of the future eventually so I'm wondering what maximizes the chance of that or accelerates us getting there. In the past I've donated to help bolivia which does a combination of nutrition and education for boys and girls</p>", "user": {"username": "Joseph Gardi"}}, {"_id": "ejEFHMJBKvZkwHCkh", "title": "Whose Fault?", "postedAt": "2022-09-30T07:14:00.030Z", "htmlBody": "<p>There is an old story that I had read in one of the Upanishads (I can't recall which one exactly), and it describes a situation that goes something like this:</p><blockquote><p><i>A blind man is walking towards a deep well. &nbsp;There is a guy sitting on a bench nearby and watching him from a distance.&nbsp;</i></p><p><i>The blind man unaware of the upcoming well, eventually falls into it.&nbsp;</i></p><p><i>The Upanishad asks the reader: <strong>Whose fault is that?</strong></i></p><p><i>Is it the blind's man fault or the man sitting the bench watching everything and yet doing nothing?&nbsp;</i></p></blockquote><p>In a simplistic setup such as this one, it is easy to point out that the fault is the man sitting on the bench. His inaction led to a painful event that could have been easily avoided.&nbsp;</p><p>The scenario becomes interesting when we try to simulate the exercise in our day-to-day life.&nbsp;</p><p>Let me illustrate by taking a hypothetical instance:</p><blockquote><p>There is a reasonably large pothole is in the road in front of your house. You have noticed the pothole and have taken a mental note to ask the concerned authorities to fix it when you have time.&nbsp;</p><p>Now, one day, you hear the sound of a bike crash.&nbsp;</p><p>Maybe the biker didn't see the pothole, or maybe he was riding irresponsibly, but the bike got into a crash due to the large pothole, and the biker got seriously injured.&nbsp;</p><p>Now, let me ask you the question: <strong>Whose fault is that?</strong></p><ol><li>Is it the<strong> the biker?</strong></li><li>Is it<strong> the concerned government official </strong>responsible for the maintenance of the road?</li><li><strong>Or is it yours?</strong> You, who in the pursuit to maximize your time for saving 10k children in Bangladesh, couldn't find an hour of your time to call up and perform basic civic duties.&nbsp;</li></ol></blockquote><p>It's a fun exercise to take any <strong>parable</strong> with its simplistic setup and apply it to our <strong>complex world setting</strong>. <strong>Isn't it?</strong> &nbsp;</p><p>One of the standard maxims of <strong>effective altruism </strong>is working on the <strong>world's most pressing issues</strong> that not many people are working on.&nbsp;</p><p><strong>And sometimes, that just happens to be right in front of your doorstep. &nbsp; &nbsp;</strong><br>&nbsp;</p>", "user": {"username": "Markovian"}}, {"_id": "BMkDcRrGWBj2j24NB", "title": "Google could build a conscious AI in three months", "postedAt": "2022-10-01T13:24:10.709Z", "htmlBody": "<p><em>Summary: Theories of consciousness do not present significant technical\nhurdles to building conscious AI systems. Recent advances in AI relate to\ncapacities that aren't obviously relevant to consciousness. Satisfying major\ntheories with current technology has been and will remain quite\npossible. The potentially short time lines to plausible digital consciousness mean that issues relating to digital minds are more pressing than they might at first seem.</em>  <strong>Key ideas are bolded. You can get the main points just by skimming these.</strong></p>\n<h2>Viability</h2>\n<p>Claim 1: Google could build a conscious AI in as little as three months\nif it wanted to.</p>\n<p>Claim 2: Microsoft<sup class=\"footnote-ref\"><a href=\"#fn-pp7taMfzJpQyxSCyC-1\" id=\"fnref-pp7taMfzJpQyxSCyC-1\">[1]</a></sup> could have done the same in 1990.</p>\n<p>I'm skeptical of both of these claims, but I think something in the\nballpark is true. <strong>Google could assemble a small team of engineers to\nquickly prototype a system that existing theories, straightforwardly\napplied, would predict is conscious.</strong> The same is true for just about\nany tech company, today or in 1990.<sup class=\"footnote-ref\"><a href=\"#fn-pp7taMfzJpQyxSCyC-2\" id=\"fnref-pp7taMfzJpQyxSCyC-2\">[2]</a></sup></p>\n<p>Philosophers and neuroscientists have little to say about why a digital\nsystem that implemented fairly simple patterns of information processing\nwould not be conscious. Even if individual theorists might have a story\nto tell about what was missing, they would probably not agree on what\nthat story was.</p>\n<p>The most prominent theories of consciousness lay out relatively vague\nrequirements for mental states to be conscious. <strong>The requirements for\nconsciousness (at least for the more plausible theories<sup class=\"footnote-ref\"><a href=\"#fn-pp7taMfzJpQyxSCyC-3\" id=\"fnref-pp7taMfzJpQyxSCyC-3\">[3]</a></sup>) generally\nhave to do with patterns of information storage, access, and\nprocessing.</strong> Theorists typically want to accommodate our uncertainty\nabout the low-level functioning of the human brain and also allow for\nconsciousness in species with brains rather different from ours. This\nmeans that their theories involve big picture brain architectures, not\nspecific cellular structures.</p>\n<p>Take the Global Workspace Theory: roughly put, conscious experiences\nresult from stored representations in a centralized cognitive workspace.\nThat workspace is characterized by its ability to broadcast its contents\nto a variety of (partially) modularized subsystems, which can in turn\nsubmit future contents to the workspace. According to the theory, any\nsystem that uses such an architecture to route information is conscious.</p>\n<p>A software system that included a global workspace would be easy to\nbuild. All you have to do is set up some modules with the right rules\nfor access to a global repository. To be convincing, these modules\nshould resemble the modules of human cognition, but it isn't obvious\nwhich kinds of faculties matter. Perhaps some modules for memory,\nperception, motor control, introspection, etc. You need these modules to\nbe able to feed information into the global workspace and receive\ninformation from it in turn. These modules need to be able to make use of the\ninformation they receive, which requires some contents usable by the\ndifferent systems.</p>\n<p>Critically for my point, <strong>complexity and competence\naren't desiderata for consciousness</strong>. The modules with access to the\nworkspace don't need to perform their assigned duties particularly well.\nGiven no significant requirements on complexity or competence, a global\nworkspace architecture could be achieved in a crude way quite quickly by\na small team of programmers. It doesn't rely on\nany genius, or any of the technological advances of the past 30 years.</p>\n<p>More generally:</p>\n<p>1.) <strong>Consciousness does not depend on general intelligence, mental\nflexibility, organic unpredictability, or creativity.</strong></p>\n<p>These traits distinguish humans from current computer programs. There\nare no programs that can produce creative insights outside of very\nconstrained contexts. Perhaps because of this, we may use these traits as a\nheuristic guide to consciousness when in doubt. In science fiction, for\ninstance, we often implicitly assess the status of alien and artificial\ncreatures without knowing anything about their internal structures. We\nnaturally regard the artificial systems that exhibit the same sorts of\ncreativity and flexibility as having conscious minds. However, these heuristics are\nnot grounded in theory.</p>\n<p>There are no obvious reasons why these traits should have to be\ncorrelated with consciousness in artificial systems. Nothing about\nconsciousness obviously requires intelligence or mental flexibility. In\ncontrast, there might be good reasons why you'd expect systems that\nevolved by natural selection to be conscious if and only if they were\nintelligent, flexible, and creative. For instance, it might be that the\narchitectures that allow for consciousness are most useful with\nintelligent systems, or help to generate creativity. But even if this is\nso, it doesn't show that such traits have to travel together in\nstructures that we design and build ourselves. Compare: since legs are\ngenerally used to get around, we should expect most animals with legs to\nbe pretty good at using them to walk or run. But we could easily build\nrobots that had legs but who would fall over whenever they tried to go\nanywhere. The fact that they are clumsy doesn't mean they lack legs.</p>\n<p>2.) <strong>Consciousness does not require and is not made easier with neural\nnetworks.</strong></p>\n<p>Neural networks are exciting because they resemble human brains and\nbecause they allow for artificial cognition that resembles human\ncognition in being flexible and creative. However, most theorists accept\nthat consciousness is multiply realizable, meaning that consciousness\ncan be produced in many different kinds of systems, including systems\nthat don't use neurons or anything like neurons.</p>\n<p>There is no obvious reason why neural networks should be better able to\nproduce the kinds of information architectures that are thought to be characteristic\nof consciousness. Most plausible major theories of consciousness have\nnothing to say about neurons or what they might contribute. It is\nunclear why neural networks should be more likely to lead to\nconsciousness.</p>\n<h2>Reception</h2>\n<p>Even though I think a tech company could build a system that checked all the\nboxes of current theories, I doubt it would convince people that their AI was really conscious (though not for particularly good reasons). If true, this provides reasons to think no company will try any time soon.\nPlausibly, <strong>a company would only set out to make a\nconscious system if they could convince their audience that they may\nhave succeeded</strong>.</p>\n<p>We can divide the question of reception into two parts:\nHow would the public respond and how would experts respond?</p>\n<p>Tech companies may soon be able to satisfy the letter of\nmost of the current major non-biological theories of consciousness, but <strong>any AI developed soon would probably still remind us more of a computer than an\nanimal</strong>. It might make simple mistakes suggestive of imperfect\ncomputer algorithms. It might be limited to a very specific domain of\nbehaviors. If it controlled a robot body, the movements might be jerky or might sound mechanical. Consider the biases people feel about animals like\nfish that don't have human physiologies. It seems likely that people would be even more biased against crude AIs.</p>\n<p>The AI wouldn't necessarily have language skills capable of expressing\nits feelings. If it did, it might talk about its consciousness in a way\nwhich mimics us rather than as the result of organic introspection<sup class=\"footnote-ref\"><a href=\"#fn-pp7taMfzJpQyxSCyC-4\" id=\"fnref-pp7taMfzJpQyxSCyC-4\">[4]</a></sup>. This might lead to the same sorts of mistakes that make LaMDA so implausibly conscious. (E.g.\nby talking about how delicious ice cream is despite never having tried\nit.) The fact that a system is just mimicking us when talking about its\nconscious experiences doesn't mean it lacks them -- human actors (e.g. in movies) still\nhave feelings, even if you can't trust their reports --\nbut it seems to me that it would make claims about their consciousness\nto be a tough sell to the general public.</p>\n<h3>The Public</h3>\n<p>The candidate system I'm imagining would probably not convince the\ngeneral public that artificial consciousness had arrived by itself.\n<strong>People have ways of thinking about minds and machines and use various\nsimple and potentially misleading heuristics for differentiating them</strong>. On these heuristics, crude systems that passed consciousness hurdles would still, I expect, be\ngrouped with the machines, because of their computer-like behavioral\nquirks and because people aren't used to thinking about computers as\nconscious.</p>\n<p>On the other hand, systems that presented the right behavioral profile may be\nregarded by the public as conscious regardless of theoretical support. If a system does manage to hook into the right heuristics, or if it reminds us more of an animal than a computer, people might be generally inclined to regard it as conscious, particularly if they can interact with it and if experts aren't generally dismissive. People are primed to anthropomorphize. We do it with our pets, with the weather, even with dots moving on a screen.</p>\n<h3>The Experts</h3>\n<p>I suspect that most <strong>experts who have endorsed theories of\nconsciousness wouldn't be inclined to attribute consciousness to a crude\nsystem</strong> that satisfied the letter of their theories. It is\nreputationally safer (in terms of both public perceptions and academic\ncredibility) to not endorse consciousness in systems that give off a\ncomputer vibe. There is a large kooky side to consciousness research\nthat the more conservative mainstream likes to distinguish itself from.\nSo many theorists will likely want some grounds on which to deny or at\nleast suspend judgement about consciousness in crude implementations of\ntheir favored architectures. On the other hand, the threat of kookiness may lose its bite if the public is receptive to an AI being conscious.</p>\n<p>Current theories are missing important criteria that might be relevant\nto artificial consciousness because they're defined primarily with the\ngoal of distinguishing conscious from unconscious states of human brains\n(or possibly conscious human brains from unconscious animals, or\nhousehold objects). They aren't built to distinguish humans from crude\nrepresentations of human architectures. It is open to most theorists to\nexpand their theories to exclude digital minds. Alternatively, they\nmay simply profess not to know what to make of apparent digital\nminds (e.g. <a href=\"https://philpapers.org/rec/PRILMA\">level-headed mysterianism</a>). This is perhaps safer and more honest, but if widely adopted,\nmeans the public would be on its own in assessing consciousness.</p>\n<h2>Implications</h2>\n<p>The possibility that a tech company could soon develop a system that was\nplausibly conscious according to popular theories should be somewhat unsettling. The main barriers to this happening seem to have more to do with the desires of companies\nto build conscious systems rather than with technical limitations. The\nskeptical reception such systems are likely to receive is good -- it provides an averse incentive that buys us more time. However, these thoughts are very tentative. There might be ways of taking advantage of our imperfect heuristics to encourage people to accept AI systems as conscious.</p>\n<p>The overall point is that <strong>timelines for apparent digital consciousness may be very short</strong>. While there are presently no large groups interested in producing digital consciousness, the situation could quickly change if consciousness becomes a selling\npoint and companies think harder about how to market their products as\nconscious, such as for chatbot friends or artificial pets. There is no clear technological hurdle to creating digital consciousness. Whether we think we have succeeded may have more to do with imperfect heuristics.</p>\n<p><strong>We're not ready, legally or socially, for potentially sentient\nartificial creatures to be created and destroyed at will for commercial\npurposes.</strong> Given the current lack of attention to digital consciousness, we're\nnot in a good position to even agree about which systems might need our\nprotection or what protections are appropriate. This is a problem.</p>\n<p>In the short run, worries about sentient artificial systems are dwarfed\nby the problems faced by humans and animals. However, there are\nlongtermist considerations that suggest we should care more about\ndigital minds now than we currently do. <strong>How we decide to incorporate\nartificial systems into our society may have a major impact on the shape\nof the future. That decision is likely to be highly sensitive to the\ninitial paths we take.</strong></p>\n<p>Because of the longterm importance of digital minds, the people who\npropose and evaluate theories of consciousness need to think harder\nabout applications to artificial systems. <strong>Three months (or three years) will not be nearly enough time to develop better theories about consciousness or to work out what policies we should put in place given our lack of certainty.</strong></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-pp7taMfzJpQyxSCyC-1\" class=\"footnote-item\"><p>Brian Tomasik makes the <a href=\"https://reducing-suffering.org/why-your-laptop-may-be-marginally-sentient/\">case</a> that Microsoft may have done so unintentionally. <a href=\"#fnref-pp7taMfzJpQyxSCyC-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pp7taMfzJpQyxSCyC-2\" class=\"footnote-item\"><p>Theories of consciousness have come along further since\n1990 than the technology relevant to implementing them. Developers in 1990 would have had a much less clear idea about what to try to build. <a href=\"#fnref-pp7taMfzJpQyxSCyC-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pp7taMfzJpQyxSCyC-3\" class=\"footnote-item\"><p>I include among more plausible theories the Global Workspace\nTheory, the various mid-level representationalist theories (E.g.\nPrinz's AIR, Tye's PANIC), first-order representationalist theories\nhigher-order theories that require metarepresentation (attention\ntracking theories, HOT theory, dual content theory, etc.). I don't\nfind IIT plausible, despite it's popularity, and am not sure what\neffect it's inclusion would have on the present arguments. Error\ntheories and indeterminacy theories are plausible, but introduce a\nrange of complications beyond the scope of this post. Some\nphilosophers have maintained that biological aspects of the brain\nare necessary for consciousness, but this view generally doesn't\ninclude a specific story about exactly what critical element exactly\nis missing. <a href=\"#fnref-pp7taMfzJpQyxSCyC-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-pp7taMfzJpQyxSCyC-4\" class=\"footnote-item\"><p>Human beings aren't inclined to talk about our conscious experiences in the customary way unprompted. We acquire ways of framing consciousness and mental states from our culture as children, so much of what we do is mimicking. Nevertheless, the frames we have acquired have been developed by people with brains like ours, so the fact that we're mimicking others (to whatever extent we are) isn't problematic in the way that it is for an AI. <a href=\"#fnref-pp7taMfzJpQyxSCyC-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Derek Shiller"}}, {"_id": "xcysfp6zb3JpCjKGq", "title": "What I learned from the criticism contest", "postedAt": "2022-10-01T13:39:15.782Z", "htmlBody": "<p>I was a judge on the <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\">Criticism and Red-teaming Contest</a>, and read 170 entries. It was overall great: hundreds of submissions and dozens of new points.&nbsp;</p><h2>Recurring patterns in the critiques</h2><p>But most people make the same points. Some of them have been made from the beginning, like 2011. You could take that as an indictment of EA's responsiveness to critics, proof that there's a problem, or merely as proof that critics don't read and that there's a small number of wide basins in criticism space. (We're launching the EA Bug Tracker to try and distinguish these scenarios, and to keep valid criticisms in sight.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6vog6r24d1d\"><sup><a href=\"#fn6vog6r24d1d\">[1]</a></sup></span>)</p><p>Trends in submissions I saw:</p><p><i>(I took out the examples because it was mean. Can back em up in DMs.)</i></p><p>&nbsp;</p><ul><li><i>Academics are stuck in 2015</i>. It's great that academics are writing full-blown papers about EA, and on average I expect this to help us fight groupthink and to bring new ideas in. But almost all of the papers submitted here are addressing a seriously outdated version of EA, before the longtermist shift, before the shift away from public calculation, before the systemic stuff.&nbsp;<br>Some of them even just criticise Singer 2009 and assume this is equivalent to criticising EA.&nbsp;<br>(I want to single out <a href=\"http://dx.doi.org/10.2139/ssrn.4118618\">Sundaram et al</a> as an exception. It is steeped in current EA while maintaining some <i>very</i> different worldviews.)</li><li><i>Normalisation</i>. For various reasons, many suggestions would make EA less distinctive. Whether that's intentional PR skulduggery, retconning a more mainstream cause into the tent, adding epicycles to show that mainstream problem x is really the biggest factor in AI risk, or just what happens when you average intuitions (the mode of a group will reflect current commonsense consensus about causes and interventions, and so not be very EA). This probably has some merit. But if we implemented all of these, we'd be destroyed.&nbsp;</li><li><i>Schism</i>. People were weirdly enthusiastic about schisming into two neartermist and longtermist movements. (They usually phrase this as a way of letting neartermist things get their due, but I see this as a sure way to doom it to normalisation instead.)</li><li><i>Stop decoupling everything</i>. The opposite mistake is to give up on <a href=\"https://blog.practicalethics.ox.ac.uk/2022/01/decoupling-contextualising-and-rationality/\">decoupling</a>, to allow the truism that 'all causes are connected' swamp focussed efforts. &nbsp; &nbsp;</li><li><i>Names</i>. People devote a huge amount of time to the connotations of different names. But obsessing over this stuff is <a href=\"https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism\">an established EA foible</a>.</li><li><i>Vast amounts of ressentiment</i>. Some critiques are just disagreements about cause prioritisation, phrased hotly as if this gave them more weight.</li><li><i>EAs underestimate uncertainty in cause prioritisation</i>. One perennial <a href=\"https://forum.effectivealtruism.org/posts/faW24r7ocbcPisgCH/a-critique-of-the-precipice-chapter-6-the-risk-landscape-red\">criticism</a> <i>which has always been true</i> is that most of cause prioritisation, the heart of EA, is incredibly nonobvious and dependent on fiddly philosophical questions.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3dgdm7gr23u\"><sup><a href=\"#fn3dgdm7gr23u\">[2]</a></sup></span>&nbsp;And yet we don't much act like we knew this, aside from a few GPI economist-philosophers. This is probably the fairest criticism I hear from non-EAs.<br>&nbsp;</li></ul><h2>Fundamental criticism takes time</h2><p>Karnofsky, describing his former view: \"<i>Most EA criticism is - and should be - about the community as it exists today, rather than about the \u201ccore ideas.\u201d The core ideas are just solid. Do the most good possible - should we really be arguing about that?\" </i>He changed his mind!</p><p><i>Really</i> fundamental challenges to your views don't move you at the time you read them. Instead they set dominoes falling; they alter some weights a little, so that the next time the problem comes up in your real life, you notice it and hold it in your attention for a fraction more of a second. And then over about 3 years, you become a different person, - and no trace of the original post remains, and no gratitude will accrue.</p><p>If the winners of the contest don't strike you as fundamental critiques, this is part of why. (The weakness of the judges is another part, but a smaller part than this, I claim. Just wait!)</p><p>My favourite example of this is <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#938EWYh5zb3rjwyor\">80k arguing with some Marxists in 2012</a>. We ended up closer than you'd have believed!</p><h2>My picks</h2><h2>Top for changing my mind</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/bo6Jsvmq9oiykbDrM/aesthetics-as-epistemic-humility\">Aesthetics as Epistemic Humility</a>.<br>I usually view \"EA doesn't have good aesthetics\" as an incredibly shallow critique - valuable for people doing outreach but basically not relevant in itself. Why should helping people <i>look good</i>? And have you seen how much most aesthetics cost?<br><br>But this post's conception is not shallow. Aesthetics as an incredibly important kind of value, to some - and conceivably a unifying frame for more conventionally morally significant values. I still don't want to allocate much money to this, but I won't call it frivolous again.<br>&nbsp;</li><li><a href=\"https://docs.google.com/document/d/1dKaPvviGNIM3xGbP8NBRd1MytXBVvGi7daP6cW4z1RQ/edit?usp=sharing\">EvN on veganism</a><br>van Nostrand's post is fairly important in itself - she is a talented health researcher, and for your own sake you should heed her. (It will be <i>amazing</i> if she does the blood tests.) But I project much greater importance onto it. Context: I was vegan for 10 years.<br><br>The movement has been overemphasising diet for a long time. This focus on personal consumption is anti-impact in a few ways: the cognitive and health risks we don't warn people about, the smuggled deontology messing up our decisions, the bias towards mere personal action making us satisfice at mere net zero.&nbsp;<br><br>There is of course a tradeoff with a large benefit to animals and a \"taking action / sincerity / caring / sacrificing\" signal, <i>but we could maintain our veganism while being honest about the costs to some people</i>. (Way more contentious is the idea of meat options at events as welcoming and counter-dogmatic. Did we really lose great EAs because we were hectoring them about meat when it wasn't the main topic? No idea, but unlike others she doesn't harp on about it, just does the science.) As you can see from the email she quotes, this post persuaded me that we got the messaging wrong and plausibly did some harm. (Net harm? Probably not, but cmon.)<br>&nbsp;</li></ul><h2>Top 5 for improving EA &nbsp;&nbsp;</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building\">Bad Omens</a><br>This post was robbed (came in just under the prize threshold). But all of you have already read it. I beg you to go look again and take it to heart. Cause-agnostic community building alienates the people we need most, in some areas. CBs should specialise. We probably shouldn't do outreach with untrained people with a prewritten bottom line.&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and\">Are you really in a race?&nbsp;</a><br>The <a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fnostalgebraist.tumblr.com%2Fpost%2F693702419803258880%2Fthis-may-already-be-obvious-to-some-of-you-but%3Ffbclid%3DIwAR3Hf7d8XEtEjlR2i1MUd-4cczG8f5w9FqUcwrQNi1MyLSetMYZWkhIFpZQ&amp;h=AT2IDDcxiLa2JMa6yEiZlOPTnH1CfWL_pGIdAeHMxsnv9axFXcdK-VmTXjY1sCB9Wz7YxW1Aoq6hdQWMxQ2fvhSwOLY7SQCCQwstwJyO0hX8XUADDbSQ8kf4KA&amp;__tn__=R]-R&amp;c[0]=AT2LEFhyna3_TtObauQoR-fKBsAUd_slVohM1viWAgeM5d5qKVCTeGmsHN57eog0d7UbA0H9E1Q48NZmFMnL63Nn0AC1m3PncGxod1jLJwt6FCDLgm9bfIQz94NYX5jqXh_cyG41o9c7bdRCWrBCXY3v_lUKRqlnM0Nl1EKPO7VizgW8NPU\">apparent</a> information cascade in AI risk circles has been bothering me a lot. Then there's the dodgy effects of thoughtless broadcasting, including the \"pivotal acts\" discourse. This was a nice, subtle intervention to make people think a bit on the most important current question in the world.</li><li>Obviously Froolow and Hazelfire and Lin</li><li><a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\">Effective altruism in the garden of ends</a><br>Alterman's post is way too long, and his \"fractal\" idea is incredibly underspecified. Nonetheless he describes how I live, and how I think most of us who aren't saints should live.&nbsp;</li><li><a href=\"https://forum.effectivealtruism.org/posts/u9CvMCCmQRgjBD828/red-teaming-a-model-for-estimating-the-value-of-longtermist\">Red teaming a model for estimating the value of longtermist interventions</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">The very best criticism </a>wasn't submitted because it would be unseemly for the author to win.<br>&nbsp;</li></ul><h2>Top for prose</h2><ul><li><a href=\"https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal\">A Critical Review of Open Philanthropy\u2019s Bet On Criminal Justice Reform. </a>Remarkable, not for its numbers, but for its <i>hermeneutics of effectiveness</i>. Nuno is faced with a puzzle - why did they do this? He asks them, and is only partially satisfied. To sate his lust for explanation, he comes up with a whole taxonomy of second-order reasons people might make suboptimal interventions. Are they true? Probably not. But I applaud his nasty imagination.</li><li><a href=\"https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous\">Karnofsky again</a>.</li><li><a href=\"https://forum.effectivealtruism.org/posts/bedstSbqaP8aDBfDr/the-nietzschean-challenge-to-effective-altruism\">On Nietzsche and do-gooding</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building\">Bad Omens</a></li></ul><h2>Top for rigour &nbsp;&nbsp;</h2><ul><li>Pettigrew, <a href=\"https://forum.effectivealtruism.org/posts/xAoZotkzcY5mvmXFY/longtermism-risk-and-extinction\">Longtermism, risk, and extinction</a>. Troubling. You can see me get stroppy in the comments; people were initially downvoting it because (I assume) they don't like the conclusion. I put 10% on this changing my mind later.</li><li><a href=\"https://forum.effectivealtruism.org/posts/gLWmeKTe68ZHnomwy/on-the-philosophical-foundations-of-ea\">On the Philosophical Foundations of EA</a>. I was ready to disdain this post hugely - a deep philosophical reading of a frigging podcast? But it's masterfully done. I absolutely do not agree that more EAs get deep into ethical or metaethical theory, but I admire this person doing it.</li><li><a href=\"https://forum.effectivealtruism.org/posts/N6hcw8CxK7D3FCD5v/existential-risk-pessimism-and-the-time-of-perils-4\">Thorstad, Existential risk pessimism and the time of perils</a></li><li><a href=\"https://psyarxiv.com/d9vcg/\">Adjusting for Publication Bias Reveals Mixed Evidence for the Impact of Cash Transfers</a></li></ul><h2>Top posts I don't quite understand in a way which I suspect means they're fundamental&nbsp;</h2><ul><li><a href=\"http://benjaminrosshoffman.com/oppression-and-production-are-competing-explanations-for-wealth-inequality/\">Hoffman on oppression</a>.</li><li><a href=\"https://forum.effectivealtruism.org/posts/zvNwSG2Xvy8x5Rtba/questioning-the-foundations-of-ea\">Wei Dai</a></li><li>Please help</li></ul><h2>Top posts I disagree with</h2><ul><li><a href=\"https://centerforreducingsuffering.org/point-by-point-critique-of-why-im-not-a-negative-utilitarian/\">Vinding on Ord</a>. Disagree with it directionally but Ord's post is surprisingly weak. Crucial topic too.</li><li>Zvi. Really impressed with his list of assumptions (only two errors).</li><li><a href=\"https://forum.effectivealtruism.org/posts/RRyHcupuDafFNXt6p/longtermism-and-computational-complexity\">you can't do longtermism because the complexity class is too hard</a>. Some extremely bad arguments (e.g. <a href=\"https://www.dwarkeshpatel.com/p/universal-explainers\">Deutsch on AI</a>) take the same form as this post - appeal to a worst-case complexity class, when this often says very little about the practical runtimes of an algorithm. But I am not confident of this.</li><li>Private submission with a bizarre view of gain of function research.</li><li><a href=\"http://dx.doi.org/10.2139/ssrn.4118618\">Sundaram et al</a><br>&nbsp;</li></ul><h2>Process</h2><p>One minor side-effect of the contest: we accidentally made people frame their mere disagreements or iterative improvements as capital-c Criticisms, more oppositional than they maybe are. You can do this for anything - the line between critique and next iteration is largely to do with tone, an expectation of being listened to, and whether you're playing to a third party audience.</p><p>&nbsp;</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6vog6r24d1d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6vog6r24d1d\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://github.com/g-leech/itila/issues/3\">Here's a teaser</a> I made in an unrelated repo.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3dgdm7gr23u\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3dgdm7gr23u\">^</a></strong></sup></span><div class=\"footnote-content\"><p>AI (i.e. not AI alignment) only rises above this because, at this point, there's no way that it's not going to have some major impact even if that's not existential.</p></div></li></ol>", "user": {"username": "technicalities"}}, {"_id": "3gmkrj3khJHndYGNe", "title": "Estimating the Current and Future Number of AI Safety Researchers", "postedAt": "2022-09-28T20:58:12.527Z", "htmlBody": "<h1>Summary</h1><p>I estimate that there are about <strong>300</strong> full-time technical AI safety researchers, <strong>100</strong> full-time non-technical AI safety researchers, and <strong>400</strong> AI safety researchers in total today. I also show that the number of technical AI safety researchers has been increasing exponentially over the past several years and could reach 1000 by the end of the 2020s.</p><h1>Introduction</h1><p>Many previous posts have estimated the number of AI safety researchers and a generally accepted order-of-magnitude estimate is 100 full-time researchers. The question of how many AI safety researchers there are is important because the value of work in an area on the margin is proportional to how neglected it is.</p><p>The purpose of this post is the analyze this question in detail and come up with hopefully a fairly accurate estimate. I'm going to focus mainly on estimating the number of technical researchers because non-technical AI safety research is more varied and difficult to analyze. Though I'll create estimates for both types of researchers.</p><p>I'll first summarize some recent estimates before coming up with my own estimate. Then I'll compare all the estimates later.</p><h3>Definitions</h3><p>I'll be using some specific terms in the post which I think are important to define to avoid misunderstanding or ambiguity. First, I'll define 'AI safety', also known as AI alignment, as work that is done to reduce existential risk from advanced AI. This kind of work tends to focus on the long-term impact of AI rather than short-term problems such as the safety of self-driving cars or AI bias.</p><p>My use of the word 'researcher' is a generic term for anyone working on AI safety and is an umbrella term for more specific roles such as research scientist, research engineer, or research analyst.</p><p>Also, I'll only be counting full-time researchers. However, since my goal is to estimate research capacity, what I'm really counting is the number of full-time equivalent researchers. For example, two part-time researchers working 20 hours per week can be counted as one full-time researcher.</p><p>I'll define technical AI safety research as research that is directly related to AI safety such as technical machine learning work or conceptual research (e.g. ELK). And non-technical research includes research related to AI governance, policy, and meta-level work such as this post.</p><h1>Past estimates</h1><ul><li>80,000 hours: <a href=\"https://80000hours.org/problem-profiles/artificial-intelligence/\">estimated</a> that there were about <strong>300 </strong>people working on reducing existential risk from AI in 2022 with a 90% confidence interval between 100 and 1,500. The estimate used data drawn from the<a href=\"https://aiwatch.issarice.com/\"> AI Watch</a> database.</li><li>A <a href=\"https://forum.effectivealtruism.org/posts/ycCBeG5SfApC3mcPQ/even-more-early-career-eas-should-try-ai-safety-technical\">recent post</a> (2022) on the EA Forum estimated that there are <strong>100-200</strong> people working full-time on AI safety technical research.</li><li>Another <a href=\"https://www.lesswrong.com/posts/oyZiwkxejBMuJZA7J/two-reasons-we-might-be-closer-to-solving-alignment-than-it\">recent post</a> (2022) on LessWrong claims that there are about <strong>150</strong> people working full-time on technical AI safety.</li><li>In a recent Twitter <a href=\"https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en\">thread</a> (2022), Benjamin Todd counters the idea that AI safety is saturated and says that there are only about 10 AI safety groups which each have about 10 researchers which is <strong>100</strong> researchers in total. He also states that this number has grown from about 30 in 2017 and that there are 100,000+ researchers working on AI capabilities&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefd48myitczbg\"><sup><a href=\"#fnd48myitczbg\">[1]</a></sup></span>.</li><li>This Vox<a href=\"https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment\"> article</a> said that about <strong>50 </strong>in the world were working full-time on technical AI safety in 2020.</li><li>This <a href=\"https://docs.google.com/presentation/d/1bwJDRC777rAf00Drthi9yT2c9b0MabWO5ZlksfvFzx8/edit#slide=id.gf171287819_0_4339\">presentation</a> estimated that fewer than<strong> 100</strong> people were working full-time on technical AI alignment in 2021.</li><li>There <a href=\"https://80000hours.org/2021/08/effective-altruism-allocation-resources-cause-areas/\">were</a> about 2000 'highly engaged' members of Effective Altruism in 2021. <strong>450</strong> of those people were working on AI safety.</li></ul><h1>Estimating the number of AI safety researchers</h1><h2>Organizational estimate</h2><p>I'll estimate the number of technical AI safety researchers and then the number of non-technical AI safety researchers. My main estimation method will be what I call an 'organizational estimate' which involves creating a list of organizations working on AI safety and then estimating the number of researchers working full-time in each organization to create a table similar to the one in <a href=\"https://l estimate\">this</a> post. I'll also estimate the number of independent researchers. Note that I'll only be counting people who work full-time on AI safety.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/hkfqqaeb9ahur5csxltr.png\"><figcaption>Organizational estimate</figcaption></figure><p>To create the estimates in the tables below I used the following sources (ordered from most to least reliable):</p><ul><li>Web pages listing all the researchers working at an organization.</li><li>Asking people who work at the organization.</li><li><a href=\"https://github.com/smcaleese/ai-publication-scraping-scripts\">Scraping </a>publications and posts from sites including The Alignment Forum, DeepMind and OpenAI and analyzing the data&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk41pv3aqlx\"><sup><a href=\"#fnk41pv3aqlx\">[2]</a></sup></span>.</li><li>LinkedIn insights to estimate the number of employees in an organization.</li></ul><p>The confidence column shows how much information went into the estimate and how confident I am about the estimate.</p><h3>Technical AI safety research organizations</h3><figure class=\"table\"><table><thead><tr><th>Name</th><th>Estimate</th><th>Lower bound (95% CI)</th><th>Upper bound (95% CI)</th><th>Overall confidence</th></tr></thead><tbody><tr><td>Other</td><td>80<strong>&nbsp;</strong><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref2uw48kb1c0b\"><sup><a href=\"#fn2uw48kb1c0b\">[3]</a></sup></span></td><td>15</td><td>150</td><td>Medium</td></tr><tr><td>Centre for Human-Compatible AI</td><td>25</td><td>5</td><td>50</td><td>Medium</td></tr><tr><td>DeepMind</td><td>20</td><td>5</td><td>60</td><td>Medium</td></tr><tr><td>OpenAI</td><td>20</td><td>5</td><td>50</td><td>Medium</td></tr><tr><td>Machine Intelligence Research Institute</td><td>10</td><td>5</td><td>20</td><td>High</td></tr><tr><td>Center for AI Safety (CAIS)</td><td>10</td><td>5</td><td>14</td><td>High</td></tr><tr><td>Fund for Alignment Research (FAR)</td><td>10</td><td>5</td><td>15</td><td>High</td></tr><tr><td>GoodAI</td><td>10</td><td>5</td><td>15</td><td>High</td></tr><tr><td>Sam Bowman</td><td>8</td><td>2</td><td>10</td><td>Medium</td></tr><tr><td>Jacob Steinhardt</td><td>8</td><td>2</td><td>10</td><td>Medium</td></tr><tr><td>David Krueger</td><td>7</td><td>5</td><td>10</td><td>High</td></tr><tr><td>Anthropic</td><td>15</td><td>5</td><td>40</td><td>Low</td></tr><tr><td>Redwood Research</td><td>12</td><td>10</td><td>20</td><td>High</td></tr><tr><td>Future of Humanity Institute</td><td>10</td><td>5</td><td>30</td><td>Medium</td></tr><tr><td>Conjecture</td><td>10</td><td>5</td><td>20</td><td>High</td></tr><tr><td>Algorithmic Alignment Group (MIT)</td><td>5</td><td>3</td><td>7</td><td>High</td></tr><tr><td>Aligned AI</td><td>4</td><td>2</td><td>5</td><td>High</td></tr><tr><td>Apart Research</td><td>4</td><td>3</td><td>6</td><td>High</td></tr><tr><td>Foundations of Cooperative AI Lab (CMU)</td><td>3</td><td>2</td><td>8</td><td>Medium</td></tr><tr><td>Alignment of Complex Systems Research Group (Prague)</td><td>2</td><td>2</td><td>8</td><td>Medium</td></tr><tr><td>Alignment research center (ARC)</td><td>2</td><td>2</td><td>5</td><td>High</td></tr><tr><td>Encultured AI</td><td>2</td><td>1</td><td>5</td><td>High</td></tr><tr><td>Totals</td><td>277</td><td>99</td><td>558</td><td>Medium</td></tr></tbody></table></figure><h3>Non-technical AI safety research organizations</h3><figure class=\"table\"><table><thead><tr><th>Name</th><th>Estimate</th><th>Lower bound (95% CI)</th><th>Upper bound (95% CI)</th><th>Overall confidence</th></tr></thead><tbody><tr><td>Centre for Security and Emerging Technology (CSET)</td><td>10</td><td>5</td><td>40</td><td>Medium</td></tr><tr><td>Epoch AI</td><td>4</td><td>2</td><td>10</td><td>High</td></tr><tr><td>Centre for the Governance of AI</td><td>10</td><td>5</td><td>15</td><td>High</td></tr><tr><td>Leverhulme Centre for the Future of Intelligence</td><td>4</td><td>3</td><td>10</td><td>Medium</td></tr><tr><td>OpenAI</td><td>10</td><td>1</td><td>20</td><td>Low</td></tr><tr><td>DeepMind</td><td>10</td><td>1</td><td>20</td><td>Low</td></tr><tr><td>Center for the Study of Existential Risk (CSER)</td><td>3</td><td>2</td><td>7</td><td>Medium</td></tr><tr><td>Future of Life Institute</td><td>4</td><td>3</td><td>6</td><td>Medium</td></tr><tr><td>Center on Long-Term Risk</td><td>5</td><td>5</td><td>10</td><td>High</td></tr><tr><td>Open Philanthropy</td><td>5</td><td>2</td><td>15</td><td>Medium</td></tr><tr><td>AI Impacts</td><td>3</td><td>2</td><td>10</td><td>High</td></tr><tr><td>Rethink Priorities</td><td>8</td><td>5</td><td>10</td><td>High</td></tr><tr><td>Other&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqw6xln23n\"><sup><a href=\"#fnqw6xln23n\">[4]</a></sup></span></td><td>10</td><td>5</td><td>30</td><td>Low</td></tr><tr><td>Totals</td><td>86</td><td>41</td><td>203</td><td>Medium</td></tr></tbody></table></figure><h2>Conclusions and notes</h2><p>Summary of the results in the tables above:</p><ul><li>Technical AI safety researchers:&nbsp;<ul><li>Point estimate: <strong>277</strong></li><li>Range: 99-558</li></ul></li><li>Non-technical AI safety researchers:<ul><li>Point estimate: <strong>86</strong></li><li>Range: 41-203</li></ul></li><li>Total AI safety researchers:<ul><li>Point estimate: <strong>363</strong></li><li>Range: 140-761</li></ul></li></ul><p>In conclusion, there are probably around <strong>300</strong> technical AI safety researchers, <strong>100</strong> non-technical AI safety researchers and around <strong>400 </strong>AI safety researchers in total.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref3hff2n0rm1t\"><sup><a href=\"#fn3hff2n0rm1t\">[5]</a></sup></span></p><h2>Comparison of estimates</h2><p>The bar charts below compare my estimates with the estimates from the \"Past estimates\" section.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/y6w4hypqhwcmovuuli54.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/qwigs2kq8qdru3hbbfgi.png 100w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/ophzyxx0vzebdlxbxtdx.png 200w, https://res.cloudinary.com/cea/image/upload/v1676745868/mirroredImages/3gmkrj3khJHndYGNe/ku5uppspbdpepnuzbnk2.png 300w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/ujv2skc3hgivl13sxprn.png 400w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/tbsqdjmqjqtoudnzrs9u.png 500w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/egfp6wngsp5clgebwx5p.png 600w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/oxkz7gql3ytve23wawri.png 700w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/tjce4xdvxn8war6pzgqy.png 800w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/aiolgxuf0expcjxsib3i.png 900w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/oa9mcnwskuhifuq4unhf.png 951w\"></figure><p>In the first chart, my estimate is higher than all the historical estimates possibly because newer estimates will tend to be higher as the number of AI safety researchers increases or because my estimate includes more organizations. My estimate is similar to the other total estimates in the second chart.</p><h1>How has the number of technical AI safety researchers changed over time?</h1><h2>Technical AI safety research organizations</h2><figure class=\"table\"><table><tbody><tr><td>Name</td><td>Number of researchers</td><td>Founding Year</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Center for AI Safety (CAIS)</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">10</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2022</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Fund for Alignment Research (FAR)</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">10</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2022</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Conjecture</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">10</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2022</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Aligned AI</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">4</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2022</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Apart Research</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">4</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2022</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Encultured AI</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2022</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Anthropic</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">15</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2021</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Redwood Research</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">12</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2021</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Alignment Research Center (ARC)</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2021</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Alignment Forum</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">50</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2018</td></tr><tr><td>Sam Bowman</td><td>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8</td><td style=\"text-align:right\">2020<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk76byx2vslk\"><sup><a href=\"#fnk76byx2vslk\">[6]</a></sup></span></td></tr><tr><td>Jacob Steinhardt</td><td>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8</td><td style=\"text-align:right\">2016<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk76byx2vslk\"><sup><a href=\"#fnk76byx2vslk\">[6]</a></sup></span></td></tr><tr><td>David Krueger</td><td>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7</td><td style=\"text-align:right\">2016<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefk76byx2vslk\"><sup><a href=\"#fnk76byx2vslk\">[6]</a></sup></span></td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Center for Human-Compatible AI</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">30</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2016</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">OpenAI</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">20</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2016</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">DeepMind</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">20</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2012</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Future of Humanity Institute (FHI)</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">10</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2005</td></tr><tr><td style=\"padding:2px 3px;vertical-align:bottom\">Machine Intelligence Research Institute (MIRI)</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">15</td><td style=\"padding:2px 3px;text-align:right;vertical-align:bottom\">2000</td></tr></tbody></table></figure><p>I graphed the data in the table above to show how the total number of technical AI safety organizations has changed over time:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/bkwfnmlfrsjkpemz2rdi.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/lpnyyjpq41lbchkx9jw7.png 100w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/rsoghrhohy07mgjzg7el.png 200w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/jlf5dqury3kzqv7yugni.png 300w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/tyjygclktmt9nfk56mpo.png 400w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/f7fvxa7hqc4gjnwaj4vm.png 500w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/gsftdtbaim06p4t3sn7u.png 600w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/ig4kvdxz0utlkcepk8wz.png 700w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/vooekgmwjngeytyausof.png 800w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/xvobhfvwkfgzbyew2wpw.png 900w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/wskswscy6zp3jm4llqwj.png 941w\"></figure><p>The blue dots are the actual number of organizations in each year and the red line is an exponential model fitting the data.</p><p>I found that the number of technical AI safety research organizations is increasing exponentially at about 14% per year which makes sense given that EA funding is increasing and AI safety seems increasingly pressing and tractable.</p><p>Then I extrapolated the same model into the future to create the following graph:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/kge7kc68leufoogfq67p.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/m3knslrgznf9voktxmwp.png 100w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/ajvukq6g5ocljjbvzmdc.png 200w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/ig1ypeaktkpmybnuusqg.png 300w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/rjcjwqcefzlpltiga6kj.png 400w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/j9nqgdeumd5b6mb5onzv.png 500w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/dutcmxfd2czwsd6axytc.png 600w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/vneiiuo2bvaltoktvx3x.png 700w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/sbxi0zq0zfgg9awoizcx.png 800w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/x8iskb0clzwmj4nmbwgi.png 900w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/gi92aaikfx0tllleifrq.png 941w\"></figure><p>The table above includes 20 technical AI safety research organizations currently in existence and the model predicts that the number of organizations will double to 40 by 2029.</p><h2>Technical AI safety researchers</h2><p>I also created a model to estimate how the total number of AI safety researchers has changed over time. In the model, I assumed that the number of researchers in each organization has increased linearly from zero when each organization was founded up to the current number in 2022. The blue dots are the data points from the model and the red line is an exponential curve fitting the dots.</p><p>The model estimates that the number of technical AI safety researchers has been increasing at a rate of about 28% per year since 2000.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/p8fct3qjxcfrnxtalemt.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/r02pdennm1cxj751wpr7.png 100w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/yuqixrpynudwvjlvptwz.png 200w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/zbwkotlcmc6qcrwsnips.png 300w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/q1obnalsiojbcfzu8ptx.png 400w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/puxv9qlo8mluvgvjjmtk.png 500w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/tmmucahliys63fpjtfkg.png 600w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/iely5ir05idlt4grgtzn.png 700w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/w2ca67vljkvyovsbhwb2.png 800w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/ila1uk9dgzg44rxb9tiu.png 900w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/xhyvim6bhnf2gykwuzh5.png 951w\"></figure><p>The next graph shows the model extrapolated into the future and predicts that the number of technical AI safety researchers will increase from about 200 in 2022 to 1000 by 2028.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/yfewkqqmvsfxyynu0pb6.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/jomgwvx3tttv6vukcvu2.png 100w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/hzifje8afon7yab90vvz.png 200w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/h21xad5vjubkett0cfqv.png 300w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/p7uyc3xnbstip7c11j9e.png 400w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/b9ifsjbk73v0gczjdv6a.png 500w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/bwfimbncchzraw2knmqg.png 600w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/s7nenfyhoo1f09v9mhfk.png 700w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/noftgbixkqwirysuyoe4.png 800w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/sn33fvjacgdabv4dpwn5.png 900w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/mf4k90abrod1cwunzpqm.png 961w\"></figure><h1>How could productivity increase in the future?</h1><p>How the overall productivity of the technical AI safety research community will increase as the number of researchers increases is unclear. A well-known law that describes the research productivity of a field is <a href=\"https://en.wikipedia.org/wiki/Lotka%27s_law\">Lotka's Law&nbsp;</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6zqbblntpwq\"><sup><a href=\"#fn6zqbblntpwq\">[7]</a></sup></span>. The formula for Lotka's Law is:</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"Y = \\frac{C}{X^n}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;\">Y</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.569em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 1.569em; top: -1.407em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span><span class=\"mjx-denominator\" style=\"width: 1.569em; bottom: -0.773em;\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.024em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.409em; padding-left: 0.115em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">n</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.569em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.18em; vertical-align: -0.773em;\" class=\"mjx-vsize\"></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><p>Y is the number of researchers who have published X articles. C is the total number of contributors in the field who have published one article and <i>n </i>is a constant which usually has a value of 2. I found that a value of 2.3 fits data from the Alignment Forum most well&nbsp;<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefkkvrm6oiwod\"><sup><a href=\"#fnkkvrm6oiwod\">[8]</a></sup></span>:</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/knrkce10wc0u6ppuzajq.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/kucebmqv3bmged7goxbo.png 90w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/tmy01ebo6kjqzz6yfksd.png 180w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/nmt2qcrtnzvngo9cgkyd.png 270w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/ftsmw5qwmn5jees69tsq.png 360w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/mbhyb3fhm0xlu2gx56z9.png 450w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/bo17lcjkjgtsz28u5tyt.png 540w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/kz17jdsncqhuus99v7tv.png 630w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/npdiwzlkmty3fn11vvoi.png 720w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/nby4fga8tot2xj4sbsbt.png 810w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/zkp2ye64efoubvyuhwwc.png 829w\"></figure><p>The graph above shows that about 80 people have published a post on the Alignment Forum in the past six months. In this case, C = 80 and n = 2.3. Then the total number of posts published can be calculated by multiplying Y by X for each value of X and adding all the values together. For example:</p><p>80 / 1^2.3 = ~80 researchers have posted 1 post -&gt; 80 * 1 = 80</p><p>80 / 2^2.3 = ~16 researchers have posted 2 posts -&gt; 16 * 2 = 32</p><p>80 / 3^23 = ~6 researchers have posted 3 posts -&gt; 6 * 3 = 18</p><p>What happens when the number of researchers is increased? In other words, what happens when the value of C is doubled?</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/g2oepfpwbh4ymvzohncz.png\" srcset=\"https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/x3upbqliqfyarlyah8xn.png 100w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/c3et9xrcpwpjjmf9o6ch.png 200w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/mfk05snqn5ys5egppric.png 300w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/iovixvrhemsy7vl8vhos.png 400w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/ckqque0cwxh2yrmshsag.png 500w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/ajke0bhd6unxk3ystqac.png 600w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/zwtg5efa5xorwmmx7nlk.png 700w, https://res.cloudinary.com/cea/image/upload/v1676745870/mirroredImages/3gmkrj3khJHndYGNe/yljx6xkxd8fdr11lromv.png 800w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/o7y5yog0to77rlipyi7t.png 900w, https://res.cloudinary.com/cea/image/upload/v1676745869/mirroredImages/3gmkrj3khJHndYGNe/qx9vlbb89dh6iagh15oj.png 910w\"></figure><p>I found that when C is doubled, the total number of articles published per year also doubles. In the chart above, the area under the red curve is exactly double the area under the blue curve. In other words, the total productivity of a research field increases linearly with the number of researchers. The reason why is that increasing the number of researchers increases the number of low-productivity and high-productivity researchers equally.</p><p>It's important to note that simply increasing the number of posts will not necessarily increase the overall rate of progress. More researchers will be helpful if large problems can be broken up and parallelized so that each individual or team can work on a sub-problem. Nevertheless, increasing the size of the field should also increase the number of talented researchers if research quality is more important.</p><h1>Conclusions</h1><p>I estimated that there are about 300 full-time technical and 100 full-time non-technical AI safety researchers today which is roughly in line with previous estimates though my estimate for the number of technical researchers is significantly higher.</p><p>To be conservative, I think the correct order-of-magnitude estimate for the number of full-time AI safety researchers is around 100 today though I expect this to increase to 1000 in a few years.</p><p>The number of technical AI safety organizations and researchers has been increasing exponentially by about 10-30% per year, I expect that trend to continue for several reasons:</p><ul><li><strong>Funding:</strong> EA funding has increased significantly over the past several years and will probably continue to increase in the future. Also, AI is and will increasingly be advanced enough to be commercially valuable which will enable companies such as OpenAI and DeepMind to continue funding AI safety research.</li><li><strong>Interest: </strong>as AI advances and the gap between current systems and AGI narrows, it will become easier and require less imagination to believe that AGI is possible. Consequently, it might become easier to get funding for AI safety research. AI safety research will also seem increasingly urgent which will motivate more people to work on it.</li><li><strong>Tractability: </strong>as time goes on, the current AI architectures will probably become increasingly similar to the architecture used in the first AGI system which will make it easier to experiment with AGI-like systems and learn useful properties about them.</li></ul><p>By extrapolating past trends, I've estimated that the number of technical AI safety organizations will double from about 20 to 40 by 2030 and the number of technical AI safety researchers will increase from about 300 in 2022 to 1000 by 2030. I find it striking how many well-known organizations working on AI safety were founded very recently. This trend suggests that some of the most influential AI safety organizations will be founded in the future.</p><p>I then found that the number of posts published per year will likely increase at the same rate as the number of researchers. If the number of researchers increases by a factor of five by the end of the decade, I expect the number of posts or papers per year to also increase by that amount.</p><p>Breaking up problems into subproblems will probably help make the most of that extra productivity. &nbsp;As the volume of articles increases, skills or tools for summarization, curation, or distillation will probably be highly valuable for informing researchers about what is currently happening in their field.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnd48myitczbg\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefd48myitczbg\">^</a></strong></sup></span><div class=\"footnote-content\"><p>&nbsp;My estimate is far lower as I would only classify researchers as 'AI capabilities' researchers if they push the state-of-the-art forward. Though the number of AI safety researchers is almost certainly lower than the number of AI capabilities researchers.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk41pv3aqlx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk41pv3aqlx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>What I did:</p><p>- Alignment Forum: scrape posts and count the number of unique authors.</p><p>- DeepMind: scrape safety-tagged publications and count the number of unique authors.</p><p>- OpenAI: manually classify publications as safety-related. Then count the number of unique authors.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn2uw48kb1c0b\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref2uw48kb1c0b\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Manually curated list of people on the Alignment Forum who don't work at any of the other organizations. Includes groups such as:</p><p>- Independent alignment researchers (e.g. John Wentworth)</p><p>- Researchers in programs such as SERI MATS and Refine (e.g. <a href=\"https://www.alignmentforum.org/users/carado-1\">carado</a>)</p><p>- Researchers in master's or PhD degrees studying AI safety (e.g. <a href=\"https://www.alignmentforum.org/users/marius-hobbhahn\">Marius Hobbhahn</a>)</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqw6xln23n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqw6xln23n\">^</a></strong></sup></span><div class=\"footnote-content\"><p>There are about 45 research profile on Google Scholar with the 'AI governance' tag. I counted about 8 researchers who weren't at the other organizations listed.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn3hff2n0rm1t\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref3hff2n0rm1t\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that the technical estimate is more accurate than the non-technical estimate because technical research is more clearly defined. I also put more research into estimating the number of technical AI safety researchers than non-technical researchers.</p><p>Also bear in mind that since I probably failed to include some organizations or groups in the table, the true figures could be higher.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnk76byx2vslk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefk76byx2vslk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>These are rough guesses but the model is fairly robust to them.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6zqbblntpwq\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6zqbblntpwq\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Edit: thank you puffymist from the LessWrong comments section for recommending Lotka's Law over Price's Law as it is more accurate.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnkkvrm6oiwod\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefkkvrm6oiwod\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In case you're wondering, the outlier on the far right of the chart is John Wentworth.</p></div></li></ol>", "user": {"username": "Stephen McAleese"}}, {"_id": "voZNdtpcfPZdmELAg", "title": "How valueable are external reviews?", "postedAt": "2022-10-01T09:01:14.951Z", "htmlBody": "<p><i>Note: I found out that there have been detailed expert reviews of Founders Pledges work in the past. See Johannes Ackvas comment below. Also, despite the focus on Founders Pledge, this doesn't mean that I think that they are especially under-reviewed. More generally, I don't mean to criticise FP - I just wanted to share this argument and see what people think about it.</i><br><br>Last time I checked, I couldn't find any in-depth, expert reviews of the cost-effectiveness estimates of Founders Pledge. I know that there isn't one for their evaluation of the Clean Air Task Force. GivingGreen and SoGive have looked at some of them, but not in-depth. (They told me this in correspondence). So, assuming I didn't miss anything, there are two possibilities:<br><br>(i) they didn't have any such reviews or<br>(ii) they had reviews, but didn't publish them<br><br>Lets first assume (i) is the case.<br><br>There seem to be large potential benefits of getting reviewed. If such an expert would find out that FP is significantly off, then this is valuable information, because it might lead investors to change the amount they donate. If FP underestimated e.g. CATFs cost-effectiveness, they might shift funding from less effective opportunities towards the CATF and if they overestimated it's cost-effectiveness the reverse might happen. Either way, if an expert review uncovers that the size of the error is sufficiently large, it is not implausible that this would improve large funding decisions.<br><br>If, however, such an expert verifies FPs models, then this is valuable information too. In that case, their research seems much more trustworthy from the outside, which plausibly attracts more investors. This is especially true, for cost-effectiveness estimates that seem spectacular. Take the claim that CATF averts 1 ton of CO2e for less than a dollar. Many people outside of EA that I talked to were initially skeptical of this. (I'm not making any claim as to the reasonableness of that estimate, I am merely commenting on its public perception.)<br><br>So it seems like there are large potential benefits of getting an expert review for organisation like FoundersPledge (all I'm saying here might well apply to many other similar organisation that I'm not as familiar with).</p><p><br>The remaining question is then: are the expected benefits of an independent analysis justifying its costs?&nbsp;<br><br>I assume that you can hire an expert researcher for less than 100$/hour and that such an analysis would take less than 4 full work weeks. At 40 hours/week the whole thing would cost less than 16.000 $. That seems unrealistically high, but let\u2019s assume it's not.&nbsp;<br><br>Estimating that <i>tens to hundreds of millions of dollars</i> are allocated on the basis of their recommendations, it still seems worth it, doesn\u2019t it?</p><p><i>Edit: I have probably overestimated this amount quite a lot. See Linch's comment</i><br><br>Now lets assume that (ii) is the case - they had independent expert reviews, but didn't publish them. In that case, for the exact reasons given above, it would be important to make them public.</p><p>What do you make of this argument?<br>&nbsp;</p>", "user": {"username": "Aaron__Maiwald"}}]