[{"_id": "W95YcBacgvqYxzEDy", "postedAt": "2023-10-12T11:43:23.227Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I'm skeptical about the tractability of making AGI development taboo within a very few years. It seems like this plan would require moderate timelines in order to be viable.</p><p>That said, I'm starting to wonder whether we should be trying to gain support for a pause in theory: specifically, people to agree that in an ideal world, we would pause AI development where we are now.</p><p>That could help open up the Overton window.</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "fXDac462eSrNDJvy2", "postedAt": "2023-10-12T14:08:40.807Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Thanks for this post Greg.&nbsp;</p><p>Re your point about scaling, the <a href=\"https://arxiv.org/pdf/2208.12852.pdf\">Michael</a> et al survey of NLP researchers suggests that researchers don't think scaling will take us all the way there.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/yskwquniyp39ktz8bmqh\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/pdh16gu3amniivzmynw0 150w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/a0mzcpejonmnhvmhwhxr 300w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/bml3ynlwtnbepbir3yze 450w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/ikvhf47xoogataps7hx9 600w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/vpzexlqwmjxjfwz3nmkq 750w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/gahlmovpf7evjlhnyiqg 900w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/xs4oqjmfnyopqmimqrkk 1050w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/sb4jnn2mgzla4tmmxtsr 1200w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/hbyfcsslx1h5ruawy2fy 1350w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/z7qdi9rhzwhx4lamnnju 1478w\"></figure><figure class=\"image\"><img src=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/ejqlvzfuby4fmpkjdupq\" srcset=\"https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/ewqauwt40xkthnqvs6gp 96w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/qi9bw0hug1y4fnmdncab 176w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/eukcijtn9w8iwpkf4w4y 256w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/oy3nk3wdjjf4hav0d2tj 336w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/kghke2wvafpyjpxdeznk 416w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/ytksad6a6rhjupin9faj 496w, https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/fXDac462eSrNDJvy2/fm8vyvxoz5h9rshigqwl 576w\"></figure><p>Figure 4.&nbsp;</p><p>Based on my limited understanding I agree with you that it seems pretty plausible that scaling does take us to human level AI and beyond, but the experts seem to disagree and I'm not sure why</p>", "parentCommentId": null, "user": null}, {"_id": "dKYx58wpy4G2rFvrt", "postedAt": "2023-10-12T14:17:05.556Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Re p(doom) being high, I don't think you need to commit to the view that the most likely outcome of AGI is doom. Surveys of AI researchers put the risk from rogue AI at 5%. In the XPT survey professional forecasters put the risk of extinction from AI at 0.03% by 2050 and 0.38% by 2100, whereas domain experts put the chance at 1.1% and 3%, respectively. Though they had longer timelines than you seem to endorse here.&nbsp;</p><p>I think your argument goes through even if the risk is 1% conditional on AGI and that seems like an estimate unlikely to upset too many people, so I would just go with that</p>", "parentCommentId": null, "user": null}, {"_id": "jeLwYRTTDp6PQReXt", "postedAt": "2023-10-12T14:47:01.319Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Interesting. I'll note that this survey was pre-GPT-4 (or even before GPT3.5 was in widespread use? May-June 2022) when (I think) people were still sceptical of LLMs being able to do well on university exams, amongst many other things. Would be interesting to see a similar survey that is post-GPT-4 (I've not been able to find anything). I predict that it will show a significantly higher % agreeing.<br><br>In general I think any survey on AI that was conducted in the pre-GPT-4 era is now woefully out of date.</p>", "parentCommentId": "fXDac462eSrNDJvy2", "user": {"username": "Greg_Colbourn"}}, {"_id": "iB7WnJtSnnatPvLpc", "postedAt": "2023-10-12T15:03:57.178Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I still don't understand where the 95% for non-doom is coming from. I think it's useful to look at actual <a href=\"https://forum.effectivealtruism.org/posts/idjzaqfGguEAaC34j/if-your-agi-x-risk-estimates-are-low-what-scenarios-make-up\">mechanisms</a> for why people think this (and so far I've found them lacking). The qualifications of the \"professional forecasters\" in the XPT survey are <a href=\"https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1?commentId=87xsiJPJmQ7c3wTb9\">in doubt</a> (and again, it was <a href=\"https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1?commentId=sE8WyJhGNGjdR992n\">pre-GPT-4</a>).</p>", "parentCommentId": "dKYx58wpy4G2rFvrt", "user": {"username": "Greg_Colbourn"}}, {"_id": "YjYaeEeheoTehbpnr", "postedAt": "2023-10-12T15:06:12.107Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>The taboo only really needs to kick in on moderate timelines, so we're in luck :) On short timelines, only massive data centres and the leading AI labs need to be regulated.</p>", "parentCommentId": "W95YcBacgvqYxzEDy", "user": {"username": "Greg_Colbourn"}}, {"_id": "GCaqhHMRkcnQ67z9i", "postedAt": "2023-10-12T15:30:15.204Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>The argument might go through even if the risk is 1%, but people sure aren't acting like that. At least in EA, broadly speaking (where I imagine the average p(doom|AGI) is closer to 10%). Also, I'd rather just say what I actually believe, even if it sounds \"alarmist\". At least I've tried to argue for it in some detail. The main reason I am prioritising this so much is because I think it's the most likely reason I, and everyone I know and love, will die. Unless we stop it. Forget longtermism and EA: people need to understand that this is a threat to their own personal near-term survival.</p>", "parentCommentId": "dKYx58wpy4G2rFvrt", "user": {"username": "Greg_Colbourn"}}, {"_id": "tCQGbFeKBcznMmf5y", "postedAt": "2023-10-12T15:48:07.782Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I agree with most of your conclusions in this post. &nbsp;I feel uncomfortable. I'll write more once I have processed some of my emotions and can think in a more clear manner.</p>", "parentCommentId": null, "user": {"username": "William the Kiwi"}}, {"_id": "Xr6iA2X2LRpazBeoY", "postedAt": "2023-10-12T16:21:30.202Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I suppose one solution might be to say that your personal view is that pdoom is &gt;50%, but a range of estimates suggest &gt;1% is plausible</p>", "parentCommentId": "GCaqhHMRkcnQ67z9i", "user": null}, {"_id": "eGoLCvFby4SzppLqd", "postedAt": "2023-10-12T17:20:22.694Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I think you come across as over-confident, not alarmist, and I think it hurts how you come across quite a lot. (We've talked a bit about the object level before.) I'd agree with John's suggested approach.</p>", "parentCommentId": "GCaqhHMRkcnQ67z9i", "user": {"username": "Isaac_Dunn"}}, {"_id": "edvRmNztqYgp68Rmw", "postedAt": "2023-10-12T17:22:51.454Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Relatedly, I also think that your arguments for \"p(doom|AGI)\" being high aren't convincing to people that don't share your intuitions, and it looks like you're relying on those (imo weak) arguments, when actually you don't need to</p>", "parentCommentId": "eGoLCvFby4SzppLqd", "user": {"username": "Isaac_Dunn"}}, {"_id": "fEofHAznqmMWWzQ3w", "postedAt": "2023-10-12T17:55:14.147Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I'm crying out for convincing gears-level arguments against (even have $1000 bounty on it), please provide some.</p>", "parentCommentId": "edvRmNztqYgp68Rmw", "user": {"username": "Greg_Colbourn"}}, {"_id": "p6nQTh76qtuec7v6o", "postedAt": "2023-10-12T18:01:45.717Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I feel like this is a case of death by epistemic modesty, especially when it isn't clear how these low p(doom) estimates are arrived at in a technical sense (and a lot seems to me like a kind of \"respectability heuristic\" cascade). We didn't do very well with Covid as a society in the UK (and many other countries), following this kind of thinking.</p>", "parentCommentId": "eGoLCvFby4SzppLqd", "user": {"username": "Greg_Colbourn"}}, {"_id": "78FffdCgbjsG4bHDz", "postedAt": "2023-10-12T18:59:07.331Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>To be fair, I think I'm partly making wrong assumptions about what exactly you're arguing for here.</p><p>On a slightly closer read, you don't actually argue in this piece that it's as high as 90% - I assumed that because I think you've argued for that previously, and I think that's what \"high\" p(doom) normally means.</p>", "parentCommentId": "fEofHAznqmMWWzQ3w", "user": {"username": "Isaac_Dunn"}}, {"_id": "Y4ApXQEFH6tLecSsn", "postedAt": "2023-10-12T19:26:52.521Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I do think it is basically ~90%, but I'm arguing here for doom being the <i>default</i> outcome of AGI; I think \"high\" can reasonably be interpreted as &gt;50%.</p>", "parentCommentId": "78FffdCgbjsG4bHDz", "user": {"username": "Greg_Colbourn"}}, {"_id": "Lyi7ga9gZx4MEGGjL", "postedAt": "2023-10-12T20:18:47.400Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I agree with most of the points in this post (AI timelines might be quite short; probability of doom given AGI in a world that looks like our current one is high; there isn't much hope for good outcomes for humanity unless AI progress is slowed down somehow). I will focus on one of the parts where I think I disagree and which feels like a crux for me on whether advocating AI pause (in current form) is a good idea.</p>\n<p>You write:</p>\n<blockquote>\n<p>But we can still have all the nice things (including a cure for ageing) without AGI; it might just take a bit longer than hoped. We don\u2019t need to be risking life and limb driving through red lights just to be getting to our dream holiday a few minutes earlier.</p>\n</blockquote>\n<p>I think framings like these do a misleading thing where they use the word \"we\" to ambiguously refer to both \"humanity as a whole\" and \"us humans who are currently alive\". The \"we\" that decides how much risk to take is the humans currently alive, but the \"we\" that enjoys the dream holiday might be humans millions of years in the future.</p>\n<p>I worry that \"AI pause\" is not being marketed honestly to the public. If <a href=\"https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty\">people like Wei Dai are right</a> (and I currently think they are), then AI development may need to be paused for millions of years potentially, and it's unclear how long it will take unaugmented or only mildly augmented humans to reach longevity escape velocity.</p>\n<p>So to a first approximation, the choice available to humans currently alive is something like:</p>\n<ul>\n<li>Option A: 10% chance utopia within our lifetime (if alignment turns out to be easy) and 90% human extinction</li>\n<li>Option B: ~100% chance death but then our descendants probably get to live in a utopia</li>\n</ul>\n<p>For philosophy nerds with low time preference and altruistic tendencies (into which I classify many EA people and also myself), Option B may seem obvious. But I think many humans existing today would rather risk it and just try to build AGI now, rather than doing any AI pause, and to the extent that they <em>say</em> they prefer pause, I think they are being deceived by the marketing or acting under <a href=\"https://www.econlib.org/being-normal/\">Caplanian Principle of Normality</a>, or else they are somehow better philosophers than I expected they would be.</p>\n<p>(Note: if you are so pessimistic about aligning AI without a pause that your probability on that is lower than the probability of unaugmented present-day humans reaching longevity escape velocity, then Option B does seem like a strictly better choice. But the older and more unhealthy you are, the less this applies to you personally.)</p>\n", "parentCommentId": null, "user": {"username": "riceissa"}}, {"_id": "YFR7Wjih2Dk9mKA37", "postedAt": "2023-10-12T22:07:00.241Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote><ul><li>Option A: 10% chance utopia within our lifetime (if alignment turns out to be easy) and 90% human extinction</li></ul></blockquote><p>Are you simplifying here, or do you actually believe that \"utopia in our lifetime\" or \"extinction\" are the <i>only two possible outcomes</i> given AGI? Do you assign a <i>0% chance</i> that we survive AGI, but don't have a utopia in the next 80 years?&nbsp;</p><p>What if AGI stalls out at human level, or is incredibly expensive, or is <a href=\"https://titotal.substack.com/p/the-first-agi-will-be-a-buggy-mess\">buggy and unreliable</a> like humans are? What if the technology required for utopia turns out to be ridiculously hard even for AGI, or substantially bottlenecked by available resources? What if technology alone <i>can't</i> create a utopia, and the extra tech just exacerbates existing conflicts? What if AGI access is restricted to world leaders, who use it for their own purposes?&nbsp;</p><p>What if we build an unaligned AGI, but catch it early and manage to defeat it in battle? What if early, shitty AGI screws up in a way that causes a worldwide ban on further AGI development? What if we build an AGI, but we keep it confined to a box and can only get limited functionality out of it? What if we build an aligned AGI, but people hate it so much that it voluntary shuts off? What if the AGI that gets built is aligned to the values of people with awful views, like religious fundamentalists? What if AGI wants nothing to do with us and flees the galaxy? What if [insert X thing I didn't think of here]?.&nbsp;</p><p>IMO, extinction and utopia are <i>both</i> unlikely outcomes. The bulk of the probability lies somewhere in the middle.&nbsp;</p>", "parentCommentId": "Lyi7ga9gZx4MEGGjL", "user": {"username": "titotal"}}, {"_id": "frQsWQ5uuubTnojLg", "postedAt": "2023-10-13T01:03:43.391Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Hope everything is okay.</p><p>PS. I'm doing AI Safety movement building in Australia and New Zealand, so if you need someone to talk to, feel free to reach out.</p>", "parentCommentId": "tCQGbFeKBcznMmf5y", "user": {"username": "casebash"}}, {"_id": "K5ADzEuFn5cJeWgbP", "postedAt": "2023-10-13T01:11:57.461Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Hmm... that isn't exactly the question I'd like the answer too, which is more scaling + minor incremental improvements + creative prompting.</p>", "parentCommentId": "fXDac462eSrNDJvy2", "user": {"username": "casebash"}}, {"_id": "kfFWYNYDZBX4SiouC", "postedAt": "2023-10-13T02:20:52.421Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I'm not an expert on most of the evidence in this post, but I'm extremely suspicious of the claim that GPT-4 represents AI that is \"~ human level at language\", unless you mean something by this that is very different from what most people would expect.</p><p>Technically, GPT-4 is superhuman at language because whatever task you are giving it is in English, and the median human's English proficiency is roughly nil. But a more commonsense interpretation of this statement is that a prompt-engineered AI and a trained human can do the task roughly as well.</p><p>What you link to shows the results of how GPT-4 performs on a bunch of different exams. This doesn't really show how language is used in the real world, especially since the exams very closely match past exams that were in the training data. It's good at some of them, but also extremely bad at others (AP English Literature and Codeforces in particular), which is an issue if you're making a claim that it's roughly human level.</p><p>Furthermore, language isn't just putting words together in the right order and with the right inflection. It also includes semantic information (what the actual meaning of the sentences is) and pragmatic information (is the language conveying what it is trying to convey, not just the literal meaning). I'm not sure whether pragmatics in particular would be relevant for AI risk, but the fact that anecdotally even GPT-4 is pretty bad at pragmatics prevents a literal interpretation of your statement.</p><p>In my opinion, the best evidence for GPT-4 not being human level at language is that, in the real world, GPT-4 is much cheaper than a human but consistently unable to outcompete humans. News organizations have a strong incentive to overhype GPT-caused automation, but the examples that they've found are mostly of people saying that either GPT-4 or GPT-3 (it's not always clear which) did their job much worse than them, but good enough for clients. Take <a href=\"https://www.washingtonpost.com/technology/2023/06/02/ai-taking-jobs/\">https://www.washingtonpost.com/technology/2023/06/02/ai-taking-jobs/</a> as a typical story.&nbsp;</p><p>Exams aren't exactly the real world, but the popular example of GPT-4 doing well on exams is <a href=\"https://www.slowboring.com/p/chatgpt-goes-to-harvard.\">https://www.slowboring.com/p/chatgpt-goes-to-harvard.</a> This both ignores citations (which is a very important part of college writing, and one that GPT-3 couldn't do whatsoever and which GPT-4 still is significantly below what I would expect from a human) and relies on the false belief that Harvard is a hard school to do well at (grade inflation!)</p><p>I still agree with two big takeaways of your post, that an AI pause would be good and that we don't necessarily need AGI for a good future, but that's more because it's robust to a lot of different beliefs about AI than because I agree with the evidence provided. Again, a lot of the evidence is stuff that I don't feel particularly knowledgeable about, I picked this claim because I've had to think about it before and because it just feels false from my experience using GPT-4.</p>", "parentCommentId": null, "user": {"username": "Benjamin M."}}, {"_id": "acvticNnevXc2j3Lv", "postedAt": "2023-10-13T05:17:28.569Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I <a href=\"https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment?commentId=ZQzvvenq6wcP5qSLs\">tend to put P(doom) around 80%</a>, so I think I'm on the pessimistic side, and I tend to think short timelines are at least a real and serious possibility that we should be planning for. Nevertheless, I disagree with a global stop or a pause being the \"only reasonable hope\"\u2014global stops and pauses seem basically unworkable to me. I'm much more excited about governmentally enforced <a href=\"https://www.lesswrong.com/posts/pnmFBjHtpfpAc6dPT/arc-evals-responsible-scaling-policies\">Responsible Scaling Policies</a>, which seem like the \"better option\" that you're missing here.</p>\n", "parentCommentId": null, "user": {"username": "evhub"}}, {"_id": "dENMThyAwR6YhrMmu", "postedAt": "2023-10-13T08:09:21.802Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I was indeed simplifying, and e.g. probably should have said \"global catastrophe\" instead of \"human extinction\" to cover cases like permanent totalitarian regimes. I think some of the scenarios you mention could happen, but also think a bunch of them are pretty unlikely, and also disagree with your conclusion that \"The bulk of the probability lies somewhere in the middle\". I might be up for discussing more specifics, but also I don't get the sense that disagreement here is a crux for either of us, so I'm also not sure how much value there would be in continuing down this thread.</p>\n", "parentCommentId": "YFR7Wjih2Dk9mKA37", "user": {"username": "riceissa"}}, {"_id": "oCL7okq7Xd62iMhuq", "postedAt": "2023-10-13T08:48:21.812Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I mention Responsible Scaling!</p><blockquote><p>Responsible Scaling policies <a href=\"https://www.youtube.com/watch?v=BKgxa9vq9Co&amp;t=2055s&amp;ab_channel=ExistentialRiskObservatory\">are</a>&nbsp;<a href=\"https://twitter.com/Simeon_Cps/status/1707146252498964645\"><u>deeply</u></a>&nbsp;<a href=\"https://twitter.com/NikSamoylov/status/1707166875019604211\"><u>flawed</u></a>; it\u2019s basically an&nbsp;<a href=\"https://twitter.com/gcolbourn/status/1706394954325512444\"><u>oxymoron</u></a> when AGI is so close. The&nbsp;<a href=\"https://twitter.com/michael_nielsen/status/1707184968009683430\"><u>danger</u></a> is already apparent enough (see above) to stop scaling now. The same applies to&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/BFbsqwCuuqueFRfpW/aim-for-conditional-pauses?commentId=gdabE9yxs3sZhbdb2\"><u>conditional pauses</u></a>,&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fZmQ6WQ6MQPa5q39R/how-to-think-about-slowing-ai?commentId=FWhRfWp5XWoyiuzZm\"><u>trying to predict</u></a> when we will get dangerous AI, or&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/fZmQ6WQ6MQPa5q39R/how-to-think-about-slowing-ai?commentId=wgjg85wrjeLffqmiX\"><u>notice \u201cnear-dangerous\u201d AI</u></a>.</p></blockquote><p>EDIT to add: I'm interested in a response from evhub (or anyone else) to the points raised against Responsible Scaling (see links for more details).</p>", "parentCommentId": "acvticNnevXc2j3Lv", "user": {"username": "Greg_Colbourn"}}, {"_id": "uvv8xabZPodbkA2Hs", "postedAt": "2023-10-13T08:51:50.051Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I know William is already aware, but for others who aren't, there are groups focused on getting a pause: <a href=\"https://pauseai.info/\">PauseAI</a> (<a href=\"https://discord.gg/eQv5R3ucAw\">Discord</a>) and the <a href=\"https://join.slack.com/t/agi-moratorium-hq/shared_invite/zt-2561n7g81-mJncdL1MO0rQbrqOuyJTWA\">AGI-Moratorium-HQ Slack</a> are two of them. And there are now quite a lot of people on X with <a href=\"https://twitter.com/search?q=%E2%8F%B8%EF%B8%8F&amp;src=typed_query&amp;f=user\">\u23f8\ufe0f</a> or <a href=\"https://twitter.com/search?q=%E2%8F%B9%EF%B8%8F&amp;src=typed_query&amp;f=user\">\u23f9\ufe0f</a> in their name. I find that it makes me feel better being pro-active about doing something about it.<br><br>I spoke with William at length yesterday. The situation is dire, but I don't think it's impossible.</p>", "parentCommentId": "tCQGbFeKBcznMmf5y", "user": {"username": "Greg_Colbourn"}}, {"_id": "xoanXsDbPKHiKTK2W", "postedAt": "2023-10-13T10:02:28.291Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>How about an Option A.1: pause for a few years or a decade to give alignment a chance to catch up? At least stop at the red lights for a bit to check whether anyone is coming, even if you are speeding!</p><blockquote><p>if you are so pessimistic about aligning AI without a pause that your probability on that is lower than the probability of unaugmented present-day humans reaching longevity escape velocity</p></blockquote><p>I think this easily goes through, even for 1-10% p(doom|AGI), as it seems like ageing is basically already a solved problem or will be within a decade or so (see the <a href=\"https://www.youtube.com/watch?v=Jk_GcwkfJGs&amp;ab_channel=PeterH.Diamandis\">video</a> I linked to - David Sinclair; and there are many other people working in the space with promising research too).</p>", "parentCommentId": "Lyi7ga9gZx4MEGGjL", "user": {"username": "Greg_Colbourn"}}, {"_id": "DxwKmCMzf4sHLkpBx", "postedAt": "2023-10-13T10:31:47.670Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote><p>the median human's English proficiency is roughly nil.</p></blockquote><p>GPT-4 is also proficient at <a href=\"https://openai.com/research/gpt-4#:~:text=GPT%2D4%203%2Dshot%20accuracy%20on%20MMLU%20across%20languages\">many other languages</a>, so I don't think English is the appropriate benchmark! Is GPT-4 as good as the median human at language in general? I think yes. In fact it's probably quite a lot better.</p><blockquote><p>anecdotally even GPT-4 is pretty bad at pragmatics</p></blockquote><p>Can you link to examples? Most examples I've seen on X are people criticising chatGPT-3.5 (or other models) and then someone coming along showing chatGPT-4 getting it right!</p><blockquote><p>GPT-4 or GPT-3 (it's not always clear which)</p></blockquote><p>It's nearly always GPT-3 (or 3.5). We only need to be concerned about the best AI models, not the lower tiers! I've heard anecdotes, in real life, of people who are using GPT-4 to do parts of their jobs - e.g. writing long emails that their boss was impressed with (they didn't tell them it was chatGPT!)</p><blockquote><p>false belief that Harvard is a hard school to do well at</p></blockquote><p>Harvard is one of the best schools in the world. The average human is quite far from being smart enough to get in to it. I don't think saying this is helping the credibility of your argument! Seems a lot like goalpost moving.</p><blockquote><p>I still agree with two big takeaways of your post, that an AI pause would be good and that we don't necessarily need AGI for a good future</p></blockquote><p>Thanks, good to know :)</p>", "parentCommentId": "kfFWYNYDZBX4SiouC", "user": {"username": "Greg_Colbourn"}}, {"_id": "oTaecJkXTpNLuuRuK", "postedAt": "2023-10-13T12:58:27.218Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Why are people downvoting my reply without comment, and upvoting evhub's comment? It's the most upvoted comment, even though he clearly didn't even ctrl-F for \"Responsible Scaling\" / notice that I'd addressed it in the OP!</p>", "parentCommentId": "oCL7okq7Xd62iMhuq", "user": {"username": "Greg_Colbourn"}}, {"_id": "DTHhSsqJgivE2A3zo", "postedAt": "2023-10-13T18:15:28.237Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I guess I'm not really sure what your objection is to Responsible Scaling Policies? I see that there's a bunch of links, but I don't really see a consistent position being staked out by the various sources you've linked to. Do you want to describe what your objection is?</p>\n<p>I guess the closest there is \"the danger is already apparent enough\" which, while true, doesn't really seem like an objection. I agree that the danger is apparent, but I don't think that advocating for a pause is a very good way to address that danger.</p>\n", "parentCommentId": "oCL7okq7Xd62iMhuq", "user": {"username": "evhub"}}, {"_id": "rkJC5sKjq6CCCcNnb", "postedAt": "2023-10-13T19:44:00.614Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p><a href=\"https://forum.effectivealtruism.org/users/evhub?mention=user\">@evhub</a> can you say more about what you envision a governmentally-enforced RSP world would look like? Is it similar to licensing? What happens when a dangerous capability eval goes off\u2014 does the government have the ability to implement a national pause?</p><p>Aside: IMO it's pretty clear that the voluntary-commitment RSP regime is insufficient, since some companies simply won't develop RSPs, and even if lots of folks adopted RSPs, the competitive pressures in favor of racing seem like they'd make it hard for anyone to pause for &gt;a few months. I was surprised/disappointed that neither ARC nor Anthropic mentioned this. ARC <a href=\"https://evals.alignment.org/blog/2023-09-26-rsp/\">says</a> some stuff about how maybe in the future one day we might have some stuff from RSPs that could maybe inform government standards, but (in my opinion) their discussion of government involvement was quite weak, perhaps even to the point of being misleading (by making it seem like the voluntary commitments will be sufficient.)</p><p>I think some of the negative reaction to responsible scaling, at least among some people I know, is that it seems like an attempt for companies to say \"trust us\u2014 we can scale responsibly, so we don't need actual government regulation.\" If the narrative is \"hey, we agree that the government should force everyone to scale responsibly, and this means that the government would have the ability to tell people that they have to <i>stop scaling</i> if the government decides it's too risky\", then I'd still probably prefer stopping right now, but I'd be much more sympathetic to the RSP position.</p>", "parentCommentId": "acvticNnevXc2j3Lv", "user": {"username": "Akash"}}, {"_id": "ThYbDtZZG7JhsRJdA", "postedAt": "2023-10-13T19:59:21.263Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote>\n<p>What happens when a dangerous capability eval goes off\u2014 does the government have the ability to implement a national pause?</p>\n</blockquote>\n<p>I think presumably the pause would just be for that company's scaling\u2014presumably other organizations that were still in compliance would still be fine.</p>\n<blockquote>\n<p>If the narrative is \"hey, we agree that the government should force everyone to scale responsibly, and this means that the government would have the ability to tell people that they have to stop scaling if the government decides it's too risky\", then I'd still probably prefer stopping right now, but I'd be much more sympathetic to the RSP position.</p>\n</blockquote>\n<p>That's definitely my position, yeah\u2014and I think it's also ARC's and Anthropic's position. I think the key thing with the current advocacy around companies doing this is that one of the best ways to get a governmentally-enforced RSP regime is for companies to first voluntarily commit to the sort of RSPs that you want the government to later enforce.</p>\n", "parentCommentId": "rkJC5sKjq6CCCcNnb", "user": {"username": "evhub"}}, {"_id": "MrrTGcmvzhsvMcc2h", "postedAt": "2023-10-13T20:06:43.707Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Thanks! A few quick responses/questions:</p><blockquote><p>I think presumably the pause would just be for that company's scaling\u2014presumably other organizations that were still in compliance would still be fine.</p></blockquote><p>I think this makes sense for certain types of dangerous capabilities (e.g., a company develops a system that has strong cyberoffensive capabilities. That company has to stop but other companies can keep going).</p><p>But what about dangerous capabilities that have more to do with AI takeover (e.g., a company develops a system that shows signs of autonomous replication, manipulation, power-seeking, deception) or scientific capabilities (e.g., the ability to develop better AI systems)?</p><p>Supposing that 3-10 other companies are within a few months of these systems, do you think at this point we need a coordinated pause, or would it be fine to just force company 1 to pause?</p><blockquote><p>That's definitely my position, yeah\u2014and I think it's also ARC's and Anthropic's position.</p></blockquote><p>Do you know if ARC or Anthropic have publicly endorsed this position anywhere? (And if not, I'd be curious for your take on why, although that's more speculative so feel free to pass).&nbsp;</p>", "parentCommentId": "ThYbDtZZG7JhsRJdA", "user": {"username": "Akash"}}, {"_id": "Qz9WLwWYSYMnqmXkK", "postedAt": "2023-10-13T21:10:10.932Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>The consistent position is that further scaling is reckless at this stage; it can't be done in a \"responsible\" way, unless you think subjecting the world to a <a href=\"https://twitter.com/liron/status/1710520914444718459\">10-25%</a> risk of extinction is a responsible thing to be doing!<br><br>What is a better way of addressing the danger? Waiting for it to get more intense and more apparent by scaling further!? Waiting until a disaster actually happens? Actually pausing, or stopping (and setting an example), rather than just advocating for a pause?</p>", "parentCommentId": "DTHhSsqJgivE2A3zo", "user": {"username": "Greg_Colbourn"}}, {"_id": "PThqpTyhphEdi2brd", "postedAt": "2023-10-13T22:01:25.227Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Perhaps the crux is related to how dangerous you think current models are? I'm quite confident that we have at least a couple additional orders of magnitude of scaling before the world ends, so I'm not too worried about stopping training of current models, or even next-generation models. But I do start to get worried with next-next-generation models.</p>\n<p>So, in my view, the key is to make sure that we have a well-enforced Responsible Scaling Policy (RSP) regime that is capable of preventing scaling unless hard safety metrics are met (I favor <a href=\"https://www.lesswrong.com/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations\">understanding-based evals</a> for this) before the next two scaling generations. That means we need to get good RSPs into law with solid enforcement behind them and\u2014at least in very short timeline worlds\u2014that needs to happen in the next few years. By far the best way to make that happen, in my opinion, is to pressure labs to put out good RSPs now that governments can build on.</p>\n", "parentCommentId": "Qz9WLwWYSYMnqmXkK", "user": {"username": "evhub"}}, {"_id": "47FKwNvoAXQGBoKWn", "postedAt": "2023-10-13T22:04:54.831Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote>\n<p>But what about dangerous capabilities that have more to do with AI takeover (e.g., a company develops a system that shows signs of autonomous replication, manipulation, power-seeking, deception) or scientific capabilities (e.g., the ability to develop better AI systems)?</p>\n<p>Supposing that 3-10 other companies are within a few months of these systems, do you think at this point we need a coordinated pause, or would it be fine to just force company 1 to pause?</p>\n</blockquote>\n<p>What should happen there is that the leading lab is forced to stop and try to demonstrate that e.g. <a href=\"https://www.lesswrong.com/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations\">they understand their model sufficiently such that they can keep scaling</a>. Then:</p>\n<ul>\n<li>If they can't do that, then the other labs catch up and they're all blocked on the same spot, which if you've put your capabilities bars at the right spots, shouldn't be dangerous.</li>\n<li>If they can do that, then they get to keep going, ahead of other labs, until they hit another blocker and need to demonstrate safety/understanding/alignment to an even greater degree.</li>\n</ul>\n", "parentCommentId": "MrrTGcmvzhsvMcc2h", "user": {"username": "evhub"}}, {"_id": "kvnC8yrZk69mrjkXv", "postedAt": "2023-10-13T22:44:23.233Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I don't think the current models are dangerous, but perhaps they <a href=\"https://www.stop.ai/proposals#header-5\">could be</a> if used for long enough on improving AI. A couple of orders of magnitude (or a couple of generations) is only a couple of years! This is soon enough to be pushing as hard as we can for a pause right now!<br><br>Why try and take it right down to the wire with RSPs? Why over-complicate things? The stakes couldn't be bigger (extinction). It's super reckless to not just be saying \"It seems quite likely we're getting to world-ending models in 2-5 years. Let's not keep going any longer. Let's just stop now.\" The tradeoff [edit: for Anthropic] for a few tens of $Bs of extra profit really doesn't seem worth it!</p>", "parentCommentId": "PThqpTyhphEdi2brd", "user": {"username": "Greg_Colbourn"}}, {"_id": "AYJmxP7ohKJaBsgKy", "postedAt": "2023-10-13T23:00:45.794Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote>\n<p>This is soon enough to be pushing as hard as we can for a pause right now!</p>\n</blockquote>\n<p>I mean, yes, obviously we should be doing everything we can right now. I just think that a RSP-gated pause is the right way to do a pause. I'm not even sure what it would mean to do a pause without an RSP-like resumption condition.</p>\n<blockquote>\n<p>Why try and take it right down to the wire with RSPs?</p>\n</blockquote>\n<p>Because it's more likely to succeed. RSPs provides very clear and legible risk-based criteria that are much more plausibly things that you could actually get a government to agree to.</p>\n<blockquote>\n<p>The tradeoff for a few tens of $Bs of extra profit really doesn't seem worth it!</p>\n</blockquote>\n<p>This seems extremely disingenuous and bad faith. That's obviously not the tradeoff and it confuses me why you would even claim that. Surely you know that I am not Sam Altman or Dario Amodei or whatever.</p>\n<p>The actual tradeoff is the probability of success. If I thought e.g. just advocating for a six month pause right now was more effective at reducing existential risk, I would do it.</p>\n", "parentCommentId": "kvnC8yrZk69mrjkXv", "user": {"username": "evhub"}}, {"_id": "B2u3b4J2kWPuRGwGu", "postedAt": "2023-10-13T23:30:36.236Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote><p>I'm not even sure what it would mean to do a pause without an RSP-like resumption condition.</p></blockquote><p>Have the resumption condition be a global consensus on an x-safety solution or a global democratic mandate for restarting (and remember there are more components of x-safety than just alignment - also misuse and multi-agent coordination).</p><blockquote><p>much more plausibly things that you could actually get a government to agree to.</p></blockquote><p>I think if governments actually properly appreciated the risks, they could agree to an unconditional pause.&nbsp;</p><blockquote><p>This seems extremely disingenuous and bad faith. That's obviously not the tradeoff and it confuses me why you would even claim that. Surely you know that I am not Sam Altman or Dario Amodei or whatever.</p></blockquote><p>Sorry. I'm looking at it at the company level. Please don't take my critiques as being directed at you personally. What is in it for Anthropic and OpenAI and DeepMind to keep going with scaling? Money and power, right? I think it's pushing it a bit at this stage to say that they, as companies, are primarily concerned with reducing x-risk. If they were they would've stopped scaling already. Forget the (suicide) race. Set an example to everyone and just stop!</p>", "parentCommentId": "AYJmxP7ohKJaBsgKy", "user": {"username": "Greg_Colbourn"}}, {"_id": "2WyaDkQocEN8Jsyv2", "postedAt": "2023-10-13T23:37:38.068Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote>\n<p>Have the resumption condition be a global consensus on an x-safety solution or a global democratic mandate for restarting (and remember there are more components of x-safety than just alignment - also misuse and multi-agent coordination).</p>\n</blockquote>\n<p>This seems basically unachievable and even if it was achievable it doesn't even seem like the right thing to do\u2014I don't actually trust the global median voter to judge whether additional scaling is safe or not. I'd much rather have rigorous technical standards then nebulous democratic standards.</p>\n<blockquote>\n<p>I think it's pushing it a bit at this stage to say that they, as companies, are primarily concerned with reducing x-risk.</p>\n</blockquote>\n<p>That's why we should be pushing them to have good RSPs! I just think you should be pushing on the RSP angle rather than the pause angle.</p>\n", "parentCommentId": "B2u3b4J2kWPuRGwGu", "user": {"username": "evhub"}}, {"_id": "svZkGsxpCD3udjRzM", "postedAt": "2023-10-14T04:09:01.654Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I wrote up a bunch of my thoughts on this in more detail <a href=\"https://forum.effectivealtruism.org/posts/geMaKHFfzHskjsnjq/rsps-are-pauses-done-right\">here</a>.</p>\n", "parentCommentId": "MrrTGcmvzhsvMcc2h", "user": {"username": "evhub"}}, {"_id": "dvmBxyMz7ZggYjtHo", "postedAt": "2023-10-14T09:40:37.780Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote><p>I'd much rather have rigorous technical standards then nebulous democratic standards.</p></blockquote><p>Fair. And where I say \"global consensus on an x-safety\", I mean expert opinion (as I say in the OP). I expect the public to remain generally a lot more <a href=\"https://forum.effectivealtruism.org/posts/E6CahapSad7psvqx4/timelines-are-short-p-doom-is-high-a-global-stop-to-frontier?commentId=2WyaDkQocEN8Jsyv2#:~:text=The%C2%A0public-,doesn%E2%80%99t%20want%20it,-%5B23%5D.%20Uncertainty\">conservative</a> than the technical experts though, in terms of risk they are willing to tolerate.</p><blockquote><p>I just think you should be pushing on the RSP angle rather than the pause angle.</p></blockquote><p>The RSP angle is part of the corporate \"big AI\" \"business as usual\" agenda. To those of us playing the outside game it seems very close to <a href=\"https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing\">safetywashing</a>.</p>", "parentCommentId": "2WyaDkQocEN8Jsyv2", "user": {"username": "Greg_Colbourn"}}, {"_id": "KwNzKjXyn5yczxq2K", "postedAt": "2023-10-14T17:41:17.412Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<blockquote>\n<p>The RSP angle is part of the corporate \"big AI\" \"business as usual\" agenda. To those of us playing the outside game it seems very close to safetywashing.</p>\n</blockquote>\n<p>I've written up more about why I think this is not true <a href=\"https://forum.effectivealtruism.org/posts/geMaKHFfzHskjsnjq/rsps-are-pauses-done-right\">here</a>.</p>\n", "parentCommentId": "dvmBxyMz7ZggYjtHo", "user": {"username": "evhub"}}, {"_id": "GkEHZcTvoPywzbrRe", "postedAt": "2023-10-15T12:01:04.761Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Thanks. I'm <a href=\"https://forum.effectivealtruism.org/posts/geMaKHFfzHskjsnjq/rsps-are-pauses-done-right?commentId=P9DGetewwSrbhuG56\">not</a> <a href=\"https://forum.effectivealtruism.org/posts/geMaKHFfzHskjsnjq/rsps-are-pauses-done-right?commentId=7H79trHCbdKHiqpLL\">convinced</a>.</p>", "parentCommentId": "KwNzKjXyn5yczxq2K", "user": {"username": "Greg_Colbourn"}}, {"_id": "txbTdj8srHt4b8etM", "postedAt": "2023-10-16T23:24:50.839Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Hi Chris thanks for reaching out. Obviously things with the world aren't ok, it seems insane that every country is staring down a massive national security risk and they haven't done much about it.<br><br>How is movement building going?</p>", "parentCommentId": "frQsWQ5uuubTnojLg", "user": {"username": "William the Kiwi"}}, {"_id": "B3ryNJBHnhL3btNsF", "postedAt": "2023-10-16T23:26:18.047Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Yea, thanks for the talk Greg, it was informative.</p>", "parentCommentId": "uvv8xabZPodbkA2Hs", "user": {"username": "William the Kiwi"}}, {"_id": "axxbrirHkwh6xpEj5", "postedAt": "2023-10-17T01:38:21.554Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Upvoted your post because you made some good points, but I think your analogy between human cloning and AI training is totally wrong.</p>\n<blockquote>\n<p>Take for example, human reproductive cloning. This is so morally abhorrent that it is not being practised anywhere in the world. There is no black market in need of a global police state to shut it down. AGI research could become equally uncool once the danger, and loss of sovereignty, it represents is sufficiently well appreciated.</p>\n</blockquote>\n<p>There is no black market in human cloning, and no police state trying to stop it, because no one benefits very much from cloning. Cloning is just not that useful. Whereas if we stop corporate AI development for 40 years but computer hardware keeps improving, anyone can get rich by training an AGI on their gaming laptop. It would be like trying to confiscate all the drug imports in a world where everyone with a cell phone is a drug addict.</p>\n", "parentCommentId": null, "user": {"username": "robirahman"}}, {"_id": "xNZi4MSwe3FJ6SBwu", "postedAt": "2023-10-17T01:38:28.414Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I would agree that \"utopia in our lifetime\" or \"extinction\" seems like a false dichotomy. What makes you say that you predict the bulk of the probability lies somewhere in the middle?</p>", "parentCommentId": "YFR7Wjih2Dk9mKA37", "user": {"username": "William the Kiwi"}}, {"_id": "Gkn5N9PitBQGbTPug", "postedAt": "2023-10-17T01:52:17.498Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>GPT4 is clearly above the median human when it comes to a range of exams. Do we have examples of GPT4's comparison to the median human in non-exam like conditions?</p>", "parentCommentId": "DxwKmCMzf4sHLkpBx", "user": {"username": "William the Kiwi"}}, {"_id": "kvnsndBgDRkMNuqbu", "postedAt": "2023-10-17T01:54:05.633Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I would agree, relying on pre-GPT4 estimates seems flawed.</p>", "parentCommentId": "jeLwYRTTDp6PQReXt", "user": {"username": "William the Kiwi"}}, {"_id": "Ds3YTdkzGaHRBLSEv", "postedAt": "2023-10-17T01:59:20.902Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>What part of Greg writing comes across as over confident?</p>", "parentCommentId": "eGoLCvFby4SzppLqd", "user": {"username": "William the Kiwi"}}, {"_id": "LZF65mDwsBww4pzGE", "postedAt": "2023-10-17T06:23:36.349Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I'll reply via PM.</p>", "parentCommentId": "txbTdj8srHt4b8etM", "user": {"username": "casebash"}}, {"_id": "jHQExzTyD86FKGL74", "postedAt": "2023-10-17T10:32:54.573Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Thanks. I also address the \"get rich\" point though! People can't get rich from it AGI because they lose control of it (/the world ends) before they get rich. AGI is not that useful either, because it's uncontrollable and has negative externalities that will come back and swamp any hoped for benefits, even for the producer (i.e. x-risk).</p><blockquote><p>Assumptions of wealth generation, economic abundance and public benefit from AGI are&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/k6K3iktCLCTHRMJsY/the-possibility-of-an-indefinite-ai-pause?commentId=AWxRScWjaqfJwHpAL\"><u>ungrounded</u></a> without proof that they are even possible - and they&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/k6K3iktCLCTHRMJsY/the-possibility-of-an-indefinite-ai-pause?commentId=Bni6aGyxKqjw8dXig\"><u>aren\u2019t</u></a> (yet), given a lack of&nbsp;<a href=\"https://twitter.com/gcolbourn/status/1709406319340331040\"><u>scalable-to-ASI</u></a> solutions to alignment, misuse, and coordination. A stop isn\u2019t taking away the ladder to heaven, for such a ladder does not exist, even in theory, as things stand.</p></blockquote>", "parentCommentId": "axxbrirHkwh6xpEj5", "user": {"username": "Greg_Colbourn"}}, {"_id": "ZiB97tYrdqDw6nDWg", "postedAt": "2023-10-19T06:35:34.026Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>AGI development is already taboo outside of tech circles. Per the <a href=\"https://theaipi.org/poll-shows-voters-oppose-open-sourcing-ai-models-support-regulatory-representation-on-boards-and-say-ai-risks-outweigh-benefits-2/\">September poll</a> by the AIPI, only 12% disagree that \"Preventing AI from quickly reaching superhuman capabilities\" should be an important AI policy goal. (56% strongly agree, 20% somewhat agree, 8% somewhat disagree, 4% strongly disagree, 12% not sure.) Despite the fact that world leaders are themselves influenced by tech circles' positions, leaders around the world are <a href=\"https://www.lesswrong.com/posts/x3JpgTnqcrzhedwAb/unga-general-debate-speeches-on-ai\">quite clear</a> that they take the risk seriously.</p><p>The only reason AGI development hasn't been halted already is that the general public does not yet know that big tech is both trying to build AGI, and actually making real progress towards it.</p>", "parentCommentId": "W95YcBacgvqYxzEDy", "user": {"username": "Odd anon"}}, {"_id": "tLhEFnkrW6DGHKQRe", "postedAt": "2023-10-19T20:32:42.738Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>I've now made post this into an<a href=\"https://twitter.com/gcolbourn/status/1715099156174266443\"> X thread</a> (with some slight edits and some condensing).</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "2eJAZ8tBMCRWk7j7z", "postedAt": "2024-02-17T11:07:39.991Z", "postId": "E6CahapSad7psvqx4", "htmlBody": "<p>Strong upvote.&nbsp;</p><p>GPT-1 was <a href=\"https://en.wikipedia.org/wiki/GPT-1\">released 2018</a>. GPT-4 has shown <a href=\"https://arxiv.org/abs/2303.12712\">sparks of AGI</a>.&nbsp;</p><p>We have early evidence of <a href=\"https://ai-improving-ai.safe.ai\">self-improvement</a> -- or conservatively -- positive feedback loops are evident.</p><p>Open AI intends to <a href=\"https://openai.com/blog/introducing-superalignment\">build ~AGI to automate alignment research</a>. Sam Altman is attempting to raise <a href=\"https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0\">$7T USD to build more GPUs</a>.&nbsp;</p><p>Anthropic CEO estimates <a href=\"https://www.youtube.com/watch?v=Nlkk3glap_U\">2-3 years until AGI</a>.&nbsp;</p><p>Meta has gone public about their goal of <a href=\"https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview\">open-sourcing AGI</a>.&nbsp;</p><p>Superalignment might <a href=\"https://dl.acm.org/doi/pdf/10.1145/3603371\">even be impossible</a>.&nbsp;</p><p>It seems to be difficult to <a href=\"https://www.lesswrong.com/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control\">defend a world against rouge </a>AGIs, it seems difficult for aligned AIs to defend us.</p>", "parentCommentId": null, "user": {"username": "\u30d0\u30eb\u30b9"}}]