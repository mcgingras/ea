[{"_id": "Q2hTZPCwytAgDLb5s", "postedAt": "2023-07-09T14:28:38.397Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<p>Some notes (written quickly!):&nbsp;</p><ol><li>You write \"it can either be an advisory system or it can be a deterministic system,\" but note that an advisory system misses out on a key benefit of what I understand as ~classic futarchy: the fact that it solves the problem of conditionality not being the same as causality. See <a href=\"https://forum.effectivealtruism.org/posts/ijohdoDbPvdeXMpiz/summary-and-takeaways-hanson-s-shall-we-vote-on-values-but#Issues_with_the_proposals\">here (from my earlier post)</a>:<ol><li><strong>Causality might diverge from conditionality</strong> in the case of advisory/indirect markets.<a href=\"https://forum.effectivealtruism.org/posts/ijohdoDbPvdeXMpiz/summary-and-takeaways-hanson-s-shall-we-vote-on-values-but#fn-3S6iFRJCAheKPetmu-10\"><sup>[10]</sup></a> Traders are sometimes rewarded for guessing at hidden info about the world\u2014information that is revealed by the fact that a policy decision was made\u2014instead of causal relationships between the policy and outcomes.<a href=\"https://forum.effectivealtruism.org/posts/ijohdoDbPvdeXMpiz/summary-and-takeaways-hanson-s-shall-we-vote-on-values-but#fn-3S6iFRJCAheKPetmu-11\"><sup>[11]</sup></a><ol><li>For instance, suppose a company is running a market to decide whether to keep an unpopular CEO, and they ask if, say, stocks conditional on the CEO remaining would be higher than stocks conditional on the CEO not staying. Then traders might think that, if it is the case that the CEO stayed, it is likely that the Board found out something really great about the CEO, which would increase expectations that the CEO would perform very well (and stocks would rise). So the market would seem to imply that the CEO is good for the company even if they were actually terrible.</li></ol></li><li>There's also a post on this (haven't re-read in a while, only vaguely remember): <a href=\"https://forum.effectivealtruism.org/posts/gDsppmv8TzrH2zKJc/prediction-market-does-not-imply-causation\">Prediction market does not imply causation</a></li></ol></li><li>I think getting a metric that's good enough would be really difficult, and your post undersells this<ol><li>E.g. it should probably take into account the cost. I'm a bit suspicious of metrics that depend on a vote 5 years from now. There are still issues with forecasting on decently long periods, and if I'm thinking about how I would bet on a metric like this, I'd go look at the people doing the voting, I'd be trying to forecast how beliefs and biases of the relevant group would change (which could be useful, but it's not exactly what grants are for), etc. If the point is ability to scale this up, the metric should be applicable to a wide variety of grants. Grant quality could depend on information that isn't accessible to a wide audience (like how much a grant supported a specific person's upskilling), etc.&nbsp;</li><li>But maybe my argument is ~<a href=\"https://normielisation.substack.com/p/cheems-mindset\">cheemsy</a> and there are versions of this that work at least as well as current grantmaking systems, or would have sufficient VoI \u2014 I'm not sure, but I want to stress that at least for me to believe that this is not a bad idea, the burden of proof would be on showing that the metric is fine.&nbsp;</li></ol></li><li>I'm also honestly not sure why you seem so skeptical that manipulation wouldn't happen, but maybe there are clever solutions to this. I haven't thought about this seriously (at least, not for around 2 years).&nbsp;</li></ol><p>Relatedly, 2 years ago I wrote two posts about futarchy \u2014 I haven't read them in forever, they probably have mistakes, and my opinions have probably shifted since then, but linking them here anyway just in case:</p><ul><li><a href=\"https://forum.effectivealtruism.org/posts/ijohdoDbPvdeXMpiz/summary-and-takeaways-hanson-s-shall-we-vote-on-values-but\">Summary and Takeaways: Hanson's \u201cShall We Vote on Values, But Bet on Beliefs?\u201d</a></li><li><a href=\"https://forum.effectivealtruism.org/posts/E4QnGsXLEEcNysADT/issues-with-futarchy\">Issues with Futarchy</a></li></ul>", "parentCommentId": null, "user": {"username": "Lizka"}}, {"_id": "7fWnHswfk23ohKJ6v", "postedAt": "2023-07-09T16:23:30.710Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<p>Thanks for the writeup, Nathan; I am indeed excited about the possibility of making better grants through forecasting/futarchic mechanisms. So I'll start from the other direction: instead of reaching for futarchy as a <a href=\"https://en.wikipedia.org/wiki/Law_of_the_instrument\">hammer</a>, start with, what are current major problems grantmakers face?</p><p>The problem that seems most important to solve: \"finding projects that turn out to be orders of magnitude more successful/impactful than the rest\". Paul Graham describes funding seed-stage startups as \"<a href=\"http://www.paulgraham.com/swan.html\">farming black swans</a>\", which rings true to me. To look at two example rounds from ACX Grants, which I've been involved in:</p><ul><li><a href=\"https://astralcodexten.substack.com/p/acx-grants-project-updates\">ACX Grants</a>: Many of the projects look good, but a handful seem to have gotten outlier success; I would count Lars and Will's Valuebase, the Oxfendazole group, and our own Manifold as having gone on to raise millions in further funding.</li><li><a href=\"https://manifund.org/rounds/acx-mini-grants\">ACX Forecasting Mini-grants:</a> Still a bit early to tell, but OPTIC and BaseRateTimes (which we missed!) seem to have hit their goals and continue on to work on cool things.</li></ul><p>So right now, I'm most interested in mechanisms that help us find such founders/projects. Just daydreaming here, is there any kind of prediction mechanism that can turn out a report as informative as the ACX Grants 1-year project update? The information value in most prediction markets is \"% chance given by the market\", which misses out on the valuable qualitative sketches given by a retroactive writeup.</p><p>Other promising things:</p><ul><li>Asking grantees to set up markets for their own outcomes; eg \"If funded, will we successfully publish a paper that receives &gt;10 citations within 1 year?\" this might clarify exactly what goals the grantees are trying to hit.</li><li>Doing some kind of impact analysis for alignment work in past years; imagine a kind of \"AI Safety Nobel Prizes\" which identify what work turned out to be the most important. This would give future forecasting tools something concrete to predict on.</li></ul>", "parentCommentId": null, "user": {"username": "akrolsmir"}}, {"_id": "ycP66yZ9YbRzh7QzX", "postedAt": "2023-07-09T16:30:29.491Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<p>Thanks for the thoughts (and your posts on Futarchy years ago, I found them to be a helpful review of the literature!)</p><blockquote><p>I'm a bit suspicious of metrics that depend on a vote 5 years from now.</p></blockquote><p>I am too, though perhaps for different reasons. Long-term forecasting has slow feedback loops, and fast feedback loops are important for designing good mechanisms. Getting futarchy to be useful probably involves a lot of trial-and-error, which is hard when it takes you 5 years to assess \"was this thing any good?\"</p>", "parentCommentId": "Q2hTZPCwytAgDLb5s", "user": {"username": "akrolsmir"}}, {"_id": "uK2CiE48BLn2j4MYP", "postedAt": "2023-07-09T17:00:48.027Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<p>I thought \"Causality might diverge from conditionality\" is a big problem for \"classic futarchy\" too? E.g. if we're betting on welfare conditional on policy A and on welfare conditional on policy B, <i>the market price for welfare conditional on policy A</i> = <i>expected welfare conditional on [policy A has the highest expected welfare at market close and thus is implemented] (or something)</i> &gt; <i>expected welfare <strong>counterfactually</strong> conditional on policy A</i>. I guess the \"&gt;\" approaches \"=\" near market closing time, under certain reasonable assumptions? Huh.</p>", "parentCommentId": "Q2hTZPCwytAgDLb5s", "user": {"username": "zsp"}}, {"_id": "cidd8kRamcWvhnWGr", "postedAt": "2023-07-09T22:08:21.339Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<p>Fwiw I think this is an issue with grantmaking too.&nbsp;</p>", "parentCommentId": "ycP66yZ9YbRzh7QzX", "user": {"username": "nathan"}}, {"_id": "DdnP8j8nyyS79AsdH", "postedAt": "2023-07-09T22:27:22.843Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<p><i>I feel slightly attacked, which I don't think was your intention. I probably would have written a blunter response to someone elses, so i guess I'm a hypocrite. I wonder what the best forum responder in the world would look like.</i></p><p>On your first point, I think you are right. Causation and prediction markets are tricky. I guess I sort of think that there are bigger bottlenecks to figure out first. But maybe that's lax of me.</p><p>On your second point. \"I'm a bit suspicious of metrics that depend on a vote 5 years from now.\" how do you reconcile this with OpenPhil's grantmaking, which probably often takes 5+ years to resolve. Do you criticise them for this? Feels like not and apples to apples comparison if you don't.</p><p>\"me to believe that this is not a bad idea,\" I mean, I think presumably it wouldn't be a bad idea if it was a $50k grantmaker with a stopgap if it seemed to be funding obviously crazy or corrupt stuff. Feels like the bar of lizka thinking it's actively good is too high. If I managed that in a quick blog post I think we'd be really impressed.</p><p>\"I'm also honestly not sure why you seem so skeptical that manipulation wouldn't happen,\" has there ever been any prediction market manipulation. Seems like there are quite a lot of markets running all the time? Other than Keynesian Beauty Contests and markets where you can literally change the criteria<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqwn9f8obxx\"><sup><a href=\"#fnqwn9f8obxx\">[1]</a></sup></span>, has it ever happened? @austin do you know of any, I can't recall any. And if we think of examples of actual market manipulation, they would probably fool grantmakers too, right? And manifold's betting is all public, so you can't pump the market. I guess, \"what's your story for how this has ever happened in a way it might happen here?\"</p><p>I guess, maybe pick one of these as your biggest criticism and I might spend 30 mins figuring out my response to it?</p><p>Also, I wish that the Futarchy post on the forum wiki was your post on futarchy. Then we could argue some of this there, in what I suggest would be the \"best\" place for it. I have edited the <a href=\"https://www.lesswrong.com/tag/futarchy\">lesswrong tag</a> in that rough direction</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqwn9f8obxx\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqwn9f8obxx\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I seem to recall someone bought a load of cinema tickets to rig a market, but in our case the way to do that would be to change the final metric, which would likely be really good.</p></div></li></ol>", "parentCommentId": "Q2hTZPCwytAgDLb5s", "user": {"username": "nathan"}}, {"_id": "eo2arGfSyhFGKx76K", "postedAt": "2023-07-10T07:46:48.563Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<blockquote><p>Many of the projects look good, but a handful seem to have gotten outlier success; I would count Lars and Will's Valuebase, the Oxfendazole group, and our own Manifold as having gone on to raise millions in further funding.</p></blockquote><p>Do you think there was a sense that this might be the case?</p><blockquote><p>So right now, I'm most interested in mechanisms that help us find such founders/projects. Just daydreaming here, is there any kind of prediction mechanism that can turn out a report as informative as the ACX Grants 1-year project update?</p></blockquote><p>I guess you could encourage anyone to make markets, not just the funders. Then have some way to select the 10 most interesting markets. If you wanted you could try and run an LLM to generate text for some kind of premortem. Seems a bit galaxy brained though.</p>", "parentCommentId": "7fWnHswfk23ohKJ6v", "user": {"username": "nathan"}}, {"_id": "BwxiNZH7GPp7Gs25F", "postedAt": "2023-08-19T21:01:00.227Z", "postId": "Hn5sdvhYZCYw6Mteg", "htmlBody": "<blockquote><p>I don't buy that someone can jump on the markets and mess with them. Manifold (or better real money) markets have great incentives against this. I am unaware of any time this has happened in a way that it would in a live prediction market.</p></blockquote><p>Manifold markets hasn't solved longterm market incentives yet:<br><a href=\"https://manifold.markets/Nu%C3%B1oSempere/this-question-will-resolve-positive-4a418ad86de3#jdUj58EdDXBbF8ZNyXaT\">https://manifold.markets/Nu%C3%B1oSempere/this-question-will-resolve-positive-4a418ad86de3#jdUj58EdDXBbF8ZNyXaT</a><br>There are even accounts dedicated to messing with Manifold markets:<br><a href=\"https://manifold.markets/ButtocksCocktoasten?tab=questions\">https://manifold.markets/ButtocksCocktoasten?tab=questions</a><br>Manifold doubled mana loan amounts this month to try to mitigate the longterm market incentive issues on the platform</p>", "parentCommentId": null, "user": {"username": "Pat Myron"}}]