[{"_id": "mrsfqCT3tnHuLLRfF", "postedAt": "2023-10-25T00:49:31.382Z", "postId": "LKbWgJjtQDaKbn9HM", "htmlBody": "<p>I'm sorry, this doesn't engage with the main point(s) you are trying to make, but I'm not sure why you use the term \"existential risk\" (which you define as risks of human extinction and undesirable lock-ins that don\u2019t involve s-risk-level suffering) when you could have just used the term \"extinction risk\".</p><p>You say:</p><blockquote><p>If you\u2019re uncertain whether humanity\u2019s future will be net positive, and therefore whether existential risk<a href=\"#fnaqcawnj5e6w\"><sup>[1]</sup></a>&nbsp;reduction is good, you might reason that we should keep civilization going for now so we can learn more and, in the future, make a better-informed decision about whether to keep it going.</p></blockquote><p>Reducing extinction risk if humanity's future is net negative is bad. However, reducing risk of \"undesirable lock-ins\" seems robustly good no matter what the expected value of the future is. So I'm not sure bucketing these two together under the heading of \"existential risk\" really works.</p>", "parentCommentId": null, "user": {"username": "jackmalde"}}, {"_id": "rRvMXdXuGcdCYMDyh", "postedAt": "2023-10-25T01:51:12.470Z", "postId": "LKbWgJjtQDaKbn9HM", "htmlBody": "<p>Thanks :) Good point.<br><br>Minor point: I don't think it's strictly true that reducing risks of undesirable lock-ins is robustly good no matter what the expected value of the future is. It could be that a lock-in is not good, but it prevents an even worse outcome from occurring.<br><br>I included other existential risks in order to counter the following argument: \"As long as we prevent non-s-risk-level undesirable lock-ins in the near-term, future people can coordinate to prevent s-risks.\" This is a version of the option value argument that isn't about extinction risk. I realize this might be a weird argument for someone to make, but I covered it to be comprehensive.<br><br>But the way I wrote this, I was pretty much just focused on extinction risk. So I agree it doesn't make a lot of sense to include other kinds of x-risks. I'll edit this now.</p>", "parentCommentId": "mrsfqCT3tnHuLLRfF", "user": {"username": "Winston"}}, {"_id": "qkKKddyqKRAMKcn2R", "postedAt": "2023-10-25T07:36:03.779Z", "postId": "LKbWgJjtQDaKbn9HM", "htmlBody": "<p>I think this is an important point. In general terms, it seems worth keeping in mind that option value also entails option <i>dis</i>value (e.g. the option of losing control and giving rise to a worst-case future).</p><p>Regarding long reflection in particular, I notice that the quotes above seem to mostly mention it in a positive light, yet its feasibility and desirability can also be separately criticized, as I've tried to do <a href=\"https://magnusvinding.com/2022/05/09/in-defense-of-research/#objections-what-about-long-reflection-and-the-division-of-labor\">elsewhere</a>:</p><blockquote><p>First, there are&nbsp;<a href=\"https://www.overcomingbias.com/p/long-reflection-is-crazy-bad-ideahtml\">reasons to doubt</a> that a condition of long reflection is feasible or even&nbsp;desirable,&nbsp;given that it would seem to require strong limits to voluntary actions that diverge from the ideal of reflection. To think that we can choose to create a condition of long reflection may be an instance of the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Illusion_of_control\">illusion of control</a>. Human civilization is likely to develop according to its immediate interests, and seems&nbsp;<a href=\"https://www.overcomingbias.com/2014/04/humanity-cant-steer-its-future-much.html\">unlikely</a>&nbsp;to ever be steered via a common process of reflection.</p><p>Second, even if we were to secure a condition of long reflection, there is no guarantee that humanity would ultimately be able to reach a sufficient level of agreement regarding the right path forward \u2014 after all, it is conceivable that a long reflection could go awfully wrong, and that bad values could win out due to poor execution or malevolent agents hijacking the process.</p><p>The limited feasibility of a long reflection suggests that there is no substitute for reflecting now. Failing to clarify and act on our values from this point onward carries a serious risk of pursuing a suboptimal path that we may not be able to reverse later. The resources we spend pursuing a long reflection (which seems unlikely to ever occur) are resources not spent on addressing issues that might be more important and more time-sensitive, such as steering away from <a href=\"https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\">worst-case outcomes</a>.&nbsp;</p></blockquote>", "parentCommentId": null, "user": {"username": "MagnusVinding"}}, {"_id": "fMEvKJhbqvardZuy4", "postedAt": "2023-10-25T12:44:45.607Z", "postId": "LKbWgJjtQDaKbn9HM", "htmlBody": "<p><strong>Executive summary:</strong> The option value argument for reducing extinction risk is weak, since it fails in the dystopian futures where it's most needed. Future agents in such worlds likely won't have the motivation or coordination to shut down civilization.</p><p><strong>Key points</strong>:</p><ol><li>The option value argument says we should reduce extinction risk so future agents can decide whether to continue civilization. But this requires dystopian futures to have the altruism and coordination to shut down, which is unlikely.</li><li>Bad futures with indifferent or malicious agents won't make the right decisions about ending civilization. So option value doesn't help in the cases where it's most needed.</li><li>Most expected disvalue comes from uncontrolled futures passing points of no return, not deliberate choices after moral reflection. So option value doesn't preserve much value for downside-focused views.</li><li>Reducing extinction risk doesn't entail reducing s-risks, which could still occur in survived dystopian futures. So it's not the most robust approach under moral uncertainty.</li><li>Some option value remains, but it is not strong enough alone to make extinction risk an overwhelming priority compared to improving future quality.</li></ol><p>&nbsp;</p><p><i>This comment was auto-generated by the EA Forum Team. Feel free to point out issues with this summary by replying to the comment, and</i><a href=\"https://forum.effectivealtruism.org/contact\"><i>&nbsp;<u>contact us</u></i></a><i> if you have feedback.</i></p>", "parentCommentId": null, "user": {"username": "SummaryBot"}}, {"_id": "GqWssSCjnofz2ZtTk", "postedAt": "2023-10-26T09:16:13.165Z", "postId": "LKbWgJjtQDaKbn9HM", "htmlBody": "<p>Good to see this point made on the forum! I discuss this as well in my 2019 MA Philosophy thesis (based off similar sources): <a href=\"http://www.sieberozendal.com/wp-content/uploads/2020/01/Rozendal-S.T.-2019-Uncertainty-About-the-Expected-Moral-Value-of-the-Long-Term-Future.-MA-Thesis.pdf\">http://www.sieberozendal.com/wp-content/uploads/2020/01/Rozendal-S.T.-2019-Uncertainty-About-the-Expected-Moral-Value-of-the-Long-Term-Future.-MA-Thesis.pdf</a></p>\n<blockquote>\n<p>Only when humanity is both able and motivated to significantly change the course of the future do we have option value. However, suppose that our descendants both have the ability and the motivation to affect the future for the good of everyone, such that a future version of humanity is wise enough to recognize when the expected value of the future is\nnegative and coordinated and powerful enough to go extinct or make other significant changes. As other authors have raised (Brauner &amp; Grosse-Holz, 2018), given such a state of affairs it seems unlikely that the future would be bad! After all, humanity would be wise, powerful, and coordinated. Most of the bad futures we are worried about do not follow from such a version of humanity, but from a version that is powerful but unwise and/or uncoordinated.</p>\n</blockquote>\n<blockquote>\n<p>To be clear, there would be a small amount of option value. There could be some fringe cases in which a wise and powerful future version of humanity would have good reason to expect the future to be better if they went extinct, and be able to do so. Or perhaps\nit would be possible for a small group of dedicated, altruistic agents to bring humanity to extinction, without risking even worse outcomes. At the same time they would need to be unable to improve humanity\u2019s trajectory significantly in any other way for extinction to be their highest priority. Furthermore, leaving open this option also works the other way\naround: a small group of ambitious individuals could make humanity go extinct if the future looks overwhelmingly positive.</p>\n</blockquote>\n<p>Never got around to putting that excerpt on the forum</p>\n", "parentCommentId": null, "user": {"username": "SiebeRozendal"}}, {"_id": "LHJEbjME7HADWN7Ct", "postedAt": "2024-01-09T03:36:22.315Z", "postId": "LKbWgJjtQDaKbn9HM", "htmlBody": "<p>It's great to have these quotes all in one place. :)</p>\n<p>In addition to the main point you made -- that the futures containing the most suffering are often the ones that it's too late to stop -- I would also argue that even reflective, human-controlled futures could be pretty terrible because a lot of humans have (by my lights) some horrifying values. For example, human-controlled futures might accept enormous s-risks for the sake of enormous positive value, might endorse strong norms of retribution, might severely punish outgroups or heterodoxy, might value giving agents free will more than preventing harm (cf. the \"free will theodicy\"), and so on.</p>\n<p>The option-value argument works best when I specifically am the one whose options are being kept open (although even in this case there can be concerns about losing my ideals, becoming selfish, being corrupted by other influences, etc). But humanity as a whole is a very different agent from myself, and I don't trust humanity to make the same choices I would; often the exact opposite.</p>\n<p>If paperclip maximizers wait to tile the universe with paperclips because they want to first engage in a Long Reflection to figure out if those paperclips should be green or blue, or whether they should instead be making staples, this isn't exactly reassuring.</p>\n", "parentCommentId": null, "user": {"username": "Brian_Tomasik"}}]