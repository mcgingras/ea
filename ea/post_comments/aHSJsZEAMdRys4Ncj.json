[{"_id": "eZoyQqkfTzxC848nt", "postedAt": "2015-07-20T20:25:49.459Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>Mr. Musk has personally donated <a href=\"http://futureoflife.org/AI/AI_beneficial\">$10 million via the Future of Life Institute</a> towards <a href=\"http://futureoflife.org/AI/2015awardees\">a variety of AI safety projects</a>. Additionally, MIRI is currently engaged in <a href=\"https://intelligence.org/2015/07/17/miris-2015-summer-fundraiser/\">its annual fundraising drive</a> with ambitious stretch goals, which include the hiring of several (and potentially many) additional researchers.</p>\n<p>With this in mind, Is the bottleneck to progress in AI Safety research the availability of funding or researchers? Stated differently, If a technically-competent person assesses AI Safety to be the most effective cause, which is approach <em>more</em> effective: Earning-to-give to MIRI or FLI, or becoming an AI Safety researcher?</p>\n", "parentCommentId": null, "user": {"username": "AABoyles"}}, {"_id": "YWtxXmaM4J74F7JQw", "postedAt": "2015-07-20T21:15:45.854Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>1) What careers that directly contribute to AI alignment should someone consider who is not likely suited as a researcher in fields like math or decision theory? 2) Which first steps are recommendable to end up in such a career?</p>\n", "parentCommentId": null, "user": null}, {"_id": "ojzki3mMRcWHNjCus", "postedAt": "2015-07-20T22:07:11.855Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>What about the harm Natural Intelligence is already doing?  Global Warming, economic collapse, wars, and so forth.</p>\n<p>1) Are there lessons we can learn from how Natural Intelligence already poorly serves the needs of humanity?</p>\n<p>2) How can we apply those lessons to shape the Natural Intelligence already in control towards the good of humanity?</p>\n", "parentCommentId": null, "user": {"username": "kikiorg"}}, {"_id": "cweHGf2XakMYJWwdE", "postedAt": "2015-07-21T01:20:08.236Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>Would it be valuable to develop a university level course on AI safety engineering to be implemented in hundreds of universities that use Russell's book worldwide, to attract more talented minds to the field? Which are the steps that would cause this to happen?</p>\n", "parentCommentId": null, "user": {"username": "Diego_Caleiro"}}, {"_id": "4Xg4eSsXkCRsAfjND", "postedAt": "2015-07-21T07:07:55.316Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>One answer: Apply for a job at a group like MIRI and tell them how much you plan to donate with your job if they don't hire you.  This gives them a broader researcher pool to draw from and lets them adapt to talent/funding bottlenecks dynamically.</p>\n", "parentCommentId": "eZoyQqkfTzxC848nt", "user": {"username": "John_Maxwell_IV"}}, {"_id": "P5HZuG5oEQrcyqgDv", "postedAt": "2015-07-21T11:36:24.914Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>Here are some questions of mine.  I haven't done a ton to follow discussions of AI safety, which means my questions will either be embarrassingly naive or will offer a critical outside perspective.  Please don't use any that fit the former case :)</p>\n<ul>\n<li><p>It seems like there's a decent chance that whole brain emulations will come before de novo AI.  Is there any &quot;friendly WBE&quot; work it makes sense to do to prepare for this case, analogous to &quot;friendly AI&quot; work?</p>\n</li>\n<li><p>Around the time AGI comes in to existence, it's important how cheap and fast the hardware available to run it on is.  If hardware is relatively expensive and slow, we can anticipate a slower (and presumably more graceful) transition.  Is there anything we can do to nudge the hardware industry away from developing ever-faster chips, so hardware will be relatively expensive and slow at the time of the transition?  For example, Musk could try to hire away researchers at semiconductor firms to work on batteries or rocket ships, but this could only be a temporary solution: the wages for such researchers would rise in response to the shortage, likely leading to more students going in to semiconductor research.  (Hiring away <em>professors that teach</em> semiconductor research might be a better idea, <a href=\"http://www.upenn.edu/gazette/0113/feature2_1.html\">assuming American companies are bad at training employees</a>.)</p>\n</li>\n<li><p>In <a href=\"http://lesswrong.com/lw/gn3/why_ai_may_not_foom/\">this essay</a>, I wrote: &quot;At some point our AGI will be just as smart as the world's AI researchers, but we can hardly expect to start seeing super-fast AI progress at that point, because the world's AI researchers haven't produced super-fast AI progress.&quot;  I still haven't seen a persuasive refutation of this position (though I haven't looked very hard).  So: Given that human AI researchers haven't produced a FOOM, is there any reason to expect that an AI equal to the level of the human AI research community would produce a FOOM?  (EDIT: A better framing might be whether or not we can expect chunky, important AI insights to be discovered in the future, or whether AGI will come out of many small cumulative developments.  I suppose the idea that the brain implements a single cortical algorithm should push us in the direction of believing there is at least one chunky undiscovered insight?)</p>\n</li>\n</ul>\n", "parentCommentId": null, "user": {"username": "John_Maxwell_IV"}}, {"_id": "FQ5P7XW2RKdNXmazL", "postedAt": "2015-07-21T12:00:57.271Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>Assuming human-level AGI is expensive and of potential military value, it seems likely the governments of USA and probably other powers like China will be strongly involved in its development.</p>\n<p>Is it now time to create an official process of international government-level coordination about AI safety? Is it realistic and desirable?</p>\n", "parentCommentId": null, "user": {"username": "DisposableUsername"}}, {"_id": "ywC3YZx8FJxWfG9iz", "postedAt": "2015-07-21T14:41:59.876Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>GiveWell's Holden Karnofsky <a href=\"http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/\">assessed the Singularity Institute</a> in 2012, and provided a thoughtful, extensive critique of the mission and approach which still remains tied for the <a href=\"http://lesswrong.com/top/\">top post on Lesswrong</a>. It seems the EA Meta-charity evaluators are still hesitant to name AI Safety (and more broadly, Existential Risk Reduction) as a potentially effective target for donations. What are you doing to change that?</p>\n", "parentCommentId": null, "user": {"username": "AABoyles"}}, {"_id": "auA84jcEN8i7W8d4H", "postedAt": "2015-07-21T15:08:16.733Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>Related: What is your estimate of the field's room-for-funding for the next few years?</p>\n", "parentCommentId": "eZoyQqkfTzxC848nt", "user": {"username": "AABoyles"}}, {"_id": "Sk5TEq8bc4hbHZgPa", "postedAt": "2015-07-22T10:07:16.240Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>It seems to me that even the most optimistic versions of friendly super-AI are discordant with current values across society. Why isn't there more discussion about how AI development itself can be regulated, delayed and stopped? What's going on in this space? What might work?</p>\n", "parentCommentId": null, "user": {"username": "tomstocker"}}, {"_id": "4P5BaWWsyEgz97CzK", "postedAt": "2015-07-22T10:07:47.756Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>Do you think that the human race is more likely to be wiped out in a world with AGI or in a world without AGI? Why?</p>\n", "parentCommentId": null, "user": {"username": "tomstocker"}}, {"_id": "NwaaeG7wsyaujtEWC", "postedAt": "2015-07-24T22:59:28.477Z", "postId": "aHSJsZEAMdRys4Ncj", "htmlBody": "<p>I would expand this to all sentient life, not just humanity. When you do that, natural intelligence looks far worse.</p>\n", "parentCommentId": "ojzki3mMRcWHNjCus", "user": {"username": "zdgroff"}}]