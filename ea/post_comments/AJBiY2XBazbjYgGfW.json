[{"_id": "hZXWYj2wRnaDFaGXy", "postedAt": "2023-12-31T13:23:10.661Z", "postId": "AJBiY2XBazbjYgGfW", "htmlBody": "<p>Both approaches are important components of a comprehensive AI safety strategy. With that said, I think that improving goal-directedness (as you've defined it here) is likely to yield more fruitful long-term results for AI safety because:&nbsp;</p><ol><li>A sufficiently advanced AGI (what is often labeled ASI, above human level) could outsmart any guardrails implemented by humans given enough time and compute power</li><li>Guardrails seem (as you mentioned) to be specifically an approach dedicated <i>stopping an unaligned AI from causing damage.</i> It does not actually get us closer to an aligned AI. If our goal is alignment, why should the primary focus be on an activity that doesn't get us any closer to aligning an AI?</li></ol>", "parentCommentId": null, "user": {"username": "Hayven Jackson"}}, {"_id": "yQkrPrJWSQwWcapLv", "postedAt": "2023-12-31T15:27:35.704Z", "postId": "AJBiY2XBazbjYgGfW", "htmlBody": "<p>Thanks for your comment!</p>\n<p>I think a sufficiently intelligent ASI is equally likely to outsmart human goal-directedness efforts as it is to outsmart guardrails.</p>\n<p>I think number 2 is a good point.</p>\n<p>There are many people who actively want to create an aligned ASI as soon as possible to reap its benefits, for whom my suggestion is not useful.</p>\n<p>But there are others who primarily want to prevent the creation of a misaligned ASI, and are willing to forgo the creation of an ASI if necessary.</p>\n<p>There are also others who want to create an aligned ASI, but are willing to considerably delay this to improve the chances that the ASI is aligned.</p>\n<p>I think my suggestion is mainly useful for these second and third groups.</p>\n", "parentCommentId": "hZXWYj2wRnaDFaGXy", "user": {"username": "freedomandutility"}}]