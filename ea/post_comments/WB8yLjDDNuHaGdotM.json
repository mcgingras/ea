[{"_id": "b88BK2rwpKbCrut9X", "postedAt": "2023-02-25T14:27:59.646Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>This is a really helpful post - thank you! It does blow my mind slightly that this isn't more broadly practiced, if the argument holds, but I think it holds!&nbsp;</p><p>I don't know enough about the market for academic papers, but I wonder if you'd be interested in writing this up for a more academic audience? You could look at some set of recent RCTs and estimate the potential savings (or, more ambitiously, the increase in power and associated improvement in detecting results)&nbsp;<br><br>Given that the argument is statistical rather than practical in any way that is specific to economics or development, do you know if this happens in biomedicine? Many trials often involve pitting newer, more expensive interventions against an existing standard of care.</p>", "parentCommentId": null, "user": {"username": "ChrisSmith"}}, {"_id": "LhpHkkNwJ3WiN5Kmv", "postedAt": "2023-02-25T16:54:00.332Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>The idea makes a lot of sense,  but my guess is that the circumstance where the cost is driven by the intervention itself isn\u2019t that common: In the context of charities, we\u2019re thinking about applying RCTs to test whether an intervention works. Generally the intervention is happening anyway. The cost of RCTs then doesn\u2019t come from applying the intervention to the treatment group - it comes from establishing the experimental conditions where you have a randomised group of participants and the ability to collect data on them.</p>\n", "parentCommentId": null, "user": {"username": "Aidan Alexander"}}, {"_id": "WgdP6PmwPP68QTkRG", "postedAt": "2023-02-25T17:43:50.521Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Great argument. My guess for why this isn't common based on a little experience is that the decision is usually sequential. First you calculate a sample size based on power requirements, and then you fundraise for that budget (and usually the grantmaker asks for your power calculations, so it does have to be sequential). This doesn't inherently prevent you from factoring intervention cost into the power calculations, but it does mean the budget constraint is not salient.</p>\n<p>I wouldn't be too surprised ex ante if there are inefficiencies in how we do randomization. This is an area with quite active research, such as <a href=\"https://www.aeaweb.org/articles?id=10.1257/aer.20201856\">this 2022 paper</a> which proposes a really basic shift in randomization procedures and yet shows its power benefits.</p>\n", "parentCommentId": null, "user": {"username": "therealslimkt"}}, {"_id": "mbnP5vviDGGh9QE7Z", "postedAt": "2023-02-25T18:24:25.023Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Often recruiting is the bottleneck in biomedicine so you want to maximise the power for a given number of participants</p>\n", "parentCommentId": "b88BK2rwpKbCrut9X", "user": {"username": "jooke"}}, {"_id": "gG8nowkuhrpc3wHBs", "postedAt": "2023-02-25T18:47:27.257Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Great suggestion, particularly as you say for trials with a super expensive treatment relative to control.<br><br>In defense of current practice, I'd like to add that a major difficulty when running medical trials &nbsp;for new therapeutics is simply <i>recruiting patients to the trial</i>. &nbsp;Many patients enroll on the trial with the aim of getting the experimental treatment, so it's a lot easier to recruit people when your trial has a 50% or 75% chance of assignment to therapeutic arm. &nbsp;<br><br>Some other important strategies that are currently hot right now:<br>Platform trials: One giant trial that has one control arm and maybe three to four treatment arms. &nbsp;Hard to do as it requires a lot of people to work together but amazing when you pull them off (e.g. we did many of these for COVID)<br>Use of historical or shared control data: Why recruit as many controls if you can integrate existing data in a statistically principled, unbiased way (easier said than done of course).</p>", "parentCommentId": null, "user": {"username": "gotech"}}, {"_id": "yFNkLjQM9HcxNY8q7", "postedAt": "2023-02-25T19:14:50.057Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>I assumed more people were aware of this. I'm using it in a trial we're about to start. But as others have said, in many  trials the treatment is not particularly more costly.  But probably a factor in detailed interventions in poverty and health in poor countries. Have you looked into how many studies in development economics and GH&amp;D with costly interventions do this?</p>\n", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "8zfAAZzbfAWbvipjK", "postedAt": "2023-02-26T01:05:05.865Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>I'm confused why the process being sequential is a reason that this isn't occurring. Suppose someone was writing a RCT grant proposal and knew in advance how expensive the treatment was compared to the control. They find the optimal ratio of treatment to control, based on the post above. Then, they ask for however much money they need to get a certain amount of power (which would be less money than they would have needed to ask for not doing this).<br><br>Or alternatively, run the sample size calculation as you suggest. Convert that into a $ figure, then use the information in the post above to get more power for that same amount of money and show the grant-maker the second version of one's power calculations.</p>", "parentCommentId": "WgdP6PmwPP68QTkRG", "user": {"username": "Hank_B"}}, {"_id": "aB9C45rkwrDgurLq7", "postedAt": "2023-02-26T01:49:29.771Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Hey Aidan-- that's a good point. I think it will probably apply to different extents for different cases, but probably not to all cases. Some scenarios I can imagine:</p><ol><li><strong>A charity uses its own funds to run an RCT of a program it already runs at scale</strong>:&nbsp;<ul><li>In this case, you are right that treatment is happening \"anyway\" and in a sense the $ saved in having a smaller treatment group will just end up being spent on more \"treatment\", &nbsp;just not in the RCT.&nbsp;</li><li>Even in this case I think the charity would prefer to fund its intervention in a non-RCT context: providing an intervention in an RCT context is inherently costlier than doing it under more normal circumstances, for example if you are delivering assets, your trucks have to drive past control villages to get to treatment ones, increasing delivery costs.&nbsp;</li><li>That's pretty small though, I agree that otherwise the intervention is basically \"already happening\" and the effective savings are smaller than implied in my post</li><li>That said, if the charity has good reason to think their intervention works and so spending more on treatment is \"good\", the value of the RCT in the first place seems lower to me</li></ul></li></ol><ul><li><strong>2) A charity uses its own funds to run an RCT of a trial program it doesn't operate at scale:</strong><ul><li>In this case, the charity is running the RCT because it isn't sure the intervention is a good one</li><li>Reducing the RCT treatment group frees up funds for the charity to spend on the programs that it does know work, with overall higher EV</li></ul></li><li><strong>3) A donor wants to fund RCTs to generate more evidence:&nbsp;</strong><ul><li>The donor is funding the RCT because they aren't sure the intervention works</li><li>Keeping RCT costs lower means they can fund more RCTs, or more proven interventions</li></ul></li><li><strong>4) A charity applies for donor funds for an RCT of a new program:</strong><ul><li>In this case, the cheaper study is more likely to get funded, so the larger control/smaller treatment is a better option for the charity</li></ul></li></ul><p>Overall, I think cases 2/3/4 benefit from the cheaper study. Scenario 1 seems more like what you have in mind and is a good point, I just think there will be enough scenarios where the cheaper trial <i>is </i>useful, and in those cases the charity might consider this treatment/control optimisation.&nbsp;</p>", "parentCommentId": "LhpHkkNwJ3WiN5Kmv", "user": {"username": "Rory Fenton"}}, {"_id": "mX6nCKxgAefKz8HBC", "postedAt": "2023-02-26T06:51:13.686Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Thanks Rory &nbsp;- I think your general idea is good, and in some cases could be a good option!</p><p>I could be wrong, but from my experience working in the development world these 4 scenarios aren't really how RCTs generally happen. Usually there will be a partnership with a RCT running NGO &nbsp;(like IPA) or a university department (J-PAL at MIT) where the partner organisation pays for and organise everything.</p><p>Sometimes scenario 4 could happen as part of a grant application</p>", "parentCommentId": "aB9C45rkwrDgurLq7", "user": {"username": "NickLaing"}}, {"_id": "HsDGbr2bqcmrewJEu", "postedAt": "2023-02-27T00:30:03.729Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>I'm surprised you retracted the comment because I agree with it and I'm not 100% sure what I meant. It is still a salience issue but I don't think the sequential process really matters for that</p>\n", "parentCommentId": "8zfAAZzbfAWbvipjK", "user": {"username": "therealslimkt"}}, {"_id": "wMPdJdngmgNusdrtE", "postedAt": "2023-02-27T04:09:51.748Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>To explain why I retracted: I re-read your original post and noticed that you were talking about salience, and I think you're probably right that this isn't a very salient aspect of the process. At first, I thought you were saying something like 'the steps occur sequentially, so the suggestion of the post can't be implemented' which seems wrong. But 'the steps occur sequentially, so it might not occur to someone to back-track in their thinking and revise the result they got in the first step afterwards' seems probably right, although I have no idea how big of an explanation that is compared to other reasons the OP's suggestion isn't very common.</p>", "parentCommentId": "HsDGbr2bqcmrewJEu", "user": {"username": "Hank_B"}}, {"_id": "PiQEBzjXNjf8gHpqQ", "postedAt": "2023-02-27T23:43:15.120Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>You seem to assume that there's a linear relationship between the intervention and the effect. This might be the case for cash transfers but it's not the case for many other interventions.</p><p>If you give someone half of a betnet they are not 50% as much protected.&nbsp;</p><p>When it comes to medical treatments it might be that certain side effects only appear at a given dose and as a result you have to do your clinical trial for the dose that you actually want to put into the pill that you sell.&nbsp;</p>", "parentCommentId": null, "user": {"username": "ChristianKleineidam"}}, {"_id": "GM47puaS9QpJtkbGu", "postedAt": "2023-02-28T02:46:30.546Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Ah, that's helpful data. My experience in RCTs mostly comes from One Acre Fund, where we ran <i>lots </i>of RCTs internally on experimental programs, or just A/B tests, but that might not be very typical!</p>", "parentCommentId": "mX6nCKxgAefKz8HBC", "user": {"username": "Rory Fenton"}}, {"_id": "6fJCcoAa33jNW8pEu", "postedAt": "2023-02-28T03:16:46.639Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Hi Christian-- agreed but my argument here is really for <i>fewer</i> &nbsp;treatment participants, not smaller treatment doses</p>", "parentCommentId": "PiQEBzjXNjf8gHpqQ", "user": {"username": "Rory Fenton"}}, {"_id": "QKFxAkesx9Y5cczuZ", "postedAt": "2023-02-28T03:33:00.403Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>As a quick data point I just checked the 6 RCTs GiveDirectly list on <a href=\"https://www.givedirectly.org/research-at-give-directly/)\">their website</a>. I figure cash is pretty expensive so it's the kind of intervention where this makes sense.&nbsp;</p><p>It looks like most cash studies, certainly with just 1 treatment arm, aren't optimising for cost:&nbsp;</p><figure class=\"table\"><table><tbody><tr><td>Study</td><td>Control</td><td>Treatment</td></tr><tr><td>The short-term impact of unconditional cash transfers to the poor: experimental evidence from Kenya</td><td>432</td><td>503</td></tr><tr><td>BENCHMARKING A CHILD NUTRITION PROGRAM<br>AGAINST CASH: EVIDENCE FROM RWANDA</td><td>74 villages</td><td>74 villages (nutrition program)<br>100 (cash)</td></tr><tr><td>Cash crop: evaluating large cash transfers to coffee<br>farming communities in Uganda</td><td>1894</td><td>1894</td></tr><tr><td>Using Household Grants to Benchmark the Cost Effectiveness of a<br>USAID Workforce Readiness Program</td><td>488</td><td>485 NGO program<br>762 cash<br>203 cash + NGO</td></tr><tr><td>General equilibrium effects of cash transfers:<br>experimental evidence from Kenya</td><td>325 villages</td><td>328 villages</td></tr><tr><td>Effects of a Universal Basic Income during the pandemic</td><td>100 villages</td><td>44 longterm UBI<br>80 shortterm UBI<br>71 lump sum</td></tr></tbody></table></figure><p>Suggests either 1) there's some value in sharing this idea more or 2) there's a good reason these economists <i>aren't </i>making this adjustment. <a href=\"https://twitter.com/emory_rchrdsn/status/1629595754762313728\">Someone on Twitter </a>suggested \"problems caused by unbalanced samples and heteroskedasticity\" but that was beyond my poor epidemiologist's understanding and they didn't clarify further.</p>", "parentCommentId": "yFNkLjQM9HcxNY8q7", "user": {"username": "Rory Fenton"}}, {"_id": "TP8LB6A49LQBetrDb", "postedAt": "2023-02-28T04:23:56.740Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>You\u2019re completely correct! However, it\u2019s worth noting this is standard practice (when the treatment makes up most of the cost, which it usually doesn\u2019t). Most statisticians will be able to tell you about this.</p>\n<p>So I think I have two comments:</p>\n<ol>\n<li>It\u2019s actually pretty neat you figured this out by yourself, and shows you have a decent intuition for the subject.</li>\n<li>However, if you\u2019re a researcher at any kind of research institution, and you run or design RCTs, this suggests an organizational problem. You\u2019re reinventing the wheel, and need to consult with a statistician. It\u2019s very, <em>very</em> difficult to do good research without a statistician, no matter how clever you are. (If you\u2019d like, I\u2019m happy to help if you send me a DM.)</li>\n</ol>\n", "parentCommentId": null, "user": {"username": "Closed Limelike Curves"}}, {"_id": "uxnbhySeaDY4dMxFC", "postedAt": "2023-02-28T04:55:46.733Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Thanks Chris, that's a cool idea. I will give it a go (in a few days, I have an EAG to recover from...)</p><p>One thing I should note is that other comments on this post are suggesting this is well known and applied, which doesn't knock the idea but would reduce the value of doing more promotion. Conversely, my super quick, low-N look into cash RCTs (in my reply below to David Reinstein) suggests it is not so common. Since the approach you suggest would partly involve listing a bunch of RCTs and their treatment/control sizes (so we can see whether they are cost-optimised), it could also serve as a nice check of just how often this adjustment is/isn't applied in RCTs</p><p>For bio, that's way outside of my field, I defer to Joshua's comment here on limited participant numbers, which makes sense. Though in a situation like early COVID vaccine trials, where perhaps you had limited treatment doses and potentially lots of willing volunteers, perhaps it would be more applicable? I guess pharma companies are heavily incentivised to optimise trial costs tho, if they don't do it there'll be a reason!</p>", "parentCommentId": "b88BK2rwpKbCrut9X", "user": {"username": "Rory Fenton"}}, {"_id": "uddiyoxcZ6TEzZBx9", "postedAt": "2023-02-28T06:54:33.052Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>This doesn't change the existence of a budget constraint, though. The partner organization, especially a grant funder like JPAL/IPA, will grant you a certain amount of their resources to use. I don't see why you wouldn't want to optimize the use of their resources.</p>\n", "parentCommentId": "mX6nCKxgAefKz8HBC", "user": {"username": "therealslimkt"}}, {"_id": "av2qS6sfXfEtzD2Xu", "postedAt": "2023-02-28T07:10:27.995Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>100% the original post stands, in any scenario we would want to optimise use of resources. I don't think JPAL/IPA is generally a funder though - they do the research themselves so they are the ones to convince ;).</p>", "parentCommentId": "uddiyoxcZ6TEzZBx9", "user": {"username": "NickLaing"}}, {"_id": "r2gs9494fiiqYw9Gr", "postedAt": "2023-02-28T13:18:45.104Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>The \u201cproblems caused by unbalanced samples\u201d doesn\u2019t seem coherent to me; I\u2019m not sure what they are talking about.</p>\n<p>If the underlying variance is different between the treatment and the control group:</p>\n<ul>\n<li>That might justify a larger sample for the group with larger variance</li>\n<li>But I would expect the expected variance to tend to be larger for the <em>treatment</em> group in many/most relevant cases</li>\n<li>Overall, there will still tend to be some efficiency advantage of having more of the less-costly group, generally the control group</li>\n</ul>\n", "parentCommentId": "QKFxAkesx9Y5cczuZ", "user": {"username": "david_reinstein"}}, {"_id": "bzASidkbtLGpWRpWe", "postedAt": "2023-03-02T19:55:41.947Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Would be super interested to see the results of some of these RCTs / AB tests. Were any of them published apart from the Lime SMS study? We're looking for great examples of learning orgs that do this and some studies from 1AF would be a great motivator/example.</p>", "parentCommentId": "GM47puaS9QpJtkbGu", "user": {"username": "roboton"}}, {"_id": "GHWkHFfbDrpr5vbrM", "postedAt": "2023-03-04T11:32:22.562Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Unbalanced samples are not a problem per se. You can run into a problem of representation/generalization for the smaller sample but this argument is independent of balancing and only has to do with small sample sizes.</p><p><a href=\"https://forum.effectivealtruism.org/users/david_reinstein?mention=user\">@david_reinstein</a> made an excellent point about heteroscedasticity / variance. To factor this into your original post: You want to optimize the cost-effectiveness of the <i>precision</i> of your group-level difference score. This is achieved by minimizing the standard errors (SE) of the group-level estimates of each sample, which are just the standard deviations (SD) divided by the square root of the respective observations. So your term would expand to:&nbsp;<br>Control-to-treat-ratio = sqrt(treatment_cost/control_cost) * control_SD/treatment_SD.<br>The problem, in practice, is that you usually know the costs a priori but not the SDs. If variances are not equal, however, I would agree with <a href=\"https://forum.effectivealtruism.org/users/david_reinstein?mention=user\">@david_reinstein</a> that the treatment group will more likely show greater variance on your outcome variable (if control group has more variance, I would rather reconsider the choice of the outcome variable).</p><p>If you want to read more about the concept of precision and its relation to statistical power (also cf. the paper that <a href=\"https://forum.effectivealtruism.org/users/karthik-tadepalli?mention=user\">@Karthik Tadepalli</a> cited), we just put together a preprint here that is supposed to double as a teaching ressource: <a href=\"https://doi.org/10.31234/osf.io/m8c4k\">https://doi.org/10.31234/osf.io/m8c4k</a> (introduction and discussion will suffice since the middle part focusses on biological/neuroscientific measurements that have vastly different properties than, e.g., questionnaire data).<br>Here is the glossary that is mentioned in the &nbsp;paper: <a href=\"https://osf.io/2wjc4\">https://osf.io/2wjc4</a><br>And here is the associated Twitter post with some digest about the most important insights: https://twitter.com/bioDGPs_DGPA/status/1616014732254756865</p>", "parentCommentId": "r2gs9494fiiqYw9Gr", "user": {"username": "Mario Reutter"}}, {"_id": "vbodx8KuLWfzjbHcM", "postedAt": "2023-03-23T23:03:21.124Z", "postId": "WB8yLjDDNuHaGdotM", "htmlBody": "<p>Actually, maybe I should clarify this. This is standard practice when you hire a decent statistician. We've known this since like... the 1940s, maybe?</p><p>But a lot of organizations and clinical trials don't do this because they don't consult with a statistician early enough. I've had people come to me and say \"hey, here's a pile of data, can you calculate a p-value?\" too many times to count. Yes, I calculated a p-value, it's like 0.06, and if you'd come to me at the <i>start</i> of the experiment we could've avoided the million-dollar boondoggle</p><p>&nbsp;that you just created.</p>", "parentCommentId": "TP8LB6A49LQBetrDb", "user": {"username": "Closed Limelike Curves"}}]